<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<title confidence="0.9992335">
Robust Approach to Abbreviating Terms:
A Discriminative Latent Variable Model with Global Information
</title>
<author confidence="0.999402">
Xu Sun†, Naoaki Okazaki†, Jun’ichi Tsujii†‡§
</author>
<affiliation confidence="0.97245225">
†Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
‡School of Computer Science, University of Manchester, UK
§National Centre for Text Mining, UK
</affiliation>
<email confidence="0.988148">
{sunxu, okazaki, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999695190476191">
The present paper describes a robust ap-
proach for abbreviating terms. First, in
order to incorporate non-local informa-
tion into abbreviation generation tasks, we
present both implicit and explicit solu-
tions: the latent variable model, or alter-
natively, the label encoding approach with
global information. Although the two ap-
proaches compete with one another, we
demonstrate that these approaches are also
complementary. By combining these two
approaches, experiments revealed that the
proposed abbreviation generator achieved
the best results for both the Chinese and
English languages. Moreover, we directly
apply our generator to perform a very dif-
ferent task from tradition, the abbreviation
recognition. Experiments revealed that the
proposed model worked robustly, and out-
performed five out of six state-of-the-art
abbreviation recognizers.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946826923077">
Abbreviations represent fully expanded forms
(e.g., hidden markov model) through the use of
shortened forms (e.g., HMM). At the same time,
abbreviations increase the ambiguity in a text.
For example, in computational linguistics, the
acronym HMM stands for hidden markov model,
whereas, in the field of biochemistry, HMM is gen-
erally an abbreviation for heavy meromyosin. As-
sociating abbreviations with their fully expanded
forms is of great importance in various NLP ap-
plications (Pakhomov, 2002; Yu et al., 2006;
HaCohen-Kerner et al., 2008).
The core technology for abbreviation disam-
biguation is to recognize the abbreviation defini-
tions in the actual text. Chang and Sch¨utze (2006)
reported that 64,242 new abbreviations were intro-
duced into the biomedical literatures in 2004. As
such, it is important to maintain sense inventories
(lists of abbreviation definitions) that are updated
with the neologisms. In addition, based on the
one-sense-per-discourse assumption, the recogni-
tion of abbreviation definitions assumes senses of
abbreviations that are locally defined in a docu-
ment. Therefore, a number of studies have at-
tempted to model the generation processes of ab-
breviations: e.g., inferring the abbreviating mech-
anism of the hidden markov model into HMM.
An obvious approach is to manually design
rules for abbreviations. Early studies attempted
to determine the generic rules that humans use
to intuitively abbreviate given words (Barrett and
Grems, 1960; Bourne and Ford, 1961). Since
the late 1990s, researchers have presented var-
ious methods by which to extract abbreviation
definitions that appear in actual texts (Taghva
and Gilbreth, 1999; Park and Byrd, 2001; Wren
and Garner, 2002; Schwartz and Hearst, 2003;
Adar, 2004; Ao and Takagi, 2005). For example,
Schwartz and Hearst (2003) implemented a simple
algorithm that mapped all alpha-numerical letters
in an abbreviation to its expanded form, starting
from the end of both the abbreviation and its ex-
panded forms, and moving from right to left.
These studies performed highly, especially for
English abbreviations. However, a more extensive
investigation of abbreviations is needed in order to
further improve definition extraction. In addition,
we cannot simply transfer the knowledge of the
hand-crafted rules from one language to another.
For instance, in English, abbreviation characters
are preferably chosen from the initial and/or cap-
ital characters in their full forms, whereas some
</bodyText>
<page confidence="0.972915">
905
</page>
<note confidence="0.976897">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
(b): Chinese Abbreviation Generation
</note>
<figureCaption confidence="0.963221">
Figure 1: English (a) and Chinese (b) abbreviation
generation as a sequential labeling problem.
</figureCaption>
<figure confidence="0.681962">
CRF DPLVM
</figure>
<figureCaption confidence="0.967874">
Figure 2: CRF vs. DPLVM. Variables x, y, and h
represent observation, label, and latent variables,
respectively.
</figureCaption>
<figure confidence="0.993914647058824">
Institute of History and Philology at Academia Sinica
历 史 语 言 研 究 所
S S S
P
[史语所]
S
P P
x1 x2 xm
y1 y2 ym
y1
h1 h2 hm
x1
x2 xm
y2 ym
p o l y g l y c o l i c a c i d
P S S S P S S S S S S S S P S S S [PGA]
(a): English Abbreviation Generation
</figure>
<bodyText confidence="0.975433696428572">
other languages, including Chinese and Japanese,
do not have word boundaries or case sensitivity.
A number of recent studies have investigated
the use of machine learning techniques. Tsuruoka
et al. (2005) formalized the processes of abbrevia-
tion generation as a sequence labeling problem. In
the present study, each character in the expanded
form is tagged with a label, y E {P, S11, where
the label P produces the current character and
the label S skips the current character. In Fig-
ure 1 (a), the abbreviation PGA is generated from
the full form polyglycolic acid because the under-
lined characters are tagged with P labels. In Fig-
ure 1 (b), the abbreviation is generated using the
2nd and 3rd characters, skipping the subsequent
three characters, and then using the 7th character.
In order to formalize this task as a sequential
labeling problem, we have assumed that the la-
bel of a character is determined by the local in-
formation of the character and its previous label.
However, this assumption is not ideal for model-
ing abbreviations. For example, the model can-
not make use of the number of words in a full
form to determine and generate a suitable num-
ber of letters for the abbreviation. In addition, the
model would be able to recognize the abbreviat-
ing process in Figure 1 (a) more reasonably if it
were able to segment the word polyglycolic into
smaller regions, e.g., poly-glycolic. Even though
humans may use global or non-local information
to abbreviate words, previous studies have not in-
corporated this information into a sequential label-
ing model.
In the present paper, we propose implicit and
explicit solutions for incorporating non-local in-
formation. The implicit solution is based on the
1Although the original paper of Tsuruoka et al. (2005) at-
tached case sensitivity information to the P label, for simplic-
ity, we herein omit this information.
discriminative probabilistic latent variable model
(DPLVM) in which non-local information is mod-
eled by latent variables. We manually encode non-
local information into the labels in order to provide
an explicit solution. We evaluate the models on the
task of abbreviation generation, in which a model
produces an abbreviation for a given full form. Ex-
perimental results indicate that the proposed mod-
els significantly outperform previous abbreviation
generation studies. In addition, we apply the pro-
posed models to the task of abbreviation recogni-
tion, in which a model extracts the abbreviation
definitions in a given text. To the extent of our
knowledge, this is the first model that can per-
form both abbreviation generation and recognition
at the state-of-the-art level, across different lan-
guages and with a simple feature set.
</bodyText>
<sectionHeader confidence="0.996127" genericHeader="method">
2 Abbreviator with Non-local
Information
</sectionHeader>
<subsectionHeader confidence="0.996246">
2.1 A Latent Variable Abbreviator
</subsectionHeader>
<bodyText confidence="0.982315555555556">
To implicitly incorporate non-local information,
we propose discriminative probabilistic latent
variable models (DPLVMs) (Morency et al., 2007;
Petrov and Klein, 2008) for abbreviating terms.
The DPLVM is a natural extension of the CRF
model (see Figure 2), which is a special case of the
DPLVM, with only one latent variable assigned for
each label. The DPLVM uses latent variables to
capture additional information that may not be ex-
pressed by the observable labels. For example, us-
ing the DPLVM, a possible feature could be “the
current character xi = X, the label yi = P, and
the latent variable hi = LV.” The non-local infor-
mation can be effectively modeled in the DPLVM,
and the additional information at the previous po-
sition or many of the other positions in the past
could be transferred via the latent variables (see
Figure 2).
</bodyText>
<page confidence="0.997541">
906
</page>
<bodyText confidence="0.999796">
Using the label set Y = {P, S}, abbreviation
generation is formalized as the task of assigning
a sequence of labels y = y1, y2, ... , ym for a
given sequence of characters x = x1, x2, ... , xm
in an expanded form. Each label, yj, is a mem-
ber of the possible labels Y . For each sequence,
we also assume a sequence of latent variables
h = h1, h2, ... , hm, which are unobservable in
training examples.
We model the conditional probability of the la-
bel sequence P(y|x) using the DPLVM,
</bodyText>
<equation confidence="0.9943565">
P (y|x, O) = E P(y|h, x, O)P(h|x, O). (1)
h
</equation>
<bodyText confidence="0.967383833333333">
Here, O represents the parameters of the model.
To ensure that the training and inference are ef-
ficient, the model is often restricted to have dis-
jointed sets of latent variables associated with each
label (Morency et al., 2007). Each hj is a member
in a set Hyp of possible latent variables for the la-
bel yj. Here, H is defined as the set of all possi-
ble latent variables, i.e., H is the union of all Hyp
sets. Since the sequences having hj V Hyp will,
by definition, yield P(y|x, O) = 0, the model is
rewritten as follows (Morency et al., 2007; Petrov
and Klein, 2008):
</bodyText>
<equation confidence="0.99291">
P(y|x, O) = E P(h|x, O). (2)
hEHy1x...xHyM
</equation>
<bodyText confidence="0.9858775">
Here, P(h|x, O) is defined by the usual formula-
tion of the conditional random field,
</bodyText>
<equation confidence="0.988532">
= exp O · f (h, x)
P(hlx, O
( I ) �yh exp O·f(h,x), (3)
</equation>
<bodyText confidence="0.996586">
where f(h, x) represents a feature vector.
Given a training set consisting of n instances,
(xi, yi) (for i = 1... n), we estimate the pa-
rameters O by maximizing the regularized log-
likelihood,
</bodyText>
<equation confidence="0.992853666666667">
n
L(O) = log P(yi|xi, O) − R(O). (4)
i=1
</equation>
<bodyText confidence="0.99995975">
The first term expresses the conditional log-
likelihood of the training data, and the second term
represents a regularizer that reduces the overfitting
problem in parameter estimation.
</bodyText>
<subsectionHeader confidence="0.991233">
2.2 Label Encoding with Global Information
</subsectionHeader>
<bodyText confidence="0.9982545">
Alternatively, we can design the labels such that
they explicitly incorporate non-local information.
</bodyText>
<figure confidence="0.75788775">
Management office of the imports and exports of endangered species
R*att#aflC7 A b, 4� _,a
S S P S S S S S S P S P S S
S0 S0 P1 S1 S1 S1 S1 S1 S1 P2 S2 P3 S3 S3
</figure>
<figureCaption confidence="0.699388333333333">
Figure 3: Comparison of the proposed label en-
coding method with global information (GI) and
the conventional label encoding method.
</figureCaption>
<bodyText confidence="0.999864535714286">
In this approach, the label yi at position i at-
taches the information of the abbreviation length
generated by its previous labels, y1, y2,.. . , yi_1.
Figure 3 shows an example of a Chinese abbre-
viation. In this encoding, a label not only con-
tains the produce or skip information, but also the
abbreviation-length information, i.e., the label in-
cludes the number of all P labels preceding the
current position. We refer to this method as label
encoding with global information (hereinafter GI).
The concept of using label encoding to incorporate
non-local information was originally proposed by
Peshkin and Pfeffer (2003).
Note that the model-complexity is increased
only by the increase in the number of labels. Since
the length of the abbreviations is usually quite
short (less than five for Chinese abbreviations and
less than 10 for English abbreviations), the model
is still tractable even when using the GI encoding.
The implicit (DPLVM) and explicit (GI) solu-
tions address the same issue concerning the in-
corporation of non-local information, and there
are advantages to combining these two solutions.
Therefore, we will combine the implicit and ex-
plicit solutions by employing the GI encoding in
the DPLVM (DPLVM+GI). The effects of this
combination will be demonstrated through experi-
ments.
</bodyText>
<subsectionHeader confidence="0.996901">
2.3 Feature Design
</subsectionHeader>
<bodyText confidence="0.999892416666667">
Next, we design two types of features: language-
independent features and language-specific fea-
tures. Language-independent features can be used
for abbreviating terms in English and Chinese. We
use the features from #1 to #3 listed in Table 1.
Feature templates #4 to #7 in Table 1 are used
for Chinese abbreviations. Templates #4 and #5
express the Pinyin reading of the characters, which
represents a Romanization of the sound. Tem-
plates #6 and #7 are designed to detect character
duplication, because identical characters will nor-
mally be skipped in the abbreviation process. On
</bodyText>
<figure confidence="0.948541642857143">
Orig.
GI
907
#1 The input char. xi_1 and xi
#2 Whether xj is a numeral, for j = (i − 3) ... i
#3 The char. bigrams starting at (i − 2) ... i
#4 The Pinyin of char. xi_1 and xi
#5 The Pinyin bigrams starting at (i − 2) ... i
#6 Whether xj = xj+1, for j = (i − 2) ... i
#7 Whether xj = xj+2, for j = (i − 3) ... i
#8 Whether xj is uppercase, for j = (i − 3) ... i
#9 Whether xj is lowercase, for j = (i − 3) ... i
#10 The char. 3-grams starting at (i − 3) ... i
#11 The char. 4-grams starting at (i − 4) ... i
</figure>
<tableCaption confidence="0.865376666666667">
Table 1: Language-independent features (#1 to
#3), Chinese-specific features (#4 through #7), and
English-specific features (#8 through #11).
</tableCaption>
<bodyText confidence="0.997673294117647">
the other hand, such duplication detection features
are not so useful for English abbreviations.
Feature templates #8–#11 are designed for En-
glish abbreviations. Features #8 and #9 encode the
orthographic information of expanded forms. Fea-
tures #10 and #11 represent a contextual n-gram
with a large window size. Since the number of
letters in Chinese (more than 10K characters) is
much larger than the number of letters in English
(26 letters), in order to avoid a possible overfitting
problem, we did not apply these feature templates
to Chinese abbreviations.
Feature templates are instantiated with values
that occur in positive training examples. We used
all of the instantiated features because we found
that the low-frequency features also improved the
performance.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999089409836066">
For Chinese abbreviation generation, we used the
corpus of Sun et al. (2008), which contains 2,914
abbreviation definitions for training, and 729 pairs
for testing. This corpus consists primarily of noun
phrases (38%), organization names (32%), and
verb phrases (21%). For English abbreviation gen-
eration, we evaluated the corpus of Tsuruoka et
al. (2005). This corpus contains 1,200 aligned
pairs extracted from MEDLINE biomedical ab-
stracts (published in 2001). For both tasks, we
converted the aligned pairs of the corpora into la-
beled full forms and used the labeled full forms as
the training/evaluation data.
The evaluation metrics used in the abbreviation
generation are exact-match accuracy (hereinafter
accuracy), including top-1 accuracy, top-2 accu-
racy, and top-3 accuracy. The top-N accuracy rep-
resents the percentage of correct abbreviations that
are covered, if we take the top N candidates from
the ranked labelings of an abbreviation generator.
We implemented the DPLVM in C++ and op-
timized the system to cope with large-scale prob-
lems. We employ the feature templates defined in
Section 2.3, taking into account these 81,827 fea-
tures for the Chinese abbreviation generation task,
and the 50,149 features for the English abbrevia-
tion generation task.
For numerical optimization, we performed a
gradient descent with the Limited-Memory BFGS
(L-BFGS) optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order
Quasi-Newton method that numerically estimates
the curvature from previous gradients and up-
dates. With no requirement on specialized Hes-
sian approximation, L-BFGS can handle large-
scale problems efficiently. Since the objective
function of the DPLVM model is non-convex,
different parameter initializations normally bring
different optimization results. Therefore, to ap-
proach closer to the global optimal point, it is
recommended to perform multiple experiments on
DPLVMs with random initialization and then se-
lect a good start point. To reduce overfitting,
we employed a L2 Gaussian weight prior (Chen
and Rosenfeld, 1999), with the objective function:
L(o) = Eni�1 log P(yiIxi,o)−IJo112/Q2. Dur-
ing training and validation, we set Q = 1 for the
DPLVM generators. We also set four latent vari-
ables for each label, in order to make a compro-
mise between accuracy and efficiency.
Note that, for the label encoding with
global information, many label transitions (e.g.,
P2S3) are actually impossible: the label tran-
sitions are strictly constrained, i.e., yiyi+1 E
{PjSj,PjPj+1, SjPj+1, SjSj1. These con-
straints on the model topology (forward-backward
lattice) are enforced by giving appropriate features
a weight of −oc, thereby forcing all forbidden la-
belings to have zero probability. Sha and Pereira
(2003) originally proposed this concept of imple-
menting transition restrictions.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.997854">
4.1 Chinese Abbreviation Generation
</subsectionHeader>
<bodyText confidence="0.9984454">
First, we present the results of the Chinese abbre-
viation generation task, as listed in Table 2. To
evaluate the impact of using latent variables, we
chose the baseline system as the DPLVM, in which
each label has only one latent variable. Since this
</bodyText>
<page confidence="0.981627">
908
</page>
<table confidence="0.999935625">
Model T1A T2A T3A Time
Heu (S08) 41.6 N/A N/A N/A
HMM (S08) 46.1 N/A N/A N/A
SVM (S08) 62.7 80.4 87.7 1.3 h
CRF 64.5 81.1 88.7 0.2 h
CRF+GI 66.8 82.5 90.0 0.5 h
DPLVM 67.6 83.8 91.3 0.4 h
DPLVM+GI (*) 72.3 87.6 94.9 1.1 h
</table>
<tableCaption confidence="0.997237">
Table 2: Results of Chinese abbreviation gener-
</tableCaption>
<bodyText confidence="0.979360333333333">
ation. T1A, T2A, and T3A represent top-1, top-
2, and top-3 accuracy, respectively. The system
marked with the * symbol is the recommended
system.
special case of the DPLVM is exactly the CRF
(see Section 2.1), this case is hereinafter denoted
as the CRF. We compared the performance of the
DPLVM with the CRFs and other baseline sys-
tems, including the heuristic system (Heu), the
HMM model, and the SVM model described in
S08, i.e., Sun et al. (2008). The heuristic method
is a simple rule that produces the initial character
of each word to generate the corresponding abbre-
viation. The SVM method described by Sun et al.
(2008) is formalized as a regression problem, in
which the abbreviation candidates are scored and
ranked.
The results revealed that the latent variable
model significantly improved the performance
over the CRF model. All of its top-1, top-2,
and top-3 accuracies were consistently better than
those of the CRF model. Therefore, this demon-
strated the effectiveness of using the latent vari-
ables in Chinese abbreviation generation.
As the case for the two alternative approaches
for incorporating non-local information, the la-
tent variable method and the label encoding
method competed with one another (see DPLVM
vs. CRF+GI). The results showed that the la-
tent variable method outperformed the GI encod-
ing method by +0.8% on the top-1 accuracy. The
reason for this could be that the label encoding ap-
proach is a solution without the adaptivity on dif-
ferent instances. We will present a detailed discus-
sion comparing DPLVM and CRF+GI for the En-
glish abbreviation generation task in the next sub-
section, where the difference is more significant.
In contrast, to a larger extent, the results demon-
strate that these two alternative approaches are
complementary. Using the GI encoding further
improved the performance of the DPLVM (with
+4.7% on top-1 accuracy). We found that major
</bodyText>
<figure confidence="0.924451">
State Tobacco Monopoly Administration
0 1 2 3 4 5 6
Length of Produced Abbr.
</figure>
<figureCaption confidence="0.9776695">
Figure 5: Percentage distribution of Chinese
abbreviations/Viterbi-labelings grouped by length.
</figureCaption>
<bodyText confidence="0.999785782608696">
improvements were achieved through the more ex-
act control of the output length. An example is
shown in Figure 4. The DPLVM made correct de-
cisions at three positions, but failed to control the
abbreviation length.2 The DPLVM+GI succeeded
on this example. To perform a detailed analysis,
we collected the statistics of the length distribution
(see Figure 5) and determined that the GI encod-
ing improved the abbreviation length distribution
of the DPLVM.
In general, the results indicate that all of the se-
quential labeling models outperformed the SVM
regression model with less training time.3 In the
SVM regression approach, a large number of neg-
ative examples are explicitly generated for the
training, which slowed the process.
The proposed method, the latent variable model
with GI encoding, is 9.6% better with respect to
the top-1 accuracy compared to the best system on
this corpus, namely, the SVM regression method.
Furthermore, the top-3 accuracy of the latent vari-
able model with GI encoding is as high as 94.9%,
which is quite encouraging for practical usage.
</bodyText>
<subsectionHeader confidence="0.97107">
4.2 English Abbreviation Generation
</subsectionHeader>
<bodyText confidence="0.998499">
In the English abbreviation generation task, we
randomly selected 1,481 instances from the gen-
</bodyText>
<footnote confidence="0.9698628">
2The Chinese abbreviation with length = 4 should have
a very low probability, e.g., only 0.6% of abbreviations with
length = 4 in this corpus.
3On Intel Dual-Core Xeon 5160/3 GHz CPU, excluding
the time for feature generation and data input/output.
</footnote>
<equation confidence="0.583361">
国 家 烟 草 专 卖 局
</equation>
<figureCaption confidence="0.894434">
Figure 4: An example of the results.
</figureCaption>
<figure confidence="0.9996675">
5 P 5
5
P
P
P1 51 P2 52 52 52
国烟专局 [Wrong]
国烟局 [Correct]
DPLVM
DPLVM+GI
P
P3
40
80
70
60
50
30
20
10
0
Gold Train
Gold Test
DPLVM
DPLVM+GI
</figure>
<page confidence="0.994039">
909
</page>
<table confidence="0.999825857142857">
Model T1A T2A T3A Time
CRF 55.8 65.1 70.8 0.3 h
CRF+GI 52.7 63.2 68.7 1.3 h
CRF+GIB 56.8 66.1 71.7 1.3 h
DPLVM 57.6 67.4 73.4 0.6 h
DPLVM+GI 53.6 63.2 69.2 2.5 h
DPLVM+GIB (*) 58.3 N/A N/A 3.0 h
</table>
<tableCaption confidence="0.9130625">
Table 3: Results of English abbreviation genera-
tion.
</tableCaption>
<figure confidence="0.9865378">
somatosensory evoked potentials
(a) P1P2 P3 P4 P5 SMEPS
(b) P P P P SEPS
(a): CRF+GI with p=0.001 [Wrong]
(b): DPLVM with p=0.191 [Correct]
</figure>
<figureCaption confidence="0.98727">
Figure 6: A result of “CRF+GI vs. DPLVM”. For
simplicity, the S labels are masked.
</figureCaption>
<bodyText confidence="0.999920096774194">
eration corpus for training, and 370 instances for
testing. Table 3 shows the experimental results.
We compared the performance of the DPLVM
with the performance of the CRFs. Whereas the
use of the latent variables still significantly im-
proves the generation performance, using the GI
encoding undermined the performance in this task.
In comparing the implicit and explicit solutions
for incorporating non-local information, we can
see that the implicit approach (the DPLVM) per-
forms much better than the explicit approach (the
GI encoding). An example is shown in Figure 6.
The CRF+GI produced a Viterbi labeling with a
low probability, which is an incorrect abbrevia-
tion. The DPLVM produced the correct labeling.
To perform a systematic analysis of the
superior-performance of DPLVM compare to
CRF+GI, we collected the probability distribu-
tions (see Figure 7) of the Viterbi labelings from
these models (“DPLVM vs. CRF+GI” is high-
lighted). The curves suggest that the data sparse-
ness problem could be the reason for the differ-
ences in performance. A large percentage (37.9%)
of the Viterbi labelings from the CRF+GI (ENG)
have very small probability values (p &lt; 0.1).
For the DPLVM (ENG), there were only a few
(0.5%) Viterbi labelings with small probabilities.
Since English abbreviations are often longer than
Chinese abbreviations (length &lt; 10 in English,
whereas length &lt; 5 in Chinese4), using the GI
encoding resulted in a larger label set in English.
</bodyText>
<footnote confidence="0.982406333333333">
4See the curve DPLVM+GI (CHN) in Figure 7, which
could explain the good results of GI encoding for the Chi-
nese task.
</footnote>
<figure confidence="0.8616945">
0 0.2 0.4 0.6 0.8 1
Probability of Viterbi labeling
</figure>
<figureCaption confidence="0.9873985">
Figure 7: For various models, the probability dis-
tributions of the produced abbreviations on the test
</figureCaption>
<bodyText confidence="0.70427">
data of the English abbreviation generation task.
</bodyText>
<figure confidence="0.801183666666667">
mitomycin C
DPLVM P P MC [Wrong]
DPLVM+GI P1 P2 P3 MMC [Correct]
</figure>
<figureCaption confidence="0.698103666666667">
Figure 8: Example of abbreviations composed
of non-initials generated by the DPLVM and the
DPLVM+GI.
</figureCaption>
<bodyText confidence="0.999878708333333">
Hence, the features become more sparse than in
the Chinese case.5 Therefore, a significant number
of features could have been inadequately trained,
resulting in Viterbi labelings with low probabili-
ties. For the latent variable approach, its curve
demonstrates that it did not cause a severe data
sparseness problem.
The aforementioned analysis also explains the
poor performance of the DPLVM+GI. However,
the DPLVM+GI can actually produce correct ab-
breviations with ‘believable’ probabilities (high
probabilities) in some ‘difficult’ instances. In
Figure 8, the DPLVM produced an incorrect la-
beling for the difficult long form, whereas the
DPLVM+GI produced the correct labeling con-
taining non-initials.
Hence, we present a simple voting method to
better combine the latent variable approach with
the GI encoding method. We refer to this new
combination as GI encoding with ‘back-off’ (here-
inafter GIB): when the abbreviation generated by
the DPLVM+GI has a ‘believable’ probability
(p &gt; 0.3 in the present case), the DPLVM+GI
then outputs it. Otherwise, the system ‘backs-off’
</bodyText>
<footnote confidence="0.993016">
5In addition, the training data of the English task is much
smaller than for the Chinese task, which could make the mod-
els more sensitive to data sparseness.
</footnote>
<figure confidence="0.990169727272727">
CRF (ENG)
CRF+GI (ENG)
DPLVM (ENG)
DPLVM+GI (ENG)
DPLVM+GI (CHN)
Ptae (% 50
40
30
20
10
0
</figure>
<page confidence="0.982419">
910
</page>
<table confidence="0.998077">
Model T1A Time
CRF+GIB 67.2 0.6 h
DPLVM+GIB (*) 72.5 1.4 h
</table>
<tableCaption confidence="0.9456545">
Table 4: Re-evaluating Chinese abbreviation gen-
eration with GIB.
</tableCaption>
<table confidence="0.99962925">
Model T1A
Heu (T05) 47.3
MEMM (T05) 55.2
DPLVM (*) 57.5
</table>
<tableCaption confidence="0.9217655">
Table 5: Results of English abbreviation genera-
tion with five-fold cross validation.
</tableCaption>
<bodyText confidence="0.998894869565217">
to the parameters trained without the GI encoding
(i.e., the DPLVM).
The results in Table 3 demonstrate that the
DPVLM+GIB model significantly outperformed
the other models because the DPLVM+GI model
improved the performance in some ‘difficult’ in-
stances. The DPVLM+GIB model was robust
even when the data sparseness problem was se-
vere.
By re-evaluating the DPLVM+GIB model for
the previous Chinese abbreviation generation task,
we demonstrate that the back-off method also im-
proved the performance of the Chinese abbrevia-
tion generators (+0.2% from DPLVM+GI; see Ta-
ble 4).
Furthermore, for interests, like Tsuruoka et al.
(2005), we performed a five-fold cross-validation
on the corpus. Concerning the training time in
the cross validation, we simply chose the DPLVM
for comparison. Table 5 shows the results of the
DPLVM, the heuristic system (Heu), and the max-
imum entropy Markov model (MEMM) described
by Tsuruoka et al. (2005).
</bodyText>
<sectionHeader confidence="0.67659" genericHeader="method">
5 Recognition as a Generation Task
</sectionHeader>
<bodyText confidence="0.999611923076923">
We directly migrate this model to the abbrevia-
tion recognition task. We simplify the abbrevia-
tion recognition to a restricted generation problem
(see Figure 9). When a context expression (CE)
with a parenthetical expression (PE) is met, the
recognizer generates the Viterbi labeling for the
CE, which leads to the PE or NULL. Then, if the
Viterbi labeling leads to the PE, we can, at the
same time, use the labeling to decide the full form
within the CE. Otherwise, NULL indicates that the
PE is not an abbreviation.
For example, in Figure 9, the recognition is re-
stricted to a generation task with five possible la-
</bodyText>
<figure confidence="0.981782333333333">
... cannulate for arterial pressure (AP)...
(1) P P AP
(2) P P AP
(3) P P AP
(4) P P AP
(5) SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS NULL
</figure>
<figureCaption confidence="0.892074666666667">
Figure 9: Abbreviation recognition as a restricted
generation problem. In some labelings, the S la-
bels are masked for simplicity.
</figureCaption>
<table confidence="0.999938363636364">
Model P R F
Schwartz &amp; Hearst (SH) 97.8 94.0 95.9
SaRAD 89.1 91.9 90.5
ALICE 96.1 92.0 94.0
Chang &amp; Sch¨utze (CS) 94.2 90.0 92.1
Nadeau &amp; Turney (NT) 95.4 87.1 91.0
Okazaki et al. (OZ) 97.3 96.9 97.1
CRF 89.8 94.8 92.1
CRF+GI 93.9 97.8 95.9
DPLVM 92.5 97.7 95.1
DPLVM+GI (*) 94.2 98.1 96.1
</table>
<tableCaption confidence="0.8503145">
Table 6: Results of English abbreviation recogni-
tion.
</tableCaption>
<bodyText confidence="0.977408259259259">
belings. Other labelings are impossible, because
they will generate an abbreviation that is not AP.
If the first or second labeling is generated, AP is
selected as an abbreviation of arterial pressure. If
the third or fourth labeling is generated, then AP
is selected as an abbreviation of cannulate for ar-
terial pressure. Finally, the fifth labeling (NULL)
indicates that AP is not an abbreviation.
To evaluate the recognizer, we use the corpus6
of Okazaki et al. (2008), which contains 864 ab-
breviation definitions collected from 1,000 MED-
LINE scientific abstracts. In implementing the
recognizer, we simply use the model from the ab-
breviation generator, with the same feature tem-
plates (31,868 features) and training method; the
major difference is in the restriction (according to
the PE) of the decoding stage and penalizing the
probability values of the NULL labelings7.
For the evaluation metrics, following Okazaki
et al. (2008), we use precision (P = k/m), re-
call (R = k/n), and the F-score defined by
6The previous abbreviation generation corpus is improper
for evaluating recognizers, and there is no related research on
this corpus. In addition, there has been no report of Chinese
abbreviation recognition because there is no data available.
The previous generation corpus (Sun et al., 2008) is improper
because it lacks local contexts.
</bodyText>
<footnote confidence="0.93422725">
7Due to the data imbalance of the training corpus, we
found the probability values of the NULL labelings are ab-
normally high. To deal with this imbalance problem, we sim-
ply penalize all NULL labelings by using P = P − 0.7.
</footnote>
<page confidence="0.98363">
911
</page>
<table confidence="0.999733333333333">
Model P R F
CRF+GIB 94.0 98.9 96.4
DPLVM+GIB 94.5 99.1 96.7
</table>
<tableCaption confidence="0.8190635">
Table 7: English abbreviation recognition with
back-off.
</tableCaption>
<bodyText confidence="0.999252307692308">
2PR/(P + R), where k represents #instances in
which the system extracts correct full forms, m
represents #instances in which the system extracts
the full forms regardless of correctness, and n rep-
resents #instances that have annotated full forms.
Following Okazaki et al. (2008), we perform 10-
fold cross validation.
We prepared six state-of-the-art abbreviation
recognizers as baselines: Schwartz and Hearst’s
method (SH) (2003), SaRAD (Adar, 2004), AL-
ICE (Ao and Takagi, 2005), Chang and Sch¨utze’s
method (CS) (Chang and Sch¨utze, 2006), Nadeau
and Turney’s method (NT) (Nadeau and Turney,
2005), and Okazaki et al.’s method (OZ) (Okazaki
et al., 2008). Some methods use implementations
on the web, including SH8, CS9, and ALICE10.
The results of other methods, such as SaRAD, NT,
and OZ, are reproduced for this corpus based on
their papers (Okazaki et al., 2008).
As can be seen in Table 6, using the latent vari-
ables significantly improved the performance (see
DPLVM vs. CRF), and using the GI encoding
improved the performance of both the DPLVM
and the CRF. With the F-score of 96.1%, the
DPLVM+GI model outperformed five of six state-
of-the-art abbreviation recognizers. Note that all
of the six systems were specifically designed and
optimized for this recognition task, whereas the
proposed model is directly transported from the
generation task. Compared with the generation
task, we find that the F-measure of the abbrevia-
tion recognition task is much higher. The major
reason for this is that there are far fewer classifi-
cation candidates of the abbreviation recognition
problem, as compared to the generation problem.
For interests, we also tested the effect of the
GIB approach. Table 7 shows that the back-off
method further improved the performance of both
the DPLVM and the CRF model.
</bodyText>
<footnote confidence="0.999817666666667">
8http://biotext.berkeley.edu/software.html
9http://abbreviation.stanford.edu/
10http://uvdb3.hgc.jp/ALICE/ALICE index.html
</footnote>
<sectionHeader confidence="0.988168" genericHeader="conclusions">
6 Conclusions and Future Research
</sectionHeader>
<bodyText confidence="0.999995733333333">
We have presented the DPLVM and GI encod-
ing by which to incorporate non-local information
in abbreviating terms. They were competing and
generally the performance of the DPLVM was su-
perior. On the other hand, we showed that the two
approaches were complementary. By combining
these approaches, we were able to achieve state-
of-the-art performance in abbreviation generation
and recognition in the same model, across differ-
ent languages, and with a simple feature set. As
discussed earlier herein, the training data is rela-
tively small. Since there are numerous unlabeled
full forms on the web, it is possible to use a semi-
supervised approach in order to make use of such
raw data. This is an area for future research.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998445">
We thank Yoshimasa Tsuruoka for providing the
English abbreviation generation corpus. We also
thank the anonymous reviewers who gave help-
ful comments. This work was partially supported
by Grant-in-Aid for Specially Promoted Research
(MEXT, Japan).
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997935">
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527–
533.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576–586.
June A. Barrett and Mandalay Grems. 1960. Abbrevi-
ating words systematically. Communications of the
ACM, 3(5):323–324.
Charles P. Bourne and Donald F. Ford. 1961. A study
of methods for systematically abbreviating english
words and names. Journal of the ACM, 8(4):538–
552.
Jeffrey T. Chang and Hinrich Sch¨utze. 2006. Abbre-
viations in biomedical text. In Sophia Ananiadou
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99–119. Artech House,
Inc.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, CMU.
Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.
2008. Combined one sense disambiguation of ab-
breviations. In Proceedings of ACL’08: HLT, Short
Papers, pages 61–64, June.
</reference>
<page confidence="0.974686">
912
</page>
<reference confidence="0.99980195">
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings
of CVPR’07, pages 1–8.
David Nadeau and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Arti�cial Intelli-
gence (AI’2005) (LNAI 3501), page 10 pages.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal optimization. Springer.
Naoaki Okazaki, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2008. A discriminative alignment model for
abbreviation recognition. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING’08), pages 657–664, Manch-
ester, UK.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
ACL’02, pages 160–167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP’01, pages 126–133.
Leonid Peshkin and Avi Pfeffer. 2003. Bayesian in-
formation extraction network. In Proceedings of IJ-
CAI’03, pages 421–426.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings
of NIPS’08.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Paci�c Symposium on
Biocomputing (PSB’03), pages 451–462.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
HLT/NAACL’03.
Xu Sun, Houfeng Wang, and Bo Wang. 2008. Pre-
dicting chinese abbreviations from definitions: An
empirical learning approach using support vector re-
gression. Journal of Computer Science and Tech-
nology, 23(4):602–611.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191–198.
Yoshimasa Tsuruoka, Sophia Ananiadou, and Jun’ichi
Tsujii. 2005. A machine learning approach to
acronym generation. In Proceedings of the ACL-
ISMB Workshop, pages 25–31.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426–434.
Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based ap-
proach for automatically disambiguating biomedical
abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380–404.
</reference>
<page confidence="0.999083">
913
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.197632">
<title confidence="0.9997525">Robust Approach to Abbreviating Terms: A Discriminative Latent Variable Model with Global Information</title>
<author confidence="0.999622">Naoaki Jun’ichi</author>
<affiliation confidence="0.999854">of Computer Science, University of Tokyo,</affiliation>
<address confidence="0.7470675">Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan of Computer Science, University of Manchester, UK</address>
<affiliation confidence="0.916391">Centre for Text Mining, UK</affiliation>
<address confidence="0.459577">okazaki,</address>
<abstract confidence="0.9993335">The present paper describes a robust approach for abbreviating terms. First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model, or alternatively, the label encoding approach with global information. Although the two approaches compete with one another, we demonstrate that these approaches are also complementary. By combining these two approaches, experiments revealed that the proposed abbreviation generator achieved the best results for both the Chinese and English languages. Moreover, we directly apply our generator to perform a very different task from tradition, the abbreviation recognition. Experiments revealed that the proposed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
</authors>
<title>SaRAD: A simple and robust abbreviation dictionary.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>533</pages>
<contexts>
<context position="2999" citStr="Adar, 2004" startWordPosition="436" endWordPosition="437">pted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted rules from one language to another. For instance, in English,</context>
<context position="28930" citStr="Adar, 2004" startWordPosition="4783" endWordPosition="4784">ll NULL labelings by using P = P − 0.7. 911 Model P R F CRF+GIB 94.0 98.9 96.4 DPLVM+GIB 94.5 99.1 96.7 Table 7: English abbreviation recognition with back-off. 2PR/(P + R), where k represents #instances in which the system extracts correct full forms, m represents #instances in which the system extracts the full forms regardless of correctness, and n represents #instances that have annotated full forms. Following Okazaki et al. (2008), we perform 10- fold cross validation. We prepared six state-of-the-art abbreviation recognizers as baselines: Schwartz and Hearst’s method (SH) (2003), SaRAD (Adar, 2004), ALICE (Ao and Takagi, 2005), Chang and Sch¨utze’s method (CS) (Chang and Sch¨utze, 2006), Nadeau and Turney’s method (NT) (Nadeau and Turney, 2005), and Okazaki et al.’s method (OZ) (Okazaki et al., 2008). Some methods use implementations on the web, including SH8, CS9, and ALICE10. The results of other methods, such as SaRAD, NT, and OZ, are reproduced for this corpus based on their papers (Okazaki et al., 2008). As can be seen in Table 6, using the latent variables significantly improved the performance (see DPLVM vs. CRF), and using the GI encoding improved the performance of both the DPL</context>
</contexts>
<marker>Adar, 2004</marker>
<rawString>Eytan Adar. 2004. SaRAD: A simple and robust abbreviation dictionary. Bioinformatics, 20(4):527– 533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Ao</author>
<author>Toshihisa Takagi</author>
</authors>
<title>ALICE: An algorithm to extract abbreviations from MEDLINE.</title>
<date>2005</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>12</volume>
<issue>5</issue>
<contexts>
<context position="3021" citStr="Ao and Takagi, 2005" startWordPosition="438" endWordPosition="441">l the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted rules from one language to another. For instance, in English, abbreviation characte</context>
<context position="28959" citStr="Ao and Takagi, 2005" startWordPosition="4787" endWordPosition="4790"> using P = P − 0.7. 911 Model P R F CRF+GIB 94.0 98.9 96.4 DPLVM+GIB 94.5 99.1 96.7 Table 7: English abbreviation recognition with back-off. 2PR/(P + R), where k represents #instances in which the system extracts correct full forms, m represents #instances in which the system extracts the full forms regardless of correctness, and n represents #instances that have annotated full forms. Following Okazaki et al. (2008), we perform 10- fold cross validation. We prepared six state-of-the-art abbreviation recognizers as baselines: Schwartz and Hearst’s method (SH) (2003), SaRAD (Adar, 2004), ALICE (Ao and Takagi, 2005), Chang and Sch¨utze’s method (CS) (Chang and Sch¨utze, 2006), Nadeau and Turney’s method (NT) (Nadeau and Turney, 2005), and Okazaki et al.’s method (OZ) (Okazaki et al., 2008). Some methods use implementations on the web, including SH8, CS9, and ALICE10. The results of other methods, such as SaRAD, NT, and OZ, are reproduced for this corpus based on their papers (Okazaki et al., 2008). As can be seen in Table 6, using the latent variables significantly improved the performance (see DPLVM vs. CRF), and using the GI encoding improved the performance of both the DPLVM and the CRF. With the F-sc</context>
</contexts>
<marker>Ao, Takagi, 2005</marker>
<rawString>Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An algorithm to extract abbreviations from MEDLINE. Journal of the American Medical Informatics Association, 12(5):576–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>June A Barrett</author>
<author>Mandalay Grems</author>
</authors>
<title>Abbreviating words systematically.</title>
<date>1960</date>
<journal>Communications of the ACM,</journal>
<volume>3</volume>
<issue>5</issue>
<contexts>
<context position="2726" citStr="Barrett and Grems, 1960" startWordPosition="391" endWordPosition="394">of abbreviation definitions) that are updated with the neologisms. In addition, based on the one-sense-per-discourse assumption, the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for En</context>
</contexts>
<marker>Barrett, Grems, 1960</marker>
<rawString>June A. Barrett and Mandalay Grems. 1960. Abbreviating words systematically. Communications of the ACM, 3(5):323–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles P Bourne</author>
<author>Donald F Ford</author>
</authors>
<title>A study of methods for systematically abbreviating english words and names.</title>
<date>1961</date>
<journal>Journal of the ACM,</journal>
<volume>8</volume>
<issue>4</issue>
<pages>552</pages>
<contexts>
<context position="2750" citStr="Bourne and Ford, 1961" startWordPosition="395" endWordPosition="398">ns) that are updated with the neologisms. In addition, based on the one-sense-per-discourse assumption, the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. How</context>
</contexts>
<marker>Bourne, Ford, 1961</marker>
<rawString>Charles P. Bourne and Donald F. Ford. 1961. A study of methods for systematically abbreviating english words and names. Journal of the ACM, 8(4):538– 552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey T Chang</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Abbreviations in biomedical text.</title>
<date>2006</date>
<booktitle>In Sophia Ananiadou and John McNaught, editors, Text Mining for Biology and Biomedicine,</booktitle>
<pages>99--119</pages>
<publisher>Artech House, Inc.</publisher>
<marker>Chang, Sch¨utze, 2006</marker>
<rawString>Jeffrey T. Chang and Hinrich Sch¨utze. 2006. Abbreviations in biomedical text. In Sophia Ananiadou and John McNaught, editors, Text Mining for Biology and Biomedicine, pages 99–119. Artech House, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108, CMU.</tech>
<contexts>
<context position="15533" citStr="Chen and Rosenfeld, 1999" startWordPosition="2564" endWordPosition="2567">r Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approximation, L-BFGS can handle largescale problems efficiently. Since the objective function of the DPLVM model is non-convex, different parameter initializations normally bring different optimization results. Therefore, to approach closer to the global optimal point, it is recommended to perform multiple experiments on DPLVMs with random initialization and then select a good start point. To reduce overfitting, we employed a L2 Gaussian weight prior (Chen and Rosenfeld, 1999), with the objective function: L(o) = Eni�1 log P(yiIxi,o)−IJo112/Q2. During training and validation, we set Q = 1 for the DPLVM generators. We also set four latent variables for each label, in order to make a compromise between accuracy and efficiency. Note that, for the label encoding with global information, many label transitions (e.g., P2S3) are actually impossible: the label transitions are strictly constrained, i.e., yiyi+1 E {PjSj,PjPj+1, SjPj+1, SjSj1. These constraints on the model topology (forward-backward lattice) are enforced by giving appropriate features a weight of −oc, thereb</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov HaCohen-Kerner</author>
<author>Ariel Kass</author>
<author>Ariel Peretz</author>
</authors>
<title>Combined one sense disambiguation of abbreviations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL’08: HLT, Short Papers,</booktitle>
<pages>61--64</pages>
<contexts>
<context position="1800" citStr="HaCohen-Kerner et al., 2008" startWordPosition="252" endWordPosition="255">tperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation definitions in the actual text. Chang and Sch¨utze (2006) reported that 64,242 new abbreviations were introduced into the biomedical literatures in 2004. As such, it is important to maintain sense inventories (lists of abbreviation definitions) that are updated with the neologisms. In addition, based on the one-sense-per-discourse assumption, the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document. Therefore, a number of studies have attempted to mod</context>
</contexts>
<marker>HaCohen-Kerner, Kass, Peretz, 2008</marker>
<rawString>Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz. 2008. Combined one sense disambiguation of abbreviations. In Proceedings of ACL’08: HLT, Short Papers, pages 61–64, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis-Philippe Morency</author>
<author>Ariadna Quattoni</author>
<author>Trevor Darrell</author>
</authors>
<title>Latent-dynamic discriminative models for continuous gesture recognition.</title>
<date>2007</date>
<booktitle>Proceedings of CVPR’07,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7296" citStr="Morency et al., 2007" startWordPosition="1152" endWordPosition="1155">revious abbreviation generation studies. In addition, we apply the proposed models to the task of abbreviation recognition, in which a model extracts the abbreviation definitions in a given text. To the extent of our knowledge, this is the first model that can perform both abbreviation generation and recognition at the state-of-the-art level, across different languages and with a simple feature set. 2 Abbreviator with Non-local Information 2.1 A Latent Variable Abbreviator To implicitly incorporate non-local information, we propose discriminative probabilistic latent variable models (DPLVMs) (Morency et al., 2007; Petrov and Klein, 2008) for abbreviating terms. The DPLVM is a natural extension of the CRF model (see Figure 2), which is a special case of the DPLVM, with only one latent variable assigned for each label. The DPLVM uses latent variables to capture additional information that may not be expressed by the observable labels. For example, using the DPLVM, a possible feature could be “the current character xi = X, the label yi = P, and the latent variable hi = LV.” The non-local information can be effectively modeled in the DPLVM, and the additional information at the previous position or many o</context>
<context position="8752" citStr="Morency et al., 2007" startWordPosition="1423" endWordPosition="1426">m for a given sequence of characters x = x1, x2, ... , xm in an expanded form. Each label, yj, is a member of the possible labels Y . For each sequence, we also assume a sequence of latent variables h = h1, h2, ... , hm, which are unobservable in training examples. We model the conditional probability of the label sequence P(y|x) using the DPLVM, P (y|x, O) = E P(y|h, x, O)P(h|x, O). (1) h Here, O represents the parameters of the model. To ensure that the training and inference are efficient, the model is often restricted to have disjointed sets of latent variables associated with each label (Morency et al., 2007). Each hj is a member in a set Hyp of possible latent variables for the label yj. Here, H is defined as the set of all possible latent variables, i.e., H is the union of all Hyp sets. Since the sequences having hj V Hyp will, by definition, yield P(y|x, O) = 0, the model is rewritten as follows (Morency et al., 2007; Petrov and Klein, 2008): P(y|x, O) = E P(h|x, O). (2) hEHy1x...xHyM Here, P(h|x, O) is defined by the usual formulation of the conditional random field, = exp O · f (h, x) P(hlx, O ( I ) �yh exp O·f(h,x), (3) where f(h, x) represents a feature vector. Given a training set consisti</context>
</contexts>
<marker>Morency, Quattoni, Darrell, 2007</marker>
<rawString>Louis-Philippe Morency, Ariadna Quattoni, and Trevor Darrell. 2007. Latent-dynamic discriminative models for continuous gesture recognition. Proceedings of CVPR’07, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Peter D Turney</author>
</authors>
<title>A supervised learning approach to acronym identification.</title>
<date>2005</date>
<booktitle>In the 8th Canadian Conference on Arti�cial Intelligence (AI’2005) (LNAI 3501),</booktitle>
<pages>10</pages>
<contexts>
<context position="29079" citStr="Nadeau and Turney, 2005" startWordPosition="4805" endWordPosition="4808">cognition with back-off. 2PR/(P + R), where k represents #instances in which the system extracts correct full forms, m represents #instances in which the system extracts the full forms regardless of correctness, and n represents #instances that have annotated full forms. Following Okazaki et al. (2008), we perform 10- fold cross validation. We prepared six state-of-the-art abbreviation recognizers as baselines: Schwartz and Hearst’s method (SH) (2003), SaRAD (Adar, 2004), ALICE (Ao and Takagi, 2005), Chang and Sch¨utze’s method (CS) (Chang and Sch¨utze, 2006), Nadeau and Turney’s method (NT) (Nadeau and Turney, 2005), and Okazaki et al.’s method (OZ) (Okazaki et al., 2008). Some methods use implementations on the web, including SH8, CS9, and ALICE10. The results of other methods, such as SaRAD, NT, and OZ, are reproduced for this corpus based on their papers (Okazaki et al., 2008). As can be seen in Table 6, using the latent variables significantly improved the performance (see DPLVM vs. CRF), and using the GI encoding improved the performance of both the DPLVM and the CRF. With the F-score of 96.1%, the DPLVM+GI model outperformed five of six stateof-the-art abbreviation recognizers. Note that all of the</context>
</contexts>
<marker>Nadeau, Turney, 2005</marker>
<rawString>David Nadeau and Peter D. Turney. 2005. A supervised learning approach to acronym identification. In the 8th Canadian Conference on Arti�cial Intelligence (AI’2005) (LNAI 3501), page 10 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical optimization.</title>
<date>1999</date>
<publisher>Springer.</publisher>
<contexts>
<context position="14883" citStr="Nocedal and Wright, 1999" startWordPosition="2470" endWordPosition="2473">top-N accuracy represents the percentage of correct abbreviations that are covered, if we take the top N candidates from the ranked labelings of an abbreviation generator. We implemented the DPLVM in C++ and optimized the system to cope with large-scale problems. We employ the feature templates defined in Section 2.3, taking into account these 81,827 features for the Chinese abbreviation generation task, and the 50,149 features for the English abbreviation generation task. For numerical optimization, we performed a gradient descent with the Limited-Memory BFGS (L-BFGS) optimization technique (Nocedal and Wright, 1999). L-BFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approximation, L-BFGS can handle largescale problems efficiently. Since the objective function of the DPLVM model is non-convex, different parameter initializations normally bring different optimization results. Therefore, to approach closer to the global optimal point, it is recommended to perform multiple experiments on DPLVMs with random initialization and then select a good start point. To reduce overfitting, we employed a L</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 1999. Numerical optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative alignment model for abbreviation recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08),</booktitle>
<pages>657--664</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="27257" citStr="Okazaki et al. (2008)" startWordPosition="4514" endWordPosition="4517"> 97.1 CRF 89.8 94.8 92.1 CRF+GI 93.9 97.8 95.9 DPLVM 92.5 97.7 95.1 DPLVM+GI (*) 94.2 98.1 96.1 Table 6: Results of English abbreviation recognition. belings. Other labelings are impossible, because they will generate an abbreviation that is not AP. If the first or second labeling is generated, AP is selected as an abbreviation of arterial pressure. If the third or fourth labeling is generated, then AP is selected as an abbreviation of cannulate for arterial pressure. Finally, the fifth labeling (NULL) indicates that AP is not an abbreviation. To evaluate the recognizer, we use the corpus6 of Okazaki et al. (2008), which contains 864 abbreviation definitions collected from 1,000 MEDLINE scientific abstracts. In implementing the recognizer, we simply use the model from the abbreviation generator, with the same feature templates (31,868 features) and training method; the major difference is in the restriction (according to the PE) of the decoding stage and penalizing the probability values of the NULL labelings7. For the evaluation metrics, following Okazaki et al. (2008), we use precision (P = k/m), recall (R = k/n), and the F-score defined by 6The previous abbreviation generation corpus is improper for</context>
<context position="28758" citStr="Okazaki et al. (2008)" startWordPosition="4758" endWordPosition="4761">Due to the data imbalance of the training corpus, we found the probability values of the NULL labelings are abnormally high. To deal with this imbalance problem, we simply penalize all NULL labelings by using P = P − 0.7. 911 Model P R F CRF+GIB 94.0 98.9 96.4 DPLVM+GIB 94.5 99.1 96.7 Table 7: English abbreviation recognition with back-off. 2PR/(P + R), where k represents #instances in which the system extracts correct full forms, m represents #instances in which the system extracts the full forms regardless of correctness, and n represents #instances that have annotated full forms. Following Okazaki et al. (2008), we perform 10- fold cross validation. We prepared six state-of-the-art abbreviation recognizers as baselines: Schwartz and Hearst’s method (SH) (2003), SaRAD (Adar, 2004), ALICE (Ao and Takagi, 2005), Chang and Sch¨utze’s method (CS) (Chang and Sch¨utze, 2006), Nadeau and Turney’s method (NT) (Nadeau and Turney, 2005), and Okazaki et al.’s method (OZ) (Okazaki et al., 2008). Some methods use implementations on the web, including SH8, CS9, and ALICE10. The results of other methods, such as SaRAD, NT, and OZ, are reproduced for this corpus based on their papers (Okazaki et al., 2008). As can b</context>
</contexts>
<marker>Okazaki, Ananiadou, Tsujii, 2008</marker>
<rawString>Naoaki Okazaki, Sophia Ananiadou, and Jun’ichi Tsujii. 2008. A discriminative alignment model for abbreviation recognition. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08), pages 657–664, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serguei Pakhomov</author>
</authors>
<title>Semi-supervised maximum entropy based approach to acronym and abbreviation normalization in medical texts.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1753" citStr="Pakhomov, 2002" startWordPosition="246" endWordPosition="247">sed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation definitions in the actual text. Chang and Sch¨utze (2006) reported that 64,242 new abbreviations were introduced into the biomedical literatures in 2004. As such, it is important to maintain sense inventories (lists of abbreviation definitions) that are updated with the neologisms. In addition, based on the one-sense-per-discourse assumption, the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document. There</context>
</contexts>
<marker>Pakhomov, 2002</marker>
<rawString>Serguei Pakhomov. 2002. Semi-supervised maximum entropy based approach to acronym and abbreviation normalization in medical texts. In Proceedings of ACL’02, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngja Park</author>
<author>Roy J Byrd</author>
</authors>
<title>Hybrid text mining for finding abbreviations and their definitions.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP’01,</booktitle>
<pages>126--133</pages>
<contexts>
<context position="2937" citStr="Park and Byrd, 2001" startWordPosition="424" endWordPosition="427">ocally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted</context>
</contexts>
<marker>Park, Byrd, 2001</marker>
<rawString>Youngja Park and Roy J. Byrd. 2001. Hybrid text mining for finding abbreviations and their definitions. In Proceedings of EMNLP’01, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Peshkin</author>
<author>Avi Pfeffer</author>
</authors>
<title>Bayesian information extraction network.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI’03,</booktitle>
<pages>421--426</pages>
<contexts>
<context position="10755" citStr="Peshkin and Pfeffer (2003)" startWordPosition="1786" endWordPosition="1789">hod. In this approach, the label yi at position i attaches the information of the abbreviation length generated by its previous labels, y1, y2,.. . , yi_1. Figure 3 shows an example of a Chinese abbreviation. In this encoding, a label not only contains the produce or skip information, but also the abbreviation-length information, i.e., the label includes the number of all P labels preceding the current position. We refer to this method as label encoding with global information (hereinafter GI). The concept of using label encoding to incorporate non-local information was originally proposed by Peshkin and Pfeffer (2003). Note that the model-complexity is increased only by the increase in the number of labels. Since the length of the abbreviations is usually quite short (less than five for Chinese abbreviations and less than 10 for English abbreviations), the model is still tractable even when using the GI encoding. The implicit (DPLVM) and explicit (GI) solutions address the same issue concerning the incorporation of non-local information, and there are advantages to combining these two solutions. Therefore, we will combine the implicit and explicit solutions by employing the GI encoding in the DPLVM (DPLVM+</context>
</contexts>
<marker>Peshkin, Pfeffer, 2003</marker>
<rawString>Leonid Peshkin and Avi Pfeffer. 2003. Bayesian information extraction network. In Proceedings of IJCAI’03, pages 421–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2008</date>
<booktitle>Proceedings of NIPS’08.</booktitle>
<contexts>
<context position="7321" citStr="Petrov and Klein, 2008" startWordPosition="1156" endWordPosition="1159">eneration studies. In addition, we apply the proposed models to the task of abbreviation recognition, in which a model extracts the abbreviation definitions in a given text. To the extent of our knowledge, this is the first model that can perform both abbreviation generation and recognition at the state-of-the-art level, across different languages and with a simple feature set. 2 Abbreviator with Non-local Information 2.1 A Latent Variable Abbreviator To implicitly incorporate non-local information, we propose discriminative probabilistic latent variable models (DPLVMs) (Morency et al., 2007; Petrov and Klein, 2008) for abbreviating terms. The DPLVM is a natural extension of the CRF model (see Figure 2), which is a special case of the DPLVM, with only one latent variable assigned for each label. The DPLVM uses latent variables to capture additional information that may not be expressed by the observable labels. For example, using the DPLVM, a possible feature could be “the current character xi = X, the label yi = P, and the latent variable hi = LV.” The non-local information can be effectively modeled in the DPLVM, and the additional information at the previous position or many of the other positions in </context>
<context position="9094" citStr="Petrov and Klein, 2008" startWordPosition="1492" endWordPosition="1495">e DPLVM, P (y|x, O) = E P(y|h, x, O)P(h|x, O). (1) h Here, O represents the parameters of the model. To ensure that the training and inference are efficient, the model is often restricted to have disjointed sets of latent variables associated with each label (Morency et al., 2007). Each hj is a member in a set Hyp of possible latent variables for the label yj. Here, H is defined as the set of all possible latent variables, i.e., H is the union of all Hyp sets. Since the sequences having hj V Hyp will, by definition, yield P(y|x, O) = 0, the model is rewritten as follows (Morency et al., 2007; Petrov and Klein, 2008): P(y|x, O) = E P(h|x, O). (2) hEHy1x...xHyM Here, P(h|x, O) is defined by the usual formulation of the conditional random field, = exp O · f (h, x) P(hlx, O ( I ) �yh exp O·f(h,x), (3) where f(h, x) represents a feature vector. Given a training set consisting of n instances, (xi, yi) (for i = 1... n), we estimate the parameters O by maximizing the regularized loglikelihood, n L(O) = log P(yi|xi, O) − R(O). (4) i=1 The first term expresses the conditional loglikelihood of the training data, and the second term represents a regularizer that reduces the overfitting problem in parameter estimatio</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Discriminative loglinear grammars with latent variables. Proceedings of NIPS’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In the 8th Paci�c Symposium on Biocomputing (PSB’03),</booktitle>
<pages>451--462</pages>
<contexts>
<context position="2987" citStr="Schwartz and Hearst, 2003" startWordPosition="432" endWordPosition="435">umber of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted rules from one language to another. For instance,</context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In the 8th Paci�c Symposium on Biocomputing (PSB’03), pages 451–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>Proceedings of HLT/NAACL’03.</booktitle>
<contexts>
<context position="16215" citStr="Sha and Pereira (2003)" startWordPosition="2671" endWordPosition="2674">Jo112/Q2. During training and validation, we set Q = 1 for the DPLVM generators. We also set four latent variables for each label, in order to make a compromise between accuracy and efficiency. Note that, for the label encoding with global information, many label transitions (e.g., P2S3) are actually impossible: the label transitions are strictly constrained, i.e., yiyi+1 E {PjSj,PjPj+1, SjPj+1, SjSj1. These constraints on the model topology (forward-backward lattice) are enforced by giving appropriate features a weight of −oc, thereby forcing all forbidden labelings to have zero probability. Sha and Pereira (2003) originally proposed this concept of implementing transition restrictions. 4 Results and Discussion 4.1 Chinese Abbreviation Generation First, we present the results of the Chinese abbreviation generation task, as listed in Table 2. To evaluate the impact of using latent variables, we chose the baseline system as the DPLVM, in which each label has only one latent variable. Since this 908 Model T1A T2A T3A Time Heu (S08) 41.6 N/A N/A N/A HMM (S08) 46.1 N/A N/A N/A SVM (S08) 62.7 80.4 87.7 1.3 h CRF 64.5 81.1 88.7 0.2 h CRF+GI 66.8 82.5 90.0 0.5 h DPLVM 67.6 83.8 91.3 0.4 h DPLVM+GI (*) 72.3 87.</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. Proceedings of HLT/NAACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Houfeng Wang</author>
<author>Bo Wang</author>
</authors>
<title>Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression.</title>
<date>2008</date>
<journal>Journal of Computer Science and Technology,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="13546" citStr="Sun et al. (2008)" startWordPosition="2269" endWordPosition="2272">10 and #11 represent a contextual n-gram with a large window size. Since the number of letters in Chinese (more than 10K characters) is much larger than the number of letters in English (26 letters), in order to avoid a possible overfitting problem, we did not apply these feature templates to Chinese abbreviations. Feature templates are instantiated with values that occur in positive training examples. We used all of the instantiated features because we found that the low-frequency features also improved the performance. 3 Experiments For Chinese abbreviation generation, we used the corpus of Sun et al. (2008), which contains 2,914 abbreviation definitions for training, and 729 pairs for testing. This corpus consists primarily of noun phrases (38%), organization names (32%), and verb phrases (21%). For English abbreviation generation, we evaluated the corpus of Tsuruoka et al. (2005). This corpus contains 1,200 aligned pairs extracted from MEDLINE biomedical abstracts (published in 2001). For both tasks, we converted the aligned pairs of the corpora into labeled full forms and used the labeled full forms as the training/evaluation data. The evaluation metrics used in the abbreviation generation are</context>
<context position="17323" citStr="Sun et al. (2008)" startWordPosition="2868" endWordPosition="2871">7 1.3 h CRF 64.5 81.1 88.7 0.2 h CRF+GI 66.8 82.5 90.0 0.5 h DPLVM 67.6 83.8 91.3 0.4 h DPLVM+GI (*) 72.3 87.6 94.9 1.1 h Table 2: Results of Chinese abbreviation generation. T1A, T2A, and T3A represent top-1, top2, and top-3 accuracy, respectively. The system marked with the * symbol is the recommended system. special case of the DPLVM is exactly the CRF (see Section 2.1), this case is hereinafter denoted as the CRF. We compared the performance of the DPLVM with the CRFs and other baseline systems, including the heuristic system (Heu), the HMM model, and the SVM model described in S08, i.e., Sun et al. (2008). The heuristic method is a simple rule that produces the initial character of each word to generate the corresponding abbreviation. The SVM method described by Sun et al. (2008) is formalized as a regression problem, in which the abbreviation candidates are scored and ranked. The results revealed that the latent variable model significantly improved the performance over the CRF model. All of its top-1, top-2, and top-3 accuracies were consistently better than those of the CRF model. Therefore, this demonstrated the effectiveness of using the latent variables in Chinese abbreviation generation</context>
<context position="28090" citStr="Sun et al., 2008" startWordPosition="4645" endWordPosition="4648">s (31,868 features) and training method; the major difference is in the restriction (according to the PE) of the decoding stage and penalizing the probability values of the NULL labelings7. For the evaluation metrics, following Okazaki et al. (2008), we use precision (P = k/m), recall (R = k/n), and the F-score defined by 6The previous abbreviation generation corpus is improper for evaluating recognizers, and there is no related research on this corpus. In addition, there has been no report of Chinese abbreviation recognition because there is no data available. The previous generation corpus (Sun et al., 2008) is improper because it lacks local contexts. 7Due to the data imbalance of the training corpus, we found the probability values of the NULL labelings are abnormally high. To deal with this imbalance problem, we simply penalize all NULL labelings by using P = P − 0.7. 911 Model P R F CRF+GIB 94.0 98.9 96.4 DPLVM+GIB 94.5 99.1 96.7 Table 7: English abbreviation recognition with back-off. 2PR/(P + R), where k represents #instances in which the system extracts correct full forms, m represents #instances in which the system extracts the full forms regardless of correctness, and n represents #insta</context>
</contexts>
<marker>Sun, Wang, Wang, 2008</marker>
<rawString>Xu Sun, Houfeng Wang, and Bo Wang. 2008. Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression. Journal of Computer Science and Technology, 23(4):602–611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazem Taghva</author>
<author>Jeff Gilbreth</author>
</authors>
<title>Recognizing acronyms and their definitions.</title>
<date>1999</date>
<journal>International Journal on Document Analysis and Recognition (IJDAR),</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="2916" citStr="Taghva and Gilbreth, 1999" startWordPosition="420" endWordPosition="423">of abbreviations that are locally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledg</context>
</contexts>
<marker>Taghva, Gilbreth, 1999</marker>
<rawString>Kazem Taghva and Jeff Gilbreth. 1999. Recognizing acronyms and their definitions. International Journal on Document Analysis and Recognition (IJDAR), 1(4):191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A machine learning approach to acronym generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACLISMB Workshop,</booktitle>
<pages>25--31</pages>
<contexts>
<context position="4585" citStr="Tsuruoka et al. (2005)" startWordPosition="714" endWordPosition="717">iation generation as a sequential labeling problem. CRF DPLVM Figure 2: CRF vs. DPLVM. Variables x, y, and h represent observation, label, and latent variables, respectively. Institute of History and Philology at Academia Sinica 历 史 语 言 研 究 所 S S S P [史语所] S P P x1 x2 xm y1 y2 ym y1 h1 h2 hm x1 x2 xm y2 ym p o l y g l y c o l i c a c i d P S S S P S S S S S S S S P S S S [PGA] (a): English Abbreviation Generation other languages, including Chinese and Japanese, do not have word boundaries or case sensitivity. A number of recent studies have investigated the use of machine learning techniques. Tsuruoka et al. (2005) formalized the processes of abbreviation generation as a sequence labeling problem. In the present study, each character in the expanded form is tagged with a label, y E {P, S11, where the label P produces the current character and the label S skips the current character. In Figure 1 (a), the abbreviation PGA is generated from the full form polyglycolic acid because the underlined characters are tagged with P labels. In Figure 1 (b), the abbreviation is generated using the 2nd and 3rd characters, skipping the subsequent three characters, and then using the 7th character. In order to formalize</context>
<context position="6143" citStr="Tsuruoka et al. (2005)" startWordPosition="978" endWordPosition="981">ate a suitable number of letters for the abbreviation. In addition, the model would be able to recognize the abbreviating process in Figure 1 (a) more reasonably if it were able to segment the word polyglycolic into smaller regions, e.g., poly-glycolic. Even though humans may use global or non-local information to abbreviate words, previous studies have not incorporated this information into a sequential labeling model. In the present paper, we propose implicit and explicit solutions for incorporating non-local information. The implicit solution is based on the 1Although the original paper of Tsuruoka et al. (2005) attached case sensitivity information to the P label, for simplicity, we herein omit this information. discriminative probabilistic latent variable model (DPLVM) in which non-local information is modeled by latent variables. We manually encode nonlocal information into the labels in order to provide an explicit solution. We evaluate the models on the task of abbreviation generation, in which a model produces an abbreviation for a given full form. Experimental results indicate that the proposed models significantly outperform previous abbreviation generation studies. In addition, we apply the </context>
<context position="13825" citStr="Tsuruoka et al. (2005)" startWordPosition="2310" endWordPosition="2313">eature templates to Chinese abbreviations. Feature templates are instantiated with values that occur in positive training examples. We used all of the instantiated features because we found that the low-frequency features also improved the performance. 3 Experiments For Chinese abbreviation generation, we used the corpus of Sun et al. (2008), which contains 2,914 abbreviation definitions for training, and 729 pairs for testing. This corpus consists primarily of noun phrases (38%), organization names (32%), and verb phrases (21%). For English abbreviation generation, we evaluated the corpus of Tsuruoka et al. (2005). This corpus contains 1,200 aligned pairs extracted from MEDLINE biomedical abstracts (published in 2001). For both tasks, we converted the aligned pairs of the corpora into labeled full forms and used the labeled full forms as the training/evaluation data. The evaluation metrics used in the abbreviation generation are exact-match accuracy (hereinafter accuracy), including top-1 accuracy, top-2 accuracy, and top-3 accuracy. The top-N accuracy represents the percentage of correct abbreviations that are covered, if we take the top N candidates from the ranked labelings of an abbreviation genera</context>
<context position="25229" citStr="Tsuruoka et al. (2005)" startWordPosition="4164" endWordPosition="4167">parameters trained without the GI encoding (i.e., the DPLVM). The results in Table 3 demonstrate that the DPVLM+GIB model significantly outperformed the other models because the DPLVM+GI model improved the performance in some ‘difficult’ instances. The DPVLM+GIB model was robust even when the data sparseness problem was severe. By re-evaluating the DPLVM+GIB model for the previous Chinese abbreviation generation task, we demonstrate that the back-off method also improved the performance of the Chinese abbreviation generators (+0.2% from DPLVM+GI; see Table 4). Furthermore, for interests, like Tsuruoka et al. (2005), we performed a five-fold cross-validation on the corpus. Concerning the training time in the cross validation, we simply chose the DPLVM for comparison. Table 5 shows the results of the DPLVM, the heuristic system (Heu), and the maximum entropy Markov model (MEMM) described by Tsuruoka et al. (2005). 5 Recognition as a Generation Task We directly migrate this model to the abbreviation recognition task. We simplify the abbreviation recognition to a restricted generation problem (see Figure 9). When a context expression (CE) with a parenthetical expression (PE) is met, the recognizer generates</context>
</contexts>
<marker>Tsuruoka, Ananiadou, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka, Sophia Ananiadou, and Jun’ichi Tsujii. 2005. A machine learning approach to acronym generation. In Proceedings of the ACLISMB Workshop, pages 25–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan D Wren</author>
<author>Harold R Garner</author>
</authors>
<title>Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries.</title>
<date>2002</date>
<journal>Methods of Information in Medicine,</journal>
<volume>41</volume>
<issue>5</issue>
<contexts>
<context position="2960" citStr="Wren and Garner, 2002" startWordPosition="428" endWordPosition="431">ocument. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted rules from one languag</context>
</contexts>
<marker>Wren, Garner, 2002</marker>
<rawString>Jonathan D. Wren and Harold R. Garner. 2002. Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries. Methods of Information in Medicine, 41(5):426–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Won Kim</author>
<author>Vasileios Hatzivassiloglou</author>
<author>John Wilbur</author>
</authors>
<title>A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations.</title>
<date>2006</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="1770" citStr="Yu et al., 2006" startWordPosition="248" endWordPosition="251"> robustly, and outperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation definitions in the actual text. Chang and Sch¨utze (2006) reported that 64,242 new abbreviations were introduced into the biomedical literatures in 2004. As such, it is important to maintain sense inventories (lists of abbreviation definitions) that are updated with the neologisms. In addition, based on the one-sense-per-discourse assumption, the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document. Therefore, a number of</context>
</contexts>
<marker>Yu, Kim, Hatzivassiloglou, Wilbur, 2006</marker>
<rawString>Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and John Wilbur. 2006. A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations. ACM Transactions on Information Systems (TOIS), 24(3):380–404.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>