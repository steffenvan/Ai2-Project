<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.083466">
<title confidence="0.983437">
A platform for collaborative semantic annotation
</title>
<author confidence="0.624523">
Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen
</author>
<email confidence="0.66594">
{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl
</email>
<affiliation confidence="0.918974">
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands
</affiliation>
<sectionHeader confidence="0.97747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994760523809524">
Data-driven approaches in computational
semantics are not common because there
are only few semantically annotated re-
sources available. We are building a
large corpus of public-domain English texts
and annotate them semi-automatically with
syntactic structures (derivations in Com-
binatory Categorial Grammar) and seman-
tic representations (Discourse Representa-
tion Structures), including events, thematic
roles, named entities, anaphora, scope, and
rhetorical structure. We have created a
wiki-like Web-based platform on which a
crowd of expert annotators (i.e. linguists)
can log in and adjust linguistic analyses in
real time, at various levels of analysis, such
as boundaries (tokens, sentences) and tags
(part of speech, lexical categories). The
demo will illustrate the different features of
the platform, including navigation, visual-
ization and editing.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999599">
Data-driven approaches in computational seman-
tics are still rare because there are not many
large annotated resources that provide empiri-
cal information about anaphora, presupposition,
scope, events, tense, thematic roles, named en-
tities, word senses, ellipsis, discourse segmenta-
tion and rhetorical relations in a single formal-
ism. This is not surprising, as it is challenging and
time-consuming to create such a resource from
scratch.
Nevertheless, our objective is to develop a
large annotated corpus of Discourse Representa-
tion Structures (Kamp and Reyle, 1993), com-
prising most of the aforementioned phenomena:
the Groningen Meaning Bank (GMB). We aim to
reach this goal by:
</bodyText>
<listItem confidence="0.9672602">
1. Providing a wiki-like platform supporting
collaborative annotation efforts;
2. Employing state-of-the-art NLP software for
bootstrapping semantic analysis;
3. Giving real-time feedback of annotation ad-
justments in their resulting syntactic and se-
mantic analysis;
4. Ensuring kerfuflie-free dissemination of
our semantic resource by considering only
public-domain texts for annotation.
</listItem>
<bodyText confidence="0.999974375">
We have developed the wiki-like platform from
scratch simply because existing annotation sys-
tems, such as GATE (Dowman et al., 2005), NITE
(Carletta et al., 2003), or UIMA (Hahn et al.,
2007), do not offer the functionality required for
deep semantic annotation combined with crowd-
sourcing.
In this description of our platform, we motivate
our choice of data and explain how we manage it
(Section 2), we describe the complete toolchain
of NLP components employed in the annotation-
feedback process (Section 3), and the Web-based
interface itself is introduced, describing how lin-
guists can adjust boundaries of tokens and sen-
tences, and revise tags of named entities, parts of
speech and lexical categories (Section 4).
</bodyText>
<sectionHeader confidence="0.991943" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999351285714286">
The goal of the Groningen Meaning Bank is to
provide a widely available corpus of texts, with
deep semantic annotations. The GMB only com-
prises texts from the public domain, whose dis-
tribution isn’t subject to copyright restrictions.
Moreover, we include texts from various genres
and sources, resulting in a rich, comprehensive
</bodyText>
<page confidence="0.951132">
92
</page>
<bodyText confidence="0.979822421052632">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 92–96,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
corpus appropriate for use in various disciplines
within NLP.
The documents in the current version of the
GMB are all in English and originate from four
main sources: (i) Voice ofAmerica (VOA), an on-
line newspaper published by the US Federal Gov-
ernment; (ii) the Manually Annotated Sub-Corpus
(MASC) from the Open American National Cor-
pus (Ide et al., 2010); (iii) country descriptions
from the CIA World Factbook (CIA) (Central In-
telligence Agency, 2006), in particular the Back-
ground and Economy sections, and (iv) a col-
lection of Aesop’s fables (AF). All these docu-
ments are in the public domain and are thus redis-
tributable, unlike for example the WSJ data used
in the Penn Treebank (Miltsakaki et al., 2004).
Each document is stored with a separate file
containing metadata. This may include the lan-
guage the text is written in, the genre, date of
publication, source, title, and terms of use of the
document. This metadata is stored as a simple
feature-value list.
The documents in the GMB are categorized
with different statuses. Initially, newly added doc-
uments are labeled as uncategorized. As we man-
ually review them, they are relabeled as either
accepted (document will be part of the next sta-
ble version, which will be released in regular in-
tervals), postponed (there is some difficulty with
the document that can possibly be solved in the
future) or rejected (something is wrong with the
document form, i.e., character encoding, or with
the content, e.g., it contains offensive material).
Currently, the GMB comprises 70K English
text documents (Table 1), corresponding to 1,3
million sentences and 31,5 million tokens.
</bodyText>
<tableCaption confidence="0.999616">
Table 1: Documents in the GMB, as of March 5, 2012
</tableCaption>
<table confidence="0.998867">
Documents VOA MASC CIA AF All
Accepted 4,651 34 515 0 5,200
Uncategorized 61,090 0 0 834 61,924
Postponed 2,397 339 3 1 2,740
Rejected 184 27 4 0 215
Total 68,322 400 522 835 70,079
</table>
<sectionHeader confidence="0.994363" genericHeader="method">
3 The NLP Toolchain
</sectionHeader>
<bodyText confidence="0.999939">
The process of building the Groningen Meaning
Bank takes place in a bootstrapping fashion. A
chain of software is run, taking the raw text docu-
ments as input. The output of this automatic pro-
cess is in the form of several layers of stand-off
annotations, i.e., files with links to the original,
raw documents.
We employ a chain of NLP components that
carry out, respectively, tokenization and sentence
boundary detection, POS tagging, lemmatization,
named entity recognition, supertagging, parsing
using the formalism of Combinatory Categorial
Grammar (Steedman, 2001), and semantic and
discourse analysis using the framework of Dis-
course Representation Theory (DRT) (Kamp and
Reyle, 1993) with rhetorical relations (Asher,
1993).
The lemmatizer used is morpha (Minnen et al.,
2001), the other steps are carried out by the C&amp;C
tools (Curran et al., 2007) and Boxer (Bos, 2008).
</bodyText>
<subsectionHeader confidence="0.99968">
3.1 Bits of Wisdom
</subsectionHeader>
<bodyText confidence="0.999962739130435">
After each step in the toolchain, the intermediate
result may be automatically adjusted by auxiliary
components that apply annotations provided by
expert users or other sources. These annotations
are represented as “Bits of Wisdom” (BOWs): tu-
ples of information regarding, for example, token
and sentence boundaries, tags, word senses or dis-
course relations. They are stored in a MySQL
database and can originate from three different
sources: (i) explicit annotation changes made by
experts using the Explorer Web interface (see Sec-
tion 4); (ii) an annotation game played by non-
experts, similar to ‘games with a purpose’ like
Phrase Detectives (Chamberlain et al., 2008) and
Jeux de Mots (Artignan et al., 2009); and (iii) ex-
ternal NLP tools (e.g. for word sense disambigua-
tion or co-reference resolution).
Since BOWs come from various sources, they
may contradict each other. In such cases, a judge
component resolves the conflict, currently by pre-
ferring the most recent expert BOW. Future work
will involve the application of different judging
techniques.
</bodyText>
<subsectionHeader confidence="0.999766">
3.2 Processing Cycle
</subsectionHeader>
<bodyText confidence="0.999889222222222">
The widely known open-source tool GNU make
is used to orchestrate the toolchain while avoid-
ing unnecessary reprocessing. The need to rerun
the toolchain for a document arises in three sit-
uations: a new BOW for that document is avail-
able; a new, improved version of one of the com-
ponents is available; or reprocessing is forced by
a user via the “reprocess” button in the Web inter-
face. A continually running program, the ‘updat-
</bodyText>
<page confidence="0.996798">
93
</page>
<figureCaption confidence="0.999877">
Figure 1: A screenshot of the web interface, displaying a tokenised document.
</figureCaption>
<bodyText confidence="0.9998685">
ing daemon’, is responsible for calling make for
the right document at the right time. It checks the
database for new BOWs or manual reprocessing
requests in very short intervals to ensure immedi-
ate response to changes experts make via the Web
interface. It also updates and rebuilds the compo-
nents in longer intervals and continuously loops
through all documents, remaking them with the
newest versions of the components. The number
of make processes that can run in parallel is con-
figurable; standard techniques of concurrent pro-
gramming are used to prevent more than one make
process from working simultaneously on the same
document.
</bodyText>
<sectionHeader confidence="0.992344" genericHeader="method">
4 The Expert Interface
</sectionHeader>
<bodyText confidence="0.999640857142857">
We developed a wiki-like Web interface, called
the GMB Explorer, that provides users access to
the Groningen Meaning Bank. It fulfills three
main functions: navigation and search through the
documents, visualization of the different levels of
annotation, and manual correction of the annota-
tions. We will discuss these functions below.
</bodyText>
<subsectionHeader confidence="0.997153">
4.1 Navigation and Search
</subsectionHeader>
<bodyText confidence="0.999975235294118">
The GMB Explorer allows navigation through the
documents of the GMB with their stand-off an-
notations (Figure 1). The default order of docu-
ments is based on their size in terms of number
of tokens. It is possible to apply filters to restrict
the set of documents to be shown: showing only
documents from a specific subcorpus, or specifi-
cally showing documents with/without warnings
generated by the NLP toolchain.
The Explorer interface comes with a built-in
search engine. It allows users to pose single- or
multi-word queries. The search results can then
be restricted further by looking for a specific lex-
ical category or part of speech. A more advanced
search system that is based on a semantic lexicon
with lexical information about all levels of anno-
tation is currently under development.
</bodyText>
<subsectionHeader confidence="0.946323">
4.2 Visualization
</subsectionHeader>
<bodyText confidence="0.999860714285714">
The different visualization options for a document
are placed in tabs: each tab corresponds to a spe-
cific layer of annotation or additional informa-
tion. Besides the raw document text, users can
view its tokenized version, an interactive deriva-
tion tree per sentence, and the semantic represen-
tation of the entire discourse in graphical DRS
format. There are three further tabs in the Ex-
plorer: a tab containing the warnings produced by
the NLP pipeline (if any), one containing the Bits
of Wisdom that have been collected for the docu-
ment, and a tab with the document metadata.
The sentences view allows the user to show or
hide sub-trees per sentence and additional infor-
mation such as POS-tags, word senses, supertags
and partial, unresolved semantics. The deriva-
tions are shown using the CCG notation, gener-
ated by XSLT stylesheets applied to Boxer’s XML
output. An example is shown in Figure 2.
The discourse view shows a fully resolved
semantic representation in the form of a DRS with
</bodyText>
<figureCaption confidence="0.881231">
Figure 2: An example of a CCG derivation as shown
in GMB Explorer.
</figureCaption>
<page confidence="0.995326">
94
</page>
<figureCaption confidence="0.9641905">
Figure 3: An example of the semantic representations
in the GMB, with DRSs representing discourse units.
</figureCaption>
<bodyText confidence="0.98615425">
rhetorical relations. Clicking on discourse units
switches the visualization between text and se-
mantic representation. Figure 3 shows how DRSs
are visualized in the Web interface.
</bodyText>
<subsectionHeader confidence="0.999035">
4.3 Editing
</subsectionHeader>
<bodyText confidence="0.999966785714286">
Some of the tabs in the Explorer interface have an
“edit” button. This allows registered users to man-
ually correct certain types of annotations. Cur-
rently, the user can edit the tokenization view and
on the derivation view. Clicking “edit” in the to-
kenization view gives an annotator the possibility
to add and remove token and sentence boundaries
in a simple and intuitive way, as Figure 4 illus-
trates. This editing is done in real-time, following
the WYSIWYG strategy, with tokens separated
by spaces and sentences separated by new lines.
In the derivation view, the annotator can change
part-of-speech tags and named entity tags by se-
lecting a tag from a drop-down list (Figure 5).
</bodyText>
<figureCaption confidence="0.997912">
Figure 4: Tokenization edit mode. Clicking on the
red ‘x’ removes a sentence boundary after the token;
clicking on the green ‘+’ adds a sentence boundary.
Figure 5: Tag edit mode, showing derivation with par-
tial DRSs and illustrating how to adjust a POS tag.
</figureCaption>
<bodyText confidence="0.999954529411765">
As the updating daemon is running continu-
ally, the document is immediately reprocessed af-
ter editing so that the user can directly view the
new annotation with his BOW taken into account.
Re-analyzing a document typically takes a few
seconds, although for very large documents it can
take longer. It is also possible to directly rerun
the NLP toolchain on a specific document via the
“reprocess” button, in order to apply the most re-
cent version of the software components involved.
The GMB Explorer shows a timestamp of the last
processing for each document.
We are currently working on developing new
editing options, which allow users to change dif-
ferent aspects of the semantic representation, such
as word senses, thematic roles, co-reference and
scope.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="conclusions">
5 Demo
</sectionHeader>
<bodyText confidence="0.999788416666667">
In the demo session we show the functionality of
the various features in the Web-based user inter-
face of the GMB Explorer, which is available on-
line via: http://gmb.let.rug.nl.
We show (i) how to navigate and search
through all the documents, including the refine-
ment of search on the basis of the lexical cate-
gory or part of speech, (ii) the operation of the dif-
ferent view options, including the raw, tokenized,
derivation and semantics view of each document,
and (iii) how adjustments to annotations can be re-
alised in the Web interface. More concretely, we
demonstrate how boundaries of tokens and sen-
tences can be adapted, and how different types of
tags can be changed (and how that affects the syn-
tactic, semantic and discourse analysis).
In sum, the demo illustrates innovation in the
way changes are made and how they improve the
linguistic analysis in real-time. Because it is a
web-based platform, it paves the way for a collab-
orative annotation effort. Currently it is actively
in use as a tool to create a large semantically an-
notated corpus for English texts: the Groningen
Meaning Bank.
</bodyText>
<page confidence="0.998173">
95
</page>
<sectionHeader confidence="0.989508" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776029850746">
Guillaume Artignan, Mountaz Hasco¨et, and Mathieu
Lafourcade. 2009. Multiscale visual analysis of
lexical networks. In 13th International Confer-
ence on Information Visualisation, pages 685–690,
Barcelona, Spain.
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robert-
son, and H. Voormann. 2003. The NITE XML
toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instru-
ments, and Computers, 35(3):353–363.
Central Intelligence Agency. 2006. The CIA World
Factbook. Potomac Books.
John Chamberlain, Massimo Poesio, and Udo Kr-
uschwitz. 2008. Addressing the Resource Bottle-
neck to Create Large-Scale Annotated Texts. In
Johan Bos and Rodolfo Delmonte, editors, Seman-
tics in Text Processing. STEP 2008 Conference Pro-
ceedings, volume 1 of Research in Computational
Semantics, pages 375–380. College Publications.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&amp;C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33–36, Prague,
Czech Republic.
Mike Dowman, Valentin Tablan, Hamish Cunning-
ham, and Borislav Popov. 2005. Web-assisted an-
notation, semantic indexing and search of television
and radio news. In Proceedings of the 14th Interna-
tional World Wide Web Conference, pages 225–234,
Chiba, Japan.
U. Hahn, E. Buyko, K. Tomanek, S. Piao, J. Mc-
Naught, Y. Tsuruoka, and S. Ananiadou. 2007.
An annotation type system for a data-driven NLP
pipeline. In Proceedings of the Linguistic Annota-
tion Workshop, pages 33–40, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: a community resource for and by the
people. In Proceedings of the ACL 2010 Confer-
ence Short Papers, pages 68–73, Stroudsburg, PA,
USA.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. In In Proceedings of LREC 2004, pages
2237–2240.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of en-
glish. Journal of Natural Language Engineering,
7(3):207–223.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
</reference>
<page confidence="0.998441">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563217">
<title confidence="0.999707">A platform for collaborative semantic annotation</title>
<author confidence="0.998012">Basile Bos Evang</author>
<affiliation confidence="0.807605">Center for Language and Cognition Groningen University of Groningen, The Netherlands</affiliation>
<abstract confidence="0.996064090909091">Data-driven approaches in computational semantics are not common because there only few semantically annotated resources available. We are building large corpus of public-domain English texts and annotate them semi-automatically with syntactic structures (derivations in Combinatory Categorial Grammar) and semantic representations (Discourse Representation Structures), including events, thematic roles, named entities, anaphora, scope, and rhetorical structure. We have created a wiki-like Web-based platform on which a crowd of expert annotators (i.e. linguists) can log in and adjust linguistic analyses in real time, at various levels of analysis, such as boundaries (tokens, sentences) and tags (part of speech, lexical categories). The demo will illustrate the different features of the platform, including navigation, visualization and editing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Guillaume Artignan</author>
<author>Mountaz Hasco¨et</author>
<author>Mathieu Lafourcade</author>
</authors>
<title>Multiscale visual analysis of lexical networks.</title>
<date>2009</date>
<booktitle>In 13th International Conference on Information Visualisation,</booktitle>
<pages>685--690</pages>
<location>Barcelona,</location>
<marker>Artignan, Hasco¨et, Lafourcade, 2009</marker>
<rawString>Guillaume Artignan, Mountaz Hasco¨et, and Mathieu Lafourcade. 2009. Multiscale visual analysis of lexical networks. In 13th International Conference on Information Visualisation, pages 685–690, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Reference to Abstract Objects in Discourse.</title>
<date>1993</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="6091" citStr="Asher, 1993" startWordPosition="930" endWordPosition="931">, taking the raw text documents as input. The output of this automatic process is in the form of several layers of stand-off annotations, i.e., files with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&amp;C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three differen</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Nicholas Asher. 1993. Reference to Abstract Objects in Discourse. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-Coverage Semantic Analysis with Boxer. In</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>277--286</pages>
<editor>J. Bos and R. Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="6238" citStr="Bos, 2008" startWordPosition="957" endWordPosition="958"> with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&amp;C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by experts using the Explorer Web interface (see Section 4); (ii) an annotation game played by none</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-Coverage Semantic Analysis with Boxer. In J. Bos and R. Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Evert</author>
<author>U Heid</author>
<author>J Kilgour</author>
<author>J Robertson</author>
<author>H Voormann</author>
</authors>
<title>The NITE XML toolkit: flexible annotation for multi-modal language data.</title>
<date>2003</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="2372" citStr="Carletta et al., 2003" startWordPosition="325" endWordPosition="328">nomena: the Groningen Meaning Bank (GMB). We aim to reach this goal by: 1. Providing a wiki-like platform supporting collaborative annotation efforts; 2. Employing state-of-the-art NLP software for bootstrapping semantic analysis; 3. Giving real-time feedback of annotation adjustments in their resulting syntactic and semantic analysis; 4. Ensuring kerfuflie-free dissemination of our semantic resource by considering only public-domain texts for annotation. We have developed the wiki-like platform from scratch simply because existing annotation systems, such as GATE (Dowman et al., 2005), NITE (Carletta et al., 2003), or UIMA (Hahn et al., 2007), do not offer the functionality required for deep semantic annotation combined with crowdsourcing. In this description of our platform, we motivate our choice of data and explain how we manage it (Section 2), we describe the complete toolchain of NLP components employed in the annotationfeedback process (Section 3), and the Web-based interface itself is introduced, describing how linguists can adjust boundaries of tokens and sentences, and revise tags of named entities, parts of speech and lexical categories (Section 4). 2 Data The goal of the Groningen Meaning Ba</context>
</contexts>
<marker>Carletta, Evert, Heid, Kilgour, Robertson, Voormann, 2003</marker>
<rawString>J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robertson, and H. Voormann. 2003. The NITE XML toolkit: flexible annotation for multi-modal language data. Behavior Research Methods, Instruments, and Computers, 35(3):353–363.</rawString>
</citation>
<citation valid="true">
<title>Central Intelligence Agency.</title>
<date>2006</date>
<publisher>Potomac Books.</publisher>
<marker>2006</marker>
<rawString>Central Intelligence Agency. 2006. The CIA World Factbook. Potomac Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chamberlain</author>
<author>Massimo Poesio</author>
<author>Udo Kruschwitz</author>
</authors>
<title>Addressing the Resource Bottleneck to Create Large-Scale Annotated Texts.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>375--380</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="6929" citStr="Chamberlain et al., 2008" startWordPosition="1063" endWordPosition="1066">diate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by experts using the Explorer Web interface (see Section 4); (ii) an annotation game played by nonexperts, similar to ‘games with a purpose’ like Phrase Detectives (Chamberlain et al., 2008) and Jeux de Mots (Artignan et al., 2009); and (iii) external NLP tools (e.g. for word sense disambiguation or co-reference resolution). Since BOWs come from various sources, they may contradict each other. In such cases, a judge component resolves the conflict, currently by preferring the most recent expert BOW. Future work will involve the application of different judging techniques. 3.2 Processing Cycle The widely known open-source tool GNU make is used to orchestrate the toolchain while avoiding unnecessary reprocessing. The need to rerun the toolchain for a document arises in three situat</context>
</contexts>
<marker>Chamberlain, Poesio, Kruschwitz, 2008</marker>
<rawString>John Chamberlain, Massimo Poesio, and Udo Kruschwitz. 2008. Addressing the Resource Bottleneck to Create Large-Scale Annotated Texts. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 375–380. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6216" citStr="Curran et al., 2007" startWordPosition="951" endWordPosition="954">and-off annotations, i.e., files with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&amp;C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by experts using the Explorer Web interface (see Section 4); (ii) an annotati</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33–36, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Dowman</author>
<author>Valentin Tablan</author>
<author>Hamish Cunningham</author>
<author>Borislav Popov</author>
</authors>
<title>Web-assisted annotation, semantic indexing and search of television and radio news.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th International World Wide Web Conference,</booktitle>
<pages>225--234</pages>
<location>Chiba, Japan.</location>
<contexts>
<context position="2342" citStr="Dowman et al., 2005" startWordPosition="320" endWordPosition="323">st of the aforementioned phenomena: the Groningen Meaning Bank (GMB). We aim to reach this goal by: 1. Providing a wiki-like platform supporting collaborative annotation efforts; 2. Employing state-of-the-art NLP software for bootstrapping semantic analysis; 3. Giving real-time feedback of annotation adjustments in their resulting syntactic and semantic analysis; 4. Ensuring kerfuflie-free dissemination of our semantic resource by considering only public-domain texts for annotation. We have developed the wiki-like platform from scratch simply because existing annotation systems, such as GATE (Dowman et al., 2005), NITE (Carletta et al., 2003), or UIMA (Hahn et al., 2007), do not offer the functionality required for deep semantic annotation combined with crowdsourcing. In this description of our platform, we motivate our choice of data and explain how we manage it (Section 2), we describe the complete toolchain of NLP components employed in the annotationfeedback process (Section 3), and the Web-based interface itself is introduced, describing how linguists can adjust boundaries of tokens and sentences, and revise tags of named entities, parts of speech and lexical categories (Section 4). 2 Data The go</context>
</contexts>
<marker>Dowman, Tablan, Cunningham, Popov, 2005</marker>
<rawString>Mike Dowman, Valentin Tablan, Hamish Cunningham, and Borislav Popov. 2005. Web-assisted annotation, semantic indexing and search of television and radio news. In Proceedings of the 14th International World Wide Web Conference, pages 225–234, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>E Buyko</author>
<author>K Tomanek</author>
<author>S Piao</author>
<author>J McNaught</author>
<author>Y Tsuruoka</author>
<author>S Ananiadou</author>
</authors>
<title>An annotation type system for a data-driven NLP pipeline.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2401" citStr="Hahn et al., 2007" startWordPosition="331" endWordPosition="334">k (GMB). We aim to reach this goal by: 1. Providing a wiki-like platform supporting collaborative annotation efforts; 2. Employing state-of-the-art NLP software for bootstrapping semantic analysis; 3. Giving real-time feedback of annotation adjustments in their resulting syntactic and semantic analysis; 4. Ensuring kerfuflie-free dissemination of our semantic resource by considering only public-domain texts for annotation. We have developed the wiki-like platform from scratch simply because existing annotation systems, such as GATE (Dowman et al., 2005), NITE (Carletta et al., 2003), or UIMA (Hahn et al., 2007), do not offer the functionality required for deep semantic annotation combined with crowdsourcing. In this description of our platform, we motivate our choice of data and explain how we manage it (Section 2), we describe the complete toolchain of NLP components employed in the annotationfeedback process (Section 3), and the Web-based interface itself is introduced, describing how linguists can adjust boundaries of tokens and sentences, and revise tags of named entities, parts of speech and lexical categories (Section 4). 2 Data The goal of the Groningen Meaning Bank is to provide a widely ava</context>
</contexts>
<marker>Hahn, Buyko, Tomanek, Piao, McNaught, Tsuruoka, Ananiadou, 2007</marker>
<rawString>U. Hahn, E. Buyko, K. Tomanek, S. Piao, J. McNaught, Y. Tsuruoka, and S. Ananiadou. 2007. An annotation type system for a data-driven NLP pipeline. In Proceedings of the Linguistic Annotation Workshop, pages 33–40, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Christiane Fellbaum</author>
<author>Collin Baker</author>
<author>Rebecca Passonneau</author>
</authors>
<title>The manually annotated sub-corpus: a community resource for and by the people.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>68--73</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3833" citStr="Ide et al., 2010" startWordPosition="558" endWordPosition="561"> and sources, resulting in a rich, comprehensive 92 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 92–96, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics corpus appropriate for use in various disciplines within NLP. The documents in the current version of the GMB are all in English and originate from four main sources: (i) Voice ofAmerica (VOA), an online newspaper published by the US Federal Government; (ii) the Manually Annotated Sub-Corpus (MASC) from the Open American National Corpus (Ide et al., 2010); (iii) country descriptions from the CIA World Factbook (CIA) (Central Intelligence Agency, 2006), in particular the Background and Economy sections, and (iv) a collection of Aesop’s fables (AF). All these documents are in the public domain and are thus redistributable, unlike for example the WSJ data used in the Penn Treebank (Miltsakaki et al., 2004). Each document is stored with a separate file containing metadata. This may include the language the text is written in, the genre, date of publication, source, title, and terms of use of the document. This metadata is stored as a simple featur</context>
</contexts>
<marker>Ide, Fellbaum, Baker, Passonneau, 2010</marker>
<rawString>Nancy Ide, Christiane Fellbaum, Collin Baker, and Rebecca Passonneau. 2010. The manually annotated sub-corpus: a community resource for and by the people. In Proceedings of the ACL 2010 Conference Short Papers, pages 68–73, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1707" citStr="Kamp and Reyle, 1993" startWordPosition="231" endWordPosition="234">g navigation, visualization and editing. 1 Introduction Data-driven approaches in computational semantics are still rare because there are not many large annotated resources that provide empirical information about anaphora, presupposition, scope, events, tense, thematic roles, named entities, word senses, ellipsis, discourse segmentation and rhetorical relations in a single formalism. This is not surprising, as it is challenging and time-consuming to create such a resource from scratch. Nevertheless, our objective is to develop a large annotated corpus of Discourse Representation Structures (Kamp and Reyle, 1993), comprising most of the aforementioned phenomena: the Groningen Meaning Bank (GMB). We aim to reach this goal by: 1. Providing a wiki-like platform supporting collaborative annotation efforts; 2. Employing state-of-the-art NLP software for bootstrapping semantic analysis; 3. Giving real-time feedback of annotation adjustments in their resulting syntactic and semantic analysis; 4. Ensuring kerfuflie-free dissemination of our semantic resource by considering only public-domain texts for annotation. We have developed the wiki-like platform from scratch simply because existing annotation systems,</context>
<context position="6051" citStr="Kamp and Reyle, 1993" startWordPosition="923" endWordPosition="926">bootstrapping fashion. A chain of software is run, taking the raw text documents as input. The output of this automatic process is in the form of several layers of stand-off annotations, i.e., files with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&amp;C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL databa</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank. In</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>2237--2240</pages>
<contexts>
<context position="4188" citStr="Miltsakaki et al., 2004" startWordPosition="618" endWordPosition="621">nt version of the GMB are all in English and originate from four main sources: (i) Voice ofAmerica (VOA), an online newspaper published by the US Federal Government; (ii) the Manually Annotated Sub-Corpus (MASC) from the Open American National Corpus (Ide et al., 2010); (iii) country descriptions from the CIA World Factbook (CIA) (Central Intelligence Agency, 2006), in particular the Background and Economy sections, and (iv) a collection of Aesop’s fables (AF). All these documents are in the public domain and are thus redistributable, unlike for example the WSJ data used in the Penn Treebank (Miltsakaki et al., 2004). Each document is stored with a separate file containing metadata. This may include the language the text is written in, the genre, date of publication, source, title, and terms of use of the document. This metadata is stored as a simple feature-value list. The documents in the GMB are categorized with different statuses. Initially, newly added documents are labeled as uncategorized. As we manually review them, they are relabeled as either accepted (document will be part of the next stable version, which will be released in regular intervals), postponed (there is some difficulty with the docu</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2004. The Penn Discourse Treebank. In In Proceedings of LREC 2004, pages 2237–2240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of english.</title>
<date>2001</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="6144" citStr="Minnen et al., 2001" startWordPosition="937" endWordPosition="940"> output of this automatic process is in the form of several layers of stand-off annotations, i.e., files with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&amp;C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by ex</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of english. Journal of Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5930" citStr="Steedman, 2001" startWordPosition="907" endWordPosition="908"> 68,322 400 522 835 70,079 3 The NLP Toolchain The process of building the Groningen Meaning Bank takes place in a bootstrapping fashion. A chain of software is run, taking the raw text documents as input. The output of this automatic process is in the form of several layers of stand-off annotations, i.e., files with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&amp;C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process. The MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>