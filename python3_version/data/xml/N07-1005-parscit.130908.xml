<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000128">
<title confidence="0.9989635">
Automatic Evaluation of Machine Translation Based on Rate of
Accomplishment of Sub-goals
</title>
<author confidence="0.985692">
Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara
</author>
<affiliation confidence="0.992412">
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.782778">
3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
</address>
<email confidence="0.996123">
{uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp
</email>
<sectionHeader confidence="0.982965" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986259259259">
The quality of a sentence translated by a
machine translation (MT) system is dif-
ficult to evaluate. We propose a method
for automatically evaluating the quality
of each translation. In general, when
translating a given sentence, one or more
conditions should be satisfied to maintain
a high translation quality. In English-
Japanese translation, for example, prepo-
sitions and infinitives must be appropri-
ately translated. We show several proce-
dures that enable evaluating the quality of
a translated sentence more appropriately
than using conventional methods. The
first procedure is constructing a test set
where the conditions are assigned to each
test-set sentence in the form of yes/no
questions. The second procedure is devel-
oping a system that determines an answer
to each question. The third procedure is
combining a measure based on the ques-
tions and conventional measures. We also
present a method for automatically gener-
ating sub-goals in the form of yes/no ques-
tions and estimating the rate of accom-
plishment of the sub-goals. Promising re-
sults are shown.
</bodyText>
<sectionHeader confidence="0.995162" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997560277777778">
In machine translation (MT) research, appropriately
evaluating the quality of MT results is an important
issue. In recent years, many researchers have tried
to automatically evaluate the quality of MT and im-
prove the performance of automatic MT evaluations
(Niessen et al., 2000; Akiba et al., 2001; Papineni et
al., 2002; NIST, 2002; Leusch et al., 2003; Turian et
al., 2003; Babych and Hartley, 2004; Lin and Och,
2004; Banerjee and Lavie, 2005; Gime´nez et al.,
2005) because improving the performance of auto-
matic MT evaluation is expected to enable us to use
and improve MT systems efficiently. For example,
Och reported that the quality of MT results was im-
proved by using automatic MT evaluation measures
for the parameter tuning of an MT system (Och,
2003). This report shows that the quality of MT re-
sults improves as the performance of automatic MT
evaluation improves.
MT systems can be ranked if a set of MT re-
sults for each system and their reference translations
are given. Usually, about 300 or more sentences
are used to automatically rank MT systems (Koehn,
2004). However, the quality of a sentence translated
by an MT system is difficult to evaluate. For exam-
ple, the results of five MTs into Japanese of the sen-
tence “The percentage of stomach cancer among the
workers appears to be the highest for any asbestos
workers.” are shown in Table 1. A conventional au-
tomatic evaluation method ranks the fifth MT result
first although its human subjective evaluation is the
lowest. This is because conventional methods are
based on the similarity between a translated sentence
and its reference translation, and they give the trans-
lated sentence a high score when the two sentences
are globally similar to each other in terms of lexical
overlap. However, in the case of the above example,
</bodyText>
<page confidence="0.544511">
33
</page>
<note confidence="0.950397">
Proceedings of NAACL HLT 2007, pages 33–40,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<tableCaption confidence="0.997721">
Table 1: Examples of conventional automatic evaluations.
</tableCaption>
<note confidence="0.971726111111111">
Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos work-
Reference translation ers.
(in Japanese) roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda .
System MT results BLEU NIST Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata 0.2111 2.1328 2 3
roudousha no tame ni demo mottomo ookii youdearu .
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto 0.2572 2.1234 2 3
roudousha no tame ni mottomo takai youni omowa re masu .
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame 0 1.8094 1 2
</note>
<bodyText confidence="0.960975076923077">
ni mo mottomo takai youni mie masu
4 roudousha no aida no igan no paasenteeji wa nin’ino ishiwata ni wa 0 1.5902 1 2
mottomo takaku mie masu .
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mot- 0.2692 2.2640 1 2
tomo takai youni mieru .
the most important thing to maintain a high trans-
lation quality is to correctly translate “for” into the
target language, and it would be difficult to detect
the importance just by comparing an MT result and
its reference translations even if the number of ref-
erence translations is increased.
In general, when translating a given sentence, one
or more conditions should be satisfied to maintain a
high translation quality. In this paper, we show that
constructing a test set where the conditions that are
mainly established from a linguistic point of view
are assigned to each test-set sentence in the form
of yes/no questions, developing a system that de-
termines an answer to each question, and combin-
ing a measure based on the questions and conven-
tional measures enable the evaluation of the quality
of a translated sentence more appropriately than us-
ing conventional methods. We also present a method
for automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals.
</bodyText>
<sectionHeader confidence="0.54026" genericHeader="introduction">
2 Test Set for Evaluating Machine
Translation Quality
</sectionHeader>
<subsectionHeader confidence="0.999903">
2.1 Test Set
</subsectionHeader>
<bodyText confidence="0.999933257142857">
Two main types of data are used for evaluating MT
quality. One type of data is constructed by arbi-
trarily collecting sentence pairs in the source- and
target-languages, and the other is constructed by in-
tensively collecting sentence pairs that include lin-
guistic phenomena that are difficult to automatically
translate. Recently, MT evaluation campaigns such
as the International Workshop on Spoken Language
Translation 1, NIST Machine Translation Evaluation
2, and HTRDP Evaluation 3 were organized to sup-
port the improvement of MT techniques. The data
used in the evaluation campaigns were arbitrarily
collected from newspaper articles or travel conver-
sation data for fair evaluation. They are classified
as the former type of data mentioned above. On the
other hand, the data provided by NTT (Ikehara et al.,
1994) and that constructed by JEIDA (Isahara, 1995)
are classified as the latter type. Almost all the data
mentioned above consist of only parallel translations
in two languages. Data with information for evaluat-
ing MT results, such as JEIDA’s are rarely found. In
this paper, we call data that consist of parallel trans-
lations collected for MT evaluation and that the in-
formation for MT evaluation is assigned to, a test
set.
The most characteristic information assigned to
the JEIDA test set is the yes/no question for assess-
ing the translation results. For example, a yes/no
question such as “Is ‘for’ translated into an expres-
sion representing a cause/reason such as ‘de’?” (in
Japanese) is assigned to a test-set sentence. We can
evaluate MT results objectively by answering the
question. An example of a test-set sample consist-
ing of an ID, a source-language sample sentence, its
reference translation, and a question is as follows.
</bodyText>
<footnote confidence="0.996557666666667">
1http://www.slt.atr.jp/IWSLT2006/
2http://www.nist.gov/speech/tests/mt/index.htm
3http://www.863data.org.cn/
</footnote>
<page confidence="0.839351">
34
</page>
<bodyText confidence="0.986653">
ID 1.1.7.1.3-1
Sample sen- The percentage of stomach can-
tence cer among the workers appears
to be the highest for any asbestos
workers.
Reference roudousha no igan no wariai wa
translation , asubesuto roudousha no tame
(in Japanese) ni saikou to naru youda .
Question Is “appear to” translated into an
auxiliary verb such as “youda”?
The questions are classified mainly in terms of
grammar, and the numbers to the left of the hyphen-
ation of each ID such as 1.1.7.1.3 represent the cat-
egories of the questions. For example, the above
question is related to catenative verbs.
The JEIDA test set consists of two parts, one for
the evaluation of English-Japanese MT and the other
for that of Japanese-English MT. We focused on the
part for English-Japanese MT. This part consists of
769 sample sentences, each of which has a yes/no
question.
The 769 sentences were translated by using five
commercial MT systems to investigate the relation-
ship between subjective evaluation based on yes/no
questions and conventional subjective evaluation
based on fluency and adequacy. The instruction for
the subjective evaluation based on fluency and ad-
equacy followed that given in the TIDES specifi-
cation (TIDES, 2002). The subjective evaluation
based on yes/no questions was done by manually
answering each question for each translation. The
subjective evaluation based on the yes/no questions
was stable; namely, it was almost independent of
the human subjects in our preliminary investigation.
There were only two questions for which the an-
swers generated inconsistency in the subjective eval-
uation when 1,500 question-answer pairs were ran-
domly sampled and evaluated by two human sub-
jects.
Then, we investigated the correlation between the
two types of subjective evaluation. The correlation
coefficients mentioned in this paper are statistically
significant at the 1% or less significance level. The
Spearman rank-order correlation coefficient is used
in this paper. In the subjective evaluation based on
yes/no questions, yes and no were numerically trans-
formed into 1 and −1. For 3,845 translations ob-
tained by using five MT systems, the correlation co-
efficients between the subjective evaluations based
on yes/no questions and based on fluency and ade-
quacy were 0.48 for fluency and 0.63 for adequacy.
These results indicate that the two subjective evalu-
ations have relatively strong correlations. The cor-
relation is especially strong between the subjective
evaluation based on yes/no questions and adequacy.
</bodyText>
<subsectionHeader confidence="0.998415">
2.2 Expansion of JEIDA Test Set
</subsectionHeader>
<bodyText confidence="0.999968285714286">
Each sample sentence in the JEIDA test set has only
one question. Therefore, in the subjective evalua-
tion using the JEIDA test set, translation errors that
do not involve the pre-assigned question are ignored
even if they are serious. Therefore, translations that
have serious errors that are not related to the ques-
tion tend to be evaluated as being of high quality.
To solve this problem, we expanded the test set by
adding new questions about translations with the se-
rious errors.
Sentences whose average grades were three or
less for fluency and adequacy for the translation re-
sults of the five MT systems were selected for the
expansion. Besides them, sentences whose average
grades were more than three for fluency and ade-
quacy for the translation results of the five MT sys-
tems were selected when a majority of evaluation
results based on yes/no questions about the transla-
tions of the five MT systems were no. The number
of selected sentences was 150. The expansion was
manually performed using the following steps.
</bodyText>
<listItem confidence="0.566979307692308">
1. Serious translation errors are extracted from the
MT results.
2. For each extracted error, questions strongly re-
lated to the error are searched for in the test set.
If related questions are found, the same types
of questions are generated for the selected sen-
tence, and the same ID as that of the related
question is assigned to each generated question.
Otherwise, questions are newly generated, and
a new ID is assigned to each generated ques-
tion.
3. Each MT result is evaluated according to each
added question.
</listItem>
<bodyText confidence="0.999021">
Eventually, one or more questions were assigned to
each selected sentence in the test set. Among the 150
</bodyText>
<page confidence="0.947593">
35
</page>
<tableCaption confidence="0.998773">
Table 2: Expanded test-set samples.
</tableCaption>
<table confidence="0.999661916666667">
ID 1.1.7.1.3-1
Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any
asbestos workers.
Reference translation roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda
(in Japanese) .
Question (Q-0) Is “appear to” translated into an auxiliary verb such as “youda”?
ID 1.1.6.1.3-5
Expanded Translation error “For” is not translated appropriately.
Question-1 (Q-1) Is “for” translated into an expression representing a cause/reason such as “...de”?
ID Additional-1
Expanded Translation error Some expressions are not translated.
Question-2 (Q-2) Are all English words translated into Japanese?
</table>
<tableCaption confidence="0.986614">
Table 3: Examples of subjective evaluations based on yes/no questions.
</tableCaption>
<table confidence="0.98821825">
System MT results Answer Fluency Adequacy
Q-0 Q-1 Q-2
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata Yes No Yes 2 3
roudousha no tame ni demo mottomo ookii youdearu .
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto Yes Yes Yes 2 3
roudousha no tame ni mottomo takai youni omowa re masu .
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no Yes No No 1 2
tame ni mo mottomo takai youni mie masu
4 roudousha no aida no igan no paasenteeji wa nin’ino ishiwata ni Yes No No 1 2
wa mottomo takaku mie masu .
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo Yes No No 1 2
mottomo takai youni mieru .
</table>
<bodyText confidence="0.999466695652174">
selected sentences, questions were newly assigned
to 103 sentences. The number of added questions
was 148. The maximum number of questions added
to a sentence was five. After expanding the test set,
the correlation coefficients between the subjective
evaluations based on yes/no questions and based on
fluency and adequacy increased from 0.48 to 0.51
for fluency and from 0.63 to 0.66 for adequacy. The
differences between the correlation coefficients ob-
tained before and after the expansion are statistically
significant at the 5% or less significance level for
adequacy. These results indicate that the expansion
of the test set significantly improves the correlation
between the subjective evaluations based on yes/no
questions and based on adequacy. When two or
more questions were assigned to a test-set sentence,
the subjective evaluation based on the questions was
decided by the majority answer. The majority an-
swers, yes and no, were numerically transformed
into 1 and −1. Ties between yes and no were trans-
formed into 0. Examples of added questions and
the subjective evaluations based on the questions are
shown in Tables 2 and 3.
</bodyText>
<sectionHeader confidence="0.714213666666667" genericHeader="method">
3 Automatic Evaluation of Machine
Translation Based on Rate of
Accomplishment of Sub-goals
</sectionHeader>
<subsectionHeader confidence="0.9967495">
3.1 A New Measure for Evaluating Machine
Translation Quality
</subsectionHeader>
<bodyText confidence="0.999931631578947">
The JEIDA test set was not designed for auto-
matic evaluation but for human subjective evalua-
tion. However, a measure for automatic MT evalu-
ation that strongly correlates fluency and adequacy
is likely to be established because the subjective
evaluation based on yes/no questions has a rela-
tively strong correlation with the subjective evalua-
tion based on fluency and adequacy, as mentioned in
Section 2. In this section, we describe a method for
automatically evaluating MT quality by predicting
an answer to each yes/no question and using those
answers.
Hereafter, we assume that each yes/no question is
defined as a sub-goal that a given translation should
satisfy and that the sub-goal is accomplished if the
answer to the corresponding yes/no question to the
sub-goal is yes. We also assume that the sub-goal
is unaccomplished if the answer is no. A new eval-
uation score, A, is defined based on a multiple lin-
</bodyText>
<page confidence="0.857277">
36
</page>
<tableCaption confidence="0.99667">
Table 4: Examples of Patterns.
</tableCaption>
<table confidence="0.8633235">
Sample sentence She lived there by herself.
Question Is “by herself” translated as “hitori de”?
Pattern The answer is yes if the pattern [hitori dake delhitori kiri de Itandoku deltanshin de] is included in a
translation. Otherwise, the answer is no.
Sample sentence They speak English in New Zealand.
Question The personal pronoun “they” is omitted in a translation like “nyuujiilando de wa eigo wo hanasu”?
Pattern The answer is yes if the pattern [karera walsore ra wa] is not included in a translation. Otherwise, the
answer is no.
</table>
<bodyText confidence="0.999579833333333">
ear regression model as follows using the rate of ac-
complishment of the sub-goals and the similarities
between a given translation and its reference trans-
lation. The best-fitted line for the observed data is
calculated by the method of least-squares (Draper
and Smith, 1981).
</bodyText>
<equation confidence="0.979592888888889">
m
A = λSi X Si (1)
i=1
(λQj X Qj + λQ,j X Qj) + λE
�
= 1 : if subgoal is accomplished 2
( )
Qj 0 : otherwise
1 : if subgoal is unaccomplished 3
</equation>
<bodyText confidence="0.996768878787879">
Q,j 0 : otherwise ( )
Here, the term Qj corresponds to the rate of accom-
plishment of the sub-goal having the i-th ID, and
λQj is a weight for the rate of accomplishment. The
term Q,j corresponds to the rate of unaccomplish-
ment of the sub-goal having the i-th ID, and λQ, j is a
weight for the rate of unaccomplishment. The value
n indicates the number of types of sub-goals. The
term λE is constant.
The term Si indicates a similarity between a trans-
lated sentence and its reference translation, and λSi
is a weight for the similarity. Many methods for cal-
culating the similarity have been proposed (Niessen
et al., 2000; Akiba et al., 2001; Papineni et al., 2002;
NIST, 2002; Leusch et al., 2003; Turian et al., 2003;
Babych and Hartley, 2004; Lin and Och, 2004;
Banerjee and Lavie, 2005; Gime´nez et al., 2005).
In our research, 23 scores, namely BLEU (Papineni
et al., 2002) with maximum n-gram lengths of 1, 2,
3, and 4, NIST (NIST, 2002) with maximum n-gram
lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003)
with exponents of 1.0, 2.0, and 3.0, METEOR (ex-
act) (Banerjee and Lavie, 2005), WER (Niessen et
al., 2000), PER (Leusch et al., 2003), and ROUGE
(Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and
4 variants (LCS, S*, SU*, W-1.2), were used to cal-
culate each similarity Si. Therefore, the value of m
in Eq. (1) was 23. Japanese word segmentation was
performed by using JUMAN 4 in our experiments.
As you can see, the definition of our new measure
is based on a combination of an evaluation measure
focusing on local information and that focusing on
global information.
</bodyText>
<subsectionHeader confidence="0.9993685">
3.2 Automatic Estimation of Rate of
Accomplishment of Sub-goals
</subsectionHeader>
<bodyText confidence="0.999985055555556">
The rate of accomplishment of sub-goals is esti-
mated by determining the answer to each question
as yes or no. This section describes a method based
on simple patterns for determining the answers.
An answer to each question is automatically de-
termined by checking whether patterns are included
in a translation or not. The patterns are constructed
for each question. All of the patterns are expressed
in hiragana characters. Before applying the pat-
terns to a given translation, the translation is trans-
formed into hiragana characters, and all punctuation
is eliminated. The transformation to hiragana char-
acters was performed by using JUMAN in our ex-
periments.
Test-set sentences, the questions assigned to
them, and the patterns constructed for the questions
are shown in Table 4. In the patterns, the symbol “�”
represents “OR”.
</bodyText>
<subsectionHeader confidence="0.998864666666667">
3.3 Automatic Sub-goal Generation and
Automatic Estimation of Rate of
Accomplishment of Sub-goals
</subsectionHeader>
<bodyText confidence="0.9998385">
We found that expressions important for maintain-
ing a high translation quality were often commonly
</bodyText>
<equation confidence="0.869627">
4http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html
n
+ E
j=1
37
</equation>
<bodyText confidence="0.996554">
included in the reference translations for each test-
set sentence. We also found that the expression was
also related to the yes/no question assigned to the
test-set sentence. Therefore, we automatically gen-
erate yes/no questions in the following steps.
</bodyText>
<listItem confidence="0.88860775">
1. For each test-set sentence, a set of words com-
monly appearing in the reference translations
are extracted.
2. For each combination of n words in the set
</listItem>
<bodyText confidence="0.951737961538462">
of words extracted in the first step, skip word
n-grams commonly appearing in the reference
translations in the same word order are selected
as a set of common skip word n-grams.
3. For each test-set sentence, the sub-goal is de-
fined as the yes/no question “Are all of the com-
mon skip word n-grams included in the transla-
tion?”
If no common skip word n-grams are found, the
yes/no question is not generated. The answer to the
yes/no question is determined to be yes if all of the
common skip word n-grams are included in a trans-
lation. Otherwise, the answer is determined to be
no.
This scheme assigns greater weight to important
phrases that should be included in the translation to
maintain a high translation quality. Our observation
is that those important phrases are often common
between human translations. A similar scheme was
proposed by Babych and Hartley (Babych and Hart-
ley, 2004) for BLEU. In their scheme, greater weight
is assigned to components that are salient through-
out the document. Therefore, their scheme focuses
on global context while our scheme focuses on local
context. We believe that the two schemes are com-
plementary to each other.
</bodyText>
<sectionHeader confidence="0.986014" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999856291666667">
In our experiments, the translation results of three
MT systems and their subjective evaluation results
were used as a development set for constructing the
patterns described in Section 3.2 and for tuning the
parameters λS,, λQP, λQ�P, and λE in Eq. (1). The
translations and evaluation results of the remaining
two MT systems were used as an evaluation set for
testing.
In the development set, each test-set sentence has
at least one question, at least one reference transla-
tion, three MT results, and subjective evaluation re-
sults of the three MT results. The patterns for deter-
mining yes/no answers were manually constructed
for the questions assigned to the 769 test-set sen-
tences. There were 917 questions assigned to them.
Among them, the patterns could be constructed for
898 questions assigned to 767 test-set sentences.
The remaining 19 questions were skipped because
making simple patterns as described in Section 3.2
was difficult; for example, one of the questions
was “Is the whole sentence translated into one sen-
tence?”. The yes/no answer determination accura-
cies obtained by using the patterns are shown in Ta-
ble 5.
</bodyText>
<tableCaption confidence="0.998089">
Table 5: Results of yes/no answer determination.
</tableCaption>
<table confidence="0.991607333333333">
Test set Accuracy
Development 97.6% (2,629/2,694)
Evaluation 82.8% (1,487/1,796)
</table>
<bodyText confidence="0.999970730769231">
We investigated the correlation between the eval-
uation score, A in Eq. (1) and the subjective eval-
uations, fluency and adequacy, for the 769 test-set
sentences. First, to maximize the correlation coeffi-
cients between the evaluation score, A, and the hu-
man subjective evaluations, fluency and adequacy,
the optimal values of λS,, λQP, λQ�P, and λE in
Eq. (1) were investigated using the development
set within a framework of multiple linear regression
modeling (Draper and Smith, 1981). Then, the cor-
relation coefficients were investigated by using the
optimal value set. The results are shown in Table 6,
7, and 8. In these tables, “Conventional method” in-
dicates the correlation coefficients obtained when A
was calculated by using only similarities Si. “Con-
ventional method (combination)” is a combination
of existing automatic evaluation methods from the
literature. “Our method (automatic)” indicates the
correlation coefficients obtained when the results of
the automatic determination of yes/no answers were
used to calculate Qj and Q�j in Eq. (1). For the 19
questions for which the patterns could not be con-
structed, Qj was set at 0. “Our method (full au-
tomatic)” indicates the correlation coefficients ob-
tained when the results of the automatic sub-goal
generation and determination of rate of accomplish-
</bodyText>
<page confidence="0.925548">
38
</page>
<tableCaption confidence="0.970344">
Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy. (A reference transla-
tion is used to calculate Si.)
</tableCaption>
<table confidence="0.995718333333333">
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.43 0.48 0.42 0.48
Conventional method (combination) 0.52 0.51 0.49 0.47
Our method (automatic) 0.90* 0.59* 0.89* 0.62*
Our method (upper bound) 0.90* 0.62* 0.90* 0.68*
</table>
<tableCaption confidence="0.983499">
Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy. (Three reference
translations are used to calculate Si.)
</tableCaption>
<table confidence="0.998482">
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.47 0.51 0.45 0.51
Conventional method (combination) 0.54 0.54 0.51 0.52
Our method (automatic) 0.90* 0.60* 0.90* 0.64*
Our method (full automatic) 0.85* 0.58 0.84* 0.60*
Our method (upper bound) 0.90* 0.62* 0.90* 0.69*
</table>
<tableCaption confidence="0.9842165">
Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy. (Five reference
translations are used to calculate Si.)
</tableCaption>
<table confidence="0.982023428571429">
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.49 0.53 0.46 0.53
Conventional method (combination) 0.56 0.56 0.52 0.54
Our method (automatic) 0.90* 0.60 0.90* 0.63*
Our method (full automatic) 0.86* 0.59 0.85* 0.60*
Our method (upper bound) 0.91* 0.63* 0.90* 0.69*
</table>
<bodyText confidence="0.988430931818182">
In these tables, * indicates significance at the 5% or less significance level.
ment of sub-goals were used to calculate Qj and Q�j
in Eq. (1). Skip word trigrams, skip word bigrams,
and skip word unigrams were used for generating
the sub-goals according to our preliminary experi-
ments. “Our method (upper bound)” indicates the
correlation coefficients obtained when human judg-
ments on the questions were used to calculate Qj
and Q�j.
As shown in Table 6, 7, and 8, our methods signif-
icantly outperform the conventional methods from
literature. Note that WER outperformed other indi-
vidual measures like BLEU and NIST in our exper-
iments, and the combination of existing automatic
evaluation methods from the literature outperformed
individual lexical similarity measures by themselves
in almost all cases. The differences between the
correlation coefficients obtained using our method
and the conventional methods are statistically sig-
nificant at the 5% or less significance level for flu-
ency and adequacy, even if the number of reference
translations increases, except in three cases shown
in Table 7 and 8. This indicates that considering
the rate of accomplishment of sub-goals to automat-
ically evaluate the quality of each translation is use-
ful, especially when the number of reference trans-
lations is small.
The differences between the correlation coeffi-
cients obtained using two automatic methods are not
significant. These results indicate that we can reduce
the development cost for constructing sub-goals.
However, there are still significant gaps between the
correlation coefficients obtained using a fully auto-
matic method and upper bounds. These gaps indi-
cate that we need further improvement in automatic
sub-goal generation and automatic estimation of rate
of accomplishment of sub-goals, which is our future
work.
Human judgments of adequacy and fluency are
known to be noisy, with varying levels of intercoder
agreement. Recent work has tended to apply cross-
judge normalization to address this issue (Blatz et
al., 2003). We would like to evaluate against the
normalized data in the future.
</bodyText>
<page confidence="0.932147">
39
</page>
<sectionHeader confidence="0.986303" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993333333333">
We demonstrated that the quality of a translated sen-
tence can be evaluated more appropriately than by
using conventional methods. That was demonstrated
by constructing a test set where the conditions that
should be satisfied to maintain a high translation
quality are assigned to each test-set sentence in the
form of a question, by developing a system that de-
termines an answer to each question, and by com-
bining a measure based on the questions and con-
ventional measures. We also presented a method for
automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals. Promising results were
obtained.
In the near future, we would like to expand the
test set to improve the upper bound obtained by
our method. We are also planning to expand the
method and improve the accuracy of the automatic
sub-goal generation and determination of the rate of
accomplishment of sub-goals. The sub-goals of a
given sentence should be generated by considering
the complexity of the sentence and the alignment in-
formation between the original source-language sen-
tence and its translation. Further advanced genera-
tion and estimation would give us information about
the erroneous parts of MT results and their quality.
We believe that future research would allow us to
develop high-quality MT systems by tuning the sys-
tem parameters based on the automatic MT evalua-
tion measures.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999698125">
The guideline for expanding the test set is based on that con-
structed by the Technical Research Committee of the AAMT
(Asia-Pacific Association for Machine Translation) The authors
would like to thank the committee members, especially, Mr.
Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro,
Mr. Masaru Fuji, and Ms. Yoshiko Matsukawa for their coop-
eration. This research is partially supported by special coordi-
nation funds for promoting science and technology.
</bodyText>
<sectionHeader confidence="0.996366" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863030303031">
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001.
Using Multiple Edit Distances to Automatically Rank Ma-
chine Translation Output. In Proceedings of the MT Summit
VIII, pages 15–20.
Bogdan Babych and Anthony Hartley. 2004. Extending the
BLEU MT Evaluation Method with Frequency Weightings.
In Proceedings of the 42nd ACL, pages 622–629.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An au-
tomatic metric for mt evaluation with improved correlation
with human judgments. In Proceedings of Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT and/or
Summarization, pages 65–72.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence Estimation for Machine Trans-
lation. Technical report, Center for Language and Speech
Processing, Johns Hopkins University. Summer Workshop
Final Report.
Norman R. Draper and Harry Smith. 1981. Applied Regression
Analysis. 2nd edition. Wiley.
Jesu´s Gime´nez, Enrique Amigo´, and Chiori Hori. 2005. Ma-
chine translation evaluation inside qarla. In Proceedings of
the IWSLT’05.
Satoru Ikehara, Satoshi Shirai, and Kentaro Ogura. 1994. Cri-
teria for Evaluating the Linguistic Quality of Japanese to
English Machine Translations. Transactions of the JSAI,
9(4):569–579. (in Japanese).
Hitoshi Isahara. 1995. JEIDA’s Test-Sets for Quality Evalua-
tion of MT Systems – Technical Evaluation from the Devel-
oper’s Point of View.
Philipp Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In Proceedings of the 2004
Conference on EMNLP, pages 388–395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A
Novel String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Proceedings of the
MT Summit IX, pages 240–247.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th COLING,
pages 501–507.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out, pages 74–81.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and Hermann
Ney. 2000. An Evaluation Tool for Machine Translation:
Fast Evaluation for MT Research. In Proceedings of the
LREC 2000, pages 39–45.
NIST. 2002. Automatic Evaluation of Machine Translation
Quality Using N-gram Co-Occurrence Statistics. Technical
report, NIST.
Franz Josef Och. 2003. Minimum Error Training in Statistical
Machine Translation. In Proceedings of the 41st ACL, pages
160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu.
2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th ACL, pages
311–318.
TIDES. 2002. Linguistic Data Annotation Specifi-
cation: Assessment of Fluency and Adequacy in
Arabic-English and Chinese-English Translations.
http://www.ldc.upenn.edu/Projects/TIDES/Translation/
TransAssess02.pdf.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of Machine Translation and its Evaluation. In Pro-
ceedings of the MT Summit IX, pages 386–393.
</reference>
<page confidence="0.916014">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959369">
<title confidence="0.995701">Automatic Evaluation of Machine Translation Based on Rate Accomplishment of Sub-goals</title>
<author confidence="0.99374">Uchimoto Kotani Zhang</author>
<affiliation confidence="0.99942">National Institute of Information and Communications</affiliation>
<address confidence="0.987488">3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289,</address>
<email confidence="0.989937">kat@khn.nict.go.jp</email>
<abstract confidence="0.9997295">The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically evaluating the quality of each translation. In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality. In English- Japanese translation, for example, prepositions and infinitives must be appropriately translated. We show several procedures that enable evaluating the quality of a translated sentence more appropriately than using conventional methods. The first procedure is constructing a test set where the conditions are assigned to each test-set sentence in the form of yes/no questions. The second procedure is developing a system that determines an answer to each question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Kenji Imamura</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using Multiple Edit Distances to Automatically Rank Machine Translation Output.</title>
<date>2001</date>
<booktitle>In Proceedings of the MT Summit VIII,</booktitle>
<pages>15--20</pages>
<contexts>
<context position="1734" citStr="Akiba et al., 2001" startWordPosition="254" endWordPosition="257">n answer to each question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be </context>
<context position="16744" citStr="Akiba et al., 2001" startWordPosition="2747" endWordPosition="2750">Here, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, S</context>
</contexts>
<marker>Akiba, Imamura, Sumita, 2001</marker>
<rawString>Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001. Using Multiple Edit Distances to Automatically Rank Machine Translation Output. In Proceedings of the MT Summit VIII, pages 15–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Extending the BLEU MT Evaluation Method with Frequency Weightings.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<pages>622--629</pages>
<contexts>
<context position="1837" citStr="Babych and Hartley, 2004" startWordPosition="272" endWordPosition="275">conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, abou</context>
<context position="16847" citStr="Babych and Hartley, 2004" startWordPosition="2765" endWordPosition="2768">nd λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate each similarity Si. Therefore, the value of m in Eq. (1) was 23. Jap</context>
<context position="20153" citStr="Babych and Hartley, 2004" startWordPosition="3316" endWordPosition="3320">f the common skip word n-grams included in the translation?” If no common skip word n-grams are found, the yes/no question is not generated. The answer to the yes/no question is determined to be yes if all of the common skip word n-grams are included in a translation. Otherwise, the answer is determined to be no. This scheme assigns greater weight to important phrases that should be included in the translation to maintain a high translation quality. Our observation is that those important phrases are often common between human translations. A similar scheme was proposed by Babych and Hartley (Babych and Hartley, 2004) for BLEU. In their scheme, greater weight is assigned to components that are salient throughout the document. Therefore, their scheme focuses on global context while our scheme focuses on local context. We believe that the two schemes are complementary to each other. 4 Experiments and Discussion In our experiments, the translation results of three MT systems and their subjective evaluation results were used as a development set for constructing the patterns described in Section 3.2 and for tuning the parameters λS,, λQP, λQ�P, and λE in Eq. (1). The translations and evaluation results of the </context>
</contexts>
<marker>Babych, Hartley, 2004</marker>
<rawString>Bogdan Babych and Anthony Hartley. 2004. Extending the BLEU MT Evaluation Method with Frequency Weightings. In Proceedings of the 42nd ACL, pages 622–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="1882" citStr="Banerjee and Lavie, 2005" startWordPosition="280" endWordPosition="283">od for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, about 300 or more sentences are used to automatic</context>
<context position="16892" citStr="Banerjee and Lavie, 2005" startWordPosition="2773" endWordPosition="2776">ment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate each similarity Si. Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by usin</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University. Summer Workshop Final Report.</institution>
<contexts>
<context position="26433" citStr="Blatz et al., 2003" startWordPosition="4282" endWordPosition="4285">icant. These results indicate that we can reduce the development cost for constructing sub-goals. However, there are still significant gaps between the correlation coefficients obtained using a fully automatic method and upper bounds. These gaps indicate that we need further improvement in automatic sub-goal generation and automatic estimation of rate of accomplishment of sub-goals, which is our future work. Human judgments of adequacy and fluency are known to be noisy, with varying levels of intercoder agreement. Recent work has tended to apply crossjudge normalization to address this issue (Blatz et al., 2003). We would like to evaluate against the normalized data in the future. 39 5 Conclusion and Future Work We demonstrated that the quality of a translated sentence can be evaluated more appropriately than by using conventional methods. That was demonstrated by constructing a test set where the conditions that should be satisfied to maintain a high translation quality are assigned to each test-set sentence in the form of a question, by developing a system that determines an answer to each question, and by combining a measure based on the questions and conventional measures. We also presented a met</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Technical report, Center for Language and Speech Processing, Johns Hopkins University. Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman R Draper</author>
<author>Harry Smith</author>
</authors>
<title>Applied Regression Analysis. 2nd edition.</title>
<date>1981</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="15958" citStr="Draper and Smith, 1981" startWordPosition="2590" endWordPosition="2593"> de] is included in a translation. Otherwise, the answer is no. Sample sentence They speak English in New Zealand. Question The personal pronoun “they” is omitted in a translation like “nyuujiilando de wa eigo wo hanasu”? Pattern The answer is yes if the pattern [karera walsore ra wa] is not included in a translation. Otherwise, the answer is no. ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation. The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981). m A = λSi X Si (1) i=1 (λQj X Qj + λQ,j X Qj) + λE � = 1 : if subgoal is accomplished 2 ( ) Qj 0 : otherwise 1 : if subgoal is unaccomplished 3 Q,j 0 : otherwise ( ) Here, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translate</context>
<context position="22197" citStr="Draper and Smith, 1981" startWordPosition="3642" endWordPosition="3645">Table 5. Table 5: Results of yes/no answer determination. Test set Accuracy Development 97.6% (2,629/2,694) Evaluation 82.8% (1,487/1,796) We investigated the correlation between the evaluation score, A in Eq. (1) and the subjective evaluations, fluency and adequacy, for the 769 test-set sentences. First, to maximize the correlation coefficients between the evaluation score, A, and the human subjective evaluations, fluency and adequacy, the optimal values of λS,, λQP, λQ�P, and λE in Eq. (1) were investigated using the development set within a framework of multiple linear regression modeling (Draper and Smith, 1981). Then, the correlation coefficients were investigated by using the optimal value set. The results are shown in Table 6, 7, and 8. In these tables, “Conventional method” indicates the correlation coefficients obtained when A was calculated by using only similarities Si. “Conventional method (combination)” is a combination of existing automatic evaluation methods from the literature. “Our method (automatic)” indicates the correlation coefficients obtained when the results of the automatic determination of yes/no answers were used to calculate Qj and Q�j in Eq. (1). For the 19 questions for whic</context>
</contexts>
<marker>Draper, Smith, 1981</marker>
<rawString>Norman R. Draper and Harry Smith. 1981. Applied Regression Analysis. 2nd edition. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesu´s Gime´nez</author>
<author>Enrique Amigo´</author>
<author>Chiori Hori</author>
</authors>
<title>Machine translation evaluation inside qarla.</title>
<date>2005</date>
<booktitle>In Proceedings of the IWSLT’05.</booktitle>
<marker>Gime´nez, Amigo´, Hori, 2005</marker>
<rawString>Jesu´s Gime´nez, Enrique Amigo´, and Chiori Hori. 2005. Machine translation evaluation inside qarla. In Proceedings of the IWSLT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoshi Shirai</author>
<author>Kentaro Ogura</author>
</authors>
<title>Criteria for Evaluating the Linguistic Quality of Japanese to English Machine Translations.</title>
<date>1994</date>
<journal>Transactions of the JSAI,</journal>
<volume>9</volume>
<issue>4</issue>
<note>(in Japanese).</note>
<contexts>
<context position="6240" citStr="Ikehara et al., 1994" startWordPosition="1012" endWordPosition="1015">d by intensively collecting sentence pairs that include linguistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation 1, NIST Machine Translation Evaluation 2, and HTRDP Evaluation 3 were organized to support the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conversation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluating MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel translations collected for MT evaluation and that the information for MT evaluation is assigned to, a test set. The most characteristic information assigned to the JEIDA test set is the yes/no question for assessing the translation results. For example, a yes/no question such as “Is ‘for’ </context>
</contexts>
<marker>Ikehara, Shirai, Ogura, 1994</marker>
<rawString>Satoru Ikehara, Satoshi Shirai, and Kentaro Ogura. 1994. Criteria for Evaluating the Linguistic Quality of Japanese to English Machine Translations. Transactions of the JSAI, 9(4):569–579. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitoshi Isahara</author>
</authors>
<title>JEIDA’s Test-Sets for Quality Evaluation</title>
<date>1995</date>
<booktitle>of MT Systems – Technical Evaluation from the Developer’s Point of View.</booktitle>
<contexts>
<context position="6286" citStr="Isahara, 1995" startWordPosition="1021" endWordPosition="1022">de linguistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation 1, NIST Machine Translation Evaluation 2, and HTRDP Evaluation 3 were organized to support the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conversation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluating MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel translations collected for MT evaluation and that the information for MT evaluation is assigned to, a test set. The most characteristic information assigned to the JEIDA test set is the yes/no question for assessing the translation results. For example, a yes/no question such as “Is ‘for’ translated into an expression representing a c</context>
</contexts>
<marker>Isahara, 1995</marker>
<rawString>Hitoshi Isahara. 1995. JEIDA’s Test-Sets for Quality Evaluation of MT Systems – Technical Evaluation from the Developer’s Point of View.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="2516" citStr="Koehn, 2004" startWordPosition="391" endWordPosition="392">5) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, about 300 or more sentences are used to automatically rank MT systems (Koehn, 2004). However, the quality of a sentence translated by an MT system is difficult to evaluate. For example, the results of five MTs into Japanese of the sentence “The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers.” are shown in Table 1. A conventional automatic evaluation method ranks the fifth MT result first although its human subjective evaluation is the lowest. This is because conventional methods are based on the similarity between a translated sentence and its reference translation, and they give the translated sentence a high score when the</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="1790" citStr="Leusch et al., 2003" startWordPosition="264" endWordPosition="267">ning a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their </context>
<context position="16800" citStr="Leusch et al., 2003" startWordPosition="2757" endWordPosition="2760">ment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate each similarity Si. T</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation. In Proceedings of the MT Summit IX, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING,</booktitle>
<pages>501--507</pages>
<contexts>
<context position="1856" citStr="Lin and Och, 2004" startWordPosition="276" endWordPosition="279">also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, about 300 or more sente</context>
<context position="16866" citStr="Lin and Och, 2004" startWordPosition="2769" endWordPosition="2772"> rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate each similarity Si. Therefore, the value of m in Eq. (1) was 23. Japanese word segmenta</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of the 20th COLING, pages 501–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="17279" citStr="Lin, 2004" startWordPosition="2847" endWordPosition="2848">the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate each similarity Si. Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN 4 in our experiments. As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information. 3.2 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determining the answer to each question as yes or no. This sectio</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of the Workshop on Text Summarization Branches Out, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the LREC</booktitle>
<pages>39--45</pages>
<contexts>
<context position="1714" citStr="Niessen et al., 2000" startWordPosition="250" endWordPosition="253">stem that determines an answer to each question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves</context>
<context position="16724" citStr="Niessen et al., 2000" startWordPosition="2743" endWordPosition="2746">Q,j 0 : otherwise ( ) Here, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 </context>
</contexts>
<marker>Niessen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Niessen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the LREC 2000, pages 39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<tech>Technical report, NIST.</tech>
<contexts>
<context position="1769" citStr="NIST, 2002" startWordPosition="262" endWordPosition="263">ure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for e</context>
<context position="16779" citStr="NIST, 2002" startWordPosition="2755" endWordPosition="2756">f accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate </context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics. Technical report, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2200" citStr="Och, 2003" startWordPosition="336" endWordPosition="337">ed to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, about 300 or more sentences are used to automatically rank MT systems (Koehn, 2004). However, the quality of a sentence translated by an MT system is difficult to evaluate. For example, the results of five MTs into Japanese of the sentence “The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers.” are shown in Table 1. </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Training in Statistical Machine Translation. In Proceedings of the 41st ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1757" citStr="Papineni et al., 2002" startWordPosition="258" endWordPosition="261">stion. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT r</context>
<context position="16767" citStr="Papineni et al., 2002" startWordPosition="2751" endWordPosition="2754">rresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIDES</author>
</authors>
<title>Linguistic Data Annotation Specification: Assessment of Fluency and Adequacy in Arabic-English and Chinese-English Translations.</title>
<date>2002</date>
<note>http://www.ldc.upenn.edu/Projects/TIDES/Translation/ TransAssess02.pdf.</note>
<contexts>
<context position="8488" citStr="TIDES, 2002" startWordPosition="1368" endWordPosition="1369">et consists of two parts, one for the evaluation of English-Japanese MT and the other for that of Japanese-English MT. We focused on the part for English-Japanese MT. This part consists of 769 sample sentences, each of which has a yes/no question. The 769 sentences were translated by using five commercial MT systems to investigate the relationship between subjective evaluation based on yes/no questions and conventional subjective evaluation based on fluency and adequacy. The instruction for the subjective evaluation based on fluency and adequacy followed that given in the TIDES specification (TIDES, 2002). The subjective evaluation based on yes/no questions was done by manually answering each question for each translation. The subjective evaluation based on the yes/no questions was stable; namely, it was almost independent of the human subjects in our preliminary investigation. There were only two questions for which the answers generated inconsistency in the subjective evaluation when 1,500 question-answer pairs were randomly sampled and evaluated by two human subjects. Then, we investigated the correlation between the two types of subjective evaluation. The correlation coefficients mentioned</context>
</contexts>
<marker>TIDES, 2002</marker>
<rawString>TIDES. 2002. Linguistic Data Annotation Specification: Assessment of Fluency and Adequacy in Arabic-English and Chinese-English Translations. http://www.ldc.upenn.edu/Projects/TIDES/Translation/ TransAssess02.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph P Turian</author>
<author>Luke Shen</author>
<author>I Dan Melamed</author>
</authors>
<title>Evaluation of Machine Translation and its Evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX,</booktitle>
<pages>386--393</pages>
<contexts>
<context position="1811" citStr="Turian et al., 2003" startWordPosition="268" endWordPosition="271">on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. MT systems can be ranked if a set of MT results for each system and their reference translation</context>
<context position="16821" citStr="Turian et al., 2003" startWordPosition="2761" endWordPosition="2764">having the i-th ID, and λQj is a weight for the rate of accomplishment. The term Q,j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ, j is a weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λE is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S*, SU*, W-1.2), were used to calculate each similarity Si. Therefore, the value o</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Evaluation of Machine Translation and its Evaluation. In Proceedings of the MT Summit IX, pages 386–393.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>