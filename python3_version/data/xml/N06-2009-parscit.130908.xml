<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010215">
<title confidence="0.9961705">
Answering the Question You Wish They Had Asked:
The Impact of Paraphrasing for Question Answering
</title>
<author confidence="0.720351">
Pablo Ariel Duboue
</author>
<affiliation confidence="0.56934">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.954088">
19 Skyline Drive
Hawthorne, NY 10532, USA
</address>
<email confidence="0.997936">
duboue@us.ibm.com
</email>
<author confidence="0.576835">
Jennifer Chu-Carroll
</author>
<affiliation confidence="0.456088">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.950782">
19 Skyline Drive
Hawthorne, NY 10532, USA
</address>
<email confidence="0.99897">
jencc@us.ibm.com
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999512">
State-of-the-art Question Answering (QA)
systems are very sensitive to variations
in the phrasing of an information need.
Finding the preferred language for such
a need is a valuable task. We investi-
gate that claim by adopting a simple MT-
based paraphrasing technique and evalu-
ating QA system performance on para-
phrased questions. We found a potential
increase of 35% in MRR with respect to
the original question.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922523809524">
In a typical Question Answering system, an input
question is analyzed to formulate a query to re-
trieve relevant documents from a target corpus (Chu-
Carroll et al., 2006; Harabagiu et al., 2006; Sun
et al., 2006). This analysis of the input question
affects the subset of documents that will be exam-
ined and ultimately plays a key role in determining
the answers the system chooses to produce. How-
ever, most existing QA systems, whether they adopt
knowledge-based, statistical, or hybrid methods, are
very sensitive to small variations in the question
form, often yielding substantially different answers
for questions that are semantically equivalent. For
example, our system’s answer to “Who invented the
telephone?” is “Alexander Graham Bell;” how-
ever, its top answer to a paraphrase of the above
question “Who is credited with the invention of the
telephone?” is “Gutenberg,” who is credited with
the invention of the printing press, while “Alexander
Graham Bell,” who is credited with the invention of
the telephone, appears in rank four.
</bodyText>
<page confidence="0.985953">
33
</page>
<bodyText confidence="0.99997535483871">
To demonstrate the ubiquity of this phenomenon,
we asked the aforementioned two questions to sev-
eral QA systems on the web, including LCC’s Pow-
erAnswer system,1 MIT’s START system,2 Answer-
Bus,3 and Ask Jeeves.4 All systems exhibited dif-
ferent behavior for the two phrasings of the ques-
tion, ranging from minor variations in documents
presented to justify an answer, to major differences
such as the presence of correct answers in the answer
list. For some systems, the more complex question
form posed sufficient difficulty that they chose not
to answer it.
In this paper we focus on investigating a high risk
but potentially high payoff approach, that of improv-
ing system performance by replacing the user ques-
tion with a paraphrased version of it. To obtain can-
didate paraphrases, we adopt a simple yet powerful
technique based on machine translation, which we
describe in the next section. Our experimental re-
sults show that we can potentially achieve a 35% rel-
ative improvement in system performance if we have
an oracle that always picks the optimal paraphrase
for each question. Our ultimate goal is to automat-
ically select from the set of candidates a high po-
tential paraphrase using a component trained against
the QA system. In Section 3, we present our ini-
tial approach to paraphrase selection which shows
that, despite the tremendous odds against selecting
performance-improving paraphrases, our conserva-
tive selection algorithm resulted in marginal im-
provement in system performance.
</bodyText>
<footnote confidence="0.9999945">
1http://www.languagecomputer.com/demos
2http://start.csail.mit.edu
3http://www.answerbus.com
4http://www.ask.com
</footnote>
<note confidence="0.581674">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33–36,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.797988166666667">
What toxins are most en—it Che tossine sono più peri- Which toxins are more
hazardous to expectant colose alle donne incinte? it—en dangerous to the preg-
mothers? nant women?
Find out about India’s Descubra sobre el pro- Discover on the program
(B) nuclear weapons pro- en—es grama de las armas nu- es—en of the nuclear weapons
gram. cleares de la India. of India.
</bodyText>
<figureCaption confidence="0.999508">
Figure 1: Example of lexical and syntactical paraphrases via MT-paraphrasing using Babelfish.
</figureCaption>
<sectionHeader confidence="0.481444" genericHeader="method">
2 MT-Based Automatic Paraphrasing
</sectionHeader>
<bodyText confidence="0.999804617647059">
To measure the impact of paraphrases on QA sys-
tems, we seek to adopt a methodology by which
paraphrases can be automatically generated from a
user question. Inspired by the use of parallel trans-
lations to mine paraphrasing lexicons (Barzilay and
McKeown, 2001) and the use of MT engines for
word sense disambiguation (Diab, 2000), we lever-
age existing machine translation systems to generate
semantically equivalent, albeit lexically and syntac-
tically distinct, questions.
Figure 1 (A) illustrates how MT-based paraphras-
ing captures lexical paraphrasing, ranging from ob-
taining simple synonyms such as hazardous and
dangerous to deriving more complex equivalent
phrases such as expectant mother and pregnant
woman. In addition to lexical paraphrasing, some
two-way translations achieve structural paraphras-
ing, as illustrated by the example in Figure 1 (B).
Using multiple MT engines can help paraphrase
diversity. For example, in Figure 1 (B), if we use the
@promt translator5 for English-to-Spanish transla-
tion and Babelfish6 for Spanish-to-English transla-
tion, we get “Find out on the nuclear armament
program of India” where both lexical and struc-
tural paraphrasings are observed.
The motivation of generating an array of lexically
and structurally distinct paraphrases is that some of
these paraphrases may better match the processing
capabilities of the underlying QA system than the
original question and are thus more likely to pro-
duce correct answers. Our observation is that while
the paraphrase set contains valuable performance-
improving phrasings, it also includes a large num-
ber of ungrammatical sentences which need to be fil-
</bodyText>
<footnote confidence="0.999975">
5http://www.online-translator.com
6http://babelfish.altavista.com
</footnote>
<subsectionHeader confidence="0.980522">
Question
</subsectionHeader>
<bodyText confidence="0.995922">
tered out to reduce negative impact on performance.
</bodyText>
<sectionHeader confidence="0.682962" genericHeader="method">
3 Using Automatic Paraphrasing in
</sectionHeader>
<subsectionHeader confidence="0.957415">
Question Answering
</subsectionHeader>
<bodyText confidence="0.948308708333333">
We use a generic architecture (Figure 2) that treats
a QA system as a black box that is invoked after a
paraphrase generation module, a feature extraction
module, and a paraphrase selection module are exe-
cuted. The preprocessing modules identifies a para-
phrase of the original question, which could be the
question itself, to send as input to the QA system.
A key advantage of treating the core QA system as
a black box is that the preprocessing modules can
be easily applied to improve the performance of any
QA system.7
We described the paraphrase generation module
in the previous section and will discuss the remain-
ing two modules below.
Feature Extraction Module. For each possible
paraphrase, we compare it against the original ques-
tion and compute the features shown in Table 1.
These are a subset of the features that we have ex-
perimented with and have found to be meaningful
for the task. All of these features are required in or-
7In our earlier experiments, we adopted an approach that
combines answers to all paraphrases through voting. These ex-
periments proved unsuccessful: in most cases, the answer to the
original question was amplified, both when right and wrong.
</bodyText>
<figureCaption confidence="0.993914">
Figure 2: System Architecture. Answer List
</figureCaption>
<figure confidence="0.999208352941176">
MT
Paraphraser
paraphrase
paraphrase
paraphrase
...
Feature
Extractor
paraphrase
paraphrase
...
paraphrase
Paraphrase
Selection
paraphrase
Q&amp;A
System
</figure>
<page confidence="0.975566">
34
</page>
<table confidence="0.981313888888889">
Feature Description Intuition
Sum The sum of the IDF scores for all terms in Paraphrases with more informative terms for
IDF the original question and the paraphrase. the corpus at hand should be preferred.
Lengths Number of query terms for each of the para- We expect QA systems to prefer shorter para-
phrase and the original question. phrases.
Cosine The distance between the vectors of both Certain paraphrases diverge too much from the
Distance questions, IDF-weighted. original.
Answer Whether answer types, as predicted by our Choosing a paraphrase that does not share an
Types question analyzer, are the same or overlap. answer type with the original question is risky.
</table>
<tableCaption confidence="0.999912">
Table 1: Our features, computed for each paraphrase by comparing it against the original question.
</tableCaption>
<bodyText confidence="0.999716617647059">
der not to lower the performance with respect to the
original question. They are ordered by their relative
contributions to the error rate reduction.
Paraphrase Selection Module. To select a para-
phrase, we used JRip, the Java re-implementation of
ripper (Cohen, 1996), a supervised rule learner in
the Weka toolkit (Witten and Frank, 2000).
We initially formulated paraphrase selection as a
three-way classification problem, with an attempt to
label each paraphrase as being “worse,” the “same,”
or “better” than the original question. Our objective
was to replace the original question with a para-
phrase labeled “better.” However, the priors for
these classes are roughly 30% for “worse,” 65% for
“same,” and 5% for “better”. Our empirical evi-
dence shows that successfully pinpointing a “better”
paraphrase improves, on average, the reciprocal rank
for a question by 0.5, while erroneously picking a
“worse” paraphrase results in a 0.75 decrease. That
is to say, errors are 1.5 times more costly than suc-
cesses (and five times more likely). This scenario
strongly suggests that a high precision algorithm is
critical for this component to be effective.
To increase precision, we took two steps. First,
we trained a cascade of two binary classifiers. The
first one classifies “worse” versus “same or better,”
with a bias for “worse.” The second classifier has
classes “worse or same” versus “better,” now with a
bias towards “better.” The second step is to constrain
the confidence of the classifier and only accept para-
phrases where the second classifier has a 100% con-
fidence. These steps are necessary to avoid decreas-
ing performance with respect to the original ques-
tion, as we will show in the next section.
</bodyText>
<sectionHeader confidence="0.998751" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.99992121875">
We trained the paraphrase selection module us-
ing our QA system, PIQUANT (Chu-Carroll et al.,
2006). Our target corpus is the AQUAINT corpus,
employed in the TREC QA track since 2002.
As for MT engines, we employed Babelfish
and Google MT,8 rule-based systems developed by
SY S T RAN and Google, respectively. We adopted
different MT engines based on the hypothesis that
differences in their translation rules will improve the
effectiveness of the paraphrasing module.
To measure performance, we trained and tested by
cross-validation over 712 questions from the TREC
9 and 10 datasets. We paraphrased the questions us-
ing the four possible combinations of MT engines
with up to 11 intermediate languages, obtaining a
total of 15,802 paraphrases. These questions were
then fed to our system and evaluated per TREC an-
swer key. We obtained a baseline MRR (top five
answers) of 0.345 running over the original ques-
tions. An oracle run, in which the best paraphrase
(or the original question) is always picked would
yield a MRR of 0.48. This potential increase is sub-
stantial, taking into account that a 35% improve-
ment separated the tenth participant from the sec-
ond in TREC-9. Our three-fold cross validation us-
ing the features and algorithm described in Section 3
yielded a MRR of 0.347. Over 712 questions, it re-
placed 14, two of which improved performance, the
rest stayed the same. On the other hand, random
selection of paraphrases decreased performance to
0.156, clearly showing the importance of selecting a
good paraphrase.
</bodyText>
<footnote confidence="0.99362">
8http://translate.google.com
</footnote>
<page confidence="0.999127">
35
</page>
<sectionHeader confidence="0.999678" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999676">
Most of the work in QA and paraphrasing focused
on folding paraphrasing knowledge into the question
analyzer or the answer locator (Rinaldi et al., 2003;
Tomuro, 2003). Our work, on the contrary, focuses
on question paraphrasing as an external component,
independent of the QA system architecture.
Some authors (Dumais et al., 2002; Echihabi et
al., 2004) considered the query sent to a search en-
gine as a “paraphrase” of the original natural lan-
guage question. For instance, Echihabi et al. (2004)
presented a large number of “reformulations” that
transformed the query into assertions that could
match the answers in text. Here we understand a
question paraphrase as a reformulation that is itself
a question, not a search engine query.
Other efforts in using paraphrasing for QA
(Duclaye et al., 2003) focused on using the Web
to obtain different verbalizations for a seed relation
(e.g., Author/Book); however, they have yet to apply
their learned paraphrases to QA.
Recently, there has been work on identifying para-
phrases equivalence classes for log analysis (Hed-
strom, 2005). Hedstrom used a vector model from
Information Retrieval that inspired our cosine mea-
sure feature described in Section 3.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99998764">
The work presented here makes contributions at
three different levels. First, we have shown that po-
tential impact of paraphrasing with respect to QA
performance is significant. Replacing a question
with a more felicitously worded question can poten-
tially result in a 35% performance increase.
Second, we performed our experiments by tap-
ping into a readily available paraphrase resource:
MT engines. Our results speak of the usefulness of
the approach in producing paraphrases. This tech-
nique of obtaining a large, although low quality,
set of paraphrases can be easily employed by other
NLP practitioners wishing to investigate the impact
of paraphrasing on their own problems.
Third, we have shown that the task of selecting a
better phrasing is amenable to learning, though more
work is required to achieve its full potential. In that
respect, the features and architecture discussed in
Section 3 are a necessary first step in that direction.
In future work, we are interested in developing
effective filtering techniques to reduce our candidate
set to a small number of high precision paraphrases,
in experimenting with state-of-the-art paraphrasers,
and in using paraphrasing to improve the stability of
the QA system.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999075">
The authors would like to thank Nelson Correa and
Annie Ying for helpful discussions and comments.
This work was supported in part by the Disruptive
Technology Office (DTO)’s Advanced Question An-
swering for Intelligence (AQUAINT) Program un-
der contract number H98230-04-C-1577.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999514261904762">
Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL-EACL 2001), Toulouse, France, July.
Jennifer Chu-Carroll, Pablo A. Duboue, John M. Prager, and Krzysztof Czuba.
2006. IBM’s piquant II in TREC 2005. In E. M. Voorhees and Lori P. Buck-
land, editors, Proceedings of the Fourthteen Text REtrieval Conference Pro-
ceedings (TREC 2005), Gaithersburg, MD, USA.
William Cohen. 1996. Learning trees and rules with set-valued features. In
Proceedings of the 14th joint American Association for Artificial Intelligence
and IAAI Conference (AAAI/IAAI-96), pages 709–716. American Association
for Artificial Intelligence.
Mona Diab. 2000. An unsupervised method for word sense tagging using parallel
corpora: A preliminary investigation. In Special Interest Group in Lexical
Semantics (SIGLEX) Workshop, Association for Computational Linguistics,
Hong Kong, China, October.
Florence Duclaye, Francois Yvon, and Olivier Collin. 2003. Learning para-
phrases to improve a question-answering system. In EACL 2003, 11th Con-
ference of the European Chapter of the Association for Computational Lin-
guistics, Workshop in NLPfor QA, Budapest, Hungary, April.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering:
is more always better? In Proc. SIGIR ’02, pages 291–298, New York, NY,
USA. ACM Press.
A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran.
2004. Multiple-engine question answering in textmap. In Proc. TREC 2003.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2006.
Employing two question answering systems. In Proc. TREC 2005.
Anna Hedstrom. 2005. Question categorization for a question answering system
using a vector space model. Master’s thesis, Department of Linguistics and
Philology (Language Technology Programme) Uppsala University, Uppsala,
Sweden.
Fabio Rinaldi, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Mollá.
2003. Exploiting paraphrases in a question answering system. In Proceedings
of the Second International Workshop on Paraphrasing, pages 25–32, July.
R. Sun, J. Jiang, Y.F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2006. Using
syntactic and semantic relation analysis in question answering. In Proc. TREC
2005.
Noriko Tomuro. 2003. Interrogative reformulation patterns and acquisition of
question paraphrases. In Proceedings of the Second International Workshop
on Paraphrasing, pages 33–40, July.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning
Tools and Techniques with Java Implementations. Morgan Kaufmann Pub-
lishers.
</reference>
<page confidence="0.998935">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.384512">
<title confidence="0.9980685">Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering</title>
<author confidence="0.999941">Pablo Ariel</author>
<affiliation confidence="0.987643">IBM T.J. Watson Research</affiliation>
<address confidence="0.820976">19 Skyline Hawthorne, NY 10532, USA</address>
<email confidence="0.999521">duboue@us.ibm.com</email>
<author confidence="0.996104">Jennifer</author>
<affiliation confidence="0.990152">IBM T.J. Watson Research</affiliation>
<address confidence="0.821528">19 Skyline Hawthorne, NY 10532, USA</address>
<email confidence="0.999908">jencc@us.ibm.com</email>
<abstract confidence="0.995357166666667">State-of-the-art Question Answering (QA) systems are very sensitive to variations in the phrasing of an information need. Finding the preferred language for such a need is a valuable task. We investigate that claim by adopting a simple MTbased paraphrasing technique and evaluating QA system performance on paraphrased questions. We found a potential increase of 35% in MRR with respect to the original question.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-EACL 2001),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="4345" citStr="Barzilay and McKeown, 2001" startWordPosition="673" endWordPosition="676"> it—en dangerous to the pregmothers? nant women? Find out about India’s Descubra sobre el pro- Discover on the program (B) nuclear weapons pro- en—es grama de las armas nu- es—en of the nuclear weapons gram. cleares de la India. of India. Figure 1: Example of lexical and syntactical paraphrases via MT-paraphrasing using Babelfish. 2 MT-Based Automatic Paraphrasing To measure the impact of paraphrases on QA systems, we seek to adopt a methodology by which paraphrases can be automatically generated from a user question. Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines for word sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. Figure 1 (A) illustrates how MT-based paraphrasing captures lexical paraphrasing, ranging from obtaining simple synonyms such as hazardous and dangerous to deriving more complex equivalent phrases such as expectant mother and pregnant woman. In addition to lexical paraphrasing, some two-way translations achieve structural paraphrasing, as illustrated by the example in Figure 1 (B). Us</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-EACL 2001), Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Pablo A Duboue</author>
<author>John M Prager</author>
<author>Krzysztof Czuba</author>
</authors>
<title>IBM’s piquant II in TREC</title>
<date>2006</date>
<booktitle>Proceedings of the Fourthteen Text REtrieval Conference Proceedings (TREC 2005),</booktitle>
<editor>In E. M. Voorhees and Lori P. Buckland, editors,</editor>
<location>Gaithersburg, MD, USA.</location>
<contexts>
<context position="9888" citStr="Chu-Carroll et al., 2006" startWordPosition="1542" endWordPosition="1545">ined a cascade of two binary classifiers. The first one classifies “worse” versus “same or better,” with a bias for “worse.” The second classifier has classes “worse or same” versus “better,” now with a bias towards “better.” The second step is to constrain the confidence of the classifier and only accept paraphrases where the second classifier has a 100% confidence. These steps are necessary to avoid decreasing performance with respect to the original question, as we will show in the next section. 4 Experimental Results We trained the paraphrase selection module using our QA system, PIQUANT (Chu-Carroll et al., 2006). Our target corpus is the AQUAINT corpus, employed in the TREC QA track since 2002. As for MT engines, we employed Babelfish and Google MT,8 rule-based systems developed by SY S T RAN and Google, respectively. We adopted different MT engines based on the hypothesis that differences in their translation rules will improve the effectiveness of the paraphrasing module. To measure performance, we trained and tested by cross-validation over 712 questions from the TREC 9 and 10 datasets. We paraphrased the questions using the four possible combinations of MT engines with up to 11 intermediate langu</context>
</contexts>
<marker>Chu-Carroll, Duboue, Prager, Czuba, 2006</marker>
<rawString>Jennifer Chu-Carroll, Pablo A. Duboue, John M. Prager, and Krzysztof Czuba. 2006. IBM’s piquant II in TREC 2005. In E. M. Voorhees and Lori P. Buckland, editors, Proceedings of the Fourthteen Text REtrieval Conference Proceedings (TREC 2005), Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
</authors>
<title>Learning trees and rules with set-valued features.</title>
<date>1996</date>
<journal>Artificial Intelligence.</journal>
<booktitle>In Proceedings of the 14th joint American Association for Artificial Intelligence and IAAI Conference (AAAI/IAAI-96),</booktitle>
<pages>709--716</pages>
<publisher>American Association for</publisher>
<contexts>
<context position="8321" citStr="Cohen, 1996" startWordPosition="1290" endWordPosition="1291">from the Distance questions, IDF-weighted. original. Answer Whether answer types, as predicted by our Choosing a paraphrase that does not share an Types question analyzer, are the same or overlap. answer type with the original question is risky. Table 1: Our features, computed for each paraphrase by comparing it against the original question. der not to lower the performance with respect to the original question. They are ordered by their relative contributions to the error rate reduction. Paraphrase Selection Module. To select a paraphrase, we used JRip, the Java re-implementation of ripper (Cohen, 1996), a supervised rule learner in the Weka toolkit (Witten and Frank, 2000). We initially formulated paraphrase selection as a three-way classification problem, with an attempt to label each paraphrase as being “worse,” the “same,” or “better” than the original question. Our objective was to replace the original question with a paraphrase labeled “better.” However, the priors for these classes are roughly 30% for “worse,” 65% for “same,” and 5% for “better”. Our empirical evidence shows that successfully pinpointing a “better” paraphrase improves, on average, the reciprocal rank for a question by</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>William Cohen. 1996. Learning trees and rules with set-valued features. In Proceedings of the 14th joint American Association for Artificial Intelligence and IAAI Conference (AAAI/IAAI-96), pages 709–716. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora: A preliminary investigation.</title>
<date>2000</date>
<booktitle>In Special Interest Group in Lexical Semantics (SIGLEX) Workshop, Association for Computational Linguistics,</booktitle>
<location>Hong Kong, China,</location>
<contexts>
<context position="4414" citStr="Diab, 2000" startWordPosition="687" endWordPosition="688">e el pro- Discover on the program (B) nuclear weapons pro- en—es grama de las armas nu- es—en of the nuclear weapons gram. cleares de la India. of India. Figure 1: Example of lexical and syntactical paraphrases via MT-paraphrasing using Babelfish. 2 MT-Based Automatic Paraphrasing To measure the impact of paraphrases on QA systems, we seek to adopt a methodology by which paraphrases can be automatically generated from a user question. Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines for word sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. Figure 1 (A) illustrates how MT-based paraphrasing captures lexical paraphrasing, ranging from obtaining simple synonyms such as hazardous and dangerous to deriving more complex equivalent phrases such as expectant mother and pregnant woman. In addition to lexical paraphrasing, some two-way translations achieve structural paraphrasing, as illustrated by the example in Figure 1 (B). Using multiple MT engines can help paraphrase diversity. For example, i</context>
</contexts>
<marker>Diab, 2000</marker>
<rawString>Mona Diab. 2000. An unsupervised method for word sense tagging using parallel corpora: A preliminary investigation. In Special Interest Group in Lexical Semantics (SIGLEX) Workshop, Association for Computational Linguistics, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florence Duclaye</author>
<author>Francois Yvon</author>
<author>Olivier Collin</author>
</authors>
<title>Learning paraphrases to improve a question-answering system.</title>
<date>2003</date>
<booktitle>In EACL 2003, 11th Conference of the European Chapter of the Association for Computational Linguistics, Workshop in NLPfor QA,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="12169" citStr="Duclaye et al., 2003" startWordPosition="1913" endWordPosition="1916">he contrary, focuses on question paraphrasing as an external component, independent of the QA system architecture. Some authors (Dumais et al., 2002; Echihabi et al., 2004) considered the query sent to a search engine as a “paraphrase” of the original natural language question. For instance, Echihabi et al. (2004) presented a large number of “reformulations” that transformed the query into assertions that could match the answers in text. Here we understand a question paraphrase as a reformulation that is itself a question, not a search engine query. Other efforts in using paraphrasing for QA (Duclaye et al., 2003) focused on using the Web to obtain different verbalizations for a seed relation (e.g., Author/Book); however, they have yet to apply their learned paraphrases to QA. Recently, there has been work on identifying paraphrases equivalence classes for log analysis (Hedstrom, 2005). Hedstrom used a vector model from Information Retrieval that inspired our cosine measure feature described in Section 3. 6 Conclusions The work presented here makes contributions at three different levels. First, we have shown that potential impact of paraphrasing with respect to QA performance is significant. Replacing</context>
</contexts>
<marker>Duclaye, Yvon, Collin, 2003</marker>
<rawString>Florence Duclaye, Francois Yvon, and Olivier Collin. 2003. Learning paraphrases to improve a question-answering system. In EACL 2003, 11th Conference of the European Chapter of the Association for Computational Linguistics, Workshop in NLPfor QA, Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>M Banko</author>
<author>E Brill</author>
<author>J Lin</author>
<author>A Ng</author>
</authors>
<title>Web question answering: is more always better?</title>
<date>2002</date>
<booktitle>In Proc. SIGIR ’02,</booktitle>
<pages>291--298</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11696" citStr="Dumais et al., 2002" startWordPosition="1835" endWordPosition="1838">er 712 questions, it replaced 14, two of which improved performance, the rest stayed the same. On the other hand, random selection of paraphrases decreased performance to 0.156, clearly showing the importance of selecting a good paraphrase. 8http://translate.google.com 35 5 Related Work Most of the work in QA and paraphrasing focused on folding paraphrasing knowledge into the question analyzer or the answer locator (Rinaldi et al., 2003; Tomuro, 2003). Our work, on the contrary, focuses on question paraphrasing as an external component, independent of the QA system architecture. Some authors (Dumais et al., 2002; Echihabi et al., 2004) considered the query sent to a search engine as a “paraphrase” of the original natural language question. For instance, Echihabi et al. (2004) presented a large number of “reformulations” that transformed the query into assertions that could match the answers in text. Here we understand a question paraphrase as a reformulation that is itself a question, not a search engine query. Other efforts in using paraphrasing for QA (Duclaye et al., 2003) focused on using the Web to obtain different verbalizations for a seed relation (e.g., Author/Book); however, they have yet to</context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering: is more always better? In Proc. SIGIR ’02, pages 291–298, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>E Hovy U Hermjakob</author>
<author>D Marcu</author>
<author>E Melz</author>
<author>D Ravichandran</author>
</authors>
<title>Multiple-engine question answering in textmap.</title>
<date>2004</date>
<booktitle>In Proc. TREC</booktitle>
<contexts>
<context position="11720" citStr="Echihabi et al., 2004" startWordPosition="1839" endWordPosition="1842">replaced 14, two of which improved performance, the rest stayed the same. On the other hand, random selection of paraphrases decreased performance to 0.156, clearly showing the importance of selecting a good paraphrase. 8http://translate.google.com 35 5 Related Work Most of the work in QA and paraphrasing focused on folding paraphrasing knowledge into the question analyzer or the answer locator (Rinaldi et al., 2003; Tomuro, 2003). Our work, on the contrary, focuses on question paraphrasing as an external component, independent of the QA system architecture. Some authors (Dumais et al., 2002; Echihabi et al., 2004) considered the query sent to a search engine as a “paraphrase” of the original natural language question. For instance, Echihabi et al. (2004) presented a large number of “reformulations” that transformed the query into assertions that could match the answers in text. Here we understand a question paraphrase as a reformulation that is itself a question, not a search engine query. Other efforts in using paraphrasing for QA (Duclaye et al., 2003) focused on using the Web to obtain different verbalizations for a seed relation (e.g., Author/Book); however, they have yet to apply their learned par</context>
</contexts>
<marker>Echihabi, Hermjakob, Marcu, Melz, Ravichandran, 2004</marker>
<rawString>A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran. 2004. Multiple-engine question answering in textmap. In Proc. TREC 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>A Hickl</author>
<author>P Wang</author>
</authors>
<title>Employing two question answering systems.</title>
<date>2006</date>
<booktitle>In Proc. TREC</booktitle>
<contexts>
<context position="949" citStr="Harabagiu et al., 2006" startWordPosition="145" endWordPosition="148">ct State-of-the-art Question Answering (QA) systems are very sensitive to variations in the phrasing of an information need. Finding the preferred language for such a need is a valuable task. We investigate that claim by adopting a simple MTbased paraphrasing technique and evaluating QA system performance on paraphrased questions. We found a potential increase of 35% in MRR with respect to the original question. 1 Introduction In a typical Question Answering system, an input question is analyzed to formulate a query to retrieve relevant documents from a target corpus (ChuCarroll et al., 2006; Harabagiu et al., 2006; Sun et al., 2006). This analysis of the input question affects the subset of documents that will be examined and ultimately plays a key role in determining the answers the system chooses to produce. However, most existing QA systems, whether they adopt knowledge-based, statistical, or hybrid methods, are very sensitive to small variations in the question form, often yielding substantially different answers for questions that are semantically equivalent. For example, our system’s answer to “Who invented the telephone?” is “Alexander Graham Bell;” however, its top answer to a paraphrase of the</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2006</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2006. Employing two question answering systems. In Proc. TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Hedstrom</author>
</authors>
<title>Question categorization for a question answering system using a vector space model.</title>
<date>2005</date>
<tech>Master’s thesis,</tech>
<institution>Department of Linguistics and Philology (Language Technology Programme) Uppsala University,</institution>
<location>Uppsala,</location>
<contexts>
<context position="12446" citStr="Hedstrom, 2005" startWordPosition="1957" endWordPosition="1959">instance, Echihabi et al. (2004) presented a large number of “reformulations” that transformed the query into assertions that could match the answers in text. Here we understand a question paraphrase as a reformulation that is itself a question, not a search engine query. Other efforts in using paraphrasing for QA (Duclaye et al., 2003) focused on using the Web to obtain different verbalizations for a seed relation (e.g., Author/Book); however, they have yet to apply their learned paraphrases to QA. Recently, there has been work on identifying paraphrases equivalence classes for log analysis (Hedstrom, 2005). Hedstrom used a vector model from Information Retrieval that inspired our cosine measure feature described in Section 3. 6 Conclusions The work presented here makes contributions at three different levels. First, we have shown that potential impact of paraphrasing with respect to QA performance is significant. Replacing a question with a more felicitously worded question can potentially result in a 35% performance increase. Second, we performed our experiments by tapping into a readily available paraphrase resource: MT engines. Our results speak of the usefulness of the approach in producing</context>
</contexts>
<marker>Hedstrom, 2005</marker>
<rawString>Anna Hedstrom. 2005. Question categorization for a question answering system using a vector space model. Master’s thesis, Department of Linguistics and Philology (Language Technology Programme) Uppsala University, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Rinaldi</author>
<author>James Dowdall</author>
<author>Kaarel Kaljurand</author>
<author>Michael Hess</author>
<author>Diego Mollá</author>
</authors>
<title>Exploiting paraphrases in a question answering system.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Workshop on Paraphrasing,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11517" citStr="Rinaldi et al., 2003" startWordPosition="1808" endWordPosition="1811">vement separated the tenth participant from the second in TREC-9. Our three-fold cross validation using the features and algorithm described in Section 3 yielded a MRR of 0.347. Over 712 questions, it replaced 14, two of which improved performance, the rest stayed the same. On the other hand, random selection of paraphrases decreased performance to 0.156, clearly showing the importance of selecting a good paraphrase. 8http://translate.google.com 35 5 Related Work Most of the work in QA and paraphrasing focused on folding paraphrasing knowledge into the question analyzer or the answer locator (Rinaldi et al., 2003; Tomuro, 2003). Our work, on the contrary, focuses on question paraphrasing as an external component, independent of the QA system architecture. Some authors (Dumais et al., 2002; Echihabi et al., 2004) considered the query sent to a search engine as a “paraphrase” of the original natural language question. For instance, Echihabi et al. (2004) presented a large number of “reformulations” that transformed the query into assertions that could match the answers in text. Here we understand a question paraphrase as a reformulation that is itself a question, not a search engine query. Other efforts</context>
</contexts>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Mollá, 2003</marker>
<rawString>Fabio Rinaldi, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Mollá. 2003. Exploiting paraphrases in a question answering system. In Proceedings of the Second International Workshop on Paraphrasing, pages 25–32, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sun</author>
<author>J Jiang</author>
<author>Y F Tan</author>
<author>H Cui</author>
<author>T-S Chua</author>
<author>M-Y Kan</author>
</authors>
<title>Using syntactic and semantic relation analysis in question answering.</title>
<date>2006</date>
<booktitle>In Proc. TREC</booktitle>
<contexts>
<context position="968" citStr="Sun et al., 2006" startWordPosition="149" endWordPosition="152">tion Answering (QA) systems are very sensitive to variations in the phrasing of an information need. Finding the preferred language for such a need is a valuable task. We investigate that claim by adopting a simple MTbased paraphrasing technique and evaluating QA system performance on paraphrased questions. We found a potential increase of 35% in MRR with respect to the original question. 1 Introduction In a typical Question Answering system, an input question is analyzed to formulate a query to retrieve relevant documents from a target corpus (ChuCarroll et al., 2006; Harabagiu et al., 2006; Sun et al., 2006). This analysis of the input question affects the subset of documents that will be examined and ultimately plays a key role in determining the answers the system chooses to produce. However, most existing QA systems, whether they adopt knowledge-based, statistical, or hybrid methods, are very sensitive to small variations in the question form, often yielding substantially different answers for questions that are semantically equivalent. For example, our system’s answer to “Who invented the telephone?” is “Alexander Graham Bell;” however, its top answer to a paraphrase of the above question “Wh</context>
</contexts>
<marker>Sun, Jiang, Tan, Cui, Chua, Kan, 2006</marker>
<rawString>R. Sun, J. Jiang, Y.F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2006. Using syntactic and semantic relation analysis in question answering. In Proc. TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
</authors>
<title>Interrogative reformulation patterns and acquisition of question paraphrases.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Workshop on Paraphrasing,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="11532" citStr="Tomuro, 2003" startWordPosition="1812" endWordPosition="1813">enth participant from the second in TREC-9. Our three-fold cross validation using the features and algorithm described in Section 3 yielded a MRR of 0.347. Over 712 questions, it replaced 14, two of which improved performance, the rest stayed the same. On the other hand, random selection of paraphrases decreased performance to 0.156, clearly showing the importance of selecting a good paraphrase. 8http://translate.google.com 35 5 Related Work Most of the work in QA and paraphrasing focused on folding paraphrasing knowledge into the question analyzer or the answer locator (Rinaldi et al., 2003; Tomuro, 2003). Our work, on the contrary, focuses on question paraphrasing as an external component, independent of the QA system architecture. Some authors (Dumais et al., 2002; Echihabi et al., 2004) considered the query sent to a search engine as a “paraphrase” of the original natural language question. For instance, Echihabi et al. (2004) presented a large number of “reformulations” that transformed the query into assertions that could match the answers in text. Here we understand a question paraphrase as a reformulation that is itself a question, not a search engine query. Other efforts in using parap</context>
</contexts>
<marker>Tomuro, 2003</marker>
<rawString>Noriko Tomuro. 2003. Interrogative reformulation patterns and acquisition of question paraphrases. In Proceedings of the Second International Workshop on Paraphrasing, pages 33–40, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="8393" citStr="Witten and Frank, 2000" startWordPosition="1300" endWordPosition="1303">hether answer types, as predicted by our Choosing a paraphrase that does not share an Types question analyzer, are the same or overlap. answer type with the original question is risky. Table 1: Our features, computed for each paraphrase by comparing it against the original question. der not to lower the performance with respect to the original question. They are ordered by their relative contributions to the error rate reduction. Paraphrase Selection Module. To select a paraphrase, we used JRip, the Java re-implementation of ripper (Cohen, 1996), a supervised rule learner in the Weka toolkit (Witten and Frank, 2000). We initially formulated paraphrase selection as a three-way classification problem, with an attempt to label each paraphrase as being “worse,” the “same,” or “better” than the original question. Our objective was to replace the original question with a paraphrase labeled “better.” However, the priors for these classes are roughly 30% for “worse,” 65% for “same,” and 5% for “better”. Our empirical evidence shows that successfully pinpointing a “better” paraphrase improves, on average, the reciprocal rank for a question by 0.5, while erroneously picking a “worse” paraphrase results in a 0.75 d</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>