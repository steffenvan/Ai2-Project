<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002140">
<title confidence="0.891483">
Hierarchical Phrase-Based Translation with
Weighted Finite State Transducers
</title>
<author confidence="0.7710495">
Gonzalo Iglesias* Adri`a de Gispert‡
Eduardo R. Banga* William Byrne‡
</author>
<affiliation confidence="0.966963">
* University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
</affiliation>
<email confidence="0.975565">
{giglesia,erbanga}@gts.tsc.uvigo.es
</email>
<affiliation confidence="0.972256">
‡ University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
</affiliation>
<email confidence="0.996777">
{ad465,wjb31}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.996624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999077764705882">
This paper describes a lattice-based decoder
for hierarchical phrase-based translation. The
decoder is implemented with standard WFST
operations as an alternative to the well-known
cube pruning procedure. We find that the
use of WFSTs rather than k-best lists requires
less pruning in translation search, resulting
in fewer search errors, direct generation of
translation lattices in the target language,
better parameter optimization, and improved
translation performance when rescoring with
long-span language models and MBR decod-
ing. We report translation experiments for
the Arabic-to-English and Chinese-to-English
NIST translation tasks and contrast the WFST-
based hierarchical decoder with hierarchical
translation under cube pruning.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913861111111">
Hierarchical phrase-based translation generates
translation hypotheses via the application of hierar-
chical rules in CYK parsing (Chiang, 2005). Cube
pruning is used to apply language models at each
cell of the CYK grid as part of the search for a
k-best list of translation candidates (Chiang, 2005;
Chiang, 2007). While this approach is very effective
and has been shown to produce very good quality
translation, the reliance on k-best lists is a limita-
tion. We take an alternative approach and describe a
lattice-based hierarchical decoder implemented with
Weighted Finite State Transducers (WFSTs). In ev-
ery CYK cell we build a single, minimal word lattice
containing all possible translations of the source sen-
tence span covered by that cell. When derivations
contain non-terminals, we use pointers to lower-
level lattices for memory efficiency. The pointers
are only expanded to the actual translations if prun-
ing is required during search; expansion is otherwise
only carried out at the upper-most cell, after the full
CYK grid has been traversed.
We describe how this decoder can be easily im-
plemented with WFSTs. For this we employ the
OpenFST libraries (Allauzen et al., 2007). Using
standard FST operations such as composition, ep-
silon removal, determinization, minimization and
shortest-path, we find this search procedure to be
simpler to implement than cube pruning. The main
modeling advantages are a significant reduction in
search errors, a simpler implementation, direct gen-
eration of target language word lattices, and better
integration with other statistical MT procedures. We
report translation results in Arabic-to-English and
Chinese-to-English translation and contrast the per-
formance of lattice-based and cube pruning hierar-
chical decoding.
</bodyText>
<sectionHeader confidence="0.679275" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.9999245">
Hierarchical phrase-based translation has emerged
as one of the dominant current approaches to statis-
tical machine translation. Hiero translation systems
incorporate many of the strengths of phrase-based
translation systems, such as feature-based transla-
tion and strong target language models, while also
allowing flexible translation and movement based
on hierarchical rules extracted from aligned paral-
lel text. We summarize some extensions to the basic
approach to put our work in context.
</bodyText>
<page confidence="0.992141">
433
</page>
<note confidence="0.898416">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.994165604651163">
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning to
improve translation speed. Venugopal et al. (2007)
introduce a Hiero variant with relaxed constraints
for hypothesis recombination during parsing; speed
and results are comparable to those of cube prun-
ing, as described by Chiang (2007). Li and Khudan-
pur (2008) report significant improvements in trans-
lation speed by taking unseen n-grams into account
within cube pruning to minimize language model re-
quests. Dyer et al. (2008) extend the translation of
source sentences to translation of input lattices fol-
lowing Chappelier et al. (1999).
Extensions to Hiero Several authors describe ex-
tensions to Hiero, to incorporate additional syntactic
information (Zollmann and Venugopal, 2006; Zhang
and Gildea, 2006; Shen et al., 2008; Marton and
Resnik, 2008), or to combine it with discriminative
latent models (Blunsom et al., 2008).
Analysis and Contrastive Experiments Zollman et
al. (2008) compare phrase-based, hierarchical and
syntax-augmented decoders for translation of Ara-
bic, Chinese, and Urdu into English. Lopez (2008)
explores whether lexical reordering or the phrase
discontiguity inherent in hierarchical rules explains
improvements over phrase-based systems. Hierar-
chical translation has also been used to great effect
in combination with other translation architectures,
e.g. (Sim et al., 2007; Rosti et al., 2007).
WFSTs for Translation There is extensive work in
using Weighted Finite State Transducer for machine
translation (Bangalore and Riccardi, 2001; Casacu-
berta, 2001; Kumar and Byrne, 2005; Mathias and
Byrne, 2006; Graehl et al., 2008).
To our knowledge, this paper presents the first de-
scription of hierarchical phrase-based translation in
terms of lattices rather than k-best lists. The next
section describes hierarchical phrase-based transla-
tion with WFSTs, including the lattice construction
over the CYK grid and pruning strategies. Sec-
tion 3 reports translation experiments for Arabic-to-
English and Chinese-to-English, and Section 4 con-
cludes.
</bodyText>
<sectionHeader confidence="0.919281" genericHeader="method">
2 Hierarchical Translation with WFSTs
</sectionHeader>
<bodyText confidence="0.978166581395349">
The translation system is based on a variant of the
CYK algorithm closely related to CYK+ (Chappe-
lier and Rajman, 1998). Parsing follows the de-
scription of Chiang (2005; 2007), maintaining back-
pointers and employing hypothesis recombination
without pruning. The underlying model is a syn-
chronous context-free grammar consisting of a set
R = {Rr} of rules Rr : N -* (&apos;yr,αr) / pr, with
‘glue’ rules, S -* (X,X) and S -* (S X,S X). If a
rule has probability pr, it is transformed to a cost cr;
here we use the tropical semiring, so cr = − log pr.
N denotes a non-terminal; in this paper, N can be
either S, X, or V (see section 3.2). T denotes the
terminals (words), and the grammar builds parses
based on strings &apos;y, α E {{S, X, V } U T}+. Each
cell in the CYK grid is specified by a non-terminal
symbol and position in the CYK grid: (N, x, y),
which spans sx+y−1 on the source sentence.
x
In effect, the source language sentence is parsed
using a context-free grammar with rules N -* &apos;y.
The generation of translations is a second step that
follows parsing. For this second step, we describe
a method to construct word lattices with all possible
translations that can be produced by the hierarchical
rules. Construction proceeds by traversing the CYK
grid along the backpointers established in parsing.
In each cell (N, x, y) in the CYK grid, we build a
target language word lattice L(N, x, y). This lat-
tice contains every translation of sx+y−1 from every
x
derivation headed by N. These lattices also contain
the translation scores on their arc weights.
The ultimate objective is the word lattice
L(S,1, J) which corresponds to all the analyses that
cover the source sentence sJ1 . Once this is built,
we can apply a target language model to L(S,1, J)
to obtain the final target language translation lattice
(Allauzen et al., 2003).
We use the approach of Mohri (2002) in applying
WFSTs to statistical NLP. This fits well with the use
of the OpenFST toolkit (Allauzen et al., 2007) to
implement our decoder.
</bodyText>
<subsectionHeader confidence="0.992025">
2.1 Lattice Construction Over the CYK Grid
</subsectionHeader>
<bodyText confidence="0.984157571428571">
In each cell (N, x, y), the set of rule indices used
by the parser is denoted R(N, x, y), i.e. for r E
R(N, x, y), N -* (&apos;yr,αr) was used in at least one
derivation involving that cell.
For each rule Rr, r E R(N, x, y), we build a lat-
tice L(N, x, y, r). This lattice is derived from the
target side of the rule αr by concatenating lattices
</bodyText>
<page confidence="0.957655">
434
</page>
<equation confidence="0.999878571428572">
R1: X → hs1 s2 s3,t1 t2i
R2: X → hs1 s2,t7 t8i
R3: X → hs3,t9i
R4: S → hX,Xi
R5: S → hS X,S Xi
L(S, 1, 3) = L(S, 1, 3,4) ⊕ L(S, 1, 3,5)
L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3,1) =
= A(t1) ⊗ A(t2)
L(S, 1, 3, 5) = L(S, 1, 2) ⊗ L(X, 3, 1)
L(S, 1, 2) = L(S, 1, 2,4) = L(X, 1, 2) =
= L(X, 1, 2,2) = A(t7) ⊗ A(t8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)
L(S, 1, 3, 5) = A(t7) ⊗ A(t8) ⊗ A(t9)
L(S, 1, 3) = (A(t1) ⊗ A(t2)) ⊕ (A(t7) ⊗ A(t8) ⊗ A(t9))
</equation>
<figureCaption confidence="0.992585">
Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid
is represented here in two dimensions (x, y). In practice only the first column accepts both non-terminals (S, X). For
this reason it is divided in two subcolumns.
</figureCaption>
<bodyText confidence="0.999393666666667">
corresponding to the elements of αr = αr1...αr|αr|.
If an αri is a terminal, creating its lattice is straight-
forward. If αri is a non-terminal, it refers to a cell
</bodyText>
<equation confidence="0.880169875">
(N′, x′, y′) lower in the grid identified by the back-
pointer BP(N, x, y, r, i); in this case, the lattice
used is L(N′, x′, y′). Taken together,
�L(N, x, y, r) = L(N,x,y,r,i) (1)
i=1..|αr|
�
A(αi) if αi E T
L(N, x, y, r, i) = L(N′, x′, y′) else
</equation>
<bodyText confidence="0.9946986">
(2)
where A(t), t E T returns a single-arc accep-
tor which accepts only the symbol t. The lattice
L(N, x, y) is then built as the union of lattices cor-
responding to the rules in R(N, x, y):
</bodyText>
<equation confidence="0.991898">
L(N, x, y) = � L(N, x, y, r) (3)
rER(N,x,y)
</equation>
<bodyText confidence="0.999421">
Lattice union and concatenation are performed
using the ⊕ and ⊗ WFST operations respectively, as
described by Allauzen et al.(2007). If a rule Rr has
a cost cr, it is applied to the exit state of the lattice
L(N, x, y, r) prior to the operation of Equation 3.
</bodyText>
<subsectionHeader confidence="0.938293">
2.1.1 An Example of Phrase-based Translation
</subsectionHeader>
<bodyText confidence="0.9998675">
Figure 1 illustrates this process for a three word
source sentence s1s2s3 under monotone phrase-
based translation. The left-hand side shows the state
of the CYK grid after parsing using the rules R1 to
R5. These include 3 rules with only terminals (R1,
R2, R3) and the glue rules (R4, R5). Arrows repre-
sent backpointers to lower-level cells. We are inter-
ested in the upper-most S cell (S, 1, 3), as it repre-
sents the search space of translation hypotheses cov-
ering the whole source sentence. Two rules (R4, R5)
are in this cell, so the lattice L(S, 1, 3) will be ob-
tained by the union of the two lattices found by the
backpointers of these two rules. This process is ex-
plicitly derived in the right-hand side of Figure 1.
</bodyText>
<subsectionHeader confidence="0.932509">
2.1.2 An Example of Hierarchical Translation
</subsectionHeader>
<bodyText confidence="0.999863333333333">
Figure 2 shows a hierarchical scenario for the
same sentence. Three rules, R6, R7, R8, are added
to the example of Figure 1, thus providing two ad-
ditional derivations. This makes use of sublattices
already produced in the creation of L(S,1, 3, 5) and
L(X, 1, 3,1) in Figure 1; these are within {}.
</bodyText>
<subsectionHeader confidence="0.998549">
2.2 A Procedure for Lattice Construction
</subsectionHeader>
<bodyText confidence="0.999973785714286">
Figure 3 presents an algorithm to build the lattice
for every cell. The algorithm uses memoization: if
a lattice for a requested cell already exists, it is re-
turned (line 2); otherwise it is constructed via equa-
tions 1,2,3. For every rule, each element of the tar-
get side (lines 3,4) is checked as terminal or non-
terminal (equation 2). If it is a terminal element
(line 5), a simple acceptor is built. If it is a non-
terminal (line 6), the lattice associated to its back-
pointer is returned (lines 7 and 8). The complete
lattice L(N, x, y, r) for each rule is built by equa-
tion 1 (line 9). The lattice L(N, x, y) for this cell
is then found by union of all the component rules
(line 10, equation 3); this lattice is then reduced by
</bodyText>
<page confidence="0.945771">
435
</page>
<equation confidence="0.999801166666667">
R6: X → hs1,t20i
R7: X → hX1 s2 X2,X1 t10 X2i
R8: X → hX1 s2 X2,X2 t10 X1i
L(5,1, 3) = L(5,1, 3, 4) ⊕{L(5,1, 3, 5)}
L(5,1, 3,4) = L(X, 1, 3) =
={L(X,1, 3, 1)} ⊕L(X,1, 3, 7) ⊕ L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6) ⊗ A(t10) ⊗ L(X, 3, 1, 3) =
= A(t20) ⊗ A(t10) ⊗ A(t9)
L(X, 1, 3, 8) = A(t9) ⊗ A(t10) ⊗ A(t20)
L(5,1, 3) = {(A(t1) ⊗ A(t2))} ⊕
⊕(A(t20) ⊗ A(t10) ⊗ A(t9)) ⊕ (A(t9) ⊗ A(t10) ⊗ A(t20))⊕
⊕{(A(t7) ⊗ A(t8) ⊗ A(t9))}
</equation>
<figureCaption confidence="0.996892">
Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within 11.
</figureCaption>
<bodyText confidence="0.6372962">
standard WFST operations (lines 11,12,13). It is
important at this point to remove any epsilon arcs
which may have been introduced by the various
WFST union, concatenation, and replacement oper-
ations (Allauzen et al., 2007).
</bodyText>
<listItem confidence="0.942738071428572">
1 function buildFst(N,x,y)
2 if I G(N, x, y) return G(N, x, y)
3 for r E R(N, x, y), Rr : N —* (γ,α)
4 for i = 1...|α|
5 if αz E T, G(N, x, y, r, i) = A(αz)
6 else
7 (N′, x′, y′) = BP (αz)
8 G(N, x, y, r, i) = buildFst(N′, x′, y′)
9 G(N, x, y, r)=®z=1..jαj G(N, x, y, r, i)
10 G(N, x, y) = ®rER(N,x,y) G(N, x, y, r)
11 fstRmEpsilon G(N, x, y)
12 fstDeterminize G(N, x, y)
13 fstMinimize G(N, x, y)
14 return G(N, x, y)
</listItem>
<figureCaption confidence="0.927722">
Figure 3: Recursive Lattice Construction.
2.3 Delayed Translation
</figureCaption>
<bodyText confidence="0.9923676875">
Equation 2 leads to the recursive construction of lat-
tices in upper-levels of the grid through the union
and concatenation of lattices from lower levels. If
equations 1 and 3 are actually carried out over fully
expanded word lattices, the memory required by the
upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as
pointers to the low-level lattices. This effectively
builds a skeleton of the desired lattice and delays
the creation of the final word lattice until a single
replacement operation is carried out in the top cell
(5, 1, J). To make this exact, we define a function
g(N, x, y) which returns a unique tag for each lattice
in each cell, and use it to redefine equation 2. With
the backpointer (N′, x′, y′) = BP(N, x, y, r, i),
these special arcs are introduced as:
</bodyText>
<equation confidence="0.839935">
G(N, x, y, r, i) = αz) if
5 A(g(N′, xA(y′)) elsez E T
(4)
</equation>
<bodyText confidence="0.999964071428571">
The resulting lattices G(N, x, y) are a mix of tar-
get language words and lattice pointers (Figure 4,
top). However each still represents the entire search
space of all translation hypotheses covering the
span. Importantly, operations on these lattices –
such as lossless size reduction via determinization
and minimization – can still be performed. Owing
to the existence of multiple hierarchical rules which
share the same low-level dependencies, these opera-
tions can greatly reduce the size of the skeleton lat-
tice; Figure 4 shows the effect on the translation ex-
ample. This process is carried out for the lattice at
every cell, even at the lowest level where there are
only sequences of word terminals. As stated, size
reductions can be significant. However not all redu-
dancy is removed, since duplicate paths may arise
through the concatenation and union of sublattices
with different spans.
At the upper-most cell, the lattice G(5,1, J) con-
tains pointers to lower-level lattices. A single FST
replace operation (Allauzen et al., 2007) recursively
substitutes all pointers by their lower-level lattices
until no pointers are left, thus producing the com-
plete target word lattice for the whole source sen-
tence. The use of the lattice pointer arc was in-
spired by the ‘lazy evaluation’ techniques developed
by Mohri et al (2000). Its implementation uses the
infrastructure provided by the OpenFST libraries for
</bodyText>
<page confidence="0.998366">
436
</page>
<figureCaption confidence="0.995099333333333">
Figure 4: Delayed translation WFST with derivations
from Figure 1 and Figure 2 before [t] and after minimiza-
tion [b].
</figureCaption>
<bodyText confidence="0.871035">
delayed composition, etc.
</bodyText>
<subsectionHeader confidence="0.996162">
2.4 Pruning in Lattice Construction
</subsectionHeader>
<bodyText confidence="0.999901611111111">
The final translation lattice L(S,1, J) can grow very
large after the pointer arcs are expanded. We there-
fore apply a word-based language model, via WFST
composition, and perform likelihood-based prun-
ing (Allauzen et al., 2007) based on the combined
translation and language model scores.
Pruning can also be performed on sublattices
during search. One simple strategy is to monitor
the number of states in the determinized lattices
L(N, x, y). If this number is above a threshold, we
expand any pointer arcs and apply a word-based lan-
guage model via composition. The resulting lattice
is then reduced by likelihood-based pruning, after
which the LM scores are removed. This search prun-
ing can be very selective. For example, the pruning
threshold can depend on the height of the cell in the
grid. In this way the risk of search errors can be
controlled.
</bodyText>
<sectionHeader confidence="0.993848" genericHeader="method">
3 Translation Experiments
</sectionHeader>
<bodyText confidence="0.999483538461539">
We report experiments on the NIST MT08 Arabic-
to-English and Chinese-to-English translation tasks.
We contrast two hierarchical phrase-based decoders.
The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as de-
scribed by Chiang (2007). In our implementation, k-
best lists contain unique hypotheses. The second de-
coder, Hiero FST (HiFST), is a lattice-based decoder
implemented with Weighted Finite State Transduc-
ers as described in the previous section. Hypotheses
are generated after determinization under the trop-
ical semiring so that scores assigned to hypotheses
arise from single minimum cost / maximum likeli-
hood derivations. We also use a variant of the k-best
decoder which works in alignment mode: given an
input k-best list, it outputs the feature scores of each
hypothesis in the list without applying any pruning.
This is used for Minimum Error Training (MET)
with the HiFST system.
These two language pairs pose very different
translation challenges. For example, Chinese-
to-English translation requires much greater word
movement than Arabic-to-English. In the frame-
work of hierarchical translation systems, we have
found that shallow decoding (see section 3.2) is
as good as full hierarchical decoding in Arabic-
to-English (Iglesias et al., 2009). In Chinese-to-
English, we have not found this to be the case.
Therefore, we contrast the performance of HiFST
and HCP under shallow hierarchical decoding for
Arabic-to-English, while for Chinese-to-English we
perform full hierarchical decoding.
Both hierarchical translation systems share a
common architecture. For both language pairs,
alignments are generated over the parallel data. The
following features are extracted and used in trans-
lation: target language model, source-to-target and
target-to-source phrase translation models, word and
rule penalties, number of usages of the glue rule,
source-to-target and target-to-source lexical models,
and three rule count features inspired by Bender et
al. (2007). The initial English language model is
a 4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. Details of the par-
allel corpus and development sets used for each lan-
guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter
estimation under IBM BLEU (Papineni et al., 2001)
is performed on the corresponding development set.
For the HCP system, MET is done following Chi-
ang (2007). For the HiFST system, we obtain a k-
</bodyText>
<figure confidence="0.9876446875">
t2
1
t1
g(X,1,2)
g(X,3,1)
2
0
g(X,3,1)
g(X,1,1)
3
t10
g(X,3,1)
4 g(X,1,1)
7
5
t10
6
g(X,1,1)
3
t10
g(X,1,2) 2
g(X,3,1)
0
t1
1
t2
6
g(X,3,1)
g(X,1,1)
4
t10
5
</figure>
<page confidence="0.99686">
437
</page>
<bodyText confidence="0.9992552">
best list from the translation lattice and extract each
feature score with the aligner variant of the k-best
decoder. After translation with optimized feature
weights, we carry out the two following rescoring
steps.
</bodyText>
<listItem confidence="0.952949285714286">
• Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using —4.7B words of English newswire text,
and apply them to rescore either 10000-best
lists generated by HCP or word lattices gener-
ated by HiFST. Lattices provide a vast search
space relative to k-best lists, with translation
lattice sizes of 1081 hypotheses reported in the
literature (Tromble et al., 2008).
• Minimum Bayes Risk (MBR). We rescore the
first 1000-best hypotheses with MBR, taking
the negative sentence level BLEU score as the
loss function (Kumar and Byrne, 2004).
</listItem>
<subsectionHeader confidence="0.999578">
3.1 Building the Rule Sets
</subsectionHeader>
<bodyText confidence="0.950476916666667">
We extract hierarchical phrases from word align-
ments, applying the same restrictions as introduced
by Chiang (2005). Additionally, following Iglesias
et al. (2009) we carry out two rule filtering strate-
gies:
• we exclude rules with two non-terminals with
the same order on the source and target side
• we consider only the 20 most frequent transla-
tions for each rule
For each development set, this produces approx-
imately 4.3M rules in Arabic-to-English and 2.0M
rules in Chinese-to-English.
</bodyText>
<subsectionHeader confidence="0.997105">
3.2 Arabic-to-English Translation
</subsectionHeader>
<bodyText confidence="0.9993725">
We translate Arabic-to-English with shallow hierar-
chical decoding, i.e. only phrases are allowed to be
substituted into non-terminals. The rules used in this
case are, in addition to the glue rules:
</bodyText>
<equation confidence="0.98936275">
X —* (ys,αs)
X —* (V ,V )
V —* (s,t)
s, t E T+; ys, αs E ({V } U T)+
</equation>
<bodyText confidence="0.9999245">
For translation model training, we use all allowed
parallel corpora in the NIST MT08 Arabic track
(—150M words per language). In addition to the
MT08 set itself, we use a development set mt02-05-
tune formed from the odd numbered sentences of the
NIST MT02 through MT05 evaluation sets; the even
numbered sentences form the validation set mt02-
05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best
lists of depth k=10000 (unique). Using deeper lists
results in excessive memory and time requirements.
In contrast, the WFST-based decoder, HiFST, re-
quires no local pruning during lattice construction
for this task and the language model is not applied
until the lattice is fully built at the upper-most cell of
the CYK grid.
Table 1 shows results for mt02-05-tune, mt02-
05-test and mt08, as measured by lowercased IBM
BLEU and TER (Snover et al., 2006). MET param-
eters are optimized for the HCP decoder. As shown
in rows ‘a’ and ‘b’, results after MET are compara-
ble.
Search Errors Since both decoders use exactly the
same features, we can measure their search errors on
a sentence-by-sentence basis. A search error is as-
signed to one of the decoders if the other has found
a hypothesis with lower cost. For mt02-05-tune, we
find that in 18.5% of the sentences HiFST finds a hy-
pothesis with lower cost than HCP. In contrast, HCP
never finds any hypothesis with lower cost for any
sentence. This is as expected: the HiFST decoder
requires no pruning prior to applying the language
model, so search is exact.
Lattice/k-best Quality Rescoring results are dif-
ferent for cube pruning and WFST-based decoders.
Whereas HCP improves by 0.9 BLEU, HiFST im-
proves over 1.5 BLEU. Clearly, search errors in HCP
not only affect the 1-best output but also the quality
of the resulting k-best lists. For HCP, this limits the
possible gain from subsequent rescoring steps such
as large LMs and MBR.
Translation Speed HCP requires an average of 1.1
seconds per input word. HiFST cuts this time by
half, producing output at a rate of 0.5 seconds per
word. It proves much more efficient to process com-
pact lattices contaning many hypotheses rather than
to independently processing each one of them in k-
best form.
</bodyText>
<page confidence="0.996499">
438
</page>
<table confidence="0.999118888888889">
decoder mt02-05 -tune mt02-05 -test mt08 TER
BLEU TER BLEU TER BLEU
a HCP 52.2 41.6 51.5 42.2 42.5 48.6
+5gram 53.1 41.0 52.5 41.5 43.3 48.3
+MBR 53.2 40.8 52.6 41.4 43.4 48.1
b HiFST 52.2 41.5 51.6 42.1 42.4 48.7
+5gram 53.3 40.6 52.7 41.3 43.7 48.1
+MBR 53.7 40.4 53.3 40.9 44.0 48.0
Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.
</table>
<tableCaption confidence="0.9842225">
Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU I TER) after MET and subsequent
rescoring steps. Decoding time reported for mt02-05-tune.
</tableCaption>
<bodyText confidence="0.9994995">
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 42.9. This is directly comparable to
the official MT08 Constrained Training Track eval-
uation results1.
</bodyText>
<subsectionHeader confidence="0.99642">
3.3 Chinese-to-English Translation
</subsectionHeader>
<bodyText confidence="0.999985">
We translate Chinese-to-English with full hierarchi-
cal decoding, i.e. hierarchical rules are allowed to be
substituted into non-terminals. We consider a maxi-
mum span of 10 words for the application of hierar-
chical rules and only glue rules are allowed at upper
levels of the CYK grid.
For translation model training, we use all avail-
able data for the GALE 2008 evaluation2, approx.
250M words per language. In addition to the MT08
set itself, we use a development set tune-nw and
a validation set test-nw. These contain a mix of
the newswire portions of MT02 through MT05 and
additional developments sets created by translation
within the GALE program. The tune-nw set has
1,755 sentences.
Again, the HCP decoder employs k-best lists of
depth k=10000. The HiFST decoder applies prun-
ing in search as described in Section 2.4, so that any
lattice in the CYK grid is pruned if it covers at least
3 source words and contains more than 10k states.
The likelihood pruning threshold relative to the best
path in the lattice is 9. This is a very broad threshold
so that very few paths are discarded.
</bodyText>
<footnote confidence="0.999920666666667">
1Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is
worth noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
</footnote>
<bodyText confidence="0.999278575757576">
Improved Optimization Table 2 shows results for
tune-nw, test-nw and mt08, as measured by lower-
cased IBM BLEU and TER. The first two rows show
results for HCP when using MET parameters opti-
mized over k-best lists produced by HCP (row ‘a’)
and by HiFST (row ‘b’). We find that using the k-
best list obtained by the HiFST decoder yields bet-
ter parameters during optimization. Tuning on the
HiFST k-best lists improves the HCP BLEU score,
as well. We find consistent improvements in BLEU;
TER also improves overall, although less consis-
tently.
Search Errors Measured over the tune-nw devel-
opment set, HiFST finds a hypothesis with lower
cost in 48.4% of the sentences. In contrast, HCP
never finds any hypothesis with a lower cost for any
sentence, indicating that the described pruning strat-
egy for HiFST is much broader than that of HCP.
Note that HCP search errors are more frequent for
this language pair. This is due to the larger search
space required in fully hierarchical translation; the
larger the search space, the more search errors will
be produced by the cube pruning k-best implemen-
tation.
Lattice/k-best Quality The lattices produced by
HiFST yield greater gains in LM rescoring than the
k-best lists produced by HCP. Including the subse-
quent MBR rescoring, translation improves as much
as 1.2 BLEU, compared to 0.7 BLEU with HCP.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 27.8, comparable to official results
in the UnConstrained Training Track of the NIST
2008 evaluation.
</bodyText>
<page confidence="0.997111">
439
</page>
<table confidence="0.999735888888889">
decoder MET k-best tune-nw TER test-nw TER mt08 TER
BLEU BLEU BLEU
a HCP HCP 31.6 59.7 31.9 59.7 – –
b HCP 31.7 60.0 32.2 59.9 27.2 60.2
+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3
+MBR 32.4 59.2 32.7 59.4 28.1 59.3
c HiFST 32.0 60.1 32.2 60.0 27.1 60.5
+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1
+MBR 32.9 58.4 33.4 58.5 28.9 58.9
</table>
<tableCaption confidence="0.9878285">
Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU1TER) after MET and subsequent
rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.
</tableCaption>
<sectionHeader confidence="0.999114" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999991238095238">
The lattice-based decoder for hierarchical phrase-
based translation described in this paper can be eas-
ily implemented using Weighted Finite State Trans-
ducers. We find many benefits in this approach
to translation. From a practical perspective, the
computational operations required can be easily car-
ried out using standard operations already imple-
mented in general purpose libraries. From a model-
ing perspective, the compact representation of mul-
tiple translation hypotheses in lattice form requires
less pruning in hierarchical search. The result is
fewer search errors and reduced overall memory use
relative to cube pruning over k-best lists. We also
find improved performance of subsequent rescor-
ing procedures which rely on the translation scores.
In direct comparison to k-best lists generated un-
der cube pruning, we find that MET parameter opti-
mization, rescoring with large language models, and
MBR decoding, are all improved when applied to
translations generated by the lattice-based hierarchi-
cal decoder.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999737833333333">
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government research
grant BES-2007-15956 (project TEC2006-13694-
C03-03).
</bodyText>
<sectionHeader confidence="0.998322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999549722222222">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL, pages 557–
564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11–23.
Srinivas Bangalore and Giuseppe Riccardi. 2001. A
finite-state approach to machine translation. In Pro-
ceedings ofNAACL.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of ASRU, pages 396–
401.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200–208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858–867.
Francisco Casacuberta. 2001. Finite-state transducers
for speech-input translation. In Proceedings ofASRU.
Jean-C´edric Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133–137.
Jean-C´edric Chappelier, Martin Rajman, Ram´on
Arag¨u´es, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95–104.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263–270.
</reference>
<page confidence="0.974628">
440
</page>
<reference confidence="0.999949666666667">
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings ofACL-HLT, pages 1012–1020.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391–427.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings ofACL, pages 144–151.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings ofHLT-NAACL, pages 169–176.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings ofHLT-EMNLP, pages 161–168.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the ACL-HLT Second Workshop on Syntax
and Structure in Statistical Translation, pages 10–18.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505–512.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings ofACL-HLT, pages 1003–1011.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
ofICASSP.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231:17–32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69–88.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings ofACL,
pages 311–318.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of HLT-NAACL,
pages 228–235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-HLT, pages 577–585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP, vol-
ume 4, pages 105–108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAMTA, pages 223–231.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620–629.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings ofHLT-NAACL, pages 500–507.
Hao Zhang and Daniel Gildea. 2006. Synchronous bi-
narization for machine translation. In Proceedings of
HLT-NAACL, pages 256–263.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation, pages 138–141.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145–1152.
</reference>
<page confidence="0.998696">
441
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411145">
<title confidence="0.997749">Hierarchical Phrase-Based Translation Weighted Finite State Transducers</title>
<author confidence="0.951655">Adri`a de_R William</author>
<affiliation confidence="0.467471">of Vigo. Dept. of Signal Processing and Communications. Vigo,</affiliation>
<address confidence="0.950848">of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge,</address>
<abstract confidence="0.999522666666667">This paper describes a lattice-based decoder for hierarchical phrase-based translation. The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure. We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="7538" citStr="Allauzen et al., 2003" startWordPosition="1146" endWordPosition="1149">struction proceeds by traversing the CYK grid along the backpointers established in parsing. In each cell (N, x, y) in the CYK grid, we build a target language word lattice L(N, x, y). This lattice contains every translation of sx+y−1 from every x derivation headed by N. These lattices also contain the translation scores on their arc weights. The ultimate objective is the word lattice L(S,1, J) which corresponds to all the analyses that cover the source sentence sJ1 . Once this is built, we can apply a target language model to L(S,1, J) to obtain the final target language translation lattice (Allauzen et al., 2003). We use the approach of Mohri (2002) in applying WFSTs to statistical NLP. This fits well with the use of the OpenFST toolkit (Allauzen et al., 2007) to implement our decoder. 2.1 Lattice Construction Over the CYK Grid In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), i.e. for r E R(N, x, y), N -* (&apos;yr,αr) was used in at least one derivation involving that cell. For each rule Rr, r E R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived from the target side of the rule αr by concatenating lattices 434 R1: X → hs1 s2 s3,t1 t2i R2: X → hs1</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of ACL, pages 557– 564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="2314" citStr="Allauzen et al., 2007" startWordPosition="331" endWordPosition="334">ghted Finite State Transducers (WFSTs). In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that cell. When derivations contain non-terminals, we use pointers to lowerlevel lattices for memory efficiency. The pointers are only expanded to the actual translations if pruning is required during search; expansion is otherwise only carried out at the upper-most cell, after the full CYK grid has been traversed. We describe how this decoder can be easily implemented with WFSTs. For this we employ the OpenFST libraries (Allauzen et al., 2007). Using standard FST operations such as composition, epsilon removal, determinization, minimization and shortest-path, we find this search procedure to be simpler to implement than cube pruning. The main modeling advantages are a significant reduction in search errors, a simpler implementation, direct generation of target language word lattices, and better integration with other statistical MT procedures. We report translation results in Arabic-to-English and Chinese-to-English translation and contrast the performance of lattice-based and cube pruning hierarchical decoding. 1.1 Related Work Hi</context>
<context position="7688" citStr="Allauzen et al., 2007" startWordPosition="1173" endWordPosition="1176"> language word lattice L(N, x, y). This lattice contains every translation of sx+y−1 from every x derivation headed by N. These lattices also contain the translation scores on their arc weights. The ultimate objective is the word lattice L(S,1, J) which corresponds to all the analyses that cover the source sentence sJ1 . Once this is built, we can apply a target language model to L(S,1, J) to obtain the final target language translation lattice (Allauzen et al., 2003). We use the approach of Mohri (2002) in applying WFSTs to statistical NLP. This fits well with the use of the OpenFST toolkit (Allauzen et al., 2007) to implement our decoder. 2.1 Lattice Construction Over the CYK Grid In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), i.e. for r E R(N, x, y), N -* (&apos;yr,αr) was used in at least one derivation involving that cell. For each rule Rr, r E R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived from the target side of the rule αr by concatenating lattices 434 R1: X → hs1 s2 s3,t1 t2i R2: X → hs1 s2,t7 t8i R3: X → hs3,t9i R4: S → hX,Xi R5: S → hS X,S Xi L(S, 1, 3) = L(S, 1, 3,4) ⊕ L(S, 1, 3,5) L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3,1) = = A(t1</context>
<context position="12377" citStr="Allauzen et al., 2007" startWordPosition="2109" endWordPosition="2112">X,1, 3, 1)} ⊕L(X,1, 3, 7) ⊕ L(X, 1, 3, 8) L(X, 1, 3, 7) = L(X, 1, 1, 6) ⊗ A(t10) ⊗ L(X, 3, 1, 3) = = A(t20) ⊗ A(t10) ⊗ A(t9) L(X, 1, 3, 8) = A(t9) ⊗ A(t10) ⊗ A(t20) L(5,1, 3) = {(A(t1) ⊗ A(t2))} ⊕ ⊕(A(t20) ⊗ A(t10) ⊗ A(t9)) ⊕ (A(t9) ⊗ A(t10) ⊗ A(t20))⊕ ⊕{(A(t7) ⊗ A(t8) ⊗ A(t9))} Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within 11. standard WFST operations (lines 11,12,13). It is important at this point to remove any epsilon arcs which may have been introduced by the various WFST union, concatenation, and replacement operations (Allauzen et al., 2007). 1 function buildFst(N,x,y) 2 if I G(N, x, y) return G(N, x, y) 3 for r E R(N, x, y), Rr : N —* (γ,α) 4 for i = 1...|α| 5 if αz E T, G(N, x, y, r, i) = A(αz) 6 else 7 (N′, x′, y′) = BP (αz) 8 G(N, x, y, r, i) = buildFst(N′, x′, y′) 9 G(N, x, y, r)=®z=1..jαj G(N, x, y, r, i) 10 G(N, x, y) = ®rER(N,x,y) G(N, x, y, r) 11 fstRmEpsilon G(N, x, y) 12 fstDeterminize G(N, x, y) 13 fstMinimize G(N, x, y) 14 return G(N, x, y) Figure 3: Recursive Lattice Construction. 2.3 Delayed Translation Equation 2 leads to the recursive construction of lattices in upper-levels of the grid through the union and conc</context>
<context position="14777" citStr="Allauzen et al., 2007" startWordPosition="2545" endWordPosition="2548">e same low-level dependencies, these operations can greatly reduce the size of the skeleton lattice; Figure 4 shows the effect on the translation example. This process is carried out for the lattice at every cell, even at the lowest level where there are only sequences of word terminals. As stated, size reductions can be significant. However not all redudancy is removed, since duplicate paths may arise through the concatenation and union of sublattices with different spans. At the upper-most cell, the lattice G(5,1, J) contains pointers to lower-level lattices. A single FST replace operation (Allauzen et al., 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence. The use of the lattice pointer arc was inspired by the ‘lazy evaluation’ techniques developed by Mohri et al (2000). Its implementation uses the infrastructure provided by the OpenFST libraries for 436 Figure 4: Delayed translation WFST with derivations from Figure 1 and Figure 2 before [t] and after minimization [b]. delayed composition, etc. 2.4 Pruning in Lattice Construction The final translation lattice L(S,1, J) can</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>A finite-state approach to machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="5163" citStr="Bangalore and Riccardi, 2001" startWordPosition="738" endWordPosition="741">d Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK a</context>
</contexts>
<marker>Bangalore, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore and Giuseppe Riccardi. 2001. A finite-state approach to machine translation. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Evgeny Matusov</author>
<author>Stefan Hahn</author>
<author>Sasa Hasan</author>
<author>Shahram Khadivi</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Arabic-to-English spoken language translation system.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU,</booktitle>
<pages>396--401</pages>
<contexts>
<context position="18226" citStr="Bender et al. (2007)" startWordPosition="3077" endWordPosition="3080">formance of HiFST and HCP under shallow hierarchical decoding for Arabic-to-English, while for Chinese-to-English we perform full hierarchical decoding. Both hierarchical translation systems share a common architecture. For both language pairs, alignments are generated over the parallel data. The following features are extracted and used in translation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kt2 1 t1 g(X,1,2) g(X,3,1) 2 0 g(X,3,1) g(X,1,1) 3 t10 g(X,3,1) 4 g(X,1</context>
</contexts>
<marker>Bender, Matusov, Hahn, Hasan, Khadivi, Ney, 2007</marker>
<rawString>Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa Hasan, Shahram Khadivi, and Hermann Ney. 2007. The RWTH Arabic-to-English spoken language translation system. In Proceedings of ASRU, pages 396– 401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="4522" citStr="Blunsom et al., 2008" startWordPosition="650" endWordPosition="653">as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL-HLT, pages 200–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="19229" citStr="Brants et al., 2007" startWordPosition="3247" endWordPosition="3250">01) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kt2 1 t1 g(X,1,2) g(X,3,1) 2 0 g(X,3,1) g(X,1,1) 3 t10 g(X,3,1) 4 g(X,1,1) 7 5 t10 6 g(X,1,1) 3 t10 g(X,1,2) 2 g(X,3,1) 0 t1 1 t2 6 g(X,3,1) g(X,1,1) 4 t10 5 437 best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using —4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments, </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP-ACL, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
</authors>
<title>Finite-state transducers for speech-input translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofASRU.</booktitle>
<contexts>
<context position="5182" citStr="Casacuberta, 2001" startWordPosition="742" endWordPosition="744">man et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely re</context>
</contexts>
<marker>Casacuberta, 2001</marker>
<rawString>Francisco Casacuberta. 2001. Finite-state transducers for speech-input translation. In Proceedings ofASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of TAPD,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="5825" citStr="Chappelier and Rajman, 1998" startWordPosition="837" endWordPosition="841">ne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998). Parsing follows the description of Chiang (2005; 2007), maintaining backpointers and employing hypothesis recombination without pruning. The underlying model is a synchronous context-free grammar consisting of a set R = {Rr} of rules Rr : N -* (&apos;yr,αr) / pr, with ‘glue’ rules, S -* (X,X) and S -* (S X,S X). If a rule has probability pr, it is transformed to a cost cr; here we use the tropical semiring, so cr = − log pr. N denotes a non-terminal; in this paper, N can be either S, X, or V (see section 3.2). T denotes the terminals (words), and the grammar builds parses based on strings &apos;y, α E</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-C´edric Chappelier and Martin Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proceedings of TAPD, pages 133–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
<author>Ram´on Arag¨u´es</author>
<author>Antoine Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of TALN,</booktitle>
<pages>95--104</pages>
<marker>Chappelier, Rajman, Arag¨u´es, Rozenknop, 1999</marker>
<rawString>Jean-C´edric Chappelier, Martin Rajman, Ram´on Arag¨u´es, and Antoine Rozenknop. 1999. Lattice parsing for speech recognition. In Proceedings of TALN, pages 95–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1272" citStr="Chiang, 2005" startWordPosition="162" endWordPosition="163">translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning. 1 Introduction Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing (Chiang, 2005). Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007). While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation. We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs). In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that c</context>
<context position="5874" citStr="Chiang (2005" startWordPosition="848" endWordPosition="849">nowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998). Parsing follows the description of Chiang (2005; 2007), maintaining backpointers and employing hypothesis recombination without pruning. The underlying model is a synchronous context-free grammar consisting of a set R = {Rr} of rules Rr : N -* (&apos;yr,αr) / pr, with ‘glue’ rules, S -* (X,X) and S -* (S X,S X). If a rule has probability pr, it is transformed to a cost cr; here we use the tropical semiring, so cr = − log pr. N denotes a non-terminal; in this paper, N can be either S, X, or V (see section 3.2). T denotes the terminals (words), and the grammar builds parses based on strings &apos;y, α E {{S, X, V } U T}+. Each cell in the CYK grid is </context>
<context position="19890" citStr="Chiang (2005)" startWordPosition="3353" endWordPosition="3354">ords of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies: • we exclude rules with two non-terminals with the same order on the source and target side • we consider only the 20 most frequent translations for each rule For each development set, this produces approximately 4.3M rules in Arabic-to-English and 2.0M rules in Chinese-to-English. 3.2 Arabic-to-English Translation We translate Arabic-to-English with shallow hierarchical decoding, i.e. only phrases are allowed to be substituted into non-terminals. The rules used in this case are, in addition to the glu</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1443" citStr="Chiang, 2007" startWordPosition="193" endWordPosition="194">tion performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning. 1 Introduction Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing (Chiang, 2005). Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007). While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation. We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs). In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that cell. When derivations contain non-terminals, we use pointers to lowerlevel lattices for memory efficiency. The pointers are only expanded to the actual translations if pru</context>
<context position="3649" citStr="Chiang (2007)" startWordPosition="519" endWordPosition="520">on. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. We summarize some extensions to the basic approach to put our work in context. 433 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to </context>
<context position="16489" citStr="Chiang (2007)" startWordPosition="2822" endWordPosition="2823">odel via composition. The resulting lattice is then reduced by likelihood-based pruning, after which the LM scores are removed. This search pruning can be very selective. For example, the pruning threshold can depend on the height of the cell in the grid. In this way the risk of search errors can be controlled. 3 Translation Experiments We report experiments on the NIST MT08 Arabicto-English and Chinese-to-English translation tasks. We contrast two hierarchical phrase-based decoders. The first decoder, Hiero Cube Pruning (HCP), is a kbest decoder using cube pruning implemented as described by Chiang (2007). In our implementation, kbest lists contain unique hypotheses. The second decoder, Hiero FST (HiFST), is a lattice-based decoder implemented with Weighted Finite State Transducers as described in the previous section. Hypotheses are generated after determinization under the tropical semiring so that scores assigned to hypotheses arise from single minimum cost / maximum likelihood derivations. We also use a variant of the k-best decoder which works in alignment mode: given an input k-best list, it outputs the feature scores of each hypothesis in the list without applying any pruning. This is u</context>
<context position="18719" citStr="Chiang (2007)" startWordPosition="3159" endWordPosition="3161">le, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kt2 1 t1 g(X,1,2) g(X,3,1) 2 0 g(X,3,1) g(X,1,1) 3 t10 g(X,3,1) 4 g(X,1,1) 7 5 t10 6 g(X,1,1) 3 t10 g(X,1,2) 2 g(X,3,1) 0 t1 1 t2 6 g(X,3,1) g(X,1,1) 4 t10 5 437 best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using —4.7B words of English newswire text, and apply t</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="4122" citStr="Dyer et al. (2008)" startWordPosition="590" endWordPosition="593">L, pages 433–441, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) ex</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings ofACL-HLT, pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="5252" citStr="Graehl et al., 2008" startWordPosition="753" endWordPosition="756">mented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998). Parsing follows the descr</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Computational Linguistics, 34(3):391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="3649" citStr="Huang and Chiang (2007)" startWordPosition="517" endWordPosition="520"> translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. We summarize some extensions to the basic approach to put our work in context. 433 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings ofACL, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="19746" citStr="Kumar and Byrne, 2004" startWordPosition="3329" endWordPosition="3332">g steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using —4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies: • we exclude rules with two non-terminals with the same order on the source and target side • we consider only the 20 most frequent translations for each rule For each development set, this produces approximately 4.3M rules in Arabic-to-English and 2.0M rules in Chinese-to-English. 3.2 Arabic-to-English Translation We translate Arabic-to-English with shallow hi</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings ofHLT-NAACL, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT-EMNLP,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="5205" citStr="Kumar and Byrne, 2005" startWordPosition="745" endWordPosition="748">ompare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappeli</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings ofHLT-EMNLP, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-HLT Second Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="3955" citStr="Li and Khudanpur (2008)" startWordPosition="563" endWordPosition="567">marize some extensions to the basic approach to put our work in context. 433 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experi</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings of the ACL-HLT Second Workshop on Syntax and Structure in Statistical Translation, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="4719" citStr="Lopez (2008)" startWordPosition="678" endWordPosition="679">r et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hi</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-scale translation models via pattern matching. In Proceedings of COLING, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="4447" citStr="Marton and Resnik, 2008" startWordPosition="638" endWordPosition="641">on during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation The</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings ofACL-HLT, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lambert Mathias</author>
<author>William Byrne</author>
</authors>
<title>Statistical phrase-based speech translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="5230" citStr="Mathias and Byrne, 2006" startWordPosition="749" endWordPosition="752">erarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998). Par</context>
</contexts>
<marker>Mathias, Byrne, 2006</marker>
<rawString>Lambert Mathias and William Byrne. 2006. Statistical phrase-based speech translation. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>The design principles of a weighted finitestate transducer library.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<pages>231--17</pages>
<contexts>
<context position="15067" citStr="Mohri et al (2000)" startWordPosition="2593" endWordPosition="2596">ated, size reductions can be significant. However not all redudancy is removed, since duplicate paths may arise through the concatenation and union of sublattices with different spans. At the upper-most cell, the lattice G(5,1, J) contains pointers to lower-level lattices. A single FST replace operation (Allauzen et al., 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence. The use of the lattice pointer arc was inspired by the ‘lazy evaluation’ techniques developed by Mohri et al (2000). Its implementation uses the infrastructure provided by the OpenFST libraries for 436 Figure 4: Delayed translation WFST with derivations from Figure 1 and Figure 2 before [t] and after minimization [b]. delayed composition, etc. 2.4 Pruning in Lattice Construction The final translation lattice L(S,1, J) can grow very large after the pointer arcs are expanded. We therefore apply a word-based language model, via WFST composition, and perform likelihood-based pruning (Allauzen et al., 2007) based on the combined translation and language model scores. Pruning can also be performed on sublattices</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2000. The design principles of a weighted finitestate transducer library. Theoretical Computer Science, 231:17–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<booktitle>In Computer Speech and Language,</booktitle>
<volume>16</volume>
<pages>69--88</pages>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. In Computer Speech and Language, volume 16, pages 69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="18542" citStr="Och, 2003" startWordPosition="3132" endWordPosition="3133">cted and used in translation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kt2 1 t1 g(X,1,2) g(X,3,1) 2 0 g(X,3,1) g(X,1,1) 3 t10 g(X,3,1) 4 g(X,1,1) 7 5 t10 6 g(X,1,1) 3 t10 g(X,1,2) 2 g(X,3,1) 0 t1 1 t2 6 g(X,3,1) g(X,1,1) 4 t10 5 437 best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-L</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="18612" citStr="Papineni et al., 2001" startWordPosition="3140" endWordPosition="3143">e-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kt2 1 t1 g(X,1,2) g(X,3,1) 2 0 g(X,3,1) g(X,1,1) 3 t10 g(X,3,1) 4 g(X,1,1) 7 5 t10 6 g(X,1,1) 3 t10 g(X,1,2) 2 g(X,3,1) 0 t1 1 t2 6 g(X,3,1) g(X,1,1) 4 t10 5 437 best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Bra</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="5020" citStr="Rosti et al., 2007" startWordPosition="718" endWordPosition="721">6; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish a</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie Dorr. 2007. Combining outputs from multiple machine translation systems. In Proceedings of HLT-NAACL, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="4421" citStr="Shen et al., 2008" startWordPosition="634" endWordPosition="637">othesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007).</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William Byrne</author>
<author>Mark Gales</author>
<author>Hichem Sahbi</author>
<author>Phil Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<volume>4</volume>
<pages>105--108</pages>
<contexts>
<context position="4999" citStr="Sim et al., 2007" startWordPosition="714" endWordPosition="717">ng and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments f</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William Byrne, Mark Gales, Hichem Sahbi, and Phil Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proceedings of ICASSP, volume 4, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="21464" citStr="Snover et al., 2006" startWordPosition="3614" endWordPosition="3617">tion sets; the even numbered sentences form the validation set mt02- 05-test. The mt02-05-tune set has 2,075 sentences. The cube pruning decoder, HCP, employs k-best lists of depth k=10000 (unique). Using deeper lists results in excessive memory and time requirements. In contrast, the WFST-based decoder, HiFST, requires no local pruning during lattice construction for this task and the language model is not applied until the lattice is fully built at the upper-most cell of the CYK grid. Table 1 shows results for mt02-05-tune, mt02- 05-test and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). MET parameters are optimized for the HCP decoder. As shown in rows ‘a’ and ‘b’, results after MET are comparable. Search Errors Since both decoders use exactly the same features, we can measure their search errors on a sentence-by-sentence basis. A search error is assigned to one of the decoders if the other has found a hypothesis with lower cost. For mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any sentence. This is as expected: the HiFST decoder requires no pruning pri</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="19574" citStr="Tromble et al., 2008" startWordPosition="3301" endWordPosition="3304">ce and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using —4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies: • we exclude rules with two non-terminals with the same order on the source and target side • we consider only the 20 most frequent translations for each rule For each development set, this p</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proceedings of EMNLP, pages 620–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Vogel Stephan</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>500--507</pages>
<contexts>
<context position="3745" citStr="Venugopal et al. (2007)" startWordPosition="531" endWordPosition="534">ation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. We summarize some extensions to the basic approach to put our work in context. 433 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic informat</context>
</contexts>
<marker>Venugopal, Zollmann, Stephan, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Vogel Stephan. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In Proceedings ofHLT-NAACL, pages 500–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="4402" citStr="Zhang and Gildea, 2006" startWordPosition="630" endWordPosition="633">axed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; R</context>
</contexts>
<marker>Zhang, Gildea, 2006</marker>
<rawString>Hao Zhang and Daniel Gildea. 2006. Synchronous binarization for machine translation. In Proceedings of HLT-NAACL, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="4378" citStr="Zollmann and Venugopal, 2006" startWordPosition="626" endWordPosition="629">oduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of NAACL Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1145--1152</pages>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay Ponte. 2008. A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT. In Proceedings of COLING, pages 1145–1152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>