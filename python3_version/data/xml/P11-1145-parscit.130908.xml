<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031166">
<title confidence="0.998585">
A Bayesian Model for Unsupervised Semantic Parsing
</title>
<author confidence="0.998905">
Ivan Titov Alexandre Klementiev
</author>
<affiliation confidence="0.999548">
Saarland University Johns Hopkins University
</affiliation>
<address confidence="0.535714">
Saarbruecken, Germany Baltimore, MD, USA
</address>
<email confidence="0.996879">
titov@mmci.uni-saarland.de aklement@jhu.edu
</email>
<sectionHeader confidence="0.995603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999584857142857">
We propose a non-parametric Bayesian model
for unsupervised semantic parsing. Follow-
ing Poon and Domingos (2009), we consider
a semantic parsing setting where the goal is to
(1) decompose the syntactic dependency tree
of a sentence into fragments, (2) assign each
of these fragments to a cluster of semanti-
cally equivalent syntactic structures, and (3)
predict predicate-argument relations between
the fragments. We use hierarchical Pitman-
Yor processes to model statistical dependen-
cies between meaning representations of pred-
icates and those of their arguments, as well
as the clusters of their syntactic realizations.
We develop a modification of the Metropolis-
Hastings split-merge sampler, resulting in an
efficient inference algorithm for the model.
The method is experimentally evaluated by us-
ing the induced semantic representation for
the question answering task in the biomedical
domain.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999162822222222">
Statistical approaches to semantic parsing have re-
cently received considerable attention. While some
methods focus on predicting a complete formal rep-
resentation of meaning (Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Mooney, 2007), others
consider more shallow forms of representation (Car-
reras and M`arquez, 2005; Liang et al., 2009). How-
ever, most of this research has concentrated on su-
pervised methods requiring large amounts of labeled
data. Such annotated resources are scarce, expensive
to create and even the largest of them tend to have
low coverage (Palmer and Sporleder, 2010), moti-
vating the need for unsupervised or semi-supervised
techniques.
Conversely, research in the closely related task
of relation extraction has focused on unsupervised
or minimally supervised methods (see, for example,
(Lin and Pantel, 2001; Yates and Etzioni, 2009)).
These approaches cluster semantically equivalent
verbalizations of relations, often relying on syn-
tactic fragments as features for relation extraction
and clustering (Lin and Pantel, 2001; Banko et al.,
2007). The success of these methods suggests that
semantic parsing can also be tackled as clustering
of syntactic realizations of predicate-argument rela-
tions. While a similar direction has been previously
explored in (Swier and Stevenson, 2004; Abend et
al., 2009; Lang and Lapata, 2010), the recent work
of (Poon and Domingos, 2009) takes it one step
further by not only predicting predicate-argument
structure of a sentence but also assigning sentence
fragments to clusters of semantically similar expres-
sions. For example, for a pair of sentences on Fig-
ure 1, in addition to inducing predicate-argument
structure, they aim to assign expressions “Steelers”
and “the Pittsburgh team” to the same semantic
class Steelers, and group expressions “defeated”
and “secured the victory over”. Such semantic rep-
resentation can be useful for entailment or question
answering tasks, as an entailment model can ab-
stract away from specifics of syntactic and lexical
realization relying instead on the induced semantic
representation. For example, the two sentences in
Figure 1 have identical semantic representation, and
therefore can be hypothesized to be equivalent.
</bodyText>
<page confidence="0.930498">
1445
</page>
<note confidence="0.9853425">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1445–1455,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.99695305">
Ravens
Ravens defeated
Winner
subj
WinPrize
Opponent
dobj
Steelers
Steelers
Ravens
Ravens secured the victory over
Winner
subj
WinPrize
dobj
Opponent
pp_over
Steelers
nmod
the Pittsburgh team
</figure>
<figureCaption confidence="0.9711125">
Figure 1: An example of two different syntactic trees with a common semantic representation WinPrize(Ravens,
Steelers).
</figureCaption>
<bodyText confidence="0.9999811625">
From the statistical modeling point of view, joint
learning of predicate-argument structure and dis-
covery of semantic clusters of expressions can also
be beneficial, because it results in a more compact
model of selectional preference, less prone to the
data-sparsity problem (Zapirain et al., 2010). In this
respect our model is similar to recent LDA-based
models of selectional preference (Ritter et al., 2010;
S´eaghdha, 2010), and can even be regarded as their
recursive and non-parametric extension.
In this paper, we adopt the above definition of un-
supervised semantic parsing and propose a Bayesian
non-parametric approach which uses hierarchical
Pitman-Yor (PY) processes (Pitman, 2002) to model
statistical dependencies between predicate and ar-
gument clusters, as well as distributions over syn-
tactic and lexical realizations of each cluster. Our
non-parametric model automatically discovers gran-
ularity of clustering appropriate for the dataset, un-
like the parametric method of (Poon and Domingos,
2009) which have to perform model selection and
use heuristics to penalize more complex models of
semantics. Additional benefits generally expected
from Bayesian modeling include the ability to en-
code prior linguistic knowledge in the form of hy-
perpriors and the potential for more reliable model-
ing of smaller datasets. More detailed discussion of
relation between the Markov Logic Network (MLN)
approach of (Poon and Domingos, 2009) and our
non-parametric method is presented in Section 3.
Hierarchical Pitman-Yor processes (or their spe-
cial case, hierarchical Dirichlet processes) have pre-
viously been used in NLP, for example, in the con-
text of syntactic parsing (Liang et al., 2007; John-
son et al., 2007). However, in all these cases the
effective size of the state space (i.e., the number
of sub-symbols in the infinite PCFG (Liang et al.,
2007), or the number of adapted productions in the
adaptor grammar (Johnson et al., 2007)) was not
very large. In our case, the state space size equals
the total number of distinct semantic clusters, and,
thus, is expected to be exceedingly large even for
moderate datasets: for example, the MLN model in-
duces 18,543 distinct clusters from 18,471 sentences
of the GENIA corpus (Poon and Domingos, 2009).
This suggests that standard inference methods for hi-
erarchical PY processes, such as Gibbs sampling,
Metropolis-Hastings (MH) sampling with uniform
proposals, or the structured mean-field algorithm,
are unlikely to result in efficient inference: for ex-
ample in standard Gibbs sampling all thousands of
alternatives should be considered at each sampling
move. Instead, we use a split-merge MH sampling
algorithm, which is a standard and efficient infer-
ence tool for non-hierarchical PY processes (Jain
and Neal, 2000; Dahl, 2003) but has not previously
been used in hierarchical setting. We extend the
sampler to include composition-decomposition of
syntactic fragments in order to cluster fragments of
variables size, as in the example Figure 1, and also
include the argument role-syntax alignment move
which attempts to improve mapping between seman-
tic roles and syntactic paths for some fixed predicate.
Evaluating unsupervised models is a challenging
task. We evaluate our model both qualitatively, ex-
amining the revealed clustering of syntactic struc-
tures, and quantitatively, on a question answering
task. In both cases, we follow (Poon and Domingos,
2009) in using the corpus of biomedical abstracts.
Our model achieves favorable results significantly
outperforming the baselines, including state-of-the-
art methods for relation extraction, and achieves
scores comparable to those of the MLN model.
The rest of the paper is structured as follows. Sec-
tion 2 begins with a definition of the semantic pars-
ing task. Sections 3 and 4 give background on the
MLN model and the Pitman-Yor processes, respec-
tively. In Sections 5 and 6, we describe our model
and the inference method. Section 7 provides both
qualitative and quantitative evaluation. Finally, ad-
</bodyText>
<page confidence="0.986739">
1446
</page>
<bodyText confidence="0.961504">
ditional related work is presented in Section 8.
</bodyText>
<sectionHeader confidence="0.849941" genericHeader="introduction">
2 Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999987741573034">
In this section, we briefly define the unsupervised
semantic parsing task and underlying aspects and as-
sumptions relevant to our model.
Unlike (Poon and Domingos, 2009), we do not
use the lambda calculus formalism to define our task
but rather treat it as an instance of frame-semantic
parsing, or a specific type of semantic role label-
ing (Gildea and Jurafsky, 2002). The reason for this
is two-fold: first, the frame semantics view is more
standard in computational linguistics, sufficient to
describe induced semantic representation and conve-
nient to relate our method to the previous work. Sec-
ond, lambda calculus is a considerably more power-
ful formalism than the predicate-argument structure
used in frame semantics, normally supporting quan-
tification and logical connectors (for example, nega-
tion and disjunction), neither of which is modeled
by our model or in (Poon and Domingos, 2009).
In frame semantics, the meaning of a predicate
is conveyed by a frame, a structure of related con-
cepts that describes a situation, its participants and
properties (Fillmore et al., 2003). Each frame is
characterized by a set of semantic roles (frame el-
ements) corresponding to the arguments of the pred-
icate. It is evoked by a frame evoking element (a
predicate). The same frame can be evoked by differ-
ent but semantically similar predicates: for exam-
ple, both verbs “buy” and “purchase” evoke frame
Commerce buy in FrameNet (Fillmore et al., 2003).
The aim of the semantic role labeling task is to
identify all of the frames evoked in a sentence and
label their semantic role fillers. We extend this task
and treat semantic parsing as recursive prediction of
predicate-argument structure and clustering of argu-
ment fillers. Thus, parsing a sentence into this rep-
resentation involves (1) decomposing the sentence
into lexical items (one or more words), (2) assigning
a cluster label (a semantic frame or a cluster of ar-
gument fillers) to every lexical item, and (3) predict-
ing argument-predicate relations between the lexical
items. This process is illustrated in Figure 1. For
the leftmost example, the sentence is decomposed
into three lexical items: “Ravens”, “defeated”
and “Steelers”, and they are assigned to clusters
Ravens, WinPrize and Steelers, respectively.
Then Ravens and Steelers are selected as a
Winner and an Opponent in the WinPrize frame.
In this work, we define a joint model for the label-
ing and argument identification stages. Similarly to
core semantic roles in FrameNet, semantic roles are
treated as frame-specific in our model, as our model
does not try to discover any correspondences be-
tween roles in different frames.
As you can see from the above description, frames
(which groups predicates with similar meaning such
as the WinPrize frame in our example) and clus-
ters of argument fillers (Ravens and Steelers) are
treated in our definition in a similar way. For con-
venience, we will refer to both types of clusters as
semantic classes.1
This definition of semantic parsing is closely re-
lated to a realistic relation extraction setting, as both
clustering of syntactic forms of relations (or extrac-
tion patterns) and clustering of argument fillers for
these relations is crucial for automatic construction
of knowledge bases (Yates and Etzioni, 2009).
In this paper, we make three assumptions. First,
we assume that each lexical item corresponds to a
subtree of the syntactic dependency graph of the
sentence. This assumption is similar to the ad-
jacency assumption in (Zettlemoyer and Collins,
2005), though ours may be more appropriate for lan-
guages with free or semi-free word order, where syn-
tactic structures are inherently non-projective. Sec-
ond, we assume that the semantic arguments are lo-
cal in the dependency tree; that is, one lexical item
can be a semantic argument of another one only if
they are connected by an arc in the dependency tree.
This is a slight simplification of the semantic role
labeling problem but one often made. Thus, the ar-
gument identification and labeling stages consist of
labeling each syntactic arc with a semantic role la-
bel. In comparison, the MLN model does not explic-
itly assume contiguity of lexical items and does not
make this directionality assumption but their clus-
tering algorithm uses initialization and clusterization
moves such that the resulting model also obeys both
of these constraints. Third, as in (Poon and Domin-
gos, 2009), we do not model polysemy as we assume
</bodyText>
<footnote confidence="0.9840715">
1Semantic classes correspond to lambda-form clusters in
(Poon and Domingos, 2009) terminology.
</footnote>
<page confidence="0.993194">
1447
</page>
<bodyText confidence="0.999896">
that each syntactic fragment corresponds to a single
semantic class. This is not a model assumption and
is only used at inference as it reduces mixing time of
the Markov chain. It is not likely to be restrictive for
the biomedical domain studied in our experiments.
As in some of the recent work on learning se-
mantic representations (Eisenstein et al., 2009; Poon
and Domingos, 2009), we assume that dependency
structures are provided for every sentence. This as-
sumption allows us to construct models of seman-
tics not Markovian within a sequence of words (see
for an example a model described in (Liang et al.,
2009)), but rather Markovian within a dependency
tree. Though we include generation of the syntac-
tic structure in our model, we would not expect that
this syntactic component would result in an accurate
syntactic model, even if trained in a supervised way,
as the chosen independence assumptions are over-
simplistic. In this way, we can use a simple gener-
ative story and build on top of the recent success in
syntactic parsing.
</bodyText>
<sectionHeader confidence="0.845027" genericHeader="method">
3 Relation to the MLN Approach
</sectionHeader>
<bodyText confidence="0.999986816326531">
The work of (Poon and Domingos, 2009) models
joint probability of the dependency tree and its latent
semantic representation using Markov Logic Net-
works (MLNs) (Richardson and Domingos, 2006),
selecting parameters (weights of first-order clauses)
to maximize the probability of the observed depen-
dency structures. For each sentence, the MLN in-
duces a Markov network, an undirected graphical
model with nodes corresponding to ground atoms
and cliques corresponding to ground clauses.
The MLN is a powerful formalism and allows for
modeling complex interaction between features of
the input (syntactic trees) and latent output (seman-
tic representation), however, unsupervised learn-
ing of semantics with general MLNs can be pro-
hibitively expensive. The reason for this is that
MLNs are undirected models and when learned to
maximize likelihood of syntactically annotated sen-
tences, they would require marginalization over se-
mantic representation but also over the entire space
of syntactic structures and lexical units. Given the
complexity of the semantic parsing task and the need
to tackle large datasets, even approximate methods
are likely to be infeasible. In order to overcome
this problem, (Poon and Domingos, 2009) group pa-
rameters and impose local normalization constraints
within each group. Given these normalization con-
straints, and additional structural constraints satis-
fied by the model, namely that the clauses should
be engineered in such a way that they induce tree-
structured graphs for every sentence, the parameters
can be estimated by a variant of the EM algorithm.
The class of such restricted MLNs is equivalent
to the class of directed graphical models over the
same set of random variables corresponding to frag-
ments of syntactic and semantic structure. Given
that the above constraints do not directly fit into the
MLN methodology, we believe that it is more nat-
ural to regard their model as a directed model with
an underlying generative story specifying how the
semantic structure is generated and how the syntac-
tic parse is drawn for this semantic structure. This
view would facilitate understanding what kind of
features can easily be integrated into the model, sim-
plify application of non-parametric Bayesian tech-
niques and expedite the use of inference techniques
designed specifically for directed models. Our ap-
proach makes one step in this direction by proposing
a non-parametric version of such generative model.
</bodyText>
<sectionHeader confidence="0.900796" genericHeader="method">
4 Hierarchical Pitman-Yor Processes
</sectionHeader>
<bodyText confidence="0.999788842105263">
The central component of our non-parametric
Bayesian model are Pitman-Yor (PY) processes,
which are a generalization of the Dirichlet processes
(DPs) (Ferguson, 1973). We use PY processes to
model distributions of semantic classes appearing as
an argument of other semantic classes. We also use
them to model distributions of syntactic realizations
for each semantic class and distributions of syntactic
dependency arcs for argument types. In this section
we present relevant background on PY processes.
For a more detailed consideration we refer the reader
to (Teh et al., 2006).
The Pitman-Yor process over a set S, denoted
PY (α, Q, H), is a stochastic process whose samples
G0 constitute probability measures on partitions of
S. In practice, we do not need to draw measures,
as they can be analytically marginalized out. The
conditional distribution of xj+1 given the previous
j draws, with G0 marginalized out, follows (Black-
</bodyText>
<page confidence="0.949884">
1448
</page>
<bodyText confidence="0.992300821428571">
well and MacQueen, 1973)
where φ1, ... , φK are K values assigned to
x1, x2, ... , xj. The number of times φk was as-
signed is denoted jk, so that j = EKk=1 jk. The
parameter β &lt; 1 controls how heavy the tail of the
distribution is: when it approaches 1, a new value is
assigned to every draw, when β = 0 the PY process
reduces to DP. The expected value of K scales as
O(αnβ) with the number of draws n, while it scales
only logarithmically for DP processes. PY processes
are expected to be more appropriate for many NLP
problems, as they model power-law type distribu-
tions common for natural language (Teh, 2006).
Hierarchical Dirichlet Processes (HDP) or hierar-
chical PY processes are used if the goal is to draw
several related probability measures for the same
set S. For example, they can be used to generate
transition distributions of a Markov model, HDP-
HMM (Teh et al., 2006; Beal et al., 2002). For
such a HMM, the top-level state proportions are
drawn from the top-level stick breaking construction
γ — GEM(α, β), and then the individual transi-
tion distributions for every state z = 1, 2,... φz are
drawn from PY (γ, α&apos;, β&apos;). The parameters α&apos; and
β&apos; control how similar the individual transition dis-
tributions φz are to the top-level state proportions γ,
or, equivalently, how similar the transition distribu-
tions are to each other.
</bodyText>
<sectionHeader confidence="0.996317" genericHeader="method">
5 A Model for Semantic Parsing
</sectionHeader>
<bodyText confidence="0.9998148">
Our model of semantics associates with each seman-
tic class a set of distributions which govern the gen-
eration of corresponding syntactic realizations2 and
the selection of semantic classes for its arguments.
Each sentence is generated starting from the root of
its dependency tree, recursively drawing a seman-
tic class, its syntactic realization, arguments and se-
mantic classes for the arguments. Below we de-
scribe the model by first defining the set of the model
parameters and then explaining the generation of in-
</bodyText>
<footnote confidence="0.976364666666667">
2Syntactic realizations are syntactic tree fragments, and
therefore they correspond both to syntactic and lexical varia-
tions.
</footnote>
<bodyText confidence="0.998155162790698">
dividual sentences. The generative story is formally
presented in Figure 2.
We associate with each semantic class c, c =
1, 2, ... , a distribution of its syntactic realizations
φc. For example, for the frame WinPrize illus-
trated in Figure 1 this distribution would concen-
trate at syntactic fragments corresponding to lexical
items “defeated”, “secured the victory” and “won”.
The distribution is drawn from DP(w(C), H(C)),
where H(C) is a base measure over syntactic sub-
trees. We use a simple generative process to define
the probability of a subtree, the underlying model is
similar to the base measures used in the Bayesian
tree-substitution grammars (Cohn et al., 2009). We
start by generating a word w uniformly from the
treebank distribution, then we decide on the num-
ber of dependents of w using the geometric distribu-
tion Geom(q(C)). For every dependent we generate
a dependency relation r and a lexical form w&apos; from
P(rjw)P(w&apos;jr), where probabilities P are based on
add-0.1 smoothed treebank counts. The process is
continued recursively. The smaller the parameter
q(C), the lower is the probability assigned to larger
sub-trees.
Parameters ψc,t and ψ+c,t, t = 1, ... , T, de-
fine a distribution over vectors (m1, m2, ... , mT)
where mt is the number of times an argument of
type t appears for a given semantic frame occur-
rence3. For the frame WinPrize these parameters
would enforce that there exists exactly one Winner
and exactly one Opponent for each occurrence of
WinPrize. The parameter ψc,t defines the probabil-
ity of having at least one argument of type t. If 0 is
drawn from ψc,t then mt = 0, otherwise the number
of additional arguments of type t (mt — 1) is drawn
from the geometric distribution Geom(ψ+c,t). This
generative story is flexible enough to accommodate
both argument types which appear at most once per
semantic class occurrence (e.g., agents), and argu-
ment types which frequently appear multiple times
per semantic class occurrence (e.g., arguments cor-
responding to descriptors).
Parameters φc,t, t = 1, ... , T, define the dis-
</bodyText>
<footnote confidence="0.9199336">
3For simplicity, we assume that each semantic class has T
associated argument types, note that this is not a restrictive as-
sumption as some of the argument types can remain unused,
and T can be selected to be sufficiently large to accommodate
all important arguments.
</footnote>
<equation confidence="0.987169714285714">
jk —
δ + Kβ + αH (1)
j+α φ&apos;` j+α
xj+1jx1, ... xj —
K
k=1
β
</equation>
<page confidence="0.917182">
1449
</page>
<table confidence="0.99967112">
Parameters:
γ ∼ GEM(α0, β0) [top-level proportions of classes]
θroot ∼ PY (αroot, βroot, γ) [distrib of sem classes at root]
for each sem class c = 1, 2, ... :
φc ∼ DP(w(C), H(C)) [distribs of synt realizations]
for each arg type t = 1, 2, ... T:
ψc,t ∼ Beta(η0, η1) [first argument generation]
ψ1t ∼ Beta(ηo , η1 ) [geom distr for more args]
φc,t ∼ DP(w(A), H(A)) [distribs of synt paths]
θc,t ∼ PY (α, β, γ) [distrib of arg fillers]
Data Generation:
for each sentence:
croot ∼ θroot [choose sem class for root]
GenSemClass(croot)
GenSemClass(c):
s ∼ φc [draw synt realization]
for each arg type t = 1, ... ,T:
if [n ∼ ψc,t] = 1: [at least one arg appears]
GenArgument(c, t) [draw one arg]
while [n ∼ ψ�c,t] = 1: [continue generation]
GenArgument(c, t) [draw more args]
GenArgument(c, t):
ac,t ∼ φc,t [draw synt relation]
c&apos;c,t ∼ θc,t [draw sem class for arg]
GenSemClass(c&apos; c,t) [recurse]
</table>
<figureCaption confidence="0.9980305">
Figure 2: The generative story for the Bayesian model for
unsupervised semantic parsing.
</figureCaption>
<bodyText confidence="0.998886225806452">
tributions over syntactic paths for the argument
type t. In our example, for argument type
Opponent, this distribution would associate most
of the probability mass with relations pp over, dobj
and pp against. These distributions are drawn from
DP(w(A), H(A)). In this paper we only consider
paths consisting of a single relation, therefore the
base probability distribution H(A) is just normalized
frequencies of dependency relations in the treebank.
The crucial part of the model are the selection-
preference parameters 0,,t, the distributions of se-
mantic classes c&apos; for each argument type t of class
c. For arguments Winner and Opponent of the
frame WinPrize these distributions would assign
most of the probability mass to semantic classes de-
noting teams or players. Distributions 0,,t are drawn
from a hierarchical PY process: first, top-level pro-
portions of classes -y are drawn from GEM(a0, Q0),
and then the individual distributions 0,,t over c&apos; are
chosen from PY (a, 0, -y).
For each sentence, we first generate a class corre-
sponding to the root of the dependency tree from the
root-specific distribution of semantic classes Broot.
Then we recursively generate classes for the entire
sentence. For a class c, we generate the syntactic
realization s and for each of the T types, decide
how many arguments of that type to generate (see
GenSemClass in Figure 2). Then we generate each
of the arguments (see GenArgument) by first gen-
erating a syntactic arc a,,t, choosing a class as its
filler c�,t and, finally, recursing.
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="method">
6 Inference
</sectionHeader>
<bodyText confidence="0.9999855">
In our model, latent states, modeled with hierarchi-
cal PY processes, correspond to distinct semantic
classes and, therefore, their number is expected to
be very large for any reasonable model of semantics.
As a result, many standard inference techniques,
such as Gibbs sampling, or the structured mean-field
method are unlikely to result in tractable inference.
One of the standard and most efficient samplers for
non-hierarchical PY processes are split-merge MH
samplers (Jain and Neal, 2000; Dahl, 2003). In this
section we explain how split-merge samplers can be
applied to our model.
</bodyText>
<subsectionHeader confidence="0.997971">
6.1 Split and Merge Moves
</subsectionHeader>
<bodyText confidence="0.99998215">
On each move, split-merge samplers decide either
to merge two states into one (in our case, merge two
semantic classes), or split one state into two. These
moves can be computed efficiently for our model of
semantics. Note that for any reasonable model of
semantics only a small subset of the entire set of se-
mantic classes can be used as an argument for some
fixed semantic class due to selectional preferences
exhibited by predicates. For instance, only teams or
players can fill arguments of the frame WinPrize
in our running example. As a result, only a small
number of terms in the joint distribution has to be
evaluated on every move we may consider.
When estimating the model, we start with assign-
ing each distinct word (or, more precisely, a tuple
of a word’s stem and its part-of-speech tag) to an
individual semantic class. Then, we would iterate
by selecting a random pair of class occurrences, and
decide, at random, whether we attempt to perform a
split-merge move or a compose-decompose move.
</bodyText>
<page confidence="0.977576">
1450
</page>
<subsectionHeader confidence="0.99343">
6.2 Compose and Decompose Moves
</subsectionHeader>
<bodyText confidence="0.999978157894737">
The compose-decompose operations modify syntac-
tic fragments assigned to semantic classes, com-
posing two neighboring dependency sub-trees or
decomposing a dependency sub-tree. If the two
randomly-selected syntactic fragments s and s&apos; cor-
respond to different classes, c and c&apos;, we attempt
to compose them into s� and create a new semantic
class c. All occurrences of s� are assigned to this new
class c. For example, if two randomly-selected oc-
currences have syntactic realizations “secure” and
“victory” they can be composed to obtain the syn-
tactic fragment “secure dobj −−) victory”. This frag-
ment will be assigned to a new semantic class which
can later be merged with other classes, such as the
ones containing syntactic realizations “defeat” or
“win”.
Conversely, if both randomly-selected syntactic
fragments are already composed in the correspond-
ing class, we attempt to split them.
</bodyText>
<subsectionHeader confidence="0.996876">
6.3 Role-Syntax Alignment Move
</subsectionHeader>
<bodyText confidence="0.999977642857143">
Merge, compose and decompose moves require re-
computation of mapping between argument types
(semantic roles) and syntactic fragments. Comput-
ing the best statistical mapping is infeasible and
proposing a random mapping will result in many
attempted moves being rejected. Instead we use
a greedy randomized search method called Gibbs
scan (Dahl, 2003). Though it is a part of the above 3
moves, this alignment move is also used on its own
to induce semantic arguments for classes (frames)
with a single syntactic realization.
The Gibbs scan procedure is also used during the
split move to select one of the newly introduced
classes for each considered syntactic fragment.
</bodyText>
<subsectionHeader confidence="0.857481">
6.4 Informed Proposals
</subsectionHeader>
<bodyText confidence="0.999979058823529">
Since the number of classes is very large, selecting
examples at random would result in a relatively low
proportion of moves getting accepted, and, conse-
quently, in a slow-mixing Markov chain. Instead of
selecting both class occurrences uniformly, we se-
lect the first occurrence from a uniform distribution
and then use a simple but effective proposal distri-
bution for selecting the second class occurrence.
Let us denote the class corresponding to the first
occurrence as c1 and its syntactic realization as s1
with a head word w1. We begin by selecting uni-
formly randomly whether to attempt a compose-
decompose or a split-merge move.
If we chose a compose-decompose move, we look
for words (children) which can be attached below
the syntactic fragment s1. We use the normalized
counts of these words conditioned on the parent s1 to
select the second word w2. We then select a random
occurrence of w2; if it is a part of syntactic realiza-
tion of c1 then a decompose move is attempted. Oth-
erwise, we try to compose the corresponding clus-
ters together.
If we selected a split-merge move, we use a dis-
tribution based on the cosine similarity of lexical
contexts of the words. The context is represented
as a vector of counts of all pairs of the form (head
word, dependency type) and (dependent, depen-
dency type). So, instead of selecting a word occur-
rence uniformly, each occurrence of every word w2
is weighted by its similarity to w1, where the simi-
larity is based on the cosine distance.
As the moves are dependent only on syntactic rep-
resentations, all the proposal distributions can be
computed once at the initialization stage.4
</bodyText>
<sectionHeader confidence="0.992793" genericHeader="method">
7 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.99938175">
We induced a semantic representation over a collec-
tion of texts and evaluated it by answering questions
about the knowledge contained in the corpus. We
used the GENIA corpus (Kim et al., 2003), a dataset
of 1999 biomedical abstracts, and a set of questions
produced by (Poon and Domingos, 2009). A exam-
ple question is shown in Figure 3.
All model hyperpriors were set to maximize the
posterior, except for w(A) and w(C), which were set
to 1.e −10 and 1.e − 35, respectively. Inference was
run for around 300,000 sampling iterations until the
percentage of accepted split-merge moves became
lower than 0.05%.
Let us examine some of the induced semantic
classes (Table 1) before turning to the question an-
swering task. Almost all of the clustered syntactic
</bodyText>
<footnote confidence="0.607117">
4In order to minimize memory usage, we used frequency
cut-off of 10. For split-merge moves, we select words based
on the cosine distance if the distance is below 0.95 and sample
the remaining words uniformly. This also reduces the required
memory usage.
</footnote>
<page confidence="0.944866">
1451
</page>
<table confidence="0.9994857">
Class Variations
1 motif, sequence, regulatory element, response ele-
ment, element, dna sequence
2 donor, individual, subject
3 important, essential, critical
4 dose, concentration
5 activation, transcriptional activation, transactiva-
tion
6 b cell, t lymphocyte, thymocyte, b lymphocyte, t
cell, t-cell line, human lymphocyte, t-lymphocyte
7 indicate, reveal, document, suggest, demonstrate
8 augment, abolish, inhibit, convert, cause, abrogate,
modulate, block, decrease, reduce, diminish, sup-
press, up-regulate, impair, reverse, enhance
9 confirm, assess, examine, study, evaluate, test, re-
solve, determine, investigate
10 nf-kappab, nf-kappa b, nfkappab, nf-kb
11 antiserum, antibody, monoclonal antibody, ab, an-
tisera, mab
12 tnfalpha, tnf-alpha, il-6, tnf
</table>
<tableCaption confidence="0.999922">
Table 1: Examples of the induced semantic classes.
</tableCaption>
<bodyText confidence="0.999397103448276">
realizations have a clear semantic connection. Clus-
ter 6, for example, clusters lymphocytes with the ex-
ception of thymocyte, a type of cell which gener-
ates T cells. Cluster 8 contains verbs roughly corre-
sponding to Cause change of position on a
scale frame in FrameNet. Verbs in class 9 are used
in the context of providing support for a finding or
an action, and many of them are listed as evoking
elements for the Evidence frame in FrameNet.
Argument types of the induced classes also show
a tendency to correspond to semantic roles. For ex-
ample, an argument type of class 2 is modeled as
a distribution over two argument parts, prep of and
prep from. The corresponding arguments define the
origin of the cells (transgenic mouse, smoker, volun-
teer, donor, ... ).
We now turn to the QA task and compare our
model (USP-BAYES) with the results of baselines
considered in (Poon and Domingos, 2009). The first
set of baselines looks for answers by attempting to
match a verb and its argument in the question with
the input text. The first version (KW) simply re-
turns the rest of the sentence on the other side of the
verb, while the second (KW-SYN) uses syntactic in-
formation to extract the subject or the object of the
verb.
Other baselines are based on state-of-the-art re-
lation extraction systems. When the extracted rela-
tion and one of the arguments match those in a given
</bodyText>
<table confidence="0.999244">
Total Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP-MLN 334 295 88%
USP-BAYES 325 259 80%
</table>
<tableCaption confidence="0.999831">
Table 2: Performance on the QA task.
</tableCaption>
<bodyText confidence="0.9998436">
question, the second argument is returned as an an-
swer. The systems include TextRunner (TR) (Banko
et al., 2007), RESOLVER (RS) (Yates and Etzioni,
2009) and DIRT (Lin and Pantel, 2001). The EX-
ACT versions of the methods return answers when
they match the question argument exactly, and the
SUB versions produce answers containing the ques-
tion argument as a substring.
Similarly to the MLN system (USP-MLN), we
generate answers as follows. We use our trained
model to parse a question, i.e. recursively decom-
pose it into lexical items and assign them to seman-
tic classes induced at training. Using this semantic
representation, we look for the type of an argument
missing in the question, which, if found, is reported
as an answer. It is clear that overly coarse clusters
of argument fillers or clustering of semantically re-
lated but not equivalent relations can hurt precision
for this evaluation method.
Each system is evaluated by counting the answers
it generates, and computing the accuracy of those
answers.5 Table 2 summarizes the results. First,
both USP models significantly outperform all other
baselines: even though the accuracy of KW-SYN
and TR-EXACT are comparable with our accuracy,
the number of correct answers returned by USP-
Bayes is 4 and 11 times smaller than those of KW-
SYN and TR-EXACT, respectively. While we are
not beating the MLN baseline, the difference is not
significant. The effective number of questions is rel-
atively small (less than 80 different questions are an-
swered by any of the models). More than 50% of
USP-BAYES mistakes were due to wrong interpre-
tation of only 5 different questions. From another
point of view, most of the mistakes are explained
</bodyText>
<footnote confidence="0.8375695">
5The true recall is not known, as computing it would require
exhaustive annotation of the entire corpus.
</footnote>
<page confidence="0.991078">
1452
</page>
<figure confidence="0.860281454545455">
Question: What does cyclosporin A suppress?
Answer: expression of EGR-2
Sentence: As with EGR-3 , expression of EGR-2 was blocked
by cyclosporin A .
Question: What inhibits tnf-alpha?
Answer: IL -10
Sentence: Our previous studies in human monocytes have
demonstrated that interleukin ( IL ) -10 inhibits lipopolysac-
charide ( LPS ) -stimulated production of inflammatory cy-
tokines , IL-1 beta, IL-6 , IL-8 , and tumor necrosis factor(
TNF) -alpha by blocking gene transcription.
</figure>
<figureCaption confidence="0.9983295">
Figure 3: An example of questions, answers by our model
and the corresponding sentences from the dataset.
</figureCaption>
<bodyText confidence="0.999968916666667">
by overly coarse clustering corresponding to just 3
classes, namely, 30%, 25% and 20% of errors are
due to the clusters 6, 8 and 12 (Figure 1), respec-
tively. Though all these clusters have clear semantic
interpretation (white blood cells, predicates corre-
sponding to changes and cykotines associated with
cancer progression, respectively), they appear to be
too coarse for the QA method we use in our exper-
iments. Though it is likely that tuning and differ-
ent heuristics may result in better scores, we chose
not to perform excessive tuning, as the evaluation
dataset is fairly small.
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999990351351351">
There is a growing body of work on statistical learn-
ing for different versions of the semantic parsing
problem (e.g., (Gildea and Jurafsky, 2002; Zettle-
moyer and Collins, 2005; Ge and Mooney, 2005;
Mooney, 2007)), however, most of these methods
rely on human annotation, or some weaker forms of
supervision (Kate and Mooney, 2007; Liang et al.,
2009; Titov and Kozhevnikov, 2010; Clarke et al.,
2010) and very little research has considered the un-
supervised setting.
In addition to the MLN model (Poon and Domin-
gos, 2009), another unsupervised method has been
proposed in (Goldwasser et al., 2011). In that work,
the task is to predict a logical formula, and the only
supervision used is a lexicon providing a small num-
ber of examples for every logical symbol. A form of
self-training is then used to bootstrap the model.
Unsupervised semantic role labeling with a gen-
erative model has also been considered (Grenager
and Manning, 2006), however, they do not attempt
to discover frames and deal only with isolated pred-
icates. Another generative model for SRL has been
proposed in (Thompson et al., 2003), but the param-
eters were estimated from fully annotated data.
The unsupervised setting has also been consid-
ering for the related problem of learning narrative
schemas (Chambers and Jurafsky, 2009). However,
their approach is quite different from our Bayesian
model as it relies on similarity functions.
Though in this work we focus solely on the un-
supervised setting, there has been some success-
ful work on semi-supervised semantic-role label-
ing, including the Framenet version of the prob-
lem (F¨urstenau and Lapata, 2009). Their method
exploits graph alignments between labeled and un-
labeled examples, and, therefore, crucially relies on
the availability of labeled examples.
</bodyText>
<sectionHeader confidence="0.994564" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999990681818182">
In this work, we introduced a non-parametric
Bayesian model for the semantic parsing problem
based on the hierarchical Pitman-Yor process. The
model defines a generative story for recursive gener-
ation of lexical items, syntactic and semantic struc-
tures. We extend the split-merge MH sampling algo-
rithm to include composition-decomposition moves,
and exploit the properties of our task to make it effi-
cient in the hierarchical setting we consider.
We plan to explore at least two directions in our
future work. First, we would like to relax some of
unrealistic assumptions made in our model: for ex-
ample, proper modeling of alterations requires joint
generation of syntactic realizations for predicate-
argument relations (Grenager and Manning, 2006;
Lang and Lapata, 2010), similarly, proper model-
ing of nominalization implies support of arguments
not immediately local in the syntactic structure. The
second general direction is the use of the unsuper-
vised methods we propose to expand the coverage of
existing semantic resources, which typically require
substantial human effort to produce.
</bodyText>
<sectionHeader confidence="0.99632" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9949984">
The authors acknowledge the support of the MMCI Clus-
ter of Excellence, and thank Chris Callison-Burch, Alexis
Palmer, Caroline Sporleder, Ben Van Durme and the
anonymous reviewers for their helpful comments and
suggestions.
</bodyText>
<page confidence="0.98424">
1453
</page>
<sectionHeader confidence="0.990097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871752380952">
O. Abend, R. Reichart, and A. Rappoport. 2009. Unsu-
pervised argument identification for semantic role la-
beling. In Proceedings of ACL-IJCNLP, pages 28–36,
Singapore.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proc. of the In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 2670–2676.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2002. The infinite hidden markov model. In
Machine Learning, pages 29–245. MIT Press.
David Blackwell and James B. MacQueen. 1973. Fergu-
son distributions via polya urn schemes. The Annals
of Statistics, 1(2):353–355.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of the 9th Conference on Natu-
ral Language Learning, CoNLL-2005, Ann Arbor, MI
USA.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In Proc. of the Conference on Com-
putational Natural Language Learning (CoNLL).
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In HLT-NAACL, pages 548–556.
David B. Dahl. 2003. An improved merge-split sampler
for conjugate dirichlet process mixture models. Tech-
nical Report 1086, Department of Statistics, Univer-
sity of Wiscosin - Madison, November.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings of
EMNLP.
Thomas S. Ferguson. 1973. A bayesian analysis of
some nonparametric problems. The Annals of Statis-
tics, 1(2):209–230.
C. J. Fillmore, C. R. Johnson, and M. R. L. Petruck.
2003. Background to framenet. International Journal
of Lexicography, 16:235–250.
Hagen F¨urstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning (CONLL-05), Ann
Arbor, Michigan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245–288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In Proc. of the Meeting of Association for
Computational Linguistics (ACL), Portland, OR, USA.
Trond Grenager and Christoph Manning. 2006. Unsu-
pervised discovery of a statistical verb lexicon. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).
Sonia Jain and Radford Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158–182.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Rochester, USA.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambigous supervision. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 895–900.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi
Tsujii. 2003. Genia corpus—a semantically annotated
corpus for bio-textmining. Bioinformatics, 19:i180–
i182.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Proceedings of the 48rd
Annual Meeting of the Association for Computational
Linguistics (ACL), Uppsala, Sweden.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical dirich-
let processes. In Joint Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
688–697, Prague, Czech Republic.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proc. of the Annual Meeting of the Association
for Computational Linguistics and International Joint
Conference on Natural Language Processing (ACL-
IJCNLP).
Dekang Lin and Patrick Pantel. 2001. Dirt – discovery
of inference rules from text. In Proc. of International
Conference on Knowledge Discovery and Data Min-
ing, pages 323–328.
</reference>
<page confidence="0.880598">
1454
</page>
<reference confidence="0.999702952380953">
Raymond J. Mooney. 2007. Learning for semantic pars-
ing. In Proceedings of the 8th International Confer-
ence on Computational Linguistics and Intelligent Text
Processing, pages 982–991.
Alexis Palmer and Caroline Sporleder. 2010. Evaluating
framenet-style semantic parsing: the role of coverage
gaps in framenet. In Proceedings of the Conference on
Computational Linguistics (COLING-2000), Beijing.
Jim Pitman. 2002. Poisson-dirichlet and gem invari-
ant distributions for split-and-merge transformations
of an interval partition. Combinatorics, Probability
and Computing, 11:501–514.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, (EMNLP-09).
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107–136.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet allocation method for selectional preferences.
In Proceedings of the 48rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Upp-
sala, Sweden.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48rd
Annual Meeting of the Association for Computational
Linguistics (ACL), Uppsala, Sweden.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labelling. In Proceedings of EMNLP, pages
95–102, Barcelona, Spain.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566–1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985–
992.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role
labeling. In In Senseval-3, pages 397–408.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Upp-
sala, Sweden.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal ofArtificial Intelligence Research,
34:255–296.
B. Zapirain, E. Agirre, L. L. M`arquez, and M. Surdeanu.
2010. Improving semantic role classification with se-
lectional prefrences. In Proceedings of the Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL 2010), Los Ange-
les.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammar. In
Proceedings of the Twenty-first Conference on Uncer-
tainty in Artificial Intelligence, Edinburgh, UK, Au-
gust.
</reference>
<page confidence="0.992964">
1455
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899053">
<title confidence="0.99998">A Bayesian Model for Unsupervised Semantic Parsing</title>
<author confidence="0.997901">Ivan Titov Alexandre Klementiev</author>
<affiliation confidence="0.999999">Saarland University Johns Hopkins University</affiliation>
<address confidence="0.999762">Saarbruecken, Germany Baltimore, MD, USA</address>
<email confidence="0.99864">titov@mmci.uni-saarland.deaklement@jhu.edu</email>
<abstract confidence="0.995358590909091">We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical Pitman- Yor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the Metropolis- Hastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Abend</author>
<author>R Reichart</author>
<author>A Rappoport</author>
</authors>
<title>Unsupervised argument identification for semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="2467" citStr="Abend et al., 2009" startWordPosition="351" endWordPosition="354">f relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailment or question answer</context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2009</marker>
<rawString>O. Abend, R. Reichart, and A. Rappoport. 2009. Unsupervised argument identification for semantic role labeling. In Proceedings of ACL-IJCNLP, pages 28–36, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="2208" citStr="Banko et al., 2007" startWordPosition="312" endWordPosition="315">Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argumen</context>
<context position="32372" citStr="Banko et al., 2007" startWordPosition="5215" endWordPosition="5218">n the other side of the verb, while the second (KW-SYN) uses syntactic information to extract the subject or the object of the verb. Other baselines are based on state-of-the-art relation extraction systems. When the extracted relation and one of the arguments match those in a given Total Correct Accuracy KW 150 67 45% KW-SYN 87 67 77% TR-EXACT 29 23 79% TR-SUB 152 81 53% RS-EXACT 53 24 45% RS-SUB 196 81 41% DIRT 159 94 59% USP-MLN 334 295 88% USP-BAYES 325 259 80% Table 2: Performance on the QA task. question, the second argument is returned as an answer. The systems include TextRunner (TR) (Banko et al., 2007), RESOLVER (RS) (Yates and Etzioni, 2009) and DIRT (Lin and Pantel, 2001). The EXACT versions of the methods return answers when they match the question argument exactly, and the SUB versions produce answers containing the question argument as a substring. Similarly to the MLN system (USP-MLN), we generate answers as follows. We use our trained model to parse a question, i.e. recursively decompose it into lexical items and assign them to semantic classes induced at training. Using this semantic representation, we look for the type of an argument missing in the question, which, if found, is rep</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl E Rasmussen</author>
</authors>
<title>The infinite hidden markov model.</title>
<date>2002</date>
<booktitle>In Machine Learning,</booktitle>
<pages>29--245</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17890" citStr="Beal et al., 2002" startWordPosition="2802" endWordPosition="2805">ery draw, when β = 0 the PY process reduces to DP. The expected value of K scales as O(αnβ) with the number of draws n, while it scales only logarithmically for DP processes. PY processes are expected to be more appropriate for many NLP problems, as they model power-law type distributions common for natural language (Teh, 2006). Hierarchical Dirichlet Processes (HDP) or hierarchical PY processes are used if the goal is to draw several related probability measures for the same set S. For example, they can be used to generate transition distributions of a Markov model, HDPHMM (Teh et al., 2006; Beal et al., 2002). For such a HMM, the top-level state proportions are drawn from the top-level stick breaking construction γ — GEM(α, β), and then the individual transition distributions for every state z = 1, 2,... φz are drawn from PY (γ, α&apos;, β&apos;). The parameters α&apos; and β&apos; control how similar the individual transition distributions φz are to the top-level state proportions γ, or, equivalently, how similar the transition distributions are to each other. 5 A Model for Semantic Parsing Our model of semantics associates with each semantic class a set of distributions which govern the generation of corresponding </context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2002</marker>
<rawString>Matthew J. Beal, Zoubin Ghahramani, and Carl E. Rasmussen. 2002. The infinite hidden markov model. In Machine Learning, pages 29–245. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blackwell</author>
<author>James B MacQueen</author>
</authors>
<title>Ferguson distributions via polya urn schemes.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>Blackwell, MacQueen, 1973</marker>
<rawString>David Blackwell and James B. MacQueen. 1973. Ferguson distributions via polya urn schemes. The Annals of Statistics, 1(2):353–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Natural Language Learning, CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of the 9th Conference on Natural Language Learning, CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</booktitle>
<contexts>
<context position="36541" citStr="Chambers and Jurafsky, 2009" startWordPosition="5900" endWordPosition="5903">ervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated predicates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). However, their approach is quite different from our Bayesian model as it relies on similarity functions. Though in this work we focus solely on the unsupervised setting, there has been some successful work on semi-supervised semantic-role labeling, including the Framenet version of the problem (F¨urstenau and Lapata, 2009). Their method exploits graph alignments between labeled and unlabeled examples, and, therefore, crucially relies on the availability of labeled examples. 9 Conclusions and Future Work In this work, we introduced a non-parametric Bayesian model for the semantic parsing prob</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proc. of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="35642" citStr="Clarke et al., 2010" startWordPosition="5752" endWordPosition="5755">method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only w</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate tree-substitution grammars.</title>
<date>2009</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>548--556</pages>
<contexts>
<context position="19677" citStr="Cohn et al., 2009" startWordPosition="3092" endWordPosition="3095">esented in Figure 2. We associate with each semantic class c, c = 1, 2, ... , a distribution of its syntactic realizations φc. For example, for the frame WinPrize illustrated in Figure 1 this distribution would concentrate at syntactic fragments corresponding to lexical items “defeated”, “secured the victory” and “won”. The distribution is drawn from DP(w(C), H(C)), where H(C) is a base measure over syntactic subtrees. We use a simple generative process to define the probability of a subtree, the underlying model is similar to the base measures used in the Bayesian tree-substitution grammars (Cohn et al., 2009). We start by generating a word w uniformly from the treebank distribution, then we decide on the number of dependents of w using the geometric distribution Geom(q(C)). For every dependent we generate a dependency relation r and a lexical form w&apos; from P(rjw)P(w&apos;jr), where probabilities P are based on add-0.1 smoothed treebank counts. The process is continued recursively. The smaller the parameter q(C), the lower is the probability assigned to larger sub-trees. Parameters ψc,t and ψ+c,t, t = 1, ... , T, define a distribution over vectors (m1, m2, ... , mT) where mt is the number of times an arg</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In HLT-NAACL, pages 548–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David B Dahl</author>
</authors>
<title>An improved merge-split sampler for conjugate dirichlet process mixture models.</title>
<date>2003</date>
<tech>Technical Report 1086,</tech>
<institution>Department of Statistics, University of Wiscosin - Madison,</institution>
<contexts>
<context position="6656" citStr="Dahl, 2003" startWordPosition="985" endWordPosition="986">inct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1, and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task. In both cases, we</context>
<context position="24419" citStr="Dahl, 2003" startWordPosition="3897" endWordPosition="3898">generating a syntactic arc a,,t, choosing a class as its filler c�,t and, finally, recursing. 6 Inference In our model, latent states, modeled with hierarchical PY processes, correspond to distinct semantic classes and, therefore, their number is expected to be very large for any reasonable model of semantics. As a result, many standard inference techniques, such as Gibbs sampling, or the structured mean-field method are unlikely to result in tractable inference. One of the standard and most efficient samplers for non-hierarchical PY processes are split-merge MH samplers (Jain and Neal, 2000; Dahl, 2003). In this section we explain how split-merge samplers can be applied to our model. 6.1 Split and Merge Moves On each move, split-merge samplers decide either to merge two states into one (in our case, merge two semantic classes), or split one state into two. These moves can be computed efficiently for our model of semantics. Note that for any reasonable model of semantics only a small subset of the entire set of semantic classes can be used as an argument for some fixed semantic class due to selectional preferences exhibited by predicates. For instance, only teams or players can fill arguments</context>
<context position="26839" citStr="Dahl, 2003" startWordPosition="4290" endWordPosition="4291">e merged with other classes, such as the ones containing syntactic realizations “defeat” or “win”. Conversely, if both randomly-selected syntactic fragments are already composed in the corresponding class, we attempt to split them. 6.3 Role-Syntax Alignment Move Merge, compose and decompose moves require recomputation of mapping between argument types (semantic roles) and syntactic fragments. Computing the best statistical mapping is infeasible and proposing a random mapping will result in many attempted moves being rejected. Instead we use a greedy randomized search method called Gibbs scan (Dahl, 2003). Though it is a part of the above 3 moves, this alignment move is also used on its own to induce semantic arguments for classes (frames) with a single syntactic realization. The Gibbs scan procedure is also used during the split move to select one of the newly introduced classes for each considered syntactic fragment. 6.4 Informed Proposals Since the number of classes is very large, selecting examples at random would result in a relatively low proportion of moves getting accepted, and, consequently, in a slow-mixing Markov chain. Instead of selecting both class occurrences uniformly, we selec</context>
</contexts>
<marker>Dahl, 2003</marker>
<rawString>David B. Dahl. 2003. An improved merge-split sampler for conjugate dirichlet process mixture models. Technical Report 1086, Department of Statistics, University of Wiscosin - Madison, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12859" citStr="Eisenstein et al., 2009" startWordPosition="1982" endWordPosition="1985">lusterization moves such that the resulting model also obeys both of these constraints. Third, as in (Poon and Domingos, 2009), we do not model polysemy as we assume 1Semantic classes correspond to lambda-form clusters in (Poon and Domingos, 2009) terminology. 1447 that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009; Poon and Domingos, 2009), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009)), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simp</context>
</contexts>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="16220" citStr="Ferguson, 1973" startWordPosition="2511" endWordPosition="2512">ntactic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, simplify application of non-parametric Bayesian techniques and expedite the use of inference techniques designed specifically for directed models. Our approach makes one step in this direction by proposing a non-parametric version of such generative model. 4 Hierarchical Pitman-Yor Processes The central component of our non-parametric Bayesian model are Pitman-Yor (PY) processes, which are a generalization of the Dirichlet processes (DPs) (Ferguson, 1973). We use PY processes to model distributions of semantic classes appearing as an argument of other semantic classes. We also use them to model distributions of syntactic realizations for each semantic class and distributions of syntactic dependency arcs for argument types. In this section we present relevant background on PY processes. For a more detailed consideration we refer the reader to (Teh et al., 2006). The Pitman-Yor process over a set S, denoted PY (α, Q, H), is a stochastic process whose samples G0 constitute probability measures on partitions of S. In practice, we do not need to dr</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R L Petruck</author>
</authors>
<title>Background to framenet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--235</pages>
<contexts>
<context position="9039" citStr="Fillmore et al., 2003" startWordPosition="1356" endWordPosition="1359">ational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009). In frame semantics, the meaning of a predicate is conveyed by a frame, a structure of related concepts that describes a situation, its participants and properties (Fillmore et al., 2003). Each frame is characterized by a set of semantic roles (frame elements) corresponding to the arguments of the predicate. It is evoked by a frame evoking element (a predicate). The same frame can be evoked by different but semantically similar predicates: for example, both verbs “buy” and “purchase” evoke frame Commerce buy in FrameNet (Fillmore et al., 2003). The aim of the semantic role labeling task is to identify all of the frames evoked in a sentence and label their semantic role fillers. We extend this task and treat semantic parsing as recursive prediction of predicate-argument structu</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C. J. Fillmore, C. R. Johnson, and M. R. L. Petruck. 2003. Background to framenet. International Journal of Lexicography, 16:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Graph alignment for semi-supervised semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>Hagen F¨urstenau and Mirella Lapata. 2009. Graph alignment for semi-supervised semantic role labeling. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CONLL-05),</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1357" citStr="Ge and Mooney, 2005" startWordPosition="186" endWordPosition="189">ns of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin an</context>
<context position="35438" citStr="Ge and Mooney, 2005" startWordPosition="5719" endWordPosition="5722">usters have clear semantic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then use</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CONLL-05), Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labelling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="8324" citStr="Gildea and Jurafsky, 2002" startWordPosition="1245" endWordPosition="1248"> the Pitman-Yor processes, respectively. In Sections 5 and 6, we describe our model and the inference method. Section 7 provides both qualitative and quantitative evaluation. Finally, ad1446 ditional related work is presented in Section 8. 2 Semantic Parsing In this section, we briefly define the unsupervised semantic parsing task and underlying aspects and assumptions relevant to our model. Unlike (Poon and Domingos, 2009), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002). The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009). In frame semantics, the meaning of a predicate is conveyed by a frame, </context>
<context position="35386" citStr="Gildea and Jurafsky, 2002" startWordPosition="5710" endWordPosition="5713"> 6, 8 and 12 (Figure 1), respectively. Though all these clusters have clear semantic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labelling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proc. of the Meeting of Association for Computational Linguistics (ACL),</booktitle>
<location>Portland, OR, USA.</location>
<contexts>
<context position="35839" citStr="Goldwasser et al., 2011" startWordPosition="5784" endWordPosition="5787">irly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated predicates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting has als</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proc. of the Meeting of Association for Computational Linguistics (ACL), Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christoph Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="36177" citStr="Grenager and Manning, 2006" startWordPosition="5842" endWordPosition="5845">Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated predicates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). However, their approach is quite different from our Bayesian model as it relies on similarity functions. Though in this work we focus solely on the unsupervised setting, there has been some successful work on semi-supervised semantic-</context>
<context position="37798" citStr="Grenager and Manning, 2006" startWordPosition="6093" endWordPosition="6096">itman-Yor process. The model defines a generative story for recursive generation of lexical items, syntactic and semantic structures. We extend the split-merge MH sampling algorithm to include composition-decomposition moves, and exploit the properties of our task to make it efficient in the hierarchical setting we consider. We plan to explore at least two directions in our future work. First, we would like to relax some of unrealistic assumptions made in our model: for example, proper modeling of alterations requires joint generation of syntactic realizations for predicateargument relations (Grenager and Manning, 2006; Lang and Lapata, 2010), similarly, proper modeling of nominalization implies support of arguments not immediately local in the syntactic structure. The second general direction is the use of the unsupervised methods we propose to expand the coverage of existing semantic resources, which typically require substantial human effort to produce. Acknowledgements The authors acknowledge the support of the MMCI Cluster of Excellence, and thank Chris Callison-Burch, Alexis Palmer, Caroline Sporleder, Ben Van Durme and the anonymous reviewers for their helpful comments and suggestions. 1453 Reference</context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christoph Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonia Jain</author>
<author>Radford Neal</author>
</authors>
<title>A split-merge markov chain monte carlo procedure for the dirichlet process mixture model.</title>
<date>2000</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<pages>13--158</pages>
<contexts>
<context position="6643" citStr="Jain and Neal, 2000" startWordPosition="981" endWordPosition="984">l induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1, and also include the argument role-syntax alignment move which attempts to improve mapping between semantic roles and syntactic paths for some fixed predicate. Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, examining the revealed clustering of syntactic structures, and quantitatively, on a question answering task. In b</context>
<context position="24406" citStr="Jain and Neal, 2000" startWordPosition="3893" endWordPosition="3896">enArgument) by first generating a syntactic arc a,,t, choosing a class as its filler c�,t and, finally, recursing. 6 Inference In our model, latent states, modeled with hierarchical PY processes, correspond to distinct semantic classes and, therefore, their number is expected to be very large for any reasonable model of semantics. As a result, many standard inference techniques, such as Gibbs sampling, or the structured mean-field method are unlikely to result in tractable inference. One of the standard and most efficient samplers for non-hierarchical PY processes are split-merge MH samplers (Jain and Neal, 2000; Dahl, 2003). In this section we explain how split-merge samplers can be applied to our model. 6.1 Split and Merge Moves On each move, split-merge samplers decide either to merge two states into one (in our case, merge two semantic classes), or split one state into two. These moves can be computed efficiently for our model of semantics. Note that for any reasonable model of semantics only a small subset of the entire set of semantic classes can be used as an argument for some fixed semantic class due to selectional preferences exhibited by predicates. For instance, only teams or players can f</context>
</contexts>
<marker>Jain, Neal, 2000</marker>
<rawString>Sonia Jain and Radford Neal. 2000. A split-merge markov chain monte carlo procedure for the dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13:158–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="5587" citStr="Johnson et al., 2007" startWordPosition="813" endWordPosition="817">models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007), or the number of adapted productions in the adaptor grammar (Johnson et al., 2007)) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inference methods for hierarchi</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambigous supervision.</title>
<date>2007</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>895--900</pages>
<contexts>
<context position="35571" citStr="Kate and Mooney, 2007" startWordPosition="5740" endWordPosition="5743">cer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning,</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambigous supervision. In Association for the Advancement of Artificial Intelligence (AAAI), pages 895–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>182</pages>
<contexts>
<context position="29030" citStr="Kim et al., 2003" startWordPosition="4661" endWordPosition="4664">airs of the form (head word, dependency type) and (dependent, dependency type). So, instead of selecting a word occurrence uniformly, each occurrence of every word w2 is weighted by its similarity to w1, where the similarity is based on the cosine distance. As the moves are dependent only on syntactic representations, all the proposal distributions can be computed once at the initialization stage.4 7 Empirical Evaluation We induced a semantic representation over a collection of texts and evaluated it by answering questions about the knowledge contained in the corpus. We used the GENIA corpus (Kim et al., 2003), a dataset of 1999 biomedical abstracts, and a set of questions produced by (Poon and Domingos, 2009). A example question is shown in Figure 3. All model hyperpriors were set to maximize the posterior, except for w(A) and w(C), which were set to 1.e −10 and 1.e − 35, respectively. Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%. Let us examine some of the induced semantic classes (Table 1) before turning to the question answering task. Almost all of the clustered syntactic 4In order to minimize memory usage, w</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2003. Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics, 19:i180– i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised induction of semantic roles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2491" citStr="Lang and Lapata, 2010" startWordPosition="355" endWordPosition="358">n has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailment or question answering tasks, as an entailm</context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>Joel Lang and Mirella Lapata. 2010. Unsupervised induction of semantic roles. In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical dirichlet processes.</title>
<date>2007</date>
<booktitle>In Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>688--697</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5564" citStr="Liang et al., 2007" startWordPosition="809" endWordPosition="812">nalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been used in NLP, for example, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007), or the number of adapted productions in the adaptor grammar (Johnson et al., 2007)) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inferenc</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 688–697, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP).</booktitle>
<contexts>
<context position="1476" citStr="Liang et al., 2009" startWordPosition="204" endWordPosition="207">dification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations</context>
<context position="13118" citStr="Liang et al., 2009" startWordPosition="2026" endWordPosition="2029">47 that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009; Poon and Domingos, 2009), we assume that dependency structures are provided for every sentence. This assumption allows us to construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009)), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simple generative story and build on top of the recent success in syntactic parsing. 3 Relation to the MLN Approach The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Net</context>
<context position="35591" citStr="Liang et al., 2009" startWordPosition="5744" endWordPosition="5747">tively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, the</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proc. of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt – discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proc. of International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="1971" citStr="Lin and Pantel, 2001" startWordPosition="279" endWordPosition="282">, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not </context>
<context position="32445" citStr="Lin and Pantel, 2001" startWordPosition="5227" endWordPosition="5230">information to extract the subject or the object of the verb. Other baselines are based on state-of-the-art relation extraction systems. When the extracted relation and one of the arguments match those in a given Total Correct Accuracy KW 150 67 45% KW-SYN 87 67 77% TR-EXACT 29 23 79% TR-SUB 152 81 53% RS-EXACT 53 24 45% RS-SUB 196 81 41% DIRT 159 94 59% USP-MLN 334 295 88% USP-BAYES 325 259 80% Table 2: Performance on the QA task. question, the second argument is returned as an answer. The systems include TextRunner (TR) (Banko et al., 2007), RESOLVER (RS) (Yates and Etzioni, 2009) and DIRT (Lin and Pantel, 2001). The EXACT versions of the methods return answers when they match the question argument exactly, and the SUB versions produce answers containing the question argument as a substring. Similarly to the MLN system (USP-MLN), we generate answers as follows. We use our trained model to parse a question, i.e. recursively decompose it into lexical items and assign them to semantic classes induced at training. Using this semantic representation, we look for the type of an argument missing in the question, which, if found, is reported as an answer. It is clear that overly coarse clusters of argument f</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt – discovery of inference rules from text. In Proc. of International Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>982--991</pages>
<contexts>
<context position="1372" citStr="Mooney, 2007" startWordPosition="190" endWordPosition="191">those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001;</context>
<context position="35453" citStr="Mooney, 2007" startWordPosition="5723" endWordPosition="5724">antic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap </context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>Raymond J. Mooney. 2007. Learning for semantic parsing. In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing, pages 982–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Palmer</author>
<author>Caroline Sporleder</author>
</authors>
<title>Evaluating framenet-style semantic parsing: the role of coverage gaps in framenet.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics (COLING-2000),</booktitle>
<location>Beijing.</location>
<contexts>
<context position="1729" citStr="Palmer and Sporleder, 2010" startWordPosition="245" endWordPosition="248">medical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations</context>
</contexts>
<marker>Palmer, Sporleder, 2010</marker>
<rawString>Alexis Palmer and Caroline Sporleder. 2010. Evaluating framenet-style semantic parsing: the role of coverage gaps in framenet. In Proceedings of the Conference on Computational Linguistics (COLING-2000), Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
</authors>
<title>Poisson-dirichlet and gem invariant distributions for split-and-merge transformations of an interval partition.</title>
<date>2002</date>
<booktitle>Combinatorics, Probability and Computing,</booktitle>
<pages>11--501</pages>
<contexts>
<context position="4564" citStr="Pitman, 2002" startWordPosition="659" endWordPosition="660"> structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010). In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S´eaghdha, 2010), and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reli</context>
</contexts>
<marker>Pitman, 2002</marker>
<rawString>Jim Pitman. 2002. Poisson-dirichlet and gem invariant distributions for split-and-merge transformations of an interval partition. Combinatorics, Probability and Computing, 11:501–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="2537" citStr="Poon and Domingos, 2009" startWordPosition="363" endWordPosition="366">upervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailment or question answering tasks, as an entailment model can abstract away from specifics of </context>
<context position="4883" citStr="Poon and Domingos, 2009" startWordPosition="703" endWordPosition="706">e (Ritter et al., 2010; S´eaghdha, 2010), and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to encode prior linguistic knowledge in the form of hyperpriors and the potential for more reliable modeling of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3. Hierarchical Pitman-Yor processes (or their special case, hierarchical Dirichlet processes) have previously been </context>
<context position="6126" citStr="Poon and Domingos, 2009" startWordPosition="905" endWordPosition="908">ample, in the context of syntactic parsing (Liang et al., 2007; Johnson et al., 2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007), or the number of adapted productions in the adaptor grammar (Johnson et al., 2007)) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inference methods for hierarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for example in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient inference tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend t</context>
<context position="8125" citStr="Poon and Domingos, 2009" startWordPosition="1210" endWordPosition="1213">ble to those of the MLN model. The rest of the paper is structured as follows. Section 2 begins with a definition of the semantic parsing task. Sections 3 and 4 give background on the MLN model and the Pitman-Yor processes, respectively. In Sections 5 and 6, we describe our model and the inference method. Section 7 provides both qualitative and quantitative evaluation. Finally, ad1446 ditional related work is presented in Section 8. 2 Semantic Parsing In this section, we briefly define the unsupervised semantic parsing task and underlying aspects and assumptions relevant to our model. Unlike (Poon and Domingos, 2009), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002). The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and convenient to relate our method to the previous work. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logica</context>
<context position="12362" citStr="Poon and Domingos, 2009" startWordPosition="1898" endWordPosition="1902">e a semantic argument of another one only if they are connected by an arc in the dependency tree. This is a slight simplification of the semantic role labeling problem but one often made. Thus, the argument identification and labeling stages consist of labeling each syntactic arc with a semantic role label. In comparison, the MLN model does not explicitly assume contiguity of lexical items and does not make this directionality assumption but their clustering algorithm uses initialization and clusterization moves such that the resulting model also obeys both of these constraints. Third, as in (Poon and Domingos, 2009), we do not model polysemy as we assume 1Semantic classes correspond to lambda-form clusters in (Poon and Domingos, 2009) terminology. 1447 that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments. As in some of the recent work on learning semantic representations (Eisenstein et al., 2009; Poon and Domingos, 2009), we assume that dependency structures are provided for every sentence. This </context>
<context position="13608" citStr="Poon and Domingos, 2009" startWordPosition="2110" endWordPosition="2113">o construct models of semantics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009)), but rather Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simple generative story and build on top of the recent success in syntactic parsing. 3 Relation to the MLN Approach The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic re</context>
<context position="29132" citStr="Poon and Domingos, 2009" startWordPosition="4678" endWordPosition="4681">selecting a word occurrence uniformly, each occurrence of every word w2 is weighted by its similarity to w1, where the similarity is based on the cosine distance. As the moves are dependent only on syntactic representations, all the proposal distributions can be computed once at the initialization stage.4 7 Empirical Evaluation We induced a semantic representation over a collection of texts and evaluated it by answering questions about the knowledge contained in the corpus. We used the GENIA corpus (Kim et al., 2003), a dataset of 1999 biomedical abstracts, and a set of questions produced by (Poon and Domingos, 2009). A example question is shown in Figure 3. All model hyperpriors were set to maximize the posterior, except for w(A) and w(C), which were set to 1.e −10 and 1.e − 35, respectively. Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%. Let us examine some of the induced semantic classes (Table 1) before turning to the question answering task. Almost all of the clustered syntactic 4In order to minimize memory usage, we used frequency cut-off of 10. For split-merge moves, we select words based on the cosine distance if</context>
<context position="31558" citStr="Poon and Domingos, 2009" startWordPosition="5064" endWordPosition="5067">erbs in class 9 are used in the context of providing support for a finding or an action, and many of them are listed as evoking elements for the Evidence frame in FrameNet. Argument types of the induced classes also show a tendency to correspond to semantic roles. For example, an argument type of class 2 is modeled as a distribution over two argument parts, prep of and prep from. The corresponding arguments define the origin of the cells (transgenic mouse, smoker, volunteer, donor, ... ). We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in (Poon and Domingos, 2009). The first set of baselines looks for answers by attempting to match a verb and its argument in the question with the input text. The first version (KW) simply returns the rest of the sentence on the other side of the verb, while the second (KW-SYN) uses syntactic information to extract the subject or the object of the verb. Other baselines are based on state-of-the-art relation extraction systems. When the extracted relation and one of the arguments match those in a given Total Correct Accuracy KW 150 67 45% KW-SYN 87 67 77% TR-EXACT 29 23 79% TR-SUB 152 81 53% RS-EXACT 53 24 45% RS-SUB 196 </context>
<context position="35763" citStr="Poon and Domingos, 2009" startWordPosition="5772" endWordPosition="5776">s, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated predicates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameter</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, (EMNLP-09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="13762" citStr="Richardson and Domingos, 2006" startWordPosition="2132" endWordPosition="2135">Markovian within a dependency tree. Though we include generation of the syntactic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are oversimplistic. In this way, we can use a simple generative story and build on top of the recent success in syntactic parsing. 3 Relation to the MLN Approach The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. For each sentence, the MLN induces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses. The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (semantic representation), however, unsupervised learning of semantics with general MLNs can be prohibitively expensive. The reason for this is that MLNs are undirect</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matt Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="4281" citStr="Ritter et al., 2010" startWordPosition="617" endWordPosition="620">ctory over Winner subj WinPrize dobj Opponent pp_over Steelers nmod the Pittsburgh team Figure 1: An example of two different syntactic trees with a common semantic representation WinPrize(Ravens, Steelers). From the statistical modeling point of view, joint learning of predicate-argument structure and discovery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010). In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S´eaghdha, 2010), and can even be regarded as their recursive and non-parametric extension. In this paper, we adopt the above definition of unsupervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and argument clusters, as well as distributions over syntactic and lexical realizations of each cluster. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 200</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Swier</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>95--102</pages>
<location>Barcelona,</location>
<contexts>
<context position="2447" citStr="Swier and Stevenson, 2004" startWordPosition="347" endWordPosition="350"> the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expressions. For example, for a pair of sentences on Figure 1, in addition to inducing predicate-argument structure, they aim to assign expressions “Steelers” and “the Pittsburgh team” to the same semantic class Steelers, and group expressions “defeated” and “secured the victory over”. Such semantic representation can be useful for entailmen</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>R. Swier and S. Stevenson. 2004. Unsupervised semantic role labelling. In Proceedings of EMNLP, pages 95–102, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="16633" citStr="Teh et al., 2006" startWordPosition="2574" endWordPosition="2577">. 4 Hierarchical Pitman-Yor Processes The central component of our non-parametric Bayesian model are Pitman-Yor (PY) processes, which are a generalization of the Dirichlet processes (DPs) (Ferguson, 1973). We use PY processes to model distributions of semantic classes appearing as an argument of other semantic classes. We also use them to model distributions of syntactic realizations for each semantic class and distributions of syntactic dependency arcs for argument types. In this section we present relevant background on PY processes. For a more detailed consideration we refer the reader to (Teh et al., 2006). The Pitman-Yor process over a set S, denoted PY (α, Q, H), is a stochastic process whose samples G0 constitute probability measures on partitions of S. In practice, we do not need to draw measures, as they can be analytically marginalized out. The conditional distribution of xj+1 given the previous j draws, with G0 marginalized out, follows (Black1448 well and MacQueen, 1973) where φ1, ... , φK are K values assigned to x1, x2, ... , xj. The number of times φk was assigned is denoted jk, so that j = EKk=1 jk. The parameter β &lt; 1 controls how heavy the tail of the distribution is: when it appr</context>
<context position="17870" citStr="Teh et al., 2006" startWordPosition="2798" endWordPosition="2801"> is assigned to every draw, when β = 0 the PY process reduces to DP. The expected value of K scales as O(αnβ) with the number of draws n, while it scales only logarithmically for DP processes. PY processes are expected to be more appropriate for many NLP problems, as they model power-law type distributions common for natural language (Teh, 2006). Hierarchical Dirichlet Processes (HDP) or hierarchical PY processes are used if the goal is to draw several related probability measures for the same set S. For example, they can be used to generate transition distributions of a Markov model, HDPHMM (Teh et al., 2006; Beal et al., 2002). For such a HMM, the top-level state proportions are drawn from the top-level stick breaking construction γ — GEM(α, β), and then the individual transition distributions for every state z = 1, 2,... φz are drawn from PY (γ, α&apos;, β&apos;). The parameters α&apos; and β&apos; control how similar the individual transition distributions φz are to the top-level state proportions γ, or, equivalently, how similar the transition distributions are to each other. 5 A Model for Semantic Parsing Our model of semantics associates with each semantic class a set of distributions which govern the generati</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="17601" citStr="Teh, 2006" startWordPosition="2754" endWordPosition="2755">ll and MacQueen, 1973) where φ1, ... , φK are K values assigned to x1, x2, ... , xj. The number of times φk was assigned is denoted jk, so that j = EKk=1 jk. The parameter β &lt; 1 controls how heavy the tail of the distribution is: when it approaches 1, a new value is assigned to every draw, when β = 0 the PY process reduces to DP. The expected value of K scales as O(αnβ) with the number of draws n, while it scales only logarithmically for DP processes. PY processes are expected to be more appropriate for many NLP problems, as they model power-law type distributions common for natural language (Teh, 2006). Hierarchical Dirichlet Processes (HDP) or hierarchical PY processes are used if the goal is to draw several related probability measures for the same set S. For example, they can be used to generate transition distributions of a Markov model, HDPHMM (Teh et al., 2006; Beal et al., 2002). For such a HMM, the top-level state proportions are drawn from the top-level stick breaking construction γ — GEM(α, β), and then the individual transition distributions for every state z = 1, 2,... φz are drawn from PY (γ, α&apos;, β&apos;). The parameters α&apos; and β&apos; control how similar the individual transition distri</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985– 992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative model for semantic role labeling. In</title>
<date>2003</date>
<booktitle>In Senseval-3,</booktitle>
<pages>397--408</pages>
<contexts>
<context position="36344" citStr="Thompson et al., 2003" startWordPosition="5870" endWordPosition="5873">o the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated predicates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the parameters were estimated from fully annotated data. The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). However, their approach is quite different from our Bayesian model as it relies on similarity functions. Though in this work we focus solely on the unsupervised setting, there has been some successful work on semi-supervised semantic-role labeling, including the Framenet version of the problem (F¨urstenau and Lapata, 2009). Their method exploits graph alignments between labeled and unlabeled exampl</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>Cynthia A. Thompson, Roger Levy, and Christopher D. Manning. 2003. A generative model for semantic role labeling. In In Senseval-3, pages 397–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Mikhail Kozhevnikov</author>
</authors>
<title>Bootstrapping semantic analyzers from non-contradictory texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="35620" citStr="Titov and Kozhevnikov, 2010" startWordPosition="5748" endWordPosition="5751"> to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self-training is then used to bootstrap the model. Unsupervised semantic role labeling with a generative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover </context>
</contexts>
<marker>Titov, Kozhevnikov, 2010</marker>
<rawString>Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrapping semantic analyzers from non-contradictory texts. In Proceedings of the 48rd Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>34--255</pages>
<contexts>
<context position="1997" citStr="Yates and Etzioni, 2009" startWordPosition="283" endWordPosition="286"> others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-</context>
<context position="11233" citStr="Yates and Etzioni, 2009" startWordPosition="1712" endWordPosition="1715">n see from the above description, frames (which groups predicates with similar meaning such as the WinPrize frame in our example) and clusters of argument fillers (Ravens and Steelers) are treated in our definition in a similar way. For convenience, we will refer to both types of clusters as semantic classes.1 This definition of semantic parsing is closely related to a realistic relation extraction setting, as both clustering of syntactic forms of relations (or extraction patterns) and clustering of argument fillers for these relations is crucial for automatic construction of knowledge bases (Yates and Etzioni, 2009). In this paper, we make three assumptions. First, we assume that each lexical item corresponds to a subtree of the syntactic dependency graph of the sentence. This assumption is similar to the adjacency assumption in (Zettlemoyer and Collins, 2005), though ours may be more appropriate for languages with free or semi-free word order, where syntactic structures are inherently non-projective. Second, we assume that the semantic arguments are local in the dependency tree; that is, one lexical item can be a semantic argument of another one only if they are connected by an arc in the dependency tre</context>
<context position="32413" citStr="Yates and Etzioni, 2009" startWordPosition="5221" endWordPosition="5224">the second (KW-SYN) uses syntactic information to extract the subject or the object of the verb. Other baselines are based on state-of-the-art relation extraction systems. When the extracted relation and one of the arguments match those in a given Total Correct Accuracy KW 150 67 45% KW-SYN 87 67 77% TR-EXACT 29 23 79% TR-SUB 152 81 53% RS-EXACT 53 24 45% RS-SUB 196 81 41% DIRT 159 94 59% USP-MLN 334 295 88% USP-BAYES 325 259 80% Table 2: Performance on the QA task. question, the second argument is returned as an answer. The systems include TextRunner (TR) (Banko et al., 2007), RESOLVER (RS) (Yates and Etzioni, 2009) and DIRT (Lin and Pantel, 2001). The EXACT versions of the methods return answers when they match the question argument exactly, and the SUB versions produce answers containing the question argument as a substring. Similarly to the MLN system (USP-MLN), we generate answers as follows. We use our trained model to parse a question, i.e. recursively decompose it into lexical items and assign them to semantic classes induced at training. Using this semantic representation, we look for the type of an argument missing in the question, which, if found, is reported as an answer. It is clear that over</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal ofArtificial Intelligence Research, 34:255–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zapirain</author>
<author>E Agirre</author>
<author>L L M`arquez</author>
<author>M Surdeanu</author>
</authors>
<title>Improving semantic role classification with selectional prefrences.</title>
<date>2010</date>
<booktitle>In Proceedings of the Meeting of the North American chapter of the Association for Computational Linguistics (NAACL</booktitle>
<location>Los Angeles.</location>
<marker>Zapirain, Agirre, M`arquez, Surdeanu, 2010</marker>
<rawString>B. Zapirain, E. Agirre, L. L. M`arquez, and M. Surdeanu. 2010. Improving semantic role classification with selectional prefrences. In Proceedings of the Meeting of the North American chapter of the Association for Computational Linguistics (NAACL 2010), Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty-first Conference on Uncertainty in Artificial Intelligence,</booktitle>
<location>Edinburgh, UK,</location>
<contexts>
<context position="1336" citStr="Zettlemoyer and Collins, 2005" startWordPosition="182" endWordPosition="185">s between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain. 1 Introduction Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Carreras and M`arquez, 2005; Liang et al., 2009). However, most of this research has concentrated on supervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), motivating the need for unsupervised or semi-supervised techniques. Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see,</context>
<context position="11482" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1752" endWordPosition="1755">ience, we will refer to both types of clusters as semantic classes.1 This definition of semantic parsing is closely related to a realistic relation extraction setting, as both clustering of syntactic forms of relations (or extraction patterns) and clustering of argument fillers for these relations is crucial for automatic construction of knowledge bases (Yates and Etzioni, 2009). In this paper, we make three assumptions. First, we assume that each lexical item corresponds to a subtree of the syntactic dependency graph of the sentence. This assumption is similar to the adjacency assumption in (Zettlemoyer and Collins, 2005), though ours may be more appropriate for languages with free or semi-free word order, where syntactic structures are inherently non-projective. Second, we assume that the semantic arguments are local in the dependency tree; that is, one lexical item can be a semantic argument of another one only if they are connected by an arc in the dependency tree. This is a slight simplification of the semantic role labeling problem but one often made. Thus, the argument identification and labeling stages consist of labeling each syntactic arc with a semantic role label. In comparison, the MLN model does n</context>
<context position="35417" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5714" endWordPosition="5718">spectively. Though all these clusters have clear semantic interpretation (white blood cells, predicates corresponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our experiments. Though it is likely that tuning and different heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. 8 Related Work There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. In addition to the MLN model (Poon and Domingos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small number of examples for every logical symbol. A form of self</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammar. In Proceedings of the Twenty-first Conference on Uncertainty in Artificial Intelligence, Edinburgh, UK, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>