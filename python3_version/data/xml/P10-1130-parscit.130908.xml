<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.953289">
Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing
</title>
<author confidence="0.998139">
Valentin I. Spitkovsky Daniel Jurafsky
</author>
<affiliation confidence="0.994021">
Computer Science Department Departments of Linguistics and
Stanford University and Google Inc. Computer Science, Stanford University
</affiliation>
<email confidence="0.992055">
valentin@google.com jurafsky@stanford.edu
</email>
<author confidence="0.855755">
Hiyan Alshawi
</author>
<affiliation confidence="0.79681">
Google Inc.
</affiliation>
<email confidence="0.996789">
hiyan@google.com
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980666666667">
We show how web mark-up can be used
to improve unsupervised dependency pars-
ing. Starting from raw bracketings of four
common HTML tags (anchors, bold, ital-
ics and underlines), we refine approximate
partial phrase boundaries to yield accurate
parsing constraints. Conversion proce-
dures fall out of our linguistic analysis of
a newly available million-word hyper-text
corpus. We demonstrate that derived con-
straints aid grammar induction by training
Klein and Manning’s Dependency Model
with Valence (DMV) on this data set: pars-
ing accuracy on Section 23 (all sentences)
of the Wall Street Journal corpus jumps
to 50.4%, beating previous state-of-the-
art by more than 5%. Web-scale exper-
iments show that the DMV, perhaps be-
cause it is unlexicalized, does not benefit
from orders of magnitude more annotated
but noisier data. Our model, trained on a
single blog, generalizes to 53.3% accuracy
out-of-domain, against the Brown corpus
— nearly 10% higher than the previous
published best. The fact that web mark-up
strongly correlates with syntactic structure
may have broad applicability in NLP.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998055384615385">
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is
a hard problem whose eventual solution promises
to benefit applications ranging from question an-
swering to speech recognition and machine trans-
lation. A restricted version of this problem that tar-
gets dependencies and assumes partial annotation
— sentence boundaries and part-of-speech (POS)
tagging — has received much attention. Klein
and Manning (2004) were the first to beat a sim-
ple parsing heuristic, the right-branching baseline;
today’s state-of-the-art systems (Headden et al.,
2009; Cohen and Smith, 2009; Spitkovsky et al.,
2010a) are rooted in their Dependency Model with
Valence (DMV), still trained using variants of EM.
Pereira and Schabes (1992) outlined three ma-
jor problems with classic EM, applied to a related
problem, constituent parsing. They extended clas-
sic inside-outside re-estimation (Baker, 1979) to
respect any bracketing constraints included with
a training corpus. This conditioning on partial
parses addressed all three problems, leading to:
(i) linguistically reasonable constituent boundaries
and induced grammars more likely to agree with
qualitative judgments of sentence structure, which
is underdetermined by unannotated text; (ii) fewer
iterations needed to reach a good grammar, coun-
tering convergence properties that sharply deterio-
rate with the number of non-terminal symbols, due
to a proliferation of local maxima; and (iii) better
(in the best case, linear) time complexity per it-
eration, versus running time that is ordinarily cu-
bic in both sentence length and the total num-
ber of non-terminals, rendering sufficiently large
grammars computationally impractical. Their al-
gorithm sometimes found good solutions from
bracketed corpora but not from raw text, sup-
porting the view that purely unsupervised, self-
organizing inference methods can miss the trees
for the forest of distributional regularities. This
was a promising break-through, but the problem
of whence to get partial bracketings was left open.
We suggest mining partial bracketings from a
cheap and abundant natural language resource: the
hyper-text mark-up that annotates web-pages. For
example, consider that anchor text can match lin-
guistic constituents, such as verb phrases, exactly:
..., whereas McCain is secure on the topic, Obama
&lt;a&gt;[VP worries about winning the pro-Israel vote]&lt;/a&gt;.
To validate this idea, we created a new data set,
novel in combining a real blog’s raw HTML with
tree-bank-like constituent structure parses, gener-
</bodyText>
<page confidence="0.932632">
1278
</page>
<note confidence="0.9421325">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1278–1287,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999202666666667">
ated automatically. Our linguistic analysis of the
most prevalent tags (anchors, bold, italics and un-
derlines) over its 1M+ words reveals a strong con-
nection between syntax and mark-up (all of our
examples draw from this corpus), inspiring several
simple techniques for automatically deriving pars-
ing constraints. Experiments with both hard and
more flexible constraints, as well as with different
styles and quantities of annotated training data —
the blog, web news and the web itself, confirm that
mark-up-induced constraints consistently improve
(otherwise unsupervised) dependency parsing.
</bodyText>
<sectionHeader confidence="0.95701" genericHeader="introduction">
2 Intuition and Motivating Examples
</sectionHeader>
<bodyText confidence="0.998692866666666">
It is natural to expect hidden structure to seep
through when a person annotates a sentence. As it
happens, a non-trivial fraction of the world’s pop-
ulation routinely annotates text diligently, if only
partially and informally.1 They inject hyper-links,
vary font sizes, and toggle colors and styles, using
mark-up technologies such as HTML and XML.
As noted, web annotations can be indicative of
phrase boundaries, e.g., in a complicated sentence:
In 1998, however, as I &lt;a&gt;[VP established in
&lt;i&gt;[NP The New Republic]&lt;/i&gt;]&lt;/a&gt; and Bill
Clinton just &lt;a&gt;[VP confirmed in his memoirs]&lt;/a&gt;,
Netanyahu changed his mind and ...
In doing so, mark-up sometimes offers useful cues
even for low-level tokenization decisions:
</bodyText>
<equation confidence="0.4205488">
[NP [NP Libyan ruler]
&lt;a&gt;[NP Mu‘ammar al-Qaddafi]&lt;/a&gt;] referred to ...
(NP (ADJP (NP (JJ Libyan) (NN ruler))
(JJ Nu))
(“‘) (NN ammar) (NNS al-Qaddafi))
</equation>
<bodyText confidence="0.9787468">
Above, a backward quote in an Arabic name con-
fuses the Stanford parser.2 Yet mark-up lines up
with the broken noun phrase, signals cohesion, and
moreover sheds light on the internal structure of
a compound. As Vadas and Curran (2007) point
out, such details are frequently omitted even from
manually compiled tree-banks that err on the side
of flat annotations of base-NPs.
Admittedly, not all boundaries between HTML
tags and syntactic constituents match up nicely:
..., but [S [NP the &lt;a&gt;&lt;i&gt;Toronto
Star&lt;/i&gt;][VP reports [NP this][PP in the
softest possible way]&lt;/a&gt;,[S stating only that ...]]]
Combining parsing with mark-up may not be
straight-forward, but there is hope: even above,
</bodyText>
<footnote confidence="0.990237">
1Even when (American) grammar schools lived up to their
name, they only taught dependencies. This was back in the
days before constituent grammars were invented.
2http://nlp.stanford.edu:8080/parser/
</footnote>
<bodyText confidence="0.999883">
one of each nested tag’s boundaries aligns; and
Toronto Star’s neglected determiner could be for-
given, certainly within a dependency formulation.
</bodyText>
<sectionHeader confidence="0.965155" genericHeader="method">
3 A High-Level Outline of Our Approach
</sectionHeader>
<bodyText confidence="0.999010722222222">
Our idea is to implement the DMV (Klein and
Manning, 2004) — a standard unsupervised gram-
mar inducer. But instead of learning the unan-
notated test set, we train with text that contains
web mark-up, using various ways of converting
HTML into parsing constraints. We still test on
WSJ (Marcus et al., 1993), in the standard way,
and also check generalization against a hidden
data set — the Brown corpus (Francis and Kucera,
1979). Our parsing constraints come from a blog
— a new corpus we created, the web and news (see
Table 1 for corpora’s sentence and token counts).
To facilitate future work, we make the final
models and our manually-constructed blog data
publicly available.3 Although we are unable
to share larger-scale resources, our main results
should be reproducible, as both linguistic analysis
and our best model rely exclusively on the blog.
</bodyText>
<table confidence="0.999203615384615">
Corpus Sentences POS Tokens
WSJ∞ 49,208 1,028,347
Section 23 2,353 48,201
WSJ45 48,418 986,830
WSJ15 15,922 163,715
Brown100 24,208 391,796
BLOG, 57,809 1,136,659
BLOGt45 56,191 1,048,404
BLOGt15 23,214 212,872
NEWS45 2,263,563,078 32,119,123,561
NEWS15 1,433,779,438 11,786,164,503
WEB45 8,903,458,234 87,269,385,640
WEB15 7,488,669,239 55,014,582,024
</table>
<tableCaption confidence="0.818284">
Table 1: Sizes of corpora derived from WSJ and
Brown, as well as those we collected from the web.
</tableCaption>
<sectionHeader confidence="0.955986" genericHeader="method">
4 Data Sets for Evaluation and Training
</sectionHeader>
<bodyText confidence="0.997292583333333">
The appeal of unsupervised parsing lies in its abil-
ity to learn from surface text alone; but (intrinsic)
evaluation still requires parsed sentences. Follow-
ing Klein and Manning (2004), we begin with ref-
erence constituent parses and compare against de-
terministically derived dependencies: after prun-
ing out all empty subtrees, punctuation and ter-
minals (tagged # and $) not pronounced where
they appear, we drop all sentences with more
than a prescribed number of tokens remaining and
use automatic “head-percolation” rules (Collins,
1999) to convert the rest, as is standard practice.
</bodyText>
<footnote confidence="0.969275">
3http://cs.stanford.edu/∼valentin/
</footnote>
<page confidence="0.979355">
1279
</page>
<table confidence="0.789692947368421">
Marked POS Bracketings
Sentences Tokens
All Multi-Token
6,047 1,136,659 7,731 6,015
of 57,809 149,483 7,731 6,015
4,934 124,527 6,482 6,015
3,295 85,423 4,476 4,212
2,103 56,390 2,952 2,789
1,402 38,265 1,988 1,874
960 27,285 1,365 1,302
692 19,894 992 952
Marked POS Bracketings
Sentences Tokens
All Multi-Token
485 14,528 710 684
333 10,484 499 479
245 7,887 365 352
42 1,519 65 63
13 466 20 20
</table>
<figure confidence="0.953215347826087">
6 235 10 10
3 136 6 6
0 0 0 0
Length
Cutoff
0
1
2
3
4
5
6
7
Length
Cutoff
8
9
10
15
20
25
30
40
</figure>
<tableCaption confidence="0.9065815">
Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those
sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).
</tableCaption>
<bodyText confidence="0.995762444444444">
Our primary reference sets are derived from the
Penn English Treebank’s Wall Street Journal por-
tion (Marcus et al., 1993): WSJ45 (sentences with
fewer than 46 tokens) and Section 23 of WSJ∞ (all
sentence lengths). We also evaluate on Brown100,
similarly derived from the parsed portion of the
Brown corpus (Francis and Kucera, 1979). While
we use WSJ45 and WSJ15 to train baseline mod-
els, the bulk of our experiments is with web data.
</bodyText>
<subsectionHeader confidence="0.997713">
4.1 A News-Style Blog: Daniel Pipes
</subsectionHeader>
<bodyText confidence="0.981863038461538">
Since there was no corpus overlaying syntactic
structure with mark-up, we began constructing a
new one by downloading articles4 from a news-
style blog. Although limited to a single genre —
political opinion, danielpipes.org is clean, consis-
tently formatted, carefully edited and larger than
WSJ (see Table 1). Spanning decades, Pipes’
editorials are mostly in-domain for POS taggers
and tree-bank-trained parsers; his recent (internet-
era) entries are thoroughly cross-referenced, con-
veniently providing just the mark-up we hoped to
study via uncluttered (printer-friendly) HTML.5
After extracting moderately clean text and
mark-up locations, we used MxTerminator (Rey-
nar and Ratnaparkhi, 1997) to detect sentence
boundaries. This initial automated pass begot mul-
tiple rounds of various semi-automated clean-ups
that involved fixing sentence breaking, modifying
parser-unfriendly tokens, converting HTML enti-
ties and non-ASCII text, correcting typos, and so
on. After throwing away annotations of fractional
words (e.g., &lt;i&gt;basmachi&lt;/i&gt;s) and tokens (e.g.,
&lt;i&gt;Sesame Street&lt;/i&gt;-like), we broke up all mark-
up that crossed sentence boundaries (i.e., loosely
speaking, replaced constructs like &lt;u&gt;...][S...&lt;/u&gt;
with &lt;u&gt;...&lt;/u&gt; ][S &lt;u&gt;...&lt;/u&gt;) and discarded any
</bodyText>
<footnote confidence="0.954843333333333">
4http://danielpipes.org/art/year/all
5http://danielpipes.org/article print.php?
id=...
</footnote>
<bodyText confidence="0.998674909090909">
tags left covering entire sentences.
We finalized two versions of the data: BLOGt,
tagged with the Stanford tagger (Toutanova and
Manning, 2000; Toutanova et al., 2003),6 and
BLOGp, parsed with Charniak’s parser (Charniak,
2001; Charniak and Johnson, 2005).7 The rea-
son for this dichotomy was to use state-of-the-art
parses to analyze the relationship between syntax
and mark-up, yet to prevent jointly tagged (and
non-standard AUX[G]) POS sequences from interfer-
ing with our (otherwise unsupervised) training.8
</bodyText>
<subsectionHeader confidence="0.993732">
4.2 Scaled up Quantity: The (English) Web
</subsectionHeader>
<bodyText confidence="0.999931428571429">
We built a large (see Table 1) but messy data set,
WEB — English-looking web-pages, pre-crawled
by a search engine. To avoid machine-generated
spam, we excluded low quality sites flagged by the
indexing system. We kept only sentence-like runs
of words (satisfying punctuation and capitalization
constraints), POS-tagged with TnT (Brants, 2000).
</bodyText>
<subsectionHeader confidence="0.994348">
4.3 Scaled up Quality: (English) Web News
</subsectionHeader>
<bodyText confidence="0.9999963">
In an effort to trade quantity for quality, we con-
structed a smaller, potentially cleaner data set,
NEWS. We reckoned editorialized content would
lead to fewer extracted non-sentences. Perhaps
surprisingly, NEWS is less than an order of magni-
tude smaller than WEB (see Table 1); in part, this
is due to less aggressive filtering — we trust sites
approved by the human editors at Google News.9
In all other respects, our pre-processing of NEWS
pages was identical to our handling of WEB data.
</bodyText>
<footnote confidence="0.686457">
6http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz
7ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz
8However, since many taggers are themselves trained on
manually parsed corpora, such as WSJ, no parser that relies
on external POS tags could be considered truly unsupervised;
for a fully unsupervised example, see Seginer’s (2007) CCL
parser, available at http://www.seggu.net/ccl/
9http://news.google.com/
</footnote>
<page confidence="0.990716">
1280
</page>
<sectionHeader confidence="0.983418" genericHeader="method">
5 Linguistic Analysis of Mark-Up
</sectionHeader>
<bodyText confidence="0.973004230769231">
Is there a connection between mark-up and syn-
tactic structure? Previous work (Barr et al., 2008)
has only examined search engine queries, show-
ing that they consist predominantly of short noun
phrases. If web mark-up shared a similar char-
acteristic, it might not provide sufficiently dis-
ambiguating cues to syntactic structure: HTML
tags could be too short (e.g., singletons like
“click &lt;a&gt;here&lt;/a&gt;”) or otherwise unhelpful in re-
solving truly difficult ambiguities (such as PP-
attachment). We began simply by counting vari-
ous basic events in BLOGP.
BLOGP +3,869 more with Count &lt; 49 50.0%
</bodyText>
<tableCaption confidence="0.943292333333333">
Table 3: Top 50% of marked POS tag sequences.
BLOG, +81 more with Count &lt; 16 1.0%
Table 4: Top 99% of dominating non-terminals.
</tableCaption>
<subsectionHeader confidence="0.995163">
5.1 Surface Text Statistics
</subsectionHeader>
<bodyText confidence="0.979166230769231">
Out of 57,809 sentences, 6,047 (10.5%) are anno-
tated (see Table 2); and 4,934 (8.5%) have multi-
token bracketings. We do not distinguish HTML
tags and track only unique bracketing end-points
within a sentence. Of these, 6,015 are multi-token
— an average per-sentence yield of 10.4%.10
10A non-trivial fraction of our corpus is older (pre-internet)
unannotated articles, so this estimate may be conservative.
As expected, many of the annotated words are
nouns, but there are adjectives, verbs and other
parts of speech too (see Table 3). Mark-up is short,
typically under five words, yet (by far) the most
frequently marked sequence of POS tags is a pair.
</bodyText>
<subsectionHeader confidence="0.999738">
5.2 Common Syntactic Subtrees
</subsectionHeader>
<bodyText confidence="0.927503230769231">
For three-quarters of all mark-up, the lowest domi-
nating non-terminal is a noun phrase (see Table 4);
there are also non-trace quantities of verb phrases
(12.9%) and other phrases, clauses and fragments.
Of the top fifteen — 35.2% of all — annotated
productions, only one is not a noun phrase (see Ta-
ble 5, left). Four of the fifteen lowest dominating
non-terminals do not match the entire bracketing
— all four miss the leading determiner, as we saw
earlier. In such cases, we recursively split internal
nodes until the bracketing aligned, as follows:
[S [NP the &lt;a&gt;Toronto Star][VP reports [NP this]
[PP in the softest possible way]&lt;/a&gt;,[S stating ...]]]
</bodyText>
<equation confidence="0.896037">
S -* NP VP -* DT NNP NNP VBZ NP PP S
</equation>
<bodyText confidence="0.975224533333333">
We can summarize productions more compactly
by using a dependency framework and clipping
off any dependents whose subtrees do not cross a
bracketing boundary, relative to the parent. Thus,
DT NNP NNP VBZ DT IN DT ]]S ]] NN
becomes DT NNP VBZ, “the &lt;a&gt;Star reports&lt;/a&gt;.”
Viewed this way, the top fifteen (now collapsed)
productions cover 59.4% of all cases and include
four verb heads, in addition to a preposition and
an adjective (see Table 5, right). This exposes five
cases of inexact matches, three of which involve
neglected determiners or adjectives to the left of
the head. In fact, the only case that cannot be ex-
plained by dropped dependents is #8, where the
daughters are marked but the parent is left out.
Most instances contributing to this pattern are flat
NPs that end with a noun, incorrectly assumed to
be the head of all other words in the phrase, e.g.,
... [NP a 1994 &lt;i&gt;New Yorker&lt;/i&gt; article] ...
As this example shows, disagreements (as well
as agreements) between mark-up and machine-
generated parse trees with automatically perco-
lated heads should be taken with a grain of salt.11
11In a relatively recent study, Ravi et al. (2008) report
that Charniak’s re-ranking parser (Charniak and Johnson,
2005) — reranking-parserAugH.tar.gz, also available
from ftp://ftp.cs.brown.edu/pub/nlparser/ — at-
tains 86.3% accuracy when trained on WSJ and tested against
Brown; its nearly 5% performance loss out-of-domain is con-
sistent with the numbers originally reported by Gildea (2001).
</bodyText>
<figure confidence="0.995483357798165">
Count POS Sequence
1,242 NNP NNP
643 NNP
419 NNP NNP NNP
414 NN
201 ]] NN
138 DT NNP NNP
138 NNS
112 ]]
102 VBD
92 DT NNP NNP NNP
85 ]] NNS
79 NNP NN
76 NN NN
61 VBN
60 NNP NNP NNP NNP
Frac Sum
16.1%
8.3 24.4
5.4 29.8
5.4 35.2
2.6 37.8
1.8 39.5
1.8 41.3
1.5 42.8
1.3 44.1
1.2 45.3
1.1 46.4
1.0 47.4
1.0 48.4
0.8 49.2
0.8 50.0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Count Non-Terminal Frac Sum
1 5,759 NP 74.5%
2 997 VP 12.9 87.4
3 524 S 6.8 94.2
4 120 PP 1.6 95.7
5 72 AD]P 0.9 96.7
6 61 FRAG 0.8 97.4
7 41 ADVP 0.5 98.0
8 39 SBAR 0.5 98.5
9 19 PRN 0.2 98.7
10 18 NX 0.2 99.0
1281
Count Constituent Production Frac Sum Count
1 746 NP → NNP NNP 9.6% 1 1,889
2 357 NP → NNP 4.6 14.3 2 623
3 266 NP → NP PP 3.4 17.7 3 470
4 183 NP → NNP NNP NNP 2.4 20.1 4 458
5 165 NP → DT NNP NNP 2.1 22.2 5 345
6 140 NP → NN 1.8 24.0 6 109
7 131 NP → DT NNP NNP NNP 1.7 25.7 7 98
8 130 NP → DT NN 1.7 27.4 8 96
9 127 NP → DT NNP NNP 1.6 29.0 9 80
10 109 S → NP VP 1.4 30.4 10 77
11 91 NP → DT NNP NNP NNP 1.2 31.6 11 74
12 82 NP → DT JJ NN 1.1 32.7 12 73
13 79 NP → NNS 1.0 33.7 13 71
14 65 NP → JJ NN 0.8 34.5 14 69
15 60 NP → NP NP 0.8 35.3 15 63
BLOGV +5,000 more with Count ≤ 60 64.7% BLOG,, +3,136
Head-Outward Spawn
Frac Sum
NNP
24.4%
NN
8.1 32.5
DT NNP
DT NN
6.1 38.6
5.9 44.5
NNS
4.5 49.0
NNPS
1.4 50.4
VBG
1.3 51.6
NNP NNP NN
1.2 52.9
VBD
1.0 53.9
IN
1.0 54.9
DT JJ NN
VBN
0.9 56.8
1.0 55.9
VBZ
0.9 57.7
POS NNP
0.9 58.6
JJ
0.8 59.4
more with Count ≤ 62 40.6%
</figure>
<tableCaption confidence="0.766549">
Table 5: Top 15 marked productions, viewed as constituents (left) and as dependencies (right), after
</tableCaption>
<bodyText confidence="0.613668">
recursively expanding any internal nodes that did not align with the bracketing (underlined). Tabulated
dependencies were collapsed, dropping any dependents that fell entirely in the same region as their parent
(i.e., both inside the bracketing, both to its left or both to its right), keeping only crossing attachments.
</bodyText>
<subsectionHeader confidence="0.975405">
5.3 Proposed Parsing Constraints
</subsectionHeader>
<bodyText confidence="0.9996016875">
The straight-forward approach — forcing mark-up
to correspond to constituents — agrees with Char-
niak’s parse trees only 48.0% of the time, e.g.,
... in [NP&lt;a&gt;[NP an analysis]&lt;/a&gt;[PP of perhaps the
most astonishing PC item I have yet stumbled upon]].
This number should be higher, as the vast major-
ity of disagreements are due to tree-bank idiosyn-
crasies (e.g., bare NPs). Earlier examples of in-
complete constituents (e.g., legitimately missing
determiners) would also be fine in many linguistic
theories (e.g., as N-bars). A dependency formula-
tion is less sensitive to such stylistic differences.
We begin with the hardest possible constraint on
dependencies, then slowly relax it. Every example
used to demonstrate a softer constraint doubles
as a counter-example against all previous versions.
</bodyText>
<listItem confidence="0.780551">
• strict — seals mark-up into attachments, i.e.,
inside a bracketing, enforces exactly one external
arc — into the overall head. This agrees with
head-percolated trees just 35.6% of the time, e.g.,
</listItem>
<bodyText confidence="0.729743">
As author of &lt;i&gt;The Satanic Verses&lt;/i&gt;, I ...
</bodyText>
<listItem confidence="0.830062">
• loose — same as strict, but allows the bracket-
ing’s head word to have external dependents. This
relaxation already agrees with head-percolated de-
pendencies 87.5% of the time, catching many
(though far from all) dropped dependents, e.g.,
• sprawl — same as loose, but now allows all
words inside a bracketing to attach external de-
pendents.12 This boosts agreement with head-
percolated trees to 95.1%, handling new cases,
e.g., where “Toronto Star” is embedded in longer
mark-up that includes its own parent — a verb:
... the &lt;a&gt;Toronto Star reports ... &lt;/a&gt; ...
• tear — allows mark-up to fracture after all,
</listItem>
<bodyText confidence="0.988002882352941">
requiring only that the external heads attaching the
pieces lie to the same side of the bracketing. This
propels agreement with percolated dependencies
to 98.9%, fixing previously broken PP-attachment
ambiguities, e.g., a fused phrase like “Fox News in
Canada” that detached a preposition from its verb:
... concession ... has raised eyebrows among those
waiting [PP for &lt;a&gt;Fox News][PP in Canada]&lt;/a&gt;.
Most of the remaining 1.1% of disagreements are
due to parser errors. Nevertheless, it is possible for
mark-up to be torn apart by external heads from
both sides. We leave this section with a (very rare)
true negative example. Below, “CSA” modifies
“authority” (to its left), appositively, while “Al-
Manar” modifies “television” (to its right):13
The French broadcasting authority, &lt;a&gt;CSA, banned
... Al-Manar&lt;/a&gt; satellite television from ...
</bodyText>
<footnote confidence="0.923825">
12This view evokes the trapezoids of the O(n3) recognizer
for split head automaton grammars (Eisner and Satta, 1999).
13But this is a stretch, since the comma after “CSA” ren-
ders the marked phrase ungrammatical even out of context.
... the &lt;i&gt;Toronto Star&lt;/i&gt; reports ...
</footnote>
<page confidence="0.996495">
1282
</page>
<sectionHeader confidence="0.999559" genericHeader="method">
6 Experimental Methods and Metrics
</sectionHeader>
<bodyText confidence="0.998518">
We implemented the DMV (Klein and Manning,
2004), consulting the details of (Spitkovsky et al.,
2010a). Crucially, we swapped out inside-outside
re-estimation in favor of Viterbi training. Not only
is it better-suited to the general problem (see §7.1),
but it also admits a trivial implementation of (most
of) the dependency constraints we proposed.14
</bodyText>
<figure confidence="0.735441">
4.5
WSJ� 5 10 15 20 25 30 35 40 45
</figure>
<figureCaption confidence="0.9979965">
Figure 1: Sentence-level cross-entropy on WSJ15
for Ad-Hoc∗ initializers of WSJ11, ... , 45}.
</figureCaption>
<bodyText confidence="0.9954">
Six settings parameterized each run:
</bodyText>
<listItem confidence="0.980223">
• INIT: 0 — default, uniform initialization; or
</listItem>
<bodyText confidence="0.8563815">
1 — a high quality initializer, pre-trained using
Ad-Hoc∗ (Spitkovsky et al., 2010a): we chose the
Laplace-smoothed model trained at WSJ15 (the
“sweet spot” data gradation) but initialized off
WSJ8, since that ad-hoc harmonic initializer has
the best cross-entropy on WSJ15 (see Figure 1).
</bodyText>
<listItem confidence="0.994628823529412">
• GENRE: 0 — default, baseline training on WSJ;
else, uses 1 — BLOGt; 2 — NEWS; or 3 — WEB.
• SCOPE: 0 — default, uses all sentences up to
length 45; if 1, trains using sentences up to length
15; if 2, re-trains on sentences up to length 45,
starting from the solution to sentences up to length
15, as recommended by Spitkovsky et al. (2010a).
• CONSTR: if 4, strict; if 3, loose; and if 2,
sprawl. We did not implement level 1, tear. Over-
constrained sentences are re-attempted at succes-
sively lower levels until they become possible to
parse, if necessary at the lowest (default) level 0.15
• TRIM: if 1, discards any sentence without a sin-
gle multi-token mark-up (shorter than its length).
• ADAPT: if 1, upon convergence, initializes re-
training on WSJ45 using the solution to &lt;GENRE&gt;,
attempting domain adaptation (Lee et al., 1991).
</listItem>
<bodyText confidence="0.9996865">
These make for 294 meaningful combinations. We
judged each one by its accuracy on WSJ45, using
standard directed scoring — the fraction of correct
dependencies over randomized “best” parse trees.
</bodyText>
<footnote confidence="0.62043225">
14We analyze the benefits of Viterbi training in a compan-
ion paper (Spitkovsky et al., 2010b), which dedicates more
space to implementation and to the WSJ baselines used here.
15At level 4, &lt;b&gt; X&lt;u&gt; Y&lt;/b&gt; Z&lt;/u&gt; is over-constrained.
</footnote>
<sectionHeader confidence="0.846096" genericHeader="method">
7 Discussion of Experimental Results
</sectionHeader>
<bodyText confidence="0.9985075">
Evaluation on Section 23 of WSJ and Brown re-
veals that blog-training beats all published state-
of-the-art numbers in every traditionally-reported
length cutoff category, with news-training not far
behind. Here is a mini-preview of these results, for
Section 23 of WSJ10 and WSJ∞ (from Table 8):
</bodyText>
<table confidence="0.9972585">
WSJ10 WSJ∞
(Cohen and Smith, 2009) 62.0 42.2
(Spitkovsky et al., 2010a) 57.1 45.0
NEWS-best 67.3 50.1
BLOGt-best 69.3 50.4
(Headden et al., 2009) 68.8
</table>
<tableCaption confidence="0.977454">
Table 6: Directed accuracies on Section 23 of
</tableCaption>
<bodyText confidence="0.931966027027027">
WSJ110,∞ } for three recent state-of-the-art sys-
tems and our best runs (as judged against WSJ45)
for NEWS and BLOGt (more details in Table 8).
Since our experimental setup involved testing
nearly three hundred models simultaneously, we
must take extreme care in analyzing and interpret-
ing these results, to avoid falling prey to any loom-
ing “data-snooping” biases.16 In a sufficiently
large pool of models, where each is trained using
a randomized and/or chaotic procedure (such as
ours), the best may look good due to pure chance.
We appealed to three separate diagnostics to con-
vince ourselves that our best results are not noise.
The most radical approach would be to write off
WSJ as a development set and to focus only on the
results from the held-out Brown corpus. It was ini-
tially intended as a test of out-of-domain general-
ization, but since Brown was in no way involved
in selecting the best models, it also qualifies as
a blind evaluation set. We observe that our best
models perform even better (and gain more — see
Table 8) on Brown than on WSJ — a strong indi-
cation that our selection process has not overfitted.
Our second diagnostic is a closer look at WSJ.
Since we cannot graph the full (six-dimensional)
set of results, we begin with a simple linear re-
gression, using accuracy on WSJ45 as the depen-
dent variable. We prefer this full factorial design
to the more traditional ablation studies because it
allows us to account for and to incorporate every
single experimental data point incurred along the
16In the standard statistical hypothesis testing setting, it
is reasonable to expect that p% of randomly chosen hy-
potheses will appear significant at the p% level simply by
chance. Consequently, multiple hypothesis testing requires
re-evaluating significance levels — adjusting raw p-values,
e.g., using the Holm-Bonferroni method (Holm, 1979).
</bodyText>
<figure confidence="0.8739268">
x-Entropy h (in bits per token) on WSJ15
lowest cross-entropy (4.32bpt) attained at WSJ8
bpt
5.5
5.0
</figure>
<page confidence="0.890032">
1283
</page>
<table confidence="0.996078384615385">
Corpus Marked Sentences All Sentences POS Tokens All Bracketings Multi-Token Bracketings
BLOGt45 5,641 56,191 1,048,404 7,021 5,346
BLOG′t45 4,516 4,516 104,267 5,771 5,346
BLOGt15 1,562 23,214 212,872 1,714 1,240
BLOG′t15 1,171 1,171 11,954 1,288 1,240
NEWS45 304,129,910 2,263,563,078 32,119,123,561 611,644,606 477,362,150
NEWS′45 205,671,761 205,671,761 2,740,258,972 453,781,081 392,600,070
NEWS15 211,659,549 1,433,779,438 11,786,164,503 365,145,549 274,791,675
NEWS′15 147,848,358 147,848,358 1,397,562,474 272,223,918 231,029,921
WEB45 1,577,208,680 8,903,458,234 87,269,385,640 3,309,897,461 2,459,337,571
WEB′45 933,115,032 933,115,032 11,552,983,379 2,084,359,555 1,793,238,913
WEB15 1,181,696,194 7,488,669,239 55,014,582,024 2,071,743,595 1,494,675,520
WEB′15 681,087,020 681,087,020 5,813,555,341 1,200,980,738 1,072,910,682
</table>
<tableCaption confidence="0.950083">
Table 7: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions,
restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime (′).
</tableCaption>
<bodyText confidence="0.977604541666667">
way. Its output is a coarse, high-level summary of
our runs, showing which factors significantly con-
tribute to changes in error rate on WSJ45:
We use a standard convention: *** for p &lt; 0.001;
** forp &lt; 0.01 (very signif.); and * for p &lt; 0.05 (signif.).
The default training mode (all parameters zero) is
estimated to score 39.9%. A good initializer gives
the biggest (double-digit) gain; both domain adap-
tation and constraints also make a positive impact.
Throwing away unannotated data hurts, as does
training out-of-domain (the blog is least bad; the
web is worst). Of course, this overview should not
be taken too seriously. Overly simplistic, a first
order model ignores interactions between parame-
ters. Furthermore, a least squares fit aims to cap-
ture central tendencies, whereas we are more in-
terested in outliers — the best-performing runs.
A major imperfection of the simple regression
model is that helpful factors that require an in-
teraction to “kick in” may not, on their own, ap-
pear statistically significant. Our third diagnostic
is to examine parameter settings that give rise to
the best-performing models, looking out for com-
binations that consistently deliver superior results.
</bodyText>
<subsectionHeader confidence="0.990148">
7.1 WSJ Baselines
</subsectionHeader>
<bodyText confidence="0.9958706">
Just two parameters apply to learning from WSJ.
Five of their six combinations are state-of-the-art,
demonstrating the power of Viterbi training; only
the default run scores worse than 45.0%, attained
by Leapfrog (Spitkovsky et al., 2010a), on WSJ45:
</bodyText>
<table confidence="0.9471727">
Settings SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 41.3 45.0 45.2
1 46.6 47.5 47.6
@45 @15 @15→45
7.2 Blog
Simply training on BLOGt instead of WSJ hurts:
GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 39.6 36.9 36.9
1 46.5 46.3 46.4
@45 @15 @15→45
</table>
<bodyText confidence="0.9207325">
The best runs use a good initializer, discard unan-
notated sentences, enforce the loose constraint on
the rest, follow up with domain adaptation and
benefit from re-training — GENRE=TRIM=ADAPT=1:
</bodyText>
<table confidence="0.964209333333333">
INIT=1 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 45.8 48.3 49.6
(sprawl) 2 46.3 49.2 49.2
(loose) 3 41.3 50.2 50.4
(strict) 4 40.7 49.9 48.7
@45 @15 @15→45
</table>
<bodyText confidence="0.98147725">
The contrast between unconstrained learning and
annotation-guided parsing is higher for the default
initializer, still using trimmed data sets (just over a
thousand sentences for BLOG′t15 — see Table 7):
</bodyText>
<figure confidence="0.9166028">
INIT=0 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 25.6 19.4 19.3
(sprawl) 2 25.2 22.7 22.5
(loose) 3 32.4 26.3 27.3
(strict) 4 36.2 38.7 40.1
@45 @15 @15→45
Above, we see a clearer benefit to our constraints.
Parameter (Indicator) Setting
Q p-value
1 ad-hoc @WSJ8,15
1 BLOGt
2 NEWS
3 WEB
1 @15
2 @15→45
2 sprawl
3 loose
4 strict
1 drop unmarked
1 WSJ re-training
ADAPT
GENRE
CONSTR
TRIM
INIT
SCOPE
11.8
-3.7
-5.3
-7.7
-0.5
-0.4
0.9
1.0
1.8
-7.4
1.5
***
0.06
***
0.40
0.53
0.23
*
***
**
Intercept (Rndjusted = 73.6%) 39.9
***
**
0.15
</figure>
<page confidence="0.946689">
1284
</page>
<table confidence="0.8418215">
7.3 News
Training on WSJ is also better than using NEWS:
GENRE=2 SCOPE=A SCOPE=1 SCOPE=2
INIT=A 40.2 38.8 38.7
1 43.4 44.0 43.8
@45 @15 @15→45
</table>
<bodyText confidence="0.98908725">
As with the blog, the best runs use the good initial-
izer, discard unannotated sentences, enforce the
loose constraint and follow up with domain adap-
tation — GENRE=2; INIT=TRIM=ADAPT=1:
</bodyText>
<table confidence="0.996980833333333">
Settings SCOPE=A SCOPE=1 SCOPE=2
CONSTR=A 46.6 45.4 45.2
(sprawl) 2 46.1 44.9 44.9
(loose) 3 49.5 48.1 48.3
(strict) 4 37.7 36.8 37.6
@45 @15 @15→45
</table>
<bodyText confidence="0.999879571428571">
With all the extra training data, the best new score
is just 49.5%. On the one hand, we are disap-
pointed by the lack of dividends to orders of mag-
nitude more data. On the other, we are comforted
that the system arrives within 1% of its best result
— 50.4%, obtained with a manually cleaned up
corpus — now using an auto-generated data set.
</bodyText>
<subsectionHeader confidence="0.99382">
7.4 Web
</subsectionHeader>
<bodyText confidence="0.983797">
The WEB-side story is more discouraging:
</bodyText>
<table confidence="0.99740275">
GENRE=3 SCOPE=A SCOPE=1 SCOPE=2
INIT=A 38.3 35.1 35.2
1 42.8 43.6 43.4
@45 @15 @15→45
</table>
<bodyText confidence="0.997091461538462">
Our best run again uses a good initializer, keeps
all sentences, still enforces the loose constraint
and follows up with domain adaptation, but per-
forms worse than all well-initialized WSJ base-
lines, scoring only 45.9% (trained at WEB15).
We suspect that the web is just too messy for
us. On top of the challenges of language iden-
tification and sentence-breaking, there is a lot of
boiler-plate; furthermore, web text can be difficult
for news-trained POS taggers. For example, note
that the verb “sign” is twice mistagged as a noun
and that “YouTube” is classified as a verb, in the
top four POS sequences of web sentences:17
</bodyText>
<table confidence="0.9993566">
POS Sequence WEB Count
Sample web sentence, chosen uniformly at random.
1 DT NNS VBN 82,858,487
All rights reserved.
2 NNP NNP NNP 65,889,181
Yuasa et al.
3 NN IN TO VB RB 31,007,783
Sign in to YouTube now!
4 NN IN IN PRP$ 11 NN 31,007,471
Sign in with your Google Account!
</table>
<footnote confidence="0.745057">
17Further evidence: TnT tags the ubiquitous but ambigu-
ous fragments “click here” and “print post” as noun phrases.
</footnote>
<subsectionHeader confidence="0.936773">
7.5 The State of the Art
</subsectionHeader>
<bodyText confidence="0.999984421052632">
Our best model gains more than 5% over previ-
ous state-of-the-art accuracy across all sentences
of WSJ’s Section 23, more than 8% on WSJ20 and
rivals the oracle skyline (Spitkovsky et al., 2010a)
on WSJ10; these gains generalize to Brown100,
where it improves by nearly 10% (see Table 8).
We take solace in the fact that our best mod-
els agree in using loose constraints. Of these,
the models trained with less data perform better,
with the best two using trimmed data sets, echo-
ing that “less is more” (Spitkovsky et al., 2010a),
pace Halevy et al. (2009). We note that orders of
magnitude more data did not improve parsing per-
formance further and suspect a different outcome
from lexicalized models: The primary benefit of
additional lower-quality data is in improved cover-
age. But with only 35 unique POS tags, data spar-
sity is hardly an issue. Extra examples of lexical
items help little and hurt when they are mistagged.
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999972413793103">
The wealth of new annotations produced in many
languages every day already fuels a number of
NLP applications. Following their early and
wide-spread use by search engines, in service of
spam-fighting and retrieval, anchor text and link
data enhanced a variety of traditional NLP tech-
niques: cross-lingual information retrieval (Nie
and Chen, 2002), translation (Lu et al., 2004), both
named-entity recognition (Mihalcea and Csomai,
2007) and categorization (Watanabe et al., 2007),
query segmentation (Tan and Peng, 2008), plus
semantic relatedness and word-sense disambigua-
tion (Gabrilovich and Markovitch, 2007; Yeh et
al., 2009). Yet several, seemingly natural, can-
didate core NLP tasks — tokenization, CJK seg-
mentation, noun-phrase chunking, and (until now)
parsing — remained conspicuously uninvolved.
Approaches related to ours arise in applications
that combine parsing with named-entity recogni-
tion (NER). For example, constraining a parser to
respect the boundaries of known entities is stan-
dard practice not only in joint modeling of (con-
stituent) parsing and NER (Finkel and Manning,
2009), but also in higher-level NLP tasks, such as
relation extraction (Mintz et al., 2009), that couple
chunking with (dependency) parsing. Although
restricted to proper noun phrases, dates, times and
quantities, we suspect that constituents identified
by trained (supervised) NER systems would also
</bodyText>
<page confidence="0.952814">
1285
</page>
<table confidence="0.999883777777778">
Model Incarnation WSJ10 WSJ20 WSJ∞
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6
default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5
WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIN=0,ADAPT=0 65.3 53.8 47.9 50.8
BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIN=1,ADAPT=1 69.3 56.8 50.4 53.3
NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIN=1,ADAPT=1 67.3 56.2 50.1 51.6
WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIN=0,ADAPT=1 64.1 52.7 46.3 46.9
EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8
</table>
<tableCaption confidence="0.898821">
Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art
systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets.
</tableCaption>
<bodyText confidence="0.988762722222222">
be helpful in constraining grammar induction.
Following Pereira and Schabes’ (1992) success
with partial annotations in training a model of
(English) constituents generatively, their idea has
been extended to discriminative estimation (Rie-
zler et al., 2002) and also proved useful in mod-
eling (Japanese) dependencies (Sassano, 2005).
There was demand for partially bracketed corpora.
Chen and Lee (1995) constructed one such corpus
by learning to partition (English) POS sequences
into chunks (Abney, 1991); Inui and Kotani (2001)
used n-gram statistics to split (Japanese) clauses.
We combine the two intuitions, using the web
to build a partially parsed corpus. Our approach
could be called lightly-supervised, since it does
not require manual annotation of a single complete
parse tree. In contrast, traditional semi-supervised
methods rely on fully-annotated seed corpora.18
</bodyText>
<sectionHeader confidence="0.996766" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9953251">
We explored novel ways of training dependency
parsing models, the best of which attains 50.4%
accuracy on Section 23 (all sentences) of WSJ,
beating all previous unsupervised state-of-the-art
by more than 5%. Extra gains stem from guid-
ing Viterbi training with web mark-up, the loose
constraint consistently delivering best results. Our
linguistic analysis of a blog reveals that web an-
notations can be converted into accurate parsing
constraints (loose: 88%; sprawl: 95%; tear: 99%)
that could be helpful to supervised methods, e.g.,
by boosting an initial parser via self-training (Mc-
Closky et al., 2006) on sentences with mark-up.
Similar techniques may apply to standard word-
processing annotations, such as font changes, and
to certain (balanced) punctuation (Briscoe, 1994).
We make our blog data set, overlaying mark-up
and syntax, publicly available. Its annotations are
18A significant effort expended in building a tree-bank
comes with the first batch of sentences (Druck et al., 2009).
75% noun phrases, 13% verb phrases, 7% simple
declarative clauses and 2% prepositional phrases,
with traces of other phrases, clauses and frag-
ments. The type of mark-up, combined with POS
tags, could make for valuable features in discrimi-
native models of parsing (Ratnaparkhi, 1999).
A logical next step would be to explore the con-
nection between syntax and mark-up for genres
other than a news-style blog and for languages
other than English. We are excited by the possi-
bilities, as unsupervised parsers are on the cusp
of becoming useful in their own right — re-
cently, Davidov et al. (2009) successfully applied
Seginer’s (2007) fully unsupervised grammar in-
ducer to the problems of pattern-acquisition and
extraction of semantic data. If the strength of the
connection between web mark-up and syntactic
structure is universal across languages and genres,
this fact could have broad implications for NLP,
with applications extending well beyond parsing.
</bodyText>
<sectionHeader confidence="0.999614" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.8311716">
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fannie
&amp; John Hertz Foundation Fellowship. We thank Angel X.
Chang, Spence Green, Christopher D. Manning, Richard
Socher, Mihai Surdeanu and the anonymous reviewers for
many helpful suggestions, and we are especially grateful to
Andy Golding, for pointing us to his sample Map-Reduce
over the Google News crawl, and to Daniel Pipes, for allow-
ing us to distribute the data set derived from his blog entries.
</reference>
<sectionHeader confidence="0.95868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999412">
S. Abney. 1991. Parsing by chunks. Principle-Based Pars-
ing: Computation and Psycholinguistics.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society ofAmerica.
C. Barr, R. Jones, and M. Regelson. 2008. The linguistic
structure of English web-search queries. In EMNLP.
T. Brants. 2000. TnT — a statistical part-of-speech tagger.
In ANLP.
</reference>
<page confidence="0.784588">
1286
</page>
<reference confidence="0.99995276635514">
T. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In ACL.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
H.-H. Chen and Y.-S. Lee. 1995. Development of a partially
bracketed corpus with part-of-speech information only. In
WVLC.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In NAACL-HLT.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
D. Davidov, R. Reichart, and A. Rappoport. 2009. Supe-
rior and efficient fully unsupervised pattern-based concept
acquisition using an unsupervised parser. In CoNLL.
G. Druck, G. Mann, and A. McCallum. 2009. Semi-
supervised learning of dependency parsers using general-
ized expectation criteria. In ACL-IJCNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
E. Gabrilovich and S. Markovitch. 2007. Computing seman-
tic relatedness using Wikipedia-based Explicit Semantic
Analysis. In IJCAI.
D. Gildea. 2001. Corpus variation and parser performance.
In EMNLP.
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple test
procedure. Scandinavian Journal of Statistics, 6.
N. Inui and Y. Kotani. 2001. Robust N-gram based syntactic
analysis using segmentation words. In PACLIC.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
C.-H. Lee, C.-H. Lin, and B.-H. Juang. 1991. A study on
speaker adaptation of the parameters of continuous den-
sity Hidden Markov Models. IEEE Trans. on Signal Pro-
cessing, 39.
W.-H. Lu, L.-F. Chien, and H.-J. Lee. 2004. Anchor text
mining for translation of Web queries: A transitive trans-
lation approach. ACM Trans. on Information Systems, 22.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In NAACL-HLT.
R. Mihalcea and A. Csomai. 2007. Wikify!: Linking docu-
ments to encyclopedic knowledge. In CIKM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant
supervision for relation extraction without labeled data. In
ACL-IJCNLP.
J.-Y. Nie and J. Chen. 2002. Exploiting the Web as paral-
lel corpora for cross-language information retrieval. Web
Intelligence.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
A. Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34.
S. Ravi, K. Knight, and R. Soricut. 2008. Automatic predic-
tion of parser accuracy. In EMNLP.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In ANLP.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional grammar and dis-
criminative estimation techniques. In ACL.
M. Sassano. 2005. Using a partially annotated corpus to
build a dependency parser for Japanese. In IJCNLP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How “Less is More” in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised de-
pendency parsing. In CoNLL.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia. In
WWW.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-of-
speech tagger. In EMNLP-VLC.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In HLT-NAACL.
D. Vadas and J. R. Curran. 2007. Adding noun phrase struc-
ture to the Penn Treebank. In ACL.
Y. Watanabe, M. Asahara, and Y. Matsumoto. 2007. A
graph-based approach to named entity categorization in
Wikipedia using conditional random fields. In EMNLP-
CoNLL.
E. Yeh, D. Ramage, C. D. Manning, E. Agirre, and A. Soroa.
2009. WikiWalk: Random walks on Wikipedia for se-
mantic relatedness. In TextGraphs.
</reference>
<page confidence="0.992255">
1287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936991">
<title confidence="0.999921">Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</title>
<author confidence="0.99985">Valentin I Spitkovsky Daniel Jurafsky</author>
<affiliation confidence="0.9993105">Computer Science Department Departments of Linguistics and Stanford University and Google Inc. Computer Science, Stanford University</affiliation>
<email confidence="0.993298">valentin@google.comjurafsky@stanford.edu</email>
<author confidence="0.954254">Hiyan Alshawi</author>
<affiliation confidence="0.999427">Google Inc.</affiliation>
<email confidence="0.99901">hiyan@google.com</email>
<abstract confidence="0.9996905">We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning’s Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-theart by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus — nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Angel X Chang</author>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Partially funded by NSF award IIS-0811974 and by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. We thank</title>
<marker>Chang, Green, Manning, </marker>
<rawString>Partially funded by NSF award IIS-0811974 and by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. We thank Angel X. Chang, Spence Green, Christopher D. Manning, Richard Socher, Mihai Surdeanu and the anonymous reviewers for many helpful suggestions, and we are especially grateful to Andy Golding, for pointing us to his sample Map-Reduce over the Google News crawl, and to Daniel Pipes, for allowing us to distribute the data set derived from his blog entries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. Principle-Based Parsing: Computation and Psycholinguistics.</title>
<date>1991</date>
<contexts>
<context position="35586" citStr="Abney, 1991" startWordPosition="5679" endWordPosition="5680">systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses. We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised methods rely on fully-annotated seed corpora.18 9 Conclusion We explored novel ways of training dependency parsing models, the best of which attains 50.4% accuracy on Section 23 (all sentences) of WSJ, beating all previous unsupervised state-of-the-art by more than 5%. Extra</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. Principle-Based Parsing: Computation and Psycholinguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</booktitle>
<contexts>
<context position="2360" citStr="Baker, 1979" startWordPosition="341" endWordPosition="342">s and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdetermined by unannotated text; (ii) fewer iterations needed to reach a good grammar, countering convergence properties that sharply deteriorate with the number of non-terminal symbols, due to a proliferation of local maxima; and (iii) better (in the best case, linear) time complexity per i</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Barr</author>
<author>R Jones</author>
<author>M Regelson</author>
</authors>
<title>The linguistic structure of English web-search queries. In</title>
<date>2008</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="13239" citStr="Barr et al., 2008" startWordPosition="1973" endWordPosition="1976">es was identical to our handling of WEB data. 6http://nlp.stanford.edu/software/ stanford-postagger-2008-09-28.tar.gz 7ftp://ftp.cs.brown.edu/pub/nlparser/ parser05Aug16.tar.gz 8However, since many taggers are themselves trained on manually parsed corpora, such as WSJ, no parser that relies on external POS tags could be considered truly unsupervised; for a fully unsupervised example, see Seginer’s (2007) CCL parser, available at http://www.seggu.net/ccl/ 9http://news.google.com/ 1280 5 Linguistic Analysis of Mark-Up Is there a connection between mark-up and syntactic structure? Previous work (Barr et al., 2008) has only examined search engine queries, showing that they consist predominantly of short noun phrases. If web mark-up shared a similar characteristic, it might not provide sufficiently disambiguating cues to syntactic structure: HTML tags could be too short (e.g., singletons like “click &lt;a&gt;here&lt;/a&gt;”) or otherwise unhelpful in resolving truly difficult ambiguities (such as PPattachment). We began simply by counting various basic events in BLOGP. BLOGP +3,869 more with Count &lt; 49 50.0% Table 3: Top 50% of marked POS tag sequences. BLOG, +81 more with Count &lt; 16 1.0% Table 4: Top 99% of dominat</context>
</contexts>
<marker>Barr, Jones, Regelson, 2008</marker>
<rawString>C. Barr, R. Jones, and M. Regelson. 2008. The linguistic structure of English web-search queries. In EMNLP. T. Brants. 2000. TnT — a statistical part-of-speech tagger. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
</authors>
<title>Parsing (with) punctuation, etc.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>Xerox European Research Laboratory.</institution>
<contexts>
<context position="36750" citStr="Briscoe, 1994" startWordPosition="5853" endWordPosition="5854">supervised state-of-the-art by more than 5%. Extra gains stem from guiding Viterbi training with web mark-up, the loose constraint consistently delivering best results. Our linguistic analysis of a blog reveals that web annotations can be converted into accurate parsing constraints (loose: 88%; sprawl: 95%; tear: 99%) that could be helpful to supervised methods, e.g., by boosting an initial parser via self-training (McClosky et al., 2006) on sentences with mark-up. Similar techniques may apply to standard wordprocessing annotations, such as font changes, and to certain (balanced) punctuation (Briscoe, 1994). We make our blog data set, overlaying mark-up and syntax, publicly available. Its annotations are 18A significant effort expended in building a tree-bank comes with the first batch of sentences (Druck et al., 2009). 75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases, with traces of other phrases, clauses and fragments. The type of mark-up, combined with POS tags, could make for valuable features in discriminative models of parsing (Ratnaparkhi, 1999). A logical next step would be to explore the connection between syntax and mark-up for genres other</context>
</contexts>
<marker>Briscoe, 1994</marker>
<rawString>T. Briscoe. 1994. Parsing (with) punctuation, etc. Technical report, Xerox European Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11490" citStr="Charniak and Johnson, 2005" startWordPosition="1721" endWordPosition="1724">ng away annotations of fractional words (e.g., &lt;i&gt;basmachi&lt;/i&gt;s) and tokens (e.g., &lt;i&gt;Sesame Street&lt;/i&gt;-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u&gt;...][S...&lt;/u&gt; with &lt;u&gt;...&lt;/u&gt; ][S &lt;u&gt;...&lt;/u&gt;) and discarded any 4http://danielpipes.org/art/year/all 5http://danielpipes.org/article print.php? id=... tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We kept only sentence-like runs of words (satisfying punctuation and capitalization constra</context>
<context position="16488" citStr="Charniak and Johnson, 2005" startWordPosition="2518" endWordPosition="2521">ly case that cannot be explained by dropped dependents is #8, where the daughters are marked but the parent is left out. Most instances contributing to this pattern are flat NPs that end with a noun, incorrectly assumed to be the head of all other words in the phrase, e.g., ... [NP a 1994 &lt;i&gt;New Yorker&lt;/i&gt; article] ... As this example shows, disagreements (as well as agreements) between mark-up and machinegenerated parse trees with automatically percolated heads should be taken with a grain of salt.11 11In a relatively recent study, Ravi et al. (2008) report that Charniak’s re-ranking parser (Charniak and Johnson, 2005) — reranking-parserAugH.tar.gz, also available from ftp://ftp.cs.brown.edu/pub/nlparser/ — attains 86.3% accuracy when trained on WSJ and tested against Brown; its nearly 5% performance loss out-of-domain is consistent with the numbers originally reported by Gildea (2001). Count POS Sequence 1,242 NNP NNP 643 NNP 419 NNP NNP NNP 414 NN 201 ]] NN 138 DT NNP NNP 138 NNS 112 ]] 102 VBD 92 DT NNP NNP NNP 85 ]] NNS 79 NNP NN 76 NN NN 61 VBN 60 NNP NNP NNP NNP Frac Sum 16.1% 8.3 24.4 5.4 29.8 5.4 35.2 2.6 37.8 1.8 39.5 1.8 41.3 1.5 42.8 1.3 44.1 1.2 45.3 1.1 46.4 1.0 47.4 1.0 48.4 0.8 49.2 0.8 50.0 </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11461" citStr="Charniak, 2001" startWordPosition="1719" endWordPosition="1720">on. After throwing away annotations of fractional words (e.g., &lt;i&gt;basmachi&lt;/i&gt;s) and tokens (e.g., &lt;i&gt;Sesame Street&lt;/i&gt;-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u&gt;...][S...&lt;/u&gt; with &lt;u&gt;...&lt;/u&gt; ][S &lt;u&gt;...&lt;/u&gt;) and discarded any 4http://danielpipes.org/art/year/all 5http://danielpipes.org/article print.php? id=... tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We kept only sentence-like runs of words (satisfying punctuati</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-H Chen</author>
<author>Y-S Lee</author>
</authors>
<title>Development of a partially bracketed corpus with part-of-speech information only.</title>
<date>1995</date>
<booktitle>In WVLC.</booktitle>
<contexts>
<context position="35483" citStr="Chen and Lee (1995)" startWordPosition="5662" endWordPosition="5665">2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses. We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised methods rely on fully-annotated seed corpora.18 9 Conclusion We explored novel ways of training dependency parsing models, the best of which attains 50.4% accuracy on Secti</context>
</contexts>
<marker>Chen, Lee, 1995</marker>
<rawString>H.-H. Chen and Y.-S. Lee. 1995. Development of a partially bracketed corpus with part-of-speech information only. In WVLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="2050" citStr="Cohen and Smith, 2009" startWordPosition="293" endWordPosition="296">oduction Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence struct</context>
<context position="23959" citStr="Cohen and Smith, 2009" startWordPosition="3854" endWordPosition="3857">ized “best” parse trees. 14We analyze the benefits of Viterbi training in a companion paper (Spitkovsky et al., 2010b), which dedicates more space to implementation and to the WSJ baselines used here. 15At level 4, &lt;b&gt; X&lt;u&gt; Y&lt;/b&gt; Z&lt;/u&gt; is over-constrained. 7 Discussion of Experimental Results Evaluation on Section 23 of WSJ and Brown reveals that blog-training beats all published stateof-the-art numbers in every traditionally-reported length cutoff category, with news-training not far behind. Here is a mini-preview of these results, for Section 23 of WSJ10 and WSJ∞ (from Table 8): WSJ10 WSJ∞ (Cohen and Smith, 2009) 62.0 42.2 (Spitkovsky et al., 2010a) 57.1 45.0 NEWS-best 67.3 50.1 BLOGt-best 69.3 50.4 (Headden et al., 2009) 68.8 Table 6: Directed accuracies on Section 23 of WSJ110,∞ } for three recent state-of-the-art systems and our best runs (as judged against WSJ45) for NEWS and BLOGt (more details in Table 8). Since our experimental setup involved testing nearly three hundred models simultaneously, we must take extreme care in analyzing and interpreting these results, to avoid falling prey to any looming “data-snooping” biases.16 In a sufficiently large pool of models, where each is trained using a </context>
<context position="34346" citStr="Cohen and Smith, 2009" startWordPosition="5517" endWordPosition="5520">th named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model Incarnation WSJ10 WSJ20 WSJ∞ DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIN=0,ADAPT=0 65.3 53.8 47.9 50.8 BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIN=1,ADAPT=1 69.3 56.8 50.4 53.3 NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIN=1,ADAPT=1 67.3 56.2 50.1 51.6 WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIN=0,ADAPT=1 64.1 52.7 46.3 46.9 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for th</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8634" citStr="Collins, 1999" startWordPosition="1287" endWordPosition="1288">as those we collected from the web. 4 Data Sets for Evaluation and Training The appeal of unsupervised parsing lies in its ability to learn from surface text alone; but (intrinsic) evaluation still requires parsed sentences. Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty subtrees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. 3http://cs.stanford.edu/∼valentin/ 1279 Marked POS Bracketings Sentences Tokens All Multi-Token 6,047 1,136,659 7,731 6,015 of 57,809 149,483 7,731 6,015 4,934 124,527 6,482 6,015 3,295 85,423 4,476 4,212 2,103 56,390 2,952 2,789 1,402 38,265 1,988 1,874 960 27,285 1,365 1,302 692 19,894 992 952 Marked POS Bracketings Sentences Tokens All Multi-Token 485 14,528 710 684 333 10,484 499 479 245 7,887 365 352 42 1,519 65 63 13 466 20 20 6 235 10 10 3 136 6 6 0 0 0 0 Length Cutoff 0 1 2 3 4 5 6 7 Length Cutoff 8 9 10 15 20 25 30 40 Table 2: Counts of s</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>R Reichart</author>
<author>A Rappoport</author>
</authors>
<title>Superior and efficient fully unsupervised pattern-based concept acquisition using an unsupervised parser.</title>
<date>2009</date>
<booktitle>In CoNLL.</booktitle>
<marker>Davidov, Reichart, Rappoport, 2009</marker>
<rawString>D. Davidov, R. Reichart, and A. Rappoport. 2009. Superior and efficient fully unsupervised pattern-based concept acquisition using an unsupervised parser. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Semisupervised learning of dependency parsers using generalized expectation criteria.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="36966" citStr="Druck et al., 2009" startWordPosition="5885" endWordPosition="5888">that web annotations can be converted into accurate parsing constraints (loose: 88%; sprawl: 95%; tear: 99%) that could be helpful to supervised methods, e.g., by boosting an initial parser via self-training (McClosky et al., 2006) on sentences with mark-up. Similar techniques may apply to standard wordprocessing annotations, such as font changes, and to certain (balanced) punctuation (Briscoe, 1994). We make our blog data set, overlaying mark-up and syntax, publicly available. Its annotations are 18A significant effort expended in building a tree-bank comes with the first batch of sentences (Druck et al., 2009). 75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases, with traces of other phrases, clauses and fragments. The type of mark-up, combined with POS tags, could make for valuable features in discriminative models of parsing (Ratnaparkhi, 1999). A logical next step would be to explore the connection between syntax and mark-up for genres other than a news-style blog and for languages other than English. We are excited by the possibilities, as unsupervised parsers are on the cusp of becoming useful in their own right — recently, Davidov et al. (2009) succe</context>
</contexts>
<marker>Druck, Mann, McCallum, 2009</marker>
<rawString>G. Druck, G. Mann, and A. McCallum. 2009. Semisupervised learning of dependency parsers using generalized expectation criteria. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head-automaton grammars.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="21277" citStr="Eisner and Satta, 1999" startWordPosition="3411" endWordPosition="3414">ows among those waiting [PP for &lt;a&gt;Fox News][PP in Canada]&lt;/a&gt;. Most of the remaining 1.1% of disagreements are due to parser errors. Nevertheless, it is possible for mark-up to be torn apart by external heads from both sides. We leave this section with a (very rare) true negative example. Below, “CSA” modifies “authority” (to its left), appositively, while “AlManar” modifies “television” (to its right):13 The French broadcasting authority, &lt;a&gt;CSA, banned ... Al-Manar&lt;/a&gt; satellite television from ... 12This view evokes the trapezoids of the O(n3) recognizer for split head automaton grammars (Eisner and Satta, 1999). 13But this is a stretch, since the comma after “CSA” renders the marked phrase ungrammatical even out of context. ... the &lt;i&gt;Toronto Star&lt;/i&gt; reports ... 1282 6 Experimental Methods and Metrics We implemented the DMV (Klein and Manning, 2004), consulting the details of (Spitkovsky et al., 2010a). Crucially, we swapped out inside-outside re-estimation in favor of Viterbi training. Not only is it better-suited to the general problem (see §7.1), but it also admits a trivial implementation of (most of) the dependency constraints we proposed.14 4.5 WSJ� 5 10 15 20 25 30 35 40 45 Figure 1: Sentenc</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="33945" citStr="Finkel and Manning, 2009" startWordPosition="5461" endWordPosition="5464">2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model Incarnation WSJ10 WSJ20 WSJ∞ DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TR</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Joint parsing and named entity recognition. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Manual of Information to Accompany a Standard Corpus of Present-Day Edited American English, for use with Digital Computers.</title>
<date>1979</date>
<institution>Department of Linguistic, Brown University.</institution>
<contexts>
<context position="7177" citStr="Francis and Kucera, 1979" startWordPosition="1066" endWordPosition="1069">080/parser/ one of each nested tag’s boundaries aligns; and Toronto Star’s neglected determiner could be forgiven, certainly within a dependency formulation. 3 A High-Level Outline of Our Approach Our idea is to implement the DMV (Klein and Manning, 2004) — a standard unsupervised grammar inducer. But instead of learning the unannotated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints. We still test on WSJ (Marcus et al., 1993), in the standard way, and also check generalization against a hidden data set — the Brown corpus (Francis and Kucera, 1979). Our parsing constraints come from a blog — a new corpus we created, the web and news (see Table 1 for corpora’s sentence and token counts). To facilitate future work, we make the final models and our manually-constructed blog data publicly available.3 Although we are unable to share larger-scale resources, our main results should be reproducible, as both linguistic analysis and our best model rely exclusively on the blog. Corpus Sentences POS Tokens WSJ∞ 49,208 1,028,347 Section 23 2,353 48,201 WSJ45 48,418 986,830 WSJ15 15,922 163,715 Brown100 24,208 391,796 BLOG, 57,809 1,136,659 BLOGt45 5</context>
<context position="9752" citStr="Francis and Kucera, 1979" startWordPosition="1480" endWordPosition="1483">10 3 136 6 6 0 0 0 0 Length Cutoff 0 1 2 3 4 5 6 7 Length Cutoff 8 9 10 15 20 25 30 40 Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence). Our primary reference sets are derived from the Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ∞ (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (Francis and Kucera, 1979). While we use WSJ45 and WSJ15 to train baseline models, the bulk of our experiments is with web data. 4.1 A News-Style Blog: Daniel Pipes Since there was no corpus overlaying syntactic structure with mark-up, we began constructing a new one by downloading articles4 from a newsstyle blog. Although limited to a single genre — political opinion, danielpipes.org is clean, consistently formatted, carefully edited and larger than WSJ (see Table 1). Spanning decades, Pipes’ editorials are mostly in-domain for POS taggers and tree-bank-trained parsers; his recent (internetera) entries are thoroughly </context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. N. Francis and H. Kucera, 1979. Manual of Information to Accompany a Standard Corpus of Present-Day Edited American English, for use with Digital Computers. Department of Linguistic, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="33457" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="5387" endWordPosition="5390">ged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking wit</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16760" citStr="Gildea (2001)" startWordPosition="2556" endWordPosition="2557">994 &lt;i&gt;New Yorker&lt;/i&gt; article] ... As this example shows, disagreements (as well as agreements) between mark-up and machinegenerated parse trees with automatically percolated heads should be taken with a grain of salt.11 11In a relatively recent study, Ravi et al. (2008) report that Charniak’s re-ranking parser (Charniak and Johnson, 2005) — reranking-parserAugH.tar.gz, also available from ftp://ftp.cs.brown.edu/pub/nlparser/ — attains 86.3% accuracy when trained on WSJ and tested against Brown; its nearly 5% performance loss out-of-domain is consistent with the numbers originally reported by Gildea (2001). Count POS Sequence 1,242 NNP NNP 643 NNP 419 NNP NNP NNP 414 NN 201 ]] NN 138 DT NNP NNP 138 NNS 112 ]] 102 VBD 92 DT NNP NNP NNP 85 ]] NNS 79 NNP NN 76 NN NN 61 VBN 60 NNP NNP NNP NNP Frac Sum 16.1% 8.3 24.4 5.4 29.8 5.4 35.2 2.6 37.8 1.8 39.5 1.8 41.3 1.5 42.8 1.3 44.1 1.2 45.3 1.1 46.4 1.0 47.4 1.0 48.4 0.8 49.2 0.8 50.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Count Non-Terminal Frac Sum 1 5,759 NP 74.5% 2 997 VP 12.9 87.4 3 524 S 6.8 94.2 4 120 PP 1.6 95.7 5 72 AD]P 0.9 96.7 6 61 FRAG 0.8 97.4 7 41 ADVP 0.5 98.0 8 39 SBAR 0.5 98.5 9 19 PRN 0.2 98.7 10 18 NX 0.2 99.0 1281 Count Constituent Pr</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Corpus variation and parser performance. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Halevy</author>
<author>P Norvig</author>
<author>F Pereira</author>
</authors>
<title>The unreasonable effectiveness of data.</title>
<date>2009</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>24</volume>
<contexts>
<context position="32460" citStr="Halevy et al. (2009)" startWordPosition="5234" endWordPosition="5237"> here” and “print post” as noun phrases. 7.5 The State of the Art Our best model gains more than 5% over previous state-of-the-art accuracy across all sentences of WSJ’s Section 23, more than 8% on WSJ20 and rivals the oracle skyline (Spitkovsky et al., 2010a) on WSJ10; these gains generalize to Brown100, where it improves by nearly 10% (see Table 8). We take solace in the fact that our best models agree in using loose constraints. Of these, the models trained with less data perform better, with the best two using trimmed data sets, echoing that “less is more” (Spitkovsky et al., 2010a), pace Halevy et al. (2009). We note that orders of magnitude more data did not improve parsing performance further and suspect a different outcome from lexicalized models: The primary benefit of additional lower-quality data is in improved coverage. But with only 35 unique POS tags, data sparsity is hardly an issue. Extra examples of lexical items help little and hurt when they are mistagged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, </context>
</contexts>
<marker>Halevy, Norvig, Pereira, 2009</marker>
<rawString>A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="2027" citStr="Headden et al., 2009" startWordPosition="289" endWordPosition="292">ability in NLP. 1 Introduction Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgm</context>
<context position="24070" citStr="Headden et al., 2009" startWordPosition="3872" endWordPosition="3875">2010b), which dedicates more space to implementation and to the WSJ baselines used here. 15At level 4, &lt;b&gt; X&lt;u&gt; Y&lt;/b&gt; Z&lt;/u&gt; is over-constrained. 7 Discussion of Experimental Results Evaluation on Section 23 of WSJ and Brown reveals that blog-training beats all published stateof-the-art numbers in every traditionally-reported length cutoff category, with news-training not far behind. Here is a mini-preview of these results, for Section 23 of WSJ10 and WSJ∞ (from Table 8): WSJ10 WSJ∞ (Cohen and Smith, 2009) 62.0 42.2 (Spitkovsky et al., 2010a) 57.1 45.0 NEWS-best 67.3 50.1 BLOGt-best 69.3 50.4 (Headden et al., 2009) 68.8 Table 6: Directed accuracies on Section 23 of WSJ110,∞ } for three recent state-of-the-art systems and our best runs (as judged against WSJ45) for NEWS and BLOGt (more details in Table 8). Since our experimental setup involved testing nearly three hundred models simultaneously, we must take extreme care in analyzing and interpreting these results, to avoid falling prey to any looming “data-snooping” biases.16 In a sufficiently large pool of models, where each is trained using a randomized and/or chaotic procedure (such as ours), the best may look good due to pure chance. We appealed to t</context>
<context position="34869" citStr="Headden et al., 2009" startWordPosition="5568" endWordPosition="5571">del Incarnation WSJ10 WSJ20 WSJ∞ DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIN=0,ADAPT=0 65.3 53.8 47.9 50.8 BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIN=1,ADAPT=1 69.3 56.8 50.4 53.3 NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIN=1,ADAPT=1 67.3 56.2 50.1 51.6 WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIN=0,ADAPT=1 64.1 52.7 46.3 46.9 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen </context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Holm</author>
</authors>
<title>A simple sequentially rejective multiple test procedure.</title>
<date>1979</date>
<journal>Scandinavian Journal of Statistics,</journal>
<volume>6</volume>
<contexts>
<context position="25985" citStr="Holm, 1979" startWordPosition="4193" endWordPosition="4194">a simple linear regression, using accuracy on WSJ45 as the dependent variable. We prefer this full factorial design to the more traditional ablation studies because it allows us to account for and to incorporate every single experimental data point incurred along the 16In the standard statistical hypothesis testing setting, it is reasonable to expect that p% of randomly chosen hypotheses will appear significant at the p% level simply by chance. Consequently, multiple hypothesis testing requires re-evaluating significance levels — adjusting raw p-values, e.g., using the Holm-Bonferroni method (Holm, 1979). x-Entropy h (in bits per token) on WSJ15 lowest cross-entropy (4.32bpt) attained at WSJ8 bpt 5.5 5.0 1283 Corpus Marked Sentences All Sentences POS Tokens All Bracketings Multi-Token Bracketings BLOGt45 5,641 56,191 1,048,404 7,021 5,346 BLOG′t45 4,516 4,516 104,267 5,771 5,346 BLOGt15 1,562 23,214 212,872 1,714 1,240 BLOG′t15 1,171 1,171 11,954 1,288 1,240 NEWS45 304,129,910 2,263,563,078 32,119,123,561 611,644,606 477,362,150 NEWS′45 205,671,761 205,671,761 2,740,258,972 453,781,081 392,600,070 NEWS15 211,659,549 1,433,779,438 11,786,164,503 365,145,549 274,791,675 NEWS′15 147,848,358 147,</context>
</contexts>
<marker>Holm, 1979</marker>
<rawString>S. Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Inui</author>
<author>Y Kotani</author>
</authors>
<title>Robust N-gram based syntactic analysis using segmentation words.</title>
<date>2001</date>
<booktitle>In PACLIC.</booktitle>
<contexts>
<context position="35610" citStr="Inui and Kotani (2001)" startWordPosition="5681" endWordPosition="5684">efault run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses. We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised methods rely on fully-annotated seed corpora.18 9 Conclusion We explored novel ways of training dependency parsing models, the best of which attains 50.4% accuracy on Section 23 (all sentences) of WSJ, beating all previous unsupervised state-of-the-art by more than 5%. Extra gains stem from guiding</context>
</contexts>
<marker>Inui, Kotani, 2001</marker>
<rawString>N. Inui and Y. Kotani. 2001. Robust N-gram based syntactic analysis using segmentation words. In PACLIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1891" citStr="Klein and Manning (2004)" startWordPosition="269" endWordPosition="272">ly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP. 1 Introduction Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three pro</context>
<context position="6807" citStr="Klein and Manning, 2004" startWordPosition="1003" endWordPosition="1006">/i&gt;][VP reports [NP this][PP in the softest possible way]&lt;/a&gt;,[S stating only that ...]]] Combining parsing with mark-up may not be straight-forward, but there is hope: even above, 1Even when (American) grammar schools lived up to their name, they only taught dependencies. This was back in the days before constituent grammars were invented. 2http://nlp.stanford.edu:8080/parser/ one of each nested tag’s boundaries aligns; and Toronto Star’s neglected determiner could be forgiven, certainly within a dependency formulation. 3 A High-Level Outline of Our Approach Our idea is to implement the DMV (Klein and Manning, 2004) — a standard unsupervised grammar inducer. But instead of learning the unannotated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints. We still test on WSJ (Marcus et al., 1993), in the standard way, and also check generalization against a hidden data set — the Brown corpus (Francis and Kucera, 1979). Our parsing constraints come from a blog — a new corpus we created, the web and news (see Table 1 for corpora’s sentence and token counts). To facilitate future work, we make the final models and our manually-constructed blog da</context>
<context position="8279" citStr="Klein and Manning (2004)" startWordPosition="1231" endWordPosition="1234">Section 23 2,353 48,201 WSJ45 48,418 986,830 WSJ15 15,922 163,715 Brown100 24,208 391,796 BLOG, 57,809 1,136,659 BLOGt45 56,191 1,048,404 BLOGt15 23,214 212,872 NEWS45 2,263,563,078 32,119,123,561 NEWS15 1,433,779,438 11,786,164,503 WEB45 8,903,458,234 87,269,385,640 WEB15 7,488,669,239 55,014,582,024 Table 1: Sizes of corpora derived from WSJ and Brown, as well as those we collected from the web. 4 Data Sets for Evaluation and Training The appeal of unsupervised parsing lies in its ability to learn from surface text alone; but (intrinsic) evaluation still requires parsed sentences. Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty subtrees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. 3http://cs.stanford.edu/∼valentin/ 1279 Marked POS Bracketings Sentences Tokens All Multi-Token 6,047 1,136,659 7,731 6,015 of 57,809 149,483 7,731 6,015 4,934 124,527 6,482 6,015 3,295 85,423 4,476</context>
<context position="21521" citStr="Klein and Manning, 2004" startWordPosition="3451" endWordPosition="3454">ction with a (very rare) true negative example. Below, “CSA” modifies “authority” (to its left), appositively, while “AlManar” modifies “television” (to its right):13 The French broadcasting authority, &lt;a&gt;CSA, banned ... Al-Manar&lt;/a&gt; satellite television from ... 12This view evokes the trapezoids of the O(n3) recognizer for split head automaton grammars (Eisner and Satta, 1999). 13But this is a stretch, since the comma after “CSA” renders the marked phrase ungrammatical even out of context. ... the &lt;i&gt;Toronto Star&lt;/i&gt; reports ... 1282 6 Experimental Methods and Metrics We implemented the DMV (Klein and Manning, 2004), consulting the details of (Spitkovsky et al., 2010a). Crucially, we swapped out inside-outside re-estimation in favor of Viterbi training. Not only is it better-suited to the general problem (see §7.1), but it also admits a trivial implementation of (most of) the dependency constraints we proposed.14 4.5 WSJ� 5 10 15 20 25 30 35 40 45 Figure 1: Sentence-level cross-entropy on WSJ15 for Ad-Hoc∗ initializers of WSJ11, ... , 45}. Six settings parameterized each run: • INIT: 0 — default, uniform initialization; or 1 — a high quality initializer, pre-trained using Ad-Hoc∗ (Spitkovsky et al., 2010</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-H Lee</author>
<author>C-H Lin</author>
<author>B-H Juang</author>
</authors>
<title>A study on speaker adaptation of the parameters of continuous density Hidden Markov Models.</title>
<date>1991</date>
<journal>IEEE Trans. on Signal Processing,</journal>
<volume>39</volume>
<contexts>
<context position="23164" citStr="Lee et al., 1991" startWordPosition="3729" endWordPosition="3732">tences up to length 45, starting from the solution to sentences up to length 15, as recommended by Spitkovsky et al. (2010a). • CONSTR: if 4, strict; if 3, loose; and if 2, sprawl. We did not implement level 1, tear. Overconstrained sentences are re-attempted at successively lower levels until they become possible to parse, if necessary at the lowest (default) level 0.15 • TRIM: if 1, discards any sentence without a single multi-token mark-up (shorter than its length). • ADAPT: if 1, upon convergence, initializes retraining on WSJ45 using the solution to &lt;GENRE&gt;, attempting domain adaptation (Lee et al., 1991). These make for 294 meaningful combinations. We judged each one by its accuracy on WSJ45, using standard directed scoring — the fraction of correct dependencies over randomized “best” parse trees. 14We analyze the benefits of Viterbi training in a companion paper (Spitkovsky et al., 2010b), which dedicates more space to implementation and to the WSJ baselines used here. 15At level 4, &lt;b&gt; X&lt;u&gt; Y&lt;/b&gt; Z&lt;/u&gt; is over-constrained. 7 Discussion of Experimental Results Evaluation on Section 23 of WSJ and Brown reveals that blog-training beats all published stateof-the-art numbers in every traditional</context>
</contexts>
<marker>Lee, Lin, Juang, 1991</marker>
<rawString>C.-H. Lee, C.-H. Lin, and B.-H. Juang. 1991. A study on speaker adaptation of the parameters of continuous density Hidden Markov Models. IEEE Trans. on Signal Processing, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-H Lu</author>
<author>L-F Chien</author>
<author>H-J Lee</author>
</authors>
<title>Anchor text mining for translation of Web queries: A transitive translation approach.</title>
<date>2004</date>
<journal>ACM Trans. on Information Systems,</journal>
<volume>22</volume>
<contexts>
<context position="33223" citStr="Lu et al., 2004" startWordPosition="5357" endWordPosition="5360">he primary benefit of additional lower-quality data is in improved coverage. But with only 35 unique POS tags, data sparsity is hardly an issue. Extra examples of lexical items help little and hurt when they are mistagged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of </context>
</contexts>
<marker>Lu, Chien, Lee, 2004</marker>
<rawString>W.-H. Lu, L.-F. Chien, and H.-J. Lee. 2004. Anchor text mining for translation of Web queries: A transitive translation approach. ACM Trans. on Information Systems, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="7053" citStr="Marcus et al., 1993" startWordPosition="1045" endWordPosition="1048">ly taught dependencies. This was back in the days before constituent grammars were invented. 2http://nlp.stanford.edu:8080/parser/ one of each nested tag’s boundaries aligns; and Toronto Star’s neglected determiner could be forgiven, certainly within a dependency formulation. 3 A High-Level Outline of Our Approach Our idea is to implement the DMV (Klein and Manning, 2004) — a standard unsupervised grammar inducer. But instead of learning the unannotated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints. We still test on WSJ (Marcus et al., 1993), in the standard way, and also check generalization against a hidden data set — the Brown corpus (Francis and Kucera, 1979). Our parsing constraints come from a blog — a new corpus we created, the web and news (see Table 1 for corpora’s sentence and token counts). To facilitate future work, we make the final models and our manually-constructed blog data publicly available.3 Although we are unable to share larger-scale resources, our main results should be reproducible, as both linguistic analysis and our best model rely exclusively on the blog. Corpus Sentences POS Tokens WSJ∞ 49,208 1,028,34</context>
<context position="9541" citStr="Marcus et al., 1993" startWordPosition="1446" endWordPosition="1449">65 1,988 1,874 960 27,285 1,365 1,302 692 19,894 992 952 Marked POS Bracketings Sentences Tokens All Multi-Token 485 14,528 710 684 333 10,484 499 479 245 7,887 365 352 42 1,519 65 63 13 466 20 20 6 235 10 10 3 136 6 6 0 0 0 0 Length Cutoff 0 1 2 3 4 5 6 7 Length Cutoff 8 9 10 15 20 25 30 40 Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence). Our primary reference sets are derived from the Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ∞ (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (Francis and Kucera, 1979). While we use WSJ45 and WSJ15 to train baseline models, the bulk of our experiments is with web data. 4.1 A News-Style Blog: Daniel Pipes Since there was no corpus overlaying syntactic structure with mark-up, we began constructing a new one by downloading articles4 from a newsstyle blog. Although limited to a single genre — political opinion, danielpipes.org is clean, consistently form</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing. In</title>
<date>2006</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="36578" citStr="McClosky et al., 2006" startWordPosition="5826" endWordPosition="5830">18 9 Conclusion We explored novel ways of training dependency parsing models, the best of which attains 50.4% accuracy on Section 23 (all sentences) of WSJ, beating all previous unsupervised state-of-the-art by more than 5%. Extra gains stem from guiding Viterbi training with web mark-up, the loose constraint consistently delivering best results. Our linguistic analysis of a blog reveals that web annotations can be converted into accurate parsing constraints (loose: 88%; sprawl: 95%; tear: 99%) that could be helpful to supervised methods, e.g., by boosting an initial parser via self-training (McClosky et al., 2006) on sentences with mark-up. Similar techniques may apply to standard wordprocessing annotations, such as font changes, and to certain (balanced) punctuation (Briscoe, 1994). We make our blog data set, overlaying mark-up and syntax, publicly available. Its annotations are 18A significant effort expended in building a tree-bank comes with the first batch of sentences (Druck et al., 2009). 75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases, with traces of other phrases, clauses and fragments. The type of mark-up, combined with POS tags, could make for v</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In NAACL-HLT. R. Mihalcea and A. Csomai. 2007. Wikify!: Linking documents to encyclopedic knowledge. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="34031" citStr="Mintz et al., 2009" startWordPosition="5475" endWordPosition="5478">ambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model Incarnation WSJ10 WSJ20 WSJ∞ DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIN=0,ADAPT=0 65.3 53.8 47.9 50.8 BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIN=1,ADA</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-Y Nie</author>
<author>J Chen</author>
</authors>
<title>Exploiting the Web as parallel corpora for cross-language information retrieval. Web Intelligence.</title>
<date>2002</date>
<contexts>
<context position="33192" citStr="Nie and Chen, 2002" startWordPosition="5352" endWordPosition="5355">outcome from lexicalized models: The primary benefit of additional lower-quality data is in improved coverage. But with only 35 unique POS tags, data sparsity is hardly an issue. Extra examples of lexical items help little and hurt when they are mistagged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parse</context>
</contexts>
<marker>Nie, Chen, 2002</marker>
<rawString>J.-Y. Nie and J. Chen. 2002. Exploiting the Web as parallel corpora for cross-language information retrieval. Web Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2197" citStr="Pereira and Schabes (1992)" startWordPosition="316" endWordPosition="319">ion promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdetermined by unannotated text; (ii) fewer iterations needed to reach a good grammar, countering convergence properties that sha</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<contexts>
<context position="37250" citStr="Ratnaparkhi, 1999" startWordPosition="5931" endWordPosition="5932">ly to standard wordprocessing annotations, such as font changes, and to certain (balanced) punctuation (Briscoe, 1994). We make our blog data set, overlaying mark-up and syntax, publicly available. Its annotations are 18A significant effort expended in building a tree-bank comes with the first batch of sentences (Druck et al., 2009). 75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases, with traces of other phrases, clauses and fragments. The type of mark-up, combined with POS tags, could make for valuable features in discriminative models of parsing (Ratnaparkhi, 1999). A logical next step would be to explore the connection between syntax and mark-up for genres other than a news-style blog and for languages other than English. We are excited by the possibilities, as unsupervised parsers are on the cusp of becoming useful in their own right — recently, Davidov et al. (2009) successfully applied Seginer’s (2007) fully unsupervised grammar inducer to the problems of pattern-acquisition and extraction of semantic data. If the strength of the connection between web mark-up and syntactic structure is universal across languages and genres, this fact could have bro</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
<author>R Soricut</author>
</authors>
<title>Automatic prediction of parser accuracy.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16418" citStr="Ravi et al. (2008)" startWordPosition="2509" endWordPosition="2512">miners or adjectives to the left of the head. In fact, the only case that cannot be explained by dropped dependents is #8, where the daughters are marked but the parent is left out. Most instances contributing to this pattern are flat NPs that end with a noun, incorrectly assumed to be the head of all other words in the phrase, e.g., ... [NP a 1994 &lt;i&gt;New Yorker&lt;/i&gt; article] ... As this example shows, disagreements (as well as agreements) between mark-up and machinegenerated parse trees with automatically percolated heads should be taken with a grain of salt.11 11In a relatively recent study, Ravi et al. (2008) report that Charniak’s re-ranking parser (Charniak and Johnson, 2005) — reranking-parserAugH.tar.gz, also available from ftp://ftp.cs.brown.edu/pub/nlparser/ — attains 86.3% accuracy when trained on WSJ and tested against Brown; its nearly 5% performance loss out-of-domain is consistent with the numbers originally reported by Gildea (2001). Count POS Sequence 1,242 NNP NNP 643 NNP 419 NNP NNP NNP 414 NN 201 ]] NN 138 DT NNP NNP 138 NNS 112 ]] 102 VBD 92 DT NNP NNP NNP 85 ]] NNS 79 NNP NN 76 NN NN 61 VBN 60 NNP NNP NNP NNP Frac Sum 16.1% 8.3 24.4 5.4 29.8 5.4 35.2 2.6 37.8 1.8 39.5 1.8 41.3 1.</context>
</contexts>
<marker>Ravi, Knight, Soricut, 2008</marker>
<rawString>S. Ravi, K. Knight, and R. Soricut. 2008. Automatic prediction of parser accuracy. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="10583" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="1604" endWordPosition="1608">ark-up, we began constructing a new one by downloading articles4 from a newsstyle blog. Although limited to a single genre — political opinion, danielpipes.org is clean, consistently formatted, carefully edited and larger than WSJ (see Table 1). Spanning decades, Pipes’ editorials are mostly in-domain for POS taggers and tree-bank-trained parsers; his recent (internetera) entries are thoroughly cross-referenced, conveniently providing just the mark-up we hoped to study via uncluttered (printer-friendly) HTML.5 After extracting moderately clean text and mark-up locations, we used MxTerminator (Reynar and Ratnaparkhi, 1997) to detect sentence boundaries. This initial automated pass begot multiple rounds of various semi-automated clean-ups that involved fixing sentence breaking, modifying parser-unfriendly tokens, converting HTML entities and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional words (e.g., &lt;i&gt;basmachi&lt;/i&gt;s) and tokens (e.g., &lt;i&gt;Sesame Street&lt;/i&gt;-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u&gt;...][S...&lt;/u&gt; with &lt;u&gt;...&lt;/u&gt; ][S &lt;u&gt;...&lt;/u&gt;) and discarded any 4http://danielpipes.org/art/year/all</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R M Kaplan</author>
<author>R Crouch</author>
<author>J T Maxwell</author>
<author>M Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="35337" citStr="Riezler et al., 2002" startWordPosition="5639" endWordPosition="5643">3 56.2 50.1 51.6 WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIN=0,ADAPT=1 64.1 52.7 46.3 46.9 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses. We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised methods rely on fully-anno</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T. Maxwell, III, and M. Johnson. 2002. Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sassano</author>
</authors>
<title>Using a partially annotated corpus to build a dependency parser for Japanese. In</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="35412" citStr="Sassano, 2005" startWordPosition="5653" endWordPosition="5654"> 46.3 46.9 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets. be helpful in constraining grammar induction. Following Pereira and Schabes’ (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). There was demand for partially bracketed corpora. Chen and Lee (1995) constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses. We combine the two intuitions, using the web to build a partially parsed corpus. Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree. In contrast, traditional semi-supervised methods rely on fully-annotated seed corpora.18 9 Conclusion We explored novel ways of training depen</context>
</contexts>
<marker>Sassano, 2005</marker>
<rawString>M. Sassano. 2005. Using a partially annotated corpus to build a dependency parser for Japanese. In IJCNLP. Y. Seginer. 2007. Fast unsupervised incremental parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="2075" citStr="Spitkovsky et al., 2010" startWordPosition="297" endWordPosition="300">earning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdeterm</context>
<context position="21573" citStr="Spitkovsky et al., 2010" startWordPosition="3459" endWordPosition="3462">, “CSA” modifies “authority” (to its left), appositively, while “AlManar” modifies “television” (to its right):13 The French broadcasting authority, &lt;a&gt;CSA, banned ... Al-Manar&lt;/a&gt; satellite television from ... 12This view evokes the trapezoids of the O(n3) recognizer for split head automaton grammars (Eisner and Satta, 1999). 13But this is a stretch, since the comma after “CSA” renders the marked phrase ungrammatical even out of context. ... the &lt;i&gt;Toronto Star&lt;/i&gt; reports ... 1282 6 Experimental Methods and Metrics We implemented the DMV (Klein and Manning, 2004), consulting the details of (Spitkovsky et al., 2010a). Crucially, we swapped out inside-outside re-estimation in favor of Viterbi training. Not only is it better-suited to the general problem (see §7.1), but it also admits a trivial implementation of (most of) the dependency constraints we proposed.14 4.5 WSJ� 5 10 15 20 25 30 35 40 45 Figure 1: Sentence-level cross-entropy on WSJ15 for Ad-Hoc∗ initializers of WSJ11, ... , 45}. Six settings parameterized each run: • INIT: 0 — default, uniform initialization; or 1 — a high quality initializer, pre-trained using Ad-Hoc∗ (Spitkovsky et al., 2010a): we chose the Laplace-smoothed model trained at W</context>
<context position="23453" citStr="Spitkovsky et al., 2010" startWordPosition="3775" endWordPosition="3778">evels until they become possible to parse, if necessary at the lowest (default) level 0.15 • TRIM: if 1, discards any sentence without a single multi-token mark-up (shorter than its length). • ADAPT: if 1, upon convergence, initializes retraining on WSJ45 using the solution to &lt;GENRE&gt;, attempting domain adaptation (Lee et al., 1991). These make for 294 meaningful combinations. We judged each one by its accuracy on WSJ45, using standard directed scoring — the fraction of correct dependencies over randomized “best” parse trees. 14We analyze the benefits of Viterbi training in a companion paper (Spitkovsky et al., 2010b), which dedicates more space to implementation and to the WSJ baselines used here. 15At level 4, &lt;b&gt; X&lt;u&gt; Y&lt;/b&gt; Z&lt;/u&gt; is over-constrained. 7 Discussion of Experimental Results Evaluation on Section 23 of WSJ and Brown reveals that blog-training beats all published stateof-the-art numbers in every traditionally-reported length cutoff category, with news-training not far behind. Here is a mini-preview of these results, for Section 23 of WSJ10 and WSJ∞ (from Table 8): WSJ10 WSJ∞ (Cohen and Smith, 2009) 62.0 42.2 (Spitkovsky et al., 2010a) 57.1 45.0 NEWS-best 67.3 50.1 BLOGt-best 69.3 50.4 (Head</context>
<context position="28596" citStr="Spitkovsky et al., 2010" startWordPosition="4557" endWordPosition="4560">est-performing runs. A major imperfection of the simple regression model is that helpful factors that require an interaction to “kick in” may not, on their own, appear statistically significant. Our third diagnostic is to examine parameter settings that give rise to the best-performing models, looking out for combinations that consistently deliver superior results. 7.1 WSJ Baselines Just two parameters apply to learning from WSJ. Five of their six combinations are state-of-the-art, demonstrating the power of Viterbi training; only the default run scores worse than 45.0%, attained by Leapfrog (Spitkovsky et al., 2010a), on WSJ45: Settings SCOPE=0 SCOPE=1 SCOPE=2 INIT=0 41.3 45.0 45.2 1 46.6 47.5 47.6 @45 @15 @15→45 7.2 Blog Simply training on BLOGt instead of WSJ hurts: GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2 INIT=0 39.6 36.9 36.9 1 46.5 46.3 46.4 @45 @15 @15→45 The best runs use a good initializer, discard unannotated sentences, enforce the loose constraint on the rest, follow up with domain adaptation and benefit from re-training — GENRE=TRIM=ADAPT=1: INIT=1 SCOPE=0 SCOPE=1 SCOPE=2 CONSTR=0 45.8 48.3 49.6 (sprawl) 2 46.3 49.2 49.2 (loose) 3 41.3 50.2 50.4 (strict) 4 40.7 49.9 48.7 @45 @15 @15→45 The contrast be</context>
<context position="32098" citStr="Spitkovsky et al., 2010" startWordPosition="5169" endWordPosition="5172">web sentences:17 POS Sequence WEB Count Sample web sentence, chosen uniformly at random. 1 DT NNS VBN 82,858,487 All rights reserved. 2 NNP NNP NNP 65,889,181 Yuasa et al. 3 NN IN TO VB RB 31,007,783 Sign in to YouTube now! 4 NN IN IN PRP$ 11 NN 31,007,471 Sign in with your Google Account! 17Further evidence: TnT tags the ubiquitous but ambiguous fragments “click here” and “print post” as noun phrases. 7.5 The State of the Art Our best model gains more than 5% over previous state-of-the-art accuracy across all sentences of WSJ’s Section 23, more than 8% on WSJ20 and rivals the oracle skyline (Spitkovsky et al., 2010a) on WSJ10; these gains generalize to Brown100, where it improves by nearly 10% (see Table 8). We take solace in the fact that our best models agree in using loose constraints. Of these, the models trained with less data perform better, with the best two using trimmed data sets, echoing that “less is more” (Spitkovsky et al., 2010a), pace Halevy et al. (2009). We note that orders of magnitude more data did not improve parsing performance further and suspect a different outcome from lexicalized models: The primary benefit of additional lower-quality data is in improved coverage. But with only </context>
<context position="34404" citStr="Spitkovsky et al., 2010" startWordPosition="5526" endWordPosition="5529">ing a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model Incarnation WSJ10 WSJ20 WSJ∞ DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIN=0,ADAPT=0 65.3 53.8 47.9 50.8 BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIN=1,ADAPT=1 69.3 56.8 50.4 53.3 NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIN=1,ADAPT=1 67.3 56.2 50.1 51.6 WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIN=0,ADAPT=1 64.1 52.7 46.3 46.9 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="2075" citStr="Spitkovsky et al., 2010" startWordPosition="297" endWordPosition="300">earning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version of this problem that targets dependencies and assumes partial annotation — sentence boundaries and part-of-speech (POS) tagging — has received much attention. Klein and Manning (2004) were the first to beat a simple parsing heuristic, the right-branching baseline; today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010a) are rooted in their Dependency Model with Valence (DMV), still trained using variants of EM. Pereira and Schabes (1992) outlined three major problems with classic EM, applied to a related problem, constituent parsing. They extended classic inside-outside re-estimation (Baker, 1979) to respect any bracketing constraints included with a training corpus. This conditioning on partial parses addressed all three problems, leading to: (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdeterm</context>
<context position="21573" citStr="Spitkovsky et al., 2010" startWordPosition="3459" endWordPosition="3462">, “CSA” modifies “authority” (to its left), appositively, while “AlManar” modifies “television” (to its right):13 The French broadcasting authority, &lt;a&gt;CSA, banned ... Al-Manar&lt;/a&gt; satellite television from ... 12This view evokes the trapezoids of the O(n3) recognizer for split head automaton grammars (Eisner and Satta, 1999). 13But this is a stretch, since the comma after “CSA” renders the marked phrase ungrammatical even out of context. ... the &lt;i&gt;Toronto Star&lt;/i&gt; reports ... 1282 6 Experimental Methods and Metrics We implemented the DMV (Klein and Manning, 2004), consulting the details of (Spitkovsky et al., 2010a). Crucially, we swapped out inside-outside re-estimation in favor of Viterbi training. Not only is it better-suited to the general problem (see §7.1), but it also admits a trivial implementation of (most of) the dependency constraints we proposed.14 4.5 WSJ� 5 10 15 20 25 30 35 40 45 Figure 1: Sentence-level cross-entropy on WSJ15 for Ad-Hoc∗ initializers of WSJ11, ... , 45}. Six settings parameterized each run: • INIT: 0 — default, uniform initialization; or 1 — a high quality initializer, pre-trained using Ad-Hoc∗ (Spitkovsky et al., 2010a): we chose the Laplace-smoothed model trained at W</context>
<context position="23453" citStr="Spitkovsky et al., 2010" startWordPosition="3775" endWordPosition="3778">evels until they become possible to parse, if necessary at the lowest (default) level 0.15 • TRIM: if 1, discards any sentence without a single multi-token mark-up (shorter than its length). • ADAPT: if 1, upon convergence, initializes retraining on WSJ45 using the solution to &lt;GENRE&gt;, attempting domain adaptation (Lee et al., 1991). These make for 294 meaningful combinations. We judged each one by its accuracy on WSJ45, using standard directed scoring — the fraction of correct dependencies over randomized “best” parse trees. 14We analyze the benefits of Viterbi training in a companion paper (Spitkovsky et al., 2010b), which dedicates more space to implementation and to the WSJ baselines used here. 15At level 4, &lt;b&gt; X&lt;u&gt; Y&lt;/b&gt; Z&lt;/u&gt; is over-constrained. 7 Discussion of Experimental Results Evaluation on Section 23 of WSJ and Brown reveals that blog-training beats all published stateof-the-art numbers in every traditionally-reported length cutoff category, with news-training not far behind. Here is a mini-preview of these results, for Section 23 of WSJ10 and WSJ∞ (from Table 8): WSJ10 WSJ∞ (Cohen and Smith, 2009) 62.0 42.2 (Spitkovsky et al., 2010a) 57.1 45.0 NEWS-best 67.3 50.1 BLOGt-best 69.3 50.4 (Head</context>
<context position="28596" citStr="Spitkovsky et al., 2010" startWordPosition="4557" endWordPosition="4560">est-performing runs. A major imperfection of the simple regression model is that helpful factors that require an interaction to “kick in” may not, on their own, appear statistically significant. Our third diagnostic is to examine parameter settings that give rise to the best-performing models, looking out for combinations that consistently deliver superior results. 7.1 WSJ Baselines Just two parameters apply to learning from WSJ. Five of their six combinations are state-of-the-art, demonstrating the power of Viterbi training; only the default run scores worse than 45.0%, attained by Leapfrog (Spitkovsky et al., 2010a), on WSJ45: Settings SCOPE=0 SCOPE=1 SCOPE=2 INIT=0 41.3 45.0 45.2 1 46.6 47.5 47.6 @45 @15 @15→45 7.2 Blog Simply training on BLOGt instead of WSJ hurts: GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2 INIT=0 39.6 36.9 36.9 1 46.5 46.3 46.4 @45 @15 @15→45 The best runs use a good initializer, discard unannotated sentences, enforce the loose constraint on the rest, follow up with domain adaptation and benefit from re-training — GENRE=TRIM=ADAPT=1: INIT=1 SCOPE=0 SCOPE=1 SCOPE=2 CONSTR=0 45.8 48.3 49.6 (sprawl) 2 46.3 49.2 49.2 (loose) 3 41.3 50.2 50.4 (strict) 4 40.7 49.9 48.7 @45 @15 @15→45 The contrast be</context>
<context position="32098" citStr="Spitkovsky et al., 2010" startWordPosition="5169" endWordPosition="5172">web sentences:17 POS Sequence WEB Count Sample web sentence, chosen uniformly at random. 1 DT NNS VBN 82,858,487 All rights reserved. 2 NNP NNP NNP 65,889,181 Yuasa et al. 3 NN IN TO VB RB 31,007,783 Sign in to YouTube now! 4 NN IN IN PRP$ 11 NN 31,007,471 Sign in with your Google Account! 17Further evidence: TnT tags the ubiquitous but ambiguous fragments “click here” and “print post” as noun phrases. 7.5 The State of the Art Our best model gains more than 5% over previous state-of-the-art accuracy across all sentences of WSJ’s Section 23, more than 8% on WSJ20 and rivals the oracle skyline (Spitkovsky et al., 2010a) on WSJ10; these gains generalize to Brown100, where it improves by nearly 10% (see Table 8). We take solace in the fact that our best models agree in using loose constraints. Of these, the models trained with less data perform better, with the best two using trimmed data sets, echoing that “less is more” (Spitkovsky et al., 2010a), pace Halevy et al. (2009). We note that orders of magnitude more data did not improve parsing performance further and suspect a different outcome from lexicalized models: The primary benefit of additional lower-quality data is in improved coverage. But with only </context>
<context position="34404" citStr="Spitkovsky et al., 2010" startWordPosition="5526" endWordPosition="5529">ing a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) parsing. Although restricted to proper noun phrases, dates, times and quantities, we suspect that constituents identified by trained (supervised) NER systems would also 1285 Model Incarnation WSJ10 WSJ20 WSJ∞ DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Leapfrog (Spitkovsky et al., 2010a) 57.1 48.7 45.0 43.6 default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIN=0,ADAPT=0 55.9 45.8 41.6 40.5 WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIN=0,ADAPT=0 65.3 53.8 47.9 50.8 BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIN=1,ADAPT=1 69.3 56.8 50.4 53.3 NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIN=1,ADAPT=1 67.3 56.2 50.1 51.6 WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIN=0,ADAPT=1 64.1 52.7 46.3 46.9 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 8: Accuracies on Section 23 of WSJ{10, 20,∞ } and Brown100 for three recent state-of-the-art systems, our default run, and </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010b. Viterbi training improves unsupervised dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Tan</author>
<author>F Peng</author>
</authors>
<title>Unsupervised query segmentation using generative language models and Wikipedia.</title>
<date>2008</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="33366" citStr="Tan and Peng, 2008" startWordPosition="5376" endWordPosition="5379">sue. Extra examples of lexical items help little and hurt when they are mistagged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-</context>
</contexts>
<marker>Tan, Peng, 2008</marker>
<rawString>B. Tan and F. Peng. 2008. Unsupervised query segmentation using generative language models and Wikipedia. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In EMNLP-VLC.</booktitle>
<contexts>
<context position="11377" citStr="Toutanova and Manning, 2000" startWordPosition="1705" endWordPosition="1708"> parser-unfriendly tokens, converting HTML entities and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional words (e.g., &lt;i&gt;basmachi&lt;/i&gt;s) and tokens (e.g., &lt;i&gt;Sesame Street&lt;/i&gt;-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u&gt;...][S...&lt;/u&gt; with &lt;u&gt;...&lt;/u&gt; ][S &lt;u&gt;...&lt;/u&gt;) and discarded any 4http://danielpipes.org/art/year/all 5http://danielpipes.org/article print.php? id=... tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C. D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger. In EMNLP-VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="11402" citStr="Toutanova et al., 2003" startWordPosition="1709" endWordPosition="1712">nverting HTML entities and non-ASCII text, correcting typos, and so on. After throwing away annotations of fractional words (e.g., &lt;i&gt;basmachi&lt;/i&gt;s) and tokens (e.g., &lt;i&gt;Sesame Street&lt;/i&gt;-like), we broke up all markup that crossed sentence boundaries (i.e., loosely speaking, replaced constructs like &lt;u&gt;...][S...&lt;/u&gt; with &lt;u&gt;...&lt;/u&gt; ][S &lt;u&gt;...&lt;/u&gt;) and discarded any 4http://danielpipes.org/art/year/all 5http://danielpipes.org/article print.php? id=... tags left covering entire sentences. We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000; Toutanova et al., 2003),6 and BLOGp, parsed with Charniak’s parser (Charniak, 2001; Charniak and Johnson, 2005).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (and non-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web We built a large (see Table 1) but messy data set, WEB — English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system. We </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vadas</author>
<author>J R Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5910" citStr="Vadas and Curran (2007)" startWordPosition="870" endWordPosition="873"> established in &lt;i&gt;[NP The New Republic]&lt;/i&gt;]&lt;/a&gt; and Bill Clinton just &lt;a&gt;[VP confirmed in his memoirs]&lt;/a&gt;, Netanyahu changed his mind and ... In doing so, mark-up sometimes offers useful cues even for low-level tokenization decisions: [NP [NP Libyan ruler] &lt;a&gt;[NP Mu‘ammar al-Qaddafi]&lt;/a&gt;] referred to ... (NP (ADJP (NP (JJ Libyan) (NN ruler)) (JJ Nu)) (“‘) (NN ammar) (NNS al-Qaddafi)) Above, a backward quote in an Arabic name confuses the Stanford parser.2 Yet mark-up lines up with the broken noun phrase, signals cohesion, and moreover sheds light on the internal structure of a compound. As Vadas and Curran (2007) point out, such details are frequently omitted even from manually compiled tree-banks that err on the side of flat annotations of base-NPs. Admittedly, not all boundaries between HTML tags and syntactic constituents match up nicely: ..., but [S [NP the &lt;a&gt;&lt;i&gt;Toronto Star&lt;/i&gt;][VP reports [NP this][PP in the softest possible way]&lt;/a&gt;,[S stating only that ...]]] Combining parsing with mark-up may not be straight-forward, but there is hope: even above, 1Even when (American) grammar schools lived up to their name, they only taught dependencies. This was back in the days before constituent grammars</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>D. Vadas and J. R. Curran. 2007. Adding noun phrase structure to the Penn Treebank. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Watanabe</author>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>A graph-based approach to named entity categorization in Wikipedia using conditional random fields.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL.</booktitle>
<contexts>
<context position="33325" citStr="Watanabe et al., 2007" startWordPosition="5370" endWordPosition="5373">ique POS tags, data sparsity is hardly an issue. Extra examples of lexical items help little and hurt when they are mistagged. 8 Related Work The wealth of new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finke</context>
</contexts>
<marker>Watanabe, Asahara, Matsumoto, 2007</marker>
<rawString>Y. Watanabe, M. Asahara, and Y. Matsumoto. 2007. A graph-based approach to named entity categorization in Wikipedia using conditional random fields. In EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Yeh</author>
<author>D Ramage</author>
<author>C D Manning</author>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>WikiWalk: Random walks on Wikipedia for semantic relatedness. In TextGraphs.</title>
<date>2009</date>
<contexts>
<context position="33476" citStr="Yeh et al., 2009" startWordPosition="5391" endWordPosition="5394">new annotations produced in many languages every day already fuels a number of NLP applications. Following their early and wide-spread use by search engines, in service of spam-fighting and retrieval, anchor text and link data enhanced a variety of traditional NLP techniques: cross-lingual information retrieval (Nie and Chen, 2002), translation (Lu et al., 2004), both named-entity recognition (Mihalcea and Csomai, 2007) and categorization (Watanabe et al., 2007), query segmentation (Tan and Peng, 2008), plus semantic relatedness and word-sense disambiguation (Gabrilovich and Markovitch, 2007; Yeh et al., 2009). Yet several, seemingly natural, candidate core NLP tasks — tokenization, CJK segmentation, noun-phrase chunking, and (until now) parsing — remained conspicuously uninvolved. Approaches related to ours arise in applications that combine parsing with named-entity recognition (NER). For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al., 2009), that couple chunking with (dependency) pars</context>
</contexts>
<marker>Yeh, Ramage, Manning, Agirre, Soroa, 2009</marker>
<rawString>E. Yeh, D. Ramage, C. D. Manning, E. Agirre, and A. Soroa. 2009. WikiWalk: Random walks on Wikipedia for semantic relatedness. In TextGraphs.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>