<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020881">
<title confidence="0.629108">
Extending MARIE: an N-gram-based SMT decoder
</title>
<author confidence="0.863418">
Josep M. Crego
</author>
<affiliation confidence="0.825752">
TALP Research Center
</affiliation>
<address confidence="0.862502">
Universitat Polit`ecnica de Catalunya
Barcelona, 08034
</address>
<email confidence="0.999043">
jmcrego@gps.tsc.upc.edu
</email>
<author confidence="0.653737">
Jos´e B. Marifno
</author>
<affiliation confidence="0.626414">
TALP Research Center
</affiliation>
<address confidence="0.733106">
Universitat Polit`ecnica de Catalunya
Barcelona,08034
</address>
<email confidence="0.996568">
canton@gps.tsc.upc.edu
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999275846153846">
In this paper we present several extensions of
MARIE1, a freely available N-gram-based sta-
tistical machine translation (SMT) decoder. The
extensions mainly consist of the ability to ac-
cept and generate word graphs and the intro-
duction of two new N-gram models in the log-
linear combination of feature functions the de-
coder implements. Additionally, the decoder is
enhanced with a caching strategy that reduces
the number of N-gram calls improving the over-
all search efficiency. Experiments are carried out
over the Eurpoean Parliament Spanish-English
translation task.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878176470588">
Research on SMT has been strongly boosted in the last
few years, partially thanks to the relatively easy develop-
ment of systems with enough competence as to achieve
rather competitive results. In parallel, tools and tech-
niques have grown in complexity, which makes it diffi-
cult to carry out state-of-the-art research without sharing
some of this toolkits. Without aiming at being exhaus-
tive, GIZA++2, SRILM3 and PHARAOH4 are probably
the best known examples.
We introduce the recent extensions made to an N-
gram-based SMT decoder (Crego et al., 2005), which al-
lowed us to tackle several translation issues (such as re-
ordering, rescoring, modeling, etc.) successfully improv-
ing accuracy, as well as efficiency results.
As far as SMT can be seen as a double-sided prob-
lem (modeling and search), the decoder emerges as a key
component, core module of any SMT system. Mainly,
</bodyText>
<footnote confidence="0.99999">
1http://gps-tsc.upc.es/soft/soft/marie
2http://www.fjoch.com/GIZA++.html
3http://www.speech.sri.com/projects/srilm/
4http://www.isi.edu/publications/licensed-sw/pharaoh/
</footnote>
<page confidence="0.998781">
213
</page>
<bodyText confidence="0.9998821875">
any technique aiming at dealing with a translation prob-
lem needs for a decoder extension to be implemented.
Particularly, the reordering problem can be more effi-
ciently (and accurate) addressed when tightly coupled
with decoding. In general, the competence of a decoder
to make use of the maximum of information in the global
search is directly connected with the likeliness of suc-
cessfully improving translations.
The paper is organized as follows. In Section 2 we
and briefly review the previous work on decoding with
special attention to N-gram-based decoding. Section 3
describes the extended log-linear combination of feature
functions after introduced the two new models. Section
4 details the particularities of the input and output word
graph extensions. Experiments are reported on section 5.
Finally, conclusions are drawn in section 6.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99974475">
The decoding problem in SMT is expressed by the next
maximization: arg maxiI �∈, P (tijsi ), where si is the
source sentence to translate and ti is a possible transla-
tion of the set T, which contains all the sentences of the
language of ti.
Given that the full search over the whole set of tar-
get language sentences is impracticable (T is an infinite
set), the translation sentence is usually built incremen-
tally, composing partial translations of the source sen-
tence, which are selected out of a limited number of trans-
lation candidates (translation units).
The first SMT decoders were word-based. Hence,
working with translation candidates of single source
words. Later appeared the phrase-based decoders,
which use translation candidates composed of sequences
of source and target words (outperforming the word-
based decoders by introducing the word context). In the
last few years syntax-based decoders have emerged aim-
ing at dealing with pair of languages with different syn-
tactical structures for which the word context introduced
</bodyText>
<note confidence="0.530947">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 213–216,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999403">
Figure 1: Generative process. Phrase-based (left) and N-gram-based (right) approaches.
</figureCaption>
<bodyText confidence="0.999932904761905">
in phrase-based decoders is not sufficient to cope with
long reorderings.
Like standard phrase-based decoders, MARIE em-
ploys translation units composed of sequences of source
and target words. In contrast, the translation con-
text is differently taken into account. Whereas phrase-
based decoders employ translation units uncontextual-
ized, MARIE takes the translation unit context into ac-
count by estimating the translation model as a standard
N-gram language model (N-gram-based decoder).
Figure 1 shows that both approaches follow the same
generative process, but they differ on the structure of
translation units. In the example, the units ’s1#t1’ and
’s2 s3#t2 t3’ of the N-gram-based approach are used
considering that both appear sequentially. This fact can
be understood as using a longer unit that includes both
(longer units are drawn in grey).
MARIE follows the maximum entropy framework,
where we can define a translation hypothesis t given a
source sentence s, as the target sentence maximizing a
log-linear combination of feature functions:
</bodyText>
<equation confidence="0.994409">
M
� ti = arg max Amhm(si,ti) } (1)
tI J
1 m=1
</equation>
<bodyText confidence="0.999993">
where Am corresponds to the weighting coefficients of
the log-linear combination, and the feature functions
hm(s, t) to a logarithmic scaling of the probabilities of
each model. See (Mari˜no et al., 2006) for further details
on the N-gram-based approach to SMT.
</bodyText>
<sectionHeader confidence="0.973282" genericHeader="method">
3 N-gram Feature Functions
</sectionHeader>
<bodyText confidence="0.999844">
Two language models (LM) are introduced in equation 1,
aiming at helping the decoder to find the right transla-
tions. Both are estimated as standard N-gram LM.
</bodyText>
<subsectionHeader confidence="0.997249">
3.1 Target-side N-gram LM
</subsectionHeader>
<bodyText confidence="0.9998689">
The first additional N-gram LM is destinated to be ap-
plied over the target sentence (tagged) words. Hence,
as the original target LM (computed over raw words),
it is also used to score the fluency of target sentences,
but aiming at achieving generalization power through us-
ing a more generalized language (such as a language of
Part-of-Speech tags) instead of the one composed of raw
words. Part-Of-Speech tags have successfully been used
in several previous experiments. however, any other tag
can be applied.
Several sequences of target tags may apply to any given
translation unit (which are passed to the decoder before it
starts the search). For instance, regarding a translation
unit with the english word ’general’ in its target side, if
POS tags were used as target tagged tags, there would ex-
ist at least two different tag options: noun and adjective.
In the search, multiple hypotheses are generated con-
cerning different target tagged sides (sequences of tags)
of a single translation unit. Therefore, on the one side, the
overall search is extended towards seeking the sequence
of target tags that better fits the sequence of target raw
words. On the other side, this extension is hurting the
overall efficiency of the decoder as additional hypotheses
appear in the search stacks while not additional transla-
tion hypotheses are being tested (only differently tagged).
This extended feature may be used toghether with a
limitation of the number of target tagged hypotheses per
translation unit. The use of a limited number of these
hypotheses implies a balance between accuracy and effi-
ciency.
</bodyText>
<subsectionHeader confidence="0.998174">
3.2 Source-side N-gram LM
</subsectionHeader>
<bodyText confidence="0.990500411764706">
The second N-gram LM is applied over the input sen-
tence tagged words. Obviously, this model only makes
sense when reordering is applied over the source words
in order to monotonize the source and target word order.
In such a case, the tagged LM is learnt over the training
set with reordered source words.
Hence, the new model is employed as a reordering
model. It scores a given source-side reordering hypoth-
esis according to the reorderings made in the training
sentences (from which the tagged LM is estimated). As
for the previous extension, source tagged words are used
instead of raw words in order to achieve generalization
power.
Additional hypotheses regarding the same translation
unit are not generated in the search as all input sentences
are uniquely tagged.
Figure 2 illustrates the use of a source POS-tagged N-
</bodyText>
<page confidence="0.993471">
214
</page>
<bodyText confidence="0.9987426">
gram LM. The probability of the sequence ’PRN VRB
NAME ADJ’ is greater than the probability of the se-
quence ’PRN VRB ADJ NAME’ for a model estimated
over the training set with reordered source words (with
english words following the spanish word order).
</bodyText>
<figureCaption confidence="0.984633">
Figure 2: Source POS-tagged N-gram LM.
</figureCaption>
<subsectionHeader confidence="0.990649">
3.3 Caching N-grams
</subsectionHeader>
<bodyText confidence="0.999872923076923">
The use of several N-gram LM’s implies a reduction in
efficiency in contrast to other models that can be imple-
mented by means of a single lookup table (one access per
probability call). The special characteristics of Ngram
LM’s introduce additional memory access to account for
backoff probabilities and lower Ngrams fallings.
Many N-gram calls are requested repeatedly, produc-
ing multiple calls of an entry. A simple strategy to reduce
additional access consists of keeping a record (cache) for
those Ngram entries already requested. A drawback for
the use of a cache consists of the additional memory ac-
cess derived of the cache maintenance (adding new and
checking for existing entries).
</bodyText>
<figureCaption confidence="0.991303">
Figure 3: Memory access derived of an N-gram call.
</figureCaption>
<bodyText confidence="0.998202888888889">
Figure 3 illustrates this situation. The call for a 3-gram
probability (requesting for the probability of the sequence
of tokens ’a b c’) may need for up to 6 memory access,
while under a phrase-based translation model the final
probability would always be reached after the first mem-
ory access. The additional access in the N-gram-based
approach are used to provide lower N-gram and backoff
probabilities in those cases that upper N-gram probabili-
ties do not exist.
</bodyText>
<sectionHeader confidence="0.988915" genericHeader="method">
4 Word Graphs
</sectionHeader>
<bodyText confidence="0.999774076923077">
Word graphs are successfully used in SMT for several ap-
plications. Basically, with the objective of reducing the
redundancy of N-best lists, which very often convey se-
rious combinatorial explosion problems.
A word graph is here described as a directed acyclic
graph G = (V, E) with one root node n0 E V .
Edges are labeled with tokens (words or translation units)
and optionally with accumulated scores. We will use
(ns(ne ′′t′′ s)), to denote an edge starting at node ns and
ending at node ne, with token t and score s. The file
format of word graphs coincides with the graph file for-
mat recognized by the CARMEL5 finite state automata
toolkit.
</bodyText>
<subsectionHeader confidence="0.989229">
4.1 Input Graph
</subsectionHeader>
<bodyText confidence="0.999946625">
We can mainly find two applications for which word
graphs are used as input of an SMT system: the recog-
nition output of an automatic speech recognition (ASR)
system; and a reordering graph, consisting of a subset of
the whole word permutations of a given input sentence.
In our case we are using the input graph as a reorder-
ing graph. The decoder introduces reordering (distortion
of source words order) by allowing only for the distor-
tion encoded in the input graph. Though, the graph is
only allowed to encode permutations of the input words.
In other words, any path in the graph must start at node
n0, finish at node nN (where nN is a unique ending node)
and cover all the input words (tokens t) in whatever order,
without repetitions.
An additional feature function (distortion model) is in-
troduced in the log-linear combination of equation 1:
</bodyText>
<equation confidence="0.995598666666667">
kI
pdistortion(uk) Pz H p(ni|ni−1) (2)
i=k1
</equation>
<bodyText confidence="0.999959421052632">
where uk refers to the kth partial translation unit covering
the source positions [k1,..., kj]. p(ni|ni−1) corresponds
to the edge score s encoded in the edge (ns(ne ′′t′′ s)),
where ni = ne and ni−1 = ns.
One of the decoding first steps consists of building
(for each input sentence) the set of translation units to be
used in the search. When the search is extended with re-
ordering abilities the set must be also extended with those
translation units that cover any sequence of input words
following any of the word orders encoded in the input
graph. The extension of the units set is specially relevant
when translation units are built from the tranining set with
reordered source words.
Given the example of figure 2, if the translation unit
’translations perfect # traducciones perfectas’ is avail-
able, the decoder should not discard it, as it provides
a right translation. Notwithstanding that its source side
does not follow the original word order of the input sen-
tence.
</bodyText>
<subsectionHeader confidence="0.991688">
4.2 Output Graph
</subsectionHeader>
<bodyText confidence="0.998629">
The goal of using an output graph is to allow for further
rescoring work. That is, to work with alternative transla-
</bodyText>
<footnote confidence="0.982764">
5http://www.isi.edu/licensed-sw/carmel/
</footnote>
<page confidence="0.998674">
215
</page>
<bodyText confidence="0.999733588235294">
tions to the single 1-best. Therefore, our proposed output
graph has some peculiarities that make it different to the
previously sketched intput graph.
The structure of edges remains the same, but obvi-
ously, paths are not forced to consist of permutations of
the same tokens (as far as we are interested into multiple
translation hypotheses), and there may also exist paths
which do not reach the ending node nN. These latter
paths are not useful in rescoring tasks, but allowed in or-
der to facilitate the study of the search graph. However,
a very easy and efficient algorithm (O(n), being n the
search size) can be used in order to discard them, before
rescoring work. Additionally, given that partial model
costs are needed in rescoring work, our decoder allows
to output the individual model costs computed for each
translation unit (token t). Costs are encoded within the
token s, as in the next example:
</bodyText>
<equation confidence="0.874232">
(0 (1 &amp;quot;o#or{1.5,0.9,0.6,0.2}&amp;quot; 6))
</equation>
<bodyText confidence="0.999945555555555">
where the token t is now composed of the translation unit
’o#or’, followed by (four) model costs.
Multiple translation hypotheses can only be extracted
if hypotheses recombinations are carefully saved. As in
(Koehn, 2004), the decoder takes a record of any recom-
bined hypothesis, allowing for a rigorous N-best genera-
tion. Model costs are referred to the current unit while the
global score s is accumulated. Notice also that translation
units (not words) are now used as tokens.
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.979173666666667">
Experiments are carried out for a Spanish-to-English
translation task using the EPPS data set, corresponding
to session transcriptions of the European Parliament.
</bodyText>
<table confidence="0.860862857142857">
Eff. base +tpos +reor +spos
Beam size = 50
w/o cache 1,820 2,170 2,970 3,260
w/ cache −50 −110 −190 −210
Beam size = 100
w/o cache 2,900 4,350 5,960 6,520
w/ cache −175 −410 −625 −640
</table>
<tableCaption confidence="0.999904">
Table 1: Translation efficiency results.
</tableCaption>
<bodyText confidence="0.990270625">
Table 1 shows translation efficiency results (mea-
sured in seconds) given two different beam search sizes.
w/cache and w/o cache indicate whether the decoder em-
ploys (or not) the cache technique (section 3.3). Sev-
eral system configuration have been tested: a baseline
monotonous system using a 4-gram translation LM and
a 5-gram target LM (base), extended with a target POS-
tagged 5-gram LM (+tpos), further extended by allow-
ing for reordering (+reor), and finally using a source-side
POS-tagged 5-gram LM (+spos).
As it can be seen, the cache technique improves the ef-
ficiency of the search in terms of decoding time. Time
results are further decreased (reduced time is shown for
the w/ cache setting) by using more N-gram LM and al-
lowing for a larger search graph (increasing the beam size
and introducing distortion).
Further details on the previous experiment can be
seen in (Crego and Mari˜no, 2006b; Crego and Mari˜no,
2006a), where additionally, the input word graph and ex-
tended N-gram tagged LM’s are successfully used to im-
prove accuracy at a very low computational cost.
Several publications can also be found in bibliography
which show the use of output graphs in rescoring tasks
allowing for clear accuracy improvements.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999946333333333">
We have presented several extensions to MARIE, a freely
available N-gram-based decoder. The extensions consist
of accepting and generating word graphs, and introducing
two N-gram LM’s over source and target tagged words.
Additionally, a caching technique is applied over the N-
gram LM’s.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996176">
This work has been funded by the European Union un-
der the integrated project TC-STAR - (IST-2002-FP6-
5067-38), the Spanish Government under the project
AVIVAVOZ - (TEC2006-13694-C03) and the Universitat
Polit`ecnica de Catalunya under UPC-RECERCA grant.
</bodyText>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988463863636364">
J.M. Crego and J.B. Mari˜no. 2006a. Integration of
postag-based source reordering into smt decoding by
an extended search graph. Proc. of the 7th Conf. of the
Association for Machine Translation in the Americas,
pages 29–36, August.
J.M. Crego and J.B. Mari˜no. 2006b. Reordering experi-
ments for n-gram-based smt. 1stIEEE/ACL Workshop
on Spoken Language Technology, December.
J.M. Crego, J.B. Mari˜no, and A. de Gispert. 2005. An
ngram-based statistical machine translation decoder.
Proc. of the 9th European Conference on Speech Com-
munication and Technology, Interspeech’05, pages
3193–3196, September.
Ph. Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. Proc. of the 6th Conf. of the Association for Ma-
chine Translation in the Americas, pages 115–124, Oc-
tober.
J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527–549.
</reference>
<page confidence="0.999146">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884147">
<title confidence="0.98868">MARIE: an SMT decoder</title>
<author confidence="0.999993">Josep M Crego</author>
<affiliation confidence="0.9997845">TALP Research Center Universitat Polit`ecnica de Catalunya</affiliation>
<address confidence="0.999857">Barcelona, 08034</address>
<email confidence="0.999799">jmcrego@gps.tsc.upc.edu</email>
<author confidence="0.999641">Jos´e B Marifno</author>
<affiliation confidence="0.9996685">TALP Research Center Universitat Polit`ecnica de Catalunya</affiliation>
<address confidence="0.916931">Barcelona,08034</address>
<email confidence="0.999821">canton@gps.tsc.upc.edu</email>
<abstract confidence="0.998347142857143">In this paper we present several extensions of a freely available statistical machine translation (SMT) decoder. The extensions mainly consist of the ability to accept and generate word graphs and the introof two new models in the loglinear combination of feature functions the decoder implements. Additionally, the decoder is enhanced with a caching strategy that reduces number of calls improving the overall search efficiency. Experiments are carried out over the Eurpoean Parliament Spanish-English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mari˜no</author>
</authors>
<title>Integration of postag-based source reordering into smt decoding by an extended search graph.</title>
<date>2006</date>
<booktitle>Proc. of the 7th Conf. of the Association for Machine Translation in the Americas,</booktitle>
<pages>29--36</pages>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>J.M. Crego and J.B. Mari˜no. 2006a. Integration of postag-based source reordering into smt decoding by an extended search graph. Proc. of the 7th Conf. of the Association for Machine Translation in the Americas, pages 29–36, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mari˜no</author>
</authors>
<title>Reordering experiments for n-gram-based smt.</title>
<date>2006</date>
<booktitle>1stIEEE/ACL Workshop on Spoken Language Technology,</booktitle>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>J.M. Crego and J.B. Mari˜no. 2006b. Reordering experiments for n-gram-based smt. 1stIEEE/ACL Workshop on Spoken Language Technology, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mari˜no</author>
<author>A de Gispert</author>
</authors>
<title>An ngram-based statistical machine translation decoder.</title>
<date>2005</date>
<booktitle>Proc. of the 9th European Conference on Speech Communication and Technology, Interspeech’05,</booktitle>
<pages>3193--3196</pages>
<marker>Crego, Mari˜no, de Gispert, 2005</marker>
<rawString>J.M. Crego, J.B. Mari˜no, and A. de Gispert. 2005. An ngram-based statistical machine translation decoder. Proc. of the 9th European Conference on Speech Communication and Technology, Interspeech’05, pages 3193–3196, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>Proc. of the 6th Conf. of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="13475" citStr="Koehn, 2004" startWordPosition="2160" endWordPosition="2161"> and efficient algorithm (O(n), being n the search size) can be used in order to discard them, before rescoring work. Additionally, given that partial model costs are needed in rescoring work, our decoder allows to output the individual model costs computed for each translation unit (token t). Costs are encoded within the token s, as in the next example: (0 (1 &amp;quot;o#or{1.5,0.9,0.6,0.2}&amp;quot; 6)) where the token t is now composed of the translation unit ’o#or’, followed by (four) model costs. Multiple translation hypotheses can only be extracted if hypotheses recombinations are carefully saved. As in (Koehn, 2004), the decoder takes a record of any recombined hypothesis, allowing for a rigorous N-best generation. Model costs are referred to the current unit while the global score s is accumulated. Notice also that translation units (not words) are now used as tokens. 5 Experiments Experiments are carried out for a Spanish-to-English translation task using the EPPS data set, corresponding to session transcriptions of the European Parliament. Eff. base +tpos +reor +spos Beam size = 50 w/o cache 1,820 2,170 2,970 3,260 w/ cache −50 −110 −190 −210 Beam size = 100 w/o cache 2,900 4,350 5,960 6,520 w/ cache </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Ph. Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Proc. of the 6th Conf. of the Association for Machine Translation in the Americas, pages 115–124, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mari˜no</author>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R Costa-juss`a</author>
</authors>
<title>N-gram based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert, P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a. 2006. N-gram based machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>