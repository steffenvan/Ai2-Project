<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998033">
Parsing Paraphrases with Joint Inference
</title>
<author confidence="0.995742">
Do Kook Choe∗ David McClosky
</author>
<affiliation confidence="0.991514">
Brown University IBM Research
</affiliation>
<address confidence="0.749939">
Providence, RI Yorktown Heights, NY
</address>
<email confidence="0.996909">
dc65@cs.brown.edu dmcclosky@us.ibm.com
</email>
<sectionHeader confidence="0.993833" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940782608696">
Treebanks are key resources for develop-
ing accurate statistical parsers. However,
building treebanks is expensive and time-
consuming for humans. For domains re-
quiring deep subject matter expertise such
as law and medicine, treebanking is even
more difficult. To reduce annotation costs
for these domains, we develop methods to
improve cross-domain parsing inference
using paraphrases. Paraphrases are eas-
ier to obtain than full syntactic analyses as
they do not require deep linguistic knowl-
edge, only linguistic fluency. A sentence
and its paraphrase may have similar syn-
tactic structures, allowing their parses to
mutually inform each other. We present
several methods to incorporate paraphrase
information by jointly parsing a sentence
with its paraphrase. These methods are ap-
plied to state-of-the-art constituency and
dependency parsers and provide signif-
icant improvements across multiple do-
mains.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999005294117647">
Parsing is the task of reconstructing the syntac-
tic structure from surface text. Many natural lan-
guage processing tasks use parse trees as a basis
for deeper analysis.
The most effective sources of supervision for
training statistical parsers are treebanks. Unfortu-
nately, treebanks are expensive, time-consuming
to create, and not available for most domains.
Compounding the problem, the accuracy of statis-
tical parsers degrades as the domain shifts away
from the supervised training corpora (Gildea,
2001; Bacchiani et al., 2006; McClosky et al.,
2006b; Surdeanu et al., 2008). Furthermore, for
∗Work performed during an IBM internship.
domains requiring subject matter experts, e.g., law
and medicine, it may not be feasible to produce
large scale treebanks since subject matter experts
generally don’t have the necessary linguistic back-
ground. It is natural to look for resources that
are more easily obtained. In this work, we ex-
plore using paraphrases. Unlike parse trees, para-
phrases can be produced quickly by humans and
don’t require extensive linguistic training. While
paraphrases are not parse trees, a sentence and its
paraphrase may have similar syntactic structures
for portions where they can be aligned.
We can improve parsers by jointly parsing a
sentence with its paraphrase and encouraging cer-
tain types of overlaps in their syntactic structures.
As a simple example, consider replacing an un-
known word in a sentence with a synonym found
in the training data. This may help disambiguate
the sentence without changing its parse tree. More
disruptive forms of paraphrasing (e.g., topicaliza-
tion) can also be handled by not requiring strict
agreement between the parses.
In this paper, we use paraphrases to improve
parsing inference within and across domains.
We develop methods using dual-decomposition
(where the parses of both sentences from a depen-
dency parser are encouraged to agree, Section 3.2)
and pair-finding (which can be applied to any n-
best parser, Section 3.3). Some paraphrases signif-
icantly disrupt syntactic structure. To counter this,
we examine relaxing agreement constraints and
building classifiers to predict when joint parsing
won’t be beneficial (Section 3.4). We show that
paraphrases can be exploited to improve cross-
domain parser inference for two state-of-the-art
parsers, especially on domains where they perform
poorly.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9832165">
Many constituency parsers can parse English
newswire text with high accuracy (Collins, 2000;
</bodyText>
<page confidence="0.712895">
1223
</page>
<note confidence="0.7830245">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Charniak and Johnson, 2005; Petrov et al., 2006;
Socher et al., 2013; Coppola and Steedman,
2013). Likewise, dependency parsers have rapidly
improved their accuracy on a variety of lan-
guages (Eisner, 1996; McDonald et al., 2005;
Nivre et al., 2007; Koo and Collins, 2010; Zhang
and McDonald, 2014; Lei et al., 2014). There
</note>
<bodyText confidence="0.99944">
are many approaches tackling the problem of im-
proving parsing accuracy both within and across
domains, including self-training/uptraining (Mc-
Closky et al., 2006b; Petrov et al., 2010), rerank-
ing (Collins, 2000; McClosky et al., 2006b), incor-
porating word clusters (Koo et al., 2008), model
combination (Petrov, 2010), automatically weight-
ing training data (McClosky et al., 2010), and us-
ing n-gram counts from large corpora (Bansal and
Klein, 2011). Using paraphrases falls into the
semi-supervised category. As we show later, in-
corporating paraphrases provides complementary
benefits to self-training.
</bodyText>
<subsectionHeader confidence="0.811922">
2.1 Paraphrases
</subsectionHeader>
<bodyText confidence="0.999685743589744">
While paraphrases are difficult to define rigor-
ously (Bhagat and Hovy, 2013), we only require a
loose definition in this work: a pair of phrases that
mean approximately the same thing. Paraphrases
can be constructed in various ways: replacing
words with synonyms, reordering clauses, adding
relative clauses, using negation and antonyms, etc.
Table 1 lists some example paraphrases.
There are a variety of paraphrase resources pro-
duced by humans (Dolan and Brockett, 2005) and
automatic methods (Ganitkevitch et al., 2013).
Recent works have shown that reliable para-
phrases can be crowdsourced at low cost (Ne-
gri et al., 2012; Burrows et al., 2013; Tschir-
sich and Hintz, 2013). Paraphrases have been
shown to help summarization (Cohn and Lap-
ata, 2013), question answering (Duboue and Chu-
Carroll, 2006; Fader et al., 2013), machine trans-
lation (Callison-Burch et al., 2006), and seman-
tic parsing (Berant and Liang, 2014). Paraphrases
have been applied to syntactic tasks, such as
prepositional phrase attachment and noun com-
pounding, where the corpus frequencies of differ-
ent syntactic constructions (approximated by web
searches) are used to help disambiguate (Nakov
and Hearst, 2005). One method for transforming
constructions is to use paraphrase templates.
How did Bob Marley die?
What killed Bob Marley?
How fast does a cheetah run?
What is a cheetah’s top speed?
He came home unexpectedly.
He wasn’t expected to arrive home like that.
They were far off and looked tiny.
From so far away, they looked tiny.
He turned and bent over the body of the Indian.
Turning, he bent over the Indian’s body.
No need to dramatize.
There is no need to dramatize.
</bodyText>
<tableCaption confidence="0.996712">
Table 1: Example paraphrases from our dataset.
</tableCaption>
<subsectionHeader confidence="0.997921">
2.2 Bilingual Parsing
</subsectionHeader>
<bodyText confidence="0.999982625">
The closest task to ours is bilingual parsing where
sentences and their translations are parsed simul-
taneously (Burkett et al., 2010). While our meth-
ods differ from those used in bilingual parsing, the
general ideas are the same.1 Translating and para-
phrasing are related transformations since both ap-
proximately preserve meaning. While syntax is
only partially preserved across these transforma-
tions, the overlapping portions can be leveraged
with joint inference to mutually disambiguate. Ex-
isting bilingual parsing methods typically require
parallel treebanks for training and parallel text at
runtime while our methods only require parallel
text at runtime. Since we do not have a paral-
lel paraphrase treebank for training, we cannot di-
rectly compare to these methods.
</bodyText>
<sectionHeader confidence="0.967542" genericHeader="method">
3 Jointly Parsing Paraphrases
</sectionHeader>
<bodyText confidence="0.999381733333333">
With a small number of exceptions, parsers typi-
cally assume that the parse of each sentence is in-
dependent. There are good reasons for this inde-
pendence assumption: it simplifies parsing infer-
ence and oftentimes it is not obvious how to relate
multiple sentences (though see Rush et al. (2012)
for one approach). In this section, we present two
methods to jointly parse paraphrases without com-
plicating inference steps. Before going into de-
tails, we give a high level picture of how jointly
parsing paraphrases can help in Figure 1. With
the baseline parser, the parse tree of the target sen-
tence is incorrect but its paraphrase (parsed by the
same parser) is parsed correctly. We use rough
alignments to map words across sentence pairs.
</bodyText>
<footnote confidence="0.996083">
1Applying our methods to bilingual parsing is left as fu-
ture work.
</footnote>
<page confidence="0.99519">
1224
</page>
<bodyText confidence="0.999927">
Note the similar syntactic relations when they are
projected across the aligned words.
Our goal is to encourage an appropriate level
of agreement between the two parses across align-
ments. We start by designing “hard” methods
which require complete agreement between the
parses. However, since parsers are imperfect and
alignments approximate, we also develop “soft”
methods which allow for disagreements. Addi-
tionally, we make procedures to decide whether to
use the original (non-joint) parse or the new joint
parse for each sentence since joint parses may be
worse in cases where the sentences are too differ-
ent and alignment fails.
</bodyText>
<subsectionHeader confidence="0.985848">
3.1 Objective
</subsectionHeader>
<bodyText confidence="0.999983583333333">
In a typical parsing setting, given a sentence (x)
and its paraphrase (y), parsers find a*(x) and b*(y)
that satisfy the following equation:2
where f is a parse-scoring function and T returns
all possible trees for a sentence. f can take many
forms, e.g., summing the scores of arcs (Eisner,
1996; McDonald et al., 2005) or multiplying prob-
abilities together (Charniak and Johnson, 2005).
The argmax over a and b of equation (1) is sepa-
rable; parsers make two sentence-level decisions.
For joint parsing, we modify the objective so that
parsers make one global decision:
</bodyText>
<equation confidence="0.990653333333333">
a*, b* = argmax f(a) + f(b) (2)
a∈T(x),b∈T(y)
: c(a,b)=0
</equation>
<bodyText confidence="0.9999362">
where c (defined below) measures the syntactic
similarity between the two trees. The smaller
c(a, b) is, the more similar a and b are. Intuitively,
joint parsers must retrieve the most similar pair of
trees with the highest sum of scores.
</bodyText>
<subsectionHeader confidence="0.826678">
3.1.1 Constraints
</subsectionHeader>
<bodyText confidence="0.9990465">
The constraint function, c, ties two trees together
using alignments as a proxy for semantic informa-
tion. An alignment is a pair of words from sen-
tences x and y that approximately mean the same
thing. For example, in Figure 1, (helpx, helpy)
is one alignment and (pestilencex, diseasey) is
</bodyText>
<footnote confidence="0.981369">
2When it is clear from context, we omit x and y to sim-
plify notation.
</footnote>
<listItem confidence="0.8253345">
Algorithm 1: Dual decomposition for jointly
parsing paraphrases pseudocode. E is the
set of all possible edges between any pair of
aligned words. Given ` aligned word pairs,
</listItem>
<equation confidence="0.322948">
E = {1, ... , `} x {1, ... , `}. a(i, j) is one if
</equation>
<bodyText confidence="0.926463619047619">
the ith aligned word is the head of jth aligned
word, zero otherwise. u(i, j) is the dual value
of an edge from the ith aligned word to the jth
aligned word. δk is the step size at kth itera-
tion.
another. To simplify joint parsing, we assume
the aligned words play the same syntactic roles
(which is obviously not always true and should
be revisited in future work). c measures the syn-
tactic similarity by computing how many pairs of
alignments have different syntactic head relations.
For the two trees in Figure 1, we see two differ-
ent relations: (help x−� dying, help 4y� dying)
and (natives x��− dying, natives y−� dying). The
rest have the same relation so c(a, b) = 2. As
we’ll show in Section 5, the constraints defined
above are too restrictive because of this strong as-
sumption. To alleviate the problem, we present
ways of appropriately changing constraints later.
We now turn to the first method of incorporating
constraints into joint parsing.
</bodyText>
<subsectionHeader confidence="0.999051">
3.2 Constraints via Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.9996928">
Dual decomposition (Rush and Collins, 2012) is
well-suited for finding the MAP assignment to
equation (2). When the parse-scoring function f
includes an arc-factored component as in McDon-
ald et al. (2005), it is straightforward to incorpo-
</bodyText>
<equation confidence="0.9978796">
Set u0(i, j) = 0 for all i, j E E
fork = 1 to K do
� X �
ak = argmax f(a) + uk(i, j)a(i, j)
aET (x)
bk = argmax iX �
bET(y) f(b) − uk(i, j)b(i, j)
i,jEE
v, uk+1 = UPDATE(uk, δk, ak, bk)
if v = 0 then return ak, bk
return aK, bK
function UPDATE(u, δ, a, b)
v = 0,u&apos;(i,j) = 0 for all i,j E E
for i, j E Edo
u&apos;(i, j) = u(i, j) − δ(a(i, j) − b(i, j))
ifa(i,j) =�b(i,j) then v = v + 1
return v, u&apos;
i,jEE
a*, b* = argmax
aET(x),bET(y)
= argmax f(a) + argmax
aET (x) bET (y)
(1)
f(b)
f(a) + f(b)
</equation>
<page confidence="0.922251">
1225
</page>
<figure confidence="0.904045">
wrong
right
(target sentence) x: help some natives dying of pestilence
(paraphrase) y: help some natives who were dying of disease
</figure>
<figureCaption confidence="0.9519335">
Figure 1: An illustration of joint parsing a sentence with its paraphrase. Unaligned words are gray. Joint
parsing encourages structural similarity and allows the parser to correct the incorrect arc.
</figureCaption>
<bodyText confidence="0.999983217391305">
rate constraints as shown in Algorithm 1. Essen-
tially, dual decomposition penalizes relations that
are different in two trees by adding/subtracting
dual values to/from arc scores. When dual de-
composition is applied in Figure 1, the arc score
of (help x−� dying) decreases and the score for
(natives x−� dying) increases in the second itera-
tion, which eventually leads the algorithm to favor
the latter.
We relax the constraints by employing soft dual
decomposition (Anzaroot et al., 2014) and replac-
ing UPDATE in Algorithm 1 with S-UPDATE from
Algorithm 2. The problem with the original con-
straints is they force every pair of alignments to
have the same relation even when some aligned
words certainly play different syntactic roles. The
introduced slack variable lets some alignments
have different relations when parsers prefer them.
Penalties bounded by the slack tend to help fix in-
correct ones and not change correct parses. In this
work, we use a single slack variable but it’s possi-
ble to have a different slack variable for each type
of dependency relation.3
</bodyText>
<subsectionHeader confidence="0.999529">
3.3 Constraints via Pair-finding
</subsectionHeader>
<bodyText confidence="0.9612875">
One shortcoming of the dual decomposition ap-
proach is that it only applies to parse-scoring func-
tions with an arc-factored component. We intro-
duce another method for estimating equation (2)
that applies to all n-best parsers.
Given the n-best parses of x and the m-best
parses of y, Algorithm 3 scans through nxm pairs
of trees and chooses the pair that satisfies equa-
tion (2). If it finds one pair with c(a, b) = 0, then it
has found the answer to the equation. Otherwise, it
3We did pilot experiments with multiple slack variables.
Since they showed only small improvements and were harder
to tune, we stuck with a single slack variable for remaining
experiments.
</bodyText>
<equation confidence="0.963721375">
function S-UPDATE(u, δ, a, b, s)
v = 0,u&apos;(i,j) = 0 for all i,j E E
for i, j E Edo
t = max(u(i,j) − δ(a(i,j) − b(i,j)),0)
u&apos;(i, j) = min(t, s)
if u&apos;(i, j) =� 0, u&apos;(i, j) =� s then
v = v + 1
return v, u&apos;
</equation>
<bodyText confidence="0.974476434782609">
Algorithm 2: The new UPDATE function of
soft dual decomposition for joint parsing. It
projects all dual values between 0 and s &gt; 0.
s is a slack variable that allows the algorithm
to avoid satisfying some constraints.
chooses the pair with the smallest c(a, b), breaking
ties using the scores of the parses (f(a) + f(b)).
This algorithm is well suited for finding solutions
to the equation but the solutions are not necessar-
ily good trees due to overly hard constraints.
The algorithm often finds bad trees far down the
n-best list because it is mainly interested in re-
trieving pairs of trees that satisfy all constraints.
Parsers find such pairs with low scores if they are
allowed to search through unrestricted space. To
mitigate the problem, we shrink the search space
by limiting n. Reducing the search space relies on
the fact that higher ranking trees are more likely to
be correct than the lower ranking ones. Note that
we decrease n because we are interested in recov-
ering the tree of the target sentence, x. m should
also be decreased to improve the parse of its para-
phrase, y.
</bodyText>
<subsectionHeader confidence="0.98021">
3.4 Logistic Regression
</subsectionHeader>
<bodyText confidence="0.9999855">
One caveat of the previous two proposed methods
is that they do not know whether the original or
joint parse of x is more accurate. Sometimes they
increase agreement between the parses at the cost
</bodyText>
<page confidence="0.532852">
1226
</page>
<equation confidence="0.991280375">
function PAIR-FINDING(a1:n, b1:m)
Set a, b = null, min = oc, max = −oc
for i = 1 to n do
for j = 1 tomdo
v = C (ai, bj)
sum = f(ai) + f(bj)
if v &lt; min then
a = ai,b = bj
min = v, max = sum
else if v = min, sum &gt; max then
a = ai,b = bj
max = sum
return a, b
function C(a, b)
v = 0
for i, j E Edo
</equation>
<construct confidence="0.610653">
if a(i, j) =� b(i, j) then v = v + 1
return v
</construct>
<bodyText confidence="0.96928675">
Algorithm 3: The pair-finding scheme with a
constraint function, c. a1:n are the n-best trees
of x and b1:m are the m-best of y.
of accuracy. To remedy this problem, we use a
classifier (specifically logistic regression) to deter-
mine whether a modified tree should be used. The
classifier can learn the error patterns produced by
each method.
</bodyText>
<sectionHeader confidence="0.829791" genericHeader="method">
3.4.1 Features
</sectionHeader>
<bodyText confidence="0.999371210526316">
Classifier features use many sources of informa-
tion: the target sentence x and its paraphrase y,
the original and new parses of x (a0 and a), and
the alignments between x and y.
Crossing Edges How many arcs cross when
alignments are drawn between paraphrases
on a plane divided by the length of x. It
roughly measures how many reorderings are
needed to change x to y.
Non-projective Edges Whether there are more
non-projective arcs in new parse (a) than the
original (a0).
Sentence Lengths Whether the length of x is
smaller than that of y. This feature exists be-
cause baseline parsers tend to perform better
on shorter sentences.
Word Overlaps The number of words in com-
mon between x and y normalized by the
length of x.
</bodyText>
<equation confidence="0.879557166666667">
REL REL + RELp
REL + RELp + RELgp REL + RELgp
CP CP + CPp
CP + CPp + CPgp CP + CPgp
REL + CP REL + CP + CPp
REL + CPp + RELgp
</equation>
<bodyText confidence="0.890103">
Table 2: Feature templates: REL is the dependency
relation between the word and its parent. CP is
the coarse part-of-speech tag (first two letters) of a
word. p and gp select the parent and grandparent
of the word respectively.
Parse Structure Templates The feature genera-
tor goes through every word in {a0, a} and
sets the appropriate boolean features from Ta-
ble 2. Features are prefixed by whether they
come from a0 or a.
</bodyText>
<sectionHeader confidence="0.986407" genericHeader="method">
4 Data and Programs
</sectionHeader>
<bodyText confidence="0.999908">
This section describes our paraphrase dataset,
parsers, and other tools used in experiments.
</bodyText>
<subsectionHeader confidence="0.992932">
4.1 Paraphrase Dataset
</subsectionHeader>
<bodyText confidence="0.999922818181818">
To evaluate the efficacy of the proposed methods
of jointly parsing paraphrases, we built a corpus
of paraphrases where one sentence in a pair of
paraphrases has a gold tree.4 We randomly sam-
pled 4,000 sentences5 from four gold treebanks:
Brown, British National Corpus (BNC), Question-
Bank6 (QB) and Wall Street Journal (section 24)
(Francis and Kuˇcera, 1989; Foster and van Gen-
abith, 2008; Judge et al., 2006; Marcus et al.,
1993). A linguist provided a paraphrase for each
sampled sentence according to these instructions:
The paraphrases should more or less
convey the same information as the orig-
inal sentence. That is, the two sentences
should logically entail each other. The
paraphrases should generally use most
of the same words (but not necessarily
in the same order). Active/passive trans-
forms, changing words with synonyms,
and rephrasings of the same idea are all
examples of transformations that para-
phrases can use (others can be used too).
</bodyText>
<footnote confidence="0.9959868">
4The dataset is available upon request.
5We use sentences with 6 to 25 tokens to keep the para-
phrasing task in the nontrivial to easy range.
6With Stanford’s updates: http://nlp.stanford.
edu/data/QuestionBank-Stanford.shtml
</footnote>
<page confidence="0.989577">
1227
</page>
<bodyText confidence="0.999830692307693">
They can be as simple as just chang-
ing a single word in some cases (though,
ideally, a variety of paraphrasing tech-
niques would be used).
We also provided 10 pairs of sentences as ex-
amples. We evaluate our methods only on the
sampled sentences from the gold corpora because
the new paraphrases do not include syntactic trees.
The data was divided into development and test-
ing sets such that development and testing share
the same distribution over the four corpora. Para-
phrases were tokenized by the BLLIP tokenizer.
See Table 3 for statistics of the dataset.7
</bodyText>
<subsectionHeader confidence="0.954273">
4.2 Meteor Word Aligner
</subsectionHeader>
<bodyText confidence="0.9999324375">
We use Meteor, a monolingual word aligner de-
veloped by Denkowski and Lavie (2014), to find
alignments between paraphrases. It uses the ex-
act matches, stems, synonyms, and paraphrases8
to form these alignments. Because it uses para-
phrases, it sometimes aligns multiple words from
sentence x to one or more words from sentence y
or vice versa. We ignore these multiword align-
ments because our methods currently only handle
single word alignments. In pilot experiments, we
also tried using a simple aligner which required
exact word matches. Joint parsing with simpler
alignments improved parsing accuracy but not as
much as Meteor.9 Thus, all results in Section 5 use
Meteor for word alignment. On average across the
four corpora, 73% of the tokens are aligned.
</bodyText>
<subsectionHeader confidence="0.994369">
4.3 Parsers
</subsectionHeader>
<bodyText confidence="0.999748857142857">
We use a dependency and constituency parser for
our experiments: RBG and BLLIP. RBG parser
(Lei et al., 2014) is a state-of-the-art dependency
parser.10 It is a third-order discriminative depen-
dency parser with low-rank tensors as part of its
features. BLLIP (Charniak and Johnson, 2005)
is a state-of-the-art constituency parser, which is
</bodyText>
<footnote confidence="0.977091">
7The distribution over four corpora is skewed because
each corpus has a different number of sentences within length
constraints. Samples are collected uniformly over all sen-
tences that satisfy the length criterion.
8Here paraphrase means a single/multiword phrase that is
semantically similar to another single/multiword.
9The pilot was conducted on fewer than 700 sentence
pairs before all paraphrases were created. We give Meteor
tokenized paraphrases with capitalization. Maximizing accu-
racy rather than coverage worked better in pilot experiments.
10http://github.com/taolei87/RBGParser,
‘master’ version from June 24th, 2014.
</footnote>
<bodyText confidence="0.998912375">
composed of a generative parser and a discrimina-
tive reranker.11
To train RBG and BLLIP, we used the standard
WSJ training set (sections 2–21, about 40,000 sen-
tences).12 We also used the self-trained BLLIP
parsing model which is trained on an additional
two million Gigaword parses generated by the
BLLIP parser (McClosky et al., 2006a).
</bodyText>
<subsectionHeader confidence="0.993354">
4.4 Logistic Regression
</subsectionHeader>
<bodyText confidence="0.999916428571429">
We use the logistic regression implementation
from Scikit-learn13 with hand-crafted features
from Section 3.4.1. The classifier decides to
whether to keep the parse trees from the joint
method. When it decides to disregard them, it re-
turns the parse from the baseline parser. We train
a separate classifier for each joint method.
</bodyText>
<sectionHeader confidence="0.999598" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999721363636364">
We ran all tuning and model design experiments
on the development set. For the final evaluation,
we tuned parameters on the development set and
evaluate them on the test set. Constituency trees
were converted to basic non-collapsed dependency
trees using Stanford Dependencies (De Marneffe
et al., 2006).14 We report unlabeled attachment
scores (UAS) for all experiments and labeled at-
tachment scores (LAS) as well in final evalua-
tion, ignoring punctuation. Averages are micro-
averages across all sentences.
</bodyText>
<subsectionHeader confidence="0.957944">
5.1 Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.998915076923077">
Since BLLIP is not arc-factored, these experi-
ments only use RBG. Several parameters need to
be fixed beforehand: the slack constant (s), the
learning rate (6), and the maximum number of it-
erations (K). We set 60 = 0.1 and 6k = δ0
2, where
t is the number of times the dual score has in-
creased (Rush et al., 2010). We choose K = 20.
These numbers were chosen from pilot studies.
The slack variable (s = 0.5) was tuned with a
grid search on values between 0.1 and 1.5 with in-
terval 0.1. We chose a value that generalizes well
across four corpora as opposed to a value that does
</bodyText>
<footnote confidence="0.786451625">
11http://github.com/BLLIP/bllip-parser
12RBG parser requires predicted POS tags. We used the
Stanford tagger (Toutanova et al., 2003) to tag WSJ and
paraphrase datasets. Training data was tagged using 20-fold
cross-validation and the paraphrases were tagged by a tagger
trained on all of WSJ training.
13http://scikit-learn.org
14Version 1.3.5, previously numbered as version 2.0.5
</footnote>
<page confidence="0.943523">
1228
</page>
<table confidence="0.999767909090909">
Development Test
BNC Brown QB WSJ Total BNC Brown QB WSJ Total
Sentences 247 558 843 352 2,000 247 558 844 351 2,000
Tokens 4,297 7,937 8,391 5,924 26,549 4,120 8,025 8,253 5,990 26,388
Tokensll 4,372 8,088 8,438 6,122 27,020 4,272 8,281 8,189 6,232 26,974
Word types 1,727 2,239 2,261 1,955 6,161 1,710 2,337 2,320 1,970 6,234
Word types 1,676 2,241 2,261 1,930 6,017 1,675 2,335 2,248 1,969 6,094
OOV 11.2 5.1 5.4 2.4 5.6 11.5 5.1 5.8 2.2 5.7
OOV11 8.6 4.7 5.4 2.6 5.1 9.3 4.8 6.0 2.4 5.3
Tokens/sent. 17.4 14.2 10.0 16.8 13.3 16.7 14.4 7.8 17.1 13.2
Avg. aligned 13.1 10.5 6.9 13.0 9.7 12.6 10.7 6.7 13.0 9.7
</table>
<tableCaption confidence="0.608896">
Table 3: Statistics for the four corpora of the paraphrase dataset. Most statistics are counted from sen-
tences with gold trees, including punctuation. 11 indicates the statistic is from the paraphrased sentences.
“Avg. aligned” is the average number of aligned tokens from the original sentences using Meteor. OOV
is the percentage of tokens not seen in the WSJ training.
</tableCaption>
<table confidence="0.99959">
Avg BNC Brown QB WSJ
RBG 86.4 89.2 90.9 75.8 93.7
+ Dual 84.7 87.5 87.8 76.0 91.0
+ S-Dual 86.8 89.8 90.9 76.5 94.0
</table>
<tableCaption confidence="0.744032666666667">
Table 4: Comparison of hard and soft dual de-
composition for joint parsing (development sec-
tion, UAS).
</tableCaption>
<bodyText confidence="0.9998996">
very well on a single corpus. As shown in Ta-
ble 4, joint parsing with hard dual decomposition
performs worse than independent parsing (RBG).
This is expected because hard dual decomposition
forces every pair of alignments to form the same
relation even when they should not. With relaxed
constraints (S-Dual), joint parsing performs sig-
nificantly better than independent parsing. Soft
dual decomposition improves across all domains
except for Brown (where it ties).
</bodyText>
<subsectionHeader confidence="0.999892">
5.2 Pair-finding
</subsectionHeader>
<bodyText confidence="0.999035636363636">
These experiments use the 50-best trees from
BLLIP parser. When converting to dependencies,
some constituency trees map to the same depen-
dency tree. In this case, trees with lower rankings
are dropped. Like joint parsing with hard dual de-
composition, joint parsing with unrestricted pair-
finding (n = 50) allows significantly worse parses
to be selected (Table 5). With small n values,
pair-finding improves over the baseline BLLIP
parser.15 Experiments with self-trained BLLIP
exhibit similar results so we use n = 2 for all
</bodyText>
<footnote confidence="0.785346">
15Decreasing m did not lead to further improvement and
thus we don’t report the results of changing m.
</footnote>
<table confidence="0.998189428571428">
n Avg BNC Brown QB WSJ
1 89.5 91.1 91.6 83.3 94.2
2 90.0 91.4 92.3 84.1 94.1
3 89.8 91.5 92.0 84.2 93.9
5 89.2 91.9 91.4 83.0 93.2
10 87.9 90.5 90.3 81.4 92.2
50 86.3 90.2 88.7 78.6 91.1
</table>
<tableCaption confidence="0.952686">
Table 5: UAS of joint parsing using the pair-
</tableCaption>
<bodyText confidence="0.985612">
finding scheme with various n values on the de-
velopment portion. n = 1 is the baseline BLLIP
parser and n &gt; 1 is BLLIP with pair-finding.
other experiments. Interestingly, each corpus has
a different optimal value for n which suggests we
might improve accuracy further if we know the do-
main of each sentence.
</bodyText>
<subsectionHeader confidence="0.996839">
5.3 Logistic Regression
</subsectionHeader>
<bodyText confidence="0.998023461538461">
The classifier is trained on sentences where parse
scores (UAS) of the proposed methods are higher
or lower than those of the baselines16 from
the development set using leave-one-out cross-
validation. We use random greedy search to select
specific features from the 15 feature templates de-
fined in Section 3.4.1. Features seen fewer than
three times in the development are thrown out.
Separate regression models are built for three dif-
ferent parsers. The logistic regression classifier
uses an Ll penalty with regularization parameter
C = 1.
Logistic regression experiments are reported in
</bodyText>
<footnote confidence="0.820221">
16We only use sentences with different scores to limit ceil-
ing effects.
</footnote>
<page confidence="0.970967">
1229
</page>
<table confidence="0.9996069">
Avg BNC Brown QB WSJ
RBG 86.4 89.2 90.9 75.8 93.7
+ S-Dual 86.8 89.8 90.9 76.5 94.0
+ Logit 86.9 89.8 91.1 76.5 94.0
BLLIP 89.5 91.1 91.6 83.3 94.2
+ Pair 90.0 91.4 92.3 84.1 94.1
+ Logit 90.3 91.3 92.1 85.2 94.3
BLLIP-ST 90.1 92.7 92.3 84.3 93.8
+ Pair 90.7 93.5 92.5 85.6 93.8
+ Logit 91.1 93.3 92.6 86.7 93.9
</table>
<tableCaption confidence="0.986353">
Table 6: Effect of using logistic regression on
</tableCaption>
<bodyText confidence="0.818749777777778">
top of each method (UAS). Leave-one-out cross-
validation is performed on the development data.
+X means augmenting the above system with X.
Table 6. All parsers benefit from employing logis-
tic regression models on top of paraphrase meth-
ods. BLLIP experiments show a larger improve-
ment than RBG. This may be because BLLIP can-
not use soft constraints so its errors are more pro-
nounced.
</bodyText>
<subsectionHeader confidence="0.995875">
5.4 Final Evaluation
</subsectionHeader>
<bodyText confidence="0.999985583333334">
We evaluate the three parsers on the test set using
the tuned parameters and logistic regression mod-
els from above. Joint parsing with paraphrases sig-
nificantly improves accuracy for all systems (Ta-
ble 7). Self-trained BLLIP with logistic regres-
sion is the most accurate, though RBG with S-
Dual provides the most consistent improvements.
Joint parsing without logistic regression (RBG
+ S-Dual) is more accurate than independent pars-
ing (RBG) overall. With the help of logistic re-
gression, the methods do at least as well as their
baseline counterparts on all domains with the ex-
ception of self-trained BLLIP on BNC. We believe
that the drop on BNC is largely due to noise as our
BNC test set is the smallest of the four. As on de-
velopment, logistic regression does not change the
accuracy much over the RBG parser with soft dual
decomposition.
Joint parsing provides the largest gains on
QuestionBank, the domain with the lowest base-
line accuracies. This fits with our goal of using
paraphrases for domain adaptation — parsing with
paraphrases helps the most on domains furthest
from our training data.
</bodyText>
<subsectionHeader confidence="0.98579">
5.5 Error analysis
</subsectionHeader>
<bodyText confidence="0.99997825">
We analyzed the errors from RBG and BLLIP
along several dimensions: by dependency label,
sentence length, dependency length, alignment
status (whether a token was aligned), percentage
of tokens aligned in the sentence, and edit distance
between the sentence pairs. Most errors are fairly
uniformly distributed across these dimensions and
indicate general structural improvements when us-
ing paraphrases. BLLIP saw a 2.2% improvement
for the ROOT relation, though RBG’s improvement
here was more moderate. For sentence lengths,
BLLIP obtains larger boosts for shorter sentences
while RBG’s are more uniform. RBG gets a 1.4%
UAS improvement on longer dependencies (6 or
more tokens) while shorter dependencies are more
modestly improved by about 0.3-0.5% UAS. Sur-
prisingly, alignment information provides no sig-
nal as to whether accuracy improves.
Additionally, we had our annotator label a por-
tion of our dataset with the set of paraphrasing
operations employed.17 While most paraphrasing
operations generally improved performance un-
der joint inference, the largest reliable gains came
from lexical replacements (e.g., synonyms).
</bodyText>
<sectionHeader confidence="0.991045" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999714">
Our methods of incorporating paraphrases im-
prove parsing across multiple domains for state-
of-the-art constituency and dependency parsers.
We leverage the fact that paraphrases often express
the same semantics with similar syntactic realiza-
tions. These provide benefits even on top of self-
training, another domain adaptation technique.
Since paraphrases are not available at most
times, our methods may seem limited. However,
there are several possible use cases. The best
case scenario is when users can be directly asked
to rephrase a question and provide a paraphrase.
For instance, question answering systems can ask
users to rephrase questions when an answer is
marked as wrong by users. Another option is to
use crowdsourcing to quickly create a paraphrase
corpus (Negri et al., 2012; Burrows et al., 2013;
Tschirsich and Hintz, 2013). As part of future
work, we plan to integrate existing larger para-
phrase resources, such as WikiAnswers (Fader et
al., 2013) and PPDB (Ganitkevitch et al., 2013).
WikiAnswers provides rough equivalence classes
of questions. PPDB includes phrasal and syntactic
alignments which could supplement our existing
alignments or be used as proxies for paraphrases.
</bodyText>
<footnote confidence="0.984708">
17See the extended version of this paper for more informa-
tion about this task and its results.
</footnote>
<page confidence="0.911681">
1230
</page>
<table confidence="0.9994557">
Avg BNC Brown QB WSJ
RBG 86.7 (81.3) 89.3 (83.7) 90.2 (84.1) 77.0 (71.0) 93.7 (89.9)
+ S-Dual 87.3 (81.7) 89.6 (83.8) 90.7 (84.6) 78.1 (71.8) 94.0 (90.2)
+ Logit 87.2 (81.6) 89.7 (83.9) 90.6 (84.5) 77.9 (71.7) 93.8 (89.9)
BLLIP 89.6 (86.1) 90.6 (87.2) 91.7 (87.9) 83.6 (79.9) 94.3 (91.6)
+ Pair 90.1 (86.5) 90.8 (87.3) 92.1 (88.4) 84.7 (80.7) 94.4 (91.6)
+ Logit 90.3 (86.8) 90.6 (87.2) 91.9 (88.1) 85.5 (81.7) 94.5 (91.7)
BLLIP-ST 90.4 (87.0) 91.8 (88.3) 92.7 (89.0) 84.8 (81.2) 94.3 (91.4)
+ Pair 90.5 (87.1) 91.1 (87.6) 92.7 (89.1) 85.5 (81.8) 94.2 (91.4)
+ Logit 91.0 (87.6) 91.4 (88.0) 92.9 (89.3) 86.6 (82.9) 94.3 (91.4)
</table>
<tableCaption confidence="0.985232">
Table 7: Final evaluation on testing data. Numbers are unlabeled attachment score (labeled attachment
</tableCaption>
<bodyText confidence="0.954680434782609">
score). +X indicates extending the above system with X. BLLIP-ST is BLLIP using the self-trained
model. Coloring indicates a significant difference over baseline (p &lt; 0.01).
While these resources are noisy, the quantity of
data may provide additional robustness. Lastly, in-
tegrating our methods with paraphrase detection
or generation systems could help provide para-
phrases on demand.
There are many other ways to extend this work.
Poor alignments are one of the larger sources of
errors and improving alignments could help dra-
matically. One simple extension is to use multiple
paraphrases and their alignments instead of just
one. More difficult would be to learn the align-
ments jointly while parsing and adaptively learn
how alignments affect syntax. Our constraints can
only capture certain types of paraphrase transfor-
mations currently and should be extended to un-
derstand common tree transformations for para-
phrases, as in (Heilman and Smith, 2010). Fi-
nally, and perhaps most importantly, our methods
apply only at inference time. We plan to investi-
gate methods which use paraphrases to augment
parsing models created at train time.
</bodyText>
<sectionHeader confidence="0.987485" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999973555555556">
We would like to thank Eugene Charniak for
the idea of using paraphrases to improve parsing,
our anonymous reviewers for their valuable feed-
back, Karen Ingraffea for constructing and classi-
fying the paraphrase corpus, Dave Buchanan and
Will Headden for last minute paper reading, the
DeepQA team at IBM for feedback and support
on the project, and Mohit Bansal and Micha El-
sner for helpful discussions.
</bodyText>
<sectionHeader confidence="0.994555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996934315789474">
Sam Anzaroot, Alexandre Passos, David Belanger, and
Andrew McCallum. 2014. Learning soft linear
constraints with application to citation field extrac-
tion. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics,
pages 593–602. Association for Computational Lin-
guistics.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer speech &amp; language,
20(1):41–68.
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 693–702. Association for Computational
Linguistics.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 1415–1425. Association
for Computational Linguistics.
Rahul Bhagat and Eduard Hovy. 2013. What is a para-
phrase? Computational Linguistics, 39(3):463–472.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchro-
nized grammars. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 127–135. Association for
Computational Linguistics.
Steven Burrows, Martin Potthast, and Benno Stein.
2013. Paraphrase acquisition via crowdsourcing and
machine learning. ACM Transactions on Intelligent
Systems and Technology (TIST), 4(3):43.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
</reference>
<page confidence="0.925204">
1231
</page>
<reference confidence="0.9946398125">
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17–24.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 173–180. Association for Computational
Linguistics.
Trevor Cohn and Mirella Lapata. 2013. An ab-
stractive approach to sentence compression. ACM
Transactions on Intelligent Systems and Technology,
4(3):41.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, pages 175–182. ICML.
Gregory Coppola and Mark Steedman. 2013. The ef-
fect of higher-order dependency features in discrimi-
native phrase-structure parsing. In ACL, pages 610–
616.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D. Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, pages 449–454.
Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
376–380. Association for Computational Linguis-
tics.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proc. of IWP.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the question you wish they had asked:
The impact of paraphrasing for question answering.
In Proceedings of the Human Language Technology
Conference of the NAACL, pages 33–36. Association
for Computational Linguistics.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics, pages 340–345. Association for Compu-
tational Linguistics.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open ques-
tion answering. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 1608–1618. Association for Compu-
tational Linguistics.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the BNC: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation. European Language Resources As-
sociation.
Winthrop Nelson Francis and Henry Kuˇcera. 1989.
Manual of information to accompany a standard
corpus of present-day edited American English, for
use with digital computers. Brown University, De-
partment of Linguistics.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758–764. Association for Computational Linguis-
tics.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 167–202.
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011–1019.
Association for Computational Linguistics.
John Judge, Aoife Cahill, and Josef Van Genabith.
2006. QuestionBank: creating a corpus of parse-
annotated questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 497–504. As-
sociation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1–11. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings ofACL: HLT, pages 595–603. Asso-
ciation for Computational Linguistics.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 1381–1391. Association
for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proceedings of the main conference on human lan-
guage technology conference of the North American
Chapter of the Association of Computational Lin-
guistics, pages 152–159. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.831619">
1232
</page>
<reference confidence="0.999832234234235">
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 337–344. Association for
Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 28–36. Association for Computational Lin-
guistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 523–530. Association for Computa-
tional Linguistics.
Preslav Nakov and Marti Hearst. 2005. Using
the web as an implicit training set: Application
to structural ambiguity resolution. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 835–842, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012.
Chinese whispers: Cooperative paraphrase acquisi-
tion. In LREC, pages 2659–2665.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02):95–135.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440.
Association for Computational Linguistics.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705–713. Association
for Computational Linguistics.
Slav Petrov. 2010. Products of random latent vari-
able grammars. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 19–27. Association for Computa-
tional Linguistics.
Alexander M. Rush and Michael Collins. 2012. A
tutorial on dual decomposition and Lagrangian re-
laxation for inference in natural language process-
ing. Journal of Artificial Intelligence Research,
45(1):305–362.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11. Association for Computa-
tional Linguistics.
Alexander M. Rush, Roi Reichart, Michael Collins,
and Amir Globerson. 2012. Improved parsing and
POS tagging using inter-sentence consistency con-
straints. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1434–1444. Association for Com-
putational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 455–465. Association for
Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159–177. Association
for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Martin Tschirsich and Gerold Hintz. 2013. Leveraging
crowdsourcing for paraphrase recognition. In Pro-
ceedings of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, pages 205–213,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Hao Zhang and Ryan McDonald. 2014. Enforcing
structural diversity in cube-pruned dependency pars-
ing. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics,
pages 656–661. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.973738">
1233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.554107">
<title confidence="0.999231">Parsing Paraphrases with Joint Inference</title>
<author confidence="0.999486">Kook McClosky</author>
<affiliation confidence="0.798488">Brown University IBM Research Providence, RI Yorktown Heights, NY</affiliation>
<email confidence="0.990913">dc65@cs.brown.edudmcclosky@us.ibm.com</email>
<abstract confidence="0.997441541666667">Treebanks are key resources for developing accurate statistical parsers. However, building treebanks is expensive and timeconsuming for humans. For domains requiring deep subject matter expertise such as law and medicine, treebanking is even more difficult. To reduce annotation costs for these domains, we develop methods to improve cross-domain parsing inference using paraphrases. Paraphrases are easier to obtain than full syntactic analyses as they do not require deep linguistic knowledge, only linguistic fluency. A sentence and its paraphrase may have similar syntactic structures, allowing their parses to mutually inform each other. We present several methods to incorporate paraphrase information by jointly parsing a sentence with its paraphrase. These methods are applied to state-of-the-art constituency and dependency parsers and provide significant improvements across multiple domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sam Anzaroot</author>
<author>Alexandre Passos</author>
<author>David Belanger</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning soft linear constraints with application to citation field extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>593--602</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12789" citStr="Anzaroot et al., 2014" startWordPosition="2068" endWordPosition="2071">aphrase. Unaligned words are gray. Joint parsing encourages structural similarity and allows the parser to correct the incorrect arc. rate constraints as shown in Algorithm 1. Essentially, dual decomposition penalizes relations that are different in two trees by adding/subtracting dual values to/from arc scores. When dual decomposition is applied in Figure 1, the arc score of (help x−� dying) decreases and the score for (natives x−� dying) increases in the second iteration, which eventually leads the algorithm to favor the latter. We relax the constraints by employing soft dual decomposition (Anzaroot et al., 2014) and replacing UPDATE in Algorithm 1 with S-UPDATE from Algorithm 2. The problem with the original constraints is they force every pair of alignments to have the same relation even when some aligned words certainly play different syntactic roles. The introduced slack variable lets some alignments have different relations when parsers prefer them. Penalties bounded by the slack tend to help fix incorrect ones and not change correct parses. In this work, we use a single slack variable but it’s possible to have a different slack variable for each type of dependency relation.3 3.3 Constraints via </context>
</contexts>
<marker>Anzaroot, Passos, Belanger, McCallum, 2014</marker>
<rawString>Sam Anzaroot, Alexandre Passos, David Belanger, and Andrew McCallum. 2014. Learning soft linear constraints with application to citation field extraction. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 593–602. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer speech &amp; language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1632" citStr="Bacchiani et al., 2006" startWordPosition="232" endWordPosition="235">ndency parsers and provide significant improvements across multiple domains. 1 Introduction Parsing is the task of reconstructing the syntactic structure from surface text. Many natural language processing tasks use parse trees as a basis for deeper analysis. The most effective sources of supervision for training statistical parsers are treebanks. Unfortunately, treebanks are expensive, time-consuming to create, and not available for most domains. Compounding the problem, the accuracy of statistical parsers degrades as the domain shifts away from the supervised training corpora (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b; Surdeanu et al., 2008). Furthermore, for ∗Work performed during an IBM internship. domains requiring subject matter experts, e.g., law and medicine, it may not be feasible to produce large scale treebanks since subject matter experts generally don’t have the necessary linguistic background. It is natural to look for resources that are more easily obtained. In this work, we explore using paraphrases. Unlike parse trees, paraphrases can be produced quickly by humans and don’t require extensive linguistic training. While paraphrases are not parse trees, a sentence and it</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer speech &amp; language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>693--702</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4618" citStr="Bansal and Klein, 2011" startWordPosition="687" endWordPosition="690">d their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources pr</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 693–702. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1415--1425</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5708" citStr="Berant and Liang, 2014" startWordPosition="854" endWordPosition="857">ive clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphrase templates. How did Bob Marley die? What killed Bob Marley? How fast does a cheetah run? What is a cheetah’s top speed? He came home unexpectedly. He wasn’t expected to arrive home like that. They were far off and looked tiny. From so far away, they looked tiny. He turne</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415–1425. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Eduard Hovy</author>
</authors>
<title>What is a paraphrase?</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="4865" citStr="Bhagat and Hovy, 2013" startWordPosition="720" endWordPosition="723">th within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013)</context>
</contexts>
<marker>Bhagat, Hovy, 2013</marker>
<rawString>Rahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, 39(3):463–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>127--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6644" citStr="Burkett et al., 2010" startWordPosition="1007" endWordPosition="1010">use paraphrase templates. How did Bob Marley die? What killed Bob Marley? How fast does a cheetah run? What is a cheetah’s top speed? He came home unexpectedly. He wasn’t expected to arrive home like that. They were far off and looked tiny. From so far away, they looked tiny. He turned and bent over the body of the Indian. Turning, he bent over the Indian’s body. No need to dramatize. There is no need to dramatize. Table 1: Example paraphrases from our dataset. 2.2 Bilingual Parsing The closest task to ours is bilingual parsing where sentences and their translations are parsed simultaneously (Burkett et al., 2010). While our methods differ from those used in bilingual parsing, the general ideas are the same.1 Translating and paraphrasing are related transformations since both approximately preserve meaning. While syntax is only partially preserved across these transformations, the overlapping portions can be leveraged with joint inference to mutually disambiguate. Existing bilingual parsing methods typically require parallel treebanks for training and parallel text at runtime while our methods only require parallel text at runtime. Since we do not have a parallel paraphrase treebank for training, we ca</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 127–135. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Burrows</author>
<author>Martin Potthast</author>
<author>Benno Stein</author>
</authors>
<title>Paraphrase acquisition via crowdsourcing and machine learning.</title>
<date>2013</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>4--3</pages>
<contexts>
<context position="5436" citStr="Burrows et al., 2013" startWordPosition="811" endWordPosition="814">cult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphras</context>
<context position="30802" citStr="Burrows et al., 2013" startWordPosition="5120" endWordPosition="5123">the same semantics with similar syntactic realizations. These provide benefits even on top of selftraining, another domain adaptation technique. Since paraphrases are not available at most times, our methods may seem limited. However, there are several possible use cases. The best case scenario is when users can be directly asked to rephrase a question and provide a paraphrase. For instance, question answering systems can ask users to rephrase questions when an answer is marked as wrong by users. Another option is to use crowdsourcing to quickly create a paraphrase corpus (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). As part of future work, we plan to integrate existing larger paraphrase resources, such as WikiAnswers (Fader et al., 2013) and PPDB (Ganitkevitch et al., 2013). WikiAnswers provides rough equivalence classes of questions. PPDB includes phrasal and syntactic alignments which could supplement our existing alignments or be used as proxies for paraphrases. 17See the extended version of this paper for more information about this task and its results. 1230 Avg BNC Brown QB WSJ RBG 86.7 (81.3) 89.3 (83.7) 90.2 (84.1) 77.0 (71.0) 93.7 (89.9) + S-Dual 87.3 (81.7) 89.6 (8</context>
</contexts>
<marker>Burrows, Potthast, Stein, 2013</marker>
<rawString>Steven Burrows, Martin Potthast, and Benno Stein. 2013. Paraphrase acquisition via crowdsourcing and machine learning. ACM Transactions on Intelligent Systems and Technology (TIST), 4(3):43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5661" citStr="Callison-Burch et al., 2006" startWordPosition="846" endWordPosition="849">ords with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphrase templates. How did Bob Marley die? What killed Bob Marley? How fast does a cheetah run? What is a cheetah’s top speed? He came home unexpectedly. He wasn’t expected to arrive home like that. They were far off and looked tin</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 17–24. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine n-best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3873" citStr="Charniak and Johnson, 2005" startWordPosition="570" endWordPosition="573">dict when joint parsing won’t be beneficial (Section 3.4). We show that paraphrases can be exploited to improve crossdomain parser inference for two state-of-the-art parsers, especially on domains where they perform poorly. 2 Related Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (</context>
<context position="9160" citStr="Charniak and Johnson, 2005" startWordPosition="1412" endWordPosition="1415">tionally, we make procedures to decide whether to use the original (non-joint) parse or the new joint parse for each sentence since joint parses may be worse in cases where the sentences are too different and alignment fails. 3.1 Objective In a typical parsing setting, given a sentence (x) and its paraphrase (y), parsers find a*(x) and b*(y) that satisfy the following equation:2 where f is a parse-scoring function and T returns all possible trees for a sentence. f can take many forms, e.g., summing the scores of arcs (Eisner, 1996; McDonald et al., 2005) or multiplying probabilities together (Charniak and Johnson, 2005). The argmax over a and b of equation (1) is separable; parsers make two sentence-level decisions. For joint parsing, we modify the objective so that parsers make one global decision: a*, b* = argmax f(a) + f(b) (2) a∈T(x),b∈T(y) : c(a,b)=0 where c (defined below) measures the syntactic similarity between the two trees. The smaller c(a, b) is, the more similar a and b are. Intuitively, joint parsers must retrieve the most similar pair of trees with the highest sum of scores. 3.1.1 Constraints The constraint function, c, ties two trees together using alignments as a proxy for semantic informati</context>
<context position="20522" citStr="Charniak and Johnson, 2005" startWordPosition="3442" endWordPosition="3445"> alignments. In pilot experiments, we also tried using a simple aligner which required exact word matches. Joint parsing with simpler alignments improved parsing accuracy but not as much as Meteor.9 Thus, all results in Section 5 use Meteor for word alignment. On average across the four corpora, 73% of the tokens are aligned. 4.3 Parsers We use a dependency and constituency parser for our experiments: RBG and BLLIP. RBG parser (Lei et al., 2014) is a state-of-the-art dependency parser.10 It is a third-order discriminative dependency parser with low-rank tensors as part of its features. BLLIP (Charniak and Johnson, 2005) is a state-of-the-art constituency parser, which is 7The distribution over four corpora is skewed because each corpus has a different number of sentences within length constraints. Samples are collected uniformly over all sentences that satisfy the length criterion. 8Here paraphrase means a single/multiword phrase that is semantically similar to another single/multiword. 9The pilot was conducted on fewer than 700 sentence pairs before all paraphrases were created. We give Meteor tokenized paraphrases with capitalization. Maximizing accuracy rather than coverage worked better in pilot experime</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-Fine n-best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>An abstractive approach to sentence compression.</title>
<date>2013</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="5540" citStr="Cohn and Lapata, 2013" startWordPosition="827" endWordPosition="831">pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphrase templates. How did Bob Marley die? What killed Bob Marley? How fast does a cheetah run? What is a chee</context>
</contexts>
<marker>Cohn, Lapata, 2013</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2013. An abstractive approach to sentence compression. ACM Transactions on Intelligent Systems and Technology, 4(3):41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>175--182</pages>
<publisher>ICML.</publisher>
<contexts>
<context position="3577" citStr="Collins, 2000" startWordPosition="533" endWordPosition="534"> a dependency parser are encouraged to agree, Section 3.2) and pair-finding (which can be applied to any nbest parser, Section 3.3). Some paraphrases significantly disrupt syntactic structure. To counter this, we examine relaxing agreement constraints and building classifiers to predict when joint parsing won’t be beneficial (Section 3.4). We show that paraphrases can be exploited to improve crossdomain parser inference for two state-of-the-art parsers, especially on domains where they perform poorly. 2 Related Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are man</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 175–182. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Coppola</author>
<author>Mark Steedman</author>
</authors>
<title>The effect of higher-order dependency features in discriminative phrase-structure parsing.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>610--616</pages>
<contexts>
<context position="3944" citStr="Coppola and Steedman, 2013" startWordPosition="582" endWordPosition="585">t paraphrases can be exploited to improve crossdomain parser inference for two state-of-the-art parsers, especially on domains where they perform poorly. 2 Related Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., </context>
</contexts>
<marker>Coppola, Steedman, 2013</marker>
<rawString>Gregory Coppola and Mark Steedman. 2013. The effect of higher-order dependency features in discriminative phrase-structure parsing. In ACL, pages 610– 616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D. Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>376--380</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19544" citStr="Denkowski and Lavie (2014)" startWordPosition="3285" endWordPosition="3288"> single word in some cases (though, ideally, a variety of paraphrasing techniques would be used). We also provided 10 pairs of sentences as examples. We evaluate our methods only on the sampled sentences from the gold corpora because the new paraphrases do not include syntactic trees. The data was divided into development and testing sets such that development and testing share the same distribution over the four corpora. Paraphrases were tokenized by the BLLIP tokenizer. See Table 3 for statistics of the dataset.7 4.2 Meteor Word Aligner We use Meteor, a monolingual word aligner developed by Denkowski and Lavie (2014), to find alignments between paraphrases. It uses the exact matches, stems, synonyms, and paraphrases8 to form these alignments. Because it uses paraphrases, it sometimes aligns multiple words from sentence x to one or more words from sentence y or vice versa. We ignore these multiword alignments because our methods currently only handle single word alignments. In pilot experiments, we also tried using a simple aligner which required exact word matches. Joint parsing with simpler alignments improved parsing accuracy but not as much as Meteor.9 Thus, all results in Section 5 use Meteor for word</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of IWP.</booktitle>
<contexts>
<context position="5261" citStr="Dolan and Brockett, 2005" startWordPosition="781" endWordPosition="784">s falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of differe</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Ariel Duboue</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Answering the question you wish they had asked: The impact of paraphrasing for question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Duboue, Chu-Carroll, 2006</marker>
<rawString>Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. Answering the question you wish they had asked: The impact of paraphrasing for question answering. In Proceedings of the Human Language Technology Conference of the NAACL, pages 33–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>340--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4051" citStr="Eisner, 1996" startWordPosition="600" endWordPosition="601">ains where they perform poorly. 2 Related Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into th</context>
<context position="9069" citStr="Eisner, 1996" startWordPosition="1401" endWordPosition="1402">roximate, we also develop “soft” methods which allow for disagreements. Additionally, we make procedures to decide whether to use the original (non-joint) parse or the new joint parse for each sentence since joint parses may be worse in cases where the sentences are too different and alignment fails. 3.1 Objective In a typical parsing setting, given a sentence (x) and its paraphrase (y), parsers find a*(x) and b*(y) that satisfy the following equation:2 where f is a parse-scoring function and T returns all possible trees for a sentence. f can take many forms, e.g., summing the scores of arcs (Eisner, 1996; McDonald et al., 2005) or multiplying probabilities together (Charniak and Johnson, 2005). The argmax over a and b of equation (1) is separable; parsers make two sentence-level decisions. For joint parsing, we modify the objective so that parsers make one global decision: a*, b* = argmax f(a) + f(b) (2) a∈T(x),b∈T(y) : c(a,b)=0 where c (defined below) measures the syntactic similarity between the two trees. The smaller c(a, b) is, the more similar a and b are. Intuitively, joint parsers must retrieve the most similar pair of trees with the highest sum of scores. 3.1.1 Constraints The constra</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th conference on Computational linguistics, pages 340–345. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1608--1618</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5610" citStr="Fader et al., 2013" startWordPosition="839" endWordPosition="842">e constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphrase templates. How did Bob Marley die? What killed Bob Marley? How fast does a cheetah run? What is a cheetah’s top speed? He came home unexpectedly. He wasn’t expected to arri</context>
<context position="30956" citStr="Fader et al., 2013" startWordPosition="5145" endWordPosition="5148">phrases are not available at most times, our methods may seem limited. However, there are several possible use cases. The best case scenario is when users can be directly asked to rephrase a question and provide a paraphrase. For instance, question answering systems can ask users to rephrase questions when an answer is marked as wrong by users. Another option is to use crowdsourcing to quickly create a paraphrase corpus (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). As part of future work, we plan to integrate existing larger paraphrase resources, such as WikiAnswers (Fader et al., 2013) and PPDB (Ganitkevitch et al., 2013). WikiAnswers provides rough equivalence classes of questions. PPDB includes phrasal and syntactic alignments which could supplement our existing alignments or be used as proxies for paraphrases. 17See the extended version of this paper for more information about this task and its results. 1230 Avg BNC Brown QB WSJ RBG 86.7 (81.3) 89.3 (83.7) 90.2 (84.1) 77.0 (71.0) 93.7 (89.9) + S-Dual 87.3 (81.7) 89.6 (83.8) 90.7 (84.6) 78.1 (71.8) 94.0 (90.2) + Logit 87.2 (81.6) 89.7 (83.9) 90.6 (84.5) 77.9 (71.7) 93.8 (89.9) BLLIP 89.6 (86.1) 90.6 (87.2) 91.7 (87.9) 83.</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1608–1618. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Parser evaluation and the BNC: Evaluating 4 constituency parsers with 3 metrics.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation. European Language Resources Association.</booktitle>
<marker>Foster, van Genabith, 2008</marker>
<rawString>Jennifer Foster and Josef van Genabith. 2008. Parser evaluation and the BNC: Evaluating 4 constituency parsers with 3 metrics. In Proceedings of the Sixth International Conference on Language Resources and Evaluation. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winthrop Nelson Francis</author>
<author>Henry Kuˇcera</author>
</authors>
<title>Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers.</title>
<date>1989</date>
<institution>Brown University, Department of Linguistics.</institution>
<marker>Francis, Kuˇcera, 1989</marker>
<rawString>Winthrop Nelson Francis and Henry Kuˇcera. 1989. Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers. Brown University, Department of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>758--764</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of NAACL-HLT, pages 758–764. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>167--202</pages>
<contexts>
<context position="1608" citStr="Gildea, 2001" startWordPosition="230" endWordPosition="231">uency and dependency parsers and provide significant improvements across multiple domains. 1 Introduction Parsing is the task of reconstructing the syntactic structure from surface text. Many natural language processing tasks use parse trees as a basis for deeper analysis. The most effective sources of supervision for training statistical parsers are treebanks. Unfortunately, treebanks are expensive, time-consuming to create, and not available for most domains. Compounding the problem, the accuracy of statistical parsers degrades as the domain shifts away from the supervised training corpora (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b; Surdeanu et al., 2008). Furthermore, for ∗Work performed during an IBM internship. domains requiring subject matter experts, e.g., law and medicine, it may not be feasible to produce large scale treebanks since subject matter experts generally don’t have the necessary linguistic background. It is natural to look for resources that are more easily obtained. In this work, we explore using paraphrases. Unlike parse trees, paraphrases can be produced quickly by humans and don’t require extensive linguistic training. While paraphrases are not parse </context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 167–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32971" citStr="Heilman and Smith, 2010" startWordPosition="5467" endWordPosition="5470">ation systems could help provide paraphrases on demand. There are many other ways to extend this work. Poor alignments are one of the larger sources of errors and improving alignments could help dramatically. One simple extension is to use multiple paraphrases and their alignments instead of just one. More difficult would be to learn the alignments jointly while parsing and adaptively learn how alignments affect syntax. Our constraints can only capture certain types of paraphrase transformations currently and should be extended to understand common tree transformations for paraphrases, as in (Heilman and Smith, 2010). Finally, and perhaps most importantly, our methods apply only at inference time. We plan to investigate methods which use paraphrases to augment parsing models created at train time. Acknowledgments We would like to thank Eugene Charniak for the idea of using paraphrases to improve parsing, our anonymous reviewers for their valuable feedback, Karen Ingraffea for constructing and classifying the paraphrase corpus, Dave Buchanan and Will Headden for last minute paper reading, the DeepQA team at IBM for feedback and support on the project, and Mohit Bansal and Micha Elsner for helpful discussio</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1011–1019. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef Van Genabith</author>
</authors>
<title>QuestionBank: creating a corpus of parseannotated questions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>497--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Judge, Cahill, Van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef Van Genabith. 2006. QuestionBank: creating a corpus of parseannotated questions. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 497–504. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4117" citStr="Koo and Collins, 2010" startWordPosition="610" endWordPosition="613">ituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraph</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL: HLT,</booktitle>
<pages>595--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4452" citStr="Koo et al., 2008" startWordPosition="662" endWordPosition="665">Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms,</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL: HLT, pages 595–603. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1381--1391</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4162" citStr="Lei et al., 2014" startWordPosition="618" endWordPosition="621">ith high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self</context>
<context position="20344" citStr="Lei et al., 2014" startWordPosition="3416" endWordPosition="3419">ple words from sentence x to one or more words from sentence y or vice versa. We ignore these multiword alignments because our methods currently only handle single word alignments. In pilot experiments, we also tried using a simple aligner which required exact word matches. Joint parsing with simpler alignments improved parsing accuracy but not as much as Meteor.9 Thus, all results in Section 5 use Meteor for word alignment. On average across the four corpora, 73% of the tokens are aligned. 4.3 Parsers We use a dependency and constituency parser for our experiments: RBG and BLLIP. RBG parser (Lei et al., 2014) is a state-of-the-art dependency parser.10 It is a third-order discriminative dependency parser with low-rank tensors as part of its features. BLLIP (Charniak and Johnson, 2005) is a state-of-the-art constituency parser, which is 7The distribution over four corpora is skewed because each corpus has a different number of sentences within length constraints. Samples are collected uniformly over all sentences that satisfy the length criterion. 8Here paraphrase means a single/multiword phrase that is semantically similar to another single/multiword. 9The pilot was conducted on fewer than 700 sent</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1381–1391. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="18123" citStr="Marcus et al., 1993" startWordPosition="3055" endWordPosition="3058">res are prefixed by whether they come from a0 or a. 4 Data and Programs This section describes our paraphrase dataset, parsers, and other tools used in experiments. 4.1 Paraphrase Dataset To evaluate the efficacy of the proposed methods of jointly parsing paraphrases, we built a corpus of paraphrases where one sentence in a pair of paraphrases has a gold tree.4 We randomly sampled 4,000 sentences5 from four gold treebanks: Brown, British National Corpus (BNC), QuestionBank6 (QB) and Wall Street Journal (section 24) (Francis and Kuˇcera, 1989; Foster and van Genabith, 2008; Judge et al., 2006; Marcus et al., 1993). A linguist provided a paraphrase for each sampled sentence according to these instructions: The paraphrases should more or less convey the same information as the original sentence. That is, the two sentences should logically entail each other. The paraphrases should generally use most of the same words (but not necessarily in the same order). Active/passive transforms, changing words with synonyms, and rephrasings of the same idea are all examples of transformations that paraphrases can use (others can be used too). 4The dataset is available upon request. 5We use sentences with 6 to 25 toke</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1655" citStr="McClosky et al., 2006" startWordPosition="236" endWordPosition="239">de significant improvements across multiple domains. 1 Introduction Parsing is the task of reconstructing the syntactic structure from surface text. Many natural language processing tasks use parse trees as a basis for deeper analysis. The most effective sources of supervision for training statistical parsers are treebanks. Unfortunately, treebanks are expensive, time-consuming to create, and not available for most domains. Compounding the problem, the accuracy of statistical parsers degrades as the domain shifts away from the supervised training corpora (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b; Surdeanu et al., 2008). Furthermore, for ∗Work performed during an IBM internship. domains requiring subject matter experts, e.g., law and medicine, it may not be feasible to produce large scale treebanks since subject matter experts generally don’t have the necessary linguistic background. It is natural to look for resources that are more easily obtained. In this work, we explore using paraphrases. Unlike parse trees, paraphrases can be produced quickly by humans and don’t require extensive linguistic training. While paraphrases are not parse trees, a sentence and its paraphrase may have s</context>
<context position="4330" citStr="McClosky et al., 2006" startWordPosition="641" endWordPosition="645">rence on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phra</context>
<context position="21540" citStr="McClosky et al., 2006" startWordPosition="3589" endWordPosition="3592">ed on fewer than 700 sentence pairs before all paraphrases were created. We give Meteor tokenized paraphrases with capitalization. Maximizing accuracy rather than coverage worked better in pilot experiments. 10http://github.com/taolei87/RBGParser, ‘master’ version from June 24th, 2014. composed of a generative parser and a discriminative reranker.11 To train RBG and BLLIP, we used the standard WSJ training set (sections 2–21, about 40,000 sentences).12 We also used the self-trained BLLIP parsing model which is trained on an additional two million Gigaword parses generated by the BLLIP parser (McClosky et al., 2006a). 4.4 Logistic Regression We use the logistic regression implementation from Scikit-learn13 with hand-crafted features from Section 3.4.1. The classifier decides to whether to keep the parse trees from the joint method. When it decides to disregard them, it returns the parse from the baseline parser. We train a separate classifier for each joint method. 5 Experiments We ran all tuning and model design experiments on the development set. For the final evaluation, we tuned parameters on the development set and evaluate them on the test set. Constituency trees were converted to basic non-collap</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006a. Effective self-training for parsing. In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics, pages 152–159. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>337--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1655" citStr="McClosky et al., 2006" startWordPosition="236" endWordPosition="239">de significant improvements across multiple domains. 1 Introduction Parsing is the task of reconstructing the syntactic structure from surface text. Many natural language processing tasks use parse trees as a basis for deeper analysis. The most effective sources of supervision for training statistical parsers are treebanks. Unfortunately, treebanks are expensive, time-consuming to create, and not available for most domains. Compounding the problem, the accuracy of statistical parsers degrades as the domain shifts away from the supervised training corpora (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b; Surdeanu et al., 2008). Furthermore, for ∗Work performed during an IBM internship. domains requiring subject matter experts, e.g., law and medicine, it may not be feasible to produce large scale treebanks since subject matter experts generally don’t have the necessary linguistic background. It is natural to look for resources that are more easily obtained. In this work, we explore using paraphrases. Unlike parse trees, paraphrases can be produced quickly by humans and don’t require extensive linguistic training. While paraphrases are not parse trees, a sentence and its paraphrase may have s</context>
<context position="4330" citStr="McClosky et al., 2006" startWordPosition="641" endWordPosition="645">rence on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phra</context>
<context position="21540" citStr="McClosky et al., 2006" startWordPosition="3589" endWordPosition="3592">ed on fewer than 700 sentence pairs before all paraphrases were created. We give Meteor tokenized paraphrases with capitalization. Maximizing accuracy rather than coverage worked better in pilot experiments. 10http://github.com/taolei87/RBGParser, ‘master’ version from June 24th, 2014. composed of a generative parser and a discriminative reranker.11 To train RBG and BLLIP, we used the standard WSJ training set (sections 2–21, about 40,000 sentences).12 We also used the self-trained BLLIP parsing model which is trained on an additional two million Gigaword parses generated by the BLLIP parser (McClosky et al., 2006a). 4.4 Logistic Regression We use the logistic regression implementation from Scikit-learn13 with hand-crafted features from Section 3.4.1. The classifier decides to whether to keep the parse trees from the joint method. When it decides to disregard them, it returns the parse from the baseline parser. We train a separate classifier for each joint method. 5 Experiments We ran all tuning and model design experiments on the development set. For the final evaluation, we tuned parameters on the development set and evaluate them on the test set. Constituency trees were converted to basic non-collap</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 337–344. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4549" citStr="McClosky et al., 2010" startWordPosition="675" endWordPosition="678">d Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists som</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: Application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>835--842</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5973" citStr="Nakov and Hearst, 2005" startWordPosition="892" endWordPosition="895"> paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphrase templates. How did Bob Marley die? What killed Bob Marley? How fast does a cheetah run? What is a cheetah’s top speed? He came home unexpectedly. He wasn’t expected to arrive home like that. They were far off and looked tiny. From so far away, they looked tiny. He turned and bent over the body of the Indian. Turning, he bent over the Indian’s body. No need to dramatize. There is no need to dramatize. Table 1: Example paraphrases from our dataset. 2.2 Bilingual Parsing The closest task to ours is bilingual parsing where sentences </context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the web as an implicit training set: Application to structural ambiguity resolution. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 835–842, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Yashar Mehdad</author>
<author>Alessandro Marchetti</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
</authors>
<title>Chinese whispers: Cooperative paraphrase acquisition.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>2659--2665</pages>
<contexts>
<context position="5414" citStr="Negri et al., 2012" startWordPosition="806" endWordPosition="810">araphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructio</context>
<context position="30780" citStr="Negri et al., 2012" startWordPosition="5116" endWordPosition="5119">rases often express the same semantics with similar syntactic realizations. These provide benefits even on top of selftraining, another domain adaptation technique. Since paraphrases are not available at most times, our methods may seem limited. However, there are several possible use cases. The best case scenario is when users can be directly asked to rephrase a question and provide a paraphrase. For instance, question answering systems can ask users to rephrase questions when an answer is marked as wrong by users. Another option is to use crowdsourcing to quickly create a paraphrase corpus (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). As part of future work, we plan to integrate existing larger paraphrase resources, such as WikiAnswers (Fader et al., 2013) and PPDB (Ganitkevitch et al., 2013). WikiAnswers provides rough equivalence classes of questions. PPDB includes phrasal and syntactic alignments which could supplement our existing alignments or be used as proxies for paraphrases. 17See the extended version of this paper for more information about this task and its results. 1230 Avg BNC Brown QB WSJ RBG 86.7 (81.3) 89.3 (83.7) 90.2 (84.1) 77.0 (71.0) 93.7 (89.9) + S-Du</context>
</contexts>
<marker>Negri, Mehdad, Marchetti, Giampiccolo, Bentivogli, 2012</marker>
<rawString>Matteo Negri, Yashar Mehdad, Alessandro Marchetti, Danilo Giampiccolo, and Luisa Bentivogli. 2012. Chinese whispers: Cooperative paraphrase acquisition. In LREC, pages 2659–2665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="4094" citStr="Nivre et al., 2007" startWordPosition="606" endWordPosition="609">ated Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show late</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3894" citStr="Petrov et al., 2006" startWordPosition="574" endWordPosition="577">t be beneficial (Section 3.4). We show that paraphrases can be exploited to improve crossdomain parser inference for two state-of-the-art parsers, especially on domains where they perform poorly. 2 Related Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automa</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>705--713</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4353" citStr="Petrov et al., 2010" startWordPosition="646" endWordPosition="649">e Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approxima</context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4486" citStr="Petrov, 2010" startWordPosition="668" endWordPosition="669">; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementary benefits to self-training. 2.1 Paraphrases While paraphrases are difficult to define rigorously (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relati</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="11281" citStr="Rush and Collins, 2012" startWordPosition="1785" endWordPosition="1788"> how many pairs of alignments have different syntactic head relations. For the two trees in Figure 1, we see two different relations: (help x−� dying, help 4y� dying) and (natives x��− dying, natives y−� dying). The rest have the same relation so c(a, b) = 2. As we’ll show in Section 5, the constraints defined above are too restrictive because of this strong assumption. To alleviate the problem, we present ways of appropriately changing constraints later. We now turn to the first method of incorporating constraints into joint parsing. 3.2 Constraints via Dual Decomposition Dual decomposition (Rush and Collins, 2012) is well-suited for finding the MAP assignment to equation (2). When the parse-scoring function f includes an arc-factored component as in McDonald et al. (2005), it is straightforward to incorpoSet u0(i, j) = 0 for all i, j E E fork = 1 to K do � X � ak = argmax f(a) + uk(i, j)a(i, j) aET (x) bk = argmax iX � bET(y) f(b) − uk(i, j)b(i, j) i,jEE v, uk+1 = UPDATE(uk, δk, ak, bk) if v = 0 then return ak, bk return aK, bK function UPDATE(u, δ, a, b) v = 0,u&apos;(i,j) = 0 for all i,j E E for i, j E Edo u&apos;(i, j) = u(i, j) − δ(a(i, j) − b(i, j)) ifa(i,j) =�b(i,j) then v = v + 1 return v, u&apos; i,jEE a*, b*</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M. Rush and Michael Collins. 2012. A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. Journal of Artificial Intelligence Research, 45(1):305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22754" citStr="Rush et al., 2010" startWordPosition="3787" endWordPosition="3790">llapsed dependency trees using Stanford Dependencies (De Marneffe et al., 2006).14 We report unlabeled attachment scores (UAS) for all experiments and labeled attachment scores (LAS) as well in final evaluation, ignoring punctuation. Averages are microaverages across all sentences. 5.1 Dual Decomposition Since BLLIP is not arc-factored, these experiments only use RBG. Several parameters need to be fixed beforehand: the slack constant (s), the learning rate (6), and the maximum number of iterations (K). We set 60 = 0.1 and 6k = δ0 2, where t is the number of times the dual score has increased (Rush et al., 2010). We choose K = 20. These numbers were chosen from pilot studies. The slack variable (s = 0.5) was tuned with a grid search on values between 0.1 and 1.5 with interval 0.1. We chose a value that generalizes well across four corpora as opposed to a value that does 11http://github.com/BLLIP/bllip-parser 12RBG parser requires predicted POS tags. We used the Stanford tagger (Toutanova et al., 2003) to tag WSJ and paraphrase datasets. Training data was tagged using 20-fold cross-validation and the paraphrases were tagged by a tagger trained on all of WSJ training. 13http://scikit-learn.org 14Versio</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Roi Reichart</author>
<author>Michael Collins</author>
<author>Amir Globerson</author>
</authors>
<title>Improved parsing and POS tagging using inter-sentence consistency constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1434--1444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7607" citStr="Rush et al. (2012)" startWordPosition="1158" endWordPosition="1161">isambiguate. Existing bilingual parsing methods typically require parallel treebanks for training and parallel text at runtime while our methods only require parallel text at runtime. Since we do not have a parallel paraphrase treebank for training, we cannot directly compare to these methods. 3 Jointly Parsing Paraphrases With a small number of exceptions, parsers typically assume that the parse of each sentence is independent. There are good reasons for this independence assumption: it simplifies parsing inference and oftentimes it is not obvious how to relate multiple sentences (though see Rush et al. (2012) for one approach). In this section, we present two methods to jointly parse paraphrases without complicating inference steps. Before going into details, we give a high level picture of how jointly parsing paraphrases can help in Figure 1. With the baseline parser, the parse tree of the target sentence is incorrect but its paraphrase (parsed by the same parser) is parsed correctly. We use rough alignments to map words across sentence pairs. 1Applying our methods to bilingual parsing is left as future work. 1224 Note the similar syntactic relations when they are projected across the aligned wor</context>
</contexts>
<marker>Rush, Reichart, Collins, Globerson, 2012</marker>
<rawString>Alexander M. Rush, Roi Reichart, Michael Collins, and Amir Globerson. 2012. Improved parsing and POS tagging using inter-sentence consistency constraints. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1434–1444. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3915" citStr="Socher et al., 2013" startWordPosition="578" endWordPosition="581">ion 3.4). We show that paraphrases can be exploited to improve crossdomain parser inference for two state-of-the-art parsers, especially on domains where they perform poorly. 2 Related Work Many constituency parsers can parse English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting tra</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455–465. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23151" citStr="Toutanova et al., 2003" startWordPosition="3854" endWordPosition="3857">ed to be fixed beforehand: the slack constant (s), the learning rate (6), and the maximum number of iterations (K). We set 60 = 0.1 and 6k = δ0 2, where t is the number of times the dual score has increased (Rush et al., 2010). We choose K = 20. These numbers were chosen from pilot studies. The slack variable (s = 0.5) was tuned with a grid search on values between 0.1 and 1.5 with interval 0.1. We chose a value that generalizes well across four corpora as opposed to a value that does 11http://github.com/BLLIP/bllip-parser 12RBG parser requires predicted POS tags. We used the Stanford tagger (Toutanova et al., 2003) to tag WSJ and paraphrase datasets. Training data was tagged using 20-fold cross-validation and the paraphrases were tagged by a tagger trained on all of WSJ training. 13http://scikit-learn.org 14Version 1.3.5, previously numbered as version 2.0.5 1228 Development Test BNC Brown QB WSJ Total BNC Brown QB WSJ Total Sentences 247 558 843 352 2,000 247 558 844 351 2,000 Tokens 4,297 7,937 8,391 5,924 26,549 4,120 8,025 8,253 5,990 26,388 Tokensll 4,372 8,088 8,438 6,122 27,020 4,272 8,281 8,189 6,232 26,974 Word types 1,727 2,239 2,261 1,955 6,161 1,710 2,337 2,320 1,970 6,234 Word types 1,676 2</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Tschirsich</author>
<author>Gerold Hintz</author>
</authors>
<title>Leveraging crowdsourcing for paraphrase recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>205--213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5465" citStr="Tschirsich and Hintz, 2013" startWordPosition="815" endWordPosition="819">sly (Bhagat and Hovy, 2013), we only require a loose definition in this work: a pair of phrases that mean approximately the same thing. Paraphrases can be constructed in various ways: replacing words with synonyms, reordering clauses, adding relative clauses, using negation and antonyms, etc. Table 1 lists some example paraphrases. There are a variety of paraphrase resources produced by humans (Dolan and Brockett, 2005) and automatic methods (Ganitkevitch et al., 2013). Recent works have shown that reliable paraphrases can be crowdsourced at low cost (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). Paraphrases have been shown to help summarization (Cohn and Lapata, 2013), question answering (Duboue and ChuCarroll, 2006; Fader et al., 2013), machine translation (Callison-Burch et al., 2006), and semantic parsing (Berant and Liang, 2014). Paraphrases have been applied to syntactic tasks, such as prepositional phrase attachment and noun compounding, where the corpus frequencies of different syntactic constructions (approximated by web searches) are used to help disambiguate (Nakov and Hearst, 2005). One method for transforming constructions is to use paraphrase templates. How did Bob Marl</context>
<context position="30831" citStr="Tschirsich and Hintz, 2013" startWordPosition="5124" endWordPosition="5127">h similar syntactic realizations. These provide benefits even on top of selftraining, another domain adaptation technique. Since paraphrases are not available at most times, our methods may seem limited. However, there are several possible use cases. The best case scenario is when users can be directly asked to rephrase a question and provide a paraphrase. For instance, question answering systems can ask users to rephrase questions when an answer is marked as wrong by users. Another option is to use crowdsourcing to quickly create a paraphrase corpus (Negri et al., 2012; Burrows et al., 2013; Tschirsich and Hintz, 2013). As part of future work, we plan to integrate existing larger paraphrase resources, such as WikiAnswers (Fader et al., 2013) and PPDB (Ganitkevitch et al., 2013). WikiAnswers provides rough equivalence classes of questions. PPDB includes phrasal and syntactic alignments which could supplement our existing alignments or be used as proxies for paraphrases. 17See the extended version of this paper for more information about this task and its results. 1230 Avg BNC Brown QB WSJ RBG 86.7 (81.3) 89.3 (83.7) 90.2 (84.1) 77.0 (71.0) 93.7 (89.9) + S-Dual 87.3 (81.7) 89.6 (83.8) 90.7 (84.6) 78.1 (71.8) </context>
</contexts>
<marker>Tschirsich, Hintz, 2013</marker>
<rawString>Martin Tschirsich and Gerold Hintz. 2013. Leveraging crowdsourcing for paraphrase recognition. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 205–213, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Enforcing structural diversity in cube-pruned dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>656--661</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4143" citStr="Zhang and McDonald, 2014" startWordPosition="614" endWordPosition="617">se English newswire text with high accuracy (Collins, 2000; 1223 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1223–1233, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Charniak and Johnson, 2005; Petrov et al., 2006; Socher et al., 2013; Coppola and Steedman, 2013). Likewise, dependency parsers have rapidly improved their accuracy on a variety of languages (Eisner, 1996; McDonald et al., 2005; Nivre et al., 2007; Koo and Collins, 2010; Zhang and McDonald, 2014; Lei et al., 2014). There are many approaches tackling the problem of improving parsing accuracy both within and across domains, including self-training/uptraining (McClosky et al., 2006b; Petrov et al., 2010), reranking (Collins, 2000; McClosky et al., 2006b), incorporating word clusters (Koo et al., 2008), model combination (Petrov, 2010), automatically weighting training data (McClosky et al., 2010), and using n-gram counts from large corpora (Bansal and Klein, 2011). Using paraphrases falls into the semi-supervised category. As we show later, incorporating paraphrases provides complementa</context>
</contexts>
<marker>Zhang, McDonald, 2014</marker>
<rawString>Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 656–661. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>