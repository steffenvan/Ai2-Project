<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999490666666667">
Using Suffix Arrays to Compute Term
Frequency and Document Frequency for
All Substrings in a Corpus
</title>
<author confidence="0.998934">
Mikio Yamamoto* Kenneth W. Churcht
</author>
<affiliation confidence="0.982233">
University of Tsukuba AT&amp;T Labs—Research
</affiliation>
<bodyText confidence="0.98995745">
Bigrams and trigrams are commonly used in statistical natural language processing; this paper
will describe techniques for working with much longer n-grams. Suffix arrays (Manber and My-
ers 1990) were first introduced to compute the frequency and location of a substring (n-gram)
in a sequence (corpus) of length N. To compute frequencies over all N(N + 1)/2 substrings in a
corpus, the substrings are grouped into a manageable number of equivalence classes. In this way,
a prohibitive computation over substrings is reduced to a manageable computation over classes.
This paper presents both the algorithms and the code that were used to compute term frequency
(tf) and document frequency (df) for all n-grams in two large corpora, an English corpus of
50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of
Mainichi Shimbun.
The second half of the paper uses these frequencies to find &amp;quot;interesting&amp;quot; substrings. Lexi-
cographers have been interested in n-grams with high mutual information (MI) where the joint
term frequency is higher than what would be expected by chance, assuming that the parts of the
n-gram combine independently. Residual inverse document frequency (RIDF) compares docu-
ment frequency to another model of chance where terms with a particular term frequency are
distributed randomly throughout the collection. MI tends to pick out phrases with noncom po-
sitional semantics (which often violate the independence assumption) whereas RIDF tends to
highlight technical terminology, names, and good keywords for information retrieval (which tend
to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is
better than either by itself in a Japanese word extraction task.
</bodyText>
<sectionHeader confidence="0.99123" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999894818181818">
We will use suffix arrays (Manber and Myers 1990) to compute a number of type/token
statistics of interest, including term frequency and document frequency, for all n-grams
in large corpora. Type/token statistics model the corpus as a sequence of N tokens
(characters, words, terms, n-grams, etc.) drawn from a vocabulary of V types. Different
tokenizing rules will be used for different corpora and for different applications. In
this work, the English text is tokenized into a sequence of English words delimited by
white space and the Japanese text is tokenized into a sequence of Japanese characters
(typically one or two bytes each).
Term frequency (tf) is the standard notion of frequency in corpus-based natural
language processing (NLP); it counts the number of times that a type (term/word/ n-
gram) appears in a corpus. Document frequency (df) is borrowed for the information
</bodyText>
<note confidence="0.813180333333333">
* Institute of Information Sciences and Electronics, 1-1-1 Termodai, Tsukuba 305-8573, Japan
t 180 Park Avenue, Florham Park, NJ 07932
Computational Linguistics Volume 27, Number 1
</note>
<bodyText confidence="0.9986661875">
retrieval literature (Sparck Jones 1972); it counts the number of documents that con-
tain a type at least once. Term frequency is an integer between 0 and N; document
frequency is an integer between 0 and D, the number of documents in the corpus.
The statistics, tf and df, and functions of these statistics such as mutual information
(MI) and inverse document frequency (IDF), are usually computed over short n-grams
such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens) (Charniak 1993;
Jelinek 1997). This paper will show how to work with much longer n-grams, including
million-grams and even billion-grams.
In corpus-based NLP, term frequencies are often converted into probabilities, using
the maximum likelihood estimator (MLE), the Good-Turing method (Katz 1987), or
Deleted Interpolation (Jelinek 1997, Chapter 15). These probabilities are used in noisy
channel applications such as speech recognition to distinguish more likely sequences
from less likely sequences, reducing the search space (perplexity) for the acoustic
recognizer. In information retrieval, document frequencies are converted into inverse
document frequency (IDF), which plays an important role in term weighting (Sparck
Jones 1972).
</bodyText>
<equation confidence="0.947558">
IDF (t) = - log2 df (t)
</equation>
<bodyText confidence="0.9949555">
IDF (t) can be interpreted as the number of bits of information the system is given if it
is told that the document in question contains the term t. Rare terms contribute more
bits than common terms.
Mutual information (MI) and residual IDF (RIDF) (Church and Gale 1995) both
compare tf and df to what would be expected by chance, using two different notions of
chance. MI compares the term frequency of an n-gram to what would be expected if the
parts combined independently, whereas RIDF combines the document frequency of a
term to what would be expected if a term with a given term frequency were randomly
distributed throughout the collection. MI tends to pick out phrases with noncompo-
sitional semantics (which often violate the independence assumption) whereas RIDF
tends to highlight technical terminology, names, and good keywords for information
retrieval (which tend to exhibit nonrandom distributions over documents).
Assuming a random distribution of a term (Poisson model), the probability Pd (k)
that a document will have exactly k instances of the term is:
</bodyText>
<equation confidence="0.987573">
e—e0k
Pd(k) --= 7(0,k) = id ,
</equation>
<bodyText confidence="0.9999615">
where 0 = np, n is the average length of a document, and p is the occurrence probability
of the term. That is,
</bodyText>
<equation confidence="0.951757666666667">
N tf tf
u — — —
DN D.
Residual IDF is defined as the following formula.
Residual IDF = observed IDF — predicted IDF
df
=_ -log —D + log(1 - pd(0))
= -log —df + log(1 - e_1 D )
D
</equation>
<bodyText confidence="0.9911215">
The rest of the paper is divided into two sections. Section 2 describes the algorithms
and the code that were used to compute term frequencies and document frequencies
</bodyText>
<page confidence="0.995901">
2
</page>
<note confidence="0.8897195">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
for all substrings in two large corpora, an English corpus of 50 million words of the
Wall Street Journal, and a Japanese corpus of 216 million characters of the Mainichi
Shimbun.
</note>
<bodyText confidence="0.999921888888889">
Section 3 uses these frequencies to find &amp;quot;interesting&amp;quot; substrings, where what counts
as &amp;quot;interesting&amp;quot; depends on the application. MI finds phrases of interest to lexicog-
raphy, general vocabulary whose distribution is far from random combination of the
parts, whereas RIDF picks out technical terminology, names, and keywords that are
useful for information retrieval, whose distribution over documents is far from uni-
form or Poisson. These observations may be particularly useful for a Japanese word
extraction task. Sequences of characters that are high in both MI and RIDF are more
likely to be words than sequences that are high in just one, which are more likely than
sequences that are high in neither.
</bodyText>
<sectionHeader confidence="0.478392" genericHeader="keywords">
2. Computing tf and df for All Substrings
</sectionHeader>
<subsectionHeader confidence="0.998279">
2.1 Suffix Arrays
</subsectionHeader>
<bodyText confidence="0.999880485714285">
This section will introduce an algorithm based on suffix arrays for computing tf and
df and many functions of these quantities for all substrings in a corpus in 0(N log N)
time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The
algorithm groups the N(N + 1)/2 substrings into at most 2N — 1 equivalence classes.
By grouping substrings in this way, many of the statistics of interest can be computed
over the relatively small number of classes, which is manageable, rather than over the
quadratic number of substrings, which would be prohibitive.
The suffix array data structure (Manber and Myers 1990) was introduced as a
database indexing technique. Suffix arrays can be viewed as a compact representa-
tion of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been
extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive
introduction to suffix trees. Hui (1992) shows how to compute df for all substrings
using generalized suffix trees. The major advantage of suffix arrays over suffix trees
is space. The space requirements for suffix trees (but not for suffix arrays) grow with
alphabet size: O(NIE 1) space, where I El is the alphabet size. The dependency on al-
phabet size is a serious issue for Japanese. Manber and Myers (1990) reported that
suffix arrays are an order of magnitude more efficient in space than suffix trees, even
in the case of relatively small alphabet size (1E1 = 96). The advantages of suffix arrays
over suffix trees become much more significant for larger alphabets such as Japanese
characters (and English words).
The suffix array data structure makes it convenient to compute the frequency
and location of a substring (n-gram) in a long sequence (corpus). The early work was
motivated by biological applications such as matching of DNA sequences. Suffix arrays
are closely related to PAT arrays (Gonnet, Baeza-Yates, and Snider 1992), which were
motivated in part by a project at the University of Waterloo to distribute the Oxford
English Dictionary with indexes on CD-ROM. PAT arrays have also been motivated
by applications in information retrieval. A similar data structure to suffix arrays was
proposed by Nagao and Mori (1994) for processing Japanese text.
The alphabet sizes vary considerably in each of these cases. DNA has a relatively
small alphabet of just 4 characters, whereas Japanese has a relatively large alphabet of
more than 5,000 characters. The methods such as suffix arrays and PAT arrays scale
naturally over alphabet size. In the experimental section (Section 3) using the Wall Street
Journal corpus, the suffix array is applied to a large corpus of English text, where the al-
phabet is assumed to be the set of all English words, an unbounded set. It is sometimes
assumed that larger alphabets are more challenging than smaller ones, but ironically,
</bodyText>
<page confidence="0.99768">
3
</page>
<note confidence="0.882797">
Computational Linguistics Volume 27, Number 1
</note>
<bodyText confidence="0.861357340425532">
it can be just the reverse because there is often an inverse relationship between the size
of the alphabet and the length of meaningful or interesting substrings. For expository
convenience, this section will use the letters of the alphabet, a—z, to denote tokens.
This section starts by reviewing the construction of suffix arrays and how they
have been used to compute the frequency and locations of a substring in a sequence.
We will then show how these methods can be applied to find not only the frequency
of a particular substring but also the frequency of all substrings. Finally, the methods
are generalized to compute document frequencies as well as term frequencies.
A suffix array, s, is an array of all N suffixes, sorted alphabetically. A suffix, s[i],
also known as a semi-infinite string, is a string that starts at position i in the corpus
and continues to the end of the corpus. In practical implementations, it is typically
denoted by a four-byte integer, i. In this way, a small (constant) amount of space is
used to represent a very long substring, which one might have thought would require
N space.
A substring, sub(i, j), is a prefix of a suffix. That is, sub(i,j), is the first j characters
of the suffix s[i]. The corpus contains N(N + 1)/2 substrings.
The algorithm, suf f ix_array, presented below takes a corpus and its length N as
input, and outputs the suffix array, s.
suf f x_ array 4-- function(corpus, N){
Initialize s to be a vector of integers from 0 to N — 1.
Let each integer denote a suffix starting at s[i] in the corpus.
Sort s so that the suffixes are in alphabetical order.
Return s. }
The C program below implements this algorithm.
char *corpus;
int suffix_compare(int *a, int *b)
{ return strcmp(corpus+*a, corpus+*b);}
int *suffix_array(int n){
int i, *s = (int *)malloc(n*sizeof(int));
for(i=0; i &lt; n; i++) s[i] = i; /* initialize */
qsort(s, n, sizeof(int), suffix_compare); /* sort */
return s;I
Figures 1 and 2 illustrate a simple example where the corpus (&amp;quot;to_be_or_not_to_be&amp;quot;)
consists of N = 18 (19 bytes): 13 alphabetic characters plus 5 spaces (and 1 null
termination). The C program (above) starts by allocating memory for the suffix array
(18 integers of 4 bytes each). The suffix array is initialized to the integers from 0
to 17. Finally, the suffix array is sorted by alphabetical order. The suffix array after
initialization is shown in Figure 1. The suffix array after sorting is shown in Figure 2.
As mentioned above, suffix arrays were designed to make it easy to compute the
frequency (tf) and locations of a substring (n-gram or term) in a sequence (corpus).
Given a substring or term, t, a binary search is used to find the first and last suffix
that start with t. Let s[i] be the first such suffix and s[j] be the last such suffix. Then
tf(t) = j — i + 1 and the term is located at positions: {s[i], s[i + 1],.. ,s[j]1, and only
these positions.
Figure 2 shows how this procedure can be used to compute the frequency and
locations of the term &amp;quot;to_be&amp;quot; in the corpus &amp;quot;to_be_or_not_to_be&amp;quot;. As illustrated in the
figure, s[i = 16] is the first suffix to start with the term &amp;quot;to_be&amp;quot; and s[j = 17] is the last
</bodyText>
<page confidence="0.996269">
4
</page>
<note confidence="0.7879435">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
Input corpus: &amp;quot;to_be_or_not to be&amp;quot;
</note>
<figureCaption confidence="0.7960195">
Figure 1
Illustration of a suffix array, s, that has just been initialized and not yet sorted. Each element in
the suffix array, s[i], is an integer denoting a suffix or a semi-infinite string, starting at
position i in the corpus and extending to the end of the corpus.
</figureCaption>
<figure confidence="0.995158014492753">
Suffix Array Suffixes denoted by s [i]
13
15
16
17
11
12
14
9
10
4
5
6
7
8
2
3
0
null
0
0
0
0
A
Initialized
Suffix Arra
S [ 0 ]
S [ 1 ]
• [ 2 ]
s[3]
A A
• • •
Suffixes denoted by slil
to_be_or not to_be
o_be_or_not to_be
_be_or_not_to be
be_or_not_to be
to_be
obe
be
bel
-----------------------
Position:
Characters:
s[0]
s[1O]
s [ ]
s[17]
15 be
2 _be_or_not_to_be
8 not to be
_ _ _
5 or not to be
_ _ _
12 to be
_ _
16 be
3 be_or_not_to_be
17
4 e_or_not_to_be
9 not_to_be
14 o_be
1 o_be_or_not_to_be
6 or_not_to_be
10 ot_to_be
7 r_not_to_be
11 t_to_be
13 to_be
0 to_be_or_not_to_be
</figure>
<figureCaption confidence="0.996397">
Figure 2
</figureCaption>
<bodyText confidence="0.9373395">
Illustration of the suffix array in Figure 1 after sorting. The integers in s are sorted so that the
semi-infinite strings are now in alphabetical order.
</bodyText>
<page confidence="0.970027">
5
</page>
<note confidence="0.431097">
Computational Linguistics Volume 27, Number 1
</note>
<bodyText confidence="0.999962296296296">
suffix to start with this term. Consequently, tf (&amp;quot;to_be&amp;quot;) = 17 — 16+1 = 2. Moreover, the
term appears at positions(&amp;quot;to_be&amp;quot;) = {s [16], s[17]} = {13, 0}, and only these positions.
Similarly, the substring &amp;quot;to&amp;quot; has the same tf and positions, as do the substrings, &amp;quot;to_&amp;quot;
and &amp;quot;to_b&amp;quot;. Although there may be N(N + 1)/2 ways to pick i and j, it will turn out
that we need only consider 2N — 1 of them when computing tf for all substrings.
Nagao and Mori (1994) ran this procedure quite successfully on a large corpus
of Japanese text. They report that it takes 0(N log N) time, assuming that the sort
step performs 0(N log N) comparisons, and that each comparison takes constant time.
While these are often reasonable assumptions, we have found that if the corpus con-
tains long repeated substrings (e.g., duplicated articles), as our English corpus does
(Paul and Baker 1992), then the sort can consume quadratic time, since each compar-
ison can take order N time. Like Nagao and Mori (1994), we were also able to apply
this procedure quite successfully to our Japanese corpus, but for the English corpus,
after 50 hours of CPU time, we gave up and turned to Manber and Myers&apos;s (1990) al-
gorithm, which took only two hours.&apos; Manber and Myers&apos; algorithm uses some clever,
but difficult to describe, techniques to achieve 0(N log N) time, even for a corpus with
long repeated substrings. For a corpus that would otherwise consume quadratic time,
the Manber and Myers algorithm is well worth the effort, but otherwise, the procedure
described above is simpler, and can even be a bit faster.
The &amp;quot;to_be_or_not_to_be&amp;quot; example used the standard English alphabet (one byte per
character). As mentioned above, suffix arrays can be generalized in a straightforward
way to work with larger alphabets such as Japanese (typically two bytes per character).
In the experimental section (Section 3), we use an open-ended set of English words as
the alphabet. Each token (English word) is represented as a four-byte pointer into a
symbol table (dictionary). The corpus &amp;quot;to_be_or_not_to_be&amp;quot;, for example, is tokenized
into six tokens: &amp;quot;to&amp;quot;, &amp;quot;be&amp;quot;, &amp;quot;or&amp;quot;, &amp;quot;not&amp;quot;, &amp;quot;to&amp;quot;, and &amp;quot;be&amp;quot;, where each token is represented
as a four-byte pointer into a dictionary.
</bodyText>
<subsectionHeader confidence="0.999959">
2.2 Longest Common Prefixes (LCPs)
</subsectionHeader>
<bodyText confidence="0.999979769230769">
Algorithms from the suffix array literature make use of an auxiliary array for storing
LCPs (longest common prefixes). The lcp array contains N + 1 integers. Each element,
lcp [i], indicates the length of the common prefix between s[i — 1] and s[i]. We pad the
lcp vector with zeros (lcp[0] = lcp[N] = 0) to simplify the discussion. The padding
avoids the need to test for certain end conditions.
Figure 3 shows the lcp vector for the suffix array of &amp;quot;to_be_or_not_to_be&amp;quot;. For
example, since s [10] and s [11] both start with the substring &amp;quot;o_be&amp;quot;, lcp [11] is set to 4, the
length of the longest common prefix. Manber and Myers (1990) use the lcp vector in
their 0(P+log N) algorithm for computing the frequency and location of a substring of
length P in a sequence of length N. They showed that the lcp vector can be computed
in 0(N log N) time. These algorithms are much faster than the obvious straightforward
implementation when the corpus contains long repeated substrings, though for many
corpora, the complications required to avoid quadratic behavior are unnecessary.
</bodyText>
<subsectionHeader confidence="0.999982">
2.3 Classes of Substrings
</subsectionHeader>
<bodyText confidence="0.999719333333333">
Thus far we have seen how to compute tf for a single n-gram, but how do we compute
tf and df for all n-grams? As mentioned above, the N(N + 1)/2 substrings will be
clustered into a relatively small number of classes, and then the statistics will be
</bodyText>
<footnote confidence="0.992062">
1 We used Doug McIlroy&apos;s implementation, available on the Web at: http://cm.bell-labs.com/cm/cs/
who/doug/ssort.c.
</footnote>
<page confidence="0.997683">
6
</page>
<bodyText confidence="0.8848635">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
computed over the classes rather than over the substrings, which would be prohibitive.
The reduction of the computation over substrings to a computation over classes is
made possible by four properties.
</bodyText>
<figureCaption confidence="0.9787746">
Properties 1-2: all substrings in a class have the same statistics (at least
for the statistics of interest, namely tf and df),
Property 3: the set of all substrings is partitioned by the classes, and
Property 4: there are many fewer classes (order N) than substrings
(order N2).
</figureCaption>
<bodyText confidence="0.9180168">
Classes are defined in terms of intervals. Let (1,]) be an interval on the suffix array,
s[i], s[i + 1], . . , s[j]. Class ((i, j)) is the set of substrings that start every suffix within the
interval and no suffix outside the interval. It follows from this construction that all
substrings in a class have tf = j - i + 1.
The set of substrings in a class can be constructed from the lcp vector:
</bodyText>
<equation confidence="0.889586">
class ((i, j)) = Is[i],n1 max(Icp[i], lcp[j + 1]) &lt; m &lt; min(lcp[i + 11, Icp[i + ..., Icp[j])},
</equation>
<bodyText confidence="0.977484">
where s[i],, denotes the first m characters of the suffix s {i]. We will refer to lcp [i] and
/cp[j+1] as bounding lcps and Icp[i+1], Icp[i+2], , lcp[j] as interior lcps. The equation
</bodyText>
<subsectionHeader confidence="0.717523">
Suffix Array; Suffix denoted by s [ii
</subsectionHeader>
<bodyText confidence="0.980541523809524">
be
_ ,
bei or not to be
not_to_be
_iornot_to_be
tobe
be
be or notbobe
q
eLor_not_to_be
no t_.t o_be
o bd jeng.th = 4
,—,
a_be_or not_tobe
Or not_t obe
Ot_tobe
rnot_tobe
ti_tobe
t;o_be
to_beLor not_tobe
The doted lines denote lcps.
</bodyText>
<figure confidence="0.934981551724138">
s [0]
S [1]
s [4]
s[5]
s [6]
s [7 ]
S [11]
s[12]
15
2
8
5
12
16
3
17
4
9
14
1
6
10
7
11
13
0
Lep vector
lcp[0] 0
lcp[1] 3
</figure>
<equation confidence="0.962959461538461">
lcp[2] 1
lcp[3] 1
lcp[4] 1
1cp [5] 0
lcp [6] 2
lcp[7] 0
lcp[8] 1
lcp[9] 0
lcp[10] 0
lcp[1 4
lcp [12] 1
lcp[13] 1
lcp [14] 0
</equation>
<footnote confidence="0.55925325">
lcp[15] 0
lcp[16] 1
1cp [17] 5
lcp[18] 0
always 0
always 0
Figure 3
The longest common prefix is a vector of N 1 integers. lcp [i] denotes the length of the
common prefix between the suffix s [i — 1] and the suffix s[i]. Thus, for example, s[10] and s[11]
share a common prefix of four characters, and therefore lcp [11] = 4. The common prefix is
highlighted by a dotted line in the suffix array. The suffix array is the same as in the previous
figure.
</footnote>
<page confidence="0.998079">
7
</page>
<figure confidence="0.898222260869565">
Computational Linguistics Volume 27, Number 1
Vertical lines denote lcps. Gray area denotes
endpoints of substrings in class(&lt;10,11&gt;).
• n o t _ to_be
.
o _ b e
o _ b e _or_not_to_be i
or r _ n ■t_to_be
,
t • •e
::rI c o 1 t. be
_ _
s [ 9 ]
s [ 1 0 ]
S [ 1 1 ]
s [ 1 2 ]
s [ 1 3 ]
s [ 1 4 ]
Boundaries
of &lt;10,11&gt;
Bounding lcps, LBL, SIL, Interior lcp of &lt;10,11&gt;
LCP-delimited Class LBL SIL tf
interval
</figure>
<table confidence="0.966994">
&lt;10,11&gt; ro_&amp;quot;, &amp;quot;o_b&amp;quot;, &amp;quot;o_bel 1 4 2
&lt;10,13&gt; ro&amp;quot;) 0 1 4
&lt;9,9&gt; rn&amp;quot;, &amp;quot;no&amp;quot;, &amp;quot;not&amp;quot;, ...I 0 infinity 1
&lt;10,10&gt; II 4 infinity 1
&lt;11,11&gt; ro_be_&amp;quot;, &amp;quot;o_be_o&amp;quot;, ...I 4 infinity 1
&lt;12,12&gt; (&amp;quot;or&amp;quot;, &amp;quot;or_&amp;quot;, &amp;quot;or_n&amp;quot;, ...} 1 infinity 1
&lt;13,13&gt; rot&amp;quot;, &amp;quot;ot_&amp;quot;, &amp;quot;ot_t&amp;quot;, ...I 1 infinity 1
&lt;14,14&gt; {&amp;quot;r&amp;quot;, &amp;quot;r_&amp;quot;, &amp;quot;r_n&amp;quot;, ...} 0 infinity 1
</table>
<figureCaption confidence="0.932567">
Figure 4
</figureCaption>
<bodyText confidence="0.922587666666667">
Six suffixes are copied from Figure 3, s[9]—s[14], along with eight of their lcp-delimited
intervals. Two of the lcp-delimited intervals are nontrivial (t f&gt; 1), and six are trivial (tf = 1).
Intervals are associated with classes, sets of substrings. These substrings start every suffix
within the interval and no suffix outside the interval. All of the substrings within a class have
the same term frequency (and document frequency).
above can be rewritten as
</bodyText>
<equation confidence="0.990394">
class((i,j)) = {s[i]mILBL(M) &lt; m MT,
</equation>
<bodyText confidence="0.97397">
where LBL (longest bounding lcp) is
</bodyText>
<equation confidence="0.529768">
LBL((i, j)) = max(lcp[i], Icp[j + 1] ),
</equation>
<bodyText confidence="0.903307">
and SIL (shortest interior lcp) is
</bodyText>
<equation confidence="0.755606">
S IL((i, j)) -= min(lcp[i + 1], lcp[i + 21, ... , lcp[j]).
</equation>
<bodyText confidence="0.993856125">
By construction, the class will be empty unless there is some room for m between
the LBL and SIL. We say that an interval is lcp-delimited when this room exists
(that is, LBL &lt; SIL). Except for trivial intervals where tf = 1 (see below), classes are
nonempty iff the interval is lcp-delimited. Moreover, the number of substrings in a
nontrivial class depends on the amount of room between the LBL and the SIL. That
is, I class ((i, j))I = SIL((i, j)) — LBL((i, j)).
Figure 4 shows eight examples of lcp-delimited intervals. The top part of the figure
highlights the interval (10,11) with dashed horizontal lines. Solid vertical lines denote
</bodyText>
<page confidence="0.926112">
8
</page>
<bodyText confidence="0.966057568627451">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
bounding lcps, and thin vertical lines denote interior lcps (there is only one interior lcp
in this case). The interval (10, 11) is lcp-delimited because the bounding lcps, lcp[10] = 0
and lcp[12] = 1, are smaller than the interior lcp, lcp[11] = 4. That is, the LBL 1) is
less than the SIL (=- 4). Thus there is room for m between the LBL of s[10],, and the SIL
of 5[10]m. The endpoints m between LBL and SIL are highlighted in gray. The class is
nonempty. Its size depends on the width of the gray area: class ((10,11)) = {s[10]„,11 &lt;
m &lt; 4} = {&amp;quot;o_&amp;quot;, &amp;quot;o_b&amp;quot;, &amp;quot;o_be&amp;quot;}. These substrings have the same tf : tf -= j — i + 1 -=
11 — 10 + 1 = 2. Each of these substrings occurs exactly twice in the corpus.
Every substring in the class starts every suffix in the interval (10, 11), and no suffix
outside (10, 11). In particular, the substring &amp;quot;o&amp;quot; is excluded from the class, because it
is shared by suffixes outside the interval, namely s [12] and s[13]. The longer substring,
&amp;quot;o_be_&amp;quot;, is excluded from the class because it is not shared by s[10], a suffix within
the interval.
We call an interval trivial if the interval starts and ends at the same place: (i, i).
The remaining six intervals mentioned in Figure 4 are trivial intervals. We call the
class of a trivial interval a trivial class. As in the nontrivial case, the class contains
all (and only) the substrings that start every suffix within the interval and no suf-
fix outside the interval. We can express the class of a trivial interval, class ((i, i)), as
{s[i]„,ILBL &lt; m &lt; SIL} . The trivial case is the same as the nontrivial case, except that
the SIL of a trivial interval is defined to be infinite. As a result, trivial classes are
usually quite large, because they contain all prefixes of s [i] that are longer than the
LBL. They cover all (and only) the substrings with tf = 1, typically the bulk of the
N(N + 1)/2 substrings in a corpus. The trivial class of the interval (11, 11), for example,
contains 13 substrings: &amp;quot;o_be_&amp;quot;, &amp;quot;o_be_o&amp;quot;, &amp;quot;o_be_or &amp;quot;, and so on. Of course, there are
some exceptions: the trivial class, class ((10 , 10)), in Figure 4, for example, is very small
(= empty set).
Not every interval is lcp-delimited. The interval, (11, 12), for example, is not lcp-
delimited because there is no room for m of s[11],, between the LBL (= 4) and the
SIL (= 1). When the interval is not lcp-delimited, the class is empty. There are no
substrings starting all the suffixes within the interval (11, 12), and not starting any
suffix outside the interval.
It is possible for lcp-delimited intervals to be nested, as in the case of (10, 11) and
(10, 13). We say that one interval (i, j) is nested within another (u, v)ifi&lt;u&lt;v&lt;j
(and (i,j) (u, v)). Nested intervals have distinct SILs and disjoint classes. (Two classes
are disjoint if the corresponding sets of substrings are disjoint.)2 The substrings in the
class of the nested interval, (u, v), are longer than the substrings in the class of the
outer interval, (i, j).
Although it is possible for lcp-delimited intervals to be nested, it is not possible for
lcp-delimited intervals to overlap. We say that one nontrivial interval (a, b) overlaps
another nontrivial interval (c, d) if a &lt; c &lt; b &lt; d. If two intervals overlap, then at
least one of the intervals is not lcp-delimited and has an empty class. If an interval
(a, b) is lcp-delimited, an overlapped interval (c, d) is not lcp-delimited. Because a
bounding lcp of (a, b) must be within (c, d) and an interior lcp of (a, b) must be a
bounding lcp of (c, d), SIL((c, d)) &lt; LBL((a,b)) &lt; SIL((a,b)) &lt; LBL((c, d)). That is, the
overlapped interval (c, d) is not lcp-delimited. The fact that lcp-delimited intervals are
nested and do not overlap will turn out to be convenient for enumerating lcp-delimited
intervals.
2 Because (u, v) is lcp-delimited, there must be a bounding lcp of (u, v) that is smaller than any lcp
within (u, v). This bounding lcp must be within (i,j), and as a result, class ((i,j)) and class((u,v))
must be disjoint.
</bodyText>
<page confidence="0.972647">
9
</page>
<note confidence="0.431305">
Computational Linguistics Volume 27, Number 1
</note>
<subsectionHeader confidence="0.992994">
2.4 Four Properties
</subsectionHeader>
<bodyText confidence="0.999007714285714">
As mentioned above, classes are constructed so that it is practical to reduce the com-
putation of various statistics over substrings to a computation over classes. This sub-
section will discuss four properties of classes that help make this reduction feasible.
The first two properties are convenient because they allow us to associate tf and
df with classes rather than with substrings. The substrings in a class all have the same
tf value (property 1) and the same df value (property 2). That is, if si and s2 are two
substrings in class((i, j)) then
</bodyText>
<figureCaption confidence="0.480333">
Property 1: tf (si) = tf (s2) = j i + 1
Property 2: df (si) = df (s2)
</figureCaption>
<bodyText confidence="0.999256285714286">
Both of these properties follow straightforwardly from the construction of intervals.
The value of tf is a simple function of the endpoints; the calculation of df is more
complicated and will be discussed in Section 2.6. While tf and df treat each member
of a class as equivalent, not all statistics do. Mutual information (MI) is an important
counter example; in most cases, M/(si) r MI(s2)•
The third property is convenient because it allows us to iterate over classes rather
than substrings, without worrying about missing any of the substrings.
</bodyText>
<figureCaption confidence="0.473434">
Property 3: The classes partition the set of all substrings.
</figureCaption>
<bodyText confidence="0.991813296296296">
There are two parts to this argument: every substring belongs to at most one class
(property 3a), and every substring belongs to at least one class (property 3b).
Demonstration of property 3a (proof by contradiction): Suppose there is a sub-
string, s, that is a member of two distinct classes: class ((i, j)) and class ((u, v)). There
are three possibilities: one interval precedes the other, they are properly nested, or
they overlap. In all three cases, s cannot be a member of both classes. If one interval
precedes the other, then there must be a bounding lcp between the two intervals which
is shorter than s. And therefore, s cannot be in both classes. The nesting case was men-
tioned previously where it was noted that nested intervals have disjoint classes. The
overlapping case was also discussed previously where it was noted that two overlap-
ping intervals cannot both be lcp-delimited, and therefore at least one of the classes
would have to be empty.
Demonstration of property 3b (constructive argument): Let s be an arbitrary sub-
string in the corpus. There will be at least one suffix in the suffix array that starts with
s. Let i be the first such suffix and let j be the last such suffix. By construction, the
interval (i,j) is lcp-delimited (LBL((i, j)) &lt; Is and SIL((i, j)) &gt; Is1), and therefore, s is
an element of class((i, j)).
Finally, as mentioned above, computing over classes is much more efficient than
computing over the substrings themselves because there are many fewer classes (at
most 2N — 1) than substrings (N(N + 1)/2).
Property 4: There are at most N nonempty classes with tf = 1 and at
most N — 1 nonempty classes with tf &gt; 1.
The first clause is relatively straightforward. There are N trivial intervals (i, i).
These are all and only the intervals with tf = 1. By construction, these intervals are
lcp-delimited, though it is possible that a few of the classes could be empty.
To argue the second clause, we make use of a uniqueness property: an lcp-
delimited interval (i,j) can be uniquely determined by an SIL and a representative ele-
</bodyText>
<page confidence="0.962696">
10
</page>
<bodyText confidence="0.988302384615385">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
ment k, where i &lt;k &lt;j. For convenience, we will choose k such that SIL( (i,j)) = lcp [k],
but we could have uniquely determined the lcp-delimited interval by choosing any k
such that i &lt;k &lt;j.
The uniqueness property can be demonstrated using a proof by contradiction.
Suppose there were two distinct lcp-delimited intervals, (i,j) and (u, v), with the same
representative k, where i &lt; k &lt;j and u &lt; k &lt; v. Since they share a common repre-
sentative, k, one interval must be nested inside the other. But nested intervals have
disjoint classes and different SILs.
Given this uniqueness property, we can determine the N — 1 upper bound on
the number of lcp-delimited intervals by considering the N — 1 elements in the lcp
vector. Each of these elements, lcp [k], has the opportunity to become the SIL of an
lcp-delimited interval (i,j) with a representative k. Thus there could be as many as
N-1 lcp-delimited intervals (though there could be fewer if some of the opportunities
don&apos;t work out). Moreover, there cannot be any more intervals with tf &gt; 1, because if
there were one, its SIL should have been in the lcp vector. (Note that this lcp counting
argument does not count trivial intervals because their SILs [= infinity] are not in the
lcp vector; the lcp vector contains integers less than N.)
From property 4, it follows that there are at most N distinct values of RIDF. The
N trivial intervals (i, i) have just one RIDF value since tf = df = 1 for these intervals.
The other N — 1 intervals could have as many as another N — 1 RIDF values. Similar
arguments hold for many other statistics that make use of tf and df, and treat all
members of a class as equivalent.
In summary, the four properties taken collectively make it practical to compute
tf, df, and RIDF over a relatively small number of classes; it would be prohibitively
expensive to compute these quantities directly over the N(N + 1)/2 substrings.
</bodyText>
<subsectionHeader confidence="0.99971">
2.5 Computing All Classes Using Suffix Arrays
</subsectionHeader>
<bodyText confidence="0.999924391304348">
This subsection describes a single-pass procedure, print_LDIs, for computing tf for
all LDIs (lcp-delimited intervals). Since lcp-delimited intervals are properly nested, the
procedure is based on a push-down stack. The procedure outputs four quantities for
each lcp-delimited interval, (i,j). The four quantities are the two endpoints (i and j), the
term frequency (tf) and a representative (k), such that i &lt;k &lt; j and lcp [k] = SIL((i, j)).
This procedure will be described twice. The first implementation is expressed in a
recursive form; the second implementation avoids recursion by implementing the stack
explicitly.
The recursive implementation is presented first, because it is simpler. The function
print_LDIs is initially called with print_LDIs (0 , 0), which will cause the function to
be called once for each value of k between 0 and N — 1. k is a representative in the
range: i &lt; k &lt; j, where i and j are the endpoints of an interval. For each of the
N values of k, a trivial LDI is reported at (k, k. In addition, there could be up to
N — 1 nontrivial intervals, where k is the representative and lep[k] is the SIL. Recall
that lcp-delimited intervals are uniquely determined by a representative k such that
I &lt; k &lt; j where SM((i,j)) /cp[k]. Not all of these candidates will produce LDIs.
The recursion searches for j&apos;s such that LBL((i,j)) &lt; SIL((i,j)), but reports intervals at
(i,j) only when the inequality is a strict inequality, that is, LBL((i,j)) &lt; SIL((i,j)). The
program stack keeps track of the left and right edges of these intervals. While Icp[k]
is monotonically increasing, the left edge is remembered on the stack, as print_LDIs
is called recursively. The recursion unwinds as lcp U] &lt; Icp[ki. Figure 5 illustrates the
function calls for computing the nontrivial lcp-delimited intervals in Figure 4. C code
is provided in Appendix A.
</bodyText>
<page confidence="0.983127">
11
</page>
<figure confidence="0.99382837037037">
Computational Linguistics Volume 27, Number 1
•
•
•
.,print_LDIs(i=9,k=10)
[
...------- rint_LDIs(i=10,k=11)
print &amp;quot;&lt;10,11&gt;, rep=11, tf=2&amp;quot;
return(j=11)
print_LDIs(i=10,k=12)
• -print_LDIs(i=12,k=13)
print nothing
(&lt;12,13&gt; isn&apos;t lcp-delimited)
return(j=13)
print &amp;quot;&lt;10,13&gt;, rep=13, tf=4&amp;quot;
return(j=13)
s[91
s[10]
S[ll]
n
...-
0 be ,- - -&amp;quot;-
o _ be
e..._....-:-,
o - et- 1 t ...
r _ n o . . .
Vertical lines denote lcps.
</figure>
<figureCaption confidence="0.973984">
Figure 5
</figureCaption>
<bodyText confidence="0.7995990625">
Trace of function calls for computing the nontrivial lcp-delimited intervals in Figure 4. In this
trace, trivial intervals are omitted. Print_LDIs(i = x,k = y) represents a function call with
arguments, i and k. Indentation represents the nest of recursive calls. Print_LDIs(i,k) searches
the right edge, j, of the non-trivial lcp-delimited interval, (i,j), whose SIL is Icp[k]. Each
representative, k, value is given to the function print_LDIs just once (dotted arcs).
print_LDIs function(i, k) {
Output a trivial lcp-delimited interval (k, k) with tf = 1.
While lcp[k] &lt; lcp[j + 1] and j +1 &lt;N, do] 4- print_LDIs(k, j + 1).
Output an interval (i,j) with tf =] — i + 1 and rep = k, if it is lcp-delimited.
Return]. }
The second implementation (below) introduces its own explicit stack, a complica-
tion that turns out to be important in practice, especially for large corpora. C code is
provided in Appendix B.
pr int _LDI s_st ack function(N){
stacki 4— an integer array for the stack of the left edges, i.
stack_k &lt;— an integer array for the stack of the representatives, k.
</bodyText>
<equation confidence="0.6944215">
stack_i[0] k— 0.
stack_k[0] •— 0.
</equation>
<bodyText confidence="0.607231">
sp &lt;— 1 (a stack pointer).
For j 0,1,2, ...,N — 1 do
Output an lcp-delimited interval (f, j) with tf = 1.
</bodyText>
<equation confidence="0.241881333333333">
While lcp[j +1] &lt; lcp[stack_k[sp — 1]] do
Output an interval (i,j) with tf =] — i + 1, if it is lcp-delimited.
sp •— sp —1.
stack_i[sp] stack_k[sp — 1].
stack _k[sp] ± 1.
sp &lt;— sp +1. }
</equation>
<page confidence="0.783211">
12
</page>
<table confidence="0.993644739130435">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
Suffix Array Suffixes denoted Document id&apos;s
by s [i] lcp[i] of s[i]
s [0] 2 _be$ 0 0
s[1] 15 _be$ 4 2
s[2] 12 _to_be$ 1 2
s[3] 5 $ 0 0
s [4] 8 $ 1 1
s[5] 18 $ 1 2
s[6] 3 be $ 0 0
s [7] 16 be$ 3 2
s [8] 4 e$ 0 0
s[9] 17 e$ 2 2
s [10] 9 not_to_be$ 0 2
S [11] 1 o_be$ 0 0
s [12] 14 o_be$ 5 2
s[13] 6 or$ 1 1
s [14] 10 ot_to_be$ 1 2
s [15] 7 r$ 0 1
s [16] 11 t_to_be$ 0 2
s[17] 0 to_be$ 1 0
s [18] 13 to_be$ 6 2
0
</table>
<equation confidence="0.927186933333333">
Input documents: dO = &amp;quot;to_be$&amp;quot;
d1 =
d2 = &amp;quot;not_to_be$&amp;quot;
Corpus = dO + d1 + d2 = &amp;quot;to_be$or$not_to_be$&amp;quot;
Resulting non-trivial lcp-delimited intervals:
(&apos;rep&apos; means a representative, k.)
(0, 1), rep= 1, tf=2, df=2
(0,2), rep= 2, tf=3, df=2
(3,5), rep= 4, tf=3, df=3
(6,7), rep= 7, tf=2, df=2
(8,9), rep= 9, tf=2, df=2
(11, 12), rep=12, tf=2, df=2
(11, 14), rep=13, tf=4, df=3
(17, 18), rep=18, tf=2, df=2
(16, 18), rep=17, tf=3, df=2
</equation>
<figureCaption confidence="0.594272">
Figure 6
</figureCaption>
<bodyText confidence="0.943999666666667">
A suffix array for a corpus consisting of three documents. The special character $ denotes the
end of a document. The procedure outputs a sequence of intervals with their term frequencies
and document frequencies. These results are also presented for the nontrivial intervals.
</bodyText>
<subsectionHeader confidence="0.999558">
2.6 Computing df for All Classes
</subsectionHeader>
<bodyText confidence="0.999762583333333">
Thus far we have seen how to compute term frequency, tf, for all substrings (n-grams)
in a sequence (corpus). This section will extend the solution to compute document
frequency, df, as well as term frequency. The solution runs in 0(N log N) time and
0(N) space. C code is provided in Appendix C.
This section will use the rurming example shown in Figure 6, where the corpus is:
&amp;quot;to_be$or$not_to_be$&amp;quot;. The corpus consists of three documents, &amp;quot;to_be$&amp;quot;, &amp;quot;orr, and
&amp;quot;not_to_be$&amp;quot;. The special character $ is used to denote the end of a document. The
procedure outputs a sequence of intervals with their term frequencies and document
frequencies. These results are also presented for the nontrivial intervals.
The suffix array is computed using the same procedures discussed above. In ad-
dition to the suffix array and the lcp vector, Figure 6 introduces a new third table
that is used to map from suffixes to document ids. This table of document ids will be
</bodyText>
<page confidence="0.994014">
13
</page>
<note confidence="0.634837">
Computational Linguistics Volume 27, Number 1
</note>
<bodyText confidence="0.988438344827586">
used by the function get_docnum to map from suffixes to document ids. Suffixes are
terminated in Figure 6 after the first end of document symbol, unlike before, where
suffixes were terminated with the end of corpus symbol.
A straightforward method for computing df for an interval is to enumerate the
suffixes within the interval and then compute their document ids, remove duplicates,
and return the number of distinct documents. Thus, for example, df(&amp;quot;o&amp;quot;) in Figure 6,
can be computed by finding corresponding interval, (11, 14), where every suffix within
the interval starts with &amp;quot;o&amp;quot; and no suffix outside the interval starts with &amp;quot;o&amp;quot;. Then
we enumerate the suffixes within the interval Is [11], s [12], s [13], s[14]}, compute their
document ids, {0, 2, 1, 2}, and remove duplicates. In the end we discover that df (&amp;quot;o&amp;quot;) =
3. That is, &amp;quot;o&amp;quot; appears in all three documents.
Unfortunately, this straightforward approach is almost certainly too slow. Some
document ids will be computed multiple times, especially when suffixes appear in
nested intervals. We take advantage of the nesting property of lcp-delimited intervals
to compute all df&apos;s efficiently. The df of an lcp-delimited interval can be computed
recursively in terms of its constituents (nested subintervals), thus avoiding unnecessary
recomputation.
The procedure print_LDIs_with_df presented below is similar to print_LDIs_-
stack but modified to compute df as well as tf. The stack keeps track of i and k, as
before, but now the stack also keeps track of df.
i, the left edge of an interval,
k, the representative (SIL = lcp[k]),
df, partial results for df, counting documents seen thus far, minus duplicates.
pr int _LDIs_with_df function(N){
stack_i 4— an integer array for the stack of the left edges, i.
stackic 4— an integer array for the stack of the representatives, k.
stack_flf an integer array for the stack of the df counter.
doclink[0..D]l : an integer array for the document link initialized with —1.
D = the number of documents.
</bodyText>
<equation confidence="0.89461425">
stack _40] &lt;— 0.
stack _k[0] &lt;— 0.
stack _df [0] &lt;— 1.
sp 4- 1 (a stack pointer).
</equation>
<listItem confidence="0.975808733333334">
(1) For j 0, 1, 2,... , N — 1 do
(2) (Output a trivial lcp-delimited interval (j,j) with tf = 1 and df -= 1.)
(3) doc &lt;— get_docnum(s[j])
(4) if doclink[doc] —1, do
(5) let x be the largest x such that doclink[doc] &gt; stack _i[x].
(6) stack _df [x] 4- stack _df [x] — 1.
(7) doclink[doc] &lt;— j .
(8) df 1.
(9) While lcp[j + 1] &lt; Icp[stack _Ic[sp — 1]] do
(10) df 4— stack _df [sp —1] + df.
(11) Output a nontrivial interval (i,j) with tf = j — i + 1 and df,
if it is lcp-delimited.
(12) sp &lt;— sp — 1.
(13) stack _i[sp] 4- stack _k[sp — 1].
(14) stack _k[sp] 4—] + 1.
</listItem>
<page confidence="0.979764">
14
</page>
<bodyText confidence="0.655132">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
</bodyText>
<listItem confidence="0.985738">
(15) stack _df[sp] &lt;— df.
(16) sp +— sp +1. }
</listItem>
<bodyText confidence="0.999924258064516">
Lines 5 and 6 take care of duplicate documents. The duplication processing makes
use of doclink (an array of length D, the number of documents in the collection), which
keeps track of which suffixes have been seen in which document. doclink is initialized
with —1 indicating that no suffixes have been seen yet. As suffixes are processed, doclink
is updated (on line 7) so that doclink [d] contains the most recently processed suffix in
document d. As illustrated in Figure 7, when j = 16 (snapshot A), the most recently
processed suffix in document 0 is 411] (&amp;quot;o_be$&amp;quot;), the most recently processed suffix
in document 1 is 415] (&amp;quot;IV), and the most recently processed suffix in document 2
is 4161 (&amp;quot;t_to_be$&amp;quot;). Thus, doclin1401= 11, doclin1411= 15, and doclin1421= 16. After
processing 417] (&amp;quot;to_be$&amp;quot;), which is in document 0, doclink[0] is updated from 11 to
17, as shown in snapshot B of Figure 7.
Stack _df keeps track of document frequencies as suffixes are processed. The invari-
ant is: stack _df [x] contains the document frequency for suffixes seen thus far starting
at i = stack_i[x]. (x is a stack offset.) When a new suffix is processed, line 5 checks for
double counting by searching for intervals on the stack (still being processed) that have
suffixes in the same document as the current suffix. If there is any double counting,
stack_df is decremented appropriately on line 6.
There is an example of this decrementing in snapshot C of Figure 7, highlighted
by the circle around the binding of df to 0 on the stack element: [i = 0, k = 17,
df = 0]. Note that df was previously bound to 1 in snapshot B. The binding of df was
decremented when processing 418] because 418] is in the same document as 416].
This duplication was identified by line 5. The decrementing was performed by line 6.
Intervals are processed in depth-first order, so that more deeply nested intervals
are processed before less deeply nested intervals. In this way, double counting is only
an issue for intervals higher on the stack. The most deeply nested intervals are trivial
intervals. They are processed first. They have a df of 1 (line 8). For the remaining
nontrivial intervals, stack _df contains the partial results for intervals in process. As the
stack is popped, the df values are aggregated up to compute the df value for the outer
intervals. The aggregation occurs on line 10 and the popping of the stack occurs on line
12. The aggregation step is illustrated in snapshots C and D of Figure 7 by the two ar-
rows with the &amp;quot;+&amp;quot; combination symbol pointing at a value of df in an output statement.
</bodyText>
<subsectionHeader confidence="0.80402">
2.7 Class Arrays
</subsectionHeader>
<bodyText confidence="0.999963">
The classes identified by the previous calculation are stored in a data structure we call
a class array, to make it relatively easy to look up the term frequency and document
frequency for an arbitrary substring. The class array is a stored list of five-tuples: (SIL,
LBL, tf, df, longest suffix). The fifth element of the five-tuple is a canonical member of
the class (the longest suffix). The five-tuples are sorted by the alphabetical order of
the canonical members. In our C code implementation, classes are represented by five
integers, one for each element in the five-tuple. Since there are N trivial classes and
at most N — 1 nontrivial classes, the class array will require at most 10N — 5 integers.
However, for many practical applications, the trivial classes can be omitted.
Figure 8 shows an example of the nontrivial class array for the corpus: &amp;quot;to_be$or$
not_to_ber. The class array makes it relatively easy to determine that the substring
&amp;quot;o&amp;quot; appears in all three documents. That is, df(&amp;quot;o&amp;quot;) = 3. We use a binary search to
find that tuple c[5] is the relevant five-tuple for &amp;quot;o&amp;quot;. Having found the relevant tuple,
it requires a simple record access to return the document frequency field.
</bodyText>
<page confidence="0.930006">
15
</page>
<figure confidence="0.720878666666667">
Volume 27, Number 1
Computational Linguistics
Local variables Stack
</figure>
<equation confidence="0.877505285714286">
j = 16
df = 1
*=-15, k=16, df=1
Doclink
doclink[0]=11
doclink[1]=15
docl in k[2]=16
lines 13-16: push(i=16, k=17, df=1)
line 1: j j+1
doclink[0]=11
doclink[1]=15
doclink[2]=16
j=17df = 1
i=16, k=17, df=1
i=15, k=16, df=1
Icp[k=17] Icp[j+1=18] lines 3-6: x
1 6 line 7: doclink[doc(j=17)=0] 4—j=17
lines 13-16: push(i=17, k=18, df=1)
(doclink[0]=17) j = 17 df = 1 i=17, k=18, df=1
doclink[1]=15
doclink[2]=16
</equation>
<figureCaption confidence="0.7329495">
lines 1-6: x
line 7:
lines 8-11: output LDI=&lt;17,
line 12: pop()
</figureCaption>
<equation confidence="0.69499025">
Icp[k=1 7] &gt; Icp[j+1 =1 8]
5 0
tf=2, df=2
-\,1=16, k=17, df=0
= , =16, df=1
doclink[0]=17
doclink[1]=15
(doclink[2]=18)
= 18 (df = 2)
(C)
Icp[k=16] &gt; Icp[j+1=18]
1 0
doclink[0]=17
doclink[1]=15
doclink[2]=18
lines 9-11: output LDI=&lt;16,18&gt;, tf=3, df=2
line 12: pop()
i=15, k=16, df=1
(D)
j = 18 df = 2
</equation>
<figureCaption confidence="0.99177">
Figure 7
</figureCaption>
<bodyText confidence="0.9194255">
Snapshots of the doclink array and the stack during the processing of print_LDIs_with_df on
the corpus: &amp;quot;to_be$or$not_to_be$&amp;quot;. The four snapshots A-D illustrate the state as j progresses
from 16 to 18. Two nontrivial intervals are emitted while j is in this range: (17,18) and (16,18).
The more deeply nested interval is emitted before the less deeply nested interval.
</bodyText>
<page confidence="0.982545">
16
</page>
<figure confidence="0.817893153846154">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
Class array The longest suffix
(Pointer to corpus) denoted by c[i] SIL LBL tf df
c[O] 0
c[1] 0
c[2] 5
c[3] 3
c [4] 4
c[5] 1
c [6] 1
11
0
1 0 3 2
</figure>
<bodyText confidence="0.892437375">
_be$ 4 1 2 2
$ 1 0 3 3
be$ 3 0 2 2
e$ 2 0 2 2
o 1 0 4 3
o_be$ 5 1 2 2
t 1 0 3 2
to_be$ 6 1 2 2
</bodyText>
<figureCaption confidence="0.762104">
Figure 8
</figureCaption>
<bodyText confidence="0.890131">
An example of the class array for the corpus: &amp;quot;to_be$or$not_to_be$&amp;quot;.
</bodyText>
<sectionHeader confidence="0.985565" genericHeader="introduction">
3. Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999849">
3.1 RIDE and MI for English and Japanese
</subsectionHeader>
<bodyText confidence="0.999652">
We used the methods described above to compute df, tf, and RIDF for all substrings in
two corpora of newspapers summarized in Table 1. MI was computed for the longest
substring in each class. The entire computation took a few hours on a MIPS10000 with
16 Gbytes of main memory. The processing time was dominated by the calculation of
the suffix array.
The English collection consists of 50 million words (113 thousand articles) of the
Wall Street Journal (distributed by the ACL/DCI) and the Japanese collection consists of
216 million characters (436 thousand articles) of the CD-Mainichi Shimbun from 1991-
1995 (which are distributed in CD-ROM format). The English corpus was tokenized
into words delimited by white space, whereas the Japanese corpus was tokenized into
characters (typically two bytes each).
Table 1 indicates that there are a large number of nontrivial classes in both corpora.
The English corpus has more substrings per nontrivial class than the Japanese corpus.
It has been noted elsewhere that the English corpus contains quite a few duplicated
articles (Paul and Baker 1992). The duplicated articles could explain why there are so
many substrings per nontrivial class in the English corpus when compared with the
Japanese corpus.
</bodyText>
<tableCaption confidence="0.9336275">
Table 1
Statistics of the English and Japanese corpora.
</tableCaption>
<table confidence="0.982222125">
Statistic Wall Street Journal Mainichi Shimbun
N (corpus size in tokens) 49,810,697 words 215,789,699 characters
V (vocabulary in types) 410,957 5,509
# articles 112,915 435,859
# nontrivial classes 16,519,064 82,442,441
# substrings in nontrivial classes 2,548,140,677 1,388,049,631
substrings per class (in nontrivial 154.3 16.8
classes)
</table>
<page confidence="0.988437">
17
</page>
<figure confidence="0.999186933333334">
Computational Linguistics Volume 27, Number 1
25
20
15
10
5
0
II •
•511••••••
in as&apos;
10 15 20 25 3.0 35 40
•I
.• Length
-5 Length
-10
</figure>
<figureCaption confidence="0.998602">
Figure 9
</figureCaption>
<bodyText confidence="0.97914075">
The left panel plots MI as a function of the length of the n-gram; the right panel plots RIDF as
a function of the length of the n-gram. Both panels were computed from the Japanese corpus.
Note that while there is more dynamic range for shorter n-grams than for longer n-grams,
there is plenty of dynamic range for n-grams well beyond bigrams and trigrams.
For subsequent processing, we excluded substrings with tf &lt; 10 to avoid noise,
resulting in about 1.4 million classes (1.6 million substrings) for English and 10 million
classes (15 million substrings) for Japanese. We computed RIDF and MI values for the
longest substring in each of these 1.4 million English classes and 10 million Japanese
classes. These values can be applied to the other substrings in these classes for RIDF,
but not for MI. (As mentioned above, two substrings in the same class need not have
the same MI value.)
MI of each longest substring, t, is computed by the following formula.
</bodyText>
<equation confidence="0.991638888888889">
lo P(1 Yz)
Mi(t = xYz) = g
p(xY)p(zIY)
(xYz)
log tf(xy)`/0( yz)
N tf (Y)
tf (xYz)tf (Y)
log
tf (xY)tf (Yz)&apos;
</equation>
<bodyText confidence="0.999252">
where x and z are tokens, and Y and xYz are n-grams (sequences of tokens). When Y
is the empty string, tf (Y) = N.
Figure 9 plots RIDF and MI values of 5,000 substrings randomly selected as a func-
tion of string length. In both cases, shorter substrings have more dynamic range. That
is, RIDF and MI vary more for bigrams than million-grams. But there is considerable
dynamic range for n-grams well beyond bigrams and trigrams.
</bodyText>
<subsectionHeader confidence="0.999928">
3.2 Little Correlation between RIDF and MI
</subsectionHeader>
<bodyText confidence="0.999962142857143">
We are interested in comparing and contrasting RIDF and MI. Figure 10 shows that
RIDF and MI are largely independent. There is little if any correlation between the
RIDF of a string and the MI of the same string. Panel (a) compares RIDF and MI
for a sample of English word sequences from the WSJ corpus (excluding unigrams);
panel (b) makes the same comparison but for Japanese phrases identified as keywords
on the CD-ROM. In both cases, there are many substrings with a large RIDF value
and a small MI, and vice versa.
</bodyText>
<page confidence="0.993248">
18
</page>
<figure confidence="0.980338">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
0 MI 10 20
(a) English strings (b) Japanese strings
(word sequences) (character sequences)
</figure>
<figureCaption confidence="0.982431">
Figure 10
</figureCaption>
<bodyText confidence="0.998743076923077">
Both panels plot RIDF versus MI. Panel (a) plots RIDF and MI for a sample of English
n-grams; panel (b) plots RIDF and MI for Japanese phrases identified as keywords on the
CD-ROM. The right panel highlights the 10% highest RIDF and 10% lowest MI with a box, as
well as the 10% lowest RIDF and the 10% highest MI. Arrows point to the boxes for clarity.
We believe the two statistics are both useful but in different ways. Both pick out
interesting n-grams, but n-grams with large MI are interesting in different ways from
n-grams with large RIDF. Consider the English word sequences in Table 2, which all
contain the word having. These sequences have large MI values and small RIDF values.
In our collaboration with lexicographers, especially those working on dictionaries for
learners, we have found considerable interest in statistics such as MI that pick out
these kinds of phrases. Collocations can be quite challenging for nonnative speakers
of the language. On the other hand, these kinds of phrases are not very good keywords
for information retrieval.
</bodyText>
<tableCaption confidence="0.848503">
Table 2
</tableCaption>
<bodyText confidence="0.960984">
English word sequences containing the word having. Note that these phrases have large MI
and low RIDF. They tend to be more interesting for lexicography than information retrieval.
The table is sorted by MI.
</bodyText>
<page confidence="0.5048">
2
</page>
<figure confidence="0.986455888888889">
-.
U-
0
cc
-
Table7
o&apos; . io .
M I 20
-10
</figure>
<table confidence="0.970035583333334">
tf df RIDF MI
18 18 —0.0 10.5
14 14 —0.0 9.7
25 23 0.1 8.9
20 20 —0.0 7.4
301 293 0.0 7.3
15 13 0.2 7.0
59 59 —0.0 6.8
18 18 —0.0 6.0
12 12 —0.0 5.9
26 26 —0.0 5.8
Phrase
</table>
<footnote confidence="0.9977582">
admits to having
admit to having
diagnosed as having
suspected of having
without having
denies having
avoid having
without ever having
Besides having
denied having
</footnote>
<page confidence="0.995842">
19
</page>
<note confidence="0.602113">
Computational Linguistics Volume 27, Number 1
</note>
<tableCaption confidence="0.980378">
Table 3
</tableCaption>
<bodyText confidence="0.675608666666667">
English word sequences containing the word Mr. (sorted by RIDF). The word sequences near
the top of the list are better keywords than the sequences near the bottom of the list. None of
them are of much interest to lexicography.
</bodyText>
<table confidence="0.999238583333334">
tf df RIDF MI Phrase
11 3 1.9 0.6 . Mr. Hinz
18 5 1.8 6.6 Mr. Bradbury
51 16 1.7 6.7 Mr. Roemer
67 25 1.4 6.8 Mr. Melamed
54 27 1.0 5.8 Mr. Burnett
11 8 0.5 1.1 Mr. Eiszner said
53 40 0.4 0.3 Mr. Johnson.
21 16 0.4 0.2 Mr. Nichols said.
13 10 0.4 0.4 . Mr. Shulman
176 138 0.3 0.5 Mr. Bush has
13 11 0.2 1.5 to Mr. Trump&apos;s
13 11 0.2 -0.9 Mr. Bowman,
35 32 0.1 1.2 wrote Mr.
12 11 0.1 1.7 Mr. Lee to
22 21 0.1 1.4 facing Mr.
11 11 -0.0 0.7 Mr. Poehl also
13 13 -0.0 1.4 inadequate. &amp;quot;Mr.
16 16 -0.0 1.6 The 41-year-old Mr.
19 19 -0.0 0.5 14 . Mr.
26 26 -0.0 0.0 in November . Mr.
27 27 -0.0 -0.0 &amp;quot;For his part , Mr.
38 38 -0.0 1.4 . AMR,
39 39 -0.0 -0.3 for instance , Mr.
</table>
<bodyText confidence="0.994306041666667">
Table 3 shows MI and RIDE values for a sample of word sequences containing
the word Mr. The table is sorted by RIDE. The sequences near the top of the list are
better keywords than the sequences further down. None of these sequences would be
of much interest to a lexicographer (unless he or she were studying names). Many of
the sequences have rather small MI values.
Table 4 shows a few word sequences starting with the word the with large MI
values. All of these sequences have high MI (by construction), but some are high
in RIDE as well (labeled B), and some are not (labeled A). Most of the sequences
are interesting in one way or another, but the A sequences are different from the
B sequences. The A sequences would be of more interest to someone studying the
grammar in the WSJ subdomain, whereas the B sequences would be of more interest
to someone studying the terminology in this subdomain. The B sequences in Table 4
tend to pick out specific events in the news, if not specific stories. The phrase, the Basic
Law, for example, picks out a pair of stories that discuss the event of the handover of
Hong Kong to China, as illustrated in the concordance shown in Table 5.
Table 6 shows a number of word sequences with high MI containing common
prepositions. The high MI indicates an interesting association, but again most have
low RIDE and are not particularly good keywords, though there are a few exceptions
(Just for Men, a well-known brand name, has a high RIDE and is a good keyword).
The Japanese substrings are similar to the English substrings. Substrings with high
RIDE pick out specific documents (and/or events) and therefore tend to be relatively
good keywords. Substrings with high MI have nonindependent distributions (if not
noncompositional semantics), and are therefore likely to be interesting to a lexicogra-
pher or linguist. Substrings that are high in both are more likely to be meaningful units
</bodyText>
<page confidence="0.989645">
20
</page>
<note confidence="0.908849">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
</note>
<tableCaption confidence="0.702394666666667">
Table 4
English word sequence containing the. All of these phrases have high MI. Some have high
RIDF, and some do not.
</tableCaption>
<table confidence="0.990465357142857">
(A): Low RIDF (poor keywords)
Phase
the up side
the will of
the sell side
the Stock Exchange of
the buy side
the down side
the will to
the saying goes
the going gets
(B): High RIDF (better keywords)
tf df RIDF MI
11 11 -0.0 11.1
73 66 0.1 9.3
16 16 -0.0 8.6
17 16 0.1 8.5
16 15 0.1 8.5
20 20 -0.0 8.4
55 54 0.0 8.3
14 14 -0.0 8.1
15 15 -0.0 7.6
tf df RIDF MI
37 3 3.6 2.3
66 8 3.0 3.6
55 7 3.0 2.0
37 5 2.9 3.6
22 3 2.9 3.4
22 3 2.9 4.1
29 4 2.9 4.2
36 5 2.8 2.3
21 3 2.8 2.3
Phase
the joint commission
the SSC
The Delaware &amp;
the NHS
the kibbutz
the NSA&apos;s
the DeBartolos
the Basic Law
the national output
</table>
<bodyText confidence="0.99975988">
(words or phrases) than substrings that are high in just one or the other. Meaningless
fragments tend to be low in both MI and RIDF.
We grouped the Japanese classes into nine cells depending on whether the RIDF
was in the top 10%, the bottom 10%, or in between, and whether the MI was in the
top 10%, the bottom 10%, or in between. Substrings in the top 10% in both RIDF and
MI tend to be meaningful words such as (in English translation) merger, stock certificate,
dictionary, wireless, and so on. Substrings in the bottom 10% in both RIDF and MI tend
to be meaningless fragments, or straightforward compositional combinations of words
such as current regular-season game. Table 7 shows examples where MI and RIDF point
in opposite directions (see highlighted rectangles in panel (b) of Figure 10).
We have observed previously that MI is high for general vocabulary (words found
in dictionary) and RIDF is high for names, technical terminology, and good keywords
for information retrieval. Table 7 suggests an intriguing pattern. Japanese uses different
character sets for general vocabulary and loan words. Words that are high in MI tend
to use the general vocabulary character sets (hiragana and kanji) whereas words that
are high in RIDF tend to use the loan word character sets (katakana and English).
(There is an important exception, though, for names, which will be discussed in the
next subsection.)
The character sets largely reflect the history of the language. Japanese uses four
character sets (Shibatani 1990). Typically, functional words of Japanese origin are writ-
ten in hiragana. Words that were borrowed from Chinese many hundreds of years ago
are written in kanji. Loan words borrowed more recently from Western languages are
written in katakana. Truly foreign words are written in the English character set (also
known as romaji). We were pleasantly surprised to discover that MI and RIDF were
distinguishing substrings on the basis of these character set distinctions.
</bodyText>
<page confidence="0.998412">
21
</page>
<note confidence="0.606511">
Computational Linguistics Volume 27, Number 1
</note>
<tableCaption confidence="0.993139">
Table 5
</tableCaption>
<bodyText confidence="0.971535">
Concordance of the phrase the Basic Law. Note that most of the instances of the Basic Law appear
in just two stories, as indicated by the doc-id (the token-id of the first word in the document).
</bodyText>
<listItem confidence="0.98239754054054">
token-id left context right context doc-id
2229521: line in the drafting of the Basic Law that will determine how Hon 2228648
2229902: s policy as expressed in the Basic Law — as Gov. Wilson&apos;s debut s 2228648
9746758: he U.S. Constitution and the Basic Law of the Federal Republic of 9746014
11824764: any changes must follow the Basic Law , Hong Kong&apos;s miniconstitut 11824269
33007637: sts a tentative draft of the Basic Law, and although this may be 33007425
33007720: the relationship between the Basic Law and the Chinese Constitutio 33007425
33007729: onstitution . Originally the Basic Law was to deal with this topic 33007425
33007945: wer of interpretation of the Basic Law shall be vested in the NPC 33007425
33007975: tation of a provision of the Basic Law, the courts of the HKSAR { 33007425
33008031: interpret provisions of the Basic Law . If a case involves the in 33007425
33008045: tation of a provision of the Basic Law concerning defense , foreig 33007425
33008115: etation of an article of the Basic Law regarding &amp;quot; defense , forei 33007425
33008205: nland representatives of the Basic Law Drafting Committee fear tha 33007425
33008398: e: Mainland drafters of the Basic Law simply do not appreciate th 33007425
33008488: pret all the articles of the Basic Law . While recognizing that th 33007425
33008506: y and power to interpret the Basic Law, it should irrevocably del 33007425
33008521: pret those provisions of the Basic Law within the scope of Hong Ko 33007425
33008545: r the tentative draft of the Basic Law , I cannot help but conclud 33007425
33008690: d of being guaranteed by the Basic Law , are being redefined out o 33007425
33008712: uncilor, , is a member of the Basic Law Drafting Committee . 33007425
39020313: sts a tentative draft of the Basic Law, and although this may be 39020101
39020396: the relationship between the Basic Law and the Chinese Constitutio 39020101
39020405: onstitution . Originally the Basic Law was to deal with this topic 39020101
39020621: wer of interpretation of the Basic Law shall be vested in the NPC 39020101
39020651: tation of a provision of the Basic Law, the courts of the HKSAR { 39020101
39020707: interpret provisions of the Basic Law . If a case involves the in 39020101
39020721: tation of a provision of the Basic Law concerning defense , foreig 39020101
39020791: etation of an article of the Basic Law regarding &amp;quot; defense, forei 39020101
39020881: nland representatives of the Basic Law Drafting Committee fear tha 39020101
39021074: e : Mainland drafters of the Basic Law simply do not appreciate th 39020101
39021164: pret all the articles of the Basic Law . While recognizing that th 39020101
39021182: y and power to interpret the Basic Law , it should irrevocably del 39020101
39021197: pret those provisions of the Basic Law within the scope of Hong Ko 39020101
39021221: r the tentative draft of the Basic Law , I cannot help but conclud 39020101
39021366: d of being guaranteed by the Basic Law, are being redefined out o 39020101
39021388: uncilor , is a member of the Basic Law Drafting Committee . 39020101
</listItem>
<subsectionHeader confidence="0.997675">
3.3 Names
</subsectionHeader>
<bodyText confidence="0.999887846153846">
As mentioned above, names are an important exception to the rule that kanji (Chinese
characters) are used for general vocabulary (words found in the dictionary) that were
borrowed hundreds of years ago and katakana characters are used for more recent
loan words (such as technical terminology). As illustrated in Table 7, kanji are also
used for the names of Japanese people and katakana are used for the names of people
from other countries.
Names are quite different in English and Japanese. Figure 11 shows a striking con-
trast in the distributions of MI and RIDF values. MI has a more compact distribution
in English than Japanese. Japanese names cluster into two groups, but English names
do not.
The names shown in Figure 11 were collected using a simple set of heuristics. For
English, we selected substrings starting with the titles Mr., Ms., or Dr. For Japanese, we
selected keywords (as identified by the CD-ROM) ending with the special character
</bodyText>
<page confidence="0.997709">
22
</page>
<note confidence="0.910018">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
</note>
<tableCaption confidence="0.897152">
Table 6
English word sequences containing common prepositions. All have high MI (by construction);
most do not have high RIDF (though there are a few exceptions such as Just for Men, a
well-known brand name).
</tableCaption>
<table confidence="0.996249035714286">
tf df RIDF MI
14 14 -0.0 14.6
15 15 -0.0 14.4
12 12 -0.0 14.1
10 5 1.0 13.8
12 4 1.6 13.8
13 13 -0.0 13.7
23 21 0.1 13.7
10 2 2.3 13.4
10 9 0.2 13.4
19 16 0.2 12.9
tf df RIDF MI
11 5 1.1 14.3
11 10 0.1 13.1
13 12 0.1 12.7
16 16 -0.0 12.6
12 12 -0.0 11.6
12 12 -0.0 11.6
22 18 0.3 11.3
11 11 -0.0 11.2
17 12 0.5 11.2
22 20 0.1 11.1
tf df RIDF MI
11 11 -0.0 12.9
13 13 -0.0 12.6
13 13 -0.0 12.5
15 15 -0.0 12.4
16 16 -0.0 11.8
61 59 0.0 11.5
17 17 -0.0 11.5
12 12 -0.0 11.3
11 11 -0.0 10.8
20 20 -0.0 10.7
Preposition = &amp;quot;for&amp;quot;
feedlots for fattening
error for subgroups
Voice for Food
Quest for Value
Friends for Education
Commissioner for Refugees
meteorologist for Weather
Just for Men
Witness for Peace
priced for reoffering
Preposition = &amp;quot;on&amp;quot;
Terrorist on Trial
War on Poverty
Institute on Drug
dead on arrival
from on high
knocking on doors
warnings on cigarette
Subcommittee on Oversight
Group on Health
free on bail
Preposition = &amp;quot;by&amp;quot;
</table>
<tableCaption confidence="0.7872939">
piece by piece
guilt by association
step by step
bit by bit
engineer by training
side by side
each by Korea&apos;s
hemmed in by
dictated by formula
70%-owned by Exxon
</tableCaption>
<table confidence="0.998125727272727">
tf df RIDF MI Preposition = &amp;quot;of&amp;quot;
11 10 0.1 16.8 Joan of Arc
12 5 1.3 16.2 Ports of Call
16 16 -0.0 16.1 Articles of Confederation
14 13 0.1 16.1 writ of mandamus
10 9 0.2 15.9 Oil of Olay
11 11 -0.0 15.8 shortness of breath
10 9 0.2 15.6 Archbishop of Canterbury
10 8 0.3 15.3 Secret of My
12 12 -0.0 15.2 Lukman of Nigeria
16 4 2.0 15.2 Days of Rage
</table>
<bodyText confidence="0.9101355">
(-shi), which is roughly the equivalent of the English titles Mr. and Ms. In both cases,
phrases were required to have tf &gt; 10.3
</bodyText>
<footnote confidence="0.996289333333333">
3 This procedure produced the interesting substring, Mr. From, where both words would normally
appear on a stop list. This name has a large RIDF. (The MI, though, is small because the parts are so
high in frequency.)
</footnote>
<page confidence="0.992785">
23
</page>
<note confidence="0.60158">
Computational Linguistics Volume 27, Number 1
</note>
<tableCaption confidence="0.995815">
Table 7
</tableCaption>
<bodyText confidence="0.775406">
Examples of keywords with extreme values of RIDE and MI that point in opposite directions.
The top half (high RIDF and low MI) tends to have more loan words, largely written in
katakana and English. The bottom half (low RIDE and high MI) tends to have more general
vocabulary, largely written in Chinese kanji.
</bodyText>
<figure confidence="0.962205805555555">
RIDF MI Substrings Features
High Low AgE(native last name)
10% 10% SUN (company name)
I) —9(foreign name)
1,(brush)
&apos;17 7— (sofa)
Low High a&lt; f2-:1/1 (huge)
10% 10% 1k0)111(3 (passive)
flFV 19 (determination)
SiRf05(native full name)
#11a0(native full name)
kanji character
English character
katakana character
hiragana character
loan word, katakana
general vocabulary
general vocabulary, kanji
general vocabulary
kanji character
kanji character
u_3
2
5
4
1
0
-4 0 4 mi 8
(a) English names
Figure 11
MI and RIDE of people&apos;s names.
12 -8 -4 °M14
(b) Japanese names
8 12
The English names have a sharp cutoff around MI = 7 due in large part to
tf (X)tf (x)
</figure>
<bodyText confidence="0.648756">
the title Mr. M/(&apos;Mr.&apos;, x) = log, log
</bodyText>
<equation confidence="0.80398525">
tf ( ivir. ) 2 tf (‘Mr. ,x) 7.4 1og2 tf (rlk 4, Since ,x)•
log„ tf ( (vt„ r.
(x), is a small positive number, typically 0-3, M/(&apos;Mr&apos;, x) &lt;7.4.
i ,x)
</equation>
<bodyText confidence="0.99810375">
Names generally have RIDE values ranging from practically nothing (for com-
mon names like Jones) to extremely large values for excellent keywords. The Japanese
names, however, cluster into two groups, those with RIDE above 0.5, and those with
RIDE below 0.5. The separation above and below RIDE 0.5, we believe, is a reflec-
</bodyText>
<page confidence="0.981749">
24
</page>
<note confidence="0.582413">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
</note>
<bodyText confidence="0.999983111111111">
tion of the well-known distinction between new information and given information in
discourse structure. It is common in both English and Japanese, for the first mention
of a name in a news article to describe the name in more detail than subsequent uses.
In English, for example, terms like spokesman or spokeswoman and appositives are quite
common for the first use of a name, and less so, for subsequent uses. In Japanese, the
pattern appears to be even more rigid than in English. The first use will very often
list the full name (first name plus last name), unlike subsequent uses, which almost
always omit the first name. As a consequence, the last name exhibits a large range
of RIDE values, as in English, but the full name will usually (90%) fall below the
RIDE = 0.5 threshold. The MI values have a broader range as well, depending on the
compositionality of the name.
To summarize, RIDE and MI can be used to identify a number of interesting
similarities and differences in the use of names. Names are interestingly different
from general vocabulary. Many names are very good keywords and have large RIDE.
General vocabulary tends to have large MI. Although we observe this basic pattern
over both English and Japanese, names bring up some interesting differences between
the two languages such as the tendency for Japanese names to fall into two groups
separated by the RIDE = 0.5 threshold.
</bodyText>
<subsectionHeader confidence="0.998425">
3.4 Word Extraction
</subsectionHeader>
<bodyText confidence="0.999950045454545">
RIDE and MI may be useful for word extraction. In many languages such as Chinese,
Japanese, and Thai, word extraction is not an easy task, because, unlike English, many
of these languages do not use delimiters between words. Automatic word extraction
can be applied to the task of dictionary maintenance. Since most NLP applications
(including word segmentation for these languages) are dictionary based, word extrac-
tion is very important for these languages. Nagao and Mori (1994) and Nagata (1996)
proposed n-gram methods for Japanese. Sproat and Shih (1990) found MI to be useful
for word extraction in Chinese.
We performed the following simple experiment to see if both MI and RIDE could
be useful for word extraction in Japanese. We extracted four random samples of 100
substrings each. The four samples cover all four combinations of high and low RIDE
and high and low MI, where high is defined to be in the top 10% and low is defined
to be in the bottom 10%. Then we manually scored each sample substring using our
own subjective judgment. Substrings were labeled &amp;quot;good&amp;quot; (the substring is a word),
&amp;quot;bad&amp;quot; (the substring is not a word), or &amp;quot;gray&amp;quot; (the judge is not sure). The results are
presented in Table 8, which shows that substrings with high scores in both dimensions
are more likely to be words than substrings that score high in just one dimension.
Substrings with low scores in both dimensions are very unlikely to be words. These
results demonstrate plausibility for the use of multiple statistics. The approach could
be combined with other methods in the literature such as Kita et al. (1994) to produce
a more practical system. In any case, automatic word extraction is not an easy task for
Japanese (Nagata 1996).
</bodyText>
<sectionHeader confidence="0.988242" genericHeader="method">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999869166666667">
Bigrams and trigrams are commonly used in statistical natural language processing;
this paper described techniques for working with much longer n-grams, including
million-grams and even billion-grams. We presented algorithms (and C code) for com-
puting term frequency (tf) and document frequency (df) for all n-grams (substrings)
in a corpus (sequence). The method took only a few hours to compute tf and df for
all the n-grams in two large corpora, an English corpus of 50 million words of Wall
</bodyText>
<page confidence="0.995681">
25
</page>
<note confidence="0.603146">
Computational Linguistics Volume 27, Number 1
</note>
<tableCaption confidence="0.992224">
Table 8
</tableCaption>
<bodyText confidence="0.995363285714286">
The combination of RIDF and MI is better in a word extraction task than either by itself,
which is better than neither. Each cell reports performance over a sample of 100 substrings.
Substrings were subjectively judged to be &amp;quot;good&amp;quot; (the substring is a word), &amp;quot;bad&amp;quot; (the
substring is not a word), or &amp;quot;gray&amp;quot; (the judge is not sure). Two performance values are
reported, indicating what percentage of the 100 substrings are words. The larger performance
values count the &amp;quot;gray&amp;quot; substrings as words; the smaller performance values count the &amp;quot;gray&amp;quot;
substrings as nonwords.
</bodyText>
<table confidence="0.99798875">
All MI MI(high 10%) MI(low 10%)
All RIDF 20-44% 2-11%
RIDF(high 10%) 29-51% 38-55% 11-35%
RIDF(low 10%) 3-18% 4-13% 0-8%
</table>
<bodyText confidence="0.951748">
Street Journal news articles and a Japanese corpus of 216 million characters of Mainichi
Shimbun news articles.
The method works by grouping substrings into classes so that the computation
of tf and df over order N2 substrings can be reduced to a computation over order N
classes. The reduction makes use of four properties:
</bodyText>
<figureCaption confidence="0.9843712">
Properties 1-2: all substrings in a class have the same statistics (at least
for the statistics of interest, namely tf and df),
Property 3: the set of all substrings are partitioned by the classes, and
Property 4: there are many fewer classes (order N) than substrings
(order N2).
</figureCaption>
<bodyText confidence="0.999987166666667">
The second half of the paper used the results of computing tf and df for all n-grams
in the two large corpora mentioned above. We compared and contrasted RIDF and
MI, statistics that are motivated by work in lexicography and information retrieval.
Both statistics compare the frequency of an n-gram to chance, but they use different
notions of chance. RIDF looks for n-grams whose distributions pick out a relatively
small number documents, unlike a random (Poisson) distribution. These n-grams tend
to be good keywords for information retrieval, such as technical terms and names. MI
looks for n-grams whose internal structure cannot be attributed to compositionality. MI
tends to pick out general vocabulary—words and phrases that appear in dictionaries.
We believe that both statistics are useful, but in different and complementary ways.
In a Japanese word extraction task, the combination of MI and RIDF performed better
than either by itself.
</bodyText>
<sectionHeader confidence="0.976662" genericHeader="method">
Appendix: C Code
</sectionHeader>
<bodyText confidence="0.956563">
The following code is also available at http: / /www.milab.is.tsukuba.ac.jp/—myama/
tfdf.
A: C Code to Print All LCP-Delimited Intervals using C Language&apos;s Stack
The function output (below) is called 2N — 1 times. It will output an interval if the
interval is lcp-delimited (LBL &lt; SIL). Trivial intervals are always lcp-delimited. Non-
trivial intervals are lcp-delimited if the bounding lcps are smaller than the SIL=--lcpfla,
</bodyText>
<page confidence="0.873503">
26
</page>
<bodyText confidence="0.8233075">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
where k is the representative.
</bodyText>
<reference confidence="0.838163305555555">
void output(int i, int j, int k)f
int LBL = (1cp[i] &gt; lcp[j+1]) ? lcp[i] : lcp[j+1];
int SIL = lcp[k];
if(i==j) printf(&amp;quot;trivial &lt;XclAd&gt;, tf=1\n&amp;quot;, I, j);
else if(LBL &lt; SIL) printf(&amp;quot;non-trivial &lt;74, 7.d&gt;, rep=%d, tf=%d\n&amp;quot;,
j, k, j-i+1);
int print_LDIs(int i, int k)f
int j = k;
output(k,k,0); /* trivial intervals */
while(lcp[k] &lt;= lcp[j+1] &amp;&amp; j+1 &lt; N) j = print_LDIs(k, j+1);
output(i,j,k); /* non-trivial intervals */
return j;}
B: C Code to Print All LCP-Delimited Intervals Using an Own Stack
print_LDIs_stack is similar to print_LDIs, but uses its own stack. It takes the corpus
size, N, as an argument.
#define STACK_SIZE 100000
#define Top_i (stack[sp-1].i)
#define Top_k (stack[sp-1].k)
struct STACK {int i; int k;} stack[STACK_SIZE];
int sp = 0; /* stack pointer */
void push(int i, int k) f
if(sp &gt;= STACK_SIZE) f
fprintf(stderr, &amp;quot;stack overflow\n&amp;quot;);
exit(2);}
stack[sp].i = i;
stack[sp++].k = k;)-
void pop() {sp--;}
void print_LDIs_stack(int N) f
int j;
push(0,0);
for(j = 0; j &lt; N; j++) f
output(j, j, 0);
while(lcp[j+1] &lt; lcp[Top_k]) f
output(Top_i, j, Top_k);
pop();}
push(Top_k, j+1);11
</reference>
<page confidence="0.988698">
27
</page>
<figure confidence="0.250609">
Computational Linguistics Volume 27, Number 1
C: C Code to Print All LCP-Delimited Intervals with tf and df
</figure>
<bodyText confidence="0.8860415">
The steps 5 and 6 of the algorithm in Section 2.6 are implemented as the function
dec_df using the binary search.
</bodyText>
<reference confidence="0.607989555555555">
#define STACK_SIZE 100000
#define Top_i (stack[sp-1].i)
#define Top_k (stack[sp-1].k)
#define Top_df (stack[sp-1].df)
struct STACK {int i; int k; int df;} stack[STACK_SIZE];
int sp = 0; /* stack pointer */
void push(int i, int k, int df) {
if(sp &gt;= STACK_SIZE){
fprintf(stderr, &amp;quot;stack overflow\n&amp;quot;);
exit(2);}
stack[sp].i = i;
stack[sp].k = k;
stack[sp++].df = df;}
void pop() {sp--;}
void output(int i, int j, int k, int df) {
int LBL;
if(lcp[i] &gt; lcp[j+1]) LBL = lcp[i];
else LBL = lcp[j+1];
if(i==j) printf (&amp;quot;trivial &lt;%d,%d&gt;, tf=1\n&amp;quot;, i, j);
else if(LBL &lt; lcp[k])
printf(&amp;quot;non-trivial &lt;%d, %d&gt;, rep&apos;/0d, tf=%d, df=%d\n&amp;quot;,
j, k, j-i+1, df);
/*
* Print_LDIs_with_df does not only print tf, but also df.
* It takes the corpus size, N, and the number of documents, D.
* doc() returns the document number of the suffix array&apos;s index.
* dec_df() decrease a df-counter in the stack when duplicate
* counting occurs.
*/
void dec_df(int docid) {
int beg=0, end=sp, mid=sp/2;
while(beg != mid) {
if(doclink[docid] &gt;= stack[mid].i) beg = mid;
else end = mid;
mid = (beg + end) / 2;
stack[mid].df--;
</reference>
<page confidence="0.983491">
28
</page>
<bodyText confidence="0.488889">
Yamamoto and Church Term Frequency and Document Frequency for All Substrings
print_LDIs_with_df (int N, int D)
</bodyText>
<equation confidence="0.9865242">
int i, j, df;
doclink = (int *)malloc(sizeof (int) * D);
for(i = 0; i &lt; D; i++) doclink[i] = -1;
push(0,0,1);
for(j = 0; j &lt; N; j++)
output(j,j,0,1);
if (doclinkidoc(j)] != -1) dec_df(doc(j));
doclink[doc(j)] = j;
df = 1;
while (1cp[j+1] &lt; lcp[Top_k])
df = Top_df + df;
output(Top_i,j,Top_k,df);
pop();
1
push(Top_k, j+1, df);
</equation>
<sectionHeader confidence="0.817212" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9419455">
We would like to thank the anonymous
reviewers for Computational Linguistics who
made insightful comments on an earlier
draft.
</bodyText>
<sectionHeader confidence="0.876669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832757575757">
Charniak, Eugene. 1993. Statistical Language
Learning. MIT Press.
Church, Kenneth W. and William A. Gale.
1995. Poisson mixtures. Natural Language
Engineering, 1(2):163-190.
Gonnet, Gaston H., Ricardo A. Baeza-Yates,
and Tim Snider. 1992. New indices for
text: PAT trees and PAT arrays. In
Information Retrieval: Data Structure &amp;
Algorithms, pages 66-82. Prentice Hall
PTR.
Gusfield, Dan. 1997. Algorithms on Strings,
Trees, and Sequences: Computer Science and
Computational Biology. Cambridge
University Press.
Hui, Lucas Chi Kwong. 1992. Color set size
problem with applications to string
matching. In Lecture Notes in Computer
Science, Volume 644. Springer (CPM92:
Combinatorial Pattern Matching, 3rd
Annual Symposium), pages 230-243.
Jelirtek, Frederick. 1997. Statistical Methods
for Speech Recognition. MIT Press.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing,
ASSP-35(3):400-401.
Kita, Kenji, Yasuhiko Kato, Takashi Omoto,
and Yoneo Yano. 1994. A comparative
study of automatic extraction of
collocations from corpora: Mutual
information vs. cost criteria. Journal of
Natural Language Processing, 1(1):21-33.
Manber, Udi and Gene Myers. 1990. Suffix
arrays: A new method for on-line string
searches. In Proceedings of The First Annual
ACM-SIAM Symposium on Discrete
Algorithms, pages 319-327. URL=http://
glimpse.cs.arizona.edu/udi.html.
McCreight, Edward M. 1976. A
space-economical suffix tree construction
algorithm. Journal of the ACM, 23:262-
272.
Nagao, Makoto and Shinsuke Mori. 1994. A
new method of n-gram statistics for large
number of n and automatic extraction of
words and phrases from large text data of
Japanese. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 611-615.
Nagata, Masaaki. 1996. Automatic
extraction of new words from Japanese
texts using generalized forward-backward
search. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, pages 48-59.
Paul, Douglas B. and Janet M. Baker. 1992.
The design for the Wall Street
Journal-based CSR corpus. In Proceedings
of DARPA Speech and Natural Language
Workshop, pages 357-361.
Shibatani, Masayoshi. 1990. The Languages of
Japan. Cambridge Language Surveys.
Cambridge University Press.
</reference>
<page confidence="0.947564">
29
</page>
<note confidence="0.355152">
Computational Linguistics Volume 27, Number 1
</note>
<reference confidence="0.999569909090909">
Sparck Jones, Karen. 1972. A statistical
interpretation of term specificity and its
application in retrieval. Journal of
Documentation, 28(1):11-21.
Sproat, Richard and Chilin Shih. 1990. A
statistical method for finding word
boundaries in Chinese text. Computer
Processing of Chinese and Oriental Languages,
4(4):336-351.
Ukkonen, Esko. 1995. On-line construction
of suffix trees. Algorithmica, 14(3):249-260.
</reference>
<page confidence="0.99881">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894260">
<title confidence="0.9758">Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus</title>
<author confidence="0.999478">Mikio Yamamoto Kenneth W Churcht</author>
<affiliation confidence="0.998851">University of Tsukuba AT&amp;T Labs—Research</affiliation>
<abstract confidence="0.99792285">Bigrams and trigrams are commonly used in statistical natural language processing; this paper will describe techniques for working with much longer n-grams. Suffix arrays (Manber and Myers 1990) were first introduced to compute the frequency and location of a substring (n-gram) a sequence (corpus) of length compute frequencies over all + 1)/2 in a corpus, the substrings are grouped into a manageable number of equivalence classes. In this way, a prohibitive computation over substrings is reduced to a manageable computation over classes. This paper presents both the algorithms and the code that were used to compute term frequency (tf) and document frequency (df) for all n-grams in two large corpora, an English corpus of million words of Street Journal a Japanese corpus of 216 million characters of Mainichi Shimbun. The second half of the paper uses these frequencies to find &amp;quot;interesting&amp;quot; substrings. Lexicographers have been interested in n-grams with high mutual information (MI) where the joint term frequency is higher than what would be expected by chance, assuming that the parts of the n-gram combine independently. Residual inverse document frequency (RIDF) compares document frequency to another model of chance where terms with a particular term frequency are distributed randomly throughout the collection. MI tends to pick out phrases with noncom positional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is better than either by itself in a Japanese word extraction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>void output(int i, int j, int k)f int LBL = (1cp[i] &gt; lcp[j+1]) ? lcp[i] : lcp[j+1]; int SIL = lcp[k]; if(i==j) printf(&amp;quot;trivial tf=1\n&amp;quot;, I, j); else if(LBL &lt; SIL) printf(&amp;quot;non-trivial &lt;74, 7.d&gt;, rep=%d, tf=%d\n&amp;quot;, j, k, j-i+1); int print_LDIs(int i, int k)f int j = k;</title>
<booktitle>output(k,k,0); /* trivial intervals */ while(lcp[k] &lt;= lcp[j+1] &amp;&amp; j+1 &lt; N) j = print_LDIs(k, j+1); output(i,j,k); /* non-trivial intervals */ return j;} B: C Code to Print All LCP-Delimited Intervals</booktitle>
<marker></marker>
<rawString>void output(int i, int j, int k)f int LBL = (1cp[i] &gt; lcp[j+1]) ? lcp[i] : lcp[j+1]; int SIL = lcp[k]; if(i==j) printf(&amp;quot;trivial &lt;XclAd&gt;, tf=1\n&amp;quot;, I, j); else if(LBL &lt; SIL) printf(&amp;quot;non-trivial &lt;74, 7.d&gt;, rep=%d, tf=%d\n&amp;quot;, j, k, j-i+1); int print_LDIs(int i, int k)f int j = k; output(k,k,0); /* trivial intervals */ while(lcp[k] &lt;= lcp[j+1] &amp;&amp; j+1 &lt; N) j = print_LDIs(k, j+1); output(i,j,k); /* non-trivial intervals */ return j;} B: C Code to Print All LCP-Delimited Intervals Using an Own Stack print_LDIs_stack is similar to print_LDIs, but uses its own stack. It takes the corpus size, N, as an argument.</rawString>
</citation>
<citation valid="false">
<booktitle>define STACK_SIZE 100000 #define Top_i (stack[sp-1].i) #define Top_k (stack[sp-1].k</booktitle>
<marker></marker>
<rawString>#define STACK_SIZE 100000 #define Top_i (stack[sp-1].i) #define Top_k (stack[sp-1].k)</rawString>
</citation>
<citation valid="false">
<authors>
<author>struct STACK</author>
</authors>
<title>{int i; int k;} stack[STACK_SIZE]; int sp = 0; /* stack pointer */ void push(int i, int k) f if(sp &gt;= STACK_SIZE) f fprintf(stderr, &amp;quot;stack overflow\n&amp;quot;); exit(2);</title>
<marker>STACK, </marker>
<rawString>struct STACK {int i; int k;} stack[STACK_SIZE]; int sp = 0; /* stack pointer */ void push(int i, int k) f if(sp &gt;= STACK_SIZE) f fprintf(stderr, &amp;quot;stack overflow\n&amp;quot;); exit(2);}</rawString>
</citation>
<citation valid="false">
<authors>
<author>i stack</author>
</authors>
<title>i; stack[sp++].k = k;)-void pop() {sp--;} void print_LDIs_stack(int N) f int j; push(0,0);</title>
<marker>stack, </marker>
<rawString>stack[sp].i = i; stack[sp++].k = k;)-void pop() {sp--;} void print_LDIs_stack(int N) f int j; push(0,0);</rawString>
</citation>
<citation valid="false">
<authors>
<author>for</author>
</authors>
<title>0; j &lt; N; j++) f output(j, j, 0); while(lcp[j+1] &lt; lcp[Top_k]) f output(Top_i, j, Top_k);</title>
<location>pop();</location>
<marker>for, </marker>
<rawString>for(j = 0; j &lt; N; j++) f output(j, j, 0); while(lcp[j+1] &lt; lcp[Top_k]) f output(Top_i, j, Top_k); pop();}</rawString>
</citation>
<citation valid="false">
<authors>
<author>push</author>
</authors>
<booktitle>j+1);11 #define STACK_SIZE 100000 #define Top_i (stack[sp-1].i) #define Top_k (stack[sp-1].k) #define Top_df (stack[sp-1].df</booktitle>
<marker>push, </marker>
<rawString>push(Top_k, j+1);11 #define STACK_SIZE 100000 #define Top_i (stack[sp-1].i) #define Top_k (stack[sp-1].k) #define Top_df (stack[sp-1].df)</rawString>
</citation>
<citation valid="false">
<authors>
<author>struct STACK</author>
</authors>
<title>{int i; int k; int df;} stack[STACK_SIZE]; int sp = 0; /* stack pointer */ void push(int i, int k, int df) { if(sp &gt;= STACK_SIZE){ fprintf(stderr, &amp;quot;stack overflow\n&amp;quot;); exit(2);</title>
<marker>STACK, </marker>
<rawString>struct STACK {int i; int k; int df;} stack[STACK_SIZE]; int sp = 0; /* stack pointer */ void push(int i, int k, int df) { if(sp &gt;= STACK_SIZE){ fprintf(stderr, &amp;quot;stack overflow\n&amp;quot;); exit(2);}</rawString>
</citation>
<citation valid="false">
<authors>
<author>i stack</author>
</authors>
<title>i; stack[sp].k = k; stack[sp++].df = df;} void pop() {sp--;} void output(int i, int j, int k, int df) { int LBL; if(lcp[i] &gt; lcp[j+1]) LBL = lcp[i]; else LBL = lcp[j+1]; if(i==j) printf (&amp;quot;trivial &lt;%d,%d&gt;, tf=1\n&amp;quot;, i, j); else if(LBL &lt; lcp[k]) printf(&amp;quot;non-trivial &lt;%d, %d&gt;, rep&apos;/0d, tf=%d, df=%d\n&amp;quot;, j, k, j-i+1, df); /* * Print_LDIs_with_df does not only print tf, but also df. * It takes the corpus size, N, and the number of documents, D. * doc() returns the document number of the suffix array&apos;s index. * dec_df() decrease a df-counter in the stack when duplicate * counting occurs.</title>
<publisher></publisher>
<marker>stack, </marker>
<rawString>stack[sp].i = i; stack[sp].k = k; stack[sp++].df = df;} void pop() {sp--;} void output(int i, int j, int k, int df) { int LBL; if(lcp[i] &gt; lcp[j+1]) LBL = lcp[i]; else LBL = lcp[j+1]; if(i==j) printf (&amp;quot;trivial &lt;%d,%d&gt;, tf=1\n&amp;quot;, i, j); else if(LBL &lt; lcp[k]) printf(&amp;quot;non-trivial &lt;%d, %d&gt;, rep&apos;/0d, tf=%d, df=%d\n&amp;quot;, j, k, j-i+1, df); /* * Print_LDIs_with_df does not only print tf, but also df. * It takes the corpus size, N, and the number of documents, D. * doc() returns the document number of the suffix array&apos;s index. * dec_df() decrease a df-counter in the stack when duplicate * counting occurs. */</rawString>
</citation>
<citation valid="false">
<title>void dec_df(int docid) { int beg=0, end=sp, mid=sp/2; while(beg</title>
<journal>mid) { if(doclink[docid] &gt;= stack[mid].i) beg = mid; else end = mid; mid = (beg + end) /</journal>
<volume>2</volume>
<pages>stack[mid].df--;</pages>
<marker></marker>
<rawString>void dec_df(int docid) { int beg=0, end=sp, mid=sp/2; while(beg != mid) { if(doclink[docid] &gt;= stack[mid].i) beg = mid; else end = mid; mid = (beg + end) / 2; stack[mid].df--;</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3505" citStr="Charniak 1993" startWordPosition="546" endWordPosition="547">i, Tsukuba 305-8573, Japan t 180 Park Avenue, Florham Park, NJ 07932 Computational Linguistics Volume 27, Number 1 retrieval literature (Sparck Jones 1972); it counts the number of documents that contain a type at least once. Term frequency is an integer between 0 and N; document frequency is an integer between 0 and D, the number of documents in the corpus. The statistics, tf and df, and functions of these statistics such as mutual information (MI) and inverse document frequency (IDF), are usually computed over short n-grams such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens) (Charniak 1993; Jelinek 1997). This paper will show how to work with much longer n-grams, including million-grams and even billion-grams. In corpus-based NLP, term frequencies are often converted into probabilities, using the maximum likelihood estimator (MLE), the Good-Turing method (Katz 1987), or Deleted Interpolation (Jelinek 1997, Chapter 15). These probabilities are used in noisy channel applications such as speech recognition to distinguish more likely sequences from less likely sequences, reducing the search space (perplexity) for the acoustic recognizer. In information retrieval, document frequenci</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Charniak, Eugene. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>Poisson mixtures.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<pages>1--2</pages>
<contexts>
<context position="4524" citStr="Church and Gale 1995" startWordPosition="698" endWordPosition="701">ch as speech recognition to distinguish more likely sequences from less likely sequences, reducing the search space (perplexity) for the acoustic recognizer. In information retrieval, document frequencies are converted into inverse document frequency (IDF), which plays an important role in term weighting (Sparck Jones 1972). IDF (t) = - log2 df (t) IDF (t) can be interpreted as the number of bits of information the system is given if it is told that the document in question contains the term t. Rare terms contribute more bits than common terms. Mutual information (MI) and residual IDF (RIDF) (Church and Gale 1995) both compare tf and df to what would be expected by chance, using two different notions of chance. MI compares the term frequency of an n-gram to what would be expected if the parts combined independently, whereas RIDF combines the document frequency of a term to what would be expected if a term with a given term frequency were randomly distributed throughout the collection. MI tends to pick out phrases with noncompositional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (whi</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Church, Kenneth W. and William A. Gale. 1995. Poisson mixtures. Natural Language Engineering, 1(2):163-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaston H Gonnet</author>
<author>Ricardo A Baeza-Yates</author>
<author>Tim Snider</author>
</authors>
<title>New indices for text: PAT trees and PAT arrays.</title>
<date>1992</date>
<booktitle>In Information Retrieval: Data Structure &amp; Algorithms,</booktitle>
<pages>66--82</pages>
<publisher>Prentice Hall PTR.</publisher>
<marker>Gonnet, Baeza-Yates, Snider, 1992</marker>
<rawString>Gonnet, Gaston H., Ricardo A. Baeza-Yates, and Tim Snider. 1992. New indices for text: PAT trees and PAT arrays. In Information Retrieval: Data Structure &amp; Algorithms, pages 66-82. Prentice Hall PTR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7720" citStr="Gusfield (1997)" startWordPosition="1243" endWordPosition="1244">groups the N(N + 1)/2 substrings into at most 2N — 1 equivalence classes. By grouping substrings in this way, many of the statistics of interest can be computed over the relatively small number of classes, which is manageable, rather than over the quadratic number of substrings, which would be prohibitive. The suffix array data structure (Manber and Myers 1990) was introduced as a database indexing technique. Suffix arrays can be viewed as a compact representation of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive introduction to suffix trees. Hui (1992) shows how to compute df for all substrings using generalized suffix trees. The major advantage of suffix arrays over suffix trees is space. The space requirements for suffix trees (but not for suffix arrays) grow with alphabet size: O(NIE 1) space, where I El is the alphabet size. The dependency on alphabet size is a serious issue for Japanese. Manber and Myers (1990) reported that suffix arrays are an order of magnitude more efficient in space than suffix trees, even in the case of relatively small alphabet size (1E1 = 96). The adv</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Gusfield, Dan. 1997. Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucas Chi Kwong Hui</author>
</authors>
<title>Color set size problem with applications to string matching.</title>
<date>1992</date>
<booktitle>In Lecture Notes in Computer Science, Volume 644. Springer (CPM92: Combinatorial Pattern Matching, 3rd Annual Symposium),</booktitle>
<pages>230--243</pages>
<contexts>
<context position="7781" citStr="Hui (1992)" startWordPosition="1252" endWordPosition="1253">lasses. By grouping substrings in this way, many of the statistics of interest can be computed over the relatively small number of classes, which is manageable, rather than over the quadratic number of substrings, which would be prohibitive. The suffix array data structure (Manber and Myers 1990) was introduced as a database indexing technique. Suffix arrays can be viewed as a compact representation of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive introduction to suffix trees. Hui (1992) shows how to compute df for all substrings using generalized suffix trees. The major advantage of suffix arrays over suffix trees is space. The space requirements for suffix trees (but not for suffix arrays) grow with alphabet size: O(NIE 1) space, where I El is the alphabet size. The dependency on alphabet size is a serious issue for Japanese. Manber and Myers (1990) reported that suffix arrays are an order of magnitude more efficient in space than suffix trees, even in the case of relatively small alphabet size (1E1 = 96). The advantages of suffix arrays over suffix trees become much more s</context>
</contexts>
<marker>Hui, 1992</marker>
<rawString>Hui, Lucas Chi Kwong. 1992. Color set size problem with applications to string matching. In Lecture Notes in Computer Science, Volume 644. Springer (CPM92: Combinatorial Pattern Matching, 3rd Annual Symposium), pages 230-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelirtek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<marker>Jelirtek, 1997</marker>
<rawString>Jelirtek, Frederick. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="3787" citStr="Katz 1987" startWordPosition="585" endWordPosition="586">uency is an integer between 0 and D, the number of documents in the corpus. The statistics, tf and df, and functions of these statistics such as mutual information (MI) and inverse document frequency (IDF), are usually computed over short n-grams such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens) (Charniak 1993; Jelinek 1997). This paper will show how to work with much longer n-grams, including million-grams and even billion-grams. In corpus-based NLP, term frequencies are often converted into probabilities, using the maximum likelihood estimator (MLE), the Good-Turing method (Katz 1987), or Deleted Interpolation (Jelinek 1997, Chapter 15). These probabilities are used in noisy channel applications such as speech recognition to distinguish more likely sequences from less likely sequences, reducing the search space (perplexity) for the acoustic recognizer. In information retrieval, document frequencies are converted into inverse document frequency (IDF), which plays an important role in term weighting (Sparck Jones 1972). IDF (t) = - log2 df (t) IDF (t) can be interpreted as the number of bits of information the system is given if it is told that the document in question conta</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Kita</author>
<author>Yasuhiko Kato</author>
<author>Takashi Omoto</author>
<author>Yoneo Yano</author>
</authors>
<title>A comparative study of automatic extraction of collocations from corpora: Mutual information vs. cost criteria.</title>
<date>1994</date>
<journal>Journal of Natural Language Processing,</journal>
<pages>1--1</pages>
<contexts>
<context position="68209" citStr="Kita et al. (1994)" startWordPosition="11998" endWordPosition="12001">each sample substring using our own subjective judgment. Substrings were labeled &amp;quot;good&amp;quot; (the substring is a word), &amp;quot;bad&amp;quot; (the substring is not a word), or &amp;quot;gray&amp;quot; (the judge is not sure). The results are presented in Table 8, which shows that substrings with high scores in both dimensions are more likely to be words than substrings that score high in just one dimension. Substrings with low scores in both dimensions are very unlikely to be words. These results demonstrate plausibility for the use of multiple statistics. The approach could be combined with other methods in the literature such as Kita et al. (1994) to produce a more practical system. In any case, automatic word extraction is not an easy task for Japanese (Nagata 1996). 4. Conclusion Bigrams and trigrams are commonly used in statistical natural language processing; this paper described techniques for working with much longer n-grams, including million-grams and even billion-grams. We presented algorithms (and C code) for computing term frequency (tf) and document frequency (df) for all n-grams (substrings) in a corpus (sequence). The method took only a few hours to compute tf and df for all the n-grams in two large corpora, an English co</context>
</contexts>
<marker>Kita, Kato, Omoto, Yano, 1994</marker>
<rawString>Kita, Kenji, Yasuhiko Kato, Takashi Omoto, and Yoneo Yano. 1994. A comparative study of automatic extraction of collocations from corpora: Mutual information vs. cost criteria. Journal of Natural Language Processing, 1(1):21-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix arrays: A new method for on-line string searches.</title>
<date>1990</date>
<booktitle>In Proceedings of The First Annual ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<pages>319--327</pages>
<note>URL=http:// glimpse.cs.arizona.edu/udi.html.</note>
<contexts>
<context position="1991" citStr="Manber and Myers 1990" startWordPosition="304" endWordPosition="307">t frequency (RIDF) compares document frequency to another model of chance where terms with a particular term frequency are distributed randomly throughout the collection. MI tends to pick out phrases with noncom positional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is better than either by itself in a Japanese word extraction task. 1. Introduction We will use suffix arrays (Manber and Myers 1990) to compute a number of type/token statistics of interest, including term frequency and document frequency, for all n-grams in large corpora. Type/token statistics model the corpus as a sequence of N tokens (characters, words, terms, n-grams, etc.) drawn from a vocabulary of V types. Different tokenizing rules will be used for different corpora and for different applications. In this work, the English text is tokenized into a sequence of English words delimited by white space and the Japanese text is tokenized into a sequence of Japanese characters (typically one or two bytes each). Term frequ</context>
<context position="7468" citStr="Manber and Myers 1990" startWordPosition="1201" endWordPosition="1204">section will introduce an algorithm based on suffix arrays for computing tf and df and many functions of these quantities for all substrings in a corpus in 0(N log N) time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The algorithm groups the N(N + 1)/2 substrings into at most 2N — 1 equivalence classes. By grouping substrings in this way, many of the statistics of interest can be computed over the relatively small number of classes, which is manageable, rather than over the quadratic number of substrings, which would be prohibitive. The suffix array data structure (Manber and Myers 1990) was introduced as a database indexing technique. Suffix arrays can be viewed as a compact representation of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive introduction to suffix trees. Hui (1992) shows how to compute df for all substrings using generalized suffix trees. The major advantage of suffix arrays over suffix trees is space. The space requirements for suffix trees (but not for suffix arrays) grow with alphabet size: O(NIE 1) space, where I El is the alphabet size. The </context>
<context position="17016" citStr="Manber and Myers (1990)" startWordPosition="2854" endWordPosition="2857">from the suffix array literature make use of an auxiliary array for storing LCPs (longest common prefixes). The lcp array contains N + 1 integers. Each element, lcp [i], indicates the length of the common prefix between s[i — 1] and s[i]. We pad the lcp vector with zeros (lcp[0] = lcp[N] = 0) to simplify the discussion. The padding avoids the need to test for certain end conditions. Figure 3 shows the lcp vector for the suffix array of &amp;quot;to_be_or_not_to_be&amp;quot;. For example, since s [10] and s [11] both start with the substring &amp;quot;o_be&amp;quot;, lcp [11] is set to 4, the length of the longest common prefix. Manber and Myers (1990) use the lcp vector in their 0(P+log N) algorithm for computing the frequency and location of a substring of length P in a sequence of length N. They showed that the lcp vector can be computed in 0(N log N) time. These algorithms are much faster than the obvious straightforward implementation when the corpus contains long repeated substrings, though for many corpora, the complications required to avoid quadratic behavior are unnecessary. 2.3 Classes of Substrings Thus far we have seen how to compute tf for a single n-gram, but how do we compute tf and df for all n-grams? As mentioned above, th</context>
</contexts>
<marker>Manber, Myers, 1990</marker>
<rawString>Manber, Udi and Gene Myers. 1990. Suffix arrays: A new method for on-line string searches. In Proceedings of The First Annual ACM-SIAM Symposium on Discrete Algorithms, pages 319-327. URL=http:// glimpse.cs.arizona.edu/udi.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward M McCreight</author>
</authors>
<title>A space-economical suffix tree construction algorithm.</title>
<date>1976</date>
<journal>Journal of the ACM,</journal>
<pages>23--262</pages>
<contexts>
<context position="7605" citStr="McCreight 1976" startWordPosition="1225" endWordPosition="1226"> corpus in 0(N log N) time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The algorithm groups the N(N + 1)/2 substrings into at most 2N — 1 equivalence classes. By grouping substrings in this way, many of the statistics of interest can be computed over the relatively small number of classes, which is manageable, rather than over the quadratic number of substrings, which would be prohibitive. The suffix array data structure (Manber and Myers 1990) was introduced as a database indexing technique. Suffix arrays can be viewed as a compact representation of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive introduction to suffix trees. Hui (1992) shows how to compute df for all substrings using generalized suffix trees. The major advantage of suffix arrays over suffix trees is space. The space requirements for suffix trees (but not for suffix arrays) grow with alphabet size: O(NIE 1) space, where I El is the alphabet size. The dependency on alphabet size is a serious issue for Japanese. Manber and Myers (1990) reported that suffix arrays are an order of magnitud</context>
</contexts>
<marker>McCreight, 1976</marker>
<rawString>McCreight, Edward M. 1976. A space-economical suffix tree construction algorithm. Journal of the ACM, 23:262-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
<author>Shinsuke Mori</author>
</authors>
<title>A new method of n-gram statistics for large number of n and automatic extraction of words and phrases from large text data of Japanese.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>611--615</pages>
<contexts>
<context position="9081" citStr="Nagao and Mori (1994)" startWordPosition="1462" endWordPosition="1465">. The suffix array data structure makes it convenient to compute the frequency and location of a substring (n-gram) in a long sequence (corpus). The early work was motivated by biological applications such as matching of DNA sequences. Suffix arrays are closely related to PAT arrays (Gonnet, Baeza-Yates, and Snider 1992), which were motivated in part by a project at the University of Waterloo to distribute the Oxford English Dictionary with indexes on CD-ROM. PAT arrays have also been motivated by applications in information retrieval. A similar data structure to suffix arrays was proposed by Nagao and Mori (1994) for processing Japanese text. The alphabet sizes vary considerably in each of these cases. DNA has a relatively small alphabet of just 4 characters, whereas Japanese has a relatively large alphabet of more than 5,000 characters. The methods such as suffix arrays and PAT arrays scale naturally over alphabet size. In the experimental section (Section 3) using the Wall Street Journal corpus, the suffix array is applied to a large corpus of English text, where the alphabet is assumed to be the set of all English words, an unbounded set. It is sometimes assumed that larger alphabets are more chall</context>
<context position="14576" citStr="Nagao and Mori (1994)" startWordPosition="2447" endWordPosition="2450"> Figure 1 after sorting. The integers in s are sorted so that the semi-infinite strings are now in alphabetical order. 5 Computational Linguistics Volume 27, Number 1 suffix to start with this term. Consequently, tf (&amp;quot;to_be&amp;quot;) = 17 — 16+1 = 2. Moreover, the term appears at positions(&amp;quot;to_be&amp;quot;) = {s [16], s[17]} = {13, 0}, and only these positions. Similarly, the substring &amp;quot;to&amp;quot; has the same tf and positions, as do the substrings, &amp;quot;to_&amp;quot; and &amp;quot;to_b&amp;quot;. Although there may be N(N + 1)/2 ways to pick i and j, it will turn out that we need only consider 2N — 1 of them when computing tf for all substrings. Nagao and Mori (1994) ran this procedure quite successfully on a large corpus of Japanese text. They report that it takes 0(N log N) time, assuming that the sort step performs 0(N log N) comparisons, and that each comparison takes constant time. While these are often reasonable assumptions, we have found that if the corpus contains long repeated substrings (e.g., duplicated articles), as our English corpus does (Paul and Baker 1992), then the sort can consume quadratic time, since each comparison can take order N time. Like Nagao and Mori (1994), we were also able to apply this procedure quite successfully to our </context>
<context position="67081" citStr="Nagao and Mori (1994)" startWordPosition="11802" endWordPosition="11805">he two languages such as the tendency for Japanese names to fall into two groups separated by the RIDE = 0.5 threshold. 3.4 Word Extraction RIDE and MI may be useful for word extraction. In many languages such as Chinese, Japanese, and Thai, word extraction is not an easy task, because, unlike English, many of these languages do not use delimiters between words. Automatic word extraction can be applied to the task of dictionary maintenance. Since most NLP applications (including word segmentation for these languages) are dictionary based, word extraction is very important for these languages. Nagao and Mori (1994) and Nagata (1996) proposed n-gram methods for Japanese. Sproat and Shih (1990) found MI to be useful for word extraction in Chinese. We performed the following simple experiment to see if both MI and RIDE could be useful for word extraction in Japanese. We extracted four random samples of 100 substrings each. The four samples cover all four combinations of high and low RIDE and high and low MI, where high is defined to be in the top 10% and low is defined to be in the bottom 10%. Then we manually scored each sample substring using our own subjective judgment. Substrings were labeled &amp;quot;good&amp;quot; (t</context>
</contexts>
<marker>Nagao, Mori, 1994</marker>
<rawString>Nagao, Makoto and Shinsuke Mori. 1994. A new method of n-gram statistics for large number of n and automatic extraction of words and phrases from large text data of Japanese. In Proceedings of the 15th International Conference on Computational Linguistics, pages 611-615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Automatic extraction of new words from Japanese texts using generalized forward-backward search.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>48--59</pages>
<contexts>
<context position="67099" citStr="Nagata (1996)" startWordPosition="11807" endWordPosition="11808">he tendency for Japanese names to fall into two groups separated by the RIDE = 0.5 threshold. 3.4 Word Extraction RIDE and MI may be useful for word extraction. In many languages such as Chinese, Japanese, and Thai, word extraction is not an easy task, because, unlike English, many of these languages do not use delimiters between words. Automatic word extraction can be applied to the task of dictionary maintenance. Since most NLP applications (including word segmentation for these languages) are dictionary based, word extraction is very important for these languages. Nagao and Mori (1994) and Nagata (1996) proposed n-gram methods for Japanese. Sproat and Shih (1990) found MI to be useful for word extraction in Chinese. We performed the following simple experiment to see if both MI and RIDE could be useful for word extraction in Japanese. We extracted four random samples of 100 substrings each. The four samples cover all four combinations of high and low RIDE and high and low MI, where high is defined to be in the top 10% and low is defined to be in the bottom 10%. Then we manually scored each sample substring using our own subjective judgment. Substrings were labeled &amp;quot;good&amp;quot; (the substring is a </context>
<context position="68331" citStr="Nagata 1996" startWordPosition="12021" endWordPosition="12022">bstring is not a word), or &amp;quot;gray&amp;quot; (the judge is not sure). The results are presented in Table 8, which shows that substrings with high scores in both dimensions are more likely to be words than substrings that score high in just one dimension. Substrings with low scores in both dimensions are very unlikely to be words. These results demonstrate plausibility for the use of multiple statistics. The approach could be combined with other methods in the literature such as Kita et al. (1994) to produce a more practical system. In any case, automatic word extraction is not an easy task for Japanese (Nagata 1996). 4. Conclusion Bigrams and trigrams are commonly used in statistical natural language processing; this paper described techniques for working with much longer n-grams, including million-grams and even billion-grams. We presented algorithms (and C code) for computing term frequency (tf) and document frequency (df) for all n-grams (substrings) in a corpus (sequence). The method took only a few hours to compute tf and df for all the n-grams in two large corpora, an English corpus of 50 million words of Wall 25 Computational Linguistics Volume 27, Number 1 Table 8 The combination of RIDF and MI i</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Nagata, Masaaki. 1996. Automatic extraction of new words from Japanese texts using generalized forward-backward search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 48-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Paul</author>
<author>Janet M Baker</author>
</authors>
<title>The design for the Wall Street Journal-based CSR corpus.</title>
<date>1992</date>
<booktitle>In Proceedings of DARPA Speech and Natural Language Workshop,</booktitle>
<pages>357--361</pages>
<contexts>
<context position="14991" citStr="Paul and Baker 1992" startWordPosition="2515" endWordPosition="2518">e substrings, &amp;quot;to_&amp;quot; and &amp;quot;to_b&amp;quot;. Although there may be N(N + 1)/2 ways to pick i and j, it will turn out that we need only consider 2N — 1 of them when computing tf for all substrings. Nagao and Mori (1994) ran this procedure quite successfully on a large corpus of Japanese text. They report that it takes 0(N log N) time, assuming that the sort step performs 0(N log N) comparisons, and that each comparison takes constant time. While these are often reasonable assumptions, we have found that if the corpus contains long repeated substrings (e.g., duplicated articles), as our English corpus does (Paul and Baker 1992), then the sort can consume quadratic time, since each comparison can take order N time. Like Nagao and Mori (1994), we were also able to apply this procedure quite successfully to our Japanese corpus, but for the English corpus, after 50 hours of CPU time, we gave up and turned to Manber and Myers&apos;s (1990) algorithm, which took only two hours.&apos; Manber and Myers&apos; algorithm uses some clever, but difficult to describe, techniques to achieve 0(N log N) time, even for a corpus with long repeated substrings. For a corpus that would otherwise consume quadratic time, the Manber and Myers algorithm is</context>
<context position="46887" citStr="Paul and Baker 1992" startWordPosition="8201" endWordPosition="8204">DCI) and the Japanese collection consists of 216 million characters (436 thousand articles) of the CD-Mainichi Shimbun from 1991- 1995 (which are distributed in CD-ROM format). The English corpus was tokenized into words delimited by white space, whereas the Japanese corpus was tokenized into characters (typically two bytes each). Table 1 indicates that there are a large number of nontrivial classes in both corpora. The English corpus has more substrings per nontrivial class than the Japanese corpus. It has been noted elsewhere that the English corpus contains quite a few duplicated articles (Paul and Baker 1992). The duplicated articles could explain why there are so many substrings per nontrivial class in the English corpus when compared with the Japanese corpus. Table 1 Statistics of the English and Japanese corpora. Statistic Wall Street Journal Mainichi Shimbun N (corpus size in tokens) 49,810,697 words 215,789,699 characters V (vocabulary in types) 410,957 5,509 # articles 112,915 435,859 # nontrivial classes 16,519,064 82,442,441 # substrings in nontrivial classes 2,548,140,677 1,388,049,631 substrings per class (in nontrivial 154.3 16.8 classes) 17 Computational Linguistics Volume 27, Number 1</context>
</contexts>
<marker>Paul, Baker, 1992</marker>
<rawString>Paul, Douglas B. and Janet M. Baker. 1992. The design for the Wall Street Journal-based CSR corpus. In Proceedings of DARPA Speech and Natural Language Workshop, pages 357-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masayoshi Shibatani</author>
</authors>
<title>The Languages of Japan. Cambridge Language Surveys.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="56690" citStr="Shibatani 1990" startWordPosition="9981" endWordPosition="9982">igh for names, technical terminology, and good keywords for information retrieval. Table 7 suggests an intriguing pattern. Japanese uses different character sets for general vocabulary and loan words. Words that are high in MI tend to use the general vocabulary character sets (hiragana and kanji) whereas words that are high in RIDF tend to use the loan word character sets (katakana and English). (There is an important exception, though, for names, which will be discussed in the next subsection.) The character sets largely reflect the history of the language. Japanese uses four character sets (Shibatani 1990). Typically, functional words of Japanese origin are written in hiragana. Words that were borrowed from Chinese many hundreds of years ago are written in kanji. Loan words borrowed more recently from Western languages are written in katakana. Truly foreign words are written in the English character set (also known as romaji). We were pleasantly surprised to discover that MI and RIDF were distinguishing substrings on the basis of these character set distinctions. 21 Computational Linguistics Volume 27, Number 1 Table 5 Concordance of the phrase the Basic Law. Note that most of the instances of </context>
</contexts>
<marker>Shibatani, 1990</marker>
<rawString>Shibatani, Masayoshi. 1990. The Languages of Japan. Cambridge Language Surveys. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>Karen</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--1</pages>
<marker>Jones, Karen, 1972</marker>
<rawString>Sparck Jones, Karen. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>A statistical method for finding word boundaries</title>
<date>1990</date>
<booktitle>in Chinese text. Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--4</pages>
<contexts>
<context position="67160" citStr="Sproat and Shih (1990)" startWordPosition="11814" endWordPosition="11817">ps separated by the RIDE = 0.5 threshold. 3.4 Word Extraction RIDE and MI may be useful for word extraction. In many languages such as Chinese, Japanese, and Thai, word extraction is not an easy task, because, unlike English, many of these languages do not use delimiters between words. Automatic word extraction can be applied to the task of dictionary maintenance. Since most NLP applications (including word segmentation for these languages) are dictionary based, word extraction is very important for these languages. Nagao and Mori (1994) and Nagata (1996) proposed n-gram methods for Japanese. Sproat and Shih (1990) found MI to be useful for word extraction in Chinese. We performed the following simple experiment to see if both MI and RIDE could be useful for word extraction in Japanese. We extracted four random samples of 100 substrings each. The four samples cover all four combinations of high and low RIDE and high and low MI, where high is defined to be in the top 10% and low is defined to be in the bottom 10%. Then we manually scored each sample substring using our own subjective judgment. Substrings were labeled &amp;quot;good&amp;quot; (the substring is a word), &amp;quot;bad&amp;quot; (the substring is not a word), or &amp;quot;gray&amp;quot; (the ju</context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Sproat, Richard and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese and Oriental Languages, 4(4):336-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>On-line construction of suffix trees.</title>
<date>1995</date>
<journal>Algorithmica,</journal>
<pages>14--3</pages>
<contexts>
<context position="7620" citStr="Ukkonen 1995" startWordPosition="1227" endWordPosition="1228">og N) time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The algorithm groups the N(N + 1)/2 substrings into at most 2N — 1 equivalence classes. By grouping substrings in this way, many of the statistics of interest can be computed over the relatively small number of classes, which is manageable, rather than over the quadratic number of substrings, which would be prohibitive. The suffix array data structure (Manber and Myers 1990) was introduced as a database indexing technique. Suffix arrays can be viewed as a compact representation of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive introduction to suffix trees. Hui (1992) shows how to compute df for all substrings using generalized suffix trees. The major advantage of suffix arrays over suffix trees is space. The space requirements for suffix trees (but not for suffix arrays) grow with alphabet size: O(NIE 1) space, where I El is the alphabet size. The dependency on alphabet size is a serious issue for Japanese. Manber and Myers (1990) reported that suffix arrays are an order of magnitude more efficien</context>
</contexts>
<marker>Ukkonen, 1995</marker>
<rawString>Ukkonen, Esko. 1995. On-line construction of suffix trees. Algorithmica, 14(3):249-260.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>