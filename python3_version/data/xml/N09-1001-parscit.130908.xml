<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.96616">
Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
</title>
<author confidence="0.998507">
Fangzhong Su Katja Markert
</author>
<affiliation confidence="0.999406">
School of Computing School of Computing
University of Leeds University of Leeds
</affiliation>
<email confidence="0.997817">
fzsu@comp.leeds.ac.uk markert@comp.leeds.ac.uk
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998185">
We supplement WordNet entries with infor-
mation on the subjectivity of its word senses.
Supervised classifiers that operate on word
sense definitions in the same way that text
classifiers operate on web or newspaper texts
need large amounts of training data. The re-
sulting data sparseness problem is aggravated
by the fact that dictionary definitions are very
short. We propose a semi-supervised mini-
mum cut framework that makes use of both
WordNet definitions and its relation structure.
The experimental results show that it outper-
forms supervised minimum cut as well as stan-
dard supervised, non-graph classification, re-
ducing the error rate by 40%. In addition, the
semi-supervised approach achieves the same
results as the supervised framework with less
than 20% of the training data.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967714285714">
There is considerable academic and commercial in-
terest in processing subjective content in text, where
subjective content refers to any expression of a pri-
vate state such as an opinion or belief (Wiebe et
al., 2005). Important strands of work include the
identification of subjective content and the determi-
nation of its polarity, i.e. whether a favourable or
unfavourable opinion is expressed.
Automatic identification of subjective content of-
ten relies on word indicators, such as unigrams
(Pang et al., 2002) or predetermined sentiment lex-
ica (Wilson et al., 2005). Thus, the word positive
in the sentence “This deal is a positive development
for our company.” gives a strong indication that
</bodyText>
<page confidence="0.845783">
1
</page>
<bodyText confidence="0.955092433333333">
the sentence contains a favourable opinion. How-
ever, such word-based indicators can be misleading
for two reasons. First, contextual indicators such as
irony and negation can reverse subjectivity or po-
larity indications (Polanyi and Zaenen, 2004). Sec-
ond, different word senses of a single word can ac-
tually be of different subjectivity or polarity. A typ-
ical subjectivity-ambiguous word, i.e. a word that
has at least one subjective and at least one objec-
tive sense, is positive, as shown by the two example
senses given below.1
(1) positive, electropositive—having a positive electric
charge;“protons are positive” (objective)
(2) plus, positive—involving advantage or good; “a
plus (or positive) factor” (subjective)
We concentrate on this latter problem by automat-
ically creating lists of subjective senses, instead
of subjective words, via adding subjectivity labels
for senses to electronic lexica, using the exam-
ple of WordNet. This is important as the prob-
lem of subjectivity-ambiguity is frequent: We (Su
and Markert, 2008) find that over 30% of words
in our dataset are subjectivity-ambiguous. Informa-
tion on subjectivity of senses can also improve other
tasks such as word sense disambiguation (Wiebe
and Mihalcea, 2006). Moreover, Andreevskaia and
Bergler (2006) show that the performance of auto-
matic annotation of subjectivity at the word level can
be hurt by the presence of subjectivity-ambiguous
words in the training sets they use.
</bodyText>
<footnote confidence="0.973338">
1All examples in this paper are from WordNet 2.0.
</footnote>
<note confidence="0.943891">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1–9,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929166666667">
We propose a semi-supervised approach based on
minimum cut in a lexical relation graph to assign
subjectivity (subjective/objective) labels to word
senses.2 Our algorithm outperforms supervised min-
imum cuts and standard supervised, non-graph clas-
sification algorithms (like SVM), reducing the error
rate by up to 40%. In addition, the semi-supervised
approach achieves the same results as the supervised
framework with less than 20% of the training data.
Our approach also outperforms prior approaches to
the subjectivity recognition of word senses and per-
forms well across two different data sets.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Section 3
describes our proposed semi-supervised minimum
cut framework in detail. Section 4 presents the ex-
perimental results and evaluation, followed by con-
clusions and future work in Section 5.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9889538">
There has been a large and diverse body of research
in opinion mining, with most research at the text
(Pang et al., 2002; Pang and Lee, 2004; Popescu and
Etzioni, 2005; Ounis et al., 2006), sentence (Kim
and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff
et al., 2003; Yu and Hatzivassiloglou, 2003) or word
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kim and Hovy, 2004; Takamura et
al., 2005; Andreevskaia and Bergler, 2006; Kaji and
Kitsuregawa, 2007) level. An up-to-date overview is
given in Pang and Lee (2008).
Graph-based algorithms for classification into
subjective/objective or positive/negative language
units have been mostly used at the sentence and
document level (Pang and Lee, 2004; Agarwal and
Bhattacharyya, 2005; Thomas et al., 2006), instead
of aiming at dictionary annotation as we do. We
also cannot use prior graph construction methods
for the document level (such as physical proxim-
ity of sentences, used in Pang and Lee (2004)) at
the word sense level. At the word level Taka-
mura et al. (2005) use a semi-supervised spin model
for word polarity determination, where the graph
2It can be argued that subjectivity labels are maybe rather
more graded than the clear-cut binary distinction we assign.
However, in Su and Markert (2008a) as well as Wiebe and Mi-
halcea (2006) we find that human can assign the binary distinc-
tion to word senses with a high level of reliability.
is constructed using a variety of information such
as gloss co-occurrences and WordNet links. Apart
from using a different graph-based model from ours,
they assume that subjectivity recognition has already
been achieved prior to polarity recognition and test
against word lists containing subjective words only.
However, Kim and Hovy (2004) and Andreevskaia
and Bergler (2006) show that subjectivity recogni-
tion might be the harder problem with lower human
agreement and automatic performance. In addition,
we deal with classification at the word sense level,
treating also subjectivity-ambiguous words, which
goes beyond the work in Takamura et al. (2005).
Word Sense Level: There are three prior ap-
proaches addressing word sense subjectivity or po-
larity classification. Esuli and Sebastiani (2006) de-
termine the polarity (positive/negative/objective) of
word senses in WordNet. However, there is no eval-
uation as to the accuracy of their approach. They
then extend their work (Esuli and Sebastiani, 2007)
by applying the Page Rank algorithm to rank the
WordNet senses in terms of how strongly a sense
possesses a given semantic property (e.g., positive
or negative). Apart from us tackling subjectivity
instead of polarity, their Page Rank graph is also
constructed focusing on WordNet glosses (linking
glosses containing the same words), whereas we
concentrate on the use of WordNet relations.
Both Wiebe and Mihalcea (2006) and our prior
work (Su and Markert, 2008) present an annota-
tion scheme for word sense subjectivity and algo-
rithms for automatic classification. Wiebe and Mi-
halcea (2006) use an algorithm relying on distribu-
tional similarity and an independent, large manually
annotated opinion corpus (MPQA) (Wiebe et al.,
2005). One of the disadvantages of their algorithm is
that it is restricted to senses that have distributionally
similar words in the MPQA corpus, excluding 23%
of their test data from automatic classification. Su
and Markert (2008) present supervised classifiers,
which rely mostly on WordNet glosses and do not
effectively exploit WordNet’s relation structure.
</bodyText>
<sectionHeader confidence="0.999763" genericHeader="method">
3 Semi-Supervised Mincuts
</sectionHeader>
<subsectionHeader confidence="0.994474">
3.1 Minimum Cuts: The Main Idea
</subsectionHeader>
<bodyText confidence="0.9510385">
Binary classification with minimum cuts (Mincuts)
in graphs is based on the idea that similar items
</bodyText>
<page confidence="0.988659">
2
</page>
<bodyText confidence="0.99994495">
should be grouped in the same cut. All items in the
training/test data are seen as vertices in a graph with
undirected weighted edges between them specifying
how strong the similarity/association between two
vertices is. We use minimum s-t cuts: the graph con-
tains two particular vertices s (source, corresponds
to subjective) and t (sink, corresponds to objective)
and each vertex u is connected to s and t via a
weighted edge that can express how likely u is to
be classified as s or t in isolation.
Binary classification of the vertices is equivalent
to splitting the graph into two disconnected subsets
of all vertices, 5 and T with s ∈ 5 and t ∈ T.
This corresponds to removing a set of edges from
the graph. As similar items should be in the same
part of the split, the best split is one which removes
edges with low weights. In other words, a minimum
cut problem is to find a partition of the graph which
minimizes the following formula, where w(u, v) ex-
presses the weight of an edge between two vertices.
</bodyText>
<equation confidence="0.9967685">
W(5, T) = � w(u, v)
u∈S,v∈T
</equation>
<bodyText confidence="0.99774625">
Globally optimal minimum cuts can be found in
polynomial time and near-linear running time in
practice, using the maximum flow algorithm (Pang
and Lee, 2004; Cormen et al., 2002).
</bodyText>
<subsectionHeader confidence="0.99865">
3.2 Why might Semi-supervised Minimum
Cuts Work?
</subsectionHeader>
<bodyText confidence="0.999981891891892">
We propose semi-supervised mincuts for subjectiv-
ity recognition on senses for several reasons.
First, our problem satisfies two major conditions
necessary for using minimum cuts. It is a bi-
nary classification problem (subjective vs. objective
senses) as is needed to divide the graph into two
components. Our dataset also lends itself naturally
to s-t Mincuts as we have two different views on the
data. Thus, the edges of a vertex (=sense) to the
source/sink can be seen as the probability of a sense
being subjective or objective without taking similar-
ity to other senses into account, for example via con-
sidering only the sense gloss. In contrast, the edges
between two senses can incorporate the WordNet re-
lation hierarchy, which is a good source of similar-
ity for our problem as many WordNet relations are
subjectivity-preserving, i.e. if two senses are con-
nected via such a relation they are likely to be both
subjective or both objective.3 An example here is
the antonym relation, where two antonyms such as
good—morally admirable and evil, wicked—morally
bad or wrong are both subjective.
Second, Mincuts can be easily expanded into
a semi-supervised framework (Blum and Chawla,
2001). This is essential as the existing labeled
datasets for our problem are small. In addition,
glosses are short, leading to sparse high dimensional
vectors in standard feature representations. Also,
WordNet connections between different parts of the
WordNet hierarchy can also be sparse, leading to
relatively isolated senses in a graph in a supervised
framework. Semi-supervised Mincuts allow us to
import unlabeled data that can serve as bridges to
isolated components. More importantly, as the unla-
beled data can be chosen to be related to the labeled
and test data, they might help pull test data to the
right cuts (categories).
</bodyText>
<subsectionHeader confidence="0.999809">
3.3 Formulation of Semi-supervised Mincuts
</subsectionHeader>
<bodyText confidence="0.999403666666667">
The formulation of our semi-supervised Mincut for
sense subjectivity classification involves the follow-
ing steps, which we later describe in more detail.
</bodyText>
<listItem confidence="0.98981455">
1. We define two vertices s (source) and t (sink),
which correspond to the “subjective” and “ob-
jective” category, respectively. Following the
definition in Blum and Chawla (2001), we call
the vertices s and t classification vertices, and
all other vertices (labeled, test, and unlabeled
data) example vertices. Each example vertex
corresponds to one WordNet sense and is con-
nected to both s and t via a weighted edge. The
latter guarantees that the graph is connected.
2. For the test and unlabeled examples, we see
the edges to the classification vertices as the
probability of them being subjective/objective
disregarding other example vertices. We use a
supervised classifier to set these edge weights.
For the labeled training examples, they are con-
nected by edges with a high constant weight to
the classification vertices that they belong to.
3. WordNet relations are used to construct the
edges between two example vertices. Such
</listItem>
<footnote confidence="0.836087">
3See Kamps et al. (2004) for an early indication of such
properties for some WordNet relations.
</footnote>
<page confidence="0.997248">
3
</page>
<bodyText confidence="0.996736604651163">
edges can exist between any pair of example
vertices, for example between two unlabeled
examples.
4. After graph construction we then employ a
maximum-flow algorithm to find the minimum
s-t cuts of the graph. The cut in which the
source vertex s lies is classified as “subjective”,
and the cut in which the sink vertex t lies is “ob-
jective”.
We now describe the above steps in more detail.
Selection of unlabeled data: Random selection
of unlabeled data might hurt the performance of
Mincuts, as they might not be related to any sense in
our training/test data (denoted by A). Thus a basic
principle is that the selected unlabeled senses should
be related to the training/test data by WordNet rela-
tions. We therefore simply scan each sense in A, and
collect all senses related to it via one of the WordNet
relations in Table 1. All such senses that are not in
A are collected in the unlabeled data set.
Weighting of edges to the classification ver-
tices: The edge weight to s and t represents how
likely it is that an example vertex is initially put in
the cut in which s (subjective) or t (objective) lies.
For unlabeled and test vertices, we use a supervised
classifier (SVM4) with the labeled data as training
data to assign the edge weights. The SVM is also
used as a baseline and its features are described in
Section 4.3. As we do not wish the Mincut to re-
verse labels of the labeled training data, we assign a
high constant weight of 5 to the edge between a la-
beled vertex and its corresponding classification ver-
tex, and a low weight of 0.01 to the edge to the other
classification vertex.
Assigning weights to WordNet relations: We
connect two vertices that are linked by one of
the ten WordNet relations in Table 1 via an edge.
Not all WordNet relations we use are subjectivity-
preserving to the same degree: for example, hy-
ponyms (such as simpleton) of objective senses
(such as person) do not have to be objective. How-
ever, we aim for high graph connectivity and we
can assign different weights to different relations
</bodyText>
<footnote confidence="0.901688666666667">
4We employ LIBSVM, available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm/. Linear kernel and prob-
ability estimates are used in this work.
</footnote>
<bodyText confidence="0.961372375">
to reflect the degree to which they are subjectivity-
preserving. Therefore, we experiment with two
methods of weight assignment. Method 1 (NoSL)
assigns the same constant weight of 1.0 to all Word-
Net relations.
Method 2 (SL) reflects different degrees of pre-
serving subjectivity. To do this, we adapt an un-
supervised method of generating a large noisy set
of subjective and objective senses from our previ-
ous work (Su and Markert, 2008). This method
uses a list of subjective words (SL)5 to classify each
WordNet sense with at least two subjective words
in its gloss as subjective and all other senses as ob-
jective. We then count how often two senses re-
lated via a given relation have the same or a dif-
ferent subjectivity label. The weight is computed
</bodyText>
<tableCaption confidence="0.964602666666667">
by #same/(#same+#different). Results are listed in
Table 1.
Table 1: Relation weights (Method 2)
</tableCaption>
<table confidence="0.999705727272727">
Method #Same #Different Weight
Antonym 2,808 309 0.90
Similar-to 6,887 1,614 0.81
Derived-from 4,630 947 0.83
Direct-Hypernym 71,915 8,600 0.89
Direct-Hyponym 71,915 8,600 0.89
Attribute 350 109 0.76
Also-see 1,037 337 0.75
Extended-Antonym 6,917 1,651 0.81
Domain 4,387 892 0.83
Domain-member 4,387 892 0.83
</table>
<figureCaption confidence="0.89318">
Example graph: An example graph is shown in
Figure 1. The three example vertices correspond
</figureCaption>
<bodyText confidence="0.834991285714286">
to the senses religious—extremely scrupulous and
conscientious, scrupulous—having scruples; aris-
ing from a sense of right and wrong; principled;
and flicker, spark, glint—a momentary flash of light
respectively. The vertex “scrupulous” is unlabeled
data derived from the vertex “religious”(a test item)
by the relation “similar-to”.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="evaluation">
4 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.766234">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999255">
We conduct the experiments on two different gold
standard datasets. One is the Micro-WNOp corpus,
</bodyText>
<footnote confidence="0.998937">
5Available at http://www.cs.pitt.edu/mpqa
</footnote>
<page confidence="0.988926">
4
</page>
<figureCaption confidence="0.999874">
Figure 1: Graph of Word Senses
</figureCaption>
<bodyText confidence="0.999704083333333">
which is representative of the part-of-speech distri-
bution in WordNet 6. It includes 298 words with
703 objective and 358 subjective WordNet senses.
The second one is the dataset created by Wiebe
and Mihalcea (2006).7 It only contains noun and
verb senses, and includes 60 words with 236 ob-
jective and 92 subjective WordNet senses. As the
Micro-WNOp set is larger and also contains adjec-
tive and adverb senses, we describe our results in
more detail on that corpus in the Section 4.3 and
4.4. In Section 4.5, we shortly discuss results on
Wiebe&amp;Mihalcea’s dataset.
</bodyText>
<subsectionHeader confidence="0.960343">
4.2 Baseline and Evaluation
</subsectionHeader>
<bodyText confidence="0.999991571428572">
We compare to a baseline that assigns the most
frequent category objective to all senses, which
achieves an accuracy of 66.3% and 72.0% on Micro-
WNOp and Wiebe&amp;Mihalcea’s dataset respectively.
We use the McNemar test at the significance level of
5% for significance statements. All evaluations are
carried out by 10-fold cross-validation.
</bodyText>
<subsectionHeader confidence="0.999164">
4.3 Standard Supervised Learning
</subsectionHeader>
<bodyText confidence="0.9986225">
We use an SVM classifier to compare our proposed
semi-supervised Mincut approach to a reasonable
</bodyText>
<footnote confidence="0.992522">
6Available at http://www.comp.leeds.ac.uk/
markert/data. This dataset was first used with a different
annotation scheme in Esuli and Sebastiani (2007) and we also
used it in Su and Markert (2008).
7Available at http://www.cs.pitt.edu/˜wiebe/
pubs/papers/goldstandard.total.acl06.
</footnote>
<bodyText confidence="0.999100023255814">
baseline.8 Three different feature types are used.
Lexical Features (L): a bag-of-words representa-
tion of the sense glosses with stop word filtering.
Relation Features (R): First, we use two features
for each of the ten WordNet relations in Table 1, de-
scribing how many relations of that type the sense
has to senses in the subjective or objective part of the
training set, respectively. This provides a non-graph
summary of subjectivity-preserving links. Second,
we manually collected a small set (denoted by
5ubj5et) of seven subjective verb and noun senses
which are close to the root in WordNet’s hypernym
tree. A typical example element of 5ubj5et is psy-
chological feature —a feature of the mental life of a
living organism, which indicates subjectivity for its
hyponyms such as hope — the general feeling that
some desire will be fulfilled. A binary feature de-
scribes whether a noun/verb sense is a hyponym of
an element of 5ubj5et.
Monosemous Feature (M): for each sense, we
scan if a monosemous word is part of its synset. If
so, we further check if the monosemous word is col-
lected in the subjective word list (SL). The intuition
is that if a monosemous word is subjective, obvi-
ously its (single) sense is subjective. For example,
the sense uncompromising, inflexible—not making
concessions is subjective, as “uncompromising” is
a monosemous word and also in SL.
We experiment with different combinations of
features and the results are listed in Table 2, prefixed
by “SVM”. All combinations perform significantly
better than the more frequent category baseline and
similarly to the supervised Naive Bayes classifier
(see S&amp;M in Table 2) we used in Su and Mark-
ert (2008). However, improvements by adding more
features remain small.
In addition, we compare to a supervised classifier
(see Lesk in Table 2) that just assigns each sense
the subjectivity label of its most similar sense in
the training data, using Lesk’s similarity measure
from Pedersen’s WordNet similarity package9. We
use Lesk as it is one of the few measures applicable
across all parts-of-speech.
</bodyText>
<footnote confidence="0.84924725">
8This SVM is also used to provide the edge weights to the
classification vertices in the Mincut approach.
9Available at http://www.d.umn.edu/˜tpederse/
similarity.html.
</footnote>
<figure confidence="0.988286">
subjective 0.83 scrupulous 0.17 objective
0.16 0.84
0.24 0.76
similar-to
0.81
religious
flicker
</figure>
<page confidence="0.937891">
5
</page>
<tableCaption confidence="0.999231">
Table 2: Results of SVM and Mincuts with different settings of feature
</tableCaption>
<table confidence="0.998031333333333">
Method Subjective Objective Accuracy
Precision Recall F-score Precision Recall F-score
Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3%
S&amp;M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%
Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4%
SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4%
L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7%
L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5%
SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1%
LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2%
LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2%
SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5%
LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7%
LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6%
SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3%
LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6%
LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4%
1 L, R and M correspond to the lexical, relation and monosemous features respectively.
</table>
<bodyText confidence="0.582551857142857">
2 SVM-L corresponds to using lexical features only for the SVM classifier. Likewise, SVM-
LRM corresponds to using a combination for lexical, relation, and monosemous features
for the SVM classifier.
3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier,
and subjective list (SL) to infer the weight of WordNet relations. Likewise, LM-NoSL
corresponds to the Mincut algorithm that uses lexical and monosemous features for the
SVM, and predefined constants for WordNet relations (without subjective list).
</bodyText>
<subsectionHeader confidence="0.999143">
4.4 Semi-supervised Graph Mincuts
</subsectionHeader>
<bodyText confidence="0.999964439024391">
Using our formulation in Section 3.3, we import
3,220 senses linked by the ten WordNet relations to
any senses in Micro-WNOp as unlabeled data. We
construct edge weights to classification vertices us-
ing the SVM discussed above and use WordNet re-
lations for links between example vertices, weighted
by either constants (NoSL) or via the method illus-
trated in Table 1 (SL). The results are also summa-
rized in Table 2. Semi-supervised Mincuts always
significantly outperform the corresponding SVM
classifiers, regardless of whether the subjectivity list
is used for setting edge weights. We can also see
that we achieve good results without using any other
knowledge sources (setting LR-NoSL).
The example in Figure 1 explains why semi-
supervised Mincuts outperforms the supervised ap-
proach. The vertex “religious” is initially assigned
the subjective/objective probabilities 0.24/0.76 by
the SVM classifier, leading to a wrong classification.
However, in our graph-based Mincut framework, the
vertex “religious” might link to other vertices (for
example, it links to the vertex “scrupulous” in the
unlabeled data by the relation “similar-to”). The
mincut algorithm will put vertices “religious” and
“scrupulous” in the same cut (subjective category) as
this results in the least cost 0.93 (ignoring the cost of
assigning the unrelated sense of “flicker”). In other
words, the edges between the vertices are likely to
correct some initially wrong classification and pull
the vertices into the right cuts.
In the following we will analyze the best mini-
mum cut algorithm LRM-SL in more detail. We
measure its accuracy for each part-of-speech in the
Micro-WNOp dataset. The number of noun, adjec-
tive, adverb and verb senses in Micro-WNOp is 484,
265, 31 and 281, respectively. The result is listed
in Table 3. The significantly better performance of
semi-supervised mincuts holds across all parts-of-
speech but the small set of adverbs, where there is
no significant difference between the baseline, SVM
and the Mincut algorithm.
</bodyText>
<page confidence="0.999683">
6
</page>
<tableCaption confidence="0.985287">
Table 3: Accuracy for Different Part-Of-Speech
</tableCaption>
<table confidence="0.99959725">
Method Noun Adjective Adverb Verb
Baseline 76.9% 61.1% 77.4% 72.6%
SVM 81.4% 63.4% 83.9% 75.1%
Mincut 88.6% 78.9% 77.4% 84.0%
</table>
<bodyText confidence="0.998817411764706">
We will now investigate how LRM-SL performs
with different sizes of labeled and unlabeled data.
All learning curves are generated via averaging 10
learning curves from 10-fold cross-validation.
Performance with different sizes of labeled data:
we randomly generate subsets of labeled data A1,
A2... A,, and guarantee that A1 C A2... C A,.
Results for the best SVM (LRM) and the best min-
imum cut (LRM-SL) are listed in Table 4, and the
corresponding learning curve is shown in Figure 2.
As can be seen, the semi-supervised Mincuts is
consistently better than SVM. Moreover, the semi-
supervised Mincut with only 200 labeled data items
performs even better than SVM with 954 training
items (78.9% vs 75.3%), showing that our semi-
supervised framework allows for a training data re-
duction of more than 80%.
</bodyText>
<figure confidence="0.9339053">
89
86
83
80
77
74
71
68
100 200 300 400 500 600 700 800 900 1000
Size of Labeled Data
</figure>
<figureCaption confidence="0.994815">
Figure 2: Learning curve with different sizes of labeled
data
</figureCaption>
<bodyText confidence="0.99996625">
The results are listed in Table 5 and Table 6 re-
spectively. The corresponding learning curves are
shown in Figure 3. We see that performance im-
proves with the increase of unlabeled data. In addi-
tion, the curves seem to converge when the size of
unlabeled data is larger than 3,000. From the results
in Tabel 5 one can also see that hyponymy is the re-
lation accounting for the largest increase.
</bodyText>
<figure confidence="0.366195333333333">
Accuracy(%)
Mincuts
SVM
</figure>
<tableCaption confidence="0.999374">
Table 4: Accuracy with different sizes of labeled data
</tableCaption>
<table confidence="0.99980925">
# labeled data SVM Mincuts
100 69.1% 72.2%
200 72.6% 78.9%
400 74.4% 82.7%
600 75.5% 83.7%
800 76.0% 84.1%
900 75.6% 84.8%
954 (all) 75.3% 84.6%
</table>
<bodyText confidence="0.9609065">
Performance with different sizes of unlabeled
data: We propose two different settings.
Option1: Use a subset of the ten relations to
generate the unlabeled data (and edges between
example vertices). For example, we first use
{antonym, similar-to} only to obtain a unlabeled
dataset U1, then use a larger subset of the relations
like {antonym, similar-to, direct-hyponym, direct-
hypernym} to generate another unlabeled dataset
U2, and so forth. Obviously, UZ is a subset of UZ+1.
Option2: Use all the ten relations to generate the
unlabeled data U. We then randomly select subsets
of U, such as subset U1, U2 and U3, and guarantee
that U1 C U2 C U3 C ... U.
</bodyText>
<tableCaption confidence="0.962999">
Table 6: Accuracy with different sizes of unlabeled data
(random selection)
</tableCaption>
<table confidence="0.99842825">
# unlabeled data Accuracy
0 75.9%
200 76.5%
500 78.6%
1000 80.2%
2000 82.8%
3000 84.0%
3220 84.6%
</table>
<bodyText confidence="0.999929444444445">
Furthermore, these results also show that a super-
vised mincut without unlabeled data performs only
on a par with other supervised classifiers (75.9%).
The reason is that if we exclude the unlabeled data,
there are only 67 WordNet relations/edges between
senses in the small Micro-WNOp dataset. In con-
trast, the use of unlabeled data adds more edges
(4,586) to the graph, which strongly affects the
graph cut partition (see also Figure 1).
</bodyText>
<subsectionHeader confidence="0.984103">
4.5 Comparison to Prior Approaches
</subsectionHeader>
<bodyText confidence="0.9872705">
In our previous work (Su and Markert, 2008), we re-
port 76.9% as the best accuracy on the same Micro-
</bodyText>
<page confidence="0.999732">
7
</page>
<tableCaption confidence="0.999876">
Table 5: Accuracy with different sizes of unlabeled data from WordNet relation
</tableCaption>
<table confidence="0.940824538461538">
Relation # unlabeled data Accuracy
{∅} 0 75.3%
{similar-to} 418 79.1%
{similar-to, antonym} 514 79.5%
{similar-to, antonym, direct-hypernym, direct- 2,721 84.4%
hyponym}
{similar-to, antonym, direct-hypernym, direct- 3,004 84.4%
hyponym, also-see, extended-antonym}
{similar-to, antonym, direct-hypernym, direct- 3,220 84.6%
hyponym, also-see, extended-antonym, derived-from,
attribute, domain, domain-member}
0 500 1000 1500 2000 2500 3000 3500
Size of Unlabeled Data
</table>
<figureCaption confidence="0.9954575">
Figure 3: Learning curve with different sizes of unlabeled
data
</figureCaption>
<bodyText confidence="0.999913375">
WNOp dataset used in the previous sections, using a
supervised Naive Bayes (S&amp;M in Tabel 2). Our best
result from Mincuts is significantly better at 84.6%
(see LRM-SL in Table 2).
For comparison to Wiebe and Mihalcea (2006),
we use their dataset for testing, henceforth called
Wiebe (see Section 4.1 for a description). Wiebe
and Mihalcea (2006) report their results in precision
and recall curves for subjective senses, such as a pre-
cision of about 55% at a recall of 50% for subjective
senses. Their F-score for subjective senses seems to
remain relatively static at 0.52 throughout their pre-
cision/recall curve.
We run our best Mincut LRM-SL algorithm with
two different settings on Wiebe. Using Micro-
WNOp as training set and Wiebe as test set, we
achieve an accuracy of 83.2%, which is similar to the
results on the Micro-WNOp dataset. At the recall of
50% we achieve a precision of 83.6% (in compari-
son to their precision of 55% at the same recall). Our
F-score is 0.63 (vs. 0.52).
To check whether the high performance is just due
to our larger training set, we also conduct 10-fold
cross-validation on Wiebe. The accuracy achieved
is 81.1% and the F-score 0.56 (vs. 0.52), suggesting
that our algorithm performs better. Our algorithm
can be used on all WordNet senses whereas theirs is
restricted to senses that have distributionally similar
words in the MPQA corpus (see Section 2). How-
ever, they use an unsupervised algorithm i.e. they
do not need labeled word senses, although they do
need a large, manually annotated opinion corpus.
</bodyText>
<sectionHeader confidence="0.990525" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999926888888889">
We propose a semi-supervised minimum cut algo-
rithm for subjectivity recognition on word senses.
The experimental results show that our proposed ap-
proach is significantly better than a standard super-
vised classification framework as well as a super-
vised Mincut. Overall, we achieve a 40% reduction
in error rates (from an error rate of about 25% to an
error rate of 15%). To achieve the results of standard
supervised approaches with our model, we need less
than 20% of their training data. In addition, we com-
pare our algorithm to previous state-of-the-art ap-
proaches, showing that our model performs better
on the same datasets.
Future work will explore other graph construc-
tion methods, such as the use of morphological re-
lations as well as thesaurus and distributional sim-
ilarity measures. We will also explore other semi-
supervised algorithms.
</bodyText>
<figure confidence="0.995440545454545">
Accuracy(%)
89
87
85
83
81
79
77
75
Option1
Option2
</figure>
<page confidence="0.986107">
8
</page>
<sectionHeader confidence="0.988705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988773">
Alekh Agarwal and Pushpak Bhattacharyya. 2005. Sen-
timent Analysis: A new Approach for Effective Use
of Linguistic Knowledge and Exploiting Similarities
in a Set of Documents to be Classified. Proceedings of
ICON’05.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for Fuzzy Sentiment: Sentiment Tag Extrac-
tion from WordNet Glosses. Proceedings of EACL’06.
Avrim Blum and Shuchi Chawla. 2001. Learning from
Labeled and Unlabeled Data using Graph Mincuts.
Proceedings of ICML’01.
Thomas Cormen, Charles Leiserson, Ronald Rivest and
Clifford Stein. 2002. Introduction to Algorithms.
Second Edition, the MIT Press.
Kushal Dave, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
Proceedings of WWW’03.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. Proceedings of LREC’06.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-
ing WordNet Synsets: An application to Opinion Min-
ing. Proceedings of ACL’07.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. Proceedings of ACL’97.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. Proceedings of
EMNLP’07.
Japp Kamps, Maarten Marx, Robert Mokken, and
Maarten de Rijke. 2004. Using WordNet to Measure
Semantic Orientation of Adjectives. Proceedings of
LREC’04.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
Sentiment of Opinions. Proceedings of COLING’04.
Soo-Min Kim and Eduard Hovy. 2005. Automatic De-
tection of Opinion Bearing Words and Sentences. Pro-
ceedings of ICJNLP’05.
Taku Kudo and Yuji Matsumoto. 2004. A Boosting
Algorithm for Classification of Semi-structured Text.
Proceedings of EMNLP’04.
Iadh Ounis, Maarten de Rijke, Craig Macdonald, Gilad
Mishne and Ian Soboroff. 2006. Overview of the
TREC-2006 Blog Track. Proceedings of TREC’06.
Bo Pang and Lillian Lee. 2004. A Sentiment Education:
Sentiment Analysis Using Subjectivity summarization
Based on Minimum Cuts. Proceedings of ACL’04.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. Proceedings of
EMNLP’02.
Livia Polanyi and Annie Zaenen. 2004. Contextual Va-
lence Shifters. Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text: The-
ories and Applications.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing Product Features and Opinions from Reviews Pro-
ceedings of EMNLP’05.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. Proceedings of CoNLL’03
Fangzhong Su and Katja Markert. 2008. From Words
to Senses: A Case Study in Subjectivity Recognition.
Proceedings of COLING’08.
Fangzhong Su and Katja Markert. 2008a. Elicit-
ing Subjectivity and Polarity Judgements on Word
Senses. Proceedings of COLING’08 workshop of Hu-
man Judgements in Computational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting Semantic Orientations of Words us-
ing Spin Model. Proceedings of ACL’05.
Matt Thomas, Bo Pang and Lilian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. Proceedings of
EMNLP’06.
Peter Turney. 2002. Thumbs up or Thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. Proceedings ofACL’02.
Peter Turney and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orienta-
tion from Association. ACM Transaction on Informa-
tion Systems.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
Answering Opinion Questions: Separating Facts from
Opinions and Identifying the Polarity of Opinion Sen-
tences. Proceedings of EMNLP’03.
Janyce Wiebe and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings ofACL’06.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity in
Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP’05.
</reference>
<page confidence="0.9971">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980868">
<title confidence="0.998044">Subjectivity Recognition on Word Senses via Semi-supervised Mincuts</title>
<author confidence="0.998097">Fangzhong Su Katja Markert</author>
<affiliation confidence="0.999871">School of Computing School of Computing University of Leeds University of Leeds</affiliation>
<email confidence="0.994416">fzsu@comp.leeds.ac.ukmarkert@comp.leeds.ac.uk</email>
<abstract confidence="0.999485842105263">We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alekh Agarwal</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Sentiment Analysis: A new Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be Classified.</title>
<date>2005</date>
<booktitle>Proceedings of ICON’05.</booktitle>
<contexts>
<context position="5065" citStr="Agarwal and Bhattacharyya, 2005" startWordPosition="771" endWordPosition="774">002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the bi</context>
</contexts>
<marker>Agarwal, Bhattacharyya, 2005</marker>
<rawString>Alekh Agarwal and Pushpak Bhattacharyya. 2005. Sentiment Analysis: A new Approach for Effective Use of Linguistic Knowledge and Exploiting Similarities in a Set of Documents to be Classified. Proceedings of ICON’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining WordNet for Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses.</title>
<date>2006</date>
<booktitle>Proceedings of EACL’06.</booktitle>
<contexts>
<context position="3005" citStr="Andreevskaia and Bergler (2006)" startWordPosition="453" endWordPosition="456">tive—involving advantage or good; “a plus (or positive) factor” (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous. Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006). Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. 1All examples in this paper are from WordNet 2.0. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms sup</context>
<context position="4759" citStr="Andreevskaia and Bergler, 2006" startWordPosition="728" endWordPosition="731"> proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a</context>
<context position="6107" citStr="Andreevskaia and Bergler (2006)" startWordPosition="942" endWordPosition="945">ls are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al. (2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their wor</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2006. Mining WordNet for Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses. Proceedings of EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<title>Learning from Labeled and Unlabeled Data using Graph Mincuts.</title>
<date>2001</date>
<booktitle>Proceedings of ICML’01.</booktitle>
<contexts>
<context position="10450" citStr="Blum and Chawla, 2001" startWordPosition="1652" endWordPosition="1655">ses into account, for example via considering only the sense gloss. In contrast, the edges between two senses can incorporate the WordNet relation hierarchy, which is a good source of similarity for our problem as many WordNet relations are subjectivity-preserving, i.e. if two senses are connected via such a relation they are likely to be both subjective or both objective.3 An example here is the antonym relation, where two antonyms such as good—morally admirable and evil, wicked—morally bad or wrong are both subjective. Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001). This is essential as the existing labeled datasets for our problem are small. In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations. Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework. Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data </context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>Avrim Blum and Shuchi Chawla. 2001. Learning from Labeled and Unlabeled Data using Graph Mincuts. Proceedings of ICML’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Cormen</author>
<author>Charles Leiserson</author>
<author>Ronald Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms. Second Edition,</title>
<date>2002</date>
<publisher>the MIT Press.</publisher>
<contexts>
<context position="9207" citStr="Cormen et al., 2002" startWordPosition="1451" endWordPosition="1454">subsets of all vertices, 5 and T with s ∈ 5 and t ∈ T. This corresponds to removing a set of edges from the graph. As similar items should be in the same part of the split, the best split is one which removes edges with low weights. In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices. W(5, T) = � w(u, v) u∈S,v∈T Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002). 3.2 Why might Semi-supervised Minimum Cuts Work? We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. First, our problem satisfies two major conditions necessary for using minimum cuts. It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components. Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data. Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objective without taking si</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2002</marker>
<rawString>Thomas Cormen, Charles Leiserson, Ronald Rivest and Clifford Stein. 2002. Introduction to Algorithms. Second Edition, the MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David Pennock</author>
</authors>
<title>Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews.</title>
<date>2003</date>
<booktitle>Proceedings of WWW’03.</booktitle>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David Pennock. 2003. Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews. Proceedings of WWW’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>Proceedings of LREC’06.</booktitle>
<contexts>
<context position="6530" citStr="Esuli and Sebastiani (2006)" startWordPosition="1005" endWordPosition="1008">jectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al. (2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations. Bo</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining. Proceedings of LREC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>PageRanking WordNet Synsets: An application to Opinion Mining.</title>
<date>2007</date>
<booktitle>Proceedings of ACL’07.</booktitle>
<contexts>
<context position="6737" citStr="Esuli and Sebastiani, 2007" startWordPosition="1037" endWordPosition="1040">ow that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al. (2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations. Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification. Wiebe and Mihalcea (2006) use an a</context>
<context position="17489" citStr="Esuli and Sebastiani (2007)" startWordPosition="2797" endWordPosition="2800">aluation We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&amp;Mihalcea’s dataset respectively. We use the McNemar test at the significance level of 5% for significance statements. All evaluations are carried out by 10-fold cross-validation. 4.3 Standard Supervised Learning We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable 6Available at http://www.comp.leeds.ac.uk/ markert/data. This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008). 7Available at http://www.cs.pitt.edu/˜wiebe/ pubs/papers/goldstandard.total.acl06. baseline.8 Three different feature types are used. Lexical Features (L): a bag-of-words representation of the sense glosses with stop word filtering. Relation Features (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjective or objective part of the training set, respectively. This provides a non-graph summary of subjectivity-preserving links. Second, we man</context>
</contexts>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2007. PageRanking WordNet Synsets: An application to Opinion Mining. Proceedings of ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>Proceedings of ACL’97.</booktitle>
<contexts>
<context position="4658" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="712" endWordPosition="715">mainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences,</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen McKeown. 1997. Predicting the Semantic Orientation of Adjectives. Proceedings of ACL’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents.</title>
<date>2007</date>
<booktitle>Proceedings of EMNLP’07.</booktitle>
<contexts>
<context position="4788" citStr="Kaji and Kitsuregawa, 2007" startWordPosition="732" endWordPosition="735">m cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model f</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents. Proceedings of EMNLP’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Japp Kamps</author>
<author>Maarten Marx</author>
<author>Robert Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using WordNet to Measure Semantic Orientation of Adjectives.</title>
<date>2004</date>
<booktitle>Proceedings of LREC’04.</booktitle>
<marker>Kamps, Marx, Mokken, de Rijke, 2004</marker>
<rawString>Japp Kamps, Maarten Marx, Robert Mokken, and Maarten de Rijke. 2004. Using WordNet to Measure Semantic Orientation of Adjectives. Proceedings of LREC’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the Sentiment of Opinions.</title>
<date>2004</date>
<booktitle>Proceedings of COLING’04.</booktitle>
<contexts>
<context position="4704" citStr="Kim and Hovy, 2004" startWordPosition="720" endWordPosition="723">sses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sens</context>
<context position="6071" citStr="Kim and Hovy (2004)" startWordPosition="937" endWordPosition="940">d that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al. (2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their </context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the Sentiment of Opinions. Proceedings of COLING’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Detection of Opinion Bearing Words and Sentences.</title>
<date>2005</date>
<booktitle>Proceedings of ICJNLP’05.</booktitle>
<contexts>
<context position="4535" citStr="Kim and Hovy, 2005" startWordPosition="694" endWordPosition="697">hes to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as </context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic Detection of Opinion Bearing Words and Sentences. Proceedings of ICJNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A Boosting Algorithm for Classification of Semi-structured Text.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP’04.</booktitle>
<contexts>
<context position="4561" citStr="Kudo and Matsumoto, 2004" startWordPosition="698" endWordPosition="701">ity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use </context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2004. A Boosting Algorithm for Classification of Semi-structured Text. Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iadh Ounis</author>
</authors>
<title>Maarten de Rijke, Craig Macdonald, Gilad Mishne and Ian Soboroff.</title>
<date>2006</date>
<booktitle>Overview of the TREC-2006 Blog Track. Proceedings of TREC’06.</booktitle>
<marker>Ounis, 2006</marker>
<rawString>Iadh Ounis, Maarten de Rijke, Craig Macdonald, Gilad Mishne and Ian Soboroff. 2006. Overview of the TREC-2006 Blog Track. Proceedings of TREC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A Sentiment Education: Sentiment Analysis Using Subjectivity summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>Proceedings of ACL’04.</booktitle>
<contexts>
<context position="4457" citStr="Pang and Lee, 2004" startWordPosition="681" endWordPosition="684">ess than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattachary</context>
<context position="9185" citStr="Pang and Lee, 2004" startWordPosition="1447" endWordPosition="1450">to two disconnected subsets of all vertices, 5 and T with s ∈ 5 and t ∈ T. This corresponds to removing a set of edges from the graph. As similar items should be in the same part of the split, the best split is one which removes edges with low weights. In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices. W(5, T) = � w(u, v) u∈S,v∈T Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002). 3.2 Why might Semi-supervised Minimum Cuts Work? We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. First, our problem satisfies two major conditions necessary for using minimum cuts. It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components. Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data. Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objec</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A Sentiment Education: Sentiment Analysis Using Subjectivity summarization Based on Minimum Cuts. Proceedings of ACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Information Retrieval</booktitle>
<pages>2--1</pages>
<contexts>
<context position="4850" citStr="Pang and Lee (2008)" startWordPosition="743" endWordPosition="746"> and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be arg</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval 2(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP’02.</booktitle>
<contexts>
<context position="1545" citStr="Pang et al., 2002" startWordPosition="229" endWordPosition="232">ieves the same results as the supervised framework with less than 20% of the training data. 1 Introduction There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005). Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed. Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005). Thus, the word positive in the sentence “This deal is a positive development for our company.” gives a strong indication that 1 the sentence contains a favourable opinion. However, such word-based indicators can be misleading for two reasons. First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004). Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at l</context>
<context position="4437" citStr="Pang et al., 2002" startWordPosition="677" endWordPosition="680">ed framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Aga</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. Proceedings of EMNLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual Valence Shifters.</title>
<date>2004</date>
<booktitle>Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</booktitle>
<contexts>
<context position="1977" citStr="Polanyi and Zaenen, 2004" startWordPosition="296" endWordPosition="299">ts polarity, i.e. whether a favourable or unfavourable opinion is expressed. Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005). Thus, the word positive in the sentence “This deal is a positive development for our company.” gives a strong indication that 1 the sentence contains a favourable opinion. However, such word-based indicators can be misleading for two reasons. First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004). Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive—having a positive electric charge;“protons are positive” (objective) (2) plus, positive—involving advantage or good; “a plus (or positive) factor” (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via a</context>
</contexts>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2004. Contextual Valence Shifters. Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<date>2005</date>
<booktitle>Extracting Product Features and Opinions from Reviews Proceedings of EMNLP’05.</booktitle>
<contexts>
<context position="4484" citStr="Popescu and Etzioni, 2005" startWordPosition="685" endWordPosition="688">training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 20</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting Product Features and Opinions from Reviews Proceedings of EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning Subjective Nouns using Extraction Pattern Bootstrapping.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL’03</booktitle>
<contexts>
<context position="4582" citStr="Riloff et al., 2003" startWordPosition="702" endWordPosition="705">nses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construct</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning Subjective Nouns using Extraction Pattern Bootstrapping. Proceedings of CoNLL’03</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>From Words to Senses: A Case Study in Subjectivity Recognition.</title>
<date>2008</date>
<booktitle>Proceedings of COLING’08.</booktitle>
<contexts>
<context position="2763" citStr="Su and Markert, 2008" startWordPosition="417" endWordPosition="420">t least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive—having a positive electric charge;“protons are positive” (objective) (2) plus, positive—involving advantage or good; “a plus (or positive) factor” (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous. Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006). Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. 1All examples in this paper are from WordNet 2.0. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1–9, Boulder, Colorado, June</context>
<context position="5589" citStr="Su and Markert (2008" startWordPosition="860" endWordPosition="863">tly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human a</context>
<context position="7200" citStr="Su and Markert, 2008" startWordPosition="1111" endWordPosition="1114">ive) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations. Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification. Wiebe and Mihalcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005). One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification. Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit Wo</context>
<context position="14916" citStr="Su and Markert, 2008" startWordPosition="2401" endWordPosition="2404">ssign different weights to different relations 4We employ LIBSVM, available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm/. Linear kernel and probability estimates are used in this work. to reflect the degree to which they are subjectivitypreserving. Therefore, we experiment with two methods of weight assignment. Method 1 (NoSL) assigns the same constant weight of 1.0 to all WordNet relations. Method 2 (SL) reflects different degrees of preserving subjectivity. To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008). This method uses a list of subjective words (SL)5 to classify each WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective. We then count how often two senses related via a given relation have the same or a different subjectivity label. The weight is computed by #same/(#same+#different). Results are listed in Table 1. Table 1: Relation weights (Method 2) Method #Same #Different Weight Antonym 2,808 309 0.90 Similar-to 6,887 1,614 0.81 Derived-from 4,630 947 0.83 Direct-Hypernym 71,915 8,600 0.89 Direct-Hyponym 71,915 8,600 0.89 Attribute </context>
<context position="17534" citStr="Su and Markert (2008)" startWordPosition="2807" endWordPosition="2810">most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&amp;Mihalcea’s dataset respectively. We use the McNemar test at the significance level of 5% for significance statements. All evaluations are carried out by 10-fold cross-validation. 4.3 Standard Supervised Learning We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable 6Available at http://www.comp.leeds.ac.uk/ markert/data. This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008). 7Available at http://www.cs.pitt.edu/˜wiebe/ pubs/papers/goldstandard.total.acl06. baseline.8 Three different feature types are used. Lexical Features (L): a bag-of-words representation of the sense glosses with stop word filtering. Relation Features (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjective or objective part of the training set, respectively. This provides a non-graph summary of subjectivity-preserving links. Second, we manually collected a small set (denoted by 5ubj5</context>
<context position="19298" citStr="Su and Markert (2008)" startWordPosition="3087" endWordPosition="3091">the monosemous word is collected in the subjective word list (SL). The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective. For example, the sense uncompromising, inflexible—not making concessions is subjective, as “uncompromising” is a monosemous word and also in SL. We experiment with different combinations of features and the results are listed in Table 2, prefixed by “SVM”. All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&amp;M in Table 2) we used in Su and Markert (2008). However, improvements by adding more features remain small. In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Lesk’s similarity measure from Pedersen’s WordNet similarity package9. We use Lesk as it is one of the few measures applicable across all parts-of-speech. 8This SVM is also used to provide the edge weights to the classification vertices in the Mincut approach. 9Available at http://www.d.umn.edu/˜tpederse/ similarity.html. subjective 0.83 scrupulous 0.17 obj</context>
<context position="26611" citStr="Su and Markert, 2008" startWordPosition="4270" endWordPosition="4273">tion) # unlabeled data Accuracy 0 75.9% 200 76.5% 500 78.6% 1000 80.2% 2000 82.8% 3000 84.0% 3220 84.6% Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%). The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small Micro-WNOp dataset. In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1). 4.5 Comparison to Prior Approaches In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro7 Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Relation # unlabeled data Accuracy {∅} 0 75.3% {similar-to} 418 79.1% {similar-to, antonym} 514 79.5% {similar-to, antonym, direct-hypernym, direct- 2,721 84.4% hyponym} {similar-to, antonym, direct-hypernym, direct- 3,004 84.4% hyponym, also-see, extended-antonym} {similar-to, antonym, direct-hypernym, direct- 3,220 84.6% hyponym, also-see, extended-antonym, derived-from, attribute, domain, domain-member} 0 500 1000 1500 2000 2500 3000 3500 Size of Unlabeled</context>
</contexts>
<marker>Su, Markert, 2008</marker>
<rawString>Fangzhong Su and Katja Markert. 2008. From Words to Senses: A Case Study in Subjectivity Recognition. Proceedings of COLING’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>Eliciting Subjectivity and Polarity Judgements on Word Senses.</title>
<date>2008</date>
<booktitle>Proceedings of COLING’08 workshop of Human Judgements in Computational Linguistics.</booktitle>
<contexts>
<context position="2763" citStr="Su and Markert, 2008" startWordPosition="417" endWordPosition="420">t least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositive—having a positive electric charge;“protons are positive” (objective) (2) plus, positive—involving advantage or good; “a plus (or positive) factor” (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous. Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006). Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. 1All examples in this paper are from WordNet 2.0. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1–9, Boulder, Colorado, June</context>
<context position="5589" citStr="Su and Markert (2008" startWordPosition="860" endWordPosition="863">tly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human a</context>
<context position="7200" citStr="Su and Markert, 2008" startWordPosition="1111" endWordPosition="1114">ive) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations. Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification. Wiebe and Mihalcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005). One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification. Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit Wo</context>
<context position="14916" citStr="Su and Markert, 2008" startWordPosition="2401" endWordPosition="2404">ssign different weights to different relations 4We employ LIBSVM, available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm/. Linear kernel and probability estimates are used in this work. to reflect the degree to which they are subjectivitypreserving. Therefore, we experiment with two methods of weight assignment. Method 1 (NoSL) assigns the same constant weight of 1.0 to all WordNet relations. Method 2 (SL) reflects different degrees of preserving subjectivity. To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008). This method uses a list of subjective words (SL)5 to classify each WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective. We then count how often two senses related via a given relation have the same or a different subjectivity label. The weight is computed by #same/(#same+#different). Results are listed in Table 1. Table 1: Relation weights (Method 2) Method #Same #Different Weight Antonym 2,808 309 0.90 Similar-to 6,887 1,614 0.81 Derived-from 4,630 947 0.83 Direct-Hypernym 71,915 8,600 0.89 Direct-Hyponym 71,915 8,600 0.89 Attribute </context>
<context position="17534" citStr="Su and Markert (2008)" startWordPosition="2807" endWordPosition="2810">most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&amp;Mihalcea’s dataset respectively. We use the McNemar test at the significance level of 5% for significance statements. All evaluations are carried out by 10-fold cross-validation. 4.3 Standard Supervised Learning We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable 6Available at http://www.comp.leeds.ac.uk/ markert/data. This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008). 7Available at http://www.cs.pitt.edu/˜wiebe/ pubs/papers/goldstandard.total.acl06. baseline.8 Three different feature types are used. Lexical Features (L): a bag-of-words representation of the sense glosses with stop word filtering. Relation Features (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjective or objective part of the training set, respectively. This provides a non-graph summary of subjectivity-preserving links. Second, we manually collected a small set (denoted by 5ubj5</context>
<context position="19298" citStr="Su and Markert (2008)" startWordPosition="3087" endWordPosition="3091">the monosemous word is collected in the subjective word list (SL). The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective. For example, the sense uncompromising, inflexible—not making concessions is subjective, as “uncompromising” is a monosemous word and also in SL. We experiment with different combinations of features and the results are listed in Table 2, prefixed by “SVM”. All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&amp;M in Table 2) we used in Su and Markert (2008). However, improvements by adding more features remain small. In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Lesk’s similarity measure from Pedersen’s WordNet similarity package9. We use Lesk as it is one of the few measures applicable across all parts-of-speech. 8This SVM is also used to provide the edge weights to the classification vertices in the Mincut approach. 9Available at http://www.d.umn.edu/˜tpederse/ similarity.html. subjective 0.83 scrupulous 0.17 obj</context>
<context position="26611" citStr="Su and Markert, 2008" startWordPosition="4270" endWordPosition="4273">tion) # unlabeled data Accuracy 0 75.9% 200 76.5% 500 78.6% 1000 80.2% 2000 82.8% 3000 84.0% 3220 84.6% Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%). The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small Micro-WNOp dataset. In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1). 4.5 Comparison to Prior Approaches In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro7 Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Relation # unlabeled data Accuracy {∅} 0 75.3% {similar-to} 418 79.1% {similar-to, antonym} 514 79.5% {similar-to, antonym, direct-hypernym, direct- 2,721 84.4% hyponym} {similar-to, antonym, direct-hypernym, direct- 3,004 84.4% hyponym, also-see, extended-antonym} {similar-to, antonym, direct-hypernym, direct- 3,220 84.6% hyponym, also-see, extended-antonym, derived-from, attribute, domain, domain-member} 0 500 1000 1500 2000 2500 3000 3500 Size of Unlabeled</context>
</contexts>
<marker>Su, Markert, 2008</marker>
<rawString>Fangzhong Su and Katja Markert. 2008a. Eliciting Subjectivity and Polarity Judgements on Word Senses. Proceedings of COLING’08 workshop of Human Judgements in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting Semantic Orientations of Words using Spin Model.</title>
<date>2005</date>
<booktitle>Proceedings of ACL’05.</booktitle>
<contexts>
<context position="4727" citStr="Takamura et al., 2005" startWordPosition="724" endWordPosition="727">Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word le</context>
<context position="6387" citStr="Takamura et al. (2005)" startWordPosition="984" endWordPosition="987">information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al. (2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also </context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting Semantic Orientations of Words using Spin Model. Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>Proceedings of EMNLP’06.</booktitle>
<contexts>
<context position="5087" citStr="Thomas et al., 2006" startWordPosition="775" endWordPosition="778">and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to wo</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang and Lilian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. Proceedings of EMNLP’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or Thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>Proceedings ofACL’02.</booktitle>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or Thumbs down? Semantic orientation applied to unsupervised classification of reviews. Proceedings ofACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring Praise and Criticism: Inference of Semantic Orientation from Association.</title>
<date>2003</date>
<journal>ACM Transaction on Information Systems.</journal>
<contexts>
<context position="4684" citStr="Turney and Littman, 2003" startWordPosition="716" endWordPosition="719">s follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (200</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring Praise and Criticism: Inference of Semantic Orientation from Association. ACM Transaction on Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences.</title>
<date>2003</date>
<booktitle>Proceedings of EMNLP’03.</booktitle>
<contexts>
<context position="4614" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="706" endWordPosition="709">l across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5. 2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document lev</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. Proceedings of EMNLP’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Micalcea</author>
</authors>
<title>Word Sense and Subjectivity.</title>
<date>2006</date>
<booktitle>Proceedings ofACL’06.</booktitle>
<marker>Wiebe, Micalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Micalcea. 2006. Word Sense and Subjectivity. Proceedings ofACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation.</title>
<date>2005</date>
<contexts>
<context position="1249" citStr="Wiebe et al., 2005" startWordPosition="185" endWordPosition="188">mum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. 1 Introduction There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005). Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed. Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005). Thus, the word positive in the sentence “This deal is a positive development for our company.” gives a strong indication that 1 the sentence contains a favourable opinion. However, such word-based indicators can be misleading for two reasons. Fir</context>
<context position="7470" citStr="Wiebe et al., 2005" startWordPosition="1152" endWordPosition="1155"> semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations. Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification. Wiebe and Mihalcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005). One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification. Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNet’s relation structure. 3 Semi-Supervised Mincuts 3.1 Minimum Cuts: The Main Idea Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items 2 should be grouped in the same cut. All items in the training/test data are seen as</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP’05.</booktitle>
<contexts>
<context position="1601" citStr="Wilson et al., 2005" startWordPosition="238" endWordPosition="241">h less than 20% of the training data. 1 Introduction There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005). Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed. Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005). Thus, the word positive in the sentence “This deal is a positive development for our company.” gives a strong indication that 1 the sentence contains a favourable opinion. However, such word-based indicators can be misleading for two reasons. First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004). Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proceedings of HLT/EMNLP’05.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>