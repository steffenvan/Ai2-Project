<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.985755">
Mining coreference relations between formulas and text using Wikipedia
</title>
<author confidence="0.951189">
Minh Nghiem Quoc 1, Keisuke Yokoi 2, Yuichiroh Matsubayashi 3 Akiko Aizawa 1 2 3
</author>
<affiliation confidence="0.989313333333333">
1 Department of Informatics, The Graduate University for Advanced Studies
2 Department of Computer Science, University of Tokyo
3 National Institute of Informatics
</affiliation>
<email confidence="0.993169">
{nqminh, kei-yoko, y-matsu, aizawa}@nii.ac.jp
</email>
<sectionHeader confidence="0.995582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856058823529">
In this paper, we address the problem of
discovering coreference relations between
formulas and the surrounding text. The
task is different from traditional coref-
erence resolution because of the unique
structure of the formulas. In this paper, we
present an approach, which we call ‘CDF
(Concept Description Formula)’, for min-
ing coreference relations between formu-
las and the concepts that refer to them.
Using Wikipedia articles as a target cor-
pus, our approach is based on surface level
text matching between formulas and text,
as well as patterns that represent relation-
ships between them. The results showed
the potential of our approach for formulas
and text coreference mining.
</bodyText>
<sectionHeader confidence="0.998871" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.987122">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999975">
Mathematical content is a valuable information
source for many users: teachers, students, re-
searchers need access to mathematical resources
for teaching, studying, or obtaining updated infor-
mation for research and development. Although
more and more mathematical content is becom-
ing available on the Web nowadays, conventional
search engines do not provide direct search of
mathematical formulas. As such, retrieving math-
ematical content remains an open issue.
Some recent studies proposed mathematical re-
trieval systems that were based on structural sim-
ilarity of equations (Adeel and Khiyal, 2008;
Yokoi and Aizawa, 2009; Nghiem et al., 2009).
However, in these studies, the semantics of the
equations is still not taken into account. As
mathematical equations follow highly abstract and
also rewritable representations, structural similar-
ity alone is insufficient as a metric for semantic
similarity.
Based on this observation, the primary goal of
this paper is to establish a method for extracting
implicit connections between mathematical for-
mulas and their names together with the descrip-
tions written in natural language text. This en-
ables keywords to be associated with the formu-
las and makes mathematical search more power-
ful. For example, it is easier for people searching
and retrieving mathematical concepts if they know
the name of the equation “a2 + b2 = c2” is
the “Pythagorean Theorem”. It could also make
mathematics more understandable and usable for
users.
While many studies have presented corefer-
ence relations among texts (Ponzetto and Poesio,
2009), no work has ever considered the corefer-
ence relations between formulas and texts. In this
paper, we use Wikipedia articles as a target cor-
pus. We chose Wikipedia for these reasons: (1)
Wikipedia uses a subset of TEX markup for math-
ematical formulas. That way, we can analyze the
content of these formulas using TEX expressions
rather than analyzing the images. (2) Wikipedia
provides a wealth of knowledge and the content
of Wikipedia is much cleaner than typical Web
pages, as explained in Giles (2005).
</bodyText>
<page confidence="0.993452">
69
</page>
<note confidence="0.847001">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 69–74,
</note>
<keyword confidence="0.328984">
Beijing, August 2010
</keyword>
<sectionHeader confidence="0.858458" genericHeader="related work">
1.2 Related Work
</sectionHeader>
<bodyText confidence="0.999967">
Ponzetto and Poesio (2006) attempted to include
semantic information extracted from WordNet
and Wikipedia into their coreference resolution
model. Shnarch et al. (2009) presented the ex-
traction of a large-scale rule base from Wikipedia
designed to cover a wide scope of the lexical
reference relations. Their rule base has compa-
rable performance with WordNet while provid-
ing largely complementary information. Yan et
al. (2009) proposed an unsupervised relation ex-
traction method for discovering and enhancing
relations associated with a specified concept in
Wikipedia. Their work combined deep linguis-
tic patterns extracted from Wikipedia with surface
patterns obtained from the Web to generate vari-
ous relations. The results of these studies showed
that Wikipedia is a knowledge-rich and promising
resource for extracting relations between repre-
sentative terms in text. However, these techniques
are not directly applicable to the coreference res-
olution between formulas and texts as we mention
in the next section.
</bodyText>
<subsectionHeader confidence="0.979583">
1.3 Challenges
</subsectionHeader>
<bodyText confidence="0.999919666666667">
There are two key challenges in solving the coref-
erence relations between formulas and texts using
Wikipedia articles.
</bodyText>
<listItem confidence="0.9545055">
• First, formulas have unique structures such
as prior operators and nested functions. In
addition, features such as gender, plural, part
of speech, and proper name, are unavail-
able with formulas for coreference resolu-
tion. Therefore, we cannot apply standard
natural language processing methods to for-
mulas.
• Second, no labeled data are available for
the coreference relations between formu-
las and texts. This means we cannot ap-
ply commonly used machine learning-based
techniques without expensive human annota-
tions.
</listItem>
<subsectionHeader confidence="0.998917">
1.4 Our Approach and Key Contributions
</subsectionHeader>
<bodyText confidence="0.999446714285714">
In this paper, we present an approach, which
we call CDF (Concept Description Formula), for
mining coreference relations between mathemat-
ical Formulas and Concepts using Wikipedia ar-
ticles. In order to address the previously men-
tioned challenges, the proposed CDF approach is
featured as follows:
</bodyText>
<listItem confidence="0.86117305">
• First, we consider not only the concept-
formula pairs but extend the relation with de-
scriptions of the concept. Note that a “con-
cept” in our study corresponds to a “name” or
a “title” of a formula, which is usually quite
short. By additionally considering words ex-
tracted from the descriptions, we have a bet-
ter chance of detecting keywords, such as
mathematical symbols, and function or vari-
able names, used in the equations.
• Second, we apply an unsupervised frame-
work in our approach. Initially, we extract
highly confident coreference pairs using sur-
face level text matching. Next, we collect
promising syntactic patterns from the de-
scriptions and then use the patterns to extract
coreference pairs. The process enables us to
deal with cases where there exist no common
words between the concepts and the formu-
las.
</listItem>
<bodyText confidence="0.9997962">
The remainder of this paper is organized as fol-
lows: In section 2, we present our method. We
then describe the experiments and results in sec-
tion 3. Section 4 concludes the paper and gives
avenues for future work.
</bodyText>
<sectionHeader confidence="0.987152" genericHeader="method">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.999649">
2.1 Overview of the Method
</subsectionHeader>
<bodyText confidence="0.974762">
In this section, we first explain the terms used in
our approach. We then provide a framework of
our method and the functions of the main mod-
ules.
Given a set of Wikipedia articles as input, our
system outputs a list of formulas along with their
names and descriptions. Herein
</bodyText>
<listItem confidence="0.999737">
• Concept: A concept C is a phrase that repre-
</listItem>
<bodyText confidence="0.865038666666667">
sents a name of a mathematical formula. In
Wikipedia, we extract candidate concepts as
noun phrases (NPs) that are either the titles of
</bodyText>
<page confidence="0.988002">
70
</page>
<bodyText confidence="0.9985255">
Wikipedia articles, section headings, or writ-
ten in bold or italic. Additional NPs that con-
tain at least one content word are also consid-
ered.
</bodyText>
<listItem confidence="0.929600076923077">
• Description: A description D is a phrase
that describes the concept. In Wikipedia, de-
scriptions often follow a concept after the
verb “be”.
• Formula: A formula F is a mathematical
formula. In Wikipedia extracted XML files,
formulas occur between the &lt; math &gt; and
&lt; /math &gt; tags. They are encoded in TEX
format.
• Candidate: A candidate is a triple of con-
cept, description and formula. Our system
will judge if the candidate is qualified, which
means the concept is related to the formula.
</listItem>
<bodyText confidence="0.9874492">
Figure 1 shows a section of a Wikipedia article
and the concepts, descriptions and formulas in this
section. Table 1 shows the extracted candidates.
Details of how to extract the concepts, descrip-
tions and formulas and how to form candidates are
described in the next sections.
Note that this ratio does not depend on size of the particular right triangle chosen, as long as it
contains the angle A, since all such triangles are similar.
The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse.
The acronym &amp;quot;SOHCAHTOA&amp;quot; is a useful mnemonic for these ratios.
</bodyText>
<figureCaption confidence="0.999224">
Figure 1: Examples of extracted paragraphs
</figureCaption>
<bodyText confidence="0.956134">
The framework of the system is shown in Fig-
ure 2. The system has four main modules.
</bodyText>
<listItem confidence="0.999148666666667">
• Text Preprocessor: processes Wikipedia ar-
ticles to extract CDF (Concept Description
Formula) candidates.
</listItem>
<bodyText confidence="0.831408333333333">
Input: Wikipedia articles
Concept Description Formula
The sine of an angle the ratio of the length of the opposite side to the A= opposite =a
length of the hypotenuse sin
hypotenuse h
a quadratic equation a polynomial equation of the second degree ax2+bx+c=0
</bodyText>
<figure confidence="0.591575">
Output: equation&apos;s references
</figure>
<figureCaption confidence="0.995601">
Figure 2: Framework of the proposed approach
</figureCaption>
<listItem confidence="0.998606">
• Text Matching: extracts reliable and qual-
ified candidates using surface level text
matching.
• Pattern Generation: generates patterns
from qualified candidates.
• Pattern Matching: extends the candidate
list using the generated patterns.
</listItem>
<subsectionHeader confidence="0.997088">
2.2 Text Preprocessor
</subsectionHeader>
<bodyText confidence="0.9999877">
This module preprocesses the text of the
Wikipedia article to extract CDF candidates.
Based on the assumption that concepts, their de-
scriptions and formulas are in the same paragraph,
we split the text into paragraphs and select para-
graphs that contain at least one formula.
On these selected paragraphs, we run Sentence
Boundary Detector, Tokenizer and Parser from
OpenNLP tools. 1 Based on the parse trees, we
extract the noun phrases (NPs) and identify NPs
representing concepts or descriptions using the
definitions in Section 2.1.
Following the general idea in Shnarch et al.
(2009), we use the “Be-Comp” rule to identify the
description of a concept in the definition sentence.
In a sentence, we extract nominal complements of
the verb ‘to be’, assign the NP that occurs after
the verb ‘to be’ as the description of the NP that
occurs before the verb. Note that some concepts
have descriptions while others do not.
</bodyText>
<footnote confidence="0.973704">
1http://opennlp.sourceforge.net/
</footnote>
<note confidence="0.556551">
The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse.
</note>
<figure confidence="0.968899527777778">
In our case
Sine, cosine and tangent
opposite
sin A=
hypotenuse
CONCEPT DESCRIPTION
a
h
FORMULA
PARAGRAPH
TITLE
In our case
CONCEPT
DESCRIPTION
cos A =
adjacent
hypotenuse
b
h
FORMULA
PARAGRAPH
CONCEPT
The tangent of an angle
is the ratio of the length of the opposite side to the length of the adjacent
side (called so because it can be represented as a line segment tangent to the circle). In our case
DESCRIPTION
tan A= opposite
adjacent
a
b
FORMULA
PARAGRAPH
Pattern Generation
Pattern Matching
Text Matching
Preprocessor
</figure>
<page confidence="0.994317">
71
</page>
<tableCaption confidence="0.998731">
Table 1: Examples of candidates
</tableCaption>
<bodyText confidence="0.9760373">
Concept Description Formula
the sine of an angle the ratio of the length of the opposite side to A = opposite = a
the length of the hypotenuse sin
hypotenuse h
the cosine of an angle the ratio of the length of the adjacent side to A = adjacent = b
the length of the hypotenuse
cos h
hypotenuse
a quadratic equation a polynomial equation of the second degree ax2 + bx + c = 0
the quadratic formula = −b±-v/ ba2−4ac
</bodyText>
<table confidence="0.6350336">
x
the complex number i i2 = −1
the Cahen–Mellin integral −y =
Z� Γ(s)y−s ds
e2,i f�
</table>
<bodyText confidence="0.998821727272727">
The “Be-Comp” rule can also identify if a for-
mula is related to the concept.
After that, we group each formula F in the
same paragraph with concept C and its descrip-
tion D to form a candidate (C, D, F). Table 1
presents candidate examples. Because we only
choose paragraphs that contain at least one for-
mula, every concept has a formula attached to it.
In order to judge the correctness of candidates,
we use the text-matching module, described in the
next section.
</bodyText>
<subsectionHeader confidence="0.997944">
2.3 Text Matching
</subsectionHeader>
<bodyText confidence="0.999131394736842">
In this step, we classify candidates using surface
text. Given a list of candidates of the form (C, D,
F), this module judges if a candidate is qualified
by using the surface text in concept, description
and formula. Because many formulas share the
same variable names or function names (or part of
these names) with their concepts (e.g. the first two
candidates in Table 1), we filter these candidates
using surface text matching.
We define the similarity between concept C,
description D and formula F by the number of
overlapped words, as in Eq. 1.
C0. In this step, function words such as articles,
pronouns, conjunctions and so on in concepts and
descriptions are ignored. Common operators in
formulas are also converted to text, such as ‘+’
‘plus’, ‘–’ ‘minus’, ‘\frac’ ‘divide’.
Using only concepts for text matching with for-
mulas might leave out various important relations.
For example, from the description of the first and
second formula in Table 1, we could extract the
variable names “opposite”, “adjacent” and “hy-
potenuse”.
By adding the description, we could get a more
accurate judgment of whether the concept and
the formula are coreferent. In this case, we can
consider the concept, description and the formula
form a coreference chain.
After this step, we have two categories, Ctrue
and C0. Ctrue contains qualified candidates while
C0 contains candidates that cannot be determined
by text matching. The formulas in C0 have little
or no text relation with their concepts and descrip-
tions. Thus, we can only judge the correctness of
these candidates by using the text around the con-
cepts, descriptions and formulas. The surrounding
text can be formed into patterns and are generated
in the next step.
</bodyText>
<table confidence="0.398516888888889">
TF ∩ TC |2.4 Pattern Generation
D |One difficulty in judging the correctness of a can-
sim(F, CD) = mi  |{|TC|,|TF|}+ Imi {|TD|T|TF|} didate is that the formula does not share any re-
(1) lation with its concept and description. The third
TF, TC and TD are sets of words extracted from candidate in Fig. 1 is an example. It should be
F, C and D, respectively. classified as a qualified instance but is left behind
Candidates with sim(F, CD) no larger than a in C0 after the “text matching” step.
threshold θ1 (1/3 in this study) are grouped into
the group Ctrue. The rest are filtered and stored in
</table>
<page confidence="0.989665">
72
</page>
<bodyText confidence="0.995261375">
In this step, we use the qualified instances in
Ctrue to generate patterns. These patterns are used
in the next step to judge the candidates in C0. Pat-
terns are generated as follows. First, the concept,
description and formula are replaced by CONC,
DESC and FORM, respectively. We then simply
take the entire string between the first and the last
appearance of CONC, DESC and FORM.
</bodyText>
<tableCaption confidence="0.809573666666667">
Table 2 presents examples of patterns extracted
from group Ctrue.
Table 2: Examples of extracted patterns
</tableCaption>
<subsectionHeader confidence="0.703553">
Pattern
</subsectionHeader>
<bodyText confidence="0.910015176470588">
CONC is DESC: FORM
CONC is DESC. In our case FORM
CONC is DESC. So, ..., FORM
CONC FORM
CONC is denoted by FORM
CONC is given by ... FORM
CONC can be written as ...: FORM
FORM where CONC is DESC
FORM satisfies CONC
Using a window surrounding the concepts and
formulas often leads to exponential growth in pat-
terns, so we limit our patterns to those between
any concept C, description D or formula F.
The patterns we obtained above are exactly the
shortest paths from the C nodes to their F node in
the parse tree. Figure 3 presents examples of these
patterns in parse trees.
</bodyText>
<figureCaption confidence="0.995857">
Figure 3: Examples of extracted patterns
</figureCaption>
<subsectionHeader confidence="0.990504">
2.5 Pattern Matching
</subsectionHeader>
<bodyText confidence="0.999938142857143">
In this step, we use patterns obtained from the
previous step to classify more candidates in C0.
We use the string distance between the patterns,
where candidates’ patterns having a string dis-
tance to any of the patterns extracted in the previ-
ous step no larger than the threshold 02 are added
into Ctrue.
</bodyText>
<sectionHeader confidence="0.997249" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997156">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999978352941177">
We collected a total of 16,406 mathematical doc-
uments from the Wikipedia Mathematics Portal.
After the preprocessing step, we selected 72,084
paragraphs that contain at least one formula. From
these paragraphs, we extracted 931,716 candi-
dates.
Because no labeled data are available for use
in this task, we randomly chose 100 candidates:
60 candidates from Ctrue after the text matching
step, 20 candidates added to Ctrue after pattern
matching with 02 = 0, and 20 candidates added
to Ctrue after pattern matching with 02 = 0.25 for
our evaluation. These candidates were annotated
manually. The sizes of the sample sets for human
judgment (60, 20 and 20) were selected approx-
imately proportional to the sizes of the obtained
candidate sets.
</bodyText>
<subsectionHeader confidence="0.78781">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999947571428571">
After the text matching step, we obtained 138,285
qualified candidates in the Ctrue group and
793,431 candidates in C0. In Ctrue, we had 6,129
different patterns. Applying these patterns to C0
by exact pattern matching (02 = 0), we obtained a
further 34,148 qualified candidates. We obtained
an additional 30,337 qualified candidates when
we increased the threshold 02 to 0.25.
For comparison, we built a baseline system.
The baseline automatically groups nearest for-
mula and concept. It had 51 correctly qualified
candidates. The results—displayed in Table 3
and depicted in Figure 4—show that our proposed
method is significantly better than the baseline in
terms of accuracy.
As we can see from the results, when we lower
the threshold, more candidates are added to Ctrue,
which means we get more formulas and formula
names; but it also lowers the accuracy. Although
the performance is not as high as other existing
coreference resolution techniques, the proposed
</bodyText>
<figure confidence="0.997298">
NP
C is D
S
VP
Root
PP
In NP
our case F
NP VP
C is NP
D : F
Root
</figure>
<page confidence="0.994785">
73
</page>
<tableCaption confidence="0.999802">
Table 3: Results of the system
</tableCaption>
<table confidence="0.997426428571429">
Module No. correct/ No. of
total CDF found
Text Matching 41 / 60 138,285
Pattern Matching 52 / 80 172,433
02 = 0
Pattern Matching 56 / 100 202,270
02 = 0.25
</table>
<bodyText confidence="0.995793">
method is a promising starting point for solving
coreference relations between formulas and sur-
rounding text.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999983352941177">
In this paper, we discuss the problem of discov-
ering coreference relations between formulas and
the surrounding texts. Although we could only
use a small number of annotated data for the eval-
uation in this paper, our preliminary experimental
results showed that our approach based on sur-
face text-based matching between formulas and
text, as well as patterns representing relationships
between them showed promise for mining math-
ematical knowledge from Wikipedia. Since this
is the first attempt to extract coreference rela-
tions between formulas and texts, there is room
for further improvement. Possible improvements
include: (1) using advanced technology for pat-
tern matching to improve the coverage of the re-
sult and (2) expanding the work by mining knowl-
edge from the Web.
</bodyText>
<sectionHeader confidence="0.999238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9536855">
Eyal Shnarch, Libby Barak and Ido Dagan. 2009.
Extracting Lexical Reference Rules from Wikipedia
Proceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP of the AFNLP, pages 450–458
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang and Mitsuru Ishizuka. 2009. Unsupervised
Relation Extraction by Mining Wikipedia Texts Us-
ing Information from the Web Proceedings of the
47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP, pages 1021–1029
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art NLP Approaches to Coreference
Pattern Matching 0
Text Matching Pattern Matching 0.25
</reference>
<figureCaption confidence="0.818941">
Figure 4: Results of the system
</figureCaption>
<reference confidence="0.97708328">
Resolution: Theory and Practical Recipes Tutorial
Abstracts of ACL-IJCNLP 2009, page 6
Minh Nghiem, Keisuke Yokoi and Akiko Aizawa.
2009. Enhancing Mathematical Search with Names
of Formulas The Workshop on E-Inclusion in Math-
ematics and Science 2009, pages 22–25
Keisuke Yokoi and Akiko Aizawa. 2009. An Ap-
proach to Similarity Search for Mathematical Ex-
pressions using MathML 2nd workshop Towards a
Digital Mathematics Library, pages 27–35
Hui Siu Cheung Muhammad Adeel and Sikandar
Hayat Khiyal. 2008. Math Go! Prototype of a
Content Based Mathematical Formula Search En-
gine Journal of Theoretical and Applied Informa-
tion Technology, Vol. 4, No. 10, pages 1002–1012
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution In Proceed-
ings of HLT-NAACL-06, pages 192–199
Jim Giles. 2005. Internet Encyclopaedias Go Head
to Head Nature Volume: 438, Issue: 7070, pages
900–901
World Wide Web Consortium. Mathematical Markup
Language (MathML) version 2.0 (second edition)
http://www.w3.org/TR/MathML2/
</reference>
<figure confidence="0.998468">
Accuracy
baseline
70
65
60
55
50
45
40
Pattern Matching 0
Text Matching Pattern Matching 0.25
220000
200000
180000
160000
140000
120000
100000
found
No. of CDF
</figure>
<page confidence="0.958037">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687271">
<title confidence="0.999529">Mining coreference relations between formulas and text using Wikipedia</title>
<author confidence="0.985396">Nghiem Quoc Keisuke Yokoi Yuichiroh Matsubayashi Akiko Aizawa</author>
<affiliation confidence="0.873955666666667">of Informatics, The Graduate University for Advanced of Computer Science, University of Institute of</affiliation>
<email confidence="0.860623">kei-yoko,y-matsu,</email>
<abstract confidence="0.999422444444445">In this paper, we address the problem of discovering coreference relations between formulas and the surrounding text. The task is different from traditional coreference resolution because of the unique structure of the formulas. In this paper, we an approach, which we call Description for mining coreference relations between formulas and the concepts that refer to them. Using Wikipedia articles as a target corpus, our approach is based on surface level text matching between formulas and text, as well as patterns that represent relationships between them. The results showed the potential of our approach for formulas and text coreference mining.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Libby Barak</author>
<author>Ido Dagan</author>
</authors>
<title>Extracting Lexical Reference Rules from Wikipedia</title>
<date>2009</date>
<booktitle>Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>450--458</pages>
<contexts>
<context position="3486" citStr="Shnarch et al. (2009)" startWordPosition="532" endWordPosition="535">et of TEX markup for mathematical formulas. That way, we can analyze the content of these formulas using TEX expressions rather than analyzing the images. (2) Wikipedia provides a wealth of knowledge and the content of Wikipedia is much cleaner than typical Web pages, as explained in Giles (2005). 69 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 69–74, Beijing, August 2010 1.2 Related Work Ponzetto and Poesio (2006) attempted to include semantic information extracted from WordNet and Wikipedia into their coreference resolution model. Shnarch et al. (2009) presented the extraction of a large-scale rule base from Wikipedia designed to cover a wide scope of the lexical reference relations. Their rule base has comparable performance with WordNet while providing largely complementary information. Yan et al. (2009) proposed an unsupervised relation extraction method for discovering and enhancing relations associated with a specified concept in Wikipedia. Their work combined deep linguistic patterns extracted from Wikipedia with surface patterns obtained from the Web to generate various relations. The results of these studies showed that Wikipedia is</context>
<context position="9514" citStr="Shnarch et al. (2009)" startWordPosition="1517" endWordPosition="1520">d patterns. 2.2 Text Preprocessor This module preprocesses the text of the Wikipedia article to extract CDF candidates. Based on the assumption that concepts, their descriptions and formulas are in the same paragraph, we split the text into paragraphs and select paragraphs that contain at least one formula. On these selected paragraphs, we run Sentence Boundary Detector, Tokenizer and Parser from OpenNLP tools. 1 Based on the parse trees, we extract the noun phrases (NPs) and identify NPs representing concepts or descriptions using the definitions in Section 2.1. Following the general idea in Shnarch et al. (2009), we use the “Be-Comp” rule to identify the description of a concept in the definition sentence. In a sentence, we extract nominal complements of the verb ‘to be’, assign the NP that occurs after the verb ‘to be’ as the description of the NP that occurs before the verb. Note that some concepts have descriptions while others do not. 1http://opennlp.sourceforge.net/ The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. In our case Sine, cosine and tangent opposite sin A= hypotenuse CONCEPT DESCRIPTION a h FORMULA PARAGRAPH TITLE In our case CONCEPT</context>
</contexts>
<marker>Shnarch, Barak, Dagan, 2009</marker>
<rawString>Eyal Shnarch, Libby Barak and Ido Dagan. 2009. Extracting Lexical Reference Rules from Wikipedia Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 450–458</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan Yan</author>
<author>Naoaki Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Zhenglu Yang and Mitsuru Ishizuka.</title>
<date>2009</date>
<booktitle>the Web Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>1021--1029</pages>
<contexts>
<context position="3745" citStr="Yan et al. (2009)" startWordPosition="573" endWordPosition="576">ages, as explained in Giles (2005). 69 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 69–74, Beijing, August 2010 1.2 Related Work Ponzetto and Poesio (2006) attempted to include semantic information extracted from WordNet and Wikipedia into their coreference resolution model. Shnarch et al. (2009) presented the extraction of a large-scale rule base from Wikipedia designed to cover a wide scope of the lexical reference relations. Their rule base has comparable performance with WordNet while providing largely complementary information. Yan et al. (2009) proposed an unsupervised relation extraction method for discovering and enhancing relations associated with a specified concept in Wikipedia. Their work combined deep linguistic patterns extracted from Wikipedia with surface patterns obtained from the Web to generate various relations. The results of these studies showed that Wikipedia is a knowledge-rich and promising resource for extracting relations between representative terms in text. However, these techniques are not directly applicable to the coreference resolution between formulas and texts as we mention in the next section. 1.3 Chall</context>
</contexts>
<marker>Yan, Okazaki, Matsuo, 2009</marker>
<rawString>Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang and Mitsuru Ishizuka. 2009. Unsupervised Relation Extraction by Mining Wikipedia Texts Using Information from the Web Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021–1029</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
</authors>
<date>2009</date>
<booktitle>State-of-the-art NLP Approaches to Coreference Pattern Matching 0 Text Matching Pattern Matching 0.25 Resolution: Theory and Practical Recipes Tutorial Abstracts of ACL-IJCNLP 2009,</booktitle>
<pages>6</pages>
<contexts>
<context position="2657" citStr="Ponzetto and Poesio, 2009" startWordPosition="400" endWordPosition="403">al of this paper is to establish a method for extracting implicit connections between mathematical formulas and their names together with the descriptions written in natural language text. This enables keywords to be associated with the formulas and makes mathematical search more powerful. For example, it is easier for people searching and retrieving mathematical concepts if they know the name of the equation “a2 + b2 = c2” is the “Pythagorean Theorem”. It could also make mathematics more understandable and usable for users. While many studies have presented coreference relations among texts (Ponzetto and Poesio, 2009), no work has ever considered the coreference relations between formulas and texts. In this paper, we use Wikipedia articles as a target corpus. We chose Wikipedia for these reasons: (1) Wikipedia uses a subset of TEX markup for mathematical formulas. That way, we can analyze the content of these formulas using TEX expressions rather than analyzing the images. (2) Wikipedia provides a wealth of knowledge and the content of Wikipedia is much cleaner than typical Web pages, as explained in Giles (2005). 69 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLP</context>
</contexts>
<marker>Ponzetto, Poesio, 2009</marker>
<rawString>Simone Paolo Ponzetto and Massimo Poesio. 2009. State-of-the-art NLP Approaches to Coreference Pattern Matching 0 Text Matching Pattern Matching 0.25 Resolution: Theory and Practical Recipes Tutorial Abstracts of ACL-IJCNLP 2009, page 6</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh Nghiem</author>
</authors>
<title>Keisuke Yokoi and Akiko Aizawa.</title>
<date>2009</date>
<booktitle>Enhancing Mathematical Search with Names of Formulas The Workshop on E-Inclusion in Mathematics and Science</booktitle>
<pages>22--25</pages>
<marker>Nghiem, 2009</marker>
<rawString>Minh Nghiem, Keisuke Yokoi and Akiko Aizawa. 2009. Enhancing Mathematical Search with Names of Formulas The Workshop on E-Inclusion in Mathematics and Science 2009, pages 22–25</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keisuke Yokoi</author>
<author>Akiko Aizawa</author>
</authors>
<title>An Approach to Similarity Search for Mathematical Expressions using MathML 2nd workshop Towards a Digital Mathematics Library,</title>
<date>2009</date>
<pages>27--35</pages>
<contexts>
<context position="1708" citStr="Yokoi and Aizawa, 2009" startWordPosition="251" endWordPosition="254">Mathematical content is a valuable information source for many users: teachers, students, researchers need access to mathematical resources for teaching, studying, or obtaining updated information for research and development. Although more and more mathematical content is becoming available on the Web nowadays, conventional search engines do not provide direct search of mathematical formulas. As such, retrieving mathematical content remains an open issue. Some recent studies proposed mathematical retrieval systems that were based on structural similarity of equations (Adeel and Khiyal, 2008; Yokoi and Aizawa, 2009; Nghiem et al., 2009). However, in these studies, the semantics of the equations is still not taken into account. As mathematical equations follow highly abstract and also rewritable representations, structural similarity alone is insufficient as a metric for semantic similarity. Based on this observation, the primary goal of this paper is to establish a method for extracting implicit connections between mathematical formulas and their names together with the descriptions written in natural language text. This enables keywords to be associated with the formulas and makes mathematical search m</context>
</contexts>
<marker>Yokoi, Aizawa, 2009</marker>
<rawString>Keisuke Yokoi and Akiko Aizawa. 2009. An Approach to Similarity Search for Mathematical Expressions using MathML 2nd workshop Towards a Digital Mathematics Library, pages 27–35</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Siu</author>
</authors>
<title>Cheung Muhammad Adeel and Sikandar Hayat Khiyal.</title>
<date>2008</date>
<journal>Journal of Theoretical and Applied Information Technology,</journal>
<volume>4</volume>
<pages>1002--1012</pages>
<marker>Siu, 2008</marker>
<rawString>Hui Siu Cheung Muhammad Adeel and Sikandar Hayat Khiyal. 2008. Math Go! Prototype of a Content Based Mathematical Formula Search Engine Journal of Theoretical and Applied Information Technology, Vol. 4, No. 10, pages 1002–1012</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution In</title>
<date>2006</date>
<booktitle>Proceedings of HLT-NAACL-06,</booktitle>
<pages>192--199</pages>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution In Proceedings of HLT-NAACL-06, pages 192–199</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Giles</author>
</authors>
<title>Internet Encyclopaedias Go Head to Head Nature Volume: 438, Issue: 7070,</title>
<date>2005</date>
<pages>900--901</pages>
<contexts>
<context position="3162" citStr="Giles (2005)" startWordPosition="487" endWordPosition="488"> for users. While many studies have presented coreference relations among texts (Ponzetto and Poesio, 2009), no work has ever considered the coreference relations between formulas and texts. In this paper, we use Wikipedia articles as a target corpus. We chose Wikipedia for these reasons: (1) Wikipedia uses a subset of TEX markup for mathematical formulas. That way, we can analyze the content of these formulas using TEX expressions rather than analyzing the images. (2) Wikipedia provides a wealth of knowledge and the content of Wikipedia is much cleaner than typical Web pages, as explained in Giles (2005). 69 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 69–74, Beijing, August 2010 1.2 Related Work Ponzetto and Poesio (2006) attempted to include semantic information extracted from WordNet and Wikipedia into their coreference resolution model. Shnarch et al. (2009) presented the extraction of a large-scale rule base from Wikipedia designed to cover a wide scope of the lexical reference relations. Their rule base has comparable performance with WordNet while providing largely complementary information. Yan et al. (2009) proposed an unsu</context>
</contexts>
<marker>Giles, 2005</marker>
<rawString>Jim Giles. 2005. Internet Encyclopaedias Go Head to Head Nature Volume: 438, Issue: 7070, pages 900–901</rawString>
</citation>
<citation valid="false">
<authors>
<author>World Wide Web Consortium</author>
</authors>
<journal>Mathematical Markup Language (MathML) version</journal>
<volume>2</volume>
<note>(second edition) http://www.w3.org/TR/MathML2/</note>
<marker>Consortium, </marker>
<rawString>World Wide Web Consortium. Mathematical Markup Language (MathML) version 2.0 (second edition) http://www.w3.org/TR/MathML2/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>