<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.840592">
Structured Sparsity in Structured Prediction
</title>
<author confidence="0.982686">
Andr´e F. T. Martins*† Noah A. Smith* Pedro M. Q. Aguiar$ M´ario A. T. Figueiredo†
</author>
<affiliation confidence="0.964556333333333">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
$Instituto de Sistemas e Rob´otica, Instituto Superior T´ecnico, Lisboa, Portugal
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Lisboa, Portugal
</affiliation>
<email confidence="0.985312">
{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt
</email>
<sectionHeader confidence="0.994946" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811375">
Linear models have enjoyed great success in
structured prediction in NLP. While a lot of
progress has been made on efficient train-
ing with several loss functions, the problem
of endowing learners with a mechanism for
feature selection is still unsolved. Common
approaches employ ad hoc filtering or L1-
regularization; both ignore the structure of the
feature space, preventing practicioners from
encoding structural prior knowledge. We fill
this gap by adopting regularizers that promote
structured sparsity, along with efficient algo-
rithms to handle them. Experiments on three
tasks (chunking, entity recognition, and de-
pendency parsing) show gains in performance,
compactness, and model interpretability.
</bodyText>
<sectionHeader confidence="0.998113" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989136685185185">
Models for structured outputs are in demand across
natural language processing, with applications in in-
formation extraction, parsing, and machine transla-
tion. State-of-the-art models usually involve linear
combinations of features and are trained discrim-
inatively; examples are conditional random fields
(Lafferty et al., 2001), structured support vector
machines (Altun et al., 2003; Taskar et al., 2003;
Tsochantaridis et al., 2004), and the structured per-
ceptron (Collins, 2002a). In all these cases, the un-
derlying optimization problems differ only in the
choice of loss function; choosing among them has
usually a small impact on predictive performance.
In this paper, we are concerned with model se-
lection: which features should be used to define the
prediction score? The fact that models with few
features (“sparse” models) are desirable for several
reasons (compactness, interpretability, good gener-
alization) has stimulated much research work which
has produced a wide variety of methods (Della Pietra
et al., 1997; Guyon and Elisseeff, 2003; McCallum,
2003). Our focus is on methods which embed this
selection into the learning problem via the regular-
ization term. We depart from previous approaches
in that we seek to make decisions jointly about all
candidate features, and we want to promote sparsity
patterns that go beyond the mere cardinality of the
set of features. For example, we want to be able to
select entire feature templates (rather than features
individually), or to make the inclusion of some fea-
tures depend on the inclusion of other features.
We achieve the goal stated above by employ-
ing regularizers which promote structured sparsity.
Such regularizers are able to encode prior knowl-
edge and guide the selection of features by model-
ing the structure of the feature space. Lately, this
type of regularizers has received a lot of attention
in computer vision, signal processing, and compu-
tational biology (Zhao et al., 2009; Kim and Xing,
2010; Jenatton et al., 2009; Obozinski et al., 2010;
Jenatton et al., 2010; Bach et al., 2011). Eisenstein
et al. (2011) employed structured sparsity in com-
putational sociolinguistics. However, none of these
works have addressed structured prediction. Here,
we combine these two levels of structure: struc-
ture in the output space, and structure in the feature
space. The result is a framework that allows build-
ing structured predictors with high predictive power,
while reducing manual feature engineering. We ob-
tain models that are interpretable, accurate, and of-
ten much more compact than L2-regularized ones.
Compared with L1-regularized models, ours are of-
ten more accurate and yield faster runtime.
1500
</bodyText>
<note confidence="0.9945095">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1500–1511,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.947962" genericHeader="method">
2 Structured Prediction
</sectionHeader>
<bodyText confidence="0.999983">
We address structured prediction problems, which
involve an input set X (e.g., sentences) and an out-
put set % assumed large and structured (e.g., tags or
parse trees). We assume that each x E X has a set
of candidate outputs �(x) C �. We consider linear
models, in which predictions are made according to
</bodyText>
<equation confidence="0.910888">
y = arg maxy∈�(x) θ · φ(x, y), (1)
</equation>
<bodyText confidence="0.999992125">
where φ(x, y) E RD is a vector of features, and θ E
RD is the vector of corresponding weights. Let D =
{(xi, yi)}Ni=1 be a training sample. We assume a cost
function is defined such that c(y, y) is the cost of
predicting y when the true output is y; our goal is to
learn θ with small expected cost on unseen data. To
achieve this goal, linear models are usually trained
by solving a problem of the form
</bodyText>
<equation confidence="0.848119">
θ = arg minθ Ω(θ) + iv EN1 L(θ, xi, yi), (2)
</equation>
<bodyText confidence="0.871405333333333">
where Ω is a regularizer and L is a loss function.
Examples of losses are: the negative conditional log-
likelihood used in CRFs (Lafferty et al., 2001),
</bodyText>
<equation confidence="0.970215">
LCRF(θ, x, y) = − log Pθ(y|x), (3)
</equation>
<bodyText confidence="0.980616333333333">
where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear
model; the margin rescaled loss of structured SVMs
(Taskar et al., 2003; Tsochantaridis et al., 2004),
</bodyText>
<equation confidence="0.9667245">
LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4)
y&apos;∈�(x)
</equation>
<bodyText confidence="0.996025">
where δφ(y0) = φ(x, y0)−φ(x, y); and the loss un-
derlying the structured perceptron (Collins, 2002a),
</bodyText>
<equation confidence="0.977172">
LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5)
</equation>
<bodyText confidence="0.999941375">
Empirical comparison among these loss functions
can be found in the literature (see, e.g., Martins et al.,
2010, who also consider interpolations of the losses
above). In practice, it has been observed that the
choice of loss has far less impact than the model de-
sign and choice of features. Hence, in this paper,
we focus our attention on the regularization term in
Eq. 2. We specifically address ways in which this
term can be used to help design the model by pro-
moting structured sparsity. While this has been a
topic of intense research in signal processing and
computational biology (Jenatton et al., 2009; Liu
and Ye, 2010; Bach et al., 2011), it has not yet re-
ceived much attention in the NLP community, where
the choice of regularization for supervised learning
has essentially been limited to the following:
</bodyText>
<listItem confidence="0.90679">
• L2-regularization (Chen and Rosenfeld, 2000):
</listItem>
<equation confidence="0.9360155">
Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02;
Bdi (6)
</equation>
<listItem confidence="0.799287333333333">
• L1-regularization (Kazama and Tsujii, 2003;
Goodman, 2004):
ΩL1
</listItem>
<equation confidence="0.609597">
τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7)
The latter is known as “Lasso,” as popularized by
Tibshirani (1996) in the context of sparse regres-
</equation>
<bodyText confidence="0.999963611111111">
sion. In the two cases above, λ and τ are nonneg-
ative coefficients controlling the intensity of the reg-
ularization. ΩL2 λusually leads to easier optimization
and robust performance; ΩL1 τencourages sparser
models, where only a few features receive nonzero
weights; see Gao et al. (2007) for an empirical com-
parison. More recently, Petrov and Klein (2008b)
applied L1 regularization for structure learning in
phrase-based parsing; a comparison with L2 appears
in Petrov and Klein (2008a). Elastic nets interpolate
between L1 and L2, having been proposed by Zou
and Hastie (2005) and used by Lavergne et al. (2010)
to regularize CRFs.
Neither of the regularizers just described “looks”
at the structure of the feature space, since they all
treat each dimension independently—we call them
unstructured regularizers, as opposed to the struc-
tured ones that we next describe.
</bodyText>
<sectionHeader confidence="0.959521" genericHeader="method">
3 Structured Sparsity
</sectionHeader>
<bodyText confidence="0.998029833333333">
We are interested in regularizers that share with ΩL1
τ
the ability to promote sparsity, so that they can be
used for selecting features. In addition, we want to
endow the feature space RD with additional struc-
ture, so that features are not penalized individually
(as in the L1-case) but collectively, encouraging en-
tire groups of features to be discarded. The choice of
groups will allow encoding prior knowledge regard-
ing the kind of sparsity patterns that are intended in
the model. This can be achieved with group-Lasso
regularization, which we next describe.
</bodyText>
<page confidence="0.500501">
1501
</page>
<subsectionHeader confidence="0.998581">
3.1 The Group Lasso
</subsectionHeader>
<bodyText confidence="0.9999466">
To capture the structure of the feature space, we
group our D features into M groups G1, ... , GM,
where each Gm ⊆ {1, ... , D}. Ahead, we dis-
cuss meaningful ways of choosing group decompo-
sitions; for now, let us assume a sensible choice is
obvious to the model designer. Denote by θm =
(Bd)dEGm the subvector of those weights that cor-
respond to the features in the m-th group, and let
d1, ... , dM be nonnegative scalars (one per group).
We consider the following group-Lasso regularizers:
</bodyText>
<equation confidence="0.982789666666667">
ΩGL
d = EM
m=1 dm11θm112. (8)
</equation>
<bodyText confidence="0.999685285714286">
These regularizers were first proposed by Bakin
(1999) and Yuan and Lin (2006) in the context of re-
gression. If d1 = ... = dM, ΩGL dbecomes the “L1
norm of the L2 norms.” Interestingly, this is also
a norm, called the mixed L2,1-norm.1 These regu-
larizers subsume the L1 and L2 cases, which corre-
spond to trivial choices of groups:
</bodyText>
<listItem confidence="0.998917">
• If each group is a singleton, i.e., M = D and
Gd = {Bd}, and d1 = ... = dM = T, we recover
L1-regularization (cf. Eqs. 7–8).
• If there is a single group spanning all the features,
</listItem>
<bodyText confidence="0.988945807692308">
i.e., M = 1 and G1 = {1, ... , D}, then the right
hand side of Eq. 8 becomes d111θ112. This is equiv-
alent to L2 regularization.2
We next present some non-trivial examples con-
cerning different topologies of 9 = {G1, ... , GM}.
Non-overlapping groups. Let us first consider
the case where 9 is a partition of the feature
space: the groups cover all the features (Um Gm =
{1, . . . ,D}), and they do not overlap (Ga ∩Gb = ∅,
ba =� b). Then, ΩGL
d is termed a non-overlapping
group-Lasso regularizer. It encourages sparsity pat-
terns in which entire groups are discarded. A ju-
dicious choice of groups can lead to very compact
1In the statistics literature, such mixed-norm regularizers,
which group features and then apply a separate norm for each
group, are called composite absolute penalties (Zhao et al.,
2009); other norms besides L2,1 can be used, such as L∞,1
(Quattoni et al., 2009; Wright et al., 2009; Eisenstein et al.,
2011).
2Note that Eqs. 8 and 6 do not become exactly the same: in
Eq. 6, the L2 norm is squared. However it can be shown that
both regularizers lead to identical learning problems (Eq. 2) up
to a transformation of the regularization constant.
models and pinpoint relevant groups of features.
The following examples lie in this category:
</bodyText>
<listItem confidence="0.714901666666667">
• The two cases above (L1 and L2 regularization).
• Label-based groups. In multi-label classification,
where � = {1, ... , L}, features are typically de-
signed as conjunctions of input features with la-
bel indicators, i.e., they take the form φ(x, y) =
ψ(x) ® ey, where ψ(x) E RDX, ey E RL has all
entries zero except the y-th entry, which is 1, and
® denotes the Kronecker product. Hence φ(x, y)
can be reshaped as a DX-by-L matrix, and we can
let each group correspond to a row. In this case,
all groups have the same size and we typically set
d1 = ... = dM. A similar design can be made
for sequence labeling problems, by considering a
similar grouping for the unigram features.3
• Template-based groups. In NLP, features are com-
</listItem>
<bodyText confidence="0.961932965517241">
monly designed via templates. For example, a
template such as w0 ∧ p0 ∧ p−1 denotes the word
in the current position (w0) conjoined with its
part-of-speech (p0) and that of the previous word
(p−1). This template encloses many features cor-
responding to different instantiantions of w0, p0,
and p−1. In §5, we learn feature templates from
the data, by associating each group to a feature
template, and letting that group contain all fea-
tures that are instantiations of this template. Since
groups have different sizes, it is a good idea to
let dm increase with the group size, so that larger
groups pay a larger penalty for being included.
Tree-structured groups. More generally, we may
let the groups in 9 overlap but be nested, i.e., we may
want them to form a hierarchy (two distinct groups
either have empty intersection or one is contained in
the other). This induces a partial order on 9 (the set
inclusion relation Q), endowing it with the structure
of a partially ordered set (poset).
A convenient graphical representation of the poset
(9, Q) is its Hasse diagram. Each group is a node
in the diagram, and an arc is drawn from group Ga
to group Gb if Gb C Ga and there is no b&apos; s.t.
Gb C Gb&apos; C Ga. When the groups are nested, this
diagram is a forest (a union of directed trees). The
corresponding regularizer enforces sparsity patterns
3The same idea is also used in multitask learning, where
labels correspond to tasks (Caruana, 1997).
</bodyText>
<page confidence="0.767103">
1502
</page>
<bodyText confidence="0.9999345">
where a group of features is only selected if all its
ancestors are also selected.4 Hence, entire subtrees
in the diagram can be pruned away. Examples are:
see Fig. 1). The result is a “coarse-to-fine” regular-
izer, which prefers to select feature templates that
are coarser before zooming into finer features.
</bodyText>
<listItem confidence="0.8247978">
• The elastic net. The diagram of 9 has a root node
for G1 = {1, ... , D} and D leaf nodes, one per
each singleton group (see Fig. 1).
• The sparse group-Lasso. This regularizer was
proposed by Friedman et al. (2010):
</listItem>
<equation confidence="0.95043">
ΩSGL
d,τ (θ) = �M&apos;
m=1 (dmkθmk2 + Tmkθmk1) ,
(9)
</equation>
<bodyText confidence="0.994148">
where the total number of groups is M = M0 +
D, and the components θ1, ... , θM&apos; are non-
overlapping. This regularizer promotes sparsity
at both group and feature levels (i.e., it eliminates
entire groups and sparsifies within each group).
Graph-structured groups. In general, the groups
in 9 may overlap without being nested. In this case,
the Hasse diagram of 9 is a directed acyclic graph
(DAG). As in the tree-structured case, a group of
features is only selected if all its ancestors are also
selected. Based on this property, Jenatton et al.
(2009) suggested a way of reverse engineering the
groups from the desired sparsity pattern. We next
describe a strategy for coarse-to-fine feature tem-
plate selection that directly builds on that idea.
Suppose that we are given M feature templates
7 = {T1, ... , TM} which are partially ordered ac-
cording to some criterion, such that if Ta :� Tb we
would like to include Tb in our model only if Ta
is also included. This criterion could be a measure
of coarseness: we may want to let coarser part-of-
speech features precede finer lexical features, e.g.,
p0 ∧ p1 :� w0 ∧ w1, or conjoined features come af-
ter their elementary parts, e.g., p0 -� p0 ∧ p1. The
order does not need to be total, so some templates
may not be comparable (e.g., we may want p0 ∧ p−1
and p0 ∧ p1 not to be comparable). To achieve
the sparsity pattern encoded in h7, :�i, we choose
9 = hG1, ... , GMi as follows: let I(Ta) be the
set of features that are instantiations of template Ta;
then define Ga = Ub:a�b I(Tb), for a = 1, ... , M.
It is easy to see that h9, ⊇i and h7, �i are isomorph
posets (their Hasse diagrams have the same shape;
4We say that a group of features G. is selected if some fea-
ture in G. (but not necessarily all) has a nonzero weight.
</bodyText>
<subsectionHeader confidence="0.99938">
3.2 Bayesian Interpretation
</subsectionHeader>
<bodyText confidence="0.9997265">
The prior knowledge encoded in the group-Lasso
regularizer (Eq. 8) comes with a Bayesian inter-
pretation, as we next describe. In a probabilistic
model (e.g. in the CRF case, where L = LCRF),
the optimization problem in Eq. 2 can be seen as
maximum a posteriori estimation of θ, where the
regularization term Ω(θ) corresponds to the neg-
ative log of a prior distribution (call it p(θ)). It
is well-known that L2-regularization corresponds to
choosing independent zero-mean Gaussian priors,
Od ∼ N(0, A−1), and that L1-regularization results
from adopting zero-mean Laplacian priors, p(Od) ∝
exp(T|Od|).
Figueiredo (2002) provided an alternative inter-
pretation of L1-regularization in terms of a two-
level hierarchical Bayes model, which happens to
generalize to the non-overlapping group-Lasso case,
where Ω = ΩGL
d . As in the L2-case, we also assume
that each parameter receives a zero-mean Gaussian
prior, but now with a group-specific variance Tm,
i.e., θm ∼ N(0, TmI) for m = 1, ... , M. This
reflects the fact that some groups should have their
feature weights shrunk more towards zero than oth-
ers. The variances Tm ≥ 0 are not pre-specified but
rather generated by a one-sided exponential hyper-
prior p(Tm|dm) ∝ exp(−d2mTm/2). It can be shown
that after marginalizing out Tm, we obtain
</bodyText>
<equation confidence="0.990950333333333">
∞p(θm |dm) = 10
p(θm|Tm)p(Tm |dm)dTm
∝ exp(−dmkθmk). (10)
</equation>
<bodyText confidence="0.8703765">
Hence, the non-overlapping group-Lasso corre-
sponds to the following two-level hierachical Bayes
model: independently for each m = 1, ... , M,
Tm ∼ Exp(d2 m/2), θm ∼ N(0,TmI). (11)
</bodyText>
<subsectionHeader confidence="0.99753">
3.3 Prox-operators
</subsectionHeader>
<bodyText confidence="0.999736">
Before introducing our learning algorithm for han-
dling group-Lasso regularization, we need to define
the concept of a Ω-proximity operator. This is the
function proxΩ : RD → RD defined as follows:
</bodyText>
<equation confidence="0.916618">
proxΩ(θ) = argmine&apos; 12kθ0 − θk2 + Ω(θ0). (12)
1503
</equation>
<figureCaption confidence="0.898907">
Figure 1: Hasse diagrams of several group-
</figureCaption>
<bodyText confidence="0.993540823529412">
based regularizers. For all tree-structured
cases, we use the same plate notation that
is traditionally used in probabilistic graphical
models. The rightmost diagram represents a
coarse-to-fine regularizer: each node is a tem-
plate involving contiguous sequences of words
(w) and POS tags (p); the symbol order ∅ --&lt;
p --&lt; w induces a template order (Ta -&lt; Tb
iff at each position i [Ta]i -&lt; [Tb]i). Digits
below each node are the group indices where
each template belongs.
Proximity operators generalize Euclidean projec-
tions and have many interesting properties; see Bach
et al. (2011) for an overview. By requiring zero to be
a subgradient of the objective function in Eq. 12, we
obtain the following closed expression (called soft-
thresholding) for the QL1
</bodyText>
<equation confidence="0.8647891">
τ-proximity operator:
{ Bd − τ if Bd &gt; τ
0 if |Bd |≤ τ
Bd + τ if Bd &lt; −τ.
For the non-overlapping group Lasso case, the prox-
imity operator is given by
�
0 if kθmk2 ≤ dm
[proxΩGL (θ)]m = 110.112−d. 110.112 θm otherwise.
(14)
</equation>
<bodyText confidence="0.9652006">
which can be seen as a generalization of Eq. 13: if
the L2-norm of the m-th group is less than dm, the
entire group is discarded; otherwise it is scaled so
that its L2-norm decreases by an amount of dm.
When groups overlap, the proximity operator
lacks a closed form. When G is tree-structured, it
can still be efficiently computed by a recursive pro-
cedure (Jenatton et al., 2010). When G is not tree-
structured, no specialized procedure is known, and a
convex optimizer is necessary to solve Eq. 12.
</bodyText>
<sectionHeader confidence="0.99335" genericHeader="method">
4 Online Prox-Grad Algorithm
</sectionHeader>
<bodyText confidence="0.999902041666667">
We now turn our attention to efficient ways of han-
dling group-Lasso regularizers. Several fast and
scalable algorithms having been proposed for train-
ing L1-regularized CRFs, based on quasi-Newton
optimization (Andrew and Gao, 2007), coordinate
descent (Sokolovska et al., 2010; Lavergne et al.,
2010), and stochastic gradients (Carpenter, 2008;
Langford et al., 2009; Tsuruoka et al., 2009). The
algorithm that we use in this paper (Alg. 1) extends
the stochastic gradient methods for group-Lasso reg-
ularization; a similar algorithm was used by Martins
et al. (2011) for multiple kernel learning.
Alg. 1 addresses the learning problem in Eq. 2 by
alternating between online (sub-)gradient steps with
respect to the loss term, and proximal steps with
respect to the regularizer. Proximal-gradient meth-
ods are very popular in sparse modeling, both in
batch (Liu and Ye, 2010; Bach et al., 2011) and on-
line (Duchi and Singer, 2009; Xiao, 2009) settings.
The reason we have chosen the algorithm of Martins
et al. (2011) is that it effectively handles overlap-
ping groups, without the need of evaluating proxΩ
(which, as seen in §3.3, can be costly if G is not tree-
structured). To do so, it decomposes Q as
</bodyText>
<equation confidence="0.999111">
Q(θ) = EJj=1 QjQj(θ) (15)
</equation>
<bodyText confidence="0.998734">
for some J ≥ 1, and nonnegative Q1, ... , QJ; each
Qj-proximal operator is assumed easy to compute.
Such a decomposition always exists: if G does not
have overlapping groups, take J = 1. Otherwise,
find J ≤ M disjoint sets G1, ... , GJ such that
�J j=1 Gj = Gand the groups on each Gj are non-
overlapping. The proximal steps are then applied
sequentially, one per each Qj. Overall, Alg. 1 satis-
fies the following important requirements:
</bodyText>
<listItem confidence="0.997576">
• Computational efficiency. Each gradient step at
round t is linear in the number of features that
fire for that instance and independent of the total
number of features D. Each proximal step is lin-
ear in the number of groups M, and does not need
be to performed every round (as we will see later).
</listItem>
<figure confidence="0.766037">
[proxΩL1
τ (θ)]d =
(13)
1504
Algorithm 1 Online Sparse Prox-Grad Algorithm
</figure>
<listItem confidence="0.98710952">
1: input: D, (Qj)J j=1, T, gravity sequence
((ujt)Jj=1)Tt=1, stepsize sequence (qt)Tt=1
2: initialize θ = 0
3: fort = 1 to T do
4: take training pair (xt, yt) E D
5: θ ← θ − qt∇L(θ; xt, yt) (gradient step)
6: for j = 1 to J do
7: θ = proxηtσjtΩj(θ) (proximal step)
8: end for
9: end for
10: output: θ
• Memory efficiency. Only a small active set of fea-
tures (those that have nonzero weights) need to
be maintained. Entire groups of features can be
deleted after each proximal step. Furthermore,
only the features which correspond to nonzero en-
tries in the gradient vector need to be inserted in
the active set; for some losses (LSVM and LSP)
many irrelevant features are never instantianted.
• Convergence. With high probability, Alg. 1 pro-
duces an c-accurate solution after T ≤ O(1/62)
rounds, for a suitable choice of stepsizes and hold-
ing ujt constant, ujt = uj (Martins et al., 2011).
This result can be generalized to any sequence
(ujt)Tt=1 such that uj = T �t 1 ujt.
</listItem>
<bodyText confidence="0.998957294117647">
We next describe several algorithmic ingredients
that make Alg. 1 effective in sparse modeling.
Budget-Driven Shrinkage. Alg. 1 requires the
choice of a “gravity sequence.” We follow Lang-
ford et al. (2009) and set (ujt)Jj=1 to zero for all t
which is not a multiple of some prespecified integer
K; this way, proximal steps need only be performed
each K rounds, yielding a significant speed-up when
the number of groups M is large. A direct adop-
tion of the method of Langford et al. (2009) would
set ujt = Kuj for those rounds; however, we have
observed that such a strategy makes the number of
groups vary substantially in early epochs. We use a
different strategy: for each Gj, we specify a budget
of Bj ≥ 0 groups (this may take into consideration
practical limitations, such as the available memory).
If t is a multiple of K, we set ujt as follows:
</bodyText>
<listItem confidence="0.938084">
1. If Gj does not have more than Bj nonzero
groups, set ujt = 0 and do nothing.
2. Otherwise, sort the groups in Gj by decreasing
order of their L2-norms. Check the L2-norms
of the Bj-th and Bj+1-th entries in the list and
set ujt as the mean of these two divided by qt.
3. Apply a qtujtQj-proximal step using Eq. 14.
At the end of this step, no more than Bj groups
will remain nonzero.5
</listItem>
<bodyText confidence="0.998717357142857">
If the average of the gravity steps converge,
limT→∞ T ET=1 ujt → uj, then the limit points
uj implicitly define the regularizer, via Q =
EJj=1 ujQj.6 Hence, we have shifted the control of
the amount of regularization to the budget constants
Bj, which unlike the uj have a clear meaning and
can be chosen under practical considerations.
Space and Time Efficiency. The proximal steps
in Alg. 1 have a scaling effect on each group, which
affects all features belonging to that group (see
Eq. 14). We want to avoid explicitly updating each
feature in the active set, which could be time con-
suming. We mention two strategies that can be used
for the non-overlapping group Lasso case.
</bodyText>
<listItem confidence="0.945627307692308">
• The first strategy is suitable when M is large and
only a few groups (« M) have features that fire
in each round; this is the case, e.g., of label-based
groups (see §3.1). It consists of making lazy up-
dates (Carpenter, 2008), i.e., to delay the update
of all features in a group until at least one of
them fires; then apply a cumulative penalty. The
amount of the penalty can be computed if one as-
signs a timestamp to each group.
• The second strategy is suitable when M is small
and some groups are very populated; this is the
typical case of template-based groups (§3.1). Two
operations need to be performed: updating each
</listItem>
<bodyText confidence="0.976377">
feature weight (in the gradient steps), and scaling
entire groups (in the proximal steps). We adapt
a trick due to Shalev-Shwartz et al. (2007): repre-
sent the weight vector of the m-th group, θm, by a
5When overlaps exist (e.g. the coarse-to-fine case), we spec-
ify a total pseudo-budget B ignoring the overlaps, which in-
duces budgets Bl, ... , BJ which sum to B. The number of
actually selected groups may be less than B, however, since in
this case some groups can be shrunk more than once. Other
heuristics are possible.
6The convergence assumption can be sidestepped by freez-
ing the vj after a fixed number of iterations.
</bodyText>
<page confidence="0.45766">
1505
</page>
<bodyText confidence="0.994168837837838">
triple (ξm, cm, pm) E R|Gm|xR+xR+, such that
θm = cmξm and 11θm112 = pm. This representa-
tion allows performing the two operations above
in constant time, and it keeps track of the group
L2-norms, necessary in the proximal updates.
For sufficient amounts of regularization, our al-
gorithm has a low memory footprint. Only features
that, at some point, intervene in the gradient com-
puted in line 5 need to be instantiated; and all fea-
tures that receive zero weights after some proximal
step can be deleted from the model (cf. Fig. 2).
Sparseptron and Debiasing. Although Alg. 1 al-
lows to simultaneously select features and learn the
model parameters, it has been observed in the sparse
modeling literature that Lasso-like regularizers usu-
ally have a strong bias which may harm predictive
performance. A post-processing stage is usually
taken (called debiasing), in which the model is re-
fitted without any regularization and using only the
selected features (Wright et al., 2009). If a final de-
biasing stage is to be performed, Alg. 1 only needs
to worry about feature selection, hence it is appeal-
ing to choose a loss function that makes this pro-
cedure as simple as possible. Examining the input
of Alg. 1, we see that both a gravity and a stepsize
sequence need to be specified. The former can be
taken care of by using budget-driven shrinkage, as
described above. The stepsize sequence can be set
�
as qt = q0/ Ft/N], which ensures convergence,
however q0 requires tuning. Fortunately, for the
structured perceptron loss LSP (Eq. 5), Alg. 1 is in-
dependent of q0, up to a scaling of θ, which does not
affect predictions (see Eq. 1).7 We call the instanti-
ation of Alg. 1 with a group-Lasso regularizer and
the loss LSP the sparseptron. Overall, we propose
the following two-stage approach:
</bodyText>
<listItem confidence="0.9950395">
1. Run the sparsepton for a few epochs and dis-
card the features with zero weights.
2. Refit the model without any regularization and
using the loss L which one wants to optimize.
</listItem>
<bodyText confidence="0.982299833333333">
7To see why this is the case, note that both gradient and
proximal updates come scaled by 770; and that the gradient of
the loss is ∇LSP(θ, xt, yt) = O(xt, yt) − O(xt, yt), where yt
is the prediction under the current model, which is insensitive to
the scaling of θ. This independence on 770 does not hold when
the loss is LSVm or LCRF.
</bodyText>
<sectionHeader confidence="0.999406" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999333">
We present experiments in three structured predic-
tion tasks for several group choices.
Text Chunking. We use the English dataset pro-
vided in the CoNLL 2000 shared task (Sang and
Buchholz, 2000), which consists of 8,936 training
and 2,012 testing sentences (sections 15–18 and 20
of the WSJ.) The input observations are the token
words and their POS tags; we want to predict the
sequences of TOB tags representing phrase chunks.
We built 96 contextual feature templates as follows:
</bodyText>
<listItem confidence="0.979942">
• Up to 5-grams of POS tags, in windows of 5 to-
kens on the left and 5 tokens on the right;
• Up to 3-grams of words, in windows of 3 tokens
on the left and 3 tokens on the right;
• Up to 2-grams of word shapes, in windows of
2 tokens on the left and 2 tokens on the right.
</listItem>
<bodyText confidence="0.931098821428571">
Each shape replaces characters by their types
(case sensitive letters, digits, and punctuation),
and deletes repeated types—e.g., Confidence
and 2,664,098 are respectively mapped to Aa
and 0,0+,0+ (Collins, 2002b).
We defined unigram features by conjoining these
templates with each of the 22 output labels. An ad-
ditional template was defined to account for label
bigrams—features in this template do not look at the
input string, but only at consecutive pairs of labels.8
We evaluate the ability of group-Lasso regular-
ization to perform feature template selection. To
do that, we ran 5 epochs of the sparseptron algo-
rithm with template-based groups and budget-driven
shrinkage (budgets of 10, 20, 30, 40, and 50 tem-
plates were tried). For each group 9m, we set dm =
log2 |Gm|, which is the average number of bits nec-
essary to encode a feature in that group, if all fea-
tures were equiprobable. We set K = 1000 (the
number of instances between consecutive proximal
steps). Then, we refit the model with 10 iterations
of the max-loss 1-best MIRA algorithm (Crammer
et al., 2006).9 Table 1 compares the F1 scores and
8State-of-the-art models use larger output contexts, such as
label trigrams and 4-grams. We resort to bigram labels as we
are mostly interested in identifying relevant unigram templates.
9This variant optimizes the LSVm loss (Martins et al., 2010).
For the refitting, we used unregularized MIRA. For the baseline
</bodyText>
<table confidence="0.922675272727273">
1506
Table 1: Results for MIRA Group Lasso B = 10 B = 20 B = 30 B = 40 B = 50
text chunking. F1 (%) 93.10 92.99 93.28 93.59 93.42 93.40
model size (# features) 5,300,396 71,075 158,844 389,065 662,018 891,378
MIRA Lasso C = 0.1 C = 0.5 C = 1 Group-Lasso B = 100 B = 200 B = 300
Spa. dev/test 70.38/74.09 69.19/71.9 70.75/72.38 71.7/74.03 71.79/73.62 72.08/75.05 71.48/73.3
8,598,246 68,565 1,017,769 1,555,683 83,036 354,872 600,646
Dut. dev/test 69.15/71.54 64.07/66.35 66.82/69.42 70.43/71.89 69.48/72.83 71.03/73.33 71.2/72.59
5,727,004 164,960 565,704 953,668 128,320 447,193 889,660
Eng. dev/test 83.95/79.81 80.92/76.95 82.58/78.84 83.38/79.35 85.62/80.26 85.86/81.47 85.03/80.91
8,376,901 232,865 870,587 1,114,016 255,165 953,178 1,719,229
</table>
<tableCaption confidence="0.992795">
Table 2: Results for named entity recognition. Each cell shows Fl (%) and the number of features.
</tableCaption>
<figure confidence="0.998553">
6 x 106
MIRA
Sparceptron + MIRA (B=30)
00 5 10 15
# Epochs
</figure>
<figureCaption confidence="0.996239">
Figure 2: Memory footprints of the MIRA and sparsep-
</figureCaption>
<bodyText confidence="0.968110794117647">
tron algorithms in text chunking. The oscillation in the
first 5 epochs (bottom line) comes from the proximal
steps each K = 1000 rounds. The features are then
frozen and 10 epochs of unregularized MIRA follow.
Overall, the sparseptron requires &lt; 7.5% of the memory
as the MIRA baseline.
the model sizes obtained with the several budgets
against those obtained by running 15 iterations of
MIRA with the original set of features. Note that
the total number of iterations is the same; yet, the
group-Lasso approach has a much smaller memory
footprint (see Fig. 2) and yields much more com-
pact models. The small memory footprint comes
from the fact that Alg. 1 may entertain a large num-
ber of features without ever instantiating all of them.
The predictive power is comparable (although some
choices of budget yield slightly better scores for the
group-Lasso approach).10
Named Entity Recognition. We experiment with
the Spanish, Dutch, and English datasets pro-
vided in the CoNLL 2002/2003 shared tasks (Sang,
2002; Sang and De Meulder, 2003). For Span-
ish, we use the POS tags provided by Car-
(described next), we used L2-regularized MIRA and tuned the
regularization constant with cross-validation.
10We also tried label-based group-Lasso and sparse group-
Lasso (§3.1), with less impressive results (omitted for space).
reras (http://www.lsi.upc.es/˜nlp/tools/
nerc/nerc.html); for English, we ignore the syn-
tactic chunk tags provided with the dataset. Hence,
all datasets have the same sort of input observations
(words and POS) and all have 9 output labels. We
use the feature templates described above plus some
additional ones (yielding a total of 452 templates):
</bodyText>
<listItem confidence="0.9824998">
• Up to 3-grams of shapes, in windows of size 3;
• For prefix/suffix sizes of 1, 2, 3, up to 3-grams of
word prefixes/suffixes, in windows of size 3;
• Up to 5-grams of case, punctuation, and digit in-
dicators, in windows of size 5.
</listItem>
<bodyText confidence="0.999880041666667">
As before, an additional feature template was de-
fined to account for label bigrams. We do feature
template selection (same setting as before) for bud-
get sizes of 100, 200, and 300. We compare with
both MIRA (using all the features) and the sparsep-
tron with a standard Lasso regularizer QL1
τ , for sev-
eral values of C = 1/(τN). Table 2 shows the re-
sults. We observe that template-based group-Lasso
wins both in terms of accuracy and compactness.
Note also that the ability to discard feature tem-
plates (rather than individual features) yields faster
test runtime than models regularized with the stan-
dard Lasso: fewer templates will need to be instan-
tiated, with a speed-up in score computation.
Multilingual Dependency Parsing. We trained
non-projective dependency parsers for 6 languages
using the CoNLL-X shared task datasets (Buchholz
and Marsi, 2006): Arabic, Danish, Dutch, Japanese,
Slovene, and Spanish. We chose the languages with
the smallest datasets, because regularization is more
important when data is scarce. The output to be pre-
dicted from each input sentence is the set of depen-
dency links, which jointly define a spanning tree.
</bodyText>
<figure confidence="0.994682056603774">
4
2
# Features
1507
Arabic Danish Japanese
x 106
Number of Features
78.5
78
77.5
77
76.5
2 4 6 8 10 12
89
0 5 10 15
x 106
92
0 2 4 6 8
x 106
UAS (%)
Group−Lasso
Group−Lasso (C2F)
Lasso
Filter−based (IG)
90
89.8
89.6
89.4
89.2
93.5
93
92.5
Slovene Spanish Turkish
84
75.5
83.5
75
83
74.5
82.5
81
82
0 0.5 1 1.5 2
x 107
76
74
0 5 10 15
x 106
84
83
82
0 2 4 6 8 10
x 106
</figure>
<figureCaption confidence="0.6720084">
Figure 3: Comparison between non-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based
method based on information gain for selecting feature templates in multilingual dependency parsing. The x-axis is
the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score. The
plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method. The standard
Lasso (which does not select templates, but individual features) is also shown for comparison.
</figureCaption>
<bodyText confidence="0.999991357142857">
We use arc-factored models, for which exact infer-
ence is tractable (McDonald et al., 2005). We de-
fined M = 684 feature templates for each candi-
date arc by conjoining the words, shapes, lemmas,
and POS of the head and the modifier, as well as
the contextual POS, and the distance and direction
of attachment. We followed the same two-stage
approach as before, and compared with a baseline
which selects feature templates by ranking them ac-
cording to the information gain criterion. This base-
line assigns a score to each template T,,t which re-
flects an empirical estimate of the mutual informa-
tion between T,,t and the binary variable A that indi-
cates the presence/absence of a dependency link:
</bodyText>
<equation confidence="0.9970795">
P(f, a) log2 P(f,
a)))(16)
</equation>
<bodyText confidence="0.999991869565217">
where P(f, a) is the joint probability of feature f
firing and an arc being active (a = 1) or innactive
(a = 0), and P(f) and P(a) are the corresponding
marginals. All probabilities are estimated from the
empirical counts of events observed in the data.
The results are plotted in Fig. 3, for budget sizes
of 200, 300, and 400. We observe that for all
but one language (Spanish is the exception), non-
overlapping group-Lasso regularization is more ef-
fective at selecting feature templates than the in-
formation gain criterion, and slightly better than
coarse-to-fine group-Lasso. For completeness, we
also display the results obtained with a standard
Lasso regularizer. Table 3 shows what kind of
feature templates were most selected for each lan-
guage. Some interesting patterns can be observed:
morphologically-rich languages with small datasets
(such as Turkish and Slovene) seem to avoid lexi-
cal features, arguably due to potential for overfitting;
in Japanese, contextual POS appear to be specially
relevant. It should be noted, however, that some
of these patterns may be properties of the datasets
rather than of the languages themselves.
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9916155">
A variant of the online proximal gradient algorithm
used in this paper was proposed by Martins et al.
</bodyText>
<table confidence="0.990716416666667">
A EIGm — fETm �
aE{0,1}
1508
Ara. Dan. Jap. Slo. Spa. Tur.
Bilexical ++ + + +
Lex. → POS +
POS → Lex. ++ + + + +
POS → POS ++ +
Middle POS ++ ++ ++ ++ ++ ++
Shape ++ ++ ++ ++
Direction + + + + +
Distance ++ + + + + +
</table>
<tableCaption confidence="0.7739235">
Table 3: Variation of feature templates that were selected
accross languages. Each line groups together similar tem-
</tableCaption>
<bodyText confidence="0.996978378378378">
plates, involving lexical, contextual POS, word shape in-
formation, as well as attachment direction and length.
Empty cells denote that very few or none of the templates
in that category was selected; + denotes that some were
selected; ++ denotes that most or all were selected.
(2011), along with a theoretical analysis. The fo-
cus there, however, was multiple kernel learning,
hence overlapping groups were not considered in
their experiments. Budget-driven shrinkage and the
sparseptron are novel techniques, at the best of our
knowledge. Apart from Martins et al. (2011), the
only work we are aware of which combines struc-
tured sparsity with structured prediction is Schmidt
and Murphy (2010); however, their goal is to pre-
dict the structure of graphical models, while we
are mostly interested in the structure of the feature
space. Schmidt and Murphy (2010) used to gener-
ative models, while our approach emphasizes dis-
criminative learning.
Mixed norm regularization has been used for a
while in statistics as a means to promote structured
sparsity. Group Lasso is due to Bakin (1999) and
Yuan and Lin (2006), after which a string of variants
and algorithms appeared (Bach, 2008; Zhao et al.,
2009; Jenatton et al., 2009; Friedman et al., 2010;
Obozinski et al., 2010). The flat (non-overlapping)
case has tight links with learning formalisms such
as multiple kernel learning (Lanckriet et al., 2004)
and multi-task learning (Caruana, 1997). The tree-
structured case has been addressed by Kim and Xing
(2010), Liu and Ye (2010) and Mairal et al. (2010),
along with L∞,1 and L2,1 regularization. Graph-
structured groups are discussed in Jenatton et al.
(2010), along with a DAG representation. In NLP,
mixed norms have been used recently by Grac¸a et al.
(2009) in posterior regularization, and by Eisenstein
et al. (2011) in a multi-task regression problem.
</bodyText>
<sectionHeader confidence="0.99907" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999987714285714">
In this paper, we have explored two levels of struc-
ture in NLP problems: structure on the outputs, and
structure on the feature space. We have shown how
the latter can be useful in model design, through the
use of regularizers which promote structured spar-
sity. We propose an online algorithm with mini-
mal memory requirements for exploring large fea-
ture spaces. Our algorithm, which specializes into
the sparseptron, yields a mechanism for selecting
entire groups of features. We apply sparseptron
for selecting feature templates in three structured
prediction tasks, with advantages over filter-based
methods, L1, and L2 regularization in terms of per-
formance, compactness, and model interpretability.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999708875">
We would like to thank all reviewers for their comments,
Eric Xing for helpful discussions, and Slav Petrov for his
comments on a draft version of this paper. A. M. was sup-
ported by a FCT/ICTI grant through the CMU-Portugal
Program, and also by Priberam. This work was partially
supported by the FET programme (EU FP7), under the
SIMBAD project (contract 213250). N. S. was supported
by NSF CAREER IIS-1054319.
</bodyText>
<sectionHeader confidence="0.996002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992376623529412">
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In Proc. of
ICML.
G. Andrew and J. Gao. 2007. Scalable training of
L1-regularized log-linear models. In Proc. of ICML.
ACM.
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2011.
Convex optimization with sparsity-inducing norms. In
Optimization for Machine Learning. MIT Press.
F. Bach. 2008. Exploring large feature spaces with hier-
archical multiple kernel learning. NIPS, 21.
S. Bakin. 1999. Adaptive regression and model selec-
tion in data mining problems. Ph.D. thesis, Australian
National University.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
B. Carpenter. 2008. Lazy sparse stochastic gradient de-
scent for regularized multinomial logistic regression.
Technical report, Technical report, Alias-i.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41–75.
1509
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models.
IEEE Transactions on Speech and Audio Processing,
8(1):37–50.
M. Collins. 2002a. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
M. Collins. 2002b. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In
Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Algo-
rithms. JMLR, 7:551–585.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19:380–393.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2873–2908.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
M.A.T. Figueiredo. 2002. Adaptive sparseness using Jef-
freys’ prior. Advances in Neural Information Process-
ing Systems.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. A note
on the group lasso and a sparse group lasso. Unpub-
lished manuscript.
J. Gao, G. Andrew, M. Johnson, and K. Toutanova. 2007.
A comparative study of parameter estimation methods
for statistical natural language processing. In Proc. of
ACL.
J. Goodman. 2004. Exponential priors for maximum en-
tropy models. In Proc. of NAACL.
J. Grac¸a, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs. parameter sparsity in latent variable mod-
els. Advances in Neural Information Processing Sys-
tems.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research, 3:1157–1182.
R. Jenatton, J.-Y. Audibert, and F. Bach. 2009. Struc-
tured variable selection with sparsity-inducing norms.
Technical report, arXiv:0904.3523.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. 2010.
Proximal methods for sparse hierarchical dictionary
learning. In Proc. of ICML.
J. Kazama and J. Tsujii. 2003. Evaluation and exten-
sion of maximum entropy models with inequality con-
straints. In Proc. of EMNLP.
S. Kim and E.P. Xing. 2010. Tree-guided group lasso for
multi-task regression with structured sparsity. In Proc.
of ICML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ofICML.
G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El
Ghaoui, and M. I. Jordan. 2004. Learning the kernel
matrix with semidefinite programming. JMLR, 5:27–
72.
J. Langford, L. Li, and T. Zhang. 2009. Sparse online
learning via truncated gradient. JMLR, 10:777–801.
T. Lavergne, O. Capp´e, and F. Yvon. 2010. Practical
very large scale CRFs. In Proc. of ACL.
J. Liu and J. Ye. 2010. Moreau-Yosida regularization for
grouped tree structure learning. In Advances in Neural
Information Processing Systems.
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. 2010.
Network flow algorithms for structured sparsity. In
Advances in Neural Information Processing Systems.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011. Online learning of
structured predictors with multiple kernels. In Proc. of
AISTATS.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proc. of UAI.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
G. Obozinski, B. Taskar, and M.I. Jordan. 2010. Joint co-
variate selection and joint subspace selection for multi-
ple classification problems. Statistics and Computing,
20(2):231–252.
S. Petrov and D. Klein. 2008a. Discriminative log-linear
grammars with latent variables. Advances in Neural
Information Processing Systems, 20:1153–1160.
S. Petrov and D. Klein. 2008b. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
Proc. of EMNLP.
A. Quattoni, X. Carreras, M. Collins, and T. Darrell.
2009. An efficient projection for l1,∞ regularization.
In Proc. of ICML.
E.F.T.K. Sang and S. Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proceedings
of CoNLL-2000 and LLL-2000.
E.F.T.K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proc. of CoNLL.
E.F.T.K. Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named entity
recognition. In Proc. of CoNLL.
1510
M. Schmidt and K. Murphy. 2010. Convex structure
learning in log-linear models: Beyond pairwise poten-
tials. In Proc. of AISTATS.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for SVM.
In ICML.
N. Sokolovska, T. Lavergne, O. Capp´e, and F. Yvon.
2010. Efficient learning of sparse conditional random
fields for supervised sequence labelling. IEEE Journal
of Selected Topics in Signal Processing, 4(6):953–964.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society
B., pages 267–288.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In ICML.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL.
S.J. Wright, R. Nowak, and M.A.T. Figueiredo. 2009.
Sparse reconstruction by separable approximation.
IEEE Transactions on Signal Processing, 57(7):2479–
2493.
L. Xiao. 2009. Dual averaging methods for regular-
ized stochastic learning and online optimization. In
Advances in Neural Information Processing Systems.
M. Yuan and Y. Lin. 2006. Model selection and estima-
tion in regression with grouped variables. Journal of
the Royal Statistical Society (B), 68(1):49.
P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hi-
erarchical model selection through composite absolute
penalties. Annals of Statistics, 37(6A):3468–3497.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society Series B (Statistical Methodology),
67(2):301–320.
</reference>
<page confidence="0.742333">
1511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.581094">
<title confidence="0.999672">Structured Sparsity in Structured Prediction</title>
<author confidence="0.995048">F T Noah A Pedro M Q M´ario A T</author>
<affiliation confidence="0.878271">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, de Sistemas e Rob´otica, Instituto Superior T´ecnico, Lisboa, de Instituto Superior T´ecnico, Lisboa,</affiliation>
<email confidence="0.977528">aguiar@isr.ist.utl.pt,mtf@lx.it.pt</email>
<abstract confidence="0.994626235294118">Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common employ ad hoc filtering or regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. We fill this gap by adopting regularizers that promote along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
</authors>
<title>Hidden Markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="1550" citStr="Altun et al., 2003" startWordPosition="206" endWordPosition="209"> structured sparsity, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide vari</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hidden Markov support vector machines. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>J Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proc. of ICML.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="18412" citStr="Andrew and Gao, 2007" startWordPosition="3146" endWordPosition="3149"> it is scaled so that its L2-norm decreases by an amount of dm. When groups overlap, the proximity operator lacks a closed form. When G is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm We now turn our attention to efficient ways of handling group-Lasso regularizers. Several fast and scalable algorithms having been proposed for training L1-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse </context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>G. Andrew and J. Gao. 2007. Scalable training of L1-regularized log-linear models. In Proc. of ICML. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bach</author>
<author>R Jenatton</author>
<author>J Mairal</author>
<author>G Obozinski</author>
</authors>
<title>Convex optimization with sparsity-inducing norms.</title>
<date>2011</date>
<booktitle>In Optimization for Machine Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3230" citStr="Bach et al., 2011" startWordPosition="479" endWordPosition="482">lates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2-regularized ones. Compared with L1-regularized models, ours are often more accurate and yield faster</context>
<context position="6039" citStr="Bach et al., 2011" startWordPosition="968" endWordPosition="971">se loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier</context>
<context position="17255" citStr="Bach et al. (2011)" startWordPosition="2940" endWordPosition="2943"> Hasse diagrams of several groupbased regularizers. For all tree-structured cases, we use the same plate notation that is traditionally used in probabilistic graphical models. The rightmost diagram represents a coarse-to-fine regularizer: each node is a template involving contiguous sequences of words (w) and POS tags (p); the symbol order ∅ --&lt; p --&lt; w induces a template order (Ta -&lt; Tb iff at each position i [Ta]i -&lt; [Tb]i). Digits below each node are the group indices where each template belongs. Proximity operators generalize Euclidean projections and have many interesting properties; see Bach et al. (2011) for an overview. By requiring zero to be a subgradient of the objective function in Eq. 12, we obtain the following closed expression (called softthresholding) for the QL1 τ-proximity operator: { Bd − τ if Bd &gt; τ 0 if |Bd |≤ τ Bd + τ if Bd &lt; −τ. For the non-overlapping group Lasso case, the proximity operator is given by � 0 if kθmk2 ≤ dm [proxΩGL (θ)]m = 110.112−d. 110.112 θm otherwise. (14) which can be seen as a generalization of Eq. 13: if the L2-norm of the m-th group is less than dm, the entire group is discarded; otherwise it is scaled so that its L2-norm decreases by an amount of dm. </context>
<context position="19073" citStr="Bach et al., 2011" startWordPosition="3251" endWordPosition="3254">10; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm of Martins et al. (2011) is that it effectively handles overlapping groups, without the need of evaluating proxΩ (which, as seen in §3.3, can be costly if G is not treestructured). To do so, it decomposes Q as Q(θ) = EJj=1 QjQj(θ) (15) for some J ≥ 1, and nonnegative Q1, ... , QJ; each Qj-proximal operator is assumed easy to compute. Such a decomposition always exists: if G does not have overlapping groups, take J = 1. Otherwise, find J ≤ M disjoint sets G1, ... , GJ such that �J j=1 Gj = Gand th</context>
</contexts>
<marker>Bach, Jenatton, Mairal, Obozinski, 2011</marker>
<rawString>F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2011. Convex optimization with sparsity-inducing norms. In Optimization for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bach</author>
</authors>
<title>Exploring large feature spaces with hierarchical multiple kernel learning.</title>
<date>2008</date>
<journal>NIPS,</journal>
<volume>21</volume>
<contexts>
<context position="37287" citStr="Bach, 2008" startWordPosition="6425" endWordPosition="6426">1), the only work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regula</context>
</contexts>
<marker>Bach, 2008</marker>
<rawString>F. Bach. 2008. Exploring large feature spaces with hierarchical multiple kernel learning. NIPS, 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bakin</author>
</authors>
<title>Adaptive regression and model selection in data mining problems.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Australian National University.</institution>
<contexts>
<context position="8551" citStr="Bakin (1999)" startWordPosition="1392" endWordPosition="1393">t describe. 1501 3.1 The Group Lasso To capture the structure of the feature space, we group our D features into M groups G1, ... , GM, where each Gm ⊆ {1, ... , D}. Ahead, we discuss meaningful ways of choosing group decompositions; for now, let us assume a sensible choice is obvious to the model designer. Denote by θm = (Bd)dEGm the subvector of those weights that correspond to the features in the m-th group, and let d1, ... , dM be nonnegative scalars (one per group). We consider the following group-Lasso regularizers: ΩGL d = EM m=1 dm11θm112. (8) These regularizers were first proposed by Bakin (1999) and Yuan and Lin (2006) in the context of regression. If d1 = ... = dM, ΩGL dbecomes the “L1 norm of the L2 norms.” Interestingly, this is also a norm, called the mixed L2,1-norm.1 These regularizers subsume the L1 and L2 cases, which correspond to trivial choices of groups: • If each group is a singleton, i.e., M = D and Gd = {Bd}, and d1 = ... = dM = T, we recover L1-regularization (cf. Eqs. 7–8). • If there is a single group spanning all the features, i.e., M = 1 and G1 = {1, ... , D}, then the right hand side of Eq. 8 becomes d111θ112. This is equivalent to L2 regularization.2 We next pre</context>
<context position="37193" citStr="Bakin (1999)" startWordPosition="6409" endWordPosition="6410">e sparseptron are novel techniques, at the best of our knowledge. Apart from Martins et al. (2011), the only work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG represent</context>
</contexts>
<marker>Bakin, 1999</marker>
<rawString>S. Bakin. 1999. Adaptive regression and model selection in data mining problems. Ph.D. thesis, Australian National University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="32573" citStr="Buchholz and Marsi, 2006" startWordPosition="5605" endWordPosition="5608">) and the sparseptron with a standard Lasso regularizer QL1 τ , for several values of C = 1/(τN). Table 2 shows the results. We observe that template-based group-Lasso wins both in terms of accuracy and compactness. Note also that the ability to discard feature templates (rather than individual features) yields faster test runtime than models regularized with the standard Lasso: fewer templates will need to be instantiated, with a speed-up in score computation. Multilingual Dependency Parsing. We trained non-projective dependency parsers for 6 languages using the CoNLL-X shared task datasets (Buchholz and Marsi, 2006): Arabic, Danish, Dutch, Japanese, Slovene, and Spanish. We chose the languages with the smallest datasets, because regularization is more important when data is scarce. The output to be predicted from each input sentence is the set of dependency links, which jointly define a spanning tree. 4 2 # Features 1507 Arabic Danish Japanese x 106 Number of Features 78.5 78 77.5 77 76.5 2 4 6 8 10 12 89 0 5 10 15 x 106 92 0 2 4 6 8 x 106 UAS (%) Group−Lasso Group−Lasso (C2F) Lasso Filter−based (IG) 90 89.8 89.6 89.4 89.2 93.5 93 92.5 Slovene Spanish Turkish 84 75.5 83.5 75 83 74.5 82.5 81 82 0 0.5 1 1.</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Lazy sparse stochastic gradient descent for regularized multinomial logistic regression.</title>
<date>2008</date>
<tech>Technical report, Technical report, Alias-i.</tech>
<contexts>
<context position="18524" citStr="Carpenter, 2008" startWordPosition="3163" endWordPosition="3164">osed form. When G is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm We now turn our attention to efficient ways of handling group-Lasso regularizers. Several fast and scalable algorithms having been proposed for training L1-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) se</context>
<context position="23337" citStr="Carpenter, 2008" startWordPosition="4037" endWordPosition="4038"> can be chosen under practical considerations. Space and Time Efficiency. The proximal steps in Alg. 1 have a scaling effect on each group, which affects all features belonging to that group (see Eq. 14). We want to avoid explicitly updating each feature in the active set, which could be time consuming. We mention two strategies that can be used for the non-overlapping group Lasso case. • The first strategy is suitable when M is large and only a few groups (« M) have features that fire in each round; this is the case, e.g., of label-based groups (see §3.1). It consists of making lazy updates (Carpenter, 2008), i.e., to delay the update of all features in a group until at least one of them fires; then apply a cumulative penalty. The amount of the penalty can be computed if one assigns a timestamp to each group. • The second strategy is suitable when M is small and some groups are very populated; this is the typical case of template-based groups (§3.1). Two operations need to be performed: updating each feature weight (in the gradient steps), and scaling entire groups (in the proximal steps). We adapt a trick due to Shalev-Shwartz et al. (2007): represent the weight vector of the m-th group, θm, by </context>
</contexts>
<marker>Carpenter, 2008</marker>
<rawString>B. Carpenter. 2008. Lazy sparse stochastic gradient descent for regularized multinomial logistic regression. Technical report, Technical report, Alias-i.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="12446" citStr="Caruana, 1997" startWordPosition="2102" endWordPosition="2103">e is contained in the other). This induces a partial order on 9 (the set inclusion relation Q), endowing it with the structure of a partially ordered set (poset). A convenient graphical representation of the poset (9, Q) is its Hasse diagram. Each group is a node in the diagram, and an arc is drawn from group Ga to group Gb if Gb C Ga and there is no b&apos; s.t. Gb C Gb&apos; C Ga. When the groups are nested, this diagram is a forest (a union of directed trees). The corresponding regularizer enforces sparsity patterns 3The same idea is also used in multitask learning, where labels correspond to tasks (Caruana, 1997). 1502 where a group of features is only selected if all its ancestors are also selected.4 Hence, entire subtrees in the diagram can be pruned away. Examples are: see Fig. 1). The result is a “coarse-to-fine” regularizer, which prefers to select feature templates that are coarser before zooming into finer features. • The elastic net. The diagram of 9 has a root node for G1 = {1, ... , D} and D leaf nodes, one per each singleton group (see Fig. 1). • The sparse group-Lasso. This regularizer was proposed by Friedman et al. (2010): ΩSGL d,τ (θ) = �M&apos; m=1 (dmkθmk2 + Tmkθmk1) , (9) where the total </context>
<context position="37549" citStr="Caruana, 1997" startWordPosition="6465" endWordPosition="6466">space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusions In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space. We have shown how the latter can be</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>R. Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for maximum entropy models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="6254" citStr="Chen and Rosenfeld, 2000" startWordPosition="1001" endWordPosition="1004"> impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) app</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing techniques for maximum entropy models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1647" citStr="Collins, 2002" startWordPosition="223" endWordPosition="224">king, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our focus </context>
<context position="5345" citStr="Collins, 2002" startWordPosition="848" endWordPosition="849">his goal, linear models are usually trained by solving a problem of the form θ = arg minθ Ω(θ) + iv EN1 L(θ, xi, yi), (2) where Ω is a regularizer and L is a loss function. Examples of losses are: the negative conditional loglikelihood used in CRFs (Lafferty et al., 2001), LCRF(θ, x, y) = − log Pθ(y|x), (3) where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear model; the margin rescaled loss of structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004), LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4) y&apos;∈�(x) where δφ(y0) = φ(x, y0)−φ(x, y); and the loss underlying the structured perceptron (Collins, 2002a), LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal pro</context>
<context position="27650" citStr="Collins, 2002" startWordPosition="4800" endWordPosition="4801">s; we want to predict the sequences of TOB tags representing phrase chunks. We built 96 contextual feature templates as follows: • Up to 5-grams of POS tags, in windows of 5 tokens on the left and 5 tokens on the right; • Up to 3-grams of words, in windows of 3 tokens on the left and 3 tokens on the right; • Up to 2-grams of word shapes, in windows of 2 tokens on the left and 2 tokens on the right. Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types—e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b). We defined unigram features by conjoining these templates with each of the 22 output labels. An additional template was defined to account for label bigrams—features in this template do not look at the input string, but only at consecutive pairs of labels.8 We evaluate the ability of group-Lasso regularization to perform feature template selection. To do that, we ran 5 epochs of the sparseptron algorithm with template-based groups and budget-driven shrinkage (budgets of 10, 20, 30, 40, and 50 templates were tried). For each group 9m, we set dm = log2 |Gm|, which is the average number of bi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002a. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1647" citStr="Collins, 2002" startWordPosition="223" endWordPosition="224">king, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our focus </context>
<context position="5345" citStr="Collins, 2002" startWordPosition="848" endWordPosition="849">his goal, linear models are usually trained by solving a problem of the form θ = arg minθ Ω(θ) + iv EN1 L(θ, xi, yi), (2) where Ω is a regularizer and L is a loss function. Examples of losses are: the negative conditional loglikelihood used in CRFs (Lafferty et al., 2001), LCRF(θ, x, y) = − log Pθ(y|x), (3) where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear model; the margin rescaled loss of structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004), LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4) y&apos;∈�(x) where δφ(y0) = φ(x, y0)−φ(x, y); and the loss underlying the structured perceptron (Collins, 2002a), LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal pro</context>
<context position="27650" citStr="Collins, 2002" startWordPosition="4800" endWordPosition="4801">s; we want to predict the sequences of TOB tags representing phrase chunks. We built 96 contextual feature templates as follows: • Up to 5-grams of POS tags, in windows of 5 tokens on the left and 5 tokens on the right; • Up to 3-grams of words, in windows of 3 tokens on the left and 3 tokens on the right; • Up to 2-grams of word shapes, in windows of 2 tokens on the left and 2 tokens on the right. Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types—e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b). We defined unigram features by conjoining these templates with each of the 22 output labels. An additional template was defined to account for label bigrams—features in this template do not look at the input string, but only at consecutive pairs of labels.8 We evaluate the ability of group-Lasso regularization to perform feature template selection. To do that, we ran 5 epochs of the sparseptron algorithm with template-based groups and budget-driven shrinkage (budgets of 10, 20, 30, 40, and 50 templates were tried). For each group 9m, we set dm = log2 |Gm|, which is the average number of bi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002b. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<tech>JMLR,</tech>
<pages>7--551</pages>
<contexts>
<context position="28515" citStr="Crammer et al., 2006" startWordPosition="4947" endWordPosition="4950">pairs of labels.8 We evaluate the ability of group-Lasso regularization to perform feature template selection. To do that, we ran 5 epochs of the sparseptron algorithm with template-based groups and budget-driven shrinkage (budgets of 10, 20, 30, 40, and 50 templates were tried). For each group 9m, we set dm = log2 |Gm|, which is the average number of bits necessary to encode a feature in that group, if all features were equiprobable. We set K = 1000 (the number of instances between consecutive proximal steps). Then, we refit the model with 10 iterations of the max-loss 1-best MIRA algorithm (Crammer et al., 2006).9 Table 1 compares the F1 scores and 8State-of-the-art models use larger output contexts, such as label trigrams and 4-grams. We resort to bigram labels as we are mostly interested in identifying relevant unigram templates. 9This variant optimizes the LSVm loss (Martins et al., 2010). For the refitting, we used unregularized MIRA. For the baseline 1506 Table 1: Results for MIRA Group Lasso B = 10 B = 20 B = 30 B = 40 B = 50 text chunking. F1 (%) 93.10 92.99 93.28 93.59 93.42 93.40 model size (# features) 5,300,396 71,075 158,844 389,065 662,018 891,378 MIRA Lasso C = 0.1 C = 0.5 C = 1 Group-L</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online Passive-Aggressive Algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--380</pages>
<contexts>
<context position="2191" citStr="Pietra et al., 1997" startWordPosition="306" endWordPosition="309"> Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our focus is on methods which embed this selection into the learning problem via the regularization term. We depart from previous approaches in that we seek to make decisions jointly about all candidate features, and we want to promote sparsity patterns that go beyond the mere cardinality of the set of features. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regul</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>Y Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>JMLR,</journal>
<pages>10--2873</pages>
<contexts>
<context position="19108" citStr="Duchi and Singer, 2009" startWordPosition="3258" endWordPosition="3261"> stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm of Martins et al. (2011) is that it effectively handles overlapping groups, without the need of evaluating proxΩ (which, as seen in §3.3, can be costly if G is not treestructured). To do so, it decomposes Q as Q(θ) = EJj=1 QjQj(θ) (15) for some J ≥ 1, and nonnegative Q1, ... , QJ; each Qj-proximal operator is assumed easy to compute. Such a decomposition always exists: if G does not have overlapping groups, take J = 1. Otherwise, find J ≤ M disjoint sets G1, ... , GJ such that �J j=1 Gj = Gand the groups on each Gj are nonoverlapp</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>J. Duchi and Y. Singer. 2009. Efficient online and batch learning using forward backward splitting. JMLR, 10:2873–2908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3256" citStr="Eisenstein et al. (2011)" startWordPosition="483" endWordPosition="486">eatures individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2-regularized ones. Compared with L1-regularized models, ours are often more accurate and yield faster runtime. 1500 Proceedings</context>
<context position="9942" citStr="Eisenstein et al., 2011" startWordPosition="1652" endWordPosition="1655">the feature space: the groups cover all the features (Um Gm = {1, . . . ,D}), and they do not overlap (Ga ∩Gb = ∅, ba =� b). Then, ΩGL d is termed a non-overlapping group-Lasso regularizer. It encourages sparsity patterns in which entire groups are discarded. A judicious choice of groups can lead to very compact 1In the statistics literature, such mixed-norm regularizers, which group features and then apply a separate norm for each group, are called composite absolute penalties (Zhao et al., 2009); other norms besides L2,1 can be used, such as L∞,1 (Quattoni et al., 2009; Wright et al., 2009; Eisenstein et al., 2011). 2Note that Eqs. 8 and 6 do not become exactly the same: in Eq. 6, the L2 norm is squared. However it can be shown that both regularizers lead to identical learning problems (Eq. 2) up to a transformation of the regularization constant. models and pinpoint relevant groups of features. The following examples lie in this category: • The two cases above (L1 and L2 regularization). • Label-based groups. In multi-label classification, where � = {1, ... , L}, features are typically designed as conjunctions of input features with label indicators, i.e., they take the form φ(x, y) = ψ(x) ® ey, where </context>
<context position="37928" citStr="Eisenstein et al. (2011)" startWordPosition="6527" endWordPosition="6530">09; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusions In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space. We have shown how the latter can be useful in model design, through the use of regularizers which promote structured sparsity. We propose an online algorithm with minimal memory requirements for exploring large feature spaces. Our algorithm, which specializes into the sparseptron, yields a mechanism for selecting entire groups of features. We apply sparseptron for selecting feature templates in three structured</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A T Figueiredo</author>
</authors>
<title>Adaptive sparseness using Jeffreys’ prior.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="15452" citStr="Figueiredo (2002)" startWordPosition="2643" endWordPosition="2644">he prior knowledge encoded in the group-Lasso regularizer (Eq. 8) comes with a Bayesian interpretation, as we next describe. In a probabilistic model (e.g. in the CRF case, where L = LCRF), the optimization problem in Eq. 2 can be seen as maximum a posteriori estimation of θ, where the regularization term Ω(θ) corresponds to the negative log of a prior distribution (call it p(θ)). It is well-known that L2-regularization corresponds to choosing independent zero-mean Gaussian priors, Od ∼ N(0, A−1), and that L1-regularization results from adopting zero-mean Laplacian priors, p(Od) ∝ exp(T|Od|). Figueiredo (2002) provided an alternative interpretation of L1-regularization in terms of a twolevel hierarchical Bayes model, which happens to generalize to the non-overlapping group-Lasso case, where Ω = ΩGL d . As in the L2-case, we also assume that each parameter receives a zero-mean Gaussian prior, but now with a group-specific variance Tm, i.e., θm ∼ N(0, TmI) for m = 1, ... , M. This reflects the fact that some groups should have their feature weights shrunk more towards zero than others. The variances Tm ≥ 0 are not pre-specified but rather generated by a one-sided exponential hyperprior p(Tm|dm) ∝ exp</context>
</contexts>
<marker>Figueiredo, 2002</marker>
<rawString>M.A.T. Figueiredo. 2002. Adaptive sparseness using Jeffreys’ prior. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
<author>T Hastie</author>
<author>R Tibshirani</author>
</authors>
<title>A note on the group lasso and a sparse group lasso.</title>
<date>2010</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="12979" citStr="Friedman et al. (2010)" startWordPosition="2196" endWordPosition="2199">idea is also used in multitask learning, where labels correspond to tasks (Caruana, 1997). 1502 where a group of features is only selected if all its ancestors are also selected.4 Hence, entire subtrees in the diagram can be pruned away. Examples are: see Fig. 1). The result is a “coarse-to-fine” regularizer, which prefers to select feature templates that are coarser before zooming into finer features. • The elastic net. The diagram of 9 has a root node for G1 = {1, ... , D} and D leaf nodes, one per each singleton group (see Fig. 1). • The sparse group-Lasso. This regularizer was proposed by Friedman et al. (2010): ΩSGL d,τ (θ) = �M&apos; m=1 (dmkθmk2 + Tmkθmk1) , (9) where the total number of groups is M = M0 + D, and the components θ1, ... , θM&apos; are nonoverlapping. This regularizer promotes sparsity at both group and feature levels (i.e., it eliminates entire groups and sparsifies within each group). Graph-structured groups. In general, the groups in 9 may overlap without being nested. In this case, the Hasse diagram of 9 is a directed acyclic graph (DAG). As in the tree-structured case, a group of features is only selected if all its ancestors are also selected. Based on this property, Jenatton et al. (2</context>
<context position="37352" citStr="Friedman et al., 2010" startWordPosition="6435" endWordPosition="6438">tured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regress</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2010</marker>
<rawString>J. Friedman, T. Hastie, and R. Tibshirani. 2010. A note on the group lasso and a sparse group lasso. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>G Andrew</author>
<author>M Johnson</author>
<author>K Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6781" citStr="Gao et al. (2007)" startWordPosition="1092" endWordPosition="1095"> essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Spa</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>J. Gao, G. Andrew, M. Johnson, and K. Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="6354" citStr="Goodman, 2004" startWordPosition="1021" endWordPosition="1022">arization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears </context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>J. Goodman. 2004. Exponential priors for maximum entropy models. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grac¸a</author>
<author>K Ganchev</author>
<author>B Taskar</author>
<author>F Pereira</author>
</authors>
<title>Posterior vs. parameter sparsity in latent variable models.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<marker>Grac¸a, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>J. Grac¸a, K. Ganchev, B. Taskar, and F. Pereira. 2009. Posterior vs. parameter sparsity in latent variable models. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Guyon</author>
<author>A Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<contexts>
<context position="2218" citStr="Guyon and Elisseeff, 2003" startWordPosition="310" endWordPosition="313">., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our focus is on methods which embed this selection into the learning problem via the regularization term. We depart from previous approaches in that we seek to make decisions jointly about all candidate features, and we want to promote sparsity patterns that go beyond the mere cardinality of the set of features. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote struc</context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>I. Guyon and A. Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jenatton</author>
<author>J-Y Audibert</author>
<author>F Bach</author>
</authors>
<title>Structured variable selection with sparsity-inducing norms.</title>
<date>2009</date>
<tech>Technical report, arXiv:0904.3523.</tech>
<contexts>
<context position="3163" citStr="Jenatton et al., 2009" startWordPosition="467" endWordPosition="470">eatures. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2-regularized ones. Compared with L</context>
<context position="6001" citStr="Jenatton et al., 2009" startWordPosition="960" endWordPosition="963">φ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regula</context>
<context position="13583" citStr="Jenatton et al. (2009)" startWordPosition="2303" endWordPosition="2306">dman et al. (2010): ΩSGL d,τ (θ) = �M&apos; m=1 (dmkθmk2 + Tmkθmk1) , (9) where the total number of groups is M = M0 + D, and the components θ1, ... , θM&apos; are nonoverlapping. This regularizer promotes sparsity at both group and feature levels (i.e., it eliminates entire groups and sparsifies within each group). Graph-structured groups. In general, the groups in 9 may overlap without being nested. In this case, the Hasse diagram of 9 is a directed acyclic graph (DAG). As in the tree-structured case, a group of features is only selected if all its ancestors are also selected. Based on this property, Jenatton et al. (2009) suggested a way of reverse engineering the groups from the desired sparsity pattern. We next describe a strategy for coarse-to-fine feature template selection that directly builds on that idea. Suppose that we are given M feature templates 7 = {T1, ... , TM} which are partially ordered according to some criterion, such that if Ta :� Tb we would like to include Tb in our model only if Ta is also included. This criterion could be a measure of coarseness: we may want to let coarser part-ofspeech features precede finer lexical features, e.g., p0 ∧ p1 :� w0 ∧ w1, or conjoined features come after t</context>
<context position="37329" citStr="Jenatton et al., 2009" startWordPosition="6431" endWordPosition="6434">of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) </context>
</contexts>
<marker>Jenatton, Audibert, Bach, 2009</marker>
<rawString>R. Jenatton, J.-Y. Audibert, and F. Bach. 2009. Structured variable selection with sparsity-inducing norms. Technical report, arXiv:0904.3523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jenatton</author>
<author>J Mairal</author>
<author>G Obozinski</author>
<author>F Bach</author>
</authors>
<title>Proximal methods for sparse hierarchical dictionary learning.</title>
<date>2010</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="3210" citStr="Jenatton et al., 2010" startWordPosition="475" endWordPosition="478">ect entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2-regularized ones. Compared with L1-regularized models, ours are often more accur</context>
<context position="18032" citStr="Jenatton et al., 2010" startWordPosition="3087" endWordPosition="3090">ding) for the QL1 τ-proximity operator: { Bd − τ if Bd &gt; τ 0 if |Bd |≤ τ Bd + τ if Bd &lt; −τ. For the non-overlapping group Lasso case, the proximity operator is given by � 0 if kθmk2 ≤ dm [proxΩGL (θ)]m = 110.112−d. 110.112 θm otherwise. (14) which can be seen as a generalization of Eq. 13: if the L2-norm of the m-th group is less than dm, the entire group is discarded; otherwise it is scaled so that its L2-norm decreases by an amount of dm. When groups overlap, the proximity operator lacks a closed form. When G is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm We now turn our attention to efficient ways of handling group-Lasso regularizers. Several fast and scalable algorithms having been proposed for training L1-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends th</context>
<context position="37765" citStr="Jenatton et al. (2010)" startWordPosition="6500" endWordPosition="6503">tured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusions In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space. We have shown how the latter can be useful in model design, through the use of regularizers which promote structured sparsity. We propose an online algorithm with minimal memory requirements for exploring large feature spaces. Our algorithm, which spe</context>
</contexts>
<marker>Jenatton, Mairal, Obozinski, Bach, 2010</marker>
<rawString>R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. 2010. Proximal methods for sparse hierarchical dictionary learning. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>J Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6338" citStr="Kazama and Tsujii, 2003" startWordPosition="1017" endWordPosition="1020">ur attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison </context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>J. Kazama and J. Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E P Xing</author>
</authors>
<title>Tree-guided group lasso for multi-task regression with structured sparsity.</title>
<date>2010</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="3140" citStr="Kim and Xing, 2010" startWordPosition="463" endWordPosition="466">lity of the set of features. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2-regularize</context>
<context position="37616" citStr="Kim and Xing (2010)" startWordPosition="6475" endWordPosition="6478">ile our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusions In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space. We have shown how the latter can be useful in model design, through the use of regularizers which prom</context>
</contexts>
<marker>Kim, Xing, 2010</marker>
<rawString>S. Kim and E.P. Xing. 2010. Tree-guided group lasso for multi-task regression with structured sparsity. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML.</booktitle>
<contexts>
<context position="1494" citStr="Lafferty et al., 2001" startWordPosition="198" endWordPosition="201">edge. We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stim</context>
<context position="5004" citStr="Lafferty et al., 2001" startWordPosition="783" endWordPosition="786">y∈�(x) θ · φ(x, y), (1) where φ(x, y) E RD is a vector of features, and θ E RD is the vector of corresponding weights. Let D = {(xi, yi)}Ni=1 be a training sample. We assume a cost function is defined such that c(y, y) is the cost of predicting y when the true output is y; our goal is to learn θ with small expected cost on unseen data. To achieve this goal, linear models are usually trained by solving a problem of the form θ = arg minθ Ω(θ) + iv EN1 L(θ, xi, yi), (2) where Ω is a regularizer and L is a loss function. Examples of losses are: the negative conditional loglikelihood used in CRFs (Lafferty et al., 2001), LCRF(θ, x, y) = − log Pθ(y|x), (3) where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear model; the margin rescaled loss of structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004), LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4) y&apos;∈�(x) where δφ(y0) = φ(x, y0)−φ(x, y); and the loss underlying the structured perceptron (Collins, 2002a), LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the ch</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R G Lanckriet</author>
<author>N Cristianini</author>
<author>P Bartlett</author>
<author>L El Ghaoui</author>
<author>M I Jordan</author>
</authors>
<title>Learning the kernel matrix with semidefinite programming.</title>
<date>2004</date>
<journal>JMLR,</journal>
<volume>5</volume>
<pages>72</pages>
<marker>Lanckriet, Cristianini, Bartlett, El Ghaoui, Jordan, 2004</marker>
<rawString>G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. 2004. Learning the kernel matrix with semidefinite programming. JMLR, 5:27– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Langford</author>
<author>L Li</author>
<author>T Zhang</author>
</authors>
<title>Sparse online learning via truncated gradient.</title>
<date>2009</date>
<journal>JMLR,</journal>
<pages>10--777</pages>
<contexts>
<context position="18547" citStr="Langford et al., 2009" startWordPosition="3165" endWordPosition="3168"> is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm We now turn our attention to efficient ways of handling group-Lasso regularizers. Several fast and scalable algorithms having been proposed for training L1-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we h</context>
<context position="21396" citStr="Langford et al. (2009)" startWordPosition="3675" endWordPosition="3679">gradient vector need to be inserted in the active set; for some losses (LSVM and LSP) many irrelevant features are never instantianted. • Convergence. With high probability, Alg. 1 produces an c-accurate solution after T ≤ O(1/62) rounds, for a suitable choice of stepsizes and holding ujt constant, ujt = uj (Martins et al., 2011). This result can be generalized to any sequence (ujt)Tt=1 such that uj = T �t 1 ujt. We next describe several algorithmic ingredients that make Alg. 1 effective in sparse modeling. Budget-Driven Shrinkage. Alg. 1 requires the choice of a “gravity sequence.” We follow Langford et al. (2009) and set (ujt)Jj=1 to zero for all t which is not a multiple of some prespecified integer K; this way, proximal steps need only be performed each K rounds, yielding a significant speed-up when the number of groups M is large. A direct adoption of the method of Langford et al. (2009) would set ujt = Kuj for those rounds; however, we have observed that such a strategy makes the number of groups vary substantially in early epochs. We use a different strategy: for each Gj, we specify a budget of Bj ≥ 0 groups (this may take into consideration practical limitations, such as the available memory). I</context>
</contexts>
<marker>Langford, Li, Zhang, 2009</marker>
<rawString>J. Langford, L. Li, and T. Zhang. 2009. Sparse online learning via truncated gradient. JMLR, 10:777–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lavergne</author>
<author>O Capp´e</author>
<author>F Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>T. Lavergne, O. Capp´e, and F. Yvon. 2010. Practical very large scale CRFs. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>J Ye</author>
</authors>
<title>Moreau-Yosida regularization for grouped tree structure learning.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6019" citStr="Liu and Ye, 2010" startWordPosition="964" endWordPosition="967">mparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusu</context>
<context position="19053" citStr="Liu and Ye, 2010" startWordPosition="3247" endWordPosition="3250">olovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm of Martins et al. (2011) is that it effectively handles overlapping groups, without the need of evaluating proxΩ (which, as seen in §3.3, can be costly if G is not treestructured). To do so, it decomposes Q as Q(θ) = EJj=1 QjQj(θ) (15) for some J ≥ 1, and nonnegative Q1, ... , QJ; each Qj-proximal operator is assumed easy to compute. Such a decomposition always exists: if G does not have overlapping groups, take J = 1. Otherwise, find J ≤ M disjoint sets G1, ... , GJ such that</context>
<context position="37635" citStr="Liu and Ye (2010)" startWordPosition="6479" endWordPosition="6482">asizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusions In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space. We have shown how the latter can be useful in model design, through the use of regularizers which promote structured spar</context>
</contexts>
<marker>Liu, Ye, 2010</marker>
<rawString>J. Liu and J. Ye. 2010. Moreau-Yosida regularization for grouped tree structure learning. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mairal</author>
<author>R Jenatton</author>
<author>G Obozinski</author>
<author>F Bach</author>
</authors>
<title>Network flow algorithms for structured sparsity.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="37660" citStr="Mairal et al. (2010)" startWordPosition="6484" endWordPosition="6487">learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusions In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space. We have shown how the latter can be useful in model design, through the use of regularizers which promote structured sparsity. We propose an onlin</context>
</contexts>
<marker>Mairal, Jenatton, Obozinski, Bach, 2010</marker>
<rawString>J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. 2010. Network flow algorithms for structured sparsity. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5502" citStr="Martins et al., 2010" startWordPosition="873" endWordPosition="876"> L is a loss function. Examples of losses are: the negative conditional loglikelihood used in CRFs (Lafferty et al., 2001), LCRF(θ, x, y) = − log Pθ(y|x), (3) where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear model; the margin rescaled loss of structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004), LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4) y&apos;∈�(x) where δφ(y0) = φ(x, y0)−φ(x, y); and the loss underlying the structured perceptron (Collins, 2002a), LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, </context>
<context position="28800" citStr="Martins et al., 2010" startWordPosition="4991" endWordPosition="4994">each group 9m, we set dm = log2 |Gm|, which is the average number of bits necessary to encode a feature in that group, if all features were equiprobable. We set K = 1000 (the number of instances between consecutive proximal steps). Then, we refit the model with 10 iterations of the max-loss 1-best MIRA algorithm (Crammer et al., 2006).9 Table 1 compares the F1 scores and 8State-of-the-art models use larger output contexts, such as label trigrams and 4-grams. We resort to bigram labels as we are mostly interested in identifying relevant unigram templates. 9This variant optimizes the LSVm loss (Martins et al., 2010). For the refitting, we used unregularized MIRA. For the baseline 1506 Table 1: Results for MIRA Group Lasso B = 10 B = 20 B = 30 B = 40 B = 50 text chunking. F1 (%) 93.10 92.99 93.28 93.59 93.42 93.40 model size (# features) 5,300,396 71,075 158,844 389,065 662,018 891,378 MIRA Lasso C = 0.1 C = 0.5 C = 1 Group-Lasso B = 100 B = 200 B = 300 Spa. dev/test 70.38/74.09 69.19/71.9 70.75/72.38 71.7/74.03 71.79/73.62 72.08/75.05 71.48/73.3 8,598,246 68,565 1,017,769 1,555,683 83,036 354,872 600,646 Dut. dev/test 69.15/71.54 64.07/66.35 66.82/69.42 70.43/71.89 69.48/72.83 71.03/73.33 71.2/72.59 5,72</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>M A T Figueiredo</author>
<author>P M Q Aguiar</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Online learning of structured predictors with multiple kernels.</title>
<date>2011</date>
<booktitle>In Proc. of AISTATS.</booktitle>
<contexts>
<context position="18747" citStr="Martins et al. (2011)" startWordPosition="3198" endWordPosition="3201">ecessary to solve Eq. 12. 4 Online Prox-Grad Algorithm We now turn our attention to efficient ways of handling group-Lasso regularizers. Several fast and scalable algorithms having been proposed for training L1-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm of Martins et al. (2011) is that it effectively handles overlapping groups, without the need of evaluating proxΩ (which, as seen in §3.3, can be costly if G is not treestructu</context>
<context position="21105" citStr="Martins et al., 2011" startWordPosition="3627" endWordPosition="3630">: end for 9: end for 10: output: θ • Memory efficiency. Only a small active set of features (those that have nonzero weights) need to be maintained. Entire groups of features can be deleted after each proximal step. Furthermore, only the features which correspond to nonzero entries in the gradient vector need to be inserted in the active set; for some losses (LSVM and LSP) many irrelevant features are never instantianted. • Convergence. With high probability, Alg. 1 produces an c-accurate solution after T ≤ O(1/62) rounds, for a suitable choice of stepsizes and holding ujt constant, ujt = uj (Martins et al., 2011). This result can be generalized to any sequence (ujt)Tt=1 such that uj = T �t 1 ujt. We next describe several algorithmic ingredients that make Alg. 1 effective in sparse modeling. Budget-Driven Shrinkage. Alg. 1 requires the choice of a “gravity sequence.” We follow Langford et al. (2009) and set (ujt)Jj=1 to zero for all t which is not a multiple of some prespecified integer K; this way, proximal steps need only be performed each K rounds, yielding a significant speed-up when the number of groups M is large. A direct adoption of the method of Langford et al. (2009) would set ujt = Kuj for t</context>
<context position="36679" citStr="Martins et al. (2011)" startWordPosition="6322" endWordPosition="6325"> languages. Each line groups together similar templates, involving lexical, contextual POS, word shape information, as well as attachment direction and length. Empty cells denote that very few or none of the templates in that category was selected; + denotes that some were selected; ++ denotes that most or all were selected. (2011), along with a theoretical analysis. The focus there, however, was multiple kernel learning, hence overlapping groups were not considered in their experiments. Budget-driven shrinkage and the sparseptron are novel techniques, at the best of our knowledge. Apart from Martins et al. (2011), the only work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Ba</context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Smith, Xing, 2011</marker>
<rawString>A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. 2011. Online learning of structured predictors with multiple kernels. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of UAI.</booktitle>
<contexts>
<context position="2235" citStr="McCallum, 2003" startWordPosition="314" endWordPosition="315">d perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our focus is on methods which embed this selection into the learning problem via the regularization term. We depart from previous approaches in that we seek to make decisions jointly about all candidate features, and we want to promote sparsity patterns that go beyond the mere cardinality of the set of features. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. S</context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>A. McCallum. 2003. Efficiently inducing features of conditional random fields. In Proc. of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="33885" citStr="McDonald et al., 2005" startWordPosition="5840" endWordPosition="5843">on-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based method based on information gain for selecting feature templates in multilingual dependency parsing. The x-axis is the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score. The plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method. The standard Lasso (which does not select templates, but individual features) is also shown for comparison. We use arc-factored models, for which exact inference is tractable (McDonald et al., 2005). We defined M = 684 feature templates for each candidate arc by conjoining the words, shapes, lemmas, and POS of the head and the modifier, as well as the contextual POS, and the distance and direction of attachment. We followed the same two-stage approach as before, and compared with a baseline which selects feature templates by ranking them according to the information gain criterion. This baseline assigns a score to each template T,,t which reflects an empirical estimate of the mutual information between T,,t and the binary variable A that indicates the presence/absence of a dependency lin</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Obozinski</author>
<author>B Taskar</author>
<author>M I Jordan</author>
</authors>
<title>Joint covariate selection and joint subspace selection for multiple classification problems.</title>
<date>2010</date>
<journal>Statistics and Computing,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="3187" citStr="Obozinski et al., 2010" startWordPosition="471" endWordPosition="474">e want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2-regularized ones. Compared with L1-regularized models, ou</context>
<context position="37377" citStr="Obozinski et al., 2010" startWordPosition="6439" endWordPosition="6442">uctured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Eisenstein et al. (2011) in a multi-task regression problem. 7 Conclusion</context>
</contexts>
<marker>Obozinski, Taskar, Jordan, 2010</marker>
<rawString>G. Obozinski, B. Taskar, and M.I. Jordan. 2010. Joint covariate selection and joint subspace selection for multiple classification problems. Statistics and Computing, 20(2):231–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>20--1153</pages>
<contexts>
<context position="6848" citStr="Petrov and Klein (2008" startWordPosition="1103" endWordPosition="1106">n (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sparsity We are interested in regularizers that share with ΩL1 τ the a</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>S. Petrov and D. Klein. 2008a. Discriminative log-linear grammars with latent variables. Advances in Neural Information Processing Systems, 20:1153–1160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Sparse multi-scale grammars for discriminative latent variable parsing.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6848" citStr="Petrov and Klein (2008" startWordPosition="1103" endWordPosition="1106">n (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sparsity We are interested in regularizers that share with ΩL1 τ the a</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>S. Petrov and D. Klein. 2008b. Sparse multi-scale grammars for discriminative latent variable parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quattoni</author>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Darrell</author>
</authors>
<title>An efficient projection for l1,∞ regularization.</title>
<date>2009</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="9895" citStr="Quattoni et al., 2009" startWordPosition="1644" endWordPosition="1647">consider the case where 9 is a partition of the feature space: the groups cover all the features (Um Gm = {1, . . . ,D}), and they do not overlap (Ga ∩Gb = ∅, ba =� b). Then, ΩGL d is termed a non-overlapping group-Lasso regularizer. It encourages sparsity patterns in which entire groups are discarded. A judicious choice of groups can lead to very compact 1In the statistics literature, such mixed-norm regularizers, which group features and then apply a separate norm for each group, are called composite absolute penalties (Zhao et al., 2009); other norms besides L2,1 can be used, such as L∞,1 (Quattoni et al., 2009; Wright et al., 2009; Eisenstein et al., 2011). 2Note that Eqs. 8 and 6 do not become exactly the same: in Eq. 6, the L2 norm is squared. However it can be shown that both regularizers lead to identical learning problems (Eq. 2) up to a transformation of the regularization constant. models and pinpoint relevant groups of features. The following examples lie in this category: • The two cases above (L1 and L2 regularization). • Label-based groups. In multi-label classification, where � = {1, ... , L}, features are typically designed as conjunctions of input features with label indicators, i.e.,</context>
</contexts>
<marker>Quattoni, Carreras, Collins, Darrell, 2009</marker>
<rawString>A. Quattoni, X. Carreras, M. Collins, and T. Darrell. 2009. An efficient projection for l1,∞ regularization. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000.</booktitle>
<contexts>
<context position="26878" citStr="Sang and Buchholz, 2000" startWordPosition="4659" endWordPosition="4662">el without any regularization and using the loss L which one wants to optimize. 7To see why this is the case, note that both gradient and proximal updates come scaled by 770; and that the gradient of the loss is ∇LSP(θ, xt, yt) = O(xt, yt) − O(xt, yt), where yt is the prediction under the current model, which is insensitive to the scaling of θ. This independence on 770 does not hold when the loss is LSVm or LCRF. 5 Experiments We present experiments in three structured prediction tasks for several group choices. Text Chunking. We use the English dataset provided in the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of 8,936 training and 2,012 testing sentences (sections 15–18 and 20 of the WSJ.) The input observations are the token words and their POS tags; we want to predict the sequences of TOB tags representing phrase chunks. We built 96 contextual feature templates as follows: • Up to 5-grams of POS tags, in windows of 5 tokens on the left and 5 tokens on the right; • Up to 3-grams of words, in windows of 3 tokens on the left and 3 tokens on the right; • Up to 2-grams of word shapes, in windows of 2 tokens on the left and 2 tokens on the right. Each shape replaces characters by their</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E.F.T.K. Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E.F.T.K. Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
</authors>
<title>Introduction to the CoNLL2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="30833" citStr="Sang, 2002" startWordPosition="5322" endWordPosition="5323">e original set of features. Note that the total number of iterations is the same; yet, the group-Lasso approach has a much smaller memory footprint (see Fig. 2) and yields much more compact models. The small memory footprint comes from the fact that Alg. 1 may entertain a large number of features without ever instantiating all of them. The predictive power is comparable (although some choices of budget yield slightly better scores for the group-Lasso approach).10 Named Entity Recognition. We experiment with the Spanish, Dutch, and English datasets provided in the CoNLL 2002/2003 shared tasks (Sang, 2002; Sang and De Meulder, 2003). For Spanish, we use the POS tags provided by Car(described next), we used L2-regularized MIRA and tuned the regularization constant with cross-validation. 10We also tried label-based group-Lasso and sparse groupLasso (§3.1), with less impressive results (omitted for space). reras (http://www.lsi.upc.es/˜nlp/tools/ nerc/nerc.html); for English, we ignore the syntactic chunk tags provided with the dataset. Hence, all datasets have the same sort of input observations (words and POS) and all have 9 output labels. We use the feature templates described above plus some </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E.F.T.K. Sang. 2002. Introduction to the CoNLL2002 shared task: Language-independent named entity recognition. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schmidt</author>
<author>K Murphy</author>
</authors>
<title>Convex structure learning in log-linear models: Beyond pairwise potentials.</title>
<date>2010</date>
<booktitle>In Proc. of AISTATS.</booktitle>
<contexts>
<context position="36801" citStr="Schmidt and Murphy (2010)" startWordPosition="6342" endWordPosition="6345"> well as attachment direction and length. Empty cells denote that very few or none of the templates in that category was selected; + denotes that some were selected; ++ denotes that most or all were selected. (2011), along with a theoretical analysis. The focus there, however, was multiple kernel learning, hence overlapping groups were not considered in their experiments. Budget-driven shrinkage and the sparseptron are novel techniques, at the best of our knowledge. Apart from Martins et al. (2011), the only work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapp</context>
</contexts>
<marker>Schmidt, Murphy, 2010</marker>
<rawString>M. Schmidt and K. Murphy. 2010. Convex structure learning in log-linear models: Beyond pairwise potentials. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>N Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for SVM.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="23881" citStr="Shalev-Shwartz et al. (2007)" startWordPosition="4132" endWordPosition="4135"> label-based groups (see §3.1). It consists of making lazy updates (Carpenter, 2008), i.e., to delay the update of all features in a group until at least one of them fires; then apply a cumulative penalty. The amount of the penalty can be computed if one assigns a timestamp to each group. • The second strategy is suitable when M is small and some groups are very populated; this is the typical case of template-based groups (§3.1). Two operations need to be performed: updating each feature weight (in the gradient steps), and scaling entire groups (in the proximal steps). We adapt a trick due to Shalev-Shwartz et al. (2007): represent the weight vector of the m-th group, θm, by a 5When overlaps exist (e.g. the coarse-to-fine case), we specify a total pseudo-budget B ignoring the overlaps, which induces budgets Bl, ... , BJ which sum to B. The number of actually selected groups may be less than B, however, since in this case some groups can be shrunk more than once. Other heuristics are possible. 6The convergence assumption can be sidestepped by freezing the vj after a fixed number of iterations. 1505 triple (ξm, cm, pm) E R|Gm|xR+xR+, such that θm = cmξm and 11θm112 = pm. This representation allows performing th</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sokolovska</author>
<author>T Lavergne</author>
<author>O Capp´e</author>
<author>F Yvon</author>
</authors>
<title>Efficient learning of sparse conditional random fields for supervised sequence labelling.</title>
<date>2010</date>
<journal>IEEE Journal of Selected Topics in Signal Processing,</journal>
<volume>4</volume>
<issue>6</issue>
<marker>Sokolovska, Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>N. Sokolovska, T. Lavergne, O. Capp´e, and F. Yvon. 2010. Efficient learning of sparse conditional random fields for supervised sequence labelling. IEEE Journal of Selected Topics in Signal Processing, 4(6):953–964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1571" citStr="Taskar et al., 2003" startWordPosition="210" endWordPosition="213">, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della</context>
<context position="5161" citStr="Taskar et al., 2003" startWordPosition="813" endWordPosition="816">le. We assume a cost function is defined such that c(y, y) is the cost of predicting y when the true output is y; our goal is to learn θ with small expected cost on unseen data. To achieve this goal, linear models are usually trained by solving a problem of the form θ = arg minθ Ω(θ) + iv EN1 L(θ, xi, yi), (2) where Ω is a regularizer and L is a loss function. Examples of losses are: the negative conditional loglikelihood used in CRFs (Lafferty et al., 2001), LCRF(θ, x, y) = − log Pθ(y|x), (3) where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear model; the margin rescaled loss of structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004), LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4) y&apos;∈�(x) where δφ(y0) = φ(x, y0)−φ(x, y); and the loss underlying the structured perceptron (Collins, 2002a), LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq.</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society B.,</journal>
<pages>267--288</pages>
<contexts>
<context position="6463" citStr="Tibshirani (1996)" startWordPosition="1042" endWordPosition="1043">del by promoting structured sparsity. While this has been a topic of intense research in signal processing and computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: • L2-regularization (Chen and Rosenfeld, 2000): Ωλ2(θ) °λ2 IθI22 = 2 ED 1 02; Bdi (6) • L1-regularization (Kazama and Tsujii, 2003; Goodman, 2004): ΩL1 τ (θ) ° τ11θ111 = τ EDd=1 |θd|. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hast</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>R. Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society B., pages 267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1601" citStr="Tsochantaridis et al., 2004" startWordPosition="214" endWordPosition="217">t algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon an</context>
<context position="5191" citStr="Tsochantaridis et al., 2004" startWordPosition="817" endWordPosition="820">function is defined such that c(y, y) is the cost of predicting y when the true output is y; our goal is to learn θ with small expected cost on unseen data. To achieve this goal, linear models are usually trained by solving a problem of the form θ = arg minθ Ω(θ) + iv EN1 L(θ, xi, yi), (2) where Ω is a regularizer and L is a loss function. Examples of losses are: the negative conditional loglikelihood used in CRFs (Lafferty et al., 2001), LCRF(θ, x, y) = − log Pθ(y|x), (3) where Pθ(y|x) a exp(θ · φ(x, y)) is a log-linear model; the margin rescaled loss of structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004), LSVM(θ, x, y) = max θ · δφ(y0) + c(y0, y), (4) y&apos;∈�(x) where δφ(y0) = φ(x, y0)−φ(x, y); and the loss underlying the structured perceptron (Collins, 2002a), LSP(θ, x, y) = maxy&apos;∈�(x) θ · δφ(y0). (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address wa</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
<author>S Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized loglinear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="18571" citStr="Tsuruoka et al., 2009" startWordPosition="3169" endWordPosition="3172"> can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm We now turn our attention to efficient ways of handling group-Lasso regularizers. Several fast and scalable algorithms having been proposed for training L1-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochastic gradient descent training for l1-regularized loglinear models with cumulative penalty. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Wright</author>
<author>R Nowak</author>
<author>M A T Figueiredo</author>
</authors>
<title>Sparse reconstruction by separable approximation.</title>
<date>2009</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>57</volume>
<issue>7</issue>
<pages>2493</pages>
<contexts>
<context position="9916" citStr="Wright et al., 2009" startWordPosition="1648" endWordPosition="1651"> 9 is a partition of the feature space: the groups cover all the features (Um Gm = {1, . . . ,D}), and they do not overlap (Ga ∩Gb = ∅, ba =� b). Then, ΩGL d is termed a non-overlapping group-Lasso regularizer. It encourages sparsity patterns in which entire groups are discarded. A judicious choice of groups can lead to very compact 1In the statistics literature, such mixed-norm regularizers, which group features and then apply a separate norm for each group, are called composite absolute penalties (Zhao et al., 2009); other norms besides L2,1 can be used, such as L∞,1 (Quattoni et al., 2009; Wright et al., 2009; Eisenstein et al., 2011). 2Note that Eqs. 8 and 6 do not become exactly the same: in Eq. 6, the L2 norm is squared. However it can be shown that both regularizers lead to identical learning problems (Eq. 2) up to a transformation of the regularization constant. models and pinpoint relevant groups of features. The following examples lie in this category: • The two cases above (L1 and L2 regularization). • Label-based groups. In multi-label classification, where � = {1, ... , L}, features are typically designed as conjunctions of input features with label indicators, i.e., they take the form φ</context>
<context position="25343" citStr="Wright et al., 2009" startWordPosition="4380" endWordPosition="4383">vene in the gradient computed in line 5 need to be instantiated; and all features that receive zero weights after some proximal step can be deleted from the model (cf. Fig. 2). Sparseptron and Debiasing. Although Alg. 1 allows to simultaneously select features and learn the model parameters, it has been observed in the sparse modeling literature that Lasso-like regularizers usually have a strong bias which may harm predictive performance. A post-processing stage is usually taken (called debiasing), in which the model is refitted without any regularization and using only the selected features (Wright et al., 2009). If a final debiasing stage is to be performed, Alg. 1 only needs to worry about feature selection, hence it is appealing to choose a loss function that makes this procedure as simple as possible. Examining the input of Alg. 1, we see that both a gravity and a stepsize sequence need to be specified. The former can be taken care of by using budget-driven shrinkage, as described above. The stepsize sequence can be set � as qt = q0/ Ft/N], which ensures convergence, however q0 requires tuning. Fortunately, for the structured perceptron loss LSP (Eq. 5), Alg. 1 is independent of q0, up to a scali</context>
</contexts>
<marker>Wright, Nowak, Figueiredo, 2009</marker>
<rawString>S.J. Wright, R. Nowak, and M.A.T. Figueiredo. 2009. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479– 2493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xiao</author>
</authors>
<title>Dual averaging methods for regularized stochastic learning and online optimization.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="19121" citStr="Xiao, 2009" startWordPosition="3262" endWordPosition="3263">arpenter, 2008; Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm of Martins et al. (2011) is that it effectively handles overlapping groups, without the need of evaluating proxΩ (which, as seen in §3.3, can be costly if G is not treestructured). To do so, it decomposes Q as Q(θ) = EJj=1 QjQj(θ) (15) for some J ≥ 1, and nonnegative Q1, ... , QJ; each Qj-proximal operator is assumed easy to compute. Such a decomposition always exists: if G does not have overlapping groups, take J = 1. Otherwise, find J ≤ M disjoint sets G1, ... , GJ such that �J j=1 Gj = Gand the groups on each Gj are nonoverlapping. The prox</context>
</contexts>
<marker>Xiao, 2009</marker>
<rawString>L. Xiao. 2009. Dual averaging methods for regularized stochastic learning and online optimization. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yuan</author>
<author>Y Lin</author>
</authors>
<title>Model selection and estimation in regression with grouped variables.</title>
<date>2006</date>
<journal>Journal of the Royal Statistical Society (B),</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="8575" citStr="Yuan and Lin (2006)" startWordPosition="1395" endWordPosition="1398">3.1 The Group Lasso To capture the structure of the feature space, we group our D features into M groups G1, ... , GM, where each Gm ⊆ {1, ... , D}. Ahead, we discuss meaningful ways of choosing group decompositions; for now, let us assume a sensible choice is obvious to the model designer. Denote by θm = (Bd)dEGm the subvector of those weights that correspond to the features in the m-th group, and let d1, ... , dM be nonnegative scalars (one per group). We consider the following group-Lasso regularizers: ΩGL d = EM m=1 dm11θm112. (8) These regularizers were first proposed by Bakin (1999) and Yuan and Lin (2006) in the context of regression. If d1 = ... = dM, ΩGL dbecomes the “L1 norm of the L2 norms.” Interestingly, this is also a norm, called the mixed L2,1-norm.1 These regularizers subsume the L1 and L2 cases, which correspond to trivial choices of groups: • If each group is a singleton, i.e., M = D and Gd = {Bd}, and d1 = ... = dM = T, we recover L1-regularization (cf. Eqs. 7–8). • If there is a single group spanning all the features, i.e., M = 1 and G1 = {1, ... , D}, then the right hand side of Eq. 8 becomes d111θ112. This is equivalent to L2 regularization.2 We next present some non-trivial ex</context>
<context position="37217" citStr="Yuan and Lin (2006)" startWordPosition="6412" endWordPosition="6415"> novel techniques, at the best of our knowledge. Apart from Martins et al. (2011), the only work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed nor</context>
</contexts>
<marker>Yuan, Lin, 2006</marker>
<rawString>M. Yuan and Y. Lin. 2006. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society (B), 68(1):49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zhao</author>
<author>G Rocha</author>
<author>B Yu</author>
</authors>
<title>Grouped and hierarchical model selection through composite absolute penalties. Annals of Statistics,</title>
<date>2009</date>
<pages>37--6</pages>
<contexts>
<context position="3120" citStr="Zhao et al., 2009" startWordPosition="459" endWordPosition="462">nd the mere cardinality of the set of features. For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compac</context>
<context position="9820" citStr="Zhao et al., 2009" startWordPosition="1630" endWordPosition="1633"> topologies of 9 = {G1, ... , GM}. Non-overlapping groups. Let us first consider the case where 9 is a partition of the feature space: the groups cover all the features (Um Gm = {1, . . . ,D}), and they do not overlap (Ga ∩Gb = ∅, ba =� b). Then, ΩGL d is termed a non-overlapping group-Lasso regularizer. It encourages sparsity patterns in which entire groups are discarded. A judicious choice of groups can lead to very compact 1In the statistics literature, such mixed-norm regularizers, which group features and then apply a separate norm for each group, are called composite absolute penalties (Zhao et al., 2009); other norms besides L2,1 can be used, such as L∞,1 (Quattoni et al., 2009; Wright et al., 2009; Eisenstein et al., 2011). 2Note that Eqs. 8 and 6 do not become exactly the same: in Eq. 6, the L2 norm is squared. However it can be shown that both regularizers lead to identical learning problems (Eq. 2) up to a transformation of the regularization constant. models and pinpoint relevant groups of features. The following examples lie in this category: • The two cases above (L1 and L2 regularization). • Label-based groups. In multi-label classification, where � = {1, ... , L}, features are typica</context>
<context position="37306" citStr="Zhao et al., 2009" startWordPosition="6427" endWordPosition="6430"> work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space. Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning. Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity. Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010). The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997). The treestructured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al. (2010), along with L∞,1 and L2,1 regularization. Graphstructured groups are discussed in Jenatton et al. (2010), along with a DAG representation. In NLP, mixed norms have been used recently by Grac¸a et al. (2009) in posterior regularization, and by Ei</context>
</contexts>
<marker>Zhao, Rocha, Yu, 2009</marker>
<rawString>P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hierarchical model selection through composite absolute penalties. Annals of Statistics, 37(6A):3468–3497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zou</author>
<author>T Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society Series B (Statistical Methodology),</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="7072" citStr="Zou and Hastie (2005)" startWordPosition="1137" endWordPosition="1140">irani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the regularization. ΩL2 λusually leads to easier optimization and robust performance; ΩL1 τencourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sparsity We are interested in regularizers that share with ΩL1 τ the ability to promote sparsity, so that they can be used for selecting features. In addition, we want to endow the feature space RD with additional structure, so that features are not penalized individually (as in the L1-case) b</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>H. Zou and T. Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B (Statistical Methodology), 67(2):301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>