<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9978835">
Inducing Example-based Semantic Frames
from a Massive Amount of Verb Uses
</title>
<author confidence="0.989631">
Daisuke Kawahara† Daniel W. Peterson$ Octavian Popescu§ Martha Palmer$
</author>
<affiliation confidence="0.895421">
†Kyoto University, Kyoto, Japan
$University of Colorado at Boulder, Boulder, CO, USA
§Fondazione Bruno Kessler, Trento, Italy
</affiliation>
<email confidence="0.971522">
dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu
</email>
<sectionHeader confidence="0.994391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981692307692">
We present an unsupervised method for in-
ducing semantic frames from verb uses in
giga-word corpora. Our semantic frames
are verb-specific example-based frames
that are distinguished according to their
senses. We use the Chinese Restau-
rant Process to automatically induce these
frames from a massive amount of verb in-
stances. In our experiments, we acquire
broad-coverage semantic frames from two
giga-word corpora, the larger comprising
20 billion words. Our experimental results
indicate the effectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965333333333">
Semantic frames are indispensable knowledge for
semantic analysis or text understanding. In the
last decade, semantic frames, such as FrameNet
(Baker et al., 1998) and PropBank (Palmer et al.,
2005), have been manually elaborated. These
resources are effectively exploited in many nat-
ural language processing (NLP) tasks, includ-
ing not only semantic parsing but also ma-
chine translation (Boas, 2002), information ex-
traction (Surdeanu et al., 2003), question answer-
ing (Narayanan and Harabagiu, 2004), paraphrase
acquisition (Ellsworth and Janin, 2007) and recog-
nition of textual entailment (Burchardt and Frank,
2006).
There have been many attempts to automati-
cally acquire frame knowledge from raw corpora
with the goal of either adding frequency informa-
tion to an existing resource or of inducing simi-
lar frames for other languages. Most of these ap-
proaches, however, focus on syntactic frames, i.e.,
subcategorization frames (e.g., (Manning, 1993;
Briscoe and Carroll, 1997; Korhonen et al., 2006;
Lippincott et al., 2012; Reichart and Korhonen,
2013)). Since subcategorization frames represent
argument patterns of verbs and are purely syn-
tactic, expressions that have the same subcatego-
rization frame can have different meanings (e.g.,
metaphors). Semantics-oriented NLP applications
based on frames, such as paraphrase acquisition
and machine translation, require consistency in the
meaning of each frame, and thus these subcatego-
rization frames are not suitable for these semantic
tasks.
Recently, there have been a few studies on au-
tomatically acquiring semantic frames (Materna,
2012; Materna, 2013). Materna induced seman-
tic frames (called LDA-Frames) from triples of
(subject, verb, object) in the British National
Corpus (BNC) based on Latent Dirichlet Allo-
cation (LDA) and the Dirichlet Process. LDA-
Frames capture limited linguistic phenomena of
these triples, and are defined across verbs based
on probabilistic topic distributions.
This paper presents a method for automati-
cally building verb-specific semantic frames from
a large raw corpus. Our semantic frames are verb-
specific like PropBank and semantically distin-
guished. A frame has several syntactic case slots,
each of which consists of words that are eligible to
fill the slot. For example, let us show three seman-
tic frames of the verb “observe”:1
</bodyText>
<equation confidence="0.509758">
observe:1
nsubj:{we, author, ...I dobj:{effect, result, ...I
prep in:{study, case, ...I ...
observe:2
nsubj:{teacher, we, ...I dobj:{child, student, ...I
prep in:{classroom, school, ...I ...
observe:3
</equation>
<bodyText confidence="0.628121625">
nsubj:{child, people, ...I dobj:{bird, animal, ...I
prep at:{range, time, ...I ...
&apos;In this paper, we use the dependency relation names
of the Stanford collapsed dependencies (de Marneffe et al.,
2006) as the notations of case slots. For instance, “nsubj”
means a nominal subject, “dobj” means a direct object, “iboj”
means an indirect object, “ccomp” means a clausal comple-
ment and “prep *” means a preposition.
</bodyText>
<page confidence="0.981644">
58
</page>
<note confidence="0.9929575">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 58–67,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.989057375">
Frequencies, which are not shown in the above ex-
amples, are attached to each semantic frame, case
slot and word, and can be effectively exploited for
the applications of these semantic frames. The fre-
quencies of words in each case slot become good
sources of selectional preferences.
Our novel contributions are summarized as fol-
lows:
</bodyText>
<listItem confidence="0.983151916666667">
• induction of semantic frames based on the
Chinese Restaurant Process (Aldous, 1985)
from only automatic parses of a web-scale
corpus,
• exploitation of the assumption of one sense
per collocation (Yarowsky, 1993) to make the
computation feasible,
• providing broad-coverage knowledge for se-
lectional preferences, and
• evaluating induced semantic frames by us-
ing an existing annotated corpus with verb
classes.
</listItem>
<sectionHeader confidence="0.999044" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999953325">
The most closely related work to our semantic
frames are LDA-Frames, which are probabilistic
semantic frames automatically induced from a raw
corpus (Materna, 2012; Materna, 2013). He used a
model based on LDA and the Dirichlet Process to
cluster verb instances of a triple (subject, verb, ob-
ject) to produce semantic frames and slots. Both
of these are represented as a probabilistic distri-
bution of words across verbs. He applied this
method to the BNC and acquired 427 frames and
144 slots (Materna, 2013). These frames are over-
generalized across verbs and might be difficult
to provide with fine-grained selectional prefer-
ences. In addition, Grenager and Manning (2006)
proposed a method for inducing PropBank-style
frames from Stanford typed dependencies ex-
tracted from raw corpora. Although these frames
are based on typed dependencies and more seman-
tic than subcategorization frames, they are not dis-
tinguished in terms of the senses of words filling a
case slot.
There are hand-crafted semantic frames in the
lexicons of FrameNet (Baker et al., 1998) and
PropBank (Palmer et al., 2005). Corpus Pattern
Analysis (CPA) frames (Hanks, 2012) are another
manually created repository of patterns for verbs.
Each pattern represents a prototypical word usage
as extracted by lexicographers from the BNC. Cre-
ating CPA is time consuming, but our proposed
method may be employed to assist in the creation
of this type of resource, as shown in Section 4.4.
Our task can be regarded as clustering of verb
instances. In this respect, the models of Parisien
and Stevenson are related to our method (Parisien
and Stevenson, 2009; Parisien and Stevenson,
2010). Parisien and Stevenson (2009) proposed
a Dirichlet Process model for clustering usages
of the verb “get.” Later, Parisien and Stevenson
(2010) proposed a Hierarchical Dirichlet Process
model for jointly clustering argument structures
(i.e., subcategorization frames) and verb classes.
However, their argument structures are not seman-
tic but syntactic, and also they did not evaluate the
resulting frames. There have also been related ap-
proaches to clustering verb types (Vlachos et al.,
2009; Sun and Korhonen, 2009; Falk et al., 2012;
Reichart and Korhonen, 2013). These methods in-
duce verb clusters in which multiple verbs partic-
ipate, and do not consider the polysemy of verbs.
Our objective is different from theirs.
Another line of related work is unsupervised
semantic parsing or semantic role labeling (Poon
and Domingos, 2009; Lang and Lapata, 2010;
Lang and Lapata, 2011a; Lang and Lapata, 2011b;
Titov and Klementiev, 2011; Titov and Klemen-
tiev, 2012). These approaches basically clus-
ter predicates and their arguments to distinguish
predicate senses and semantic roles of arguments.
Modi et al. (2012) extended the model of Titov and
Klementiev (2012) to jointly induce semantic roles
and frames using the Chinese Restaurant Process,
which is also used in our approach. However,
they did not aim at building a lexicon of semantic
frames, but at distinguishing verbs that have dif-
ferent senses in a relatively small annotated cor-
pus. Applying this method to a large corpus could
produce a frame lexicon, but its scalability would
be a big problem.
For other languages than English, Kawahara
and Kurohashi (2006a) proposed a method for au-
tomatically compiling Japanese semantic frames
from a large web corpus. They applied con-
ventional agglomerative clustering to predicate-
argument structures using word/frame similarity
based on a manually-crafted thesaurus. Since
Japanese is head-final and has case-marking post-
positions, it seems easier to build semantic frames
with it than with other languages such as English.
They also achieved an improvement in depen-
dency parsing and predicate-argument structure
</bodyText>
<page confidence="0.997942">
59
</page>
<bodyText confidence="0.9685705">
analysis by using their resulting frames (Kawahara
and Kurohashi, 2006b).
</bodyText>
<sectionHeader confidence="0.981092" genericHeader="method">
3 Method for Inducing Semantic Frames
</sectionHeader>
<bodyText confidence="0.999824461538462">
Our objective is to automatically induce verb-
specific example-based semantic frames. Each se-
mantic frame consists of a partial set of syntactic
slots: nsubj, dobj, iobj, ccomp and prep *. Each
slot consists of words with frequencies, which
could provide broad-coverage selectional prefer-
ences.
Frames for a verb should be semantically distin-
guished. That is to say, each frame should consist
of predicate-argument structures that have consis-
tent usages or meanings.
Our procedure to automatically generate seman-
tic frames from verb usages is as follows:
</bodyText>
<listItem confidence="0.911810166666667">
1. apply dependency parsing to a raw corpus
and extract predicate-argument structures for
each verb from the automatic parses,
2. merge the predicate-argument structures that
have presumably the same meaning based on
the assumption of one sense per collocation
to get a set of initial frames, and
3. apply clustering to the initial frames based
on the Chinese Restaurant Process to produce
the final semantic frames.
Each of these steps is described in the following
sections in detail.
</listItem>
<subsectionHeader confidence="0.998371">
3.1 Extracting Predicate-argument
Structures from a Raw Corpus
</subsectionHeader>
<bodyText confidence="0.704302102564102">
We first apply dependency parsing to a large raw
corpus. We use the Stanford parser with Stanford
dependencies (de Marneffe et al., 2006).2 Col-
lapsed dependencies are adopted to directly extract
prepositional phrases.
Then, we extract predicate-argument structures
from the dependency parses. Dependents that have
the following dependency relations to a verb are
extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp,
prep *
Here, we do not distinguish adjuncts from argu-
ments. All extracted dependents of a verb are han-
dled as arguments. This distinction is left for fu-
ture work, but this will be performed using slot
zhttp://nlp.stanford.edu/software/lex-parser.shtml
Sentences:
They observed the effects of ...
This statistical ability to observe an effect ...
We did not observe a residual effect of ...
He could observe the results at the same time ...
My first opportunity to observe the results of ...
You can observe beautiful birds ...
Children may then observe birds ...
...
Predicate-argument structures:
nsubj:they observe dobj:effect
observe dobj:effect
nsubj:we observe dobj:effect
nsubj:he observe dobj:result prep at:time
observe dobj:result
nsubj:you observe dobj:bird
nsubj:child observe dobj:bird
...
Initial frames:
nsubj:{they, we, ...} observe dobj:{effect}
nsubj:{he, ...} observe dobj:{result} prep at:{time}
nsubj:{you, child, ...} observe dobj:{bird}
...
</bodyText>
<figureCaption confidence="0.983402">
Figure 1: Examples of predicate-argument struc-
tures and initial frames for the verb “observe.”
</figureCaption>
<bodyText confidence="0.9956132">
frequencies in the applications of semantic frames
or the method proposed by Abend and Rappoport
(2010).
We apply the following processes to extracted
predicate-argument structures:
</bodyText>
<listItem confidence="0.997832111111111">
• A verb and an argument are lemmatized, and
only the head of an argument is preserved for
compound nouns.
• Phrasal verbs are also distinguished from
non-phrasal verbs. For example, “look up”
has independent frames from “look.”
• The passive voice of a verb is distinguished
from the active voice, and thus these have in-
dependent frames. Passive voice is detected
using the part-of-speech tag “VBN” (past
participle). The alignment between frames of
active and passive voices will be done after
the induction of frames using the model of
Sasano et al. (2013) in the future.
• “xcomp” (open clausal complement) is re-
named to “ccomp” (clausal complement) and
“xsubj” (controlling subject) is renamed to
“nsubj” (nominal subject). This is because
</listItem>
<page confidence="0.958959">
60
</page>
<bodyText confidence="0.891772">
these usages as predicate-argument structures
are not different.
</bodyText>
<listItem confidence="0.917311">
• A capitalized argument with the part-of
speech “NNP” (singular proper noun) or
“NNPS” (plural proper noun) is general-
ized to (name). Similarly, an argument of
“ccomp” is generalized to (comp) since the
content of a clausal complement is not impor-
tant.
</listItem>
<bodyText confidence="0.995987">
Extracted predicate-argument structures are
collected for each verb and the subsequent pro-
cesses are applied to the predicate-argument struc-
tures of each verb. Figure 1 shows examples of
predicate-argument structures for “observe.”
</bodyText>
<subsectionHeader confidence="0.9993625">
3.2 Constructing Initial Frames from
Predicate-argument Structures
</subsectionHeader>
<bodyText confidence="0.999966794117647">
A straightforward way to produce semantic frames
is to cluster the extracted predicate-argument
structures directly. Since our objective is to com-
pile broad-coverage semantic frames, a massive
amount of predicate-argument structures should
be fed into the clustering. It would take prohibitive
computational costs to conduct the sampling pro-
cedure, which is described in the next section.
To make the computation feasible, we merge the
predicate-argument structures that have the same
or similar meaning to get initial frames. These ini-
tial frames are the input of the subsequent cluster-
ing process. For this merge, we assume one sense
per collocation (Yarowsky, 1993) for predicate-
argument structures.
For each predicate-argument structure of a verb,
we couple the verb and an argument to make a unit
for sense disambiguation. We select an argument
in the following order by considering the degree of
effect on the verb sense:3
word, e.g., “dobj:effect”) are merged into an ini-
tial frame (Figure 1). After this process, we dis-
card minor initial frames that occur fewer than 10
times.
For example, we have 732,292 instances
(predicate-argument structures) for the verb “ob-
serve” in the web corpus that is used in our exper-
iment (its details are described in Section 4.1). As
the result of this merging process, we obtain 6,530
initial frames, which become an input for the clus-
tering. This means that this process accelerates the
speed of clustering more than 100 times.
The precision of this process will be evaluated
in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.9970555">
3.3 Clustering using Chinese Restaurant
Process
</subsectionHeader>
<bodyText confidence="0.999461625">
We cluster initial frames for each verb to produce
final semantic frames using the Chinese Restau-
rant Process (Aldous, 1985). We regard each ini-
tial frame as an instance in the usual clustering of
the Chinese Restaurant Process.
We calculate the posterior probability of a se-
mantic frame fj given an initial frame vi as fol-
lows:
</bodyText>
<equation confidence="0.999636333333333">
n(f3-) &apos; P(vi |fj) fj 7� new
Nα +αP(fj |vi) a N+α &apos; P(vi|fj) fj = new,
(1)
</equation>
<bodyText confidence="0.980680444444444">
where N is the number of initial frames for the
target verb and n(fj) is the current number of ini-
tial frames assigned to the semantic frame fj. α
is a hyper-parameter that determines how likely
it is for a new semantic frame to be created. In
this equation, the first term is the Dirichlet process
prior and the second term is the likelihood of vi.
P(vi|fj) is defined based on the Dirichlet-
Multinomial distribution as follows:
</bodyText>
<equation confidence="0.803849">
dobj, ccomp, nsubj, prep *, iobj. P(vi|fj) = � P(w|fj)count(vi,w), (2)
wEV
</equation>
<bodyText confidence="0.99973125">
This selection of a predominant argument order
above is justified by relative comparisons of the
discriminative power of the different slots for CPA
frames (Popescu, 2013). If a predicate-argument
structure does not have any of the above slots, it is
discarded.
Then, the predicate-argument structures that
have the same verb and argument pair (slot and
</bodyText>
<footnote confidence="0.620523">
3If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.
</footnote>
<bodyText confidence="0.989089">
where V is the vocabulary in all case slots cooc-
curring with the verb. It is distinguished by
the case slot, and thus consists of pairs of slots
and words, e.g., “nsubj:child” and “dobj:bird.”
count(vi, w) is the number of w in the initial
</bodyText>
<equation confidence="0.9512935">
frame vi.
P(w|fj) is defined as follows:
_ count (fj, w) + Q
P(w|fj) &amp;EV count(fj,t) + |V  |&apos; Q, (3)
</equation>
<page confidence="0.970303">
61
</page>
<bodyText confidence="0.999945166666667">
where count(fj, w) is the current number of w in
the frame fj, and Q is a hyper-parameter of Dirich-
let distribution. For a new semantic frame, this
probability is uniform (1/IV ).
We use Gibbs sampling to realize this cluster-
ing.
</bodyText>
<sectionHeader confidence="0.997915" genericHeader="evaluation">
4 Experiments and Evaluations
</sectionHeader>
<subsectionHeader confidence="0.980116">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.99998464516129">
We use two kinds of large-scale corpora: a web
corpus and the English Gigaword corpus.
To prepare a web corpus, we first crawled the
web. We extracted sentences from each web
page that seems to be written in English based
on the encoding information. Then, we selected
sentences that consist of at most 40 words, and
removed duplicated sentences. From this pro-
cess, we obtained a corpus of one billion sen-
tences, totaling approximately 20 billion words.
We focused on verbs whose frequency was more
than 1,000. There were 19,649 verbs, includ-
ing phrasal verbs, and separating passive and ac-
tive constructions. We extracted 2,032,774,982
predicate-argument structures.
We also used the English Gigaword corpus
(LDC2011T07; English Gigaword Fifth Edition)
to induce semantic frames. This corpus consists
of approximately 180 million sentences, which to-
taling four billion words. There were 7,356 verbs
after applying the same frequency threshold as the
web corpus. We extracted 423,778,278 predicate-
argument structures from this corpus.
We set the hyper-parameters α in (1) and Q in
(3) to 1.0. The frame assignments for all the com-
ponents were initialized randomly. We took 100
samples for each initial frame and selected the
frame assignment that has the highest probability.
These parameters were determined according to a
preliminary experiment to manually examine the
quality of resulting frames.
</bodyText>
<subsectionHeader confidence="0.981286">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9996555">
We executed the per-verb clustering tasks on a PC
cluster. It finished within a few hours for most
verbs, but it took a couple of days for very frequent
verbs, such as “get” and “say.” The clustering pro-
duced an average number of semantic frames per
verb of 15.2 for the web corpus and 18.5 for the
Gigaword corpus. Examples of induced semantic
frames from the web corpus are shown in Table 1.
</bodyText>
<table confidence="0.99968964">
slot instances
observe:1 nsubj i:5850, we:5201, he:3796, you:3669, ...
dobj what:7091, people:2272, this:2262, ...
prep in way:254, world:204, life:194, ...
...
observe:2 nsubj we:11135, you:1321, i:1317, ...
dobj change:5091, difference:2719, ...
prep in study:622, case:382, cell:362, ...
...
observe:3 nsubj student:3921, i:2240, we:2174, ...
dobj child:2323, class:2184, student:2025, ...
prep in classroom:555, action:509, ...
...
accept:1 nsubj we:44833, i:6873, order:4051, ...
dobj card:28835, payment:22569, ...
prep for payment:1166, convenience:1147, ...
...
accept:2 nsubj i:10568, we:9300, you:5106, ...
dobj that:14180, this:12061, it:7756, ...
prep as part:1879, fact:1085, truth:926, ...
...
accept:3 nsubj people:7459, he:6696, we:5515, ...
dobj christ:13766, jesus:6528, it:5612, ...
prep as savior:5591, lord:597, one:469, ...
...
</table>
<tableCaption confidence="0.998237">
Table 1: Examples of resulting frames for the verb
</tableCaption>
<bodyText confidence="0.881803333333333">
“observe” and “accept” induced from the web cor-
pus. The number following an instance word rep-
resents its frequency.
</bodyText>
<subsectionHeader confidence="0.999347">
4.3 Evaluation of Induced Semantic Frames
</subsectionHeader>
<bodyText confidence="0.99995164">
We evaluate precision and coverage of induced se-
mantic frames. To measure the precision of in-
duced semantic frames, we adopt the purity met-
ric, which is usually used to evaluate clustering re-
sults. However, the problem is that it is impossible
to assign gold-standard classes to the huge num-
ber of instances. To automatically measure the
purity of the induced semantic frames, we make
use of the SemLink corpus (Loper et al., 2007), in
which VerbNet classes (Kipper-Schuler, 2005) and
PropBank/FrameNet frames are assigned to each
instance. We make a test set that contains 157 pol-
ysemous verbs that occur 10 or more times in the
SemLink corpus (sections 02-21 of the Wall Street
Journal). We first add these instances to the in-
stances from a raw corpus and apply clustering to
these merged instances. Then, we compare the in-
duced semantic frames of the SemLink instances
with their gold-standard classes. We adopt Verb-
Net classes and PropBank frames as gold-standard
classes.
For each group of verb-specific semantic
frames, we measure the purity of the frames as the
percentage of SemLink instances belonging to the
majority gold class in their respective cluster. Let
</bodyText>
<page confidence="0.995806">
62
</page>
<table confidence="0.97866125">
PU CO Mac F1
Mac Mic Mac Mic Mic
against One frame 0.799 0.802 0.917 0.952 0.854 0.870
VerbNet Initial frames 0.985 0.982 0.755 0.812 0.855 0.889
Induced sem frames 0.900 0.901 0.886 0.928 0.893 0.914
against One frame 0.901 0.872 T T 0.909 0.910
PropBank Initial frames 0.994 0.993 T T 0.858 0.893
Induced sem frames 0.965 0.949 T T 0.924 0.939
</table>
<tableCaption confidence="0.9524525">
Table 2: Evaluation results of semantic frames from the web corpus against VerbNet classes and Prop-
Bank frames. “Mac” means a macro average and “Mic” means a micro average.
</tableCaption>
<table confidence="0.998006375">
PU CO Mac F1
Mac Mic Mac Mic Mic
against One frame 0.799 0.804 0.855 0.920 0.826 0.858
VerbNet Initial frames 0.985 0.981 0.666 0.758 0.795 0.855
Induced sem frames 0.916 0.909 0.796 0.880 0.852 0.894
against One frame 0.901 0.874 T T 0.877 0.896
PropBank Initial frames 0.994 0.993 T T 0.798 0.859
Induced sem frames 0.968 0.953 T T 0.874 0.915
</table>
<tableCaption confidence="0.952933">
Table 3: Evaluation results of semantic frames from the Gigaword corpus against VerbNet classes and
PropBank frames. “Mac” means a macro average and “Mic” means a micro average.
</tableCaption>
<bodyText confidence="0.9999244">
N denote the total number of SemLink instances
of the target verb, Gj the set of instances belong-
ing to the j-th gold class and Fi the set of instances
belonging to the i-th frame. The purity (PU) can
then be written as follows:
</bodyText>
<equation confidence="0.4024925">
max Gj n Fi . (4)
j
</equation>
<bodyText confidence="0.999729319148937">
For example, a frame of the verb “observe” con-
tains 11 SemLink instances, and eight out of them
belong to the class SAY-37.7, which is the ma-
jority class among these 11 instances. PU is cal-
culated by summing up such counts over all the
frames of this verb.
Usually, inverse purity or collocation is used
to measure the recall of normal clustering tasks.
However, these recall measures do not fit our task.
This is because it is not a real error to have similar
separate frames. Instead, we want to avoid hav-
ing so many frames that we cannot provide broad-
coverage selectional preferences due to sparsity.
To judge this aspect, we measure coverage.
The coverage (CO) measures to what extent
predicate-argument structures of the target verb in
a test set are included in one of frames of the verb.
We use the predicate-argument structures of the
above 157 verbs from the SemLink corpus, which
are the same ones used in the evaluation of PU.
We judge a predicate-argument structure as cor-
rect if all of its argument words (of the target slot
described in Section 3.1) are included in the corre-
sponding slot of a frame. If the clustering gets bet-
ter, the value of CO will get higher, because merg-
ing instances by clustering alleviates data sparsity.
These per-verb scores are aggregated into an
overall score by averaging over all verbs. We use
two ways of averaging: a macro average and a mi-
cro average. The macro average is a simple av-
erage of scores for individual verbs. The micro
average is obtained by weighting the scores for in-
dividual verbs proportional to the number of in-
stances for that verb. Finally, we use the harmonic
mean (F1) of purity and coverage as a single mea-
sure of clustering quality.
For comparison, we adopt the following two
baseline methods:
One frame a frame into which all the instances
for a verb are merged
Initial frames the initial frames without cluster-
ing (described in Section 3.2)
Table 2 and Table 3 list evaluation results for
semantic frames induced from the web corpus and
the Gigaword corpus, respectively.4 Note that CO
does not consider gold-standard classes, and thus
the values of CO are the same for the VerbNet
</bodyText>
<footnote confidence="0.802166">
4We did not adopt inverse purity, but its values for the
induced semantic frames range from 0.42 to 0.49.
</footnote>
<equation confidence="0.853897">
PU = 1 �
N
i
</equation>
<page confidence="0.99129">
63
</page>
<bodyText confidence="0.999886434782609">
and PropBank evaluations. The induced frames
outperformed the two baseline methods in terms
of F1 in most cases. While the coverage of the
web frames was higher than that of the Giga-
word frames, as expected, the purity of the web
frames was slightly lower than that of the Giga-
word frames. This degradation might be caused
by the noise in the web corpus.
The purity of the initial frames was around
98%-99%, which means that there were few cases
that the one-sense-per-collocation assumption was
violated.
Modi et al. (2012) reported a purity of 77.9%
for the assignment of FrameNet frames to the
FrameNet corpus. We also conducted the above
purity evaluation against FrameNet frames for 140
verbs.5 We obtained a macro average of 92.9%
and a micro average of 89.2% for the web frames,
and a macro average of 93.2% and a micro average
of 89.8% for the Gigaword frames. It is difficult
to directly compare these results with Modi et al.
(2012), but our frame assignments seem to have
higher accuracy.
</bodyText>
<subsectionHeader confidence="0.993667">
4.4 Evaluation against CPA Frames
</subsectionHeader>
<bodyText confidence="0.999476631578948">
Corpus Pattern Analysis (CPA) is a technique for
linking word usage to prototypical syntagmatic
patterns.6 The resource was built manually by in-
vestigating examples in the BNC, and the set of
corpus examples used to induce each pattern is
given. For example, the following three patterns
describe the usage of the verb “accommodate.”
[Human 1] accommodate [Human 2]
[Building] accommodate [Eventuality]
[Human] accommodate [Self] to [Eventuality]
In this paper, we use CPA to evaluate the quality
of the automatically induced frames. By compar-
ing the induced frames to CPA patterns, we can
evaluate the correctness and relevance of this ap-
proach from a human point of view. To do that,
we associate semantic features to the set of words
in each slot in the frames, using SUMO (Niles
and Pease, 2001). For example, take the follow-
ing frame for the verb “accomplish”:
</bodyText>
<equation confidence="0.452988333333333">
accomplish:1
nsubj:{you, leader, employee, ...}
dobj:{developing, progress, objective, ...}.
</equation>
<footnote confidence="0.99951">
5Since FrameNet frames are not assigned to all the verbs
of SemLink, the number of verbs is different from the evalu-
ations against VerbNet and PropBank.
6http://deb.fi.muni.cz/pdev/
</footnote>
<table confidence="0.99535075">
all K-means
Entropy (E) 0.790 0.516
Recovery Rate (RC) 0.347 0.630
Purity (P) 0.462 0.696
</table>
<tableCaption confidence="0.999366">
Table 4: CPA Evaluation.
</tableCaption>
<bodyText confidence="0.986839421052631">
Using SUMO, we map this frame to the following:
nsubj: [Human]
dobj: [SubjectiveAssessmentAttribute],
which corresponds to pattern 3 for “accomplish”
in CPA.
We also associate SUMO attributes to the CPA
patterns with more than 10 examples (716 verbs).
There are many patterns of SUMO attributes for
any CPA frame or induced frame, since each
filler word in a particular slot can have more
than one SUMO attribute. We filter out the
non-discriminative SUMO attributes following the
technique described in Popescu (2013). Using
this, we obtain SUMO attributes for both CPA
clusters and induced frames, and we can use the
standard entropy-based measures to evaluate the
match between the two types of patterns: E — en-
tropy, RC — recovery rate, and P — purity (Li et
al., 2004):
</bodyText>
<equation confidence="0.846231666666667">
m · ej, RC = 1
mj
· pj, pj = max pij, (6)
m i
mij , (7)
pij lo�2 pij, pij = mi
</equation>
<bodyText confidence="0.999911176470588">
where mj is the number of induced frames corre-
sponding to topic j, mij is the number of induced
frames in cluster j and annotated with the CPA
pattern i, m is the total number of induced frames,
L is the number of CPA patterns, and K is the
number of induced frames.
We also consider a K-means clustering process,
with K set as 2 or 3 depending on the number of
SUMO-attributed patterns. The K-means evalu-
ation is carried out considering only the centroid
of the cluster, which corresponds to the prototypi-
cal induced semantic frame with SUMO attributes.
We compute E, RC and P using formulae (5) -
(7) for each verb and then compute the macro av-
erage, considering all the frames and only the K-
means centroids, respectively. The results for the
induced web frames are displayed in Table 4.
</bodyText>
<equation confidence="0.998551583333333">
K
E=
j=1
m
K,L � pij , (5)
j,i=1 mi
K
P=
j=1
L
ej =
i=1
</equation>
<page confidence="0.990453">
64
</page>
<bodyText confidence="0.9999735">
The evaluation method presented here over-
comes some of the drawbacks of the previous ap-
proaches (Materna, 2012; Materna, 2013). First,
we did not limit the evaluation to the most frequent
patterns. Second, the mapping was carried out au-
tomatically and not by hand. The results above
compare favorably with the previous approaches,
especially considering that no filtering procedures
were applied to the induced frames. We anticipate
that the results based on the prototypical induced
frames with SUMO attributes would be competi-
tive. Our post-analysis revealed that the entropy
can be lowered further if an automatic filtering
based on frequencies is applied.
</bodyText>
<subsectionHeader confidence="0.9956435">
4.5 Evaluation of the Quality of Selectional
Preferences
</subsectionHeader>
<bodyText confidence="0.99996365">
We also investigated the quality of selectional
preferences within the induced semantic frames.
The only publicly available test data for selectional
preferences, to our knowledge, is from Chambers
and Jurafsky (2010). This data consists of quadru-
ples (verb, relation, word, confounder) and does
not contain their context.7
A typical way for using our semantic frames is
to select an appropriate frame for an input sen-
tence and judge the eligibility of the word uses
against the selected frame. However, due to the
lack of context for the above data, it is difficult to
select a corresponding semantic frame for a test
quadruple and thus the induced semantic frames
cannot be naturally applied to this data. To in-
vestigate the potential for selectional preferences
of the semantic frames, we approximately match
a quadruple with each of the semantic frames of
the verb and select the frame that has the highest
probability as follows:
</bodyText>
<equation confidence="0.821662">
P(w|v, rel, fi), (8)
</equation>
<bodyText confidence="0.999971636363636">
where w is the word or confounder, v is the verb,
rel is the relation and fi is a semantic frame. By
comparing the probabilities of the word and the
confounder, we select either of them according to
the higher probability. For tie breaking in the case
that no frames are found for the verb or both the
word and confounder are not found in the case slot,
we randomly select either of them in the same way
as Chambers and Jurafsky (2010).
We use the “neighbor frequency” set, which is
the most difficult among the three sets included
</bodyText>
<footnote confidence="0.556834">
7A document ID of the English Gigaword corpus is avail-
able, but it is difficult to recover the context of each instance
from this information.
</footnote>
<bodyText confidence="0.999954413793103">
in the data. It contains 6,767 quadruples and the
relations consist of three classes: subject, object
and preposition, which has no distinction of ac-
tual prepositions. To link these relations with our
case slots, we manually aligned the subject with
the nsubj (nominal subject) slot, the object with
the dobj (direct object) slot and the preposition
with prep * (all the prepositions) slots. For the
preposition relation, we choose the highest prob-
ability among all the preposition slots in a frame.
To match the generalized (name) with the word in
a quadruple, we change the word to (name) if it is
capitalized and not a capitalized personal pronoun.
Our semantic frames from the Gigaword corpus
achieved an accuracy of 81.7%8 and those from
the web corpus achieved an accuracy of 80.2%.
This slight deterioration seems to come from the
noise in the web corpus. The best performance
in Chambers and Jurafsky (2010) is 81.7% on
this “neighbor frequency” set, which was achieved
by conditional probabilities with the Erk (2007)’s
smoothing method calculated from the English Gi-
gaword corpus. Our approach for selectional pref-
erences does not use smoothing like Erk (2007),
but it achieved equivalent performance to the pre-
vious work. If we applied our semantic frames to a
verb instance with its context, a more precise judg-
ment of selectional preferences would be possible
with appropriate frame selection.
</bodyText>
<sectionHeader confidence="0.998924" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99993525">
This paper has described an unsupervised method
for inducing semantic frames from instances of
each verb in giga-word corpora. This method is
clustering based on the Chinese Restaurant Pro-
cess. The resulting frame data are open to the pub-
lic and also can be searched by inputting a verb via
our web interface.9
As applications of the resulting frames, we plan
to integrate them into syntactic parsing, semantic
role labeling and verb sense disambiguation. For
instance, Kawahara and Kurohashi (2006b) im-
proved accuracy of dependency parsing based on
Japanese semantic frames automatically induced
from a large raw corpus. It is valuable and promis-
ing to apply our semantic frames to these NLP
tasks.
</bodyText>
<footnote confidence="0.9895525">
8Since the dataset was created from the NYT 2001 portion
of the English Gigaword Corpus, we built semantic frames
again from the Gigaword corpus except this part.
9http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
</footnote>
<equation confidence="0.9791635">
P(w) = max
i
</equation>
<page confidence="0.998692">
65
</page>
<sectionHeader confidence="0.99818" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995489">
This work was supported by Kyoto University
John Mung Program and JST CREST. We grate-
fully acknowledge the support of the National Sci-
ence Foundation Grant NSF 1116782 - RI: Small:
A Bayesian Approach to Dynamic Lexical Re-
sources for Flexible Language Processing. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.9977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99426532631579">
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 226–236.
David Aldous. 1985. Exchangeability and related top-
ics. ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII
―1983, pages 1–198.
Collin Baker, Charles J. Fillmore, and John Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 86–90.
Hans C. Boas. 2002. Bilingual framenet dictionaries
for machine translation. In Proceedings of the 3rd
International Conference on Language Resources
and Evaluation, pages 1364–1371.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing, pages 356–363.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating textual entailment with LFG and FrameNet
frames. In Proceedings of the 2nd PASCAL Recog-
nizing Textual Entailment Workshop, pages 92–97.
Nathanael Chambers and Daniel Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 445–453.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449–
454.
Michael Ellsworth and Adam Janin. 2007. Mu-
taphrase: Paraphrasing with framenet. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 143–150.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216–223.
Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel.
2012. Classifying french verbs using french and en-
glish lexical resources. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 854–863.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 1–
8.
Patrick Hanks. 2012. How people use words to make
meanings: Semantic types meet valencies. Input,
Process and Product: Developments in Teaching
and Language Corpora, pages 54–69.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation, pages 1344–1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, pages 176–183.
Karin Kipper-Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, pages 345–352.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 939–947.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1117–1126.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1320–1331.
Tao Li, Sheng Ma, and Mitsunori Ogihara. 2004.
Entropy-based criterion in categorical clustering. In
Proceedings of the 21st International Conference on
Machine Learning, volume 4, pages 536–543.
</reference>
<page confidence="0.856158">
66
</page>
<reference confidence="0.99972715">
Thomas Lippincott, Anna Korhonen, and Diarmuid
O´ S´eaghdha. 2012. Learning syntactic verb frames
using graphical models. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 420–429.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics.
Christopher Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In Proceedings of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, pages 235–
242.
Jiˇr´ı Materna. 2012. LDA-Frames: An unsupervised
approach to generating semantic frames. In Alexan-
der Gelbukh, editor, Proceedings of the 13th Inter-
national Conference CICLing 2012, Part I, volume
7181 of Lecture Notes in Computer Science, pages
376–387. Springer Berlin / Heidelberg.
Jiˇr´ı Materna. 2013. Parameter estimation for LDA-
Frames. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 482–486.
Ashutosh Modi, Ivan Titov, and Alexandre Klementiev.
2012. Unsupervised induction of frame-semantic
representations. In Proceedings of the NAACL-HLT
Workshop on the Induction of Linguistic Structure,
pages 1–7.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693–701.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Proceedings of the International
Conference on Formal Ontology in Information Sys-
tems, pages 2–9.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Christopher Parisien and Suzanne Stevenson. 2009.
Modelling the acquisition of verb polysemy in chil-
dren. In Proceedings of the CogSci2009 Workshop
on Distributional Semantics beyond Concrete Con-
cepts, pages 17–22.
Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
Bayesian model. In Proceedings of the 32nd annual
meeting of the Cognitive Science Society.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10.
Octavian Popescu. 2013. Learning corpus patterns us-
ing finite state automata. In Proceedings of the 10th
International Conference on Computational Seman-
tics, pages 191–203.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through DPP-based verb cluster-
ing. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 862–872.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi,
and Manabu Okumura. 2013. Automatic knowl-
edge acquisition for case alternation between the
passive and active voices in Japanese. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1213–1223.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638–647.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8–15.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1445–1455.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12–22.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
74–82.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, pages 266–271.
</reference>
<page confidence="0.999502">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.249776">
<title confidence="0.9828355">Inducing Example-based Semantic from a Massive Amount of Verb Uses</title>
<author confidence="0.535364">W</author>
<affiliation confidence="0.91856">University, Kyoto,</affiliation>
<address confidence="0.7100535">of Colorado at Boulder, Boulder, CO, Bruno Kessler, Trento, Italy</address>
<abstract confidence="0.999659357142857">We present an unsupervised method for inducing semantic frames from verb uses in giga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Fully unsupervised core-adjunct argument classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>226--236</pages>
<contexts>
<context position="11389" citStr="Abend and Rappoport (2010)" startWordPosition="1708" endWordPosition="1711"> may then observe birds ... ... Predicate-argument structures: nsubj:they observe dobj:effect observe dobj:effect nsubj:we observe dobj:effect nsubj:he observe dobj:result prep at:time observe dobj:result nsubj:you observe dobj:bird nsubj:child observe dobj:bird ... Initial frames: nsubj:{they, we, ...} observe dobj:{effect} nsubj:{he, ...} observe dobj:{result} prep at:{time} nsubj:{you, child, ...} observe dobj:{bird} ... Figure 1: Examples of predicate-argument structures and initial frames for the verb “observe.” frequencies in the applications of semantic frames or the method proposed by Abend and Rappoport (2010). We apply the following processes to extracted predicate-argument structures: • A verb and an argument are lemmatized, and only the head of an argument is preserved for compound nouns. • Phrasal verbs are also distinguished from non-phrasal verbs. For example, “look up” has independent frames from “look.” • The passive voice of a verb is distinguished from the active voice, and thus these have independent frames. Passive voice is detected using the part-of-speech tag “VBN” (past participle). The alignment between frames of active and passive voices will be done after the induction of frames u</context>
</contexts>
<marker>Abend, Rappoport, 2010</marker>
<rawString>Omri Abend and Ari Rappoport. 2010. Fully unsupervised core-adjunct argument classification. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 226–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<booktitle>Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII ―1983,</booktitle>
<pages>1--198</pages>
<contexts>
<context position="4471" citStr="Aldous, 1985" startWordPosition="652" endWordPosition="653">he 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 58–67, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Frequencies, which are not shown in the above examples, are attached to each semantic frame, case slot and word, and can be effectively exploited for the applications of these semantic frames. The frequencies of words in each case slot become good sources of selectional preferences. Our novel contributions are summarized as follows: • induction of semantic frames based on the Chinese Restaurant Process (Aldous, 1985) from only automatic parses of a web-scale corpus, • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Materna, 2012; Materna, 2013). He used a model based on LDA and the Dirichlet Process to cluster verb insta</context>
<context position="14541" citStr="Aldous, 1985" startWordPosition="2204" endWordPosition="2205">le, we have 732,292 instances (predicate-argument structures) for the verb “observe” in the web corpus that is used in our experiment (its details are described in Section 4.1). As the result of this merging process, we obtain 6,530 initial frames, which become an input for the clustering. This means that this process accelerates the speed of clustering more than 100 times. The precision of this process will be evaluated in Section 4.3. 3.3 Clustering using Chinese Restaurant Process We cluster initial frames for each verb to produce final semantic frames using the Chinese Restaurant Process (Aldous, 1985). We regard each initial frame as an instance in the usual clustering of the Chinese Restaurant Process. We calculate the posterior probability of a semantic frame fj given an initial frame vi as follows: n(f3-) &apos; P(vi |fj) fj 7� new Nα +αP(fj |vi) a N+α &apos; P(vi|fj) fj = new, (1) where N is the number of initial frames for the target verb and n(fj) is the current number of initial frames assigned to the semantic frame fj. α is a hyper-parameter that determines how likely it is for a new semantic frame to be created. In this equation, the first term is the Dirichlet process prior and the second </context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>David Aldous. 1985. Exchangeability and related topics. ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII ―1983, pages 1–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles J Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="1066" citStr="Baker et al., 1998" startWordPosition="141" endWordPosition="144">ga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to </context>
<context position="5872" citStr="Baker et al., 1998" startWordPosition="871" endWordPosition="874">thod to the BNC and acquired 427 frames and 144 slots (Materna, 2013). These frames are overgeneralized across verbs and might be difficult to provide with fine-grained selectional preferences. In addition, Grenager and Manning (2006) proposed a method for inducing PropBank-style frames from Stanford typed dependencies extracted from raw corpora. Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed method may be employed to assist in the creation of this type of resource, as shown in Section 4.4. Our task can be regarded as clustering of verb instances. In this respect, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Paris</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles J. Fillmore, and John Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans C Boas</author>
</authors>
<title>Bilingual framenet dictionaries for machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1364--1371</pages>
<contexts>
<context position="1302" citStr="Boas, 2002" startWordPosition="179" endWordPosition="180"> our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006</context>
</contexts>
<marker>Boas, 2002</marker>
<rawString>Hans C. Boas. 2002. Bilingual framenet dictionaries for machine translation. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1364–1371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<contexts>
<context position="1879" citStr="Briscoe and Carroll, 1997" startWordPosition="263" endWordPosition="266">arsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frame</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 356–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Anette Frank</author>
</authors>
<title>Approximating textual entailment with LFG and FrameNet frames.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd PASCAL Recognizing Textual Entailment Workshop,</booktitle>
<pages>92--97</pages>
<contexts>
<context position="1520" citStr="Burchardt and Frank, 2006" startWordPosition="207" endWordPosition="210">oduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have di</context>
</contexts>
<marker>Burchardt, Frank, 2006</marker>
<rawString>Aljoscha Burchardt and Anette Frank. 2006. Approximating textual entailment with LFG and FrameNet frames. In Proceedings of the 2nd PASCAL Recognizing Textual Entailment Workshop, pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Improving the use of pseudo-words for evaluating selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>445--453</pages>
<contexts>
<context position="28962" citStr="Chambers and Jurafsky (2010)" startWordPosition="4633" endWordPosition="4636">the previous approaches, especially considering that no filtering procedures were applied to the induced frames. We anticipate that the results based on the prototypical induced frames with SUMO attributes would be competitive. Our post-analysis revealed that the entropy can be lowered further if an automatic filtering based on frequencies is applied. 4.5 Evaluation of the Quality of Selectional Preferences We also investigated the quality of selectional preferences within the induced semantic frames. The only publicly available test data for selectional preferences, to our knowledge, is from Chambers and Jurafsky (2010). This data consists of quadruples (verb, relation, word, confounder) and does not contain their context.7 A typical way for using our semantic frames is to select an appropriate frame for an input sentence and judge the eligibility of the word uses against the selected frame. However, due to the lack of context for the above data, it is difficult to select a corresponding semantic frame for a test quadruple and thus the induced semantic frames cannot be naturally applied to this data. To investigate the potential for selectional preferences of the semantic frames, we approximately match a qua</context>
<context position="31291" citStr="Chambers and Jurafsky (2010)" startWordPosition="5034" endWordPosition="5037"> object with the dobj (direct object) slot and the preposition with prep * (all the prepositions) slots. For the preposition relation, we choose the highest probability among all the preposition slots in a frame. To match the generalized (name) with the word in a quadruple, we change the word to (name) if it is capitalized and not a capitalized personal pronoun. Our semantic frames from the Gigaword corpus achieved an accuracy of 81.7%8 and those from the web corpus achieved an accuracy of 80.2%. This slight deterioration seems to come from the noise in the web corpus. The best performance in Chambers and Jurafsky (2010) is 81.7% on this “neighbor frequency” set, which was achieved by conditional probabilities with the Erk (2007)’s smoothing method calculated from the English Gigaword corpus. Our approach for selectional preferences does not use smoothing like Erk (2007), but it achieved equivalent performance to the previous work. If we applied our semantic frames to a verb instance with its context, a more precise judgment of selectional preferences would be possible with appropriate frame selection. 5 Conclusion This paper has described an unsupervised method for inducing semantic frames from instances of </context>
</contexts>
<marker>Chambers, Jurafsky, 2010</marker>
<rawString>Nathanael Chambers and Daniel Jurafsky. 2010. Improving the use of pseudo-words for evaluating selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 445–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449– 454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Ellsworth</author>
<author>Adam Janin</author>
</authors>
<title>Mutaphrase: Paraphrasing with framenet.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>143--150</pages>
<contexts>
<context position="1454" citStr="Ellsworth and Janin, 2007" startWordPosition="197" endWordPosition="200">imental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, </context>
</contexts>
<marker>Ellsworth, Janin, 2007</marker>
<rawString>Michael Ellsworth and Adam Janin. 2007. Mutaphrase: Paraphrasing with framenet. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 143–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="31402" citStr="Erk (2007)" startWordPosition="5053" endWordPosition="5054">tion, we choose the highest probability among all the preposition slots in a frame. To match the generalized (name) with the word in a quadruple, we change the word to (name) if it is capitalized and not a capitalized personal pronoun. Our semantic frames from the Gigaword corpus achieved an accuracy of 81.7%8 and those from the web corpus achieved an accuracy of 80.2%. This slight deterioration seems to come from the noise in the web corpus. The best performance in Chambers and Jurafsky (2010) is 81.7% on this “neighbor frequency” set, which was achieved by conditional probabilities with the Erk (2007)’s smoothing method calculated from the English Gigaword corpus. Our approach for selectional preferences does not use smoothing like Erk (2007), but it achieved equivalent performance to the previous work. If we applied our semantic frames to a verb instance with its context, a more precise judgment of selectional preferences would be possible with appropriate frame selection. 5 Conclusion This paper has described an unsupervised method for inducing semantic frames from instances of each verb in giga-word corpora. This method is clustering based on the Chinese Restaurant Process. The resultin</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Falk</author>
<author>Claire Gardent</author>
<author>Jean-Charles Lamirel</author>
</authors>
<title>Classifying french verbs using french and english lexical resources.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>854--863</pages>
<contexts>
<context position="7001" citStr="Falk et al., 2012" startWordPosition="1047" endWordPosition="1050">elated to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of</context>
</contexts>
<marker>Falk, Gardent, Lamirel, 2012</marker>
<rawString>Ingrid Falk, Claire Gardent, and Jean-Charles Lamirel. 2012. Classifying french verbs using french and english lexical resources. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5487" citStr="Grenager and Manning (2006)" startWordPosition="810" endWordPosition="813">frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Materna, 2012; Materna, 2013). He used a model based on LDA and the Dirichlet Process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and slots. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 427 frames and 144 slots (Materna, 2013). These frames are overgeneralized across verbs and might be difficult to provide with fine-grained selectional preferences. In addition, Grenager and Manning (2006) proposed a method for inducing PropBank-style frames from Stanford typed dependencies extracted from raw corpora. Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted b</context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christopher D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 1– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>How people use words to make meanings: Semantic types meet valencies.</title>
<date>2012</date>
<booktitle>Input, Process and Product: Developments in Teaching and Language Corpora,</booktitle>
<pages>54--69</pages>
<contexts>
<context position="5959" citStr="Hanks, 2012" startWordPosition="886" endWordPosition="887">neralized across verbs and might be difficult to provide with fine-grained selectional preferences. In addition, Grenager and Manning (2006) proposed a method for inducing PropBank-style frames from Stanford typed dependencies extracted from raw corpora. Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed method may be employed to assist in the creation of this type of resource, as shown in Section 4.4. Our task can be regarded as clustering of verb instances. In this respect, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of th</context>
</contexts>
<marker>Hanks, 2012</marker>
<rawString>Patrick Hanks. 2012. How people use words to make meanings: Semantic types meet valencies. Input, Process and Product: Developments in Teaching and Language Corpora, pages 54–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1344--1347</pages>
<contexts>
<context position="8087" citStr="Kawahara and Kurohashi (2006" startWordPosition="1221" endWordPosition="1224">uster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. For other languages than English, Kawahara and Kurohashi (2006a) proposed a method for automatically compiling Japanese semantic frames from a large web corpus. They applied conventional agglomerative clustering to predicateargument structures using word/frame similarity based on a manually-crafted thesaurus. Since Japanese is head-final and has case-marking postpositions, it seems easier to build semantic frames with it than with other languages such as English. They also achieved an improvement in dependency parsing and predicate-argument structure 59 analysis by using their resulting frames (Kawahara and Kurohashi, 2006b). 3 Method for Inducing Semant</context>
<context position="32294" citStr="Kawahara and Kurohashi (2006" startWordPosition="5193" endWordPosition="5196"> its context, a more precise judgment of selectional preferences would be possible with appropriate frame selection. 5 Conclusion This paper has described an unsupervised method for inducing semantic frames from instances of each verb in giga-word corpora. This method is clustering based on the Chinese Restaurant Process. The resulting frame data are open to the public and also can be searched by inputting a verb via our web interface.9 As applications of the resulting frames, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation. For instance, Kawahara and Kurohashi (2006b) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a large raw corpus. It is valuable and promising to apply our semantic frames to these NLP tasks. 8Since the dataset was created from the NYT 2001 portion of the English Gigaword Corpus, we built semantic frames again from the Gigaword corpus except this part. 9http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/ P(w) = max i 65 Acknowledgments This work was supported by Kyoto University John Mung Program and JST CREST. We gratefully acknowledge the support of the National Science Found</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006a. Case frame compilation from the web using highperformance computing. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="8087" citStr="Kawahara and Kurohashi (2006" startWordPosition="1221" endWordPosition="1224">uster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. For other languages than English, Kawahara and Kurohashi (2006a) proposed a method for automatically compiling Japanese semantic frames from a large web corpus. They applied conventional agglomerative clustering to predicateargument structures using word/frame similarity based on a manually-crafted thesaurus. Since Japanese is head-final and has case-marking postpositions, it seems easier to build semantic frames with it than with other languages such as English. They also achieved an improvement in dependency parsing and predicate-argument structure 59 analysis by using their resulting frames (Kawahara and Kurohashi, 2006b). 3 Method for Inducing Semant</context>
<context position="32294" citStr="Kawahara and Kurohashi (2006" startWordPosition="5193" endWordPosition="5196"> its context, a more precise judgment of selectional preferences would be possible with appropriate frame selection. 5 Conclusion This paper has described an unsupervised method for inducing semantic frames from instances of each verb in giga-word corpora. This method is clustering based on the Chinese Restaurant Process. The resulting frame data are open to the public and also can be searched by inputting a verb via our web interface.9 As applications of the resulting frames, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation. For instance, Kawahara and Kurohashi (2006b) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a large raw corpus. It is valuable and promising to apply our semantic frames to these NLP tasks. 8Since the dataset was created from the NYT 2001 portion of the English Gigaword Corpus, we built semantic frames again from the Gigaword corpus except this part. 9http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/ P(w) = max i 65 Acknowledgments This work was supported by Kyoto University John Mung Program and JST CREST. We gratefully acknowledge the support of the National Science Found</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006b. A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. In Proceedings of the Human Language Technology Conference of the NAACL, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper-Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="19771" citStr="Kipper-Schuler, 2005" startWordPosition="3049" endWordPosition="3050">cept” induced from the web corpus. The number following an instance word represents its frequency. 4.3 Evaluation of Induced Semantic Frames We evaluate precision and coverage of induced semantic frames. To measure the precision of induced semantic frames, we adopt the purity metric, which is usually used to evaluate clustering results. However, the problem is that it is impossible to assign gold-standard classes to the huge number of instances. To automatically measure the purity of the induced semantic frames, we make use of the SemLink corpus (Loper et al., 2007), in which VerbNet classes (Kipper-Schuler, 2005) and PropBank/FrameNet frames are assigned to each instance. We make a test set that contains 157 polysemous verbs that occur 10 or more times in the SemLink corpus (sections 02-21 of the Wall Street Journal). We first add these instances to the instances from a raw corpus and apply clustering to these merged instances. Then, we compare the induced semantic frames of the SemLink instances with their gold-standard classes. We adopt VerbNet classes and PropBank frames as gold-standard classes. For each group of verb-specific semantic frames, we measure the purity of the frames as the percentage </context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>Karin Kipper-Schuler. 2005. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Ted Briscoe</author>
</authors>
<title>A large subcategorization lexicon for natural language processing applications.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>345--352</pages>
<contexts>
<context position="1902" citStr="Korhonen et al., 2006" startWordPosition="267" endWordPosition="270">nslation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Mater</context>
</contexts>
<marker>Korhonen, Krymolowski, Briscoe, 2006</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Ted Briscoe. 2006. A large subcategorization lexicon for natural language processing applications. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 345–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised induction of semantic roles.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>939--947</pages>
<contexts>
<context position="7323" citStr="Lang and Lapata, 2010" startWordPosition="1098" endWordPosition="1101">i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying thi</context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>Joel Lang and Mirella Lapata. 2010. Unsupervised induction of semantic roles. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 939–947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1117--1126</pages>
<contexts>
<context position="7346" citStr="Lang and Lapata, 2011" startWordPosition="1102" endWordPosition="1105"> frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011a. Unsupervised semantic role induction via split-merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1117–1126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction with graph partitioning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1320--1331</pages>
<contexts>
<context position="7346" citStr="Lang and Lapata, 2011" startWordPosition="1102" endWordPosition="1105"> frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011b. Unsupervised semantic role induction with graph partitioning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Li</author>
<author>Sheng Ma</author>
<author>Mitsunori Ogihara</author>
</authors>
<title>Entropy-based criterion in categorical clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning,</booktitle>
<volume>4</volume>
<pages>536--543</pages>
<contexts>
<context position="27098" citStr="Li et al., 2004" startWordPosition="4304" endWordPosition="4307"> We also associate SUMO attributes to the CPA patterns with more than 10 examples (716 verbs). There are many patterns of SUMO attributes for any CPA frame or induced frame, since each filler word in a particular slot can have more than one SUMO attribute. We filter out the non-discriminative SUMO attributes following the technique described in Popescu (2013). Using this, we obtain SUMO attributes for both CPA clusters and induced frames, and we can use the standard entropy-based measures to evaluate the match between the two types of patterns: E — entropy, RC — recovery rate, and P — purity (Li et al., 2004): m · ej, RC = 1 mj · pj, pj = max pij, (6) m i mij , (7) pij lo�2 pij, pij = mi where mj is the number of induced frames corresponding to topic j, mij is the number of induced frames in cluster j and annotated with the CPA pattern i, m is the total number of induced frames, L is the number of CPA patterns, and K is the number of induced frames. We also consider a K-means clustering process, with K set as 2 or 3 depending on the number of SUMO-attributed patterns. The K-means evaluation is carried out considering only the centroid of the cluster, which corresponds to the prototypical induced s</context>
</contexts>
<marker>Li, Ma, Ogihara, 2004</marker>
<rawString>Tao Li, Sheng Ma, and Mitsunori Ogihara. 2004. Entropy-based criterion in categorical clustering. In Proceedings of the 21st International Conference on Machine Learning, volume 4, pages 536–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lippincott</author>
<author>Anna Korhonen</author>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Learning syntactic verb frames using graphical models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>420--429</pages>
<marker>Lippincott, Korhonen, S´eaghdha, 2012</marker>
<rawString>Thomas Lippincott, Anna Korhonen, and Diarmuid O´ S´eaghdha. 2012. Learning syntactic verb frames using graphical models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Szu-Ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Combining lexical resources: mapping between PropBank and VerbNet.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Computational Linguistics.</booktitle>
<contexts>
<context position="19722" citStr="Loper et al., 2007" startWordPosition="3041" endWordPosition="3044">resulting frames for the verb “observe” and “accept” induced from the web corpus. The number following an instance word represents its frequency. 4.3 Evaluation of Induced Semantic Frames We evaluate precision and coverage of induced semantic frames. To measure the precision of induced semantic frames, we adopt the purity metric, which is usually used to evaluate clustering results. However, the problem is that it is impossible to assign gold-standard classes to the huge number of instances. To automatically measure the purity of the induced semantic frames, we make use of the SemLink corpus (Loper et al., 2007), in which VerbNet classes (Kipper-Schuler, 2005) and PropBank/FrameNet frames are assigned to each instance. We make a test set that contains 157 polysemous verbs that occur 10 or more times in the SemLink corpus (sections 02-21 of the Wall Street Journal). We first add these instances to the instances from a raw corpus and apply clustering to these merged instances. Then, we compare the induced semantic frames of the SemLink instances with their gold-standard classes. We adopt VerbNet classes and PropBank frames as gold-standard classes. For each group of verb-specific semantic frames, we me</context>
</contexts>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007. Combining lexical resources: mapping between PropBank and VerbNet. In Proceedings of the 7th International Workshop on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="1852" citStr="Manning, 1993" startWordPosition="261" endWordPosition="262">only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatical</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Christopher Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 235– 242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇr´ı Materna</author>
</authors>
<title>LDA-Frames: An unsupervised approach to generating semantic frames.</title>
<date>2012</date>
<booktitle>Proceedings of the 13th International Conference CICLing 2012, Part I,</booktitle>
<volume>7181</volume>
<pages>376--387</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="2495" citStr="Materna, 2012" startWordPosition="353" endWordPosition="354">orhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Materna, 2013). Materna induced semantic frames (called LDA-Frames) from triples of (subject, verb, object) in the British National Corpus (BNC) based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. LDAFrames capture limited linguistic phenomena of these triples, and are defined across verbs based on probabilistic topic distributions. This paper presents a method for automatically building verb-specific semantic frames from a large raw corpus. Our semantic frames are verbspecific like PropBank and semantically distinguished. A frame has several syntactic case slots, each of w</context>
<context position="4977" citStr="Materna, 2012" startWordPosition="728" endWordPosition="729">ummarized as follows: • induction of semantic frames based on the Chinese Restaurant Process (Aldous, 1985) from only automatic parses of a web-scale corpus, • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Materna, 2012; Materna, 2013). He used a model based on LDA and the Dirichlet Process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and slots. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 427 frames and 144 slots (Materna, 2013). These frames are overgeneralized across verbs and might be difficult to provide with fine-grained selectional preferences. In addition, Grenager and Manning (2006) proposed a method for inducing PropBank-style frames from Stanford typed dependencies ext</context>
<context position="28138" citStr="Materna, 2012" startWordPosition="4512" endWordPosition="4513">n the number of SUMO-attributed patterns. The K-means evaluation is carried out considering only the centroid of the cluster, which corresponds to the prototypical induced semantic frame with SUMO attributes. We compute E, RC and P using formulae (5) - (7) for each verb and then compute the macro average, considering all the frames and only the Kmeans centroids, respectively. The results for the induced web frames are displayed in Table 4. K E= j=1 m K,L � pij , (5) j,i=1 mi K P= j=1 L ej = i=1 64 The evaluation method presented here overcomes some of the drawbacks of the previous approaches (Materna, 2012; Materna, 2013). First, we did not limit the evaluation to the most frequent patterns. Second, the mapping was carried out automatically and not by hand. The results above compare favorably with the previous approaches, especially considering that no filtering procedures were applied to the induced frames. We anticipate that the results based on the prototypical induced frames with SUMO attributes would be competitive. Our post-analysis revealed that the entropy can be lowered further if an automatic filtering based on frequencies is applied. 4.5 Evaluation of the Quality of Selectional Prefe</context>
</contexts>
<marker>Materna, 2012</marker>
<rawString>Jiˇr´ı Materna. 2012. LDA-Frames: An unsupervised approach to generating semantic frames. In Alexander Gelbukh, editor, Proceedings of the 13th International Conference CICLing 2012, Part I, volume 7181 of Lecture Notes in Computer Science, pages 376–387. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇr´ı Materna</author>
</authors>
<title>Parameter estimation for LDAFrames.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>482--486</pages>
<contexts>
<context position="2511" citStr="Materna, 2013" startWordPosition="355" endWordPosition="356"> 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Materna, 2013). Materna induced semantic frames (called LDA-Frames) from triples of (subject, verb, object) in the British National Corpus (BNC) based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. LDAFrames capture limited linguistic phenomena of these triples, and are defined across verbs based on probabilistic topic distributions. This paper presents a method for automatically building verb-specific semantic frames from a large raw corpus. Our semantic frames are verbspecific like PropBank and semantically distinguished. A frame has several syntactic case slots, each of which consists of</context>
<context position="4993" citStr="Materna, 2013" startWordPosition="730" endWordPosition="731">llows: • induction of semantic frames based on the Chinese Restaurant Process (Aldous, 1985) from only automatic parses of a web-scale corpus, • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Materna, 2012; Materna, 2013). He used a model based on LDA and the Dirichlet Process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and slots. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 427 frames and 144 slots (Materna, 2013). These frames are overgeneralized across verbs and might be difficult to provide with fine-grained selectional preferences. In addition, Grenager and Manning (2006) proposed a method for inducing PropBank-style frames from Stanford typed dependencies extracted from raw </context>
<context position="28154" citStr="Materna, 2013" startWordPosition="4514" endWordPosition="4515"> SUMO-attributed patterns. The K-means evaluation is carried out considering only the centroid of the cluster, which corresponds to the prototypical induced semantic frame with SUMO attributes. We compute E, RC and P using formulae (5) - (7) for each verb and then compute the macro average, considering all the frames and only the Kmeans centroids, respectively. The results for the induced web frames are displayed in Table 4. K E= j=1 m K,L � pij , (5) j,i=1 mi K P= j=1 L ej = i=1 64 The evaluation method presented here overcomes some of the drawbacks of the previous approaches (Materna, 2012; Materna, 2013). First, we did not limit the evaluation to the most frequent patterns. Second, the mapping was carried out automatically and not by hand. The results above compare favorably with the previous approaches, especially considering that no filtering procedures were applied to the induced frames. We anticipate that the results based on the prototypical induced frames with SUMO attributes would be competitive. Our post-analysis revealed that the entropy can be lowered further if an automatic filtering based on frequencies is applied. 4.5 Evaluation of the Quality of Selectional Preferences We also i</context>
</contexts>
<marker>Materna, 2013</marker>
<rawString>Jiˇr´ı Materna. 2013. Parameter estimation for LDAFrames. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 482–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashutosh Modi</author>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Unsupervised induction of frame-semantic representations.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="7579" citStr="Modi et al. (2012)" startWordPosition="1137" endWordPosition="1140">un and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. For other languages than English, Kawahara and Kurohashi (2006a) proposed a method for automatically compiling Japanese semantic frames from a large web c</context>
<context position="24559" citStr="Modi et al. (2012)" startWordPosition="3891" endWordPosition="3894">rity, but its values for the induced semantic frames range from 0.42 to 0.49. PU = 1 � N i 63 and PropBank evaluations. The induced frames outperformed the two baseline methods in terms of F1 in most cases. While the coverage of the web frames was higher than that of the Gigaword frames, as expected, the purity of the web frames was slightly lower than that of the Gigaword frames. This degradation might be caused by the noise in the web corpus. The purity of the initial frames was around 98%-99%, which means that there were few cases that the one-sense-per-collocation assumption was violated. Modi et al. (2012) reported a purity of 77.9% for the assignment of FrameNet frames to the FrameNet corpus. We also conducted the above purity evaluation against FrameNet frames for 140 verbs.5 We obtained a macro average of 92.9% and a micro average of 89.2% for the web frames, and a macro average of 93.2% and a micro average of 89.8% for the Gigaword frames. It is difficult to directly compare these results with Modi et al. (2012), but our frame assignments seem to have higher accuracy. 4.4 Evaluation against CPA Frames Corpus Pattern Analysis (CPA) is a technique for linking word usage to prototypical syntag</context>
</contexts>
<marker>Modi, Titov, Klementiev, 2012</marker>
<rawString>Ashutosh Modi, Ivan Titov, and Alexandre Klementiev. 2012. Unsupervised induction of frame-semantic representations. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>693--701</pages>
<contexts>
<context position="1402" citStr="Narayanan and Harabagiu, 2004" startWordPosition="191" endWordPosition="194">rpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent a</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question answering based on semantic structures. In Proceedings of the 20th International Conference on Computational Linguistics, pages 693–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Niles</author>
<author>Adam Pease</author>
</authors>
<title>Towards a standard upper ontology.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Formal Ontology in Information Systems,</booktitle>
<pages>2--9</pages>
<contexts>
<context position="25868" citStr="Niles and Pease, 2001" startWordPosition="4110" endWordPosition="4113">and the set of corpus examples used to induce each pattern is given. For example, the following three patterns describe the usage of the verb “accommodate.” [Human 1] accommodate [Human 2] [Building] accommodate [Eventuality] [Human] accommodate [Self] to [Eventuality] In this paper, we use CPA to evaluate the quality of the automatically induced frames. By comparing the induced frames to CPA patterns, we can evaluate the correctness and relevance of this approach from a human point of view. To do that, we associate semantic features to the set of words in each slot in the frames, using SUMO (Niles and Pease, 2001). For example, take the following frame for the verb “accomplish”: accomplish:1 nsubj:{you, leader, employee, ...} dobj:{developing, progress, objective, ...}. 5Since FrameNet frames are not assigned to all the verbs of SemLink, the number of verbs is different from the evaluations against VerbNet and PropBank. 6http://deb.fi.muni.cz/pdev/ all K-means Entropy (E) 0.790 0.516 Recovery Rate (RC) 0.347 0.630 Purity (P) 0.462 0.696 Table 4: CPA Evaluation. Using SUMO, we map this frame to the following: nsubj: [Human] dobj: [SubjectiveAssessmentAttribute], which corresponds to pattern 3 for “accom</context>
</contexts>
<marker>Niles, Pease, 2001</marker>
<rawString>Ian Niles and Adam Pease. 2001. Towards a standard upper ontology. In Proceedings of the International Conference on Formal Ontology in Information Systems, pages 2–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1101" citStr="Palmer et al., 2005" startWordPosition="147" endWordPosition="150">es are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing</context>
<context position="5907" citStr="Palmer et al., 2005" startWordPosition="877" endWordPosition="880">rames and 144 slots (Materna, 2013). These frames are overgeneralized across verbs and might be difficult to provide with fine-grained selectional preferences. In addition, Grenager and Manning (2006) proposed a method for inducing PropBank-style frames from Stanford typed dependencies extracted from raw corpora. Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed method may be employed to assist in the creation of this type of resource, as shown in Section 4.4. Our task can be regarded as clustering of verb instances. In this respect, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Modelling the acquisition of verb polysemy in children.</title>
<date>2009</date>
<booktitle>In Proceedings of the CogSci2009 Workshop on Distributional Semantics beyond Concrete Concepts,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="6434" citStr="Parisien and Stevenson, 2009" startWordPosition="963" endWordPosition="966">d semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed method may be employed to assist in the creation of this type of resource, as shown in Section 4.4. Our task can be regarded as clustering of verb instances. In this respect, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). T</context>
</contexts>
<marker>Parisien, Stevenson, 2009</marker>
<rawString>Christopher Parisien and Suzanne Stevenson. 2009. Modelling the acquisition of verb polysemy in children. In Proceedings of the CogSci2009 Workshop on Distributional Semantics beyond Concrete Concepts, pages 17–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Learning verb alternations in a usage-based Bayesian model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd annual meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="6465" citStr="Parisien and Stevenson, 2010" startWordPosition="967" endWordPosition="970">ons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed method may be employed to assist in the creation of this type of resource, as shown in Section 4.4. Our task can be regarded as clustering of verb instances. In this respect, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb cluste</context>
</contexts>
<marker>Parisien, Stevenson, 2010</marker>
<rawString>Christopher Parisien and Suzanne Stevenson. 2010. Learning verb alternations in a usage-based Bayesian model. In Proceedings of the 32nd annual meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="7300" citStr="Poon and Domingos, 2009" startWordPosition="1094" endWordPosition="1097">ing argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotat</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
</authors>
<title>Learning corpus patterns using finite state automata.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics,</booktitle>
<pages>191--203</pages>
<contexts>
<context position="15496" citStr="Popescu, 2013" startWordPosition="2374" endWordPosition="2375">nd n(fj) is the current number of initial frames assigned to the semantic frame fj. α is a hyper-parameter that determines how likely it is for a new semantic frame to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of vi. P(vi|fj) is defined based on the DirichletMultinomial distribution as follows: dobj, ccomp, nsubj, prep *, iobj. P(vi|fj) = � P(w|fj)count(vi,w), (2) wEV This selection of a predominant argument order above is justified by relative comparisons of the discriminative power of the different slots for CPA frames (Popescu, 2013). If a predicate-argument structure does not have any of the above slots, it is discarded. Then, the predicate-argument structures that have the same verb and argument pair (slot and 3If a predicate-argument structure has multiple prepositional phrases, one of them is randomly selected. where V is the vocabulary in all case slots cooccurring with the verb. It is distinguished by the case slot, and thus consists of pairs of slots and words, e.g., “nsubj:child” and “dobj:bird.” count(vi, w) is the number of w in the initial frame vi. P(w|fj) is defined as follows: _ count (fj, w) + Q P(w|fj) &amp;EV</context>
<context position="26843" citStr="Popescu (2013)" startWordPosition="4259" endWordPosition="4260"> (E) 0.790 0.516 Recovery Rate (RC) 0.347 0.630 Purity (P) 0.462 0.696 Table 4: CPA Evaluation. Using SUMO, we map this frame to the following: nsubj: [Human] dobj: [SubjectiveAssessmentAttribute], which corresponds to pattern 3 for “accomplish” in CPA. We also associate SUMO attributes to the CPA patterns with more than 10 examples (716 verbs). There are many patterns of SUMO attributes for any CPA frame or induced frame, since each filler word in a particular slot can have more than one SUMO attribute. We filter out the non-discriminative SUMO attributes following the technique described in Popescu (2013). Using this, we obtain SUMO attributes for both CPA clusters and induced frames, and we can use the standard entropy-based measures to evaluate the match between the two types of patterns: E — entropy, RC — recovery rate, and P — purity (Li et al., 2004): m · ej, RC = 1 mj · pj, pj = max pij, (6) m i mij , (7) pij lo�2 pij, pij = mi where mj is the number of induced frames corresponding to topic j, mij is the number of induced frames in cluster j and annotated with the CPA pattern i, m is the total number of induced frames, L is the number of CPA patterns, and K is the number of induced frame</context>
</contexts>
<marker>Popescu, 2013</marker>
<rawString>Octavian Popescu. 2013. Learning corpus patterns using finite state automata. In Proceedings of the 10th International Conference on Computational Semantics, pages 191–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Improved lexical acquisition through DPP-based verb clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>862--872</pages>
<contexts>
<context position="1957" citStr="Reichart and Korhonen, 2013" startWordPosition="275" endWordPosition="278">urdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Materna, 2013). Materna induced semantic frames (called LDA-</context>
<context position="7031" citStr="Reichart and Korhonen, 2013" startWordPosition="1051" endWordPosition="1054">d (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) t</context>
</contexts>
<marker>Reichart, Korhonen, 2013</marker>
<rawString>Roi Reichart and Anna Korhonen. 2013. Improved lexical acquisition through DPP-based verb clustering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Manabu Okumura</author>
</authors>
<title>Automatic knowledge acquisition for case alternation between the passive and active voices in Japanese.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1213--1223</pages>
<contexts>
<context position="12027" citStr="Sasano et al. (2013)" startWordPosition="1811" endWordPosition="1814">lowing processes to extracted predicate-argument structures: • A verb and an argument are lemmatized, and only the head of an argument is preserved for compound nouns. • Phrasal verbs are also distinguished from non-phrasal verbs. For example, “look up” has independent frames from “look.” • The passive voice of a verb is distinguished from the active voice, and thus these have independent frames. Passive voice is detected using the part-of-speech tag “VBN” (past participle). The alignment between frames of active and passive voices will be done after the induction of frames using the model of Sasano et al. (2013) in the future. • “xcomp” (open clausal complement) is renamed to “ccomp” (clausal complement) and “xsubj” (controlling subject) is renamed to “nsubj” (nominal subject). This is because 60 these usages as predicate-argument structures are not different. • A capitalized argument with the part-of speech “NNP” (singular proper noun) or “NNPS” (plural proper noun) is generalized to (name). Similarly, an argument of “ccomp” is generalized to (comp) since the content of a clausal complement is not important. Extracted predicate-argument structures are collected for each verb and the subsequent proce</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, Okumura, 2013</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi, and Manabu Okumura. 2013. Automatic knowledge acquisition for case alternation between the passive and active voices in Japanese. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>638--647</pages>
<contexts>
<context position="6982" citStr="Sun and Korhonen, 2009" startWordPosition="1043" endWordPosition="1046">sien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) ex</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="1350" citStr="Surdeanu et al., 2003" startWordPosition="184" endWordPosition="187">verage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian model for unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1445--1455</pages>
<contexts>
<context position="7399" citStr="Titov and Klementiev, 2011" startWordPosition="1110" endWordPosition="1113">nt structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalabilit</context>
</contexts>
<marker>Titov, Klementiev, 2011</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2011. A Bayesian model for unsupervised semantic parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1445–1455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="7428" citStr="Titov and Klementiev, 2012" startWordPosition="1114" endWordPosition="1118">ic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. For</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A Bayesian approach to unsupervised semantic role induction. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="6958" citStr="Vlachos et al., 2009" startWordPosition="1039" endWordPosition="1042">ct, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of argument</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and constrained dirichlet process mixture models for verb clustering. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>266--271</pages>
<contexts>
<context position="4600" citStr="Yarowsky, 1993" startWordPosition="672" endWordPosition="673">pril 26-30 2014. c�2014 Association for Computational Linguistics Frequencies, which are not shown in the above examples, are attached to each semantic frame, case slot and word, and can be effectively exploited for the applications of these semantic frames. The frequencies of words in each case slot become good sources of selectional preferences. Our novel contributions are summarized as follows: • induction of semantic frames based on the Chinese Restaurant Process (Aldous, 1985) from only automatic parses of a web-scale corpus, • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Materna, 2012; Materna, 2013). He used a model based on LDA and the Dirichlet Process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and slots. Both of these are represented as a probabilistic d</context>
<context position="13502" citStr="Yarowsky, 1993" startWordPosition="2032" endWordPosition="2033">o cluster the extracted predicate-argument structures directly. Since our objective is to compile broad-coverage semantic frames, a massive amount of predicate-argument structures should be fed into the clustering. It would take prohibitive computational costs to conduct the sampling procedure, which is described in the next section. To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames. These initial frames are the input of the subsequent clustering process. For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures. For each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation. We select an argument in the following order by considering the degree of effect on the verb sense:3 word, e.g., “dobj:effect”) are merged into an initial frame (Figure 1). After this process, we discard minor initial frames that occur fewer than 10 times. For example, we have 732,292 instances (predicate-argument structures) for the verb “observe” in the web corpus that is used in our experiment (its details are described in Section 4.</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proceedings of the Workshop on Human Language Technology, pages 266–271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>