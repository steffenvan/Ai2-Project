<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001130">
<title confidence="0.9973045">
Organizing Encyclopedic Knowledge based on the Web and its
Application to Question Answering
</title>
<author confidence="0.966673">
Atsushi Fujii Tetsuya Ishikawa
</author>
<affiliation confidence="0.7383925">
University of Library and University of Library and
Information Science Information Science
1-2 Kasuga, Tsukuba 1-2 Kasuga, Tsukuba
305-8550, Japan 305-8550, Japan
CREST, Japan Science and ishikawa@ulis.ac.jp
Technology Corporation
</affiliation>
<email confidence="0.992474">
fujii@ulis.ac.jp
</email>
<sectionHeader confidence="0.995487" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998378153846154">
We propose a method to generate large-scale
encyclopedic knowledge, which is valuable
for much NLP research, based on the Web.
We first search the Web for pages contain-
ing a term in question. Then we use lin-
guistic patterns and HTML structures to ex-
tract text fragments describing the term. Fi-
nally, we organize extracted term descrip-
tions based on word senses and domains. In
addition, we apply an automatically gener-
ated encyclopedia to a question answering
system targeting the Japanese Information-
Technology Engineers Examination.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953423076923">
Reflecting the growth in utilization of the World Wide
Web, a number of Web-based language processing
methods have been proposed within the natural lan-
guage processing (NLP), information retrieval (IR)
and artificial intelligence (AI) communities. A sam-
ple of these includes methods to extract linguistic
resources (Fujii and Ishikawa, 2000; Resnik, 1999;
Soderland, 1997), retrieve useful information in re-
sponse to user queries (Etzioni, 1997; McCallum et
al., 1999) and mine/discover knowledge latent in the
Web (Inokuchi et al., 1999).
In this paper, mainly from an NLP point of view,
we explore a method to produce linguistic resources.
Specifically, we enhance the method proposed by Fu-
jii and Ishikawa (2000), which extracts encyclopedic
knowledge (i.e., term descriptions) from the Web.
In brief, their method searches the Web for pages
containing a term in question, and uses linguistic ex-
pressions and HTML layouts to extract fragments de-
scribing the term. They also use a language model to
discard non-linguistic fragments. In addition, a clus-
tering method is used to divide descriptions into a spe-
cific number of groups.
On the one hand, their method is expected to en-
hance existing encyclopedias, where vocabulary size
is relatively limited, and therefore the quantity prob-
lems has been resolved.
On the other hand, encyclopedias extracted from the
Web are not comparable with existing ones in terms of
quality. In hand-crafted encyclopedias, term descrip-
tions are carefully organized based on domains and
word senses, which are especially effective for human
usage. However, the output of Fujii’s method is simply
a set of unorganized term descriptions. Although clus-
tering is optionally performed, resultant clusters are
not necessarily related to explicit criteria, such as word
senses and domains.
To sum up, our belief is that by combining extrac-
tion and organization methods, we can enhance both
quantity and quality of Web-based encyclopedias.
Motivated by this background, we introduce an or-
ganization model to Fujii’s method and reformalize
the whole framework. In other words, our proposed
method is not only extraction but generation of ency-
clopedic knowledge.
Section 2 explains the overall design of our ency-
clopedia generation system, and Section 3 elaborates
on our organization model. Section 4 then explores
a method for applying our resultant encyclopedia to
NLP research, specifically, question answering. Sec-
tion 5 performs a number of experiments to evaluate
our methods.
</bodyText>
<sectionHeader confidence="0.991736" genericHeader="method">
2 System Design
</sectionHeader>
<subsectionHeader confidence="0.540821">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.993041176470588">
Figure 1 depicts the overall design of our system,
which generates an encyclopedia for input terms.
Our system, which is currently implemented for
Japanese, consists of three modules: “retrieval,” “ex-
traction” and “organization,” among which the orga-
nization module is newly introduced in this paper. In
principle, the remaining two modules (“retrieval” and
“extraction”) are the same as proposed by Fujii and
Ishikawa (2000).
In Figure 1, terms can be submitted either on-line or
off-line. A reasonable method is that while the system
periodically updates the encyclopedia off-line, terms
unindexed in the encyclopedia are dynamically pro-
cessed in real-time usage. In either case, our system
processes input terms one by one.
We briefly explain each module in the following
three sections, respectively.
</bodyText>
<figure confidence="0.5192">
term(s)
</figure>
<figureCaption confidence="0.9924165">
Figure 1: The overall design of our Web-based ency-
clopedia generation system.
</figureCaption>
<subsectionHeader confidence="0.989746">
2.2 Retrieval
</subsectionHeader>
<bodyText confidence="0.999946428571429">
The retrieval module searches the Web for pages con-
taining an input term, for which existing Web search
engines can be used, and those with broad coverage
are desirable.
However, search engines performing query expan-
sion are not always desirable, because they usually re-
trieve a number of pages which do not contain an in-
put keyword. Since the extraction module (see Sec-
tion 2.3) analyzes the usage of the input term in re-
trieved pages, pages not containing the term are of no
use for our purpose.
Thus, we use as the retrieval module “Google,”
which is one of the major search engines and does not
conduct query expansion1.
</bodyText>
<subsectionHeader confidence="0.995414">
2.3 Extraction
</subsectionHeader>
<bodyText confidence="0.997975833333333">
In the extraction module, given Web pages containing
an input term, newline codes, redundant white spaces
and HTML tags that are not used in the following pro-
cesses are discarded to standardize the page format.
Second, we approximately identify a region describ-
ing the term in the page, for which two rules are used.
</bodyText>
<footnote confidence="0.941148">
1http://www.google.com/
</footnote>
<bodyText confidence="0.999784043478261">
The first rule is based on Japanese linguistic patterns
typically used for term descriptions, such as “X toha
Y dearu (X is Y).” Following the method proposed
by Fujii and Ishikawa (2000), we semi-automatically
produced 20 patterns based on the Japanese CD-ROM
World Encyclopedia (Heibonsha, 1998), which in-
cludes approximately 80,000 entries related to various
fields. It is expected that a region including the sen-
tence that matched with one of those patterns can be a
term description.
The second rule is based on HTML layout. In a typ-
ical case, a term in question is highlighted as a heading
with tags such as &lt;DT&gt;, &lt;B&gt; and &lt;Hx&gt; (“x” denotes
a digit), followed by its description. In some cases,
terms are marked with the anchor &lt;A&gt; tag, providing
hyperlinks to pages where they are described.
Finally, based on the region briefly identified by the
above method, we extract a page fragment as a term
description. Since term descriptions usually consist of
a logical segment (such as a paragraph) rather than a
single sentence, we extract a fragment that matched
with one of the following patterns, which are sorted
according to preference in descending order:
</bodyText>
<listItem confidence="0.998778">
1. description tagged with &lt;DD&gt; in the case where
the term is tagged with &lt;DT&gt;2,
2. paragraph tagged with &lt;P&gt;,
3. itemization tagged with &lt;UL&gt;,
4. N sentences, where we empirically set N = 3.
</listItem>
<subsectionHeader confidence="0.858927">
2.4 Organization
</subsectionHeader>
<bodyText confidence="0.99992105">
As discussed in Section 1, organizing information ex-
tracted from the Web is crucial in our framework. For
this purpose, we classify extracted term descriptions
based on word senses and domains.
Although a number of methods have been proposed
to generate word senses (for example, one based on the
vector space model (Sch¨utze, 1998)), it is still difficult
to accurately identify word senses without explicit dic-
tionaries that define sense candidates.
In addition, since word senses are often associated
with domains (Yarowsky, 1995), word senses can be
consequently distinguished by way of determining the
domain of each description. For example, different
senses for “pipeline (processing method/transportation
pipe)” are associated with the computer and construc-
tion domains (fields), respectively.
To sum up, the organization module classifies term
descriptions based on domains, for which we use do-
main and description models. In Section 3, we elabo-
rate on our organization model.
</bodyText>
<footnote confidence="0.3691645">
2&lt;DT&gt; and &lt;DD&gt; are inherently provided to describe
terms in HTML.
</footnote>
<figure confidence="0.992218636363636">
encyclopedia
organization
extraction
retrieval
description
model
extraction
rules
domain
model
Web
</figure>
<sectionHeader confidence="0.960136" genericHeader="method">
3 Statistical Organization Model
</sectionHeader>
<subsectionHeader confidence="0.989098">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999968714285714">
Given one or more (in most cases more than one)
descriptions for a single input term, the organization
module selects appropriate description(s) for each do-
main related to the term.
We do not need all the extracted descriptions as fi-
nal outputs, because they are usually similar to one
another, and thus are redundant.
For the moment, we assume that we know a priori
which domains are related to the input term.
From the viewpoint of probability theory, our task
here is to select descriptions with greater probability
for given domains. The probability for description d
given domain c, P(d|c), is commonly transformed as
in Equation (1), through use of the Bayesian theorem.
</bodyText>
<equation confidence="0.999951">
P(c|d) · P (d)
P(d|c) = (1)
P (c)
</equation>
<bodyText confidence="0.999978484848485">
In practice, P(c) can be omitted because this factor is
a constant, and thus does not affect the relative proba-
bility for different descriptions.
In Equation (1), P(c|d) models a probability that d
corresponds to domain c. P(d) models a probability
that d can be a description for the term in question,
disregarding the domain. We shall call them domain
and description models, respectively.
To sum up, in principle we select d’s that are
strongly associated with a specific domain, and are
likely to be descriptions themselves.
Extracted descriptions are not linguistically under-
standable in the case where the extraction process is
unsuccessful and retrieved pages inherently contain
non-linguistic information (such as special characters
and e-mail addresses).
To resolve this problem, Fujii and Ishikawa (2000)
used a language model to filter out descriptions with
low perplexity. However, in this paper we integrated
a description model, which is practically the same as
a language model, with an organization model. The
new framework is more understandable with respect
to probability theory.
In practice, we first use Equation (1) to compute
P(d|c) for all the c’s predefined in the domain model.
Then we discard such c’s whose P(d|c) is below a spe-
cific threshold. As a result, for the input term, related
domains and descriptions are simultaneously selected.
Thus, we do not have to know a priori which domains
are related to each term.
In the following two sections, we explain methods
to realize the domain and description models, respec-
tively.
</bodyText>
<subsectionHeader confidence="0.990666">
3.2 Domain Model
</subsectionHeader>
<bodyText confidence="0.999869166666667">
The domain model quantifies the extent to which de-
scription d is associated with domain c, which is fun-
damentally a categorization task. Among a number
of existing categorization methods, we experimentally
used one proposed by Iwayama and Tokunaga (1994),
which formulates P(c|d) as in Equation (2).
</bodyText>
<equation confidence="0.980652">
� P(t|c) · P(t|d) (2)
P (c|d) = P (c) · P(t)
t
</equation>
<bodyText confidence="0.9949069">
Here, P(t|d), P(t|c) and P(t) denote probabilities
that word t appears in d, c and all the domains, respec-
tively. We regard P(c) as a constant. While P(t|d) is
simply a relative frequency of t in d, we need prede-
fined domains to compute P(t|c) and P(t). For this
purpose, the use of large-scale corpora annotated with
domains is desirable.
However, since those resources are prohibitively
expensive, we used the “Nova” dictionary for
Japanese/English machine translation systems3, which
includes approximately one million entries related to
19 technical fields as listed below:
aeronautics, biotechnology, business, chem-
istry, computers, construction, defense,
ecology, electricity, energy, finance, law,
mathematics, mechanics, medicine, metals,
oceanography, plants, trade.
We extracted words from dictionary entries to esti-
mate P(t|c) and P(t), which are relative frequencies
of t in c and all the domains, respectively. We used
the ChaSen morphological analyzer (Matsumoto et al.,
1997) to extract words from Japanese entries. We also
used English entries because Japanese descriptions of-
ten contain English words.
It may be argued that statistics extracted from dic-
tionaries are unreliable, because word frequencies in
real word usage are missing. However, words that are
representative for a domain tend to be frequently used
in compound word entries associated with the domain,
and thus our method is a practical approximation.
</bodyText>
<subsectionHeader confidence="0.980855">
3.3 Description Model
</subsectionHeader>
<bodyText confidence="0.9999586">
The description model quantifies the extent to which a
given page fragment is feasible as a description for the
input term. In principle, we decompose the description
model into language and quality properties, as shown
in Equation (3).
</bodyText>
<equation confidence="0.999252">
P(d) = PL(d) · PQ(d) (3)
</equation>
<bodyText confidence="0.9950045">
Here, PL(d) and PQ(d) denote language and quality
models, respectively.
</bodyText>
<footnote confidence="0.955559">
3Produced by NOVA, Inc.
</footnote>
<bodyText confidence="0.999957666666667">
It is expected that the quality model discards in-
correct or misleading information contained in Web
pages. For this purpose, a number of quality rating
methods for Web pages (Amento et al., 2000; Zhu and
Gauch, 2000) can be used.
However, since Google (i.e., the search engine used
in our system) rates the quality of pages based on
hyperlink information, and selectively retrieves those
with higher quality (Brin and Page, 1998), we tenta-
tively regarded PQ(d) as a constant. Thus, in practice
the description model is approximated solely with the
language model as in Equation (4).
</bodyText>
<equation confidence="0.998996">
P(d) ≈ PL(d) (4)
</equation>
<bodyText confidence="0.9998344375">
Statistical approaches to language modeling have
been used in much NLP research, such as machine
translation (Brown et al., 1993) and speech recogni-
tion (Bahl et al., 1983). Our model is almost the same
as existing models, but is different in two respects.
First, while general language models quantify the
extent to which a given word sequence is linguisti-
cally acceptable, our model also quantifies the extent
to which the input is acceptable as a term description.
Thus, we trained the model based on an existing ma-
chine readable encyclopedia.
We used the ChaSen morphological analyzer to
segment the Japanese CD-ROM World Encyclope-
dia (Heibonsha, 1998) into words (we replaced head-
words with a common symbol), and then used the
CMU-Cambridge toolkit (Clarkson and Rosenfeld,
1997) to model a word-based trigram.
Consequently, descriptions in which word se-
quences are more similar to those in the World En-
cyclopedia are assigned greater probability scores
through our language model.
Second, P(d), which is a product of probabilities
for N-grams in d, is quite sensitive to the length of d.
In the cases of machine translation and speech recog-
nition, this problem is less crucial because multiple
candidates compared based on the language model are
almost equivalent in terms of length.
However, since in our case length of descriptions are
significantly different, shorter descriptions are more
likely to be selected, regardless of the quality. To avoid
this problem, we normalize P(d) by the number of
words contained in d.
</bodyText>
<sectionHeader confidence="0.998463" genericHeader="method">
4 Application
</sectionHeader>
<subsectionHeader confidence="0.733699">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999634692307692">
Encyclopedias generated through our Web-based
method can be used in a number of applications, in-
cluding human usage, thesaurus production (Hearst,
1992; Nakamura and Nagao, 1988) and natural lan-
guage understanding in general.
Among the above applications, natural language un-
derstanding (NLU) is the most challenging from a sci-
entific point of view. Current practical NLU research
includes dialogue, information extraction and question
answering, among which we focus solely on question
answering (QA) in this paper.
A straightforward application is to answer inter-
rogative questions like “What is X?” in which a QA
system searches the encyclopedia database for one or
more descriptions related to X (this application is also
effective for dialog systems).
In general, the performance of QA systems are eval-
uated based on coverage and accuracy. Coverage is
the ratio between the number of questions answered
(disregarding their correctness) and the total number
of questions. Accuracy is the ratio between the num-
ber of correct answers and the total number of answers
made by the system.
While coverage can be estimated objectively and
systematically, estimating accuracy relies on human
subjects (because there is no absolute description for
term X), and thus is expensive.
In view of this problem, we targeted Information
Technology Engineers Examinations4, which are bian-
nual (spring and autumn) examinations necessary for
candidates to qualify to be IT engineers in Japan.
Among a number of classes, we focused on the
“Class II” examination, which requires fundamental
and general knowledge related to information technol-
ogy. Approximately half of questions are associated
with IT technical terms.
Since past examinations and answers are open to the
public, we can evaluate the performance of our QA
system with minimal cost.
</bodyText>
<subsectionHeader confidence="0.99751">
4.2 Analyzing IT Engineers Examinations
</subsectionHeader>
<bodyText confidence="0.999701181818182">
The Class II examination consists of quadruple-choice
questions, among which technical term questions can
be subdivided into two types.
In the first type of question, examinees choose
the most appropriate description for a given technical
term, such as “memory interleave” and “router.”
In the second type of question, examinees choose
the most appropriate term for a given question, for
which we show examples collected from the exami-
nation in the autumn of 1999 (translated into English
by one of the authors) as follows:
</bodyText>
<listItem confidence="0.9076092">
1. Which data structure is most appropriate for
FIFO (First-In First-Out)?
a) binary trees, b) queues, c) stacks, d) heaps
2. Choose the LAN access method in which mul-
tiple terminals transmit data simultaneously and
</listItem>
<footnote confidence="0.9983755">
4Japan Information-Technology Engineers Examination
Center. http://www.jitec.jipdec.or.jp/
</footnote>
<bodyText confidence="0.8392326">
thus they potentially collide.
a) ATM, b) CSM/CD, c) FDDI, d) token ring
In the autumn of 1999, out of 80 questions, the num-
ber of the first and second types were 22 and 18, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.999442">
4.3 Implementing a QA system
</subsectionHeader>
<bodyText confidence="0.999992212121212">
For the first type of question, human examinees would
search their knowledge base (i.e., memory) for the de-
scription of a given term, and compare that description
with four candidates. Then they would choose the can-
didate that is most similar to the description.
For the second type of question, human examinees
would search their knowledge base for the description
of each of four candidate terms. Then they would
choose the candidate term whose description is most
similar to the question description.
The mechanism of our QA system is analogous to
the above human methods. However, unlike human
examinees, our system uses an encyclopedia generated
from the Web as a knowledge base.
In addition, our system selectively uses term de-
scriptions categorized into domains related to infor-
mation technology. In other words, the description
of “pipeline (transportation pipe)” is irrelevant or mis-
leading to answer questions associated with “pipeline
(processing method).”
To compute the similarity between two descriptions,
we used techniques developed in IR research, in which
the similarity between a user query and each document
in a collection is usually quantified based on word fre-
quencies. In our case, a question and four possible
answers correspond to query and document collection,
respectively. We used a probabilistic method (Robert-
son and Walker, 1994), which is one of the major IR
methods.
To sum up, given a question, its type and four
choices, our QA system chooses one of four candi-
dates as the answer, in which the resolution algorithm
varies depending on the question type.
</bodyText>
<sectionHeader confidence="0.656707" genericHeader="method">
4.4 Related Work
</sectionHeader>
<bodyText confidence="0.999972875">
Motivated partially by the TREC-8 QA collec-
tion (Voorhees and Tice, 2000), question answering
has of late become one of the major topics within the
NLP/IR communities.
In fact, a number of QA systems targeting
the TREC QA collection have recently been pro-
posed (Harabagiu et al., 2000; Moldovan and
Harabagiu, 2000; Prager et al., 2000). Those sys-
tems are commonly termed “open-domain” systems,
because questions expressed in natural language are
not necessarily limited to explicit axes, including who,
what, when, where, how and why.
However, Moldovan and Harabagiu (2000) found
that each of the TREC questions can be recast as ei-
ther a single axis or a combination of axes. They also
found that out of the 200 TREC questions, 64 ques-
tions (approximately one third) were associated with
the what axis, for which the Web-based encyclopedia
is expected to improve the quality of answers.
Although Harabagiu et al. (2000) proposed a
knowledge-based QA system, most existing systems
rely on conventional IR and shallow NLP methods.
The use of encyclopedic knowledge for QA systems,
as we demonstrated, needs to be further explored.
</bodyText>
<sectionHeader confidence="0.997947" genericHeader="method">
5 Experimentation
</sectionHeader>
<subsectionHeader confidence="0.771146">
5.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999992304347826">
We conducted a number of experiments to investigate
the effectiveness of our methods.
First, we generated an encyclopedia by way of our
Web-based method (see Sections 2 and 3), and evalu-
ated the quality of the encyclopedia itself.
Second, we applied the generated encyclopedia to
our QA system (see Section 4), and evaluated its per-
formance. The second experiment can be seen as a
task-oriented evaluation for our encyclopedia genera-
tion method.
In the first experiment, we collected 96 terms from
technical term questions in the Class II examination
(the autumn of 1999). We used as test inputs those 96
terms and generated an encyclopedia, which was used
in the second experiment.
For all the 96 test terms, Google (see Section 2.2)
retrieved a positive number of pages, and the average
number of pages for one term was 196,503. Since
Google practically outputs contents of the top 1,000
pages, the remaining pages were not used in our ex-
periments.
In the following two sections, we explain the first
and second experiments, respectively.
</bodyText>
<subsectionHeader confidence="0.999309">
5.2 Evaluating Encyclopedia Generation
</subsectionHeader>
<bodyText confidence="0.99990532">
For each test term, our method first computed P(dlc)
using Equation (1) and discarded domains whose
P(dJc) was below 0.05. Then, for each remaining do-
main, descriptions with higher P(dlc) were selected as
the final outputs.
We selected the top three (not one) descriptions for
each domain, because reading a couple of descriptions,
which are short paragraphs, is not laborious for human
users in real-world usage. As a result, at least one de-
scription was generated for 85 test terms, disregarding
the correctness. The number of resultant descriptions
was 326 (3.8 per term). We analyzed those descrip-
tions from different perspectives.
First, we analyzed the distribution of the Google
ranks for the Web pages from which the top three de-
scriptions were eventually retained. Figure 2 shows
the result, where we have combined the pages in
groups of 50, so that the leftmost bar, for example, de-
notes the number of used pages whose original Google
ranks ranged from 1 to 50.
Although the first group includes the largest number
of pages, other groups are also related to a relatively
large number of pages. In other words, our method
exploited a number of low ranking pages, which are
not browsed or utilized by most Web users.
</bodyText>
<figure confidence="0.9963444">
70
60
50
40
30
20
10
0
0 100 200 300 400 500 600 700 800 900 1000
ranking
</figure>
<figureCaption confidence="0.974058">
Figure 2: Distribution of rankings for original pages in
Google.
</figureCaption>
<bodyText confidence="0.9997381">
Second, we analyzed the distribution of domains
assigned to the 326 resultant descriptions. Figure 3
shows the result, in which, as expected, most descrip-
tions were associated with the computer domain.
However, the law domain was unexpectedly asso-
ciated with a relatively great number of descriptions.
We manually analyzed the resultant descriptions and
found that descriptions for which appropriate domains
are not defined in our domain model, such as sports,
tended to be categorized into the law domain.
</bodyText>
<figureCaption confidence="0.982605142857143">
computers (200), law (41), electricity (28),
plants (15), medicine (10), finance (8),
mathematics (8), mechanics (5), biotechnology (4),
construction (2), ecology (2), chemistry (1),
energy (1), oceanography (1)
Figure 3: Distribution of domains related to the 326
resultant descriptions.
</figureCaption>
<bodyText confidence="0.999980795454546">
Third, we evaluated the accuracy of our method,
that is, the quality of an encyclopedia our method gen-
erated. For this purpose, each of the resultant descrip-
tions was judged as to whether or not it is a correct de-
scription for a term in question. Each domain assigned
to descriptions was also judged correct or incorrect.
We analyzed the result on a description-by-
description basis, that is, all the generated descriptions
were considered independent of one another. The ratio
of correct descriptions, disregarding the domain cor-
rectness, was 58.0% (189/326), and the ratio of cor-
rect descriptions categorized into the correct domain
was 47.9% (156/326).
However, since all the test terms are inherently re-
lated to the IT field, we focused solely on descriptions
categorized into the computer domain. In this case,
the ratio of correct descriptions, disregarding the do-
main correctness, was 62.0% (124/200), and the ratio
of correct descriptions categorized into the correct do-
main was 61.5% (123/200).
In addition, we analyzed the result on a term-by-
term basis, because reading only a couple of descrip-
tions is not crucial. In other words, we evaluated
each term (not description), and in the case where at
least one correct description categorized into the cor-
rect domain was generated for a term in question, we
judged it correct. The ratio of correct terms was 89.4%
(76/85), and in the case where we focused solely on the
computer domain, the ratio was 84.8% (67/79).
In other words, by reading a couple of descriptions
(3.8 descriptions per term), human users can obtain
knowledge of approximately 90% of input terms.
Finally, we compared the resultant descriptions with
an existing dictionary. For this purpose, we used the
“Nichigai” computer dictionary (Nichigai Associates,
1996), which lists approximately 30,000 Japanese
technical terms related to the computer field, and con-
tains descriptions for 13,588 terms. In the Nichigai
dictionary, 42 out of the 96 test terms were described.
Our method, which generated correct descriptions as-
sociated with the computer domain for 67 input terms,
enhanced the Nichigai dictionary in terms of quantity.
These results indicate that our method for generat-
ing encyclopedias is of operational quality.
</bodyText>
<subsectionHeader confidence="0.999284">
5.3 Evaluating Question Answering
</subsectionHeader>
<bodyText confidence="0.9914196">
We used as test inputs 40 questions, which are related
to technical terms collected from the Class II exami-
nation in the autumn of 1999.
The objective here is not only to evaluate the perfor-
mance of our QA system itself, but also to evaluate the
quality of the encyclopedia generated by our method.
Thus, as performed in the first experiment (Sec-
tion 5.2), we used the Nichigai computer dictionary as
a baseline encyclopedia. We compared the following
three different resources as a knowledge base:
</bodyText>
<listItem confidence="0.9764306">
• the Nichigai dictionary (“Nichigai”),
• the descriptions generated in the first experiment
(“Web”),
• combination of both resources (“Nichigai +
Web”).
</listItem>
<bodyText confidence="0.975618454545455">
# of pages
Table 1 shows the result of our comparative exper-
iment, in which “C” and “A” denote coverage and ac-
curacy, respectively, for variations of our QA system.
Since all the questions we used are quadruple-
choice, in case the system cannot answer the question,
random choice can be performed to improve the cov-
erage to 100%. Thus, for each knowledge resource we
compared cases without/with random choice, which
are denoted “w/o Random” and “w/ Random” in Ta-
ble 1, respectively.
</bodyText>
<tableCaption confidence="0.8700665">
Table 1: Coverage and accuracy (%) for different ques-
tion answering methods.
</tableCaption>
<table confidence="0.9921262">
w/o Random w/ Random
Resource C A C A
Nichigai 50.0 65.0 100 45.0
Web 92.5 48.6 100 46.9
Nichigai + Web 95.0 63.2 100 61.3
</table>
<bodyText confidence="0.999901166666667">
In the case where random choice was not per-
formed, the Web-based encyclopedia noticeably im-
proved the coverage for the Nichigai dictionary, but
decreased the accuracy. However, by combining both
resources, the accuracy was noticeably improved, and
the coverage was comparable with that for the Nichi-
gai dictionary.
On the other hand, in the case where random choice
was performed, the Nichigai dictionary and the Web-
based encyclopedia were comparable in terms of both
the coverage and accuracy. Additionally, by combin-
ing both resources, the accuracy was further improved.
We also investigated the performance of our QA
system where descriptions related to the computer do-
main are solely used. However, coverage/accuracy did
not significantly change, because as shown in Figure 3,
most of the descriptions were inherently related to the
computer domain.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999952540540541">
The World Wide Web has been an unprecedentedly
enormous information source, from which a number
of language processing methods have been explored
to extract, retrieve and discover various types of infor-
mation.
In this paper, we aimed at generating encyclopedic
knowledge, which is valuable for many applications
including human usage and natural language under-
standing. For this purpose, we reformalized an exist-
ing Web-based extraction method, and proposed a new
statistical organization model to improve the quality of
extracted data.
Given a term for which encyclopedic knowledge
(i.e., descriptions) is to be generated, our method se-
quentially performs a) retrieval of Web pages contain-
ing the term, b) extraction of page fragments describ-
ing the term, and c) organizing extracted descriptions
based on domains (and consequently word senses).
In addition, we proposed a question answering sys-
tem, which answers interrogative questions associated
with what, by using a Web-based encyclopedia as a
knowledge base. For the purpose of evaluation, we
used as test inputs technical terms collected from the
Class II IT engineers examination, and found that the
encyclopedia generated through our method was of
operational quality and quantity.
We also used test questions from the Class II exam-
ination, and evaluated the Web-based encyclopedia in
terms of question answering. We found that our Web-
based encyclopedia improved the system coverage ob-
tained solely with an existing dictionary. In addition,
when we used both resources, the performance was
further improved.
Future work would include generating information
associated with more complex interrogations, such as
ones related to how and why, so as to enhance Web-
based natural language understanding.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999905">
The authors would like to thank NOVA, Inc. for their
support with the Nova dictionary and Katunobu Itou
(The National Institute of Advanced Industrial Science
and Technology, Japan) for his insightful comments on
this paper.
</bodyText>
<sectionHeader confidence="0.997116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982413747747748">
Brian Amento, Loren Terveen, and Will Hill. 2000.
Does “authority” mean quality? predicting expert
quality ratings of Web documents. In Proceedings
of the 23rd Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 296–303.
Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mer-
cer. 1983. A maximum linklihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
5(2):179–190.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1–7):107–117.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal language modeling using the CMU-Cambridge
toolkit. In Proceedings of EuroSpeech’97, pages
2707–2710.
Oren Etzioni. 1997. Moving up the information food
chain. AI Magazine, 18(2):11–18.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488–495.
Sanda M. Harabagiu, Marius A. Pas¸ca, and Steven J.
Maiorano. 2000. Experiments with open-domain
textual question answering. In Proceedings of the
18th International Conference on Computational
Linguistics, pages 292–298.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th International Conference on Computa-
tional Linguistics, pages 539–545.
Hitachi Digital Heibonsha. 1998. CD-ROM World
Encyclopedia. (In Japanese).
Akihiro Inokuchi, Takashi Washio, Hiroshi Motoda,
Kouhei Kumasawa, and Naohide Arai. 1999. Bas-
ket analysis for graph structured data. In Proceed-
ings of the 3rd Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining, pages 420–431.
Makoto Iwayama and Takenobu Tokunaga. 1994. A
probabilistic model for text categorization: Based
on a single random variable with multiple values. In
Proceedings of the 4th Conference on Applied Nat-
ural Language Processing, pages 162–167.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological analysis
system ChaSen manual. Technical Report NAIST-
IS-TR97007, NAIST. (In Japanese).
Andrew McCallum, Kamal Nigam, Jason Rennie, and
Kristie Seymore. 1999. A machine learning ap-
proach to building domain-specific search engines.
In Proceedings of the 16th International Joint Con-
ference on Artificial Intelligence, pages 662–667.
Dan Moldovan and Sanda Harabagiu. 2000. The
structure and performance of an open-domain ques-
tion answering system. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 563–570.
Jun’ichi Nakamura and Makoto Nagao. 1988. Extrac-
tion of semantic information from an ordinary En-
glish dictionary and its evaluation. In Proceedings
of the 10th International Conference on Computa-
tional Linguistics, pages 459–464.
Nichigai Associates. 1996. English-Japanese com-
puter terminology dictionary. (In Japanese).
John Prager, Eric Brown, and Anni Coden. 2000.
Question-answering by predictive annotation. In
Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 184–191.
Philip Resnik. 1999. Mining the Web for bilingual
texts. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 527–534.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 232–241.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Stephen Soderland. 1997. Learning to extract text-
based information from the World Wide Web. In
Proceedings of 3rd International Conference on
Knowledge Discovery and Data Mining.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 200–207.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189–196.
Xiaolan Zhu and Susan Gauch. 2000. Incorporating
quality metrics in centralized/distributed informa-
tion retrieval on the World Wide Web. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 288–295.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239449">
<title confidence="0.997987">Organizing Encyclopedic Knowledge based on the Web and its Application to Question Answering</title>
<author confidence="0.996325">Atsushi Fujii Tetsuya Ishikawa</author>
<affiliation confidence="0.999833">University of Library and University of Library and Information Science Information Science</affiliation>
<address confidence="0.990551">1-2 Kasuga, Tsukuba 1-2 Kasuga, Tsukuba 305-8550, Japan 305-8550, Japan</address>
<affiliation confidence="0.7308435">CREST, Japan Science and ishikawa@ulis.ac.jp Technology Corporation</affiliation>
<email confidence="0.951421">fujii@ulis.ac.jp</email>
<abstract confidence="0.999499923076923">We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web. We first search the Web for pages containing a term in question. Then we use linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese Information-</abstract>
<intro confidence="0.504245">Technology Engineers Examination.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brian Amento</author>
<author>Loren Terveen</author>
<author>Will Hill</author>
</authors>
<title>Does “authority” mean quality? predicting expert quality ratings of Web documents.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>296--303</pages>
<contexts>
<context position="12563" citStr="Amento et al., 2000" startWordPosition="1978" endWordPosition="1981">hus our method is a practical approximation. 3.3 Description Model The description model quantifies the extent to which a given page fragment is feasible as a description for the input term. In principle, we decompose the description model into language and quality properties, as shown in Equation (3). P(d) = PL(d) · PQ(d) (3) Here, PL(d) and PQ(d) denote language and quality models, respectively. 3Produced by NOVA, Inc. It is expected that the quality model discards incorrect or misleading information contained in Web pages. For this purpose, a number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ(d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P(d) ≈ PL(d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost </context>
</contexts>
<marker>Amento, Terveen, Hill, 2000</marker>
<rawString>Brian Amento, Loren Terveen, and Will Hill. 2000. Does “authority” mean quality? predicting expert quality ratings of Web documents. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 296–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum linklihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="13141" citStr="Bahl et al., 1983" startWordPosition="2073" endWordPosition="2076">thods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ(d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P(d) ≈ PL(d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia. We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used the CMU-Cambridge toolkit (Clarkson and Rose</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum linklihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks,</journal>
<pages>30--1</pages>
<contexts>
<context position="12798" citStr="Brin and Page, 1998" startWordPosition="2016" endWordPosition="2019">model into language and quality properties, as shown in Equation (3). P(d) = PL(d) · PQ(d) (3) Here, PL(d) and PQ(d) denote language and quality models, respectively. 3Produced by NOVA, Inc. It is expected that the quality model discards incorrect or misleading information contained in Web pages. For this purpose, a number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ(d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P(d) ≈ PL(d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the inpu</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13098" citStr="Brown et al., 1993" startWordPosition="2065" endWordPosition="2068"> this purpose, a number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ(d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P(d) ≈ PL(d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia. We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of EuroSpeech’97,</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="13753" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="2171" endWordPosition="2174">ahl et al., 1983). Our model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia. We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997) to model a word-based trigram. Consequently, descriptions in which word sequences are more similar to those in the World Encyclopedia are assigned greater probability scores through our language model. Second, P(d), which is a product of probabilities for N-grams in d, is quite sensitive to the length of d. In the cases of machine translation and speech recognition, this problem is less crucial because multiple candidates compared based on the language model are almost equivalent in terms of length. However, since in our case length of descriptions are significantly different, shorter descrip</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of EuroSpeech’97, pages 2707–2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
</authors>
<title>Moving up the information food chain.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="1376" citStr="Etzioni, 1997" startWordPosition="198" endWordPosition="199">tion, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract fragments describing the term. They also use a language model to discard non-linguistic fragments. In additi</context>
</contexts>
<marker>Etzioni, 1997</marker>
<rawString>Oren Etzioni. 1997. Moving up the information food chain. AI Magazine, 18(2):11–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Utilizing the World Wide Web as an encyclopedia: Extracting term descriptions from semi-structured texts.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>488--495</pages>
<contexts>
<context position="1272" citStr="Fujii and Ishikawa, 2000" startWordPosition="181" endWordPosition="184">nts describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract frag</context>
<context position="3892" citStr="Fujii and Ishikawa (2000)" startWordPosition="588" endWordPosition="591">es a method for applying our resultant encyclopedia to NLP research, specifically, question answering. Section 5 performs a number of experiments to evaluate our methods. 2 System Design 2.1 Overview Figure 1 depicts the overall design of our system, which generates an encyclopedia for input terms. Our system, which is currently implemented for Japanese, consists of three modules: “retrieval,” “extraction” and “organization,” among which the organization module is newly introduced in this paper. In principle, the remaining two modules (“retrieval” and “extraction”) are the same as proposed by Fujii and Ishikawa (2000). In Figure 1, terms can be submitted either on-line or off-line. A reasonable method is that while the system periodically updates the encyclopedia off-line, terms unindexed in the encyclopedia are dynamically processed in real-time usage. In either case, our system processes input terms one by one. We briefly explain each module in the following three sections, respectively. term(s) Figure 1: The overall design of our Web-based encyclopedia generation system. 2.2 Retrieval The retrieval module searches the Web for pages containing an input term, for which existing Web search engines can be u</context>
<context position="5540" citStr="Fujii and Ishikawa (2000)" startWordPosition="860" endWordPosition="863">” which is one of the major search engines and does not conduct query expansion1. 2.3 Extraction In the extraction module, given Web pages containing an input term, newline codes, redundant white spaces and HTML tags that are not used in the following processes are discarded to standardize the page format. Second, we approximately identify a region describing the term in the page, for which two rules are used. 1http://www.google.com/ The first rule is based on Japanese linguistic patterns typically used for term descriptions, such as “X toha Y dearu (X is Y).” Following the method proposed by Fujii and Ishikawa (2000), we semi-automatically produced 20 patterns based on the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998), which includes approximately 80,000 entries related to various fields. It is expected that a region including the sentence that matched with one of those patterns can be a term description. The second rule is based on HTML layout. In a typical case, a term in question is highlighted as a heading with tags such as &lt;DT&gt;, &lt;B&gt; and &lt;Hx&gt; (“x” denotes a digit), followed by its description. In some cases, terms are marked with the anchor &lt;A&gt; tag, providing hyperlinks to pages where they are </context>
<context position="9448" citStr="Fujii and Ishikawa (2000)" startWordPosition="1483" endWordPosition="1486">y that d corresponds to domain c. P(d) models a probability that d can be a description for the term in question, disregarding the domain. We shall call them domain and description models, respectively. To sum up, in principle we select d’s that are strongly associated with a specific domain, and are likely to be descriptions themselves. Extracted descriptions are not linguistically understandable in the case where the extraction process is unsuccessful and retrieved pages inherently contain non-linguistic information (such as special characters and e-mail addresses). To resolve this problem, Fujii and Ishikawa (2000) used a language model to filter out descriptions with low perplexity. However, in this paper we integrated a description model, which is practically the same as a language model, with an organization model. The new framework is more understandable with respect to probability theory. In practice, we first use Equation (1) to compute P(d|c) for all the c’s predefined in the domain model. Then we discard such c’s whose P(d|c) is below a specific threshold. As a result, for the input term, related domains and descriptions are simultaneously selected. Thus, we do not have to know a priori which do</context>
</contexts>
<marker>Fujii, Ishikawa, 2000</marker>
<rawString>Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing the World Wide Web as an encyclopedia: Extracting term descriptions from semi-structured texts. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 488–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Marius A Pas¸ca</author>
<author>Steven J Maiorano</author>
</authors>
<title>Experiments with open-domain textual question answering.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>292--298</pages>
<marker>Harabagiu, Pas¸ca, Maiorano, 2000</marker>
<rawString>Sanda M. Harabagiu, Marius A. Pas¸ca, and Steven J. Maiorano. 2000. Experiments with open-domain textual question answering. In Proceedings of the 18th International Conference on Computational Linguistics, pages 292–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="14676" citStr="Hearst, 1992" startWordPosition="2319" endWordPosition="2320">n the cases of machine translation and speech recognition, this problem is less crucial because multiple candidates compared based on the language model are almost equivalent in terms of length. However, since in our case length of descriptions are significantly different, shorter descriptions are more likely to be selected, regardless of the quality. To avoid this problem, we normalize P(d) by the number of words contained in d. 4 Application 4.1 Overview Encyclopedias generated through our Web-based method can be used in a number of applications, including human usage, thesaurus production (Hearst, 1992; Nakamura and Nagao, 1988) and natural language understanding in general. Among the above applications, natural language understanding (NLU) is the most challenging from a scientific point of view. Current practical NLU research includes dialogue, information extraction and question answering, among which we focus solely on question answering (QA) in this paper. A straightforward application is to answer interrogative questions like “What is X?” in which a QA system searches the encyclopedia database for one or more descriptions related to X (this application is also effective for dialog syst</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitachi Digital Heibonsha</author>
</authors>
<date>1998</date>
<journal>CD-ROM World Encyclopedia. (In Japanese).</journal>
<contexts>
<context position="5650" citStr="Heibonsha, 1998" startWordPosition="876" endWordPosition="877">le, given Web pages containing an input term, newline codes, redundant white spaces and HTML tags that are not used in the following processes are discarded to standardize the page format. Second, we approximately identify a region describing the term in the page, for which two rules are used. 1http://www.google.com/ The first rule is based on Japanese linguistic patterns typically used for term descriptions, such as “X toha Y dearu (X is Y).” Following the method proposed by Fujii and Ishikawa (2000), we semi-automatically produced 20 patterns based on the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998), which includes approximately 80,000 entries related to various fields. It is expected that a region including the sentence that matched with one of those patterns can be a term description. The second rule is based on HTML layout. In a typical case, a term in question is highlighted as a heading with tags such as &lt;DT&gt;, &lt;B&gt; and &lt;Hx&gt; (“x” denotes a digit), followed by its description. In some cases, terms are marked with the anchor &lt;A&gt; tag, providing hyperlinks to pages where they are described. Finally, based on the region briefly identified by the above method, we extract a page fragment as </context>
<context position="13625" citStr="Heibonsha, 1998" startWordPosition="2153" endWordPosition="2154">ing have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia. We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997) to model a word-based trigram. Consequently, descriptions in which word sequences are more similar to those in the World Encyclopedia are assigned greater probability scores through our language model. Second, P(d), which is a product of probabilities for N-grams in d, is quite sensitive to the length of d. In the cases of machine translation and speech recognition, this problem is less crucial because multiple candidates compared based on the language model are almo</context>
</contexts>
<marker>Heibonsha, 1998</marker>
<rawString>Hitachi Digital Heibonsha. 1998. CD-ROM World Encyclopedia. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Inokuchi</author>
<author>Takashi Washio</author>
<author>Hiroshi Motoda</author>
<author>Kouhei Kumasawa</author>
<author>Naohide Arai</author>
</authors>
<title>Basket analysis for graph structured data.</title>
<date>1999</date>
<booktitle>In Proceedings of the 3rd Pacific-Asia Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>420--431</pages>
<contexts>
<context position="1470" citStr="Inokuchi et al., 1999" startWordPosition="211" endWordPosition="214"> targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract fragments describing the term. They also use a language model to discard non-linguistic fragments. In addition, a clustering method is used to divide descriptions into a specific number of groups. On th</context>
</contexts>
<marker>Inokuchi, Washio, Motoda, Kumasawa, Arai, 1999</marker>
<rawString>Akihiro Inokuchi, Takashi Washio, Hiroshi Motoda, Kouhei Kumasawa, and Naohide Arai. 1999. Basket analysis for graph structured data. In Proceedings of the 3rd Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 420–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Iwayama</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>A probabilistic model for text categorization: Based on a single random variable with multiple values.</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied Natural Language Processing,</booktitle>
<pages>162--167</pages>
<contexts>
<context position="10461" citStr="Iwayama and Tokunaga (1994)" startWordPosition="1649" endWordPosition="1652">el. Then we discard such c’s whose P(d|c) is below a specific threshold. As a result, for the input term, related domains and descriptions are simultaneously selected. Thus, we do not have to know a priori which domains are related to each term. In the following two sections, we explain methods to realize the domain and description models, respectively. 3.2 Domain Model The domain model quantifies the extent to which description d is associated with domain c, which is fundamentally a categorization task. Among a number of existing categorization methods, we experimentally used one proposed by Iwayama and Tokunaga (1994), which formulates P(c|d) as in Equation (2). � P(t|c) · P(t|d) (2) P (c|d) = P (c) · P(t) t Here, P(t|d), P(t|c) and P(t) denote probabilities that word t appears in d, c and all the domains, respectively. We regard P(c) as a constant. While P(t|d) is simply a relative frequency of t in d, we need predefined domains to compute P(t|c) and P(t). For this purpose, the use of large-scale corpora annotated with domains is desirable. However, since those resources are prohibitively expensive, we used the “Nova” dictionary for Japanese/English machine translation systems3, which includes approximate</context>
</contexts>
<marker>Iwayama, Tokunaga, 1994</marker>
<rawString>Makoto Iwayama and Takenobu Tokunaga. 1994. A probabilistic model for text categorization: Based on a single random variable with multiple values. In Proceedings of the 4th Conference on Applied Natural Language Processing, pages 162–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura.</title>
<date>1997</date>
<note>Japanese morphological analysis system ChaSen manual. Technical Report NAISTIS-TR97007, NAIST. (In Japanese).</note>
<marker>Matsumoto, 1997</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological analysis system ChaSen manual. Technical Report NAISTIS-TR97007, NAIST. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
<author>Jason Rennie</author>
<author>Kristie Seymore</author>
</authors>
<title>A machine learning approach to building domain-specific search engines.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>662--667</pages>
<contexts>
<context position="1400" citStr="McCallum et al., 1999" startWordPosition="200" endWordPosition="203">an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract fragments describing the term. They also use a language model to discard non-linguistic fragments. In addition, a clustering method </context>
</contexts>
<marker>McCallum, Nigam, Rennie, Seymore, 1999</marker>
<rawString>Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. 1999. A machine learning approach to building domain-specific search engines. In Proceedings of the 16th International Joint Conference on Artificial Intelligence, pages 662–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>The structure and performance of an open-domain question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>563--570</pages>
<contexts>
<context position="19364" citStr="Moldovan and Harabagiu, 2000" startWordPosition="3057" endWordPosition="3060">ctively. We used a probabilistic method (Robertson and Walker, 1994), which is one of the major IR methods. To sum up, given a question, its type and four choices, our QA system chooses one of four candidates as the answer, in which the resolution algorithm varies depending on the question type. 4.4 Related Work Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000), question answering has of late become one of the major topics within the NLP/IR communities. In fact, a number of QA systems targeting the TREC QA collection have recently been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000). Those systems are commonly termed “open-domain” systems, because questions expressed in natural language are not necessarily limited to explicit axes, including who, what, when, where, how and why. However, Moldovan and Harabagiu (2000) found that each of the TREC questions can be recast as either a single axis or a combination of axes. They also found that out of the 200 TREC questions, 64 questions (approximately one third) were associated with the what axis, for which the Web-based encyclopedia is expected to improve the quality of answers. Although Harabagiu et al. </context>
</contexts>
<marker>Moldovan, Harabagiu, 2000</marker>
<rawString>Dan Moldovan and Sanda Harabagiu. 2000. The structure and performance of an open-domain question answering system. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 563–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Nakamura</author>
<author>Makoto Nagao</author>
</authors>
<title>Extraction of semantic information from an ordinary English dictionary and its evaluation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics,</booktitle>
<pages>459--464</pages>
<contexts>
<context position="14703" citStr="Nakamura and Nagao, 1988" startWordPosition="2321" endWordPosition="2324"> machine translation and speech recognition, this problem is less crucial because multiple candidates compared based on the language model are almost equivalent in terms of length. However, since in our case length of descriptions are significantly different, shorter descriptions are more likely to be selected, regardless of the quality. To avoid this problem, we normalize P(d) by the number of words contained in d. 4 Application 4.1 Overview Encyclopedias generated through our Web-based method can be used in a number of applications, including human usage, thesaurus production (Hearst, 1992; Nakamura and Nagao, 1988) and natural language understanding in general. Among the above applications, natural language understanding (NLU) is the most challenging from a scientific point of view. Current practical NLU research includes dialogue, information extraction and question answering, among which we focus solely on question answering (QA) in this paper. A straightforward application is to answer interrogative questions like “What is X?” in which a QA system searches the encyclopedia database for one or more descriptions related to X (this application is also effective for dialog systems). In general, the perfo</context>
</contexts>
<marker>Nakamura, Nagao, 1988</marker>
<rawString>Jun’ichi Nakamura and Makoto Nagao. 1988. Extraction of semantic information from an ordinary English dictionary and its evaluation. In Proceedings of the 10th International Conference on Computational Linguistics, pages 459–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nichigai Associates</author>
</authors>
<title>English-Japanese computer terminology dictionary.</title>
<date>1996</date>
<note>(In Japanese).</note>
<contexts>
<context position="25243" citStr="Associates, 1996" startWordPosition="4013" endWordPosition="4014">cription), and in the case where at least one correct description categorized into the correct domain was generated for a term in question, we judged it correct. The ratio of correct terms was 89.4% (76/85), and in the case where we focused solely on the computer domain, the ratio was 84.8% (67/79). In other words, by reading a couple of descriptions (3.8 descriptions per term), human users can obtain knowledge of approximately 90% of input terms. Finally, we compared the resultant descriptions with an existing dictionary. For this purpose, we used the “Nichigai” computer dictionary (Nichigai Associates, 1996), which lists approximately 30,000 Japanese technical terms related to the computer field, and contains descriptions for 13,588 terms. In the Nichigai dictionary, 42 out of the 96 test terms were described. Our method, which generated correct descriptions associated with the computer domain for 67 input terms, enhanced the Nichigai dictionary in terms of quantity. These results indicate that our method for generating encyclopedias is of operational quality. 5.3 Evaluating Question Answering We used as test inputs 40 questions, which are related to technical terms collected from the Class II ex</context>
</contexts>
<marker>Associates, 1996</marker>
<rawString>Nichigai Associates. 1996. English-Japanese computer terminology dictionary. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
</authors>
<title>Question-answering by predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="19386" citStr="Prager et al., 2000" startWordPosition="3061" endWordPosition="3064">ic method (Robertson and Walker, 1994), which is one of the major IR methods. To sum up, given a question, its type and four choices, our QA system chooses one of four candidates as the answer, in which the resolution algorithm varies depending on the question type. 4.4 Related Work Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000), question answering has of late become one of the major topics within the NLP/IR communities. In fact, a number of QA systems targeting the TREC QA collection have recently been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000). Those systems are commonly termed “open-domain” systems, because questions expressed in natural language are not necessarily limited to explicit axes, including who, what, when, where, how and why. However, Moldovan and Harabagiu (2000) found that each of the TREC questions can be recast as either a single axis or a combination of axes. They also found that out of the 200 TREC questions, 64 questions (approximately one third) were associated with the what axis, for which the Web-based encyclopedia is expected to improve the quality of answers. Although Harabagiu et al. (2000) proposed a know</context>
</contexts>
<marker>Prager, Brown, Coden, 2000</marker>
<rawString>John Prager, Eric Brown, and Anni Coden. 2000. Question-answering by predictive annotation. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for bilingual texts.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>527--534</pages>
<contexts>
<context position="1286" citStr="Resnik, 1999" startWordPosition="185" endWordPosition="186">inally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract fragments describi</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Mining the Web for bilingual texts. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 527–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="18804" citStr="Robertson and Walker, 1994" startWordPosition="2959" endWordPosition="2963">criptions categorized into domains related to information technology. In other words, the description of “pipeline (transportation pipe)” is irrelevant or misleading to answer questions associated with “pipeline (processing method).” To compute the similarity between two descriptions, we used techniques developed in IR research, in which the similarity between a user query and each document in a collection is usually quantified based on word frequencies. In our case, a question and four possible answers correspond to query and document collection, respectively. We used a probabilistic method (Robertson and Walker, 1994), which is one of the major IR methods. To sum up, given a question, its type and four choices, our QA system chooses one of four candidates as the answer, in which the resolution algorithm varies depending on the question type. 4.4 Related Work Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000), question answering has of late become one of the major topics within the NLP/IR communities. In fact, a number of QA systems targeting the TREC QA collection have recently been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000). Those systems ar</context>
</contexts>
<marker>Robertson, Walker, 1994</marker>
<rawString>S. E. Robertson and S. Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning to extract textbased information from the World Wide Web.</title>
<date>1997</date>
<booktitle>In Proceedings of 3rd International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="1304" citStr="Soderland, 1997" startWordPosition="187" endWordPosition="188">anize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract fragments describing the term. They </context>
</contexts>
<marker>Soderland, 1997</marker>
<rawString>Stephen Soderland. 1997. Learning to extract textbased information from the World Wide Web. In Proceedings of 3rd International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Dawn M Tice</author>
</authors>
<title>Building a question answering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="19123" citStr="Voorhees and Tice, 2000" startWordPosition="3017" endWordPosition="3020">esearch, in which the similarity between a user query and each document in a collection is usually quantified based on word frequencies. In our case, a question and four possible answers correspond to query and document collection, respectively. We used a probabilistic method (Robertson and Walker, 1994), which is one of the major IR methods. To sum up, given a question, its type and four choices, our QA system chooses one of four candidates as the answer, in which the resolution algorithm varies depending on the question type. 4.4 Related Work Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000), question answering has of late become one of the major topics within the NLP/IR communities. In fact, a number of QA systems targeting the TREC QA collection have recently been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000). Those systems are commonly termed “open-domain” systems, because questions expressed in natural language are not necessarily limited to explicit axes, including who, what, when, where, how and why. However, Moldovan and Harabagiu (2000) found that each of the TREC questions can be recast as either a single axis or a combination of ax</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen M. Voorhees and Dawn M. Tice. 2000. Building a question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="7259" citStr="Yarowsky, 1995" startWordPosition="1144" endWordPosition="1145">agged with &lt;UL&gt;, 4. N sentences, where we empirically set N = 3. 2.4 Organization As discussed in Section 1, organizing information extracted from the Web is crucial in our framework. For this purpose, we classify extracted term descriptions based on word senses and domains. Although a number of methods have been proposed to generate word senses (for example, one based on the vector space model (Sch¨utze, 1998)), it is still difficult to accurately identify word senses without explicit dictionaries that define sense candidates. In addition, since word senses are often associated with domains (Yarowsky, 1995), word senses can be consequently distinguished by way of determining the domain of each description. For example, different senses for “pipeline (processing method/transportation pipe)” are associated with the computer and construction domains (fields), respectively. To sum up, the organization module classifies term descriptions based on domains, for which we use domain and description models. In Section 3, we elaborate on our organization model. 2&lt;DT&gt; and &lt;DD&gt; are inherently provided to describe terms in HTML. encyclopedia organization extraction retrieval description model extraction rules</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolan Zhu</author>
<author>Susan Gauch</author>
</authors>
<title>Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>288--295</pages>
<contexts>
<context position="12585" citStr="Zhu and Gauch, 2000" startWordPosition="1982" endWordPosition="1985">ractical approximation. 3.3 Description Model The description model quantifies the extent to which a given page fragment is feasible as a description for the input term. In principle, we decompose the description model into language and quality properties, as shown in Equation (3). P(d) = PL(d) · PQ(d) (3) Here, PL(d) and PQ(d) denote language and quality models, respectively. 3Produced by NOVA, Inc. It is expected that the quality model discards incorrect or misleading information contained in Web pages. For this purpose, a number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ(d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P(d) ≈ PL(d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost the same as existing m</context>
</contexts>
<marker>Zhu, Gauch, 2000</marker>
<rawString>Xiaolan Zhu and Susan Gauch. 2000. Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 288–295.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>