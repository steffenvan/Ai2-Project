<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.234776">
<title confidence="0.9959475">
The GATE Crowdsourcing Plugin: Crowdsourcing Annotated Corpora
Made Easy
</title>
<author confidence="0.999287">
Kalina Bontcheva, Ian Roberts, Leon Derczynski, Dominic Rout
</author>
<affiliation confidence="0.998707">
University of Sheffield
</affiliation>
<email confidence="0.995786">
{kalina,ian,leon,d.rout}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.99735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896736842106">
Crowdsourcing is an increasingly popu-
lar, collaborative approach for acquiring
annotated corpora. Despite this, reuse
of corpus conversion tools and user in-
terfaces between projects is still problem-
atic, since these are not generally made
available. This demonstration will intro-
duce the new, open-source GATE Crowd-
sourcing plugin, which offers infrastruc-
tural support for mapping documents to
crowdsourcing units and back, as well as
automatically generating reusable crowd-
sourcing interfaces for NLP classification
and selection tasks. The entire work-
flow will be demonstrated on: annotating
named entities; disambiguating words and
named entities with respect to DBpedia
URIs; annotation of opinion holders and
targets; and sentiment.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802806451613">
Annotation science (Hovy, 2010; Stede and
Huang, 2012) and general purpose corpus anno-
tation tools (e.g. Bontcheva et al. (2013)) have
evolved in response to the need for creating high-
quality NLP corpora. Crowdsourcing is a popu-
lar collaborative approach that has been applied
to acquiring annotated corpora and a wide range
of other linguistic resources (Callison-Burch and
Dredze, 2010; Fort et al., 2011; Wang et al., 2012).
Although the use of this approach is intensifying,
especially paid-for crowdsourcing, the reuse of an-
notation guidelines, task designs, and user inter-
faces between projects is still problematic, since
these are generally not made available, despite
their important role in result quality (Khanna et
al., 2010).
A big outstanding challenge for crowdsourc-
ing projects is that the cost to define a single
annotation task remains quite substantial. This
demonstration will introduce the new, open-source
GATE Crowdsourcing plugin, which offers in-
frastructural support for mapping documents to
crowdsourcing units, as well as automatically gen-
erated, reusable user interfaces1 for NLP classi-
fication and selection tasks. Their use will be
demonstrated on annotating named entities (selec-
tion task), disambiguating words and named enti-
ties with respect to DBpedia URIs (classification
task), annotation of opinion holders and targets
(selection task), as well as sentiment (classifica-
tion task).
</bodyText>
<subsectionHeader confidence="0.8544135">
2 Crowdsourcing Stages and the Role of
Infrastructural Support
</subsectionHeader>
<bodyText confidence="0.999988714285714">
Conceptually, the process of crowdsourcing anno-
tated corpora can be broken down into four main
stages, within which there are a number of largely
infrastructural steps. In particular, data prepara-
tion and transformation into CrowdFlower units,
creation of the annotation UI, creation and upload
of gold units for quality control, and finally map-
ping judgements back into documents and aggre-
gating all judgements into a finished corpus.
The rest of this section discusses in more de-
tail where reusable components and infrastructural
support for automatic data mapping and user inter-
face generation are necessary, in order to reduce
the overhead of crowdsourcing NLP corpora.
</bodyText>
<subsectionHeader confidence="0.988713">
2.1 Project Definition
</subsectionHeader>
<bodyText confidence="0.999829714285714">
An important part of project definition is the map-
ping of the NLP problem into one or more crowd-
sourcing tasks, which are sufficiently simple to be
carried out by non-experts and with a good qual-
ity. What are helpful here are reusable patterns
for how best to crowdsource different kinds of
NLP corpora. The GATE Crowdsourcing plugin
</bodyText>
<footnote confidence="0.8125925">
1Currently for CrowdFlower, which unlike Amazon Me-
chanical Turk is available globally.
</footnote>
<page confidence="0.993424">
97
</page>
<note confidence="0.6105205">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 97–100,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999716666666667">
currently provides such patterns for selection and
classification tasks.
This stage also focuses on setup of the task pa-
rameters (e.g. number of crowd workers per task,
payment per task) and piloting the project, in order
to tune in its design. With respect to task param-
eters, infrastructural support is helpful, in order
to enable automatic splitting of longer documents
across crowdsourcing tasks.
</bodyText>
<subsectionHeader confidence="0.997908">
2.2 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999969125">
This stage, in particular, can benefit significantly
from infrastructural support and reusable compo-
nents, in order to collect the data (e.g. crawl
the web, download samples from Twitter), pre-
process it with linguistic tools (e.g. tokenisation,
POS tagging, entity recognition), and then map
automatically from documents and sentences to
crowdsourcing micro-tasks.
</bodyText>
<subsectionHeader confidence="0.999001">
2.3 Running the Crowdsourcing Project
</subsectionHeader>
<bodyText confidence="0.9999545">
This is the main phase of each crowdsourcing
project. It consists of three kinds of tasks: task
workflow and management, contributor manage-
ment (including profiling and retention), and qual-
ity control. Paid-for marketplaces like Amazon
Mechanical Turk and CrowdFlower already pro-
vide this support. As with conventional corpus an-
notation, quality control is particularly challeng-
ing, and additional NLP-specific infrastructural
support can help.
</bodyText>
<subsectionHeader confidence="0.99931">
2.4 Data Evaluation and Aggregation
</subsectionHeader>
<bodyText confidence="0.999986857142857">
In this phase, additional NLP-specific, infrastruc-
tural support is needed for evaluating and aggre-
gating the multiple contributor inputs into a com-
plete linguistic resource, and in assessing the re-
sulting overall quality.
Next we demonstrate how these challenges have
been addressed in our work.
</bodyText>
<sectionHeader confidence="0.998764" genericHeader="method">
3 The GATE Crowdsourcing Plugin
</sectionHeader>
<bodyText confidence="0.9999468">
To address these NLP-specific requirements,
we implemented a generic, open-source GATE
Crowdsourcing plugin, which makes it very easy
to set up and conduct crowdsourcing-based corpus
annotation from within GATE’s visual interface.
</bodyText>
<subsectionHeader confidence="0.962498">
3.1 Physical representation for documents
and annotations
</subsectionHeader>
<bodyText confidence="0.9885765">
Documents and their annotations are encoded in
the GATE stand-off XML format (Cunningham
</bodyText>
<figureCaption confidence="0.999391">
Figure 1: Classification UI Configuration
</figureCaption>
<bodyText confidence="0.999950153846154">
et al., 2002), which was chosen for its support
for overlapping annotations and the wide range of
automatic pre-processing tools available. GATE
also has support for the XCES standard (Ide et al.,
2000) and others (e.g. CoNLL) if preferred. An-
notations are grouped in separate annotation sets:
one for the automatically pre-annotated annota-
tions, one for the crowdsourced judgements, and
a consensus set, which can be considered as the fi-
nal resulting corpus annotation layer. In this way,
provenance is fully tracked, which makes it possi-
ble to experiment with methods that consider more
than one answer as potentially correct.
</bodyText>
<subsectionHeader confidence="0.9655055">
3.2 Automatic data mapping to
CrowdFlower
</subsectionHeader>
<bodyText confidence="0.999881583333333">
The plugin expects documents to be pre-
segmented into paragraphs, sentences and word
tokens, using a tokeniser, POS tagger, and sen-
tence splitter – e.g. those built in to GATE (Cun-
ningham et al., 2002). The GATE Crowdsourcing
plugin allows choice between these of which to
use as the crowdsourcing task unit; e.g., to show
one sentence per unit or one paragraph. In the
demonstration we will show both automatic map-
ping at sentence level (for named entity annota-
tion) and at paragraph level (for named entity dis-
ambiguation).
</bodyText>
<subsectionHeader confidence="0.999601">
3.3 Automatic user interface generation
</subsectionHeader>
<bodyText confidence="0.9995235">
The User Interfaces (UIs) applicable to various
task types tend to fall into a set of categories, the
most commonly used being categorisation, selec-
tion, and text input. The GATE Crowdsourcing
plugin provides generalised and re-usable, auto-
matically generated interfaces for categorisation
</bodyText>
<page confidence="0.988656">
98
</page>
<figureCaption confidence="0.9999965">
Figure 2: Classification Interface: Sense Disambiguation Example
Figure 3: Sequential Selection Interface: Named Entity Recognition Example
</figureCaption>
<bodyText confidence="0.996631761904762">
and selection.
In the first step, task name, instructions, and
classification choices are provided, in a UI config-
uration dialog (see Figure 1). In this example, the
instructions are for disambiguating named entities.
We have configured three fixed choices, which ap-
ply to each entity classification task.
For some categorisation NLP annotation tasks
(e.g. classifying sentiment in tweets into posi-
tive, negative, and neutral), fixed categories are
sufficient. In others, where the available category
choices depend on the text that is being classi-
fied (e.g. the possible disambiguations of Paris
are different from those of London), choices are
defined through annotations on each of the clas-
sification targets. In this case case, the UI gen-
erator then takes these annotations as a parame-
ter and automatically creates the different category
choices, specific to each crowdsourcing unit. Fig-
ure 2 shows an example for sense disambiguation,
which combines two unit-specific classes with the
three fixed classification categories shown before.
Figure 3 shows the CrowdFlower-based user in-
terface for word-constrained sequential selection,
which in this case is parameterised for named en-
tity annotation. In sequential selection, sub-units
are defined in the UI configuration – tokens, for
this example. The annotators are instructed to
click on all words that constitute the desired se-
quence (the annotation guidelines are given as a
parameter during the automatic user interface gen-
eration).
Since the text may not contain a sequence to be
annotated, we also generate an explicit confirma-
tion checkbox. This forces annotators to declare
that they have made the selection or there is noth-
ing to be selected in this text. CrowdFlower can
then use gold units and test the correctness of the
selections, even in cases where no sequences are
selected in the text. In addition, requiring at least
some worker interaction and decision-making in
every task improves overall result quality.
</bodyText>
<subsectionHeader confidence="0.996495">
3.4 Quality control
</subsectionHeader>
<bodyText confidence="0.999953785714286">
The key mechanism for spam prevention and qual-
ity control in CrowdFlower is test data, which
we also refer to as gold units. These are com-
pleted examples which are mixed in with the un-
processed data shown to workers, and used to
evaluate worker performance. The GATE Crowd-
sourcing plugin supports automatic creation of
gold units from GATE annotations having a fea-
ture correct. The value of that feature is then
taken to be the answer expected from the human
annotator. Gold units need to be 10%–30% of the
units to be annotated. The minimum performance
threshold for workers can be set in the job config-
uration.
</bodyText>
<subsectionHeader confidence="0.919063">
3.5 Automatic data import from
CrowdFlower and adjudication
</subsectionHeader>
<bodyText confidence="0.946975">
On completion, the plugin automatically imports
collected multiple judgements back into GATE
</bodyText>
<page confidence="0.997588">
99
</page>
<figureCaption confidence="0.998719">
Figure 4: CrowdFlower Judgements in GATE
</figureCaption>
<bodyText confidence="0.984912388888889">
and the original documents are enriched with the
crowdsourced information, modelled as multiple
annotations (one per contributor). Figure 4 shows
judgements that have been imported from Crowd-
Flower and stored as annotations on the original
document. One useful feature is the trust metric,
assigned by CrowdFlower for this judgement.
GATE’s existing tools for calculating inter-
annotator agreement and for corpus analysis are
used to gain further insights into the quality of the
collected information. If manual adjudication is
required, GATE’s existing annotations stack edi-
tor is used to show in parallel the annotations im-
ported from CrowdFlower, so that differences in
judgement can easily be seen and resolved. Alter-
natively, automatic adjudication via majority vote
or other more sophisticated strategies can be im-
plemented in GATE as components.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999906315789474">
This paper described the GATE Crowdsourcing
plugin2 and the reusable components that it pro-
vides for automatic mapping of corpora to micro-
tasks and vice versa, as well as the generic se-
quence selection and classification user interfaces.
These are easily configurable for a wide range
of NLP corpus annotation tasks and, as part of
this demonstration, several example crowdsourc-
ing projects will be shown.
Future work will focus on expanding the num-
ber of reusable components, the implementation
of reusable automatic adjudication algorithms,
and providing support for crowdsourcing through
games-with-a-purpose (GWAPs).
Acknowledgments This was part of the uComp
project (www.ucomp.eu). uComp receives the
funding support of EPSRC EP/K017896/1, FWF
1097-N23, and ANR-12-CHRI-0003-03, in the
framework of the CHIST-ERA ERA-NET.
</bodyText>
<footnote confidence="0.997674">
2It is available to download from http://gate.ac.uk/ .
</footnote>
<sectionHeader confidence="0.997023" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842976744186">
Kalina Bontcheva, Hamish Cunningham, Ian Roberts,
Angus. Roberts, Valentin. Tablan, Niraj Aswani, and
Genevieve Gorrell. 2013. GATE Teamware: A
Web-based, Collaborative Text Annotation Frame-
work. Language Resources and Evaluation,
47:1007—1029.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon’s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 1–12.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an Architecture for Development of Robust HLT
Applications. In Proceedings of the 40th An-
nual Meeting on Association for Computational
Linguistics, 7–12 July 2002, ACL ’02, pages
168–175, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Karen Fort, Gilles Adda, and K. Bretonnel Cohen.
2011. Amazon mechanical turk: Gold mine or coal
mine? Computational Linguistics, 37(2):413 –420.
Eduard Hovy. 2010. Annotation. In Tutorial Abstracts
ofACL.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES:
An XML-based Standard for Linguistic Corpora.
In Proceedings of the second International Confer-
ence on Language Resources and Evaluation (LREC
2000), 30 May – 2 Jun 2000, pages 825–830,
Athens, Greece.
Shashank Khanna, Aishwarya Ratan, James Davis, and
William Thies. 2010. Evaluating and improving the
usability of Mechanical Turk for low-income work-
ers in India. In Proceedings of the first ACM sympo-
sium on computing for development. ACM.
Manfred Stede and Chu-Ren Huang. 2012. Inter-
operability and reusability: the science of annota-
tion. Language Resources and Evaluation, 46:91–
94. 10.1007/s10579-011-9164-x.
A. Wang, C.D.V. Hoang, and M. Y. Kan. 2012. Per-
spectives on Crowdsourcing Annotations for Natu-
ral Language Processing. Language Resources and
Evaluation, Mar:1–23.
</reference>
<page confidence="0.990748">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845691">
<title confidence="0.951989">The GATE Crowdsourcing Plugin: Crowdsourcing Annotated Corpora Made Easy</title>
<author confidence="0.997829">Kalina Bontcheva</author>
<author confidence="0.997829">Ian Roberts</author>
<author confidence="0.997829">Leon Derczynski</author>
<author confidence="0.997829">Dominic</author>
<affiliation confidence="0.999263">University of Sheffield</affiliation>
<abstract confidence="0.9967551">Crowdsourcing is an increasingly popular, collaborative approach for acquiring annotated corpora. Despite this, reuse of corpus conversion tools and user interfaces between projects is still problematic, since these are not generally made available. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural support for mapping documents to crowdsourcing units and back, as well as automatically generating reusable crowdsourcing interfaces for NLP classification and selection tasks. The entire workflow will be demonstrated on: annotating named entities; disambiguating words and named entities with respect to DBpedia URIs; annotation of opinion holders and targets; and sentiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Valentin Tablan Roberts</author>
<author>Niraj Aswani</author>
<author>Genevieve Gorrell</author>
</authors>
<title>GATE Teamware: A Web-based, Collaborative Text Annotation Framework. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>47--1007</pages>
<marker>Roberts, Aswani, Gorrell, 2013</marker>
<rawString>Kalina Bontcheva, Hamish Cunningham, Ian Roberts, Angus. Roberts, Valentin. Tablan, Niraj Aswani, and Genevieve Gorrell. 2013. GATE Teamware: A Web-based, Collaborative Text Annotation Framework. Language Resources and Evaluation, 47:1007—1029.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="1347" citStr="Callison-Burch and Dredze, 2010" startWordPosition="185" endWordPosition="188">ication and selection tasks. The entire workflow will be demonstrated on: annotating named entities; disambiguating words and named entities with respect to DBpedia URIs; annotation of opinion holders and targets; and sentiment. 1 Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single annotation task remains quite substantial. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural sup</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: an Architecture for Development of Robust HLT Applications.</title>
<date>2002</date>
<journal>ACL</journal>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, 7–12</booktitle>
<volume>02</volume>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6719" citStr="Cunningham et al., 2002" startWordPosition="990" endWordPosition="994">tions are grouped in separate annotation sets: one for the automatically pre-annotated annotations, one for the crowdsourced judgements, and a consensus set, which can be considered as the final resulting corpus annotation layer. In this way, provenance is fully tracked, which makes it possible to experiment with methods that consider more than one answer as potentially correct. 3.2 Automatic data mapping to CrowdFlower The plugin expects documents to be presegmented into paragraphs, sentences and word tokens, using a tokeniser, POS tagger, and sentence splitter – e.g. those built in to GATE (Cunningham et al., 2002). The GATE Crowdsourcing plugin allows choice between these of which to use as the crowdsourcing task unit; e.g., to show one sentence per unit or one paragraph. In the demonstration we will show both automatic mapping at sentence level (for named entity annotation) and at paragraph level (for named entity disambiguation). 3.3 Automatic user interface generation The User Interfaces (UIs) applicable to various task types tend to fall into a set of categories, the most commonly used being categorisation, selection, and text input. The GATE Crowdsourcing plugin provides generalised and re-usable,</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: an Architecture for Development of Robust HLT Applications. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, 7–12 July 2002, ACL ’02, pages 168–175, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Fort</author>
<author>Gilles Adda</author>
<author>K Bretonnel Cohen</author>
</authors>
<title>Amazon mechanical turk: Gold mine or coal mine?</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<pages>420</pages>
<contexts>
<context position="1366" citStr="Fort et al., 2011" startWordPosition="189" endWordPosition="192">entire workflow will be demonstrated on: annotating named entities; disambiguating words and named entities with respect to DBpedia URIs; annotation of opinion holders and targets; and sentiment. 1 Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single annotation task remains quite substantial. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural support for mapping do</context>
</contexts>
<marker>Fort, Adda, Cohen, 2011</marker>
<rawString>Karen Fort, Gilles Adda, and K. Bretonnel Cohen. 2011. Amazon mechanical turk: Gold mine or coal mine? Computational Linguistics, 37(2):413 –420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<date>2010</date>
<booktitle>Annotation. In Tutorial Abstracts ofACL.</booktitle>
<contexts>
<context position="990" citStr="Hovy, 2010" startWordPosition="131" endWordPosition="132">projects is still problematic, since these are not generally made available. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural support for mapping documents to crowdsourcing units and back, as well as automatically generating reusable crowdsourcing interfaces for NLP classification and selection tasks. The entire workflow will be demonstrated on: annotating named entities; disambiguating words and named entities with respect to DBpedia URIs; annotation of opinion holders and targets; and sentiment. 1 Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since t</context>
</contexts>
<marker>Hovy, 2010</marker>
<rawString>Eduard Hovy. 2010. Annotation. In Tutorial Abstracts ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>P Bonhomme</author>
<author>L Romary</author>
</authors>
<title>XCES: An XML-based Standard for Linguistic Corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the second International Conference on Language Resources and Evaluation (LREC 2000), 30 May –</booktitle>
<volume>2</volume>
<pages>825--830</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="6050" citStr="Ide et al., 2000" startWordPosition="883" endWordPosition="886">dress these NLP-specific requirements, we implemented a generic, open-source GATE Crowdsourcing plugin, which makes it very easy to set up and conduct crowdsourcing-based corpus annotation from within GATE’s visual interface. 3.1 Physical representation for documents and annotations Documents and their annotations are encoded in the GATE stand-off XML format (Cunningham Figure 1: Classification UI Configuration et al., 2002), which was chosen for its support for overlapping annotations and the wide range of automatic pre-processing tools available. GATE also has support for the XCES standard (Ide et al., 2000) and others (e.g. CoNLL) if preferred. Annotations are grouped in separate annotation sets: one for the automatically pre-annotated annotations, one for the crowdsourced judgements, and a consensus set, which can be considered as the final resulting corpus annotation layer. In this way, provenance is fully tracked, which makes it possible to experiment with methods that consider more than one answer as potentially correct. 3.2 Automatic data mapping to CrowdFlower The plugin expects documents to be presegmented into paragraphs, sentences and word tokens, using a tokeniser, POS tagger, and sent</context>
</contexts>
<marker>Ide, Bonhomme, Romary, 2000</marker>
<rawString>N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-based Standard for Linguistic Corpora. In Proceedings of the second International Conference on Language Resources and Evaluation (LREC 2000), 30 May – 2 Jun 2000, pages 825–830, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shashank Khanna</author>
<author>Aishwarya Ratan</author>
<author>James Davis</author>
<author>William Thies</author>
</authors>
<title>Evaluating and improving the usability of Mechanical Turk for low-income workers in India.</title>
<date>2010</date>
<booktitle>In Proceedings of the first ACM symposium on computing for development.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="1697" citStr="Khanna et al., 2010" startWordPosition="239" endWordPosition="242"> al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single annotation task remains quite substantial. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural support for mapping documents to crowdsourcing units, as well as automatically generated, reusable user interfaces1 for NLP classification and selection tasks. Their use will be demonstrated on annotating named entities (selection task), disambiguating words and named entities with respect to DBpedia URIs (classification task), annotation of opinion h</context>
</contexts>
<marker>Khanna, Ratan, Davis, Thies, 2010</marker>
<rawString>Shashank Khanna, Aishwarya Ratan, James Davis, and William Thies. 2010. Evaluating and improving the usability of Mechanical Turk for low-income workers in India. In Proceedings of the first ACM symposium on computing for development. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
<author>Chu-Ren Huang</author>
</authors>
<title>Interoperability and reusability: the science of annotation.</title>
<date>2012</date>
<journal>Language Resources and Evaluation,</journal>
<volume>46</volume>
<pages>10--1007</pages>
<contexts>
<context position="1014" citStr="Stede and Huang, 2012" startWordPosition="133" endWordPosition="136">still problematic, since these are not generally made available. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural support for mapping documents to crowdsourcing units and back, as well as automatically generating reusable crowdsourcing interfaces for NLP classification and selection tasks. The entire workflow will be demonstrated on: annotating named entities; disambiguating words and named entities with respect to DBpedia URIs; annotation of opinion holders and targets; and sentiment. 1 Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not m</context>
</contexts>
<marker>Stede, Huang, 2012</marker>
<rawString>Manfred Stede and Chu-Ren Huang. 2012. Interoperability and reusability: the science of annotation. Language Resources and Evaluation, 46:91– 94. 10.1007/s10579-011-9164-x.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wang</author>
<author>C D V Hoang</author>
<author>M Y Kan</author>
</authors>
<title>Perspectives on Crowdsourcing Annotations for Natural Language Processing. Language Resources and Evaluation,</title>
<date>2012</date>
<location>Mar:1–23.</location>
<contexts>
<context position="1386" citStr="Wang et al., 2012" startWordPosition="193" endWordPosition="196">l be demonstrated on: annotating named entities; disambiguating words and named entities with respect to DBpedia URIs; annotation of opinion holders and targets; and sentiment. 1 Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single annotation task remains quite substantial. This demonstration will introduce the new, open-source GATE Crowdsourcing plugin, which offers infrastructural support for mapping documents to crowdsour</context>
</contexts>
<marker>Wang, Hoang, Kan, 2012</marker>
<rawString>A. Wang, C.D.V. Hoang, and M. Y. Kan. 2012. Perspectives on Crowdsourcing Annotations for Natural Language Processing. Language Resources and Evaluation, Mar:1–23.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>