<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.990255">
Cross-Cutting Models of Lexical Semantics
</title>
<author confidence="0.997875">
Joseph Reisinger
</author>
<affiliation confidence="0.998625">
Department of Computer Sciences
The University of Texas at Austin
</affiliation>
<address confidence="0.915229">
Austin, TX 78712
</address>
<email confidence="0.999146">
joeraii@cs.utexas.edu
</email>
<author confidence="0.996031">
Raymond Mooney
</author>
<affiliation confidence="0.9985605">
Department of Computer Sciences
The University of Texas at Austin
</affiliation>
<address confidence="0.915307">
Austin, TX 78712
</address>
<email confidence="0.999357">
mooney@cs.utexas.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99936525">
Context-dependent word similarity can be
measured over multiple cross-cutting dimen-
sions. For example, lung and breath are sim-
ilar thematically, while authoritative and su-
perficial occur in similar syntactic contexts,
but share little semantic similarity. Both of
these notions of similarity play a role in deter-
mining word meaning, and hence lexical se-
mantic models must take them both into ac-
count. Towards this end, we develop a novel
model, Multi-View Mixture (MVM), that rep-
resents words as multiple overlapping clus-
terings. MVM finds multiple data partitions
based on different subsets of features, sub-
ject to the marginal constraint that feature sub-
sets are distributed according to Latent Dirich-
let Allocation. Intuitively, this constraint fa-
vors feature partitions that have coherent top-
ical semantics. Furthermore, MVM uses soft
feature assignment, hence the contribution of
each data point to each clustering view is vari-
able, isolating the impact of data only to views
where they assign the most features. Through
a series of experiments, we demonstrate the
utility of MVM as an inductive bias for captur-
ing relations between words that are intuitive
to humans, outperforming related models such
as Latent Dirichlet Allocation.
</bodyText>
<sectionHeader confidence="0.999328" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998895833333333">
Humans categorize objects using multiple orthogo-
nal taxonomic systems, where category generaliza-
tion depends critically on what features are relevant
to one particular system. For example, foods can be
organized in terms of their nutritional value (high in
fiber) or situationally (commonly eaten for Thanks-
</bodyText>
<page confidence="0.883037">
1405
</page>
<bodyText confidence="0.996641138888889">
giving; Shafto et al. (2006)). Human knowledge-
bases such as Wikipedia also exhibit such multiple
clustering structure (e.g. people are organized by oc-
cupation or by nationality). The effects of these
overlapping categorization systems manifest them-
selves at the lexical semantic level (Murphy, 2002),
implying that lexicographical word senses and tra-
ditional computational models of word-sense based
on clustering or exemplar activation are too impov-
erished to capture the rich dynamics of word usage.
In this work, we introduce a novel probabilis-
tic clustering method, Multi-View Mixture (MVM),
based on cross-cutting categorization (Shafto et al.,
2006) that generalizes traditional vector-space or
distributional models of lexical semantics (Curran,
2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Tur-
ney, 2006). Cross-cutting categorization finds multi-
ple feature subsets (categorization systems) that pro-
duce high quality clusterings of the data. For exam-
ple words might be clustered based on their part of
speech, or based on their thematic usage. Context-
dependent variation in word usage can be accounted
for by leveraging multiple latent categorization sys-
tems. In particular, cross-cutting models can be used
to capture both syntagmatic and paradigmatic no-
tions of word relatedness, breaking up word features
into multiple categorization systems and then com-
puting similarity separately for each system.
MVM leverages primitives from Dirichlet-Process
Mixture Models (DPMMs) and Latent Dirichlet Al-
location (LDA). Each clustering (view) in MVM con-
sists of a distribution over features and data and
views are further subdivided into clusters based on a
DPMM. View marginal distributions are determined
by LDA, allowing data features to be distributed over
multiple views, explaining subsets of features.
</bodyText>
<note confidence="0.9484515">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1405–1415,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999861428571428">
We evaluate MVM against several other model-
based clustering procedures in a series of human
evaluation tasks, measuring its ability to find mean-
ingful syntagmatic and paradigmatic structure. We
find that MVM finds more semantically and syntac-
tically coherent fine-grained structure, using both
common and rare n-gram contexts.
</bodyText>
<sectionHeader confidence="0.9387265" genericHeader="method">
2 Mixture Modeling and Lexical
Semantics
</sectionHeader>
<bodyText confidence="0.999975147058824">
Distributional, or vector space methods attempt to
model word meaning by embedding words in a com-
mon metric space, whose dimensions are derived
from, e.g., word collocations (Sch¨utze, 1998), syn-
tactic relations (Pad´o and Lapata, 2007), or latent
semantic spaces (Finkelstein et al., 2001; Landauer
and Dumais, 1997; Turian et al., 2010). The distribu-
tional hypothesis addresses the problem of modeling
word similarity (Curran, 2004; Miller and Charles,
1991; Sch¨utze, 1998; Turney, 2006), and can be ex-
tended to selectional preference (Resnik, 1997) and
lexical substitution (McCarthy and Navigli, 2007) as
well. Such methods are highly scalable (Gorman
and Curran, 2006) and have been applied in infor-
mation retrieval (Manning et al., 2008), large-scale
taxonomy induction (Snow et al., 2006), and knowl-
edge acquisition (Van Durme and Pas¸ca, 2008).
Vector space models fail to capture the richness
of word meaning since similarity is not a globally
consistent metric. It violates, e.g., the triangle in-
equality: the sum of distances from bat to club and
club to association is less than the distance from bat
to association (Griffiths et al., 2007; Tversky and
Gati, 1982).1 Erk (2007) circumvents this problem
by representing words as multiple exemplars derived
directly from word occurrences and embedded in a
common vector space to capture context-dependent
usage. Likewise Reisinger and Mooney (2010) take
a similar approach using mixture modeling com-
bined with a background variation model to generate
multiple prototype vectors for polysemous words.
Both of these approaches still ultimately embed
all words in a single metric space and hence argue
for globally consistent metrics that capture human
</bodyText>
<footnote confidence="0.876771">
1Similarity also has been shown to violate symmetry (e.g.
people have the intuition that China is more similar to North
Korea than North Korea is to China).
</footnote>
<bodyText confidence="0.999764260869565">
intuitive notions of “similarity.” Rather than assum-
ing a global metric embedding exists, in this work
we simply leverage the cluster assumption, e.g. that
similar words should appear in the same clusters, in
particular extending it to multiple clusterings. The
cluster assumption is a natural fit for lexical seman-
tics, as partitions can account for metric violations.
The end result is a model capable of representing
multiple, overlapping similarity metrics that result
in disparate valid clusterings leveraging the
Subspace Hypothesis: For any pair of
words, the set of “active” features govern-
ing their apparent similarity differs. For
example wine and bottle are similar and
wine and vinegar are similar, but it would
not be reasonable to expect that the fea-
tures governing such similarity computa-
tions to overlap much, despite occurring
in similar documents.
MVM can extract multiple competing notions of sim-
ilarity, for example both paradigmatic, or thematic
similarity, and syntagmatic or syntactic similarity, in
addition to more fine grained relations.
</bodyText>
<sectionHeader confidence="0.989159" genericHeader="method">
3 Multi-View Clustering with MVM
</sectionHeader>
<bodyText confidence="0.999994904761905">
As feature dimensionality increases, the number of
ways the data can exhibit interesting structure goes
up exponentially. Clustering is commonly used to
explain data, but often there are several equally
valid, competing clusterings, keying off of different
subsets of features, especially in high-dimensional
settings such as text mining (Niu et al., 2010). For
example, company websites can be clustered by sec-
tor or by geographic location, with one particular
clustering becoming predominant when a majority
of features correlate with it. In fact, informative fea-
tures in one clustering may be noise in another, e.g.
the occurrence of CEO is not necessarily discrimi-
native when clustering companies by industry sec-
tor, but may be useful in other clusterings. Multi-
ple clustering is one approach to inferring feature
subspaces that lead to high quality data partitions.
Multiple clustering also improves the flexibility of
generative clustering models, as a single model is
no longer required to explain all the variance in the
feature dimensions (Mansinghka et al., 2009).
</bodyText>
<page confidence="0.988676">
1406
</page>
<table confidence="0.5263218125">
and is ___ and are ___
we are ___ which was ___
he is ___ who are ___
unwilling
willing
reluctant
refusing
glad
brand new ___ results for ___
selection of ___ the latest ___
___ for sale to buy ___
samsung toyota dunlop
panasonic nissan yokohama
toshiba mercedes toyo
sony volvo uniroyal
epson audi michelin
</table>
<figureCaption confidence="0.993521">
Figure 1: Example clusterings from MVM applied to
</figureCaption>
<bodyText confidence="0.940208866666666">
Google n-gram data. Top contexts (features) for each
view are shown, along with examples of word clusters.
Although these particular examples are interpretable, in
general the relationship captured by the view’s context
subspace is not easily summarized.
MVM is a multinomial-Dirichlet multiple clus-
tering procedure for distributional lexical seman-
tics that fits multiple, overlapping Dirichlet Process
Mixture Models (DPMM) to a set of word data. Fea-
tures are distributed across the set of clusterings
(views) using LDA, and each DPMM is fit using a
subset of the features. This reduces clustering noise
and allows MVM to capture multiple ways in which
the data can be partitioned. Figure 1 shows a sim-
ple example, and Figure 2 shows a larger sample of
feature-view assignments from a 3-view MVM fit to
contexts drawn from the Google n-gram corpus.
We implement MVM using generative model
primitives drawn from Latent Dirichlet Allocation
(LDA) and the Dirichlet Process (DP). ⑤M⑤ disparate
clusterings (views) are inferred jointly from a set of
data D ✏ twd⑤d P r1 ... Ds✉. Each data vector
wd is associated with a probability distribution over
views θ⑤M⑤
d . Empirically, θ⑤M⑤
d is represented as a
set of feature-view assignments zd, sampled via the
standard LDA collapsed Gibbs sampler. Hence, each
view maintains a separate distribution over features.
The generative model for feature-view assignment is
</bodyText>
<equation confidence="0.865228">
given by
θ⑤M⑤ d⑤α ✒ Dirichlet♣αq, d P D,
φm⑤β ✒ Dirichlet♣βq, m P ⑤M⑤,
zdn⑤θd ✒ Discrete♣θdq, n P ⑤wd⑤,
wdn⑤φzdnm ✒Discrete♣φzdnmq, n P ⑤wd⑤,
</equation>
<bodyText confidence="0.991269133333333">
where α and β are hyperparameters smoothing the
per-document topic distributions and per-topic word
distributions respectively.
Conditional on the feature-view assignment tz✉,
a clustering is inferred for each view using the Chi-
nese Restaurant Process representation of the DP.
The clustering probability is given by
p♣c⑤z,wq ✾ p♣tcm✉, z, wq
p♣wrz✏ms
d ⑤cm, zqp♣cm⑤zq.
where p♣cm⑤zq is a prior on the clustering for view
m, i.e. the DPMM, and p♣wrz✏ms
d ⑤cm, zq is the like-
lihood of the clustering cm given the data point wd
restricted to the features assigned to view m:
</bodyText>
<equation confidence="0.82736">
rz✏ms def
wd ✏ twid⑤zid ✏ m✉.
</equation>
<bodyText confidence="0.99986148">
Thus, we treat the m clusterings cm as conditionally
independent given the feature-view assignments.
The feature-view assignments tz✉ act as a set of
marginal constraints on the multiple clusterings, and
the impact that each data point can have on each
clustering is limited by the number of features as-
signed to it. For example, in a two-view model,
zid ✏ 1 might be set for all syntactic features (yield-
ing a syntagmatic clustering) while zid ✏ 2 is set for
document features (paradigmatic clustering).
By allowing the clustering model capacity to vary
via the DPMM, MVM can naturally account for the
semantic variance of the view. This provides a novel
mechanism for handling feature noise: noisy fea-
tures can be assigned to a separate view with poten-
tially a small number of clusters. This phenomenon
is apparent in cluster 1, view 1 in the example in
figure 2, where place names and adjectives are clus-
tered together using rare contexts
From a topic modeling perspective, MVM finds
topic refinements within each view, similar to hier-
archical methods such as the nested Chinese Restau-
rant Process (Blei et al., 2003). The main differ-
ence is that the features assigned to the second “re-
fined topics” level are constrained by the higher
</bodyText>
<figure confidence="0.988547681528662">
exceedingly
sincerely
logically
justly
appropriately
about
because
⑤ D ⑤
➵
d✏1
M m1-1
1407
___
private message to ___
___
the city of
presence of ___
posted by ___ at
___
going to ___
from the ___ to
___
of — may
degree of ___
___
open this result in
high school
along the ___
home page
way of =
___
by to
written by ___
and was ___
___
the very —
___
___
___
___
in an
wasborn
___
by on
city of
were ___ in
hotels in
name of
born in
who is
could be
of human
___
as as
the ___ must be
___
are to
also ___ the
like ___ and
hotels ___ hotels
___
dsl dsl
was to
who had
___
of a and
we are ___
is also —___
near the ___
___
of being ___
posts by —
___
many ___ and
___
and an —
was the ___ of
town of ___
located in ___
___
an ___ and
the more ___
the american ___
___
he is —
in these ___
___
been ___ and
estate in ___
but the ___ of
___
the little —
of ___ from the
which ___ the
___
i was =
___
is an —
a kind of ___
___
welcome to ___
be ___ or
___
side of the ___
first ___ of
create a ___
___
and is —
___
the of that
do not ___
___
and their
that was ___
___
of have
___
real estate in ___
___
she was —
so many —
and are ___
who are —
___
were not ___
___
his of
___
that are
___
___
in the
you are ___
might be
___
the does not
___
a more ___
some of
the ___ family
to an
___
to be ___ and
be ___ to
___
said that
___
to a
___
message to ___
—
___
which was ___
___
and his
___
of the ___ were
</figure>
<figureCaption confidence="0.991826">
Figure 2: Topics with Senses: Shows top 20% of features for each view in a 3-view MVM fit to Google n-gram context
data; different views place different mass on different sets of features. Cluster groupings within each view are shown.
</figureCaption>
<bodyText confidence="0.783864">
View 1 cluster 2 and View 3 cluster 1 both contain past-tense verbs, but only overlap on a subset of syntactic features.
</bodyText>
<page confidence="0.887734">
1408
</page>
<figure confidence="0.999687472440945">
Cluster 1
Cluster 2
Cluster 1
Cluster 1
Cluster 2
arbitrary
austin
baltimore
characteristic
comparative
dallas
evolutionary
franklin
fundamental
inadequate
inferior
integral
jackson
kent
likelihood
liverpool
mystical
newcastle
pittsburgh
poetic
proportional
psychological
radical
richmond
singular
betrayed
conquered
disappointed
divorced
embarked
frustrated
guarded
hated
knocked
murdered
praised
stationed
stole
summoned
wounded
secretly
arbitrary
betrayed
characteristic
conquered
disappointed
divorced
embarked
evolutionary
examine
franklin
frustrated
fundamental
guarded
hated
inadequate
inferior
integral
jackson
knocked
likelihood
murdered
mystictal
ppraised
proportional
radical
secretly
singular
stationed
stole
summoned
systematic
wounded
kent
lc
te rpool
manr
newcastle
austin
baltimore
charlotte
dallas
pittsburgh
richmond
austin
betrayed
charlotte
conquered
disappointed
divorced
embarked
frustrated
guarded
hated
jackson
kent
mknourderedcked
newcastle
praised
richmond
secretly
stationed
stole
summoned
wounded
arbitrary
characteristic
comparative
evolutionary
fundamental
inadequate
inferior
integral
mystical
poetic
psychological
radical
singular
systematic
View 1
View 2
View 3
</figure>
<bodyText confidence="0.849254433333333">
level, similar to hierarchical clustering. Unlike hi- Given such word representation data, MVM gener-
erarchical clustering, however, the top level top- ates a fixed set of M context views corresponding to
ics/views form an admixture, allowing individual dominant eigenvectors in local syntactic or seman-
features from a single data point to be assigned to tic space. Within each view, MVM partitions words
multiple views. into clusters based on each word’s local representa-
The most similar model to ours is Cross-cutting tion in that view; that is, based on the set of con-
categorization (CCC), which fits multiple DPMMs to text features it allocates to the view. Words have a
non-overlapping partitions of features (Mansinghka non-uniform affinity for each view, and hence may
et al., 2009; Shafto et al., 2006). Unlike MVM, not be present in every clustering (Figure 2). This
CCC partitions features among multiple DPMMs, is important as different ways of drawing distinc-
hence all occurrences of a particular feature will tions between words do not necessarily apply to all
end up in a single clustering, instead of assigning words. In contrast, LDA finds locally consistent col-
them softly using LDA. Such hard feature partition- lections of contexts but does not further subdivide
ing does not admit an efficient sampling procedure, words into clusters given that set of contexts. Hence,
and hence Shafto et al. (2006) rely on Metropolis- it may miss more fine-grained structure, even with
Hastings steps to perform feature assignment, mak- increased model complexity.
ing the model less scalable. 4 Experimental Setup
3.1 Word Representation 4.1 Corpora
MVM is trained as a lexical semantic model on We derive word features from three corpora: (1) the
Web-scale n-gram and semantic context data. N- English Google Web n-gram corpus, containing n-
gram contexts are drawn from a combination of the gram contexts up to 5-gram that occur more than 40
Google n-gram and Google books n-gram corpora, times in a 1T word corpus of Web text, (2) the En-
with the head word removed: e.g. for the term ar- glish Google Books n-gram corpus2, consisting of
chitect, we collect contexts such as the of the n-gram contexts up to 5-gram that occur more than
house, an is a, and the of the universe. Se- 40 times in a 500B word corpus of books, and (3) a
mantic contexts are derived from word occurrence snapshot of the English Wikipedia3 taken on Octo-
in Wikipedia documents: each document a word ap- ber 11, 2010 containing over 3M articles.
pears in is added as a potential feature for that word. MVM is trained on a sample of 20k English words
This co-occurrence matrix is the transpose of the drawn uniformly at random from the top 200k En-
standard bag-of-words document representation. glish terms appearing in Wikipedia (different parts
</bodyText>
<table confidence="0.946547833333333">
In this paper we focus on two representations: of speech were sampled from the Google n-gram
1. Syntax-only – Words are represented as bags corpus according to their observed frequency). Two
of ngram contexts derived slot-filling procedure versions of the syntax-only dataset are created from
described above. different subsets of the Google n-gram corpora: (1)
2. Syntax+Documents – The syntax-only repre- the common subset contains all syntactic contexts
sentation is augmented with additional docu- appearing more than 200 times in the combined cor-
ment contexts drawn from Wikipedia. pus, and (2) the rare subset, containing only contexts
Models trained on the syntax-only set are only ca- that appear 50 times or fewer.
pable of capturing syntagmatic similarity relations, 4.2 Human Evaluation
that is, words that tend to appear in similar contexts. Our main goal in this work is to find models that
In contrast, the syntax+documents set broadens the capture aspects of the syntactic and semantic orga-
scope of modelable similarity relations, allowing for nization of word in text that are intuitive to humans.
paradigmatic similarity (e.g. words that are topically
related, but do not necessarily share common syntac-
tic contexts).
1409
2http://ngrams.googlelabs.com/datasets
3http://wikipedia.org
Word Intrusion
country to metal dues humor
or less floral premiums ingenuity
a year nylon pensions advertisers
per day what did delight
or more ruby damages astonishment
Context Intrusion
is characterized top of the
symptoms of of understood
cases of along the
in cases of portion of the
real estate in side of the
Document Intrusion
Puerto Rican cuisine Adolf Hitler History of the Han Dynasty
Greek cuisine List of General Hospital characters Romance of the Three Kingdoms
ThinkPad History of France List of dog diseases
Palestinian cuisine Joachim von Ribbentrop Conquest of Wu by Jin
Field ration World War I Mongolia
</table>
<tableCaption confidence="0.9957595">
Table 1: Example questions from the three intrusion tasks, in order of difficulty (left to right, easy to hard; computed
from inter-annotator agreement). Italics show intruder items.
</tableCaption>
<bodyText confidence="0.999924148148148">
According to the use theory of meaning, lexical se-
mantic knowledge is equivalent to knowing the con-
texts that words appear in, and hence being able to
form reasonable hypotheses about the relatedness of
syntactic contexts.
Vector space models are commonly evaluated by
comparing their similarity predictions to a nom-
inal set of human similarity judgments (Curran,
2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Tur-
ney, 2006). In this work, since we are evaluating
models that potentially yield many different simi-
larity scores, we take a different approach, scoring
clusters on their semantic and syntactic coherence
using a set intrusion task (Chang et al., 2009).
In set intrusion, human raters are shown a set of
options from a coherent group and asked to identify
a single intruder drawn from a different group. We
extend intrusion to three different lexical semantic
tasks: (1) context intrusion, where the top contexts
from each cluster are used, (3) document intrusion,
where the top document contexts from each clus-
ter are used, and (2) word intrusion, where the top
words from each cluster are used. For each clus-
ter, the top four contexts/words are selected and ap-
pended with another context/word from a different
cluster.4 The resulting set is then shuffled, and the
human raters are asked to identify the intruder, af-
</bodyText>
<footnote confidence="0.707985">
4Choosing four elements from the cluster uniformly at ran-
dom instead of the top by probability led to lower performance
across all models.
</footnote>
<bodyText confidence="0.99998425">
ter being given a short introduction (with common
examples) to the task. Table 1 shows sample ques-
tions of varying degrees of difficulty. As the seman-
tic coherence and distinctness from other clusters in-
creases, this task becomes easier.
Set intrusion is a more robust way to account for
human similarity judgments than asking directly for
a numeric score (e.g., the Miller and Charles (1991)
set) as less calibration is required across raters. Fur-
thermore, the additional cluster context significantly
reduces the variability of responses.
Human raters were recruited from Amazon’s Me-
chanical Turk. A total of 1256 raters completed
30438 evaluations for 5780 unique intrusion tasks
(5 evaluations per task). 2736 potentially fraudulent
evaluations from 11 raters were rejected.5 Table 3
summarizes inter-annotator agreement. Overall we
found n ti 0.4 for most tasks; a set of comments
about the task difficulty is given in Table 2, drawn
from an anonymous public message board.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9991565">
We trained DPMM, LDA and MVM models
on the syntax-only and syntax+documents
data across a wide range of settings for M E
13, 5, 7, 10, 20, 30, 50,100, 200, 300, 500, 1000✉,6
</bodyText>
<footnote confidence="0.8232048">
5(Rater Quality) Fraudulent Turkers were identified using
a combination of average answer time, answer entropy, average
agreement with other raters, and adjusted answer accuracy.
6LDA is run on a different range of M settings from MVM
(50-1000 vs 3-100) in order to keep the effective number of
</footnote>
<page confidence="0.978897">
1410
</page>
<figure confidence="0.999715075471698">
context intrusion
document intrusion
word intrusion
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
context intrusion
word intrusion
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
●
●
●
● ●
● ● ● ●
● ● ●
● ●
●
% correct
(a) Syntax-only, common n-gram contexts.
context intrusion
word intrusion
●
● ●
●
●
●
●
●
●
●
● ● ●
●
● ●
●
●
●
● ●
●
●
● ●
●
● ●
● ●
● ●
●
● ●
●
●
●
●
% correct
(c) Syntax+Documents, common n-gram contexts.
</figure>
<figureCaption confidence="0.999619">
Figure 3: Average scores for each model broken down by parameterization and data source. Error bars depict 95%
confidence intervals. X-axis labels show Model-views-α-β. Dots show average rater scores; bar-charts show standard
</figureCaption>
<bodyText confidence="0.83473">
quantile ranges and median score.
</bodyText>
<page confidence="0.966476">
1411
</page>
<figure confidence="0.998763927536231">
DPMM−0.1−0.1
DPMM−0.1−0.01
LDA−50M−0.1−0.1
LDA−50M−0.1−0.01
LDA−100M−0.1−0.1
LDA−100M−0.1−0.01
LDA−200M−0.1−0.1
LDA−200M−0.1−0.01
LDA−300M−0.1−0.1
LDA−300M−0.1−0.01
LDA−500M−0.1−0.1
LDA−500M−0.1−0.01
LDA−1000M−0.1−0.1
LDA−1000M−0.1−0.01
MVM−3M−0.1−0.01
MVM−5M−0.1−0.01
MVM−5M−0.1−0.005
MVM−10M−0.1−0.01
MVM−10M−0.1−0.005
MVM−20M−0.1−0.01
MVM−30M−0.1−0.01
MVM−50M−0.1−0.01
MVM−100M−0.1−0.01
DPMM−0.1−0.1
DPMM−0.1−0.01
LDA−50M−0.1−0.1
LDA−50M−0.1−0.01
LDA−100M−0.1−0.1
LDA−100M−0.1−0.01
LDA−200M−0.1−0.1
LDA−200M−0.1−0.01
LDA−300M−0.1−0.1
LDA−300M−0.1−0.01
LDA−500M−0.1−0.1
LDA−500M−0.1−0.01
LDA−1000M−0.1−0.1
LDA−1000M−0.1−0.01
MVM−3M−0.1−0.01
MVM−5M−0.1−0.01
MVM−5M−0.1−0.005
MVM−10M−0.1−0.01
MVM−10M−0.1−0.005
MVM−20M−0.1−0.01
MVM−30M−0.1−0.01
MVM−50M−0.1−0.01
MVM−100M−0.1−0.01
DPMM−0.1−0.1
DPMM−0.1−0.01
LDA−50M−0.1−0.1
LDA−50M−0.1−0.01
LDA−100M−0.1−0.1
LDA−100M−0.1−0.01
LDA−200M−0.1−0.1
LDA−200M−0.1−0.01
LDA−300M−0.1−0.1
LDA−300M−0.1−0.01
LDA−500M−0.1−0.1
LDA−500M−0.1−0.01
LDA−1000M−0.1−0.1
LDA−1000M−0.1−0.01
MVM−3M−0.1−0.01
MVM−5M−0.1−0.01
MVM−5M−0.1−0.005
MVM−10M−0.1−0.01
MVM−10M−0.1−0.005
MVM−20M−0.1−0.01
MVM−30M−0.1−0.01
MVM−50M−0.1−0.01
MVM−100M−0.1−0.01
</figure>
<tableCaption confidence="0.972397916666667">
U1 I just tried 30 of the what doesn’t belong ones.
They took about 30 seconds each due to think-
ing time so not worth it for me.
U2 I don’t understand the fill in the blank ones to
be honest. I just kinda pick one,since I don’t
know what’s expected lol
U3 Your not filling in the blank just ignore the
blank and think about how the words they show
relate to each other and choose the one that
relates least. Some have just words and no
blanks.
U4 These seem very subjective to mw. i hope
there isn’t definite correct answers because
some of them make me go [emoticon of head-
scratching]
U5 I looked and have no idea. I guess I’m a word
idiot because I don’t see the relation between
the words in the preview HIT - too scared to try
any of these.
U6 I didn’t dive in but I did more than I should have
they were just too easy. Most of them I could
tell what did not belong, some were pretty iffy
though.
Table 2: Sample of comments about the task taken verba-
</tableCaption>
<bodyText confidence="0.928269636363636">
tim from a public Mechanical Turk user message board
(TurkerNation). Overall the raters report the task to be
difficult, but engaging.
α E 10.1, 0.01}, and β E 10.1, 0.05, 0.01} in
order to understand how they perform relatively
on the intrusion tasks and also how sensitive they
are to various parameter settings.7 Models were
run until convergence, defined as no increase in
log-likelihood on the training set for 100 Gibbs
samples. Average runtimes varied from a few hours
to a few days, depending on the number of clusters
or topics. There is little computational overhead
for MVM compared to LDA or DPMM with a similar
number of clusters.
Overall, MVM significantly outperforms both LDA
and DPMM (measured as % of intruders correctly
identified) as the number of clusters increases.
Coarse-grained lexical semantic distinctions are
easy for humans to make, and hence models with
fewer clusters tend to outperform models with more
clusters. Since high granularity predictions are more
clusters (and hence model capacity) roughly comparable.
</bodyText>
<footnote confidence="0.898224333333333">
7We did not compare directly to Cross-cutting categoriza-
tion, as the Metropolis-Hasting steps required that model were
too prohibitively expensive to scale to the Google n-gram data.
</footnote>
<figure confidence="0.572874">
(b) Syntax-only, rare n-gram contexts.
</figure>
<figureCaption confidence="0.9985515">
Figure 4: Scatterplot of model size vs. avg score for MVM
(dashed, purple) and LDA (dotted, orange).
</figureCaption>
<bodyText confidence="0.9980405">
useful for downstream tasks, we focus on the inter-
play between model complexity and performance.
</bodyText>
<subsectionHeader confidence="0.887257">
5.1 Syntax-only Model
</subsectionHeader>
<bodyText confidence="0.99995225">
For common n-gram context features, MVM perfor-
mance is significantly less variable than LDA on both
the word intrusion and context intrusion tasks, and
furthermore significantly outperforms DPMM (Fig-
ure 3(a)). For context intrusion, DPMM, LDA, and
MVM average 57.4%, 49.5% and 64.5% accuracy
respectively; for word intrusion, DPMM, LDA, and
MVM average 66.7%, 66.1% and 73.6% accuracy
respectively (averaged over all parameter settings).
These models vary significantly in the average num-
ber of clusters used: 373.5 for DPMM, 358.3 for LDA
and 639.8 for MVM, i.e. the MVM model is signifi-
</bodyText>
<figure confidence="0.999467705882353">
●
●
●
context intrusion
● ● ● ●
●
● ●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
● ●
●
●
18 2 22
10 . 10 10 . 102.4 102.6 102.8 103 103.2
model size (clusters)
word intrusion
●
●
● ●●
● ● ●
●
● ●
● ●
●
●
● ●
●
● ● ●
●
●
●
●
word intrusion
●
●
context intrusion
●
●
● ● ●
●
●
102 102.5 103
●
model size (clusters)
(a) Syntax-only, common n-gram contexts.
% correct 1.0
% correct 0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
1.0
0.5
0.0
</figure>
<page confidence="0.974369">
1412
</page>
<table confidence="0.9989802">
Model Syntax Syntax+Documents Overall
DPMM 0.30 0.40 0.33
LDA 0.33 0.39 0.35
MVM 0.44 0.49 0.46
Overall 0.37 0.43 0.39
</table>
<tableCaption confidence="0.9307232">
Table 3: Fleiss’ K scores for various model and data com-
binations. Results from MVM have higher K scores than
LDA or DPMM; likewise Syntax+Documents data yields
higher agreement, primarily due to the relative ease of the
document intrusion task.
</tableCaption>
<bodyText confidence="0.999795416666667">
cantly more granular. Figure 4(a) breaks out model
performance by model complexity, demonstrating
that MVM has a significant edge over LDA as model
complexity increases.
For rare n-gram contexts, we obtain similar re-
sults, with MVM scores being less variable across
model parameterizations and complexity (Figure
3(b)). In general, LDA performance degrades faster
as model complexity increases for rare contexts, due
to the increased data sparsity (Figure 4(b)). For
context intrusion, DPMM, LDA, and MVM average
45.9%, 36.1% and 50.9% accuracy respectively;
for word intrusion, DPMM, LDA, and MVM aver-
age 67.4%, 45.6% and 67.9% accuracy; MVM per-
formance does not differ significantly from DPMM,
but both outperform LDA. Average cluster sizes are
more uniform across model types for rare contexts:
384.0 for DPMM, 358.3 for LDA and 391 for MVM.
Human performance on the context intrusion task
is significantly more variable than on the word-
intrusion task, reflecting the additional complexity.
In all models, there is a high correlation between
rater scores and per-cluster likelihood, indicating
that model confidence reflects noise in the data.
</bodyText>
<subsectionHeader confidence="0.816728">
5.2 Syntax+Documents Model
</subsectionHeader>
<bodyText confidence="0.999992">
With the syntax+documents training set, MVM sig-
nificantly outperforms LDA across a wide range of
model settings. MVM also outperforms DPMM for
word and document intrusion. For context intru-
sion, DPMM, LDA, and MVM average 68.0%, 51.3%
and 66.9% respectively;8 for word intrusion, DPMM,
LDA, and MVM average 56.3%, 64.0% and 74.9%
respectively; for document intrusion, DPMM, LDA,
</bodyText>
<footnote confidence="0.9702225">
8High DPMM accuracy is driven by the low number of clus-
ters: 46.5 for DPMM vs. 358.3 for LDA and 725.6 for MVM.
</footnote>
<figureCaption confidence="0.82267575">
model size (clusters)
Figure 5: Scatterplot of model size vs. avg score for
MVM (dashed, purple) and LDA (dotted, orange); Syn-
tax+Documents data.
</figureCaption>
<bodyText confidence="0.99967219047619">
and MVM average 41.5%, 49.7% and 60.6% re-
spectively. Qualitatively, models trained on syn-
tax+document yield a higher degree of paradig-
matic clusters which have intuitive thematic struc-
ture. Performance on document intrusion is sig-
nificantly lower and more variable, reflecting the
higher degree of world knowledge required. As with
the previous data set, performance of MVM mod-
els trained on syntax+documents data degrades less
slowly as the cluster granularity increases (Figure 5).
One interesting question is to what degree MVM
views partition syntax and document features versus
LDA topics. That is, to what degree do the MVM
views capture purely syntagmatic or purely paradig-
matic variation? We measured view entropy for all
three models, treating syntactic features and docu-
ment features as different class labels. MVM with
M = 50 views obtained an entropy score of 0.045,
while LDA with M = 50 obtained 0.073, and the
best DPMM model 0.082.9 Thus MVM views may in-
deed capture pure syntactic or thematic clusterings.
</bodyText>
<footnote confidence="0.9761245">
9The low entropy scores reflect the higher percentage of syn-
tactic contexts overall.
</footnote>
<figure confidence="0.999532208333333">
0.0
1.0
0.5
0.0
context intrusion
●
●
● ● ●
●
●
●
●
●
●
●
●
document intrusion
●
● ● ●
●
●
●
●
●
●
●
●
● ● ● ● ● ●
● ●
word intrusion
●
● ● ● ● ●
● ● ●
●●
●
●
●
●
●
●
●
102 102.5 103 103.5
1.0
0.5
1.0
0.5
0.0
% correct
</figure>
<page confidence="0.5299495">
1413
5.3 Discussion (Explicit Feature Selection) In this work we rely on
</page>
<bodyText confidence="0.996908152173913">
As cluster granularity increases, we find that MVM smoothing to reduce the noise of over-broad extrac-
accounts for feature noise better than either LDA tion rather than performing feature selection explic-
or DPMM, yielding more coherent clusters. (Chang itly. All of the models in this paper can be combined
et al., 2009) note that LDA performance degrades with feature selection methods to remove noisy fea-
significantly on a related task as the number of top- tures, and it would be particularly interesting to draw
ics increases, reflecting the increasing difficulty for parallels to models of “clutter” in vision.
humans in grasping the connection between terms (Hierarchical Cross-Categorization) Human con-
in the same topic. This suggests that as topics be- cept organization consists of multiple overlapping
come more ne-grained in models with larger num- local ontologies, similar to the loose ontological
ber of topics, they are less useful for humans. In structure of Wikipedia. Furthermore, each ontologi-
this work, we find that although MVM and LDA per- cal system has a different set of salient properties. It
form similarity on average, MVM clusters are signif- would be interesting to extend MVM to model hier-
icantly more interpretable than LDA clusters as the archy explicitly, and compare against baselines such
granularity increases (Figures 4 and 5). We argue as Brown clustering (Brown et al., 1992), the nested
that models capable of making such fine-grained se- Chinese Restaurant Process (Blei et al., 2003) and
mantic distinctions are more desirable. the hierarchical Pachinko Allocation Model (Mimno
The results presented in the previous two sections et al., 2007).
hold both for unbiased cluster selection (e.g. where 7 Conclusion
clusters are drawn uniformly at random from the This paper introduced MVM, a novel approach to
model) and when cluster selection is biased based modeling lexical semantic organization using mul-
on model probability (results shown). Biased selec- tiple cross-cutting clusterings capable of captur-
tion potentially gives an advantage to MVM, which ing multiple lexical similarity relations jointly in
generates many more small clusters than either LDA the same model. In addition to robustly handling
or DPMM, helping it account for noise. homonymy and polysemy, MVM naturally captures
6 Future Work both syntagmatic and paradigmatic notions of word
Models based on cross-cutting categorization is similarity. MVM performs favorably compared to
a novel approach to lexical semantics and hence other generative lexical semantic models on a set of
should be evaluated on standard baseline tasks, e.g. human evaluations, over a wide range of model set-
contextual paraphrase or lexical substitution (Mc- tings and textual data sources.
Carthy and Navigli, 2007). Additional areas for fu- Acknowledgements
ture work include: We would like to thank the anonymous reviewers for
(Latent Relation Modeling) Clusterings formed their extensive comments. This work was supported
from feature partitions in MVM can be viewed as a by a Google PhD Fellowship to the first author.
form of implicit relation extraction; that is, instead References
of relying on explicit surface patterns in text, rela- David Blei, Thomas Griffiths, Michael Jordan, and
tions between words or concepts are identified in- Joshua Tenenbaum. 2003. Hierarchical topic
directly based on common syntactic patterns. For models and the nested Chinese restaurant process.
example, clusterings that divide cities by geography In Proc. NIPS-2003.
or clusterings partition adjectives by their polarity. Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
(Latent Semantic Language Modeling) Genera- Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
tive models such as MVM can be used to build bet- Class-based n-gram models of natural language.
ter priors for class-based language modeling (Brown Computational Linguistics, 18:467–479.
et al., 1992). The rare n-gram results demonstrate
that MVM is potentially useful for tail contexts; i.e.
inferring tail probabilities from low counts.
1414
</bodyText>
<reference confidence="0.998473777777777">
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models.
In NIPS.
James Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edin-
burgh.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proc. of the ACL.
Association for Computer Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211–240.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Vikash K. Mansinghka, Eric Jonas, Cap Petschu-
lat, Beau Cronin, Patrick Shafto, and Joshua B.
Tenenbaum. 2009. Cross-categorization: A
method for discovering multiple overlapping clus-
terings. In Proc. of Nonparametric Bayes Work-
shop at NIPS 2009.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical substitu-
tion task. In SemEval ’07: Proceedings of the 4th
International Workshop on Semantic Evaluations.
Association for Computational Linguistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1–28.
David Mimno, Wei Li, and Andrew McCallum.
2007. Mixtures of hierarchical topics with
pachinko allocation. In ICML.
Gregory L. Murphy. 2002. The Big Book of Con-
cepts. The MIT Press.
Donglin Niu, Jennifer G. Dy, and Michael I. Jor-
dan. 2010. Multiple non-redundant spectral
clustering views. In Johannes F¨urnkranz and
Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learn-
ing (ICML-10), pages 831–838.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161–199.
Joseph Reisinger and Raymond J. Mooney. 2010.
A mixture model with sharing for lexical seman-
tics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2010).
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52–57. ACL.
Hinrich Sch¨utze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97–123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of
the ACL.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
Amos Tversky and Itamar Gati. 1982. Similarity,
separability, and the triangle inequality. Psycho-
logical Review, 89(2):123–154.
Benjamin Van Durme and Marius Pas¸ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
</reference>
<page confidence="0.992916">
1415
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523528">
<title confidence="0.999545">Cross-Cutting Models of Lexical Semantics</title>
<author confidence="0.999954">Joseph Reisinger</author>
<affiliation confidence="0.999882">Department of Computer Sciences The University of Texas at Austin</affiliation>
<address confidence="0.983197">Austin, TX 78712</address>
<email confidence="0.999702">joeraii@cs.utexas.edu</email>
<author confidence="0.940279">Raymond</author>
<affiliation confidence="0.859757666666667">Department of Computer The University of Texas at Austin, TX</affiliation>
<email confidence="0.999195">mooney@cs.utexas.edu</email>
<abstract confidence="0.998128413793104">Context-dependent word similarity can be over multiple dimen- For example, simthematically, while suin similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel Multi-View Mixture that repwords as multiple clusmultiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirichlet Allocation. Intuitively, this constraint favors feature partitions that have coherent topsemantics. Furthermore, soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the of an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Chong Wang</author>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="20579" citStr="Chang et al., 2009" startWordPosition="3296" endWordPosition="3299">cal semantic knowledge is equivalent to knowing the contexts that words appear in, and hence being able to form reasonable hypotheses about the relatedness of syntactic contexts. Vector space models are commonly evaluated by comparing their similarity predictions to a nominal set of human similarity judgments (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). In this work, since we are evaluating models that potentially yield many different similarity scores, we take a different approach, scoring clusters on their semantic and syntactic coherence using a set intrusion task (Chang et al., 2009). In set intrusion, human raters are shown a set of options from a coherent group and asked to identify a single intruder drawn from a different group. We extend intrusion to three different lexical semantic tasks: (1) context intrusion, where the top contexts from each cluster are used, (3) document intrusion, where the top document contexts from each cluster are used, and (2) word intrusion, where the top words from each cluster are used. For each cluster, the top four contexts/words are selected and appended with another context/word from a different cluster.4 The resulting set is then shuf</context>
</contexts>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2616" citStr="Curran, 2004" startWordPosition="384" endWordPosition="385">by occupation or by nationality). The effects of these overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage. In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categorization (Shafto et al., 2006) that generalizes traditional vector-space or distributional models of lexical semantics (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). Cross-cutting categorization finds multiple feature subsets (categorization systems) that produce high quality clusterings of the data. For example words might be clustered based on their part of speech, or based on their thematic usage. Contextdependent variation in word usage can be accounted for by leveraging multiple latent categorization systems. In particular, cross-cutting models can be used to capture both syntagmatic and paradigmatic notions of word relatedness, breaking up word features into multiple categorization systems and </context>
<context position="4667" citStr="Curran, 2004" startWordPosition="685" endWordPosition="686">. We find that MVM finds more semantically and syntactically coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of dista</context>
<context position="20284" citStr="Curran, 2004" startWordPosition="3251" endWordPosition="3252">trop Conquest of Wu by Jin Field ration World War I Mongolia Table 1: Example questions from the three intrusion tasks, in order of difficulty (left to right, easy to hard; computed from inter-annotator agreement). Italics show intruder items. According to the use theory of meaning, lexical semantic knowledge is equivalent to knowing the contexts that words appear in, and hence being able to form reasonable hypotheses about the relatedness of syntactic contexts. Vector space models are commonly evaluated by comparing their similarity predictions to a nominal set of human similarity judgments (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). In this work, since we are evaluating models that potentially yield many different similarity scores, we take a different approach, scoring clusters on their semantic and syntactic coherence using a set intrusion task (Chang et al., 2009). In set intrusion, human raters are shown a set of options from a coherent group and asked to identify a single intruder drawn from a different group. We extend intrusion to three different lexical semantic tasks: (1) context intrusion, where the top contexts from each cluster are used, (3) document int</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL.</booktitle>
<publisher>Association</publisher>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="5424" citStr="Erk (2007)" startWordPosition="806" endWordPosition="807">cCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived directly from word occurrences and embedded in a common vector space to capture context-dependent usage. Likewise Reisinger and Mooney (2010) take a similar approach using mixture modeling combined with a background variation model to generate multiple prototype vectors for polysemous words. Both of these approaches still ultimately embed all words in a single metric space and hence argue for globally consistent metrics that capture human 1Similarity also has been shown to violate symmetry (e.g. people have the intui</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proc. of the ACL. Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW</booktitle>
<contexts>
<context position="4523" citStr="Finkelstein et al., 2001" startWordPosition="662" endWordPosition="665">ther modelbased clustering procedures in a series of human evaluation tasks, measuring its ability to find meaningful syntagmatic and paradigmatic structure. We find that MVM finds more semantically and syntactically coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Proc. of WWW 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gorman</author>
<author>James R Curran</author>
</authors>
<title>Scaling distributional similarity to large corpora.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="4908" citStr="Gorman and Curran, 2006" startWordPosition="719" endWordPosition="722"> model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived direct</context>
</contexts>
<marker>Gorman, Curran, 2006</marker>
<rawString>James Gorman and James R. Curran. 2006. Scaling distributional similarity to large corpora. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<pages>114--2007</pages>
<contexts>
<context position="5386" citStr="Griffiths et al., 2007" startWordPosition="798" endWordPosition="801">ference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived directly from word occurrences and embedded in a common vector space to capture context-dependent usage. Likewise Reisinger and Mooney (2010) take a similar approach using mixture modeling combined with a background variation model to generate multiple prototype vectors for polysemous words. Both of these approaches still ultimately embed all words in a single metric space and hence argue for globally consistent metrics that capture human 1Similarity also has been shown to violat</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114:2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="4550" citStr="Landauer and Dumais, 1997" startWordPosition="666" endWordPosition="669"> procedures in a series of human evaluation tasks, measuring its ability to find meaningful syntagmatic and paradigmatic structure. We find that MVM finds more semantically and syntactically coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meani</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Vikash K Mansinghka</author>
<author>Eric Jonas</author>
<author>Cap Petschulat</author>
<author>Beau Cronin</author>
<author>Patrick Shafto</author>
<author>B Joshua</author>
</authors>
<marker>Mansinghka, Jonas, Petschulat, Cronin, Shafto, Joshua, </marker>
<rawString>Vikash K. Mansinghka, Eric Jonas, Cap Petschulat, Beau Cronin, Patrick Shafto, and Joshua B.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tenenbaum</author>
</authors>
<title>Cross-categorization: A method for discovering multiple overlapping clusterings.</title>
<date>2009</date>
<booktitle>In Proc. of Nonparametric Bayes Workshop at NIPS</booktitle>
<marker>Tenenbaum, 2009</marker>
<rawString>Tenenbaum. 2009. Cross-categorization: A method for discovering multiple overlapping clusterings. In Proc. of Nonparametric Bayes Workshop at NIPS 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4840" citStr="McCarthy and Navigli, 2007" startWordPosition="708" endWordPosition="711">nd Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents thi</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. SemEval-2007 task 10: English lexical substitution task. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="4693" citStr="Miller and Charles, 1991" startWordPosition="687" endWordPosition="690"> MVM finds more semantically and syntactically coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and </context>
<context position="21775" citStr="Miller and Charles (1991)" startWordPosition="3496" endWordPosition="3499">he resulting set is then shuffled, and the human raters are asked to identify the intruder, af4Choosing four elements from the cluster uniformly at random instead of the top by probability led to lower performance across all models. ter being given a short introduction (with common examples) to the task. Table 1 shows sample questions of varying degrees of difficulty. As the semantic coherence and distinctness from other clusters increases, this task becomes easier. Set intrusion is a more robust way to account for human similarity judgments than asking directly for a numeric score (e.g., the Miller and Charles (1991) set) as less calibration is required across raters. Furthermore, the additional cluster context significantly reduces the variability of responses. Human raters were recruited from Amazon’s Mechanical Turk. A total of 1256 raters completed 30438 evaluations for 5780 unique intrusion tasks (5 evaluations per task). 2736 potentially fraudulent evaluations from 11 raters were rejected.5 Table 3 summarizes inter-annotator agreement. Overall we found n ti 0.4 for most tasks; a set of comments about the task difficulty is given in Table 2, drawn from an anonymous public message board. 5 Results We </context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with pachinko allocation.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>David Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory L Murphy</author>
</authors>
<title>The Big Book of Concepts.</title>
<date>2002</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2158" citStr="Murphy, 2002" startWordPosition="320" endWordPosition="321">n Humans categorize objects using multiple orthogonal taxonomic systems, where category generalization depends critically on what features are relevant to one particular system. For example, foods can be organized in terms of their nutritional value (high in fiber) or situationally (commonly eaten for Thanks1405 giving; Shafto et al. (2006)). Human knowledgebases such as Wikipedia also exhibit such multiple clustering structure (e.g. people are organized by occupation or by nationality). The effects of these overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage. In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categorization (Shafto et al., 2006) that generalizes traditional vector-space or distributional models of lexical semantics (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). Cross-cutting categorization finds multiple feature subsets (categorization systems) </context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Gregory L. Murphy. 2002. The Big Book of Concepts. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donglin Niu</author>
<author>Jennifer G Dy</author>
<author>Michael I Jordan</author>
</authors>
<title>Multiple non-redundant spectral clustering views.</title>
<date>2010</date>
<booktitle>In Johannes F¨urnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10),</booktitle>
<pages>831--838</pages>
<contexts>
<context position="7555" citStr="Niu et al., 2010" startWordPosition="1126" endWordPosition="1129">curring in similar documents. MVM can extract multiple competing notions of similarity, for example both paradigmatic, or thematic similarity, and syntagmatic or syntactic similarity, in addition to more fine grained relations. 3 Multi-View Clustering with MVM As feature dimensionality increases, the number of ways the data can exhibit interesting structure goes up exponentially. Clustering is commonly used to explain data, but often there are several equally valid, competing clusterings, keying off of different subsets of features, especially in high-dimensional settings such as text mining (Niu et al., 2010). For example, company websites can be clustered by sector or by geographic location, with one particular clustering becoming predominant when a majority of features correlate with it. In fact, informative features in one clustering may be noise in another, e.g. the occurrence of CEO is not necessarily discriminative when clustering companies by industry sector, but may be useful in other clusterings. Multiple clustering is one approach to inferring feature subspaces that lead to high quality data partitions. Multiple clustering also improves the flexibility of generative clustering models, as</context>
</contexts>
<marker>Niu, Dy, Jordan, 2010</marker>
<rawString>Donglin Niu, Jennifer G. Dy, and Michael I. Jordan. 2010. Multiple non-redundant spectral clustering views. In Johannes F¨urnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 831–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>A mixture model with sharing for lexical semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010).</booktitle>
<contexts>
<context position="5643" citStr="Reisinger and Mooney (2010)" startWordPosition="834" endWordPosition="837"> et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived directly from word occurrences and embedded in a common vector space to capture context-dependent usage. Likewise Reisinger and Mooney (2010) take a similar approach using mixture modeling combined with a background variation model to generate multiple prototype vectors for polysemous words. Both of these approaches still ultimately embed all words in a single metric space and hence argue for globally consistent metrics that capture human 1Similarity also has been shown to violate symmetry (e.g. people have the intuition that China is more similar to North Korea than North Korea is to China). intuitive notions of “similarity.” Rather than assuming a global metric embedding exists, in this work we simply leverage the cluster assumpt</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. A mixture model with sharing for lexical semantics. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics,</booktitle>
<pages>52--57</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4786" citStr="Resnik, 1997" startWordPosition="703" endWordPosition="704">re n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics, pages 52–57. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Shafto</author>
<author>Charles Kemp</author>
<author>Vikash Mansinghka</author>
<author>Matthew Gordon</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Learning cross-cutting systems of categories.</title>
<date>2006</date>
<booktitle>In Proc. CogSci</booktitle>
<contexts>
<context position="1887" citStr="Shafto et al. (2006)" startWordPosition="279" endWordPosition="282">views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation. 1 Introduction Humans categorize objects using multiple orthogonal taxonomic systems, where category generalization depends critically on what features are relevant to one particular system. For example, foods can be organized in terms of their nutritional value (high in fiber) or situationally (commonly eaten for Thanks1405 giving; Shafto et al. (2006)). Human knowledgebases such as Wikipedia also exhibit such multiple clustering structure (e.g. people are organized by occupation or by nationality). The effects of these overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage. In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categoriz</context>
<context position="15796" citStr="Shafto et al., 2006" startWordPosition="2530" endWordPosition="2533">rresponding to ics/views form an admixture, allowing individual dominant eigenvectors in local syntactic or semanfeatures from a single data point to be assigned to tic space. Within each view, MVM partitions words multiple views. into clusters based on each word’s local representaThe most similar model to ours is Cross-cutting tion in that view; that is, based on the set of concategorization (CCC), which fits multiple DPMMs to text features it allocates to the view. Words have a non-overlapping partitions of features (Mansinghka non-uniform affinity for each view, and hence may et al., 2009; Shafto et al., 2006). Unlike MVM, not be present in every clustering (Figure 2). This CCC partitions features among multiple DPMMs, is important as different ways of drawing distinchence all occurrences of a particular feature will tions between words do not necessarily apply to all end up in a single clustering, instead of assigning words. In contrast, LDA finds locally consistent colthem softly using LDA. Such hard feature partition- lections of contexts but does not further subdivide ing does not admit an efficient sampling procedure, words into clusters given that set of contexts. Hence, and hence Shafto et a</context>
</contexts>
<marker>Shafto, Kemp, Mansinghka, Gordon, Tenenbaum, 2006</marker>
<rawString>Patrick Shafto, Charles Kemp, Vikash Mansinghka, Matthew Gordon, and Joshua B. Tenenbaum. 2006. Learning cross-cutting systems of categories. In Proc. CogSci 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5030" citStr="Snow et al., 2006" startWordPosition="738" endWordPosition="741">¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived directly from word occurrences and embedded in a common vector space to capture context-dependent usage. Likewise Reisinger and </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="4572" citStr="Turian et al., 2010" startWordPosition="670" endWordPosition="673">human evaluation tasks, measuring its ability to find meaningful syntagmatic and paradigmatic structure. We find that MVM finds more semantically and syntactically coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="2671" citStr="Turney, 2006" startWordPosition="392" endWordPosition="394"> overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage. In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categorization (Shafto et al., 2006) that generalizes traditional vector-space or distributional models of lexical semantics (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). Cross-cutting categorization finds multiple feature subsets (categorization systems) that produce high quality clusterings of the data. For example words might be clustered based on their part of speech, or based on their thematic usage. Contextdependent variation in word usage can be accounted for by leveraging multiple latent categorization systems. In particular, cross-cutting models can be used to capture both syntagmatic and paradigmatic notions of word relatedness, breaking up word features into multiple categorization systems and then computing similarity separately for each system. M</context>
<context position="4724" citStr="Turney, 2006" startWordPosition="693" endWordPosition="694">ally coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less tha</context>
<context position="20339" citStr="Turney, 2006" startWordPosition="3259" endWordPosition="3261">ngolia Table 1: Example questions from the three intrusion tasks, in order of difficulty (left to right, easy to hard; computed from inter-annotator agreement). Italics show intruder items. According to the use theory of meaning, lexical semantic knowledge is equivalent to knowing the contexts that words appear in, and hence being able to form reasonable hypotheses about the relatedness of syntactic contexts. Vector space models are commonly evaluated by comparing their similarity predictions to a nominal set of human similarity judgments (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). In this work, since we are evaluating models that potentially yield many different similarity scores, we take a different approach, scoring clusters on their semantic and syntactic coherence using a set intrusion task (Chang et al., 2009). In set intrusion, human raters are shown a set of options from a coherent group and asked to identify a single intruder drawn from a different group. We extend intrusion to three different lexical semantic tasks: (1) context intrusion, where the top contexts from each cluster are used, (3) document intrusion, where the top document contexts from each clust</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
<author>Itamar Gati</author>
</authors>
<title>Similarity, separability, and the triangle inequality.</title>
<date>1982</date>
<journal>Psychological Review,</journal>
<volume>89</volume>
<issue>2</issue>
<contexts>
<context position="5411" citStr="Tversky and Gati, 1982" startWordPosition="802" endWordPosition="805">nd lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived directly from word occurrences and embedded in a common vector space to capture context-dependent usage. Likewise Reisinger and Mooney (2010) take a similar approach using mixture modeling combined with a background variation model to generate multiple prototype vectors for polysemous words. Both of these approaches still ultimately embed all words in a single metric space and hence argue for globally consistent metrics that capture human 1Similarity also has been shown to violate symmetry (e.g. people h</context>
</contexts>
<marker>Tversky, Gati, 1982</marker>
<rawString>Amos Tversky and Itamar Gati. 1982. Similarity, separability, and the triangle inequality. Psychological Review, 89(2):123–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI</booktitle>
<marker>Van Durme, Pas¸ca, 2008</marker>
<rawString>Benjamin Van Durme and Marius Pas¸ca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In Proc. of AAAI 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>