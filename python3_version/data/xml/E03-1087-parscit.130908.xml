<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001875">
<title confidence="0.998381">
A Cascaded Finite-State Parser for German
</title>
<author confidence="0.989282">
Michael Schiehlen
</author>
<affiliation confidence="0.983276">
Institute for Computational Linguistics, University of Stuttgart,
</affiliation>
<address confidence="0.919157">
Azenbergstr. 12, D-70174 Stuttgart
</address>
<email confidence="0.99817">
mike@adler.ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991025">
The paper presents two approaches to
partial parsing of German: a tagger
trained on dependency tuples, and a cas-
caded finite-state parser (Abney, 1997).
For the tagging approach, the effects of
choosing different representations of de-
pendency tuples are investigated. Per-
formance of the finite-state parser is
boosted by delaying syntactically un-
solvable disambiguation problems via
underspecification. Both approaches are
evaluated on a 340,000-token corpus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956666666666">
Traditional parsers are often quite brittle, and op-
timize precision over recall. It is therefore impor-
tant to also look at shallow approaches that come
at virtually no cost in manual labour but can po-
tentially supplement more knowledge-prone ap-
proaches. The paper discusses one such approach
which gets by with a tree bank and a tagger. An-
other issue in parsing is speed, which can only
be gained by deterministic processing. Determin-
istic parsers return exactly one syntactic reading,
which forces them to solve many locally unsolv-
able puzzles. Abney (1997) suggests a way out of
this dilemma: The parser leaves ambiguities unre-
solved if they are contained in a local domain. So
at least ambiguities of this kind can conceivably be
handed over to some expert disambiguation mod-
ule. The paper fleshes out this idea and shows its
impact on overall performance.
</bodyText>
<sectionHeader confidence="0.997652" genericHeader="introduction">
2 Evaluation Method
</sectionHeader>
<bodyText confidence="0.999975484848485">
Instead of using the prevalent PARSEVAL mea-
sures, we opted for a dependency-based evalua-
tion (Lin, 1995), which is arguably (Srinivas et
al., 1996) (Kiibler and Telljohann, 2002) fairer to
partial parsers. In a dependency structure, every
word token (dependent) is related to another token
(head) over a grammatical role, but for one word
token, which is called the root. Thus, a parser
constructing a dependency structure needs to as-
sociate every word token either with a head to-
ken plus grammatical role or mark it as the root or
&apos;TOP&apos; node. The task can be seen as a classifica-
tion problem and measured in (labelled) precision
and recall. To simplify the task, grammatical roles
can be neglected (unlabelled precision and recall).
The details deserve some attention. With
Kiibler and Telljohann (2002) and in contrast to
Lin (1995), we assume that PPs are headed
by their internal NPs, and that conjoined
phrases have multiple heads (the conjuncts),
with the conjunction linked to the last con-
junct. Carroll et al. (1998) introduce additional
links for control phenomena, map several to-
kens to one node (e g linked preposition—noun
and determiner—noun pairs), and allow nodes for
elided words (e.g. in pro-/topic-drop and gap-
ping). An important objection is that the weight
of words is determined quite arbitrarily (Clark
and Hockenmaier, 2002). Thus, we adopt Lin&apos;s
scheme with the above provisos.
Training and test sets for the experiments de-
scribed below were derived from a tokenized ver-
sion of the Negra tree bank of German newspaper
</bodyText>
<page confidence="0.998375">
163
</page>
<bodyText confidence="0.998914333333333">
texts (Skut et al., 1997), comprising ca. 340,000
tokens in 19,547 sentences. Different tagging
qualities were taken into account by alternatively
using Part-Of-Speech tags determined by the Tree
Tagger (Schmid, 1994) (tagger tags), POS tags de-
termined by the Tree Tagger trained on the tree
bank (lexicon tags), or the POS tags of the tree
bank (ideal tags). All experiments were run on a
SUN Blade-1000.
</bodyText>
<sectionHeader confidence="0.95576" genericHeader="method">
3 Tagging Approach
</sectionHeader>
<bodyText confidence="0.999990212121212">
The head tokens in dependency tuples can be
coded in several ways. The position method rep-
resents a head token by its position in the sentence
(poshead). On the Negra tree bank, this method
yields 121 unlabelled and 1810 labelled&apos; classes.
The distance method codes a head token by giv-
ing the distance to the dependent (poshead-posdep),
yielding 123 unlabelled but only 1139 labelled
classes. Lin (1995) represents the head token by
its word type and a position indicator which en-
codes the direction where the head can be found
and the number of tokens of identical type between
head and dependent (e.g. &lt; first token with same
word type on the left, &gt;&gt;&gt; third token with same
word type on the right, etc.). To get fewer classes,
we use the category2 of the head token instead of
its word type. The resulting method (which we
will call nth-tag method) yields 115 unlabelled
and 639 labelled classes.
For the experiment, the trigram-based Tree Tag-
ger was used to map tokens directly to the depen-
dency classes (see for a similar approach (Srini-
vas et al., 1996)). Performance was degraded
when the tagger got information on both word
type and POS tag of the tokens, so we only used
POS tag. We didn&apos;t test the position method.
Figure 1 shows results achieved via 10-fold cross-
validation with Ideal and Tagger tags. The tag-
ger always gives a unique answer, but head to-
kens not found in the string count as not assigned,
hence the discrepancy between precision and re-
call. A figure is also given for the percentage
of sentences getting a completely correct parse.
</bodyText>
<footnote confidence="0.9867665">
1NEGRA distinguishes 33 grammatical roles.
2Better performance is achieved when only the category
information in the POS tag is used, but not Verb Form, or
distinction between common and proper nouns.
</footnote>
<table confidence="0.926204666666667">
labelled unlabelled cor-
prec rec speed prec rec speed rect
I dist 63.07 60.06 3.41 62.69 61.10 19.80 4.63
I nth 73.86 67.92 14.03 72.02 64.83 122.45 5.22
T dist 61.00 58.08 2.48 61.32 59.88 26.62 3.97
T nth 71.04 64.93 10.67 70.13 63.04 77.33 4.39
</table>
<figureCaption confidence="0.99797">
Figure 1: Evaluation Results for Tagger
</figureCaption>
<bodyText confidence="0.999788428571429">
Processing speed is measured in Words Per Sec-
ond. We also combined the distance and nth-tag
method by using a greedy method to choose be-
tween them on the basis of the POS tag of the to-
ken and the proposed result. This hybrid method
achieved 80.99%/75.82% labelled precision recall
on Ideal tags and 78.02%172.83% on Tagger tags.
</bodyText>
<sectionHeader confidence="0.931784" genericHeader="method">
4 Cascaded Finite-State Parser
</sectionHeader>
<subsectionHeader confidence="0.943147">
4.1 Description of the Parser
</subsectionHeader>
<bodyText confidence="0.957712782608696">
The parser described here essentially relies on
techniques also used by Abney (1997). It basically
consists of a noun chunker and a clause chunker.
The noun chunker is deterministic, but recog-
nizes recursive noun chunks in several passes.
Morphological information on case, number and
gender coded is computed with bit vectors (Abney,
1997). A noun chunk is defined as an NP or PP
with all adjuncts at the beginning (e.g. adverbs)
and at the end (e.g. PPs and relative clauses)
stripped off (Brants, 1999) (Schiehlen, 2002).
The clause chunker consists of three determin-
istic transducers recognizing verb-final, verb-first,
and verb-second clauses. The parser aims to deter-
mine full clauses rather than the &amp;quot;simplex claus-
es&amp;quot; of Abney (1997) (i.e. non-recursive &amp;quot;core&amp;quot;
parts of clauses). The verb-final clause transducer
e.g. works from right to left so that subclauses are
maximally embedded. Example (1) shows chun-
ker output (a flat parse tree) after the recognition
phase.
NPriom,dat,akk
NPnom,akk
</bodyText>
<listItem confidence="0.98437975">
(1) Udo hat eine sehr nette Frau aus Rio .
Udo has a very nice wife from Rio.
or: A very nice woman has Udo from Rio.
or: A very nice woman from Rio has Udo.
</listItem>
<bodyText confidence="0.628768">
aus
dal
</bodyText>
<page confidence="0.993968">
164
</page>
<bodyText confidence="0.999612222222222">
An interpretation step follows, where non-
deterministic transducers insert further syntactic
structure (e.g. adjective phrases, phrases for co-
ordinated VPs and prepositions) and grammatical
roles3. The pertinent information is coded in the
finite-state grammars although it is not seen by the
recognition transducers. See below a rule in the
grammar, semicolon symbols are only needed for
interpretation.
</bodyText>
<listItem confidence="0.8054075">
(2) det ;SPR ( JADJP ( adv ;ADJ )* adja ;HD
;]ADJP )* nn ;HD FINAL:NP
</listItem>
<bodyText confidence="0.9998465">
Verb government and verb complexes can only
be computed after coordinated VPs have been in-
serted, since auxiliaries may distribute. Exam-
ple (3) shows the parse tree after interpretation.
Finally, a deterministic transducer recognizes sub-
categorization frames using a grammar automati-
cally constructed from lexically specified frames
and introduces a fine-grained differentiation of the
complement relation (61 additional grammatical
roles). See example (4) for output.
</bodyText>
<equation confidence="0.505162666666667">
NOM;AK
AKK; OM
NP NP PP
</equation>
<listItem confidence="0.593856">
(3) Udo hat eine sehr nette Frau aus Rio .
</listItem>
<bodyText confidence="0.99849275">
If the frame transducer fails, an unspecified gram-
matical role is left (Carroll et al., 1998). Such roles
are counted as correct only in a set of figures that
we shall call half-labelled precision and recall.
</bodyText>
<subsectionHeader confidence="0.972811">
4.2 Explicit Underspecification
</subsectionHeader>
<bodyText confidence="0.97668578125">
An apparent drawback of deterministic parsers is
the need for forced guessing, i.e. the need to make
decisions without access to the requisite disam-
biguating knowledge. Cases in point are PP at-
tachment and (sometimes) determination of case
3There are 13 grammatical roles: head, adjunct, apposi-
tion, complement, adjunct or complement, conjunction, first
part of conjunction, measure phrase, marker, specifier, sub-
ject, governed verb, unconnected.
in German (cf. example (4)). In context-free pars-
ing, the solution to this problem is conservation of
ambiguities in the output: Difficult decisions are
delayed to a later stage. Similar techniques can be
used in finite-state parsing (Elworthy et al., 2001).
Underspecification can be elegantly imple-
mented with context variables (Maxwell III and
Kaplan, 1989) (Done, 1997). Since subcatego-
rization ambiguities are specific to main verbs in
clauses and never interact across clause bound-
aries, the clause nodes themselves can be inter-
preted as context variables. The different op-
tions are implicitly encoded by bringing the vary-
ing grammatical roles of a constituent node in a
clause-wide uniform order (e.g. in example (4)
position 1: first NP nominative, second NP ac-
cusative; position 2: first NP accusative, sec-
ond NP nominative). VP coordination sometimes
gives rise to structures with constituents figuring
in several subcategorization frames at once. In this
case several lists of grammatical roles are associ-
ated with the constituent, one for each conjunct in
left-to-right order (cf. example (5)).
</bodyText>
<listItem confidence="0.779895333333333">
(4) Hans((N;A) (N;D)) [[kennt Maria((A;N))]
und [hilft Karla((D;N))]].
Hans knows Maria and helps Karla.
or: Maria knows and Karla helps Hans.
or: Maria knows Hans and he helps Karla.
or: Hans knows Maria and Karla helps him.
</listItem>
<bodyText confidence="0.992427">
In a final processing step, the constituent trees are
converted into dependency tuples. In this step,
attachment and subcategorization ambiguities are
overtly represented with context variables, cf. (6):
</bodyText>
<equation confidence="0.8370172">
(5) Lido/0 hat/1 [1a]:NPnom,[1b]:NPakk
hat/1 TOP
eine/2 Frau/5 SPR
sehr/3 nette/4 ADJ
nette/4 Frau/5 ADJ
Frau/5 hat/1 [1a]:NPakk,[1b]:NPnom
aus/6 Rio/7 MRK
Rio/7 hat/1 ADJ [lAO]
Frau/5 ADJ [1A1]
./8 TOP
</equation>
<bodyText confidence="0.974919666666667">
Riezler et al. (2002) evaluate underspecified syn-
tactic representations by distinguishing lower
bound performance (random choice of a parse)
</bodyText>
<figure confidence="0.9934555">
CMP
&amp;quot;nom;dat;akk
ADJP MR
ADIVD
(6) Udo hat eine sehr nette Frau aus Rio .
ADJ
</figure>
<page confidence="0.81502">
165
</page>
<note confidence="0.485626">
References
Steven Abney. 1997. Partial Parsing via Finite-State
</note>
<bodyText confidence="0.82945025">
Cascades. Journal of Natural Language Engineering,
2(4):337-344.
and upper bound performance (selection of the
best parse according to the test set).
</bodyText>
<subsectionHeader confidence="0.98854">
4.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.9989014">
Currently, the speed of the finite-state parser is at
2430 Words Per Second, but this figure can still be
improved by compiling the backtracking necessi-
tated in Abney&apos;s (1997) approach into the transi-
tion tables. See Figure 2 for results of parsing
</bodyText>
<table confidence="0.985371625">
labelled half-lab. unlab. cor-
prec rec prec rec prec rec rect
I lower 82.9 76.1 85.0 77.9 89.6 82.2 12.5
I upper 90.9 83.4 92.9 85.2 95.0 87.2 39.0
L lower 82.3 74.7 84.2 76.5 89.3 81.1 11.7
L upper 90.3 82.0 92.2 83.7 94.7 85.9 36.5
T lower 81.5 71.3 83.5 73.0 88.6 77.5 10.6
T upper 88.9 77.7 90.9 79.5 93.6 81.8 31.0
</table>
<figureCaption confidence="0.96865">
Figure 2: Results for Finite-State Parser
</figureCaption>
<bodyText confidence="0.999911428571429">
on Ideal, Lexicon and Tagger tags. The last col-
umn of the table shows the percentage of com-
pletely correct analyses of sentences. For the
lower bound, only unambiguous sentence analy-
ses count as correct. When we combined chun-
ker and tagger results using the greedy method,
performance was boosted to 94.48%/87.28% la-
belled precision/recall on ideal tags (upper-bound)
and 94.36%/86.35% (lower-bound). These fig-
ures can be compared with the values reported by
Neumann et al. (2000) (precision 89.68%, recall
84.75%) although they used a much smaller cor-
pus for evaluation (10,400 tokens) which was not
annotated independently.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998997">
The paper presents a cascaded finite-state parser
incorporating some degree of underspecification.
The idea is that such syntactically unresolvable
ambiguities are later resolved by expert disam-
biguation modules. The performance of the finite-
state parser has been compared with a very simple
tagging approach which nevertheless gets more
than 50% of the dependency structure correct. I
am grateful to Helmut Schmid for discussion and
to the reviewers for hints on literature.
</bodyText>
<reference confidence="0.997597673469388">
Thorsten Brants. 1999. Cascaded Markov Models. In Pro-
ceedings of EACL&apos;99, Bergen, Norway.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser Evaluation: a Survey and a New Proposal. In Pro-
ceedings of LREC, pages 447-454, Granada.
Stephen Clark and Julia Hockenmaier. 2002. Evaluating
a Wide-Coverage CCG Parser. In Beyond PARSE VAL -
Tivoards Improved Evaluation Measures for Parsign Sys-
tems (LREC Workshop).
Jochen Done. 1997. Efficient Construction of Underspeci-
fied Semantics under Massive Ambiguity. In Proceedings
of ACL&apos;97, pages 386-393, Madrid, Spain.
David Elworthy, Tony Rose, Amanda Clare, and Aaron
Kotcheff. 2001. A natural language system for retrieval
of captioned images. Journal of Natural Language Engi-
neering, 7(2):117-142.
Sandra Kibler and Heike Telljohann. 2002. Towards a
Dependency-Oriented Evaluation for Partial Parsing. In
Beyond PARSE VAL - Towards Improved Evaluation Mea-
sures for Parsing Systems (LREC Workshop).
Dekang Lin. 1995. A Dependency-based Method for Eval-
uating Broad-Coverage Parsers. In Proceedings of the
IJCAI-95, pages 1420-1425, Montreal.
John T. Maxwell 111 and Ronald M. Kaplan. 1989. An
overview of disjunctive constraint satisfaction. In Pro-
ceedings of the International Workshop on Parsing Tech-
nologies, Pittsburgh, PA.
Gunter Neumann, Christian Braun, and Jakub Piskorski.
2000. A Divide-and-Conquer Strategy for Shallow Pars-
ing of German Free Text. In Proceedings of ANLP&apos;00,
pages 239-246, Seattle, WA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-Functional
Grammar and Discriminative Estimation Techniques. In
Proceedings of ACE02.
Michael Schiehlen. 2002. Experiments in German Noun
Chunking. In Proceedings of COLING &apos;02, Taipei.
Helmut Schmid. 1994. Probabilistic Part-Of-Speech Tag-
ging Using Decision Trees. Technical report, Institut fiir
maschinelle Sprachverarbeitung, Universitat Stuttgart.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1997. An Annotation Scheme for Free Word
Order Languages. In Proceedings of the ANLP-97, Wash-
ington, DC.
B. Srinivas, Christine Doran, Beth Ann Hockey, and Ar-
avind Joshi. 1996. An approach to Robust Partial Parsing
and Evaluation Metrics. In Proceedings of the ESSLLI96
Workshop on Robust Parsing, pages 70-82, Prague.
</reference>
<page confidence="0.998764">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885632">
<title confidence="0.999739">A Cascaded Finite-State Parser for German</title>
<author confidence="0.999948">Michael Schiehlen</author>
<affiliation confidence="0.999739">Institute for Computational Linguistics, University of Stuttgart,</affiliation>
<address confidence="0.927535">Stuttgart</address>
<abstract confidence="0.995440692307692">The paper presents two approaches to partial parsing of German: a tagger trained on dependency tuples, and a cascaded finite-state parser (Abney, 1997). For the tagging approach, the effects of choosing different representations of dependency tuples are investigated. Performance of the finite-state parser is boosted by delaying syntactically unsolvable disambiguation problems via underspecification. Both approaches are evaluated on a 340,000-token corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Cascaded Markov Models.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL&apos;99,</booktitle>
<location>Bergen,</location>
<contexts>
<context position="6471" citStr="Brants, 1999" startWordPosition="1064" endWordPosition="1065">n Ideal tags and 78.02%172.83% on Tagger tags. 4 Cascaded Finite-State Parser 4.1 Description of the Parser The parser described here essentially relies on techniques also used by Abney (1997). It basically consists of a noun chunker and a clause chunker. The noun chunker is deterministic, but recognizes recursive noun chunks in several passes. Morphological information on case, number and gender coded is computed with bit vectors (Abney, 1997). A noun chunk is defined as an NP or PP with all adjuncts at the beginning (e.g. adverbs) and at the end (e.g. PPs and relative clauses) stripped off (Brants, 1999) (Schiehlen, 2002). The clause chunker consists of three deterministic transducers recognizing verb-final, verb-first, and verb-second clauses. The parser aims to determine full clauses rather than the &amp;quot;simplex clauses&amp;quot; of Abney (1997) (i.e. non-recursive &amp;quot;core&amp;quot; parts of clauses). The verb-final clause transducer e.g. works from right to left so that subclauses are maximally embedded. Example (1) shows chunker output (a flat parse tree) after the recognition phase. NPriom,dat,akk NPnom,akk (1) Udo hat eine sehr nette Frau aus Rio . Udo has a very nice wife from Rio. or: A very nice woman has U</context>
</contexts>
<marker>Brants, 1999</marker>
<rawString>Thorsten Brants. 1999. Cascaded Markov Models. In Proceedings of EACL&apos;99, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser Evaluation: a Survey and a New Proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>447--454</pages>
<location>Granada.</location>
<contexts>
<context position="2581" citStr="Carroll et al. (1998)" startWordPosition="401" endWordPosition="404">ependency structure needs to associate every word token either with a head token plus grammatical role or mark it as the root or &apos;TOP&apos; node. The task can be seen as a classification problem and measured in (labelled) precision and recall. To simplify the task, grammatical roles can be neglected (unlabelled precision and recall). The details deserve some attention. With Kiibler and Telljohann (2002) and in contrast to Lin (1995), we assume that PPs are headed by their internal NPs, and that conjoined phrases have multiple heads (the conjuncts), with the conjunction linked to the last conjunct. Carroll et al. (1998) introduce additional links for control phenomena, map several tokens to one node (e g linked preposition—noun and determiner—noun pairs), and allow nodes for elided words (e.g. in pro-/topic-drop and gapping). An important objection is that the weight of words is determined quite arbitrarily (Clark and Hockenmaier, 2002). Thus, we adopt Lin&apos;s scheme with the above provisos. Training and test sets for the experiments described below were derived from a tokenized version of the Negra tree bank of German newspaper 163 texts (Skut et al., 1997), comprising ca. 340,000 tokens in 19,547 sentences. </context>
<context position="8239" citStr="Carroll et al., 1998" startWordPosition="1343" endWordPosition="1346">ernment and verb complexes can only be computed after coordinated VPs have been inserted, since auxiliaries may distribute. Example (3) shows the parse tree after interpretation. Finally, a deterministic transducer recognizes subcategorization frames using a grammar automatically constructed from lexically specified frames and introduces a fine-grained differentiation of the complement relation (61 additional grammatical roles). See example (4) for output. NOM;AK AKK; OM NP NP PP (3) Udo hat eine sehr nette Frau aus Rio . If the frame transducer fails, an unspecified grammatical role is left (Carroll et al., 1998). Such roles are counted as correct only in a set of figures that we shall call half-labelled precision and recall. 4.2 Explicit Underspecification An apparent drawback of deterministic parsers is the need for forced guessing, i.e. the need to make decisions without access to the requisite disambiguating knowledge. Cases in point are PP attachment and (sometimes) determination of case 3There are 13 grammatical roles: head, adjunct, apposition, complement, adjunct or complement, conjunction, first part of conjunction, measure phrase, marker, specifier, subject, governed verb, unconnected. in Ge</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser Evaluation: a Survey and a New Proposal. In Proceedings of LREC, pages 447-454, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Evaluating a Wide-Coverage CCG Parser.</title>
<date>2002</date>
<booktitle>In Beyond PARSE VAL -Tivoards Improved Evaluation Measures for Parsign Systems (LREC Workshop).</booktitle>
<contexts>
<context position="2904" citStr="Clark and Hockenmaier, 2002" startWordPosition="450" endWordPosition="453">n and recall). The details deserve some attention. With Kiibler and Telljohann (2002) and in contrast to Lin (1995), we assume that PPs are headed by their internal NPs, and that conjoined phrases have multiple heads (the conjuncts), with the conjunction linked to the last conjunct. Carroll et al. (1998) introduce additional links for control phenomena, map several tokens to one node (e g linked preposition—noun and determiner—noun pairs), and allow nodes for elided words (e.g. in pro-/topic-drop and gapping). An important objection is that the weight of words is determined quite arbitrarily (Clark and Hockenmaier, 2002). Thus, we adopt Lin&apos;s scheme with the above provisos. Training and test sets for the experiments described below were derived from a tokenized version of the Negra tree bank of German newspaper 163 texts (Skut et al., 1997), comprising ca. 340,000 tokens in 19,547 sentences. Different tagging qualities were taken into account by alternatively using Part-Of-Speech tags determined by the Tree Tagger (Schmid, 1994) (tagger tags), POS tags determined by the Tree Tagger trained on the tree bank (lexicon tags), or the POS tags of the tree bank (ideal tags). All experiments were run on a SUN Blade-1</context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>Stephen Clark and Julia Hockenmaier. 2002. Evaluating a Wide-Coverage CCG Parser. In Beyond PARSE VAL -Tivoards Improved Evaluation Measures for Parsign Systems (LREC Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jochen Done</author>
</authors>
<title>Efficient Construction of Underspecified Semantics under Massive Ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL&apos;97,</booktitle>
<pages>386--393</pages>
<location>Madrid,</location>
<contexts>
<context position="9207" citStr="Done, 1997" startWordPosition="1490" endWordPosition="1491">metimes) determination of case 3There are 13 grammatical roles: head, adjunct, apposition, complement, adjunct or complement, conjunction, first part of conjunction, measure phrase, marker, specifier, subject, governed verb, unconnected. in German (cf. example (4)). In context-free parsing, the solution to this problem is conservation of ambiguities in the output: Difficult decisions are delayed to a later stage. Similar techniques can be used in finite-state parsing (Elworthy et al., 2001). Underspecification can be elegantly implemented with context variables (Maxwell III and Kaplan, 1989) (Done, 1997). Since subcategorization ambiguities are specific to main verbs in clauses and never interact across clause boundaries, the clause nodes themselves can be interpreted as context variables. The different options are implicitly encoded by bringing the varying grammatical roles of a constituent node in a clause-wide uniform order (e.g. in example (4) position 1: first NP nominative, second NP accusative; position 2: first NP accusative, second NP nominative). VP coordination sometimes gives rise to structures with constituents figuring in several subcategorization frames at once. In this case se</context>
</contexts>
<marker>Done, 1997</marker>
<rawString>Jochen Done. 1997. Efficient Construction of Underspecified Semantics under Massive Ambiguity. In Proceedings of ACL&apos;97, pages 386-393, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
<author>Tony Rose</author>
<author>Amanda Clare</author>
<author>Aaron Kotcheff</author>
</authors>
<title>A natural language system for retrieval of captioned images.</title>
<date>2001</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>7--2</pages>
<contexts>
<context position="9091" citStr="Elworthy et al., 2001" startWordPosition="1472" endWordPosition="1475">. the need to make decisions without access to the requisite disambiguating knowledge. Cases in point are PP attachment and (sometimes) determination of case 3There are 13 grammatical roles: head, adjunct, apposition, complement, adjunct or complement, conjunction, first part of conjunction, measure phrase, marker, specifier, subject, governed verb, unconnected. in German (cf. example (4)). In context-free parsing, the solution to this problem is conservation of ambiguities in the output: Difficult decisions are delayed to a later stage. Similar techniques can be used in finite-state parsing (Elworthy et al., 2001). Underspecification can be elegantly implemented with context variables (Maxwell III and Kaplan, 1989) (Done, 1997). Since subcategorization ambiguities are specific to main verbs in clauses and never interact across clause boundaries, the clause nodes themselves can be interpreted as context variables. The different options are implicitly encoded by bringing the varying grammatical roles of a constituent node in a clause-wide uniform order (e.g. in example (4) position 1: first NP nominative, second NP accusative; position 2: first NP accusative, second NP nominative). VP coordination someti</context>
</contexts>
<marker>Elworthy, Rose, Clare, Kotcheff, 2001</marker>
<rawString>David Elworthy, Tony Rose, Amanda Clare, and Aaron Kotcheff. 2001. A natural language system for retrieval of captioned images. Journal of Natural Language Engineering, 7(2):117-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kibler</author>
<author>Heike Telljohann</author>
</authors>
<title>Towards a Dependency-Oriented Evaluation for Partial Parsing.</title>
<date>2002</date>
<booktitle>In Beyond PARSE VAL - Towards Improved Evaluation Measures for Parsing Systems (LREC Workshop).</booktitle>
<marker>Kibler, Telljohann, 2002</marker>
<rawString>Sandra Kibler and Heike Telljohann. 2002. Towards a Dependency-Oriented Evaluation for Partial Parsing. In Beyond PARSE VAL - Towards Improved Evaluation Measures for Parsing Systems (LREC Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A Dependency-based Method for Evaluating Broad-Coverage Parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of the IJCAI-95,</booktitle>
<pages>1420--1425</pages>
<location>Montreal.</location>
<contexts>
<context position="1661" citStr="Lin, 1995" startWordPosition="250" endWordPosition="251">ich can only be gained by deterministic processing. Deterministic parsers return exactly one syntactic reading, which forces them to solve many locally unsolvable puzzles. Abney (1997) suggests a way out of this dilemma: The parser leaves ambiguities unresolved if they are contained in a local domain. So at least ambiguities of this kind can conceivably be handed over to some expert disambiguation module. The paper fleshes out this idea and shows its impact on overall performance. 2 Evaluation Method Instead of using the prevalent PARSEVAL measures, we opted for a dependency-based evaluation (Lin, 1995), which is arguably (Srinivas et al., 1996) (Kiibler and Telljohann, 2002) fairer to partial parsers. In a dependency structure, every word token (dependent) is related to another token (head) over a grammatical role, but for one word token, which is called the root. Thus, a parser constructing a dependency structure needs to associate every word token either with a head token plus grammatical role or mark it as the root or &apos;TOP&apos; node. The task can be seen as a classification problem and measured in (labelled) precision and recall. To simplify the task, grammatical roles can be neglected (unla</context>
<context position="3931" citStr="Lin (1995)" startWordPosition="623" endWordPosition="624">(tagger tags), POS tags determined by the Tree Tagger trained on the tree bank (lexicon tags), or the POS tags of the tree bank (ideal tags). All experiments were run on a SUN Blade-1000. 3 Tagging Approach The head tokens in dependency tuples can be coded in several ways. The position method represents a head token by its position in the sentence (poshead). On the Negra tree bank, this method yields 121 unlabelled and 1810 labelled&apos; classes. The distance method codes a head token by giving the distance to the dependent (poshead-posdep), yielding 123 unlabelled but only 1139 labelled classes. Lin (1995) represents the head token by its word type and a position indicator which encodes the direction where the head can be found and the number of tokens of identical type between head and dependent (e.g. &lt; first token with same word type on the left, &gt;&gt;&gt; third token with same word type on the right, etc.). To get fewer classes, we use the category2 of the head token instead of its word type. The resulting method (which we will call nth-tag method) yields 115 unlabelled and 639 labelled classes. For the experiment, the trigram-based Tree Tagger was used to map tokens directly to the dependency cla</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A Dependency-based Method for Evaluating Broad-Coverage Parsers. In Proceedings of the IJCAI-95, pages 1420-1425, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>An overview of disjunctive constraint satisfaction.</title>
<date>1989</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="9194" citStr="Kaplan, 1989" startWordPosition="1488" endWordPosition="1489">achment and (sometimes) determination of case 3There are 13 grammatical roles: head, adjunct, apposition, complement, adjunct or complement, conjunction, first part of conjunction, measure phrase, marker, specifier, subject, governed verb, unconnected. in German (cf. example (4)). In context-free parsing, the solution to this problem is conservation of ambiguities in the output: Difficult decisions are delayed to a later stage. Similar techniques can be used in finite-state parsing (Elworthy et al., 2001). Underspecification can be elegantly implemented with context variables (Maxwell III and Kaplan, 1989) (Done, 1997). Since subcategorization ambiguities are specific to main verbs in clauses and never interact across clause boundaries, the clause nodes themselves can be interpreted as context variables. The different options are implicitly encoded by bringing the varying grammatical roles of a constituent node in a clause-wide uniform order (e.g. in example (4) position 1: first NP nominative, second NP accusative; position 2: first NP accusative, second NP nominative). VP coordination sometimes gives rise to structures with constituents figuring in several subcategorization frames at once. In</context>
</contexts>
<marker>Kaplan, 1989</marker>
<rawString>John T. Maxwell 111 and Ronald M. Kaplan. 1989. An overview of disjunctive constraint satisfaction. In Proceedings of the International Workshop on Parsing Technologies, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunter Neumann</author>
<author>Christian Braun</author>
<author>Jakub Piskorski</author>
</authors>
<title>A Divide-and-Conquer Strategy for Shallow Parsing of German Free Text.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP&apos;00,</booktitle>
<pages>239--246</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="12124" citStr="Neumann et al. (2000)" startWordPosition="1955" endWordPosition="1958">.7 85.9 36.5 T lower 81.5 71.3 83.5 73.0 88.6 77.5 10.6 T upper 88.9 77.7 90.9 79.5 93.6 81.8 31.0 Figure 2: Results for Finite-State Parser on Ideal, Lexicon and Tagger tags. The last column of the table shows the percentage of completely correct analyses of sentences. For the lower bound, only unambiguous sentence analyses count as correct. When we combined chunker and tagger results using the greedy method, performance was boosted to 94.48%/87.28% labelled precision/recall on ideal tags (upper-bound) and 94.36%/86.35% (lower-bound). These figures can be compared with the values reported by Neumann et al. (2000) (precision 89.68%, recall 84.75%) although they used a much smaller corpus for evaluation (10,400 tokens) which was not annotated independently. 5 Conclusion The paper presents a cascaded finite-state parser incorporating some degree of underspecification. The idea is that such syntactically unresolvable ambiguities are later resolved by expert disambiguation modules. The performance of the finitestate parser has been compared with a very simple tagging approach which nevertheless gets more than 50% of the dependency structure correct. I am grateful to Helmut Schmid for discussion and to the </context>
</contexts>
<marker>Neumann, Braun, Piskorski, 2000</marker>
<rawString>Gunter Neumann, Christian Braun, and Jakub Piskorski. 2000. A Divide-and-Conquer Strategy for Shallow Parsing of German Free Text. In Proceedings of ANLP&apos;00, pages 239-246, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of ACE02.</booktitle>
<contexts>
<context position="10595" citStr="Riezler et al. (2002)" startWordPosition="1701" endWordPosition="1704">ia((A;N))] und [hilft Karla((D;N))]]. Hans knows Maria and helps Karla. or: Maria knows and Karla helps Hans. or: Maria knows Hans and he helps Karla. or: Hans knows Maria and Karla helps him. In a final processing step, the constituent trees are converted into dependency tuples. In this step, attachment and subcategorization ambiguities are overtly represented with context variables, cf. (6): (5) Lido/0 hat/1 [1a]:NPnom,[1b]:NPakk hat/1 TOP eine/2 Frau/5 SPR sehr/3 nette/4 ADJ nette/4 Frau/5 ADJ Frau/5 hat/1 [1a]:NPakk,[1b]:NPnom aus/6 Rio/7 MRK Rio/7 hat/1 ADJ [lAO] Frau/5 ADJ [1A1] ./8 TOP Riezler et al. (2002) evaluate underspecified syntactic representations by distinguishing lower bound performance (random choice of a parse) CMP &amp;quot;nom;dat;akk ADJP MR ADIVD (6) Udo hat eine sehr nette Frau aus Rio . ADJ 165 References Steven Abney. 1997. Partial Parsing via Finite-State Cascades. Journal of Natural Language Engineering, 2(4):337-344. and upper bound performance (selection of the best parse according to the test set). 4.3 Evaluation Results Currently, the speed of the finite-state parser is at 2430 Words Per Second, but this figure can still be improved by compiling the backtracking necessitated in </context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques. In Proceedings of ACE02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schiehlen</author>
</authors>
<title>Experiments in German Noun Chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING &apos;02,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="6489" citStr="Schiehlen, 2002" startWordPosition="1066" endWordPosition="1067">d 78.02%172.83% on Tagger tags. 4 Cascaded Finite-State Parser 4.1 Description of the Parser The parser described here essentially relies on techniques also used by Abney (1997). It basically consists of a noun chunker and a clause chunker. The noun chunker is deterministic, but recognizes recursive noun chunks in several passes. Morphological information on case, number and gender coded is computed with bit vectors (Abney, 1997). A noun chunk is defined as an NP or PP with all adjuncts at the beginning (e.g. adverbs) and at the end (e.g. PPs and relative clauses) stripped off (Brants, 1999) (Schiehlen, 2002). The clause chunker consists of three deterministic transducers recognizing verb-final, verb-first, and verb-second clauses. The parser aims to determine full clauses rather than the &amp;quot;simplex clauses&amp;quot; of Abney (1997) (i.e. non-recursive &amp;quot;core&amp;quot; parts of clauses). The verb-final clause transducer e.g. works from right to left so that subclauses are maximally embedded. Example (1) shows chunker output (a flat parse tree) after the recognition phase. NPriom,dat,akk NPnom,akk (1) Udo hat eine sehr nette Frau aus Rio . Udo has a very nice wife from Rio. or: A very nice woman has Udo from Rio. or: A</context>
</contexts>
<marker>Schiehlen, 2002</marker>
<rawString>Michael Schiehlen. 2002. Experiments in German Noun Chunking. In Proceedings of COLING &apos;02, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-Of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<tech>Technical report, Institut fiir maschinelle</tech>
<institution>Sprachverarbeitung, Universitat Stuttgart.</institution>
<contexts>
<context position="3320" citStr="Schmid, 1994" startWordPosition="518" endWordPosition="519">noun pairs), and allow nodes for elided words (e.g. in pro-/topic-drop and gapping). An important objection is that the weight of words is determined quite arbitrarily (Clark and Hockenmaier, 2002). Thus, we adopt Lin&apos;s scheme with the above provisos. Training and test sets for the experiments described below were derived from a tokenized version of the Negra tree bank of German newspaper 163 texts (Skut et al., 1997), comprising ca. 340,000 tokens in 19,547 sentences. Different tagging qualities were taken into account by alternatively using Part-Of-Speech tags determined by the Tree Tagger (Schmid, 1994) (tagger tags), POS tags determined by the Tree Tagger trained on the tree bank (lexicon tags), or the POS tags of the tree bank (ideal tags). All experiments were run on a SUN Blade-1000. 3 Tagging Approach The head tokens in dependency tuples can be coded in several ways. The position method represents a head token by its position in the sentence (poshead). On the Negra tree bank, this method yields 121 unlabelled and 1810 labelled&apos; classes. The distance method codes a head token by giving the distance to the dependent (poshead-posdep), yielding 123 unlabelled but only 1139 labelled classes.</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-Of-Speech Tagging Using Decision Trees. Technical report, Institut fiir maschinelle Sprachverarbeitung, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An Annotation Scheme for Free Word Order Languages.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP-97,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="3128" citStr="Skut et al., 1997" startWordPosition="490" endWordPosition="493">with the conjunction linked to the last conjunct. Carroll et al. (1998) introduce additional links for control phenomena, map several tokens to one node (e g linked preposition—noun and determiner—noun pairs), and allow nodes for elided words (e.g. in pro-/topic-drop and gapping). An important objection is that the weight of words is determined quite arbitrarily (Clark and Hockenmaier, 2002). Thus, we adopt Lin&apos;s scheme with the above provisos. Training and test sets for the experiments described below were derived from a tokenized version of the Negra tree bank of German newspaper 163 texts (Skut et al., 1997), comprising ca. 340,000 tokens in 19,547 sentences. Different tagging qualities were taken into account by alternatively using Part-Of-Speech tags determined by the Tree Tagger (Schmid, 1994) (tagger tags), POS tags determined by the Tree Tagger trained on the tree bank (lexicon tags), or the POS tags of the tree bank (ideal tags). All experiments were run on a SUN Blade-1000. 3 Tagging Approach The head tokens in dependency tuples can be coded in several ways. The position method represents a head token by its position in the sentence (poshead). On the Negra tree bank, this method yields 121</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An Annotation Scheme for Free Word Order Languages. In Proceedings of the ANLP-97, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
<author>Christine Doran</author>
<author>Beth Ann Hockey</author>
<author>Aravind Joshi</author>
</authors>
<title>An approach to Robust Partial Parsing and Evaluation Metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI96 Workshop on Robust Parsing,</booktitle>
<pages>70--82</pages>
<location>Prague.</location>
<contexts>
<context position="1704" citStr="Srinivas et al., 1996" startWordPosition="255" endWordPosition="258">ministic processing. Deterministic parsers return exactly one syntactic reading, which forces them to solve many locally unsolvable puzzles. Abney (1997) suggests a way out of this dilemma: The parser leaves ambiguities unresolved if they are contained in a local domain. So at least ambiguities of this kind can conceivably be handed over to some expert disambiguation module. The paper fleshes out this idea and shows its impact on overall performance. 2 Evaluation Method Instead of using the prevalent PARSEVAL measures, we opted for a dependency-based evaluation (Lin, 1995), which is arguably (Srinivas et al., 1996) (Kiibler and Telljohann, 2002) fairer to partial parsers. In a dependency structure, every word token (dependent) is related to another token (head) over a grammatical role, but for one word token, which is called the root. Thus, a parser constructing a dependency structure needs to associate every word token either with a head token plus grammatical role or mark it as the root or &apos;TOP&apos; node. The task can be seen as a classification problem and measured in (labelled) precision and recall. To simplify the task, grammatical roles can be neglected (unlabelled precision and recall). The details d</context>
<context position="4587" citStr="Srinivas et al., 1996" startWordPosition="739" endWordPosition="743">word type and a position indicator which encodes the direction where the head can be found and the number of tokens of identical type between head and dependent (e.g. &lt; first token with same word type on the left, &gt;&gt;&gt; third token with same word type on the right, etc.). To get fewer classes, we use the category2 of the head token instead of its word type. The resulting method (which we will call nth-tag method) yields 115 unlabelled and 639 labelled classes. For the experiment, the trigram-based Tree Tagger was used to map tokens directly to the dependency classes (see for a similar approach (Srinivas et al., 1996)). Performance was degraded when the tagger got information on both word type and POS tag of the tokens, so we only used POS tag. We didn&apos;t test the position method. Figure 1 shows results achieved via 10-fold crossvalidation with Ideal and Tagger tags. The tagger always gives a unique answer, but head tokens not found in the string count as not assigned, hence the discrepancy between precision and recall. A figure is also given for the percentage of sentences getting a completely correct parse. 1NEGRA distinguishes 33 grammatical roles. 2Better performance is achieved when only the category i</context>
</contexts>
<marker>Srinivas, Doran, Hockey, Joshi, 1996</marker>
<rawString>B. Srinivas, Christine Doran, Beth Ann Hockey, and Aravind Joshi. 1996. An approach to Robust Partial Parsing and Evaluation Metrics. In Proceedings of the ESSLLI96 Workshop on Robust Parsing, pages 70-82, Prague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>