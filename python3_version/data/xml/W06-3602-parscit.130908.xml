<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.975704">
Efficient Dynamic Programming Search Algorithms for Phrase-Based SMT
</title>
<author confidence="0.870436">
Christoph Tillmann
</author>
<affiliation confidence="0.603058">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.605272">
Yorktown Heights, NY 10598
</address>
<email confidence="0.996498">
ctill@us.ibm.com
</email>
<sectionHeader confidence="0.995594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999609">
This paper presents a series of efficient
dynamic-programming (DP) based algorithms
for phrase-based decoding and alignment
computation in statistical machine translation
(SMT). The DP-based decoding algorithms are
analyzed in terms of shortest path-finding al-
gorithms, where the similarity to DP-based
decoding algorithms in speech recognition is
demonstrated. The paper contains the follow-
ing original contributions: 1) the DP-based de-
coding algorithm in (Tillmann and Ney, 2003)
is extended in a formal way to handle phrases
and a novel pruning strategy with increased
translation speed is presented 2) a novel align-
ment algorithm is presented that computes a
phrase alignment efficiently in the case that it
is consistent with an underlying word align-
ment. Under certain restrictions, both algo-
rithms handle MT-related problems efficiently
that are generally NP complete (Knight, 1999).
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731461538461">
This paper deals with dynamic programming based de-
coding and alignment algorithms for phrase-based SMT.
Dynamic Programming based search algorithms are be-
ing used in speech recognition (Jelinek, 1998; Ney et
al., 1992) as well as in statistical machine translation
(Tillmann et al., 1997; Niessen et al., 1998; Tillmann
and Ney, 2003). Here, the decoding algorithms are de-
scribed as shortest path finding algorithms in regularly
structured search graphs or search grids. Under certain
restrictions, e.g. start and end point restrictions for the
path, the shortest path computed corresponds to a rec-
ognized word sequence or a generated target language
translation. In these algorithms, a shortest-path search
</bodyText>
<page confidence="0.990606">
9
</page>
<figureCaption confidence="0.839485">
Figure 1: Illustration of a DP-based algorithm to solve
a traveling salesman problem with cities. The visited
cities correspond to processed source positions.
</figureCaption>
<bodyText confidence="0.998703684210526">
is carried out in one pass over some input along a spe-
cific ’direction’: in speech recognition the search is time-
synchronous, the single-word based search algorithm in
(Tillmann et al., 1997) is (source) position-synchronous
or left-to-right, the search algorithm in (Niessen et al.,
1998) is (target) position-synchronous or bottom-to-top,
and the search algorithm in (Tillmann and Ney, 2003) is
so-called cardinality-synchronous.
Taking into account the different word order between
source and target language sentences, it becomes less ob-
vious that a SMT search algorithm can be described as a
shortest path finding algorithm. But this has been shown
by linking decoding to a dynamic-programming solution
for the traveling salesman problem. This algorithm due
to (Held and Karp, 1962) is a special case of a shortest
path finding algorithm (Dreyfus and Law, 1977). The
regularly structured search graph for this problem is il-
lustrated in Fig. 1: all paths from the left-most to the
right-most vertex correspond to a translation of the in-
</bodyText>
<figure confidence="0.980614685714286">
((1),1)
((1,2),2)
((1,4),4)
((1,5),5)
((1,3),3)
((1,2,3),2)
((1,2,4),2)
((1,3,4),3)
((1,4,5),5)
((1,2,5),2)
((1,3,5),3)
((1,4,5),4)
((1,3,4),4)
((1,3,5),5)
((1,2,3),3)
((1,2,4),4)
((1,2,5),5)
((1,2,3,5),5)
((1,2,4,5),5)
((1,3,4,5),5)
((1,2,3,4),4)
((1,2,4,5),4)
((1,3,4,5),4)
((1,2,3,4),2)
((1,2,3,5),2)
((1,2,4,5),2)
((1,2,3,4),3)
((1,2,3,5),3)
((1,3,4,5),3)
((1,2,3,4,5),2)
((1,2,3,4,5),3)
((1,2,3,4,5),5)
((1,2,3,4,5),4)
Final
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 9–16,
</figure>
<subsectionHeader confidence="0.492439">
New York City, New York, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999238307692308">
put sentence, where each source position is processed ex-
actly once. In this paper, the DP-based search algorithm
in (Tillmann and Ney, 2003) is extended in a formal way
to handle phrase-based translation. Two versions of a
phrase-based decoder for SMT that search slightly dif-
ferent search graphs are presented: a multi-beam decoder
reported in the literature and a single-beam decoder with
increased translation speed 1. A common analysis of all
the search algorithms above in terms of a shortest-path
finding algorithm for a directed acyclic graph (dag) is
presented. This analysis provides a simple way of ana-
lyzing the complexity of DP-based search algorithm.
Generally, the regular search space can only be fully
searched for small search grids under appropriate restric-
tions, i.e. the monotonicity restrictions in (Tillmann et
al., 1997) or the inverted search graph in (Niessen et al.,
1998). For larger search spaces as are required for con-
tinuous speech recognition (Ney et al., 1992) 2 or phrase-
based decoding in SMT, the search space cannot be fully
searched: suitably defined lists of path hypothesis are
maintained that partially explore the search space. The
number of hypotheses depends locally on the number hy-
potheses whose score is close to the top scoring hypothe-
sis: this set of hypotheses is called the beam.
The translation model used in this paper is a phrase-
based model, where the translation units are so-called
blocks: a block is a pair of phrases which are transla-
tions of each other. For example, Fig. 2 shows an Arabic-
English translation example that uses blocks. During
decoding, we view translation as a block segmentation
process, where the input sentence is segmented from left
to right and the target sentence is generated from bottom
to top, one block at a time. In practice, a largely mono-
tone block sequence is generated except for the possibil-
ity to swap some neighbor blocks. During decoding, we
try to minimize the score of a block sequence
under the restriction that the concatenated source phrases
of the blocks yield a segmentation of the input sen-
tence:
</bodyText>
<equation confidence="0.966672">
(1)
</equation>
<bodyText confidence="0.993249">
Here, is -dimensional feature vector with
real-valued features and is the corresponding weight
vector as described in Section 5. The fact that a given
block covers some source interval is implicit in this
notation.
</bodyText>
<footnote confidence="0.993720714285714">
1The multi-beam decoder is similar to the decoder presented
in (Koehn, 2004) which is a standard decoder used in phrase-
based SMT. A multi-beam decoder is also used in (Al-Onaizan
et al., 2004) and (Berger et al., 1996).
2In that work, there is a distinction between within-word and
between-word search, which is not relevant for phrase-based
decoding where only exact phrase matches are searched.
</footnote>
<figureCaption confidence="0.937455666666667">
Figure 2: An Arabic-English block translation example,
where the Arabic words are romanized. A sequence of
blocks is generated.
</figureCaption>
<bodyText confidence="0.9999173">
This paper is structured as follows: Section 2 intro-
duces the multi-beam and the single-beam DP-based de-
coders. Section 3 presents an analysis of all the graph-
based shortest-path finding algorithm mentioned above:
a search algorithm for a directed acyclic graph (dag).
Section 4 shows an efficient phrasal alignment algorithm
that gives an algorithmic justification for learning blocks
from word-aligned training. Finally, Section 5 presents
an evaluation of the beam-search decoders on an Arabic-
English decoding task.
</bodyText>
<sectionHeader confidence="0.85523" genericHeader="method">
2 Beam-Search Decoding Algorithms
</sectionHeader>
<bodyText confidence="0.99959975">
In this section, we introduce two beam-search algorithms
for SMT: a multi-beam algorithm and single-beam algo-
rithm. The multi-beam search algorithm is presented first,
since it is conceptually simpler.
</bodyText>
<subsectionHeader confidence="0.989347">
2.1 Multi-Beam Decoder
</subsectionHeader>
<bodyText confidence="0.8905073">
For the multi-beam decoder makes use of search states
that are -tuples of the following type:
(2)
is the state history, that depends on the block generation
model. In our case, , where is
the interval where the most recent block matched the in-
put sentence, and are the final two target words of
the partial translation produced thus far. is the so-called
coverage vector that ensures that a consistent block align-
ment is obtained during decoding and that the decoding
</bodyText>
<page confidence="0.999404">
10
</page>
<tableCaption confidence="0.982546">
Table 1: Multi-beam ( -Beam) decoding algorithm,
</tableCaption>
<bodyText confidence="0.973574894736842">
which is similar to (Koehn, 2004). The decoders differ in
their pruning strategy: here, each state list is pruned
only once, whereas the decoder in (Koehn, 2004) prunes
a state list every time a new hypothesis is entered.
input: source sentence with words
can be carried out efficiently. It keeps track of the already
processed input sentence positions. is the cost of the
shortest path (distance) from some initial state to the
current state . The baseline decoder maintains
state lists with entries of the above type, where is the
number of input words. The states are stored in lists or
stacks that support lookup operations to check whether a
given state tuple is already present in a list and what its
score is.
The use of a coverage vector is related to a DP-based
solution for the traveling salesman problem as illustrated
in Fig. 1. The algorithm keeps track of sets of visited
cities along with the identity of the last visited city. Cities
correspond to source sentence positions . The vertexes
in this graph correspond to set of already visited cities.
Since the traveling salesman problem (and also the trans-
lation model) uses only local costs, the order in which
the source positions have been processed can be ignored.
Conceptually, the re-ordering problem is linearized by
searching a path through the set inclusion graph in Fig. 1.
Phrase-based decoding is handle by an almost identical
algorithm: the last visited position is replaced by an
interval .
The states are stored in lists or stacks that support
lookup operations to check whether a given state tuple is
already present in a list and what its score is. Extending
the partial block translation that is represented by a state
with a single block generates a new state . Here,
is the source interval where block matches the
input sentence. The state transition is defined as follows:
(3)
The state fields are updated on a component-by-
component basis. is the coverage vec-
</bodyText>
<tableCaption confidence="0.755131">
Table 2: Single-beam ( -Beam) decoding algorithm (re-
lated to (Lowerre and Reddy, 1980)).
</tableCaption>
<bodyText confidence="0.999572344827586">
tor obtained by adding all the positions from the inter-
val . The new state history is defined as
where and are the final two tar-
get words of the target phrase of . Some special
cases, e.g. where has less than two target words, are
taken into account. The path cost is computed as
, where the transition cost
is computed from the history and the matching block
as defined in Section 5.
The decoder in Table 1 fills state sets
. All the coverage vectors for states in the set
cover the same number of source positions . When
a state set is processed, the decoder has finished pro-
cessing all states in the sets where . Before ex-
panding a state set, the decoder prunes a state set based on
its coverage vector and the path costs only: two different
pruning strategies are used that have been introduced in
(Tillmann and Ney, 2003): 1) coverage pruning prunes
states that share the same coverage vector , 2) cardi-
nality pruning prunes states according to the cardinal-
ity of covered positions: all states in the beam are
compared with each other. Since the states are kept in
separate lists, which are pruned independently of
each others, this decoder version is called multi-beam
decoder. The decoder uses a matcher function when ex-
panding a state: for a state it looks for uncovered source
positions to find source phrase matches for blocks. Up-
dating a state in Table 1 includes adding the state if it is
not yet present or updating its shortest path cost : if the
</bodyText>
<figure confidence="0.861110173913043">
and for
for each do
Prune state set
for each state in do
matcher: for each
update for
end
end
output: translation from lowest cost state in
output: translation from lowest cost state in
input: source sentence with words
for each do
for each state in do
if CLOSED? then
matcher: for each
else
scanner: for single
update for
end
Prune state set
Swap ,
end
end
</figure>
<page confidence="0.991636">
11
</page>
<bodyText confidence="0.9991081875">
state is already in only the state with the lower path
cost is kept. This inserting/updating operation is also
called recombination or relaxation in the context of a
dag search algorithm (cf. Section 3). The update proce-
dure also stores for each state its predecessor state in a
so-called back-pointer array (Ney et al., 1992). The final
block alignment and target translation can be recovered
from this back-pointer array once the final state set
has been computed. is the source phrase length of
the matching block when going from to . This al-
gorithm is similar to the beam-search algorithm presented
in (Koehn, 2004): it allows states to be added to a stack
that is not the stack for the successor cardinality. is the
initial decoder state, where no source position is covered:
. For the final states in all source positions are
covered.
</bodyText>
<subsectionHeader confidence="0.952626">
2.2 Single-Beam Implementation
</subsectionHeader>
<bodyText confidence="0.999238577777778">
The second implementation uses two lists to keep a single
beam of active states. This corresponds to a beam-search
decoder in speech recognition, where path hypotheses
corresponding to word sequences are processed in a time-
synchronous way and at a given time step only hypothe-
ses within some percentage of the best hypothesis are
kept (Lowerre and Reddy, 1980). The single-beam de-
coder processes hypotheses cardinality-synchronously,
i.e. the states at stage generate new states at position
. In order to make the use of a single beam possible,
we slightly modify the state transitions in Eq. 3:
Here, Eq. 5 corresponds to the matcher definition in Eq. 3.
We add an additional field that is a pointer keeping track
of how much of the recent source phrase match has been
covered. In Eq. 5, when a block is matched to the input
sentence, this pointer is set to position k where the most
recent block match starts. We use a dot to indicate that
when a block is matched, the matching position of the
predecessor state can be ignored. While the pointer is
not yet equal to the end position of the match , it is in-
creased as shown in Eq. 4. The path cost
is set: , where is the state transition cost
divided by the source phrase length of block :
we evenly spread the cost of generating over all source
positions being matched. The new coverage vector
is obtained from by adding the scanned position :
. The algorithm that makes use of the above
definitions is shown in Table 2. The states are stored in
only two state sets and : contains the most prob-
able hypotheses that were kept in the last beam pruning
step all of which cover source positions. contains all
the hypotheses in the current beam that cover source
positions. The single-beam decoder in Table 2 uses two
procedures: the scanner and the matcher correspond to
the state transitions in Eq. 4 and Eq. 5. Here, the matcher
simply matches a block to an uncovered portion of the
input sentence. After the matcher has matched a block,
that block is processed in a cardinality-synchronous way
using the scanner procedure as described above. The
predicate CLOSED is used to switch between match-
ing and scanning states. The predicate CLOSED is
true if the pointer is equal to the match end position
(this is stored in ). At this point, the position-by-
position match of the source phrase is completed and we
can search for additional block matches.
</bodyText>
<sectionHeader confidence="0.996338" genericHeader="method">
3 DP Shortest Path Algorithm for dag
</sectionHeader>
<bodyText confidence="0.999927">
This section analyzes the relationship between the block
decoding algorithms in this paper and a single-source
shortest path finding algorithm for a directed acyclic
graphs (dag). We closely follow the presentation in (Cor-
men et al., 2001) and only sketch the algorithm here: a
dag is a weighted graph for which a topolog-
ical sort of its vertex set exists: all the vertexes can be
enumerated in linear order. For such a weighted graph,
the shortest path from a single source can be computed
in time, where is the number of ver-
texes and number of edges in the graph. The dag
search algorithm runs over all vertexes in topological
order. Assuming an adjacency-list representation of the
dag, for each vertex , we loop over all successor ver-
texes , where each vertex with its adjacency-list is
processed exactly once. During the search, we maintain
for each vertex an attribute , which is an upper
bound on the shortest path cost from the source vertex
to the vertex . This shortest path estimate is updated
or relaxed each time the vertex occurs in some adja-
cency list. Ignoring the pruning, the -Beam decoding
algorithm in Table 1 and the dag search algorithm can be
compared as follows: states correspond to dag vertexes
and state transitions correspond to dag edges. Using two
loops for the multi-beam decoder while generating states
in stages is just a way of generating a topological sort of
the search states on the fly: a linear order of search states
is generated by appending the search states in the state
lists , , etc. .
The analysis in terms of a dag shortest path algorithm
can be used for a simple complexity analysis of the pro-
posed algorithms. Local state transitions correspond to
an adjacency-list traversal in the dag search algorithm.
These involve costly lookup operations, e.g. language,
distortion and translation model probability lookup. Typ-
ically the computation time for update operations on lists
is negligible compared to these probability lookups.
So, the search algorithm complexity is simply computed
as the number of edges in the search graph:
(this analysis is implicit in (Tillmann,
</bodyText>
<page confidence="0.997193">
12
</page>
<bodyText confidence="0.999915526315789">
2001)). Without proof, for the search algorithm in Sec-
tion 2.1 we observe that the number of states is finite and
that all the states are actually reachable from the start
state . This way for the single-word based search in
(Tillmann and Ney, 2003), a complexity of
is shown, where is the size of the target vo-
cabulary and is the length of the input sentence. The
complexity is dominated by the exponential number of
coverage vectors that occur in the search, and the com-
plexity of phrase-based decoding is higher yet since its
hypotheses store a source interval rather than a sin-
gle source position . In the general case, no efficient
search algorithm exists to search all word or phrase re-
orderings (Knight, 1999). Efficient search algorithms can
be derived by the restricting the allowable coverage vec-
tors (Tillmann, 2001) to local word re-ordering only. An
efficient phrase alignment method that does not make use
of re-ordering restriction is demonstrated in the following
section.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="method">
4 Efficient Block Alignment Algorithm
</sectionHeader>
<bodyText confidence="0.99984892">
A common approach to phrase-based SMT is to learn
phrasal translation pairs from word-aligned training data
(Och and Ney, 2004). Here, a word alignment is a
subset of the Cartesian product of source and target posi-
tions:
Here, is the target sentence length and is the source
sentence length. The phrase learning approach in (Och
and Ney, 2004) takes two alignments: a source-to-target
alignment and a target-to-source alignment . The
intersection of these two alignments is computed to ob-
tain a high-precision word alignment. Here, we note that
if the intersection covers all source and target positions
(as shown in Fig. 4), it constitutes a bijection between
source and target sentence positions, since the intersect-
ing alignments are functions according to their definition
in (Brown et al., 1993) 3. In this paper, an algorithmic jus-
tification for restricting blocks based on word alignments
is given. We assume that source and target sentence are
given, and the task is to compute the lowest scoring block
alignment. Such an algorithm might be important in some
discriminative training procedure that relies on decoding
the training data efficiently.
To restrict the block selection based on word aligned
training data, interval projection functions are defined as
follows 4: is a source interval and is an target inter-
</bodyText>
<tableCaption confidence="0.5623222">
3(Tillmann, 2003) reports an intersection coverage of about
% for Arabic-English parallel data, and a coverage of
% for Chinese-English data. In the case of uncomplete cov-
erage, the current algorithm can be extended as described in
Section 4.1.
</tableCaption>
<footnote confidence="0.7710805">
4(Och and Ney, 2004) defines the notion of consistency
for the set of phrasal translations that are learned from word-
</footnote>
<figureCaption confidence="0.900941333333333">
Figure 3: Following the definition in Eq. 6, the left pic-
ture shows three admissible block links while the right
picture shows three non-admissible block links.
</figureCaption>
<bodyText confidence="0.978489131578947">
val. is the set of target positions such that the
alignment point occurs in the alignment set and
is covered by the source interval . is defined
accordingly. Formally, the definitions look like this:
In order to obtain a particularly simple block alignment
algorithm, the allowed block links are restricted
by an ADMISSIBILITY restriction, which is defined as fol-
lows:
is admissible iff (6)
and
Admissibility is related to the word re-ordering problem:
for the source positions in an interval and for the target
positions in an interval , all word re-ordering involving
these positions has to take place within the block defined
by and . Without an underlying alignment each
pair of source and target intervals would define a possi-
ble block link: the admissibility reduces the number of
block links drastically. Examples of admissible and non-
admissible blocks are shown in Fig. 3.
If the alignment is a bijection, by definition each tar-
get position is aligned to exactly one source position
and vice versa and source and target sentence have the
same length. Because of the admissibility definition, a
target interval clumping alone is sufficient to determine
the source interval clumping and the clump alignment.
In Fig. 4, a bijection word alignment for a sentence pair
that consists of source and target words is
shown, where the alignment links that yield a bijection
are shown as solid dots. Four admissible block align-
ments are shown as well. An admissible block alignment
is always guaranteed to exist: the block that covers all
source and target position is admissible by definition. The
underlying word alignment and the admissibility restric-
tion play together to reduce the number of block align-
ments: out of all eight possible target clumpings, only
aligned training data which is equivalent.
and
and
</bodyText>
<page confidence="0.998928">
13
</page>
<tableCaption confidence="0.80250625">
Table 3: Efficient DP-based block alignment algorithm
using an underlying word alignment . For simplicity
reasons, the block score is computed based on the
block identity only.
</tableCaption>
<figureCaption confidence="0.962947333333333">
Figure 5: An example for a block alignment involving
a non-aligned column. The right-most alignment is not
allowed by the closure restriction.
</figureCaption>
<bodyText confidence="0.994803333333333">
target clumping is generated sequentially from bottom-
to-top and it induces some source clumping in an order
which is defined by the word alignment.
</bodyText>
<subsectionHeader confidence="0.976491">
4.1 Incomplete Bijection Coverage
</subsectionHeader>
<bodyText confidence="0.945918">
In this section, an algorithm is sketched that works if
the intersection coverage is not complete. In this case,
a given target interval may produce several admissible
block links since it can be coupled with different source
intervals to form admissible block links, e.g. in Fig. 5, the
target interval is linked to two source intervals and
both resulting block links do not violate the admissibility
restriction. The minimum score block translation can be
computed using either the one-beam or the multi-beam
algorithm presented earlier. The search state definition in
Eq. 2 is modified to keep track of the current target posi-
tion the same way as the recursive quantity does
this in the algorithm in Table 3:
input: Parallel sentence pair and alignment .
initialization:
</bodyText>
<figure confidence="0.990182125">
, where
if block results from admissible
block link , where
traceback:
- find best end hypothesis:
for .
for each do
(7)
</figure>
<figureCaption confidence="0.985753">
Figure 4: Four admissible block alignments in case the
word alignment intersection is a bijection. The block
alignment which covers the whole sentence pair with a
single block is not shown.
</figureCaption>
<bodyText confidence="0.993451454545455">
five yield segmentations with admissible block links.
The DP-based algorithm to compute the block sequence
with the highest score is shown in Table 3. Here, the
following auxiliary quantity is used:
:= score of the best partial segmentation
that covers the target interval .
Target intervals are processed from bottom to top. A
target interval is projected using the word
alignment , where a given target interval might not yield
an admissible block. For the initialization, we set
and the final score is obtained as . The
complexity of the algorithm is where the time to
compute the cost and the time to compute the inter-
val projections are ignored. Using the alignment links ,
the segmentation problem is essentially linearized: the
Additionally, a complex block history as defined in Sec-
tion 2 can be used. Before the search is carried out, the set
of admissible block links for each target interval is pre-
computed and stored in a table where a simple look-up
for each target interval is carried out during align-
ment. The efficiency of the block alignment algorithm
depends on the alignment intersection coverage.
</bodyText>
<sectionHeader confidence="0.97435" genericHeader="method">
5 Beam-Search Results
</sectionHeader>
<bodyText confidence="0.9943595">
In this section, we present results for the beam-search
algorithms introduced in Section 2. The MT03 Arabic-
English NIST evaluation test set consisting of sen-
tences with Arabic words is used for the experi-
ments. Translation results in terms of uncased BLEU us-
ing reference translations are reported in Table 4 and
Table 5 for the single-beam ( -Beam) and the multi-
beam ( -Beam) search algorithm. For all re-ordering
experiments, the notion of skips is used (Tillmann and
Ney, 2003) to restrict the phrase re-ordering: the number
of skips restricts the number of holes in the coverage vec-
tor for a left-to-right traversal of the input sentence. All
</bodyText>
<page confidence="0.99956">
14
</page>
<tableCaption confidence="0.9355435">
Table 4: Effect of the skip parameter for the two search
strategies. and window width .
</tableCaption>
<table confidence="0.99443925">
Skip BLEU CPU BLEU CPU
-Beam [secs] -Beam [secs]
0
1
</table>
<page confidence="0.544283">
2
3
</page>
<bodyText confidence="0.992251627906977">
re-ordering takes place in a window of size , such
that only local block re-ordering is handled.
The following block bigram scoring is used: a
block pair with corresponding source phrase
matches is represented as a feature-vector
. The feature-vector components are
the negative logarithm of some probabilities as well as
a word-penalty feature. The real-valued features in-
clude the following: a block translation score derived
from phrase occurrence statistics , a trigram language
model to predict target words , a lexical weight-
ing score for the block internal words , a distortion
model as well as the negative target phrase length
. The transition cost is computed as
, where is a weight vector that sums
up to : . The weights are trained us-
ing a procedure similar to (Och, 2003) on held-out test
data. A block set of million blocks, which are not
filtered according to any particular test set is used, which
has been generated by a phrase-pair selection algorithm
similar to (Al-Onaizan et al., 2004). The training data is
sentence-aligned consisting of million training sen-
tence pairs.
Beam-search results are presented in terms of two
pruning thresholds: the coverage pruning threshold
and the cardinality pruning threshold (Tillmann and
Ney, 2003). To carry out the pruning, the minimum cost
with respect to each coverage set and cardinality are
computed for a state set . For the coverage pruning,
states are distinguished according to the subset of cov-
ered positions . The minimum cost is defined
as: . For the cardinality
pruning, states are distinguished according to the cardi-
nality of subsets of covered positions. The min-
imum cost is defined for all hypotheses with the
same cardinality : .
States in are pruned if the shortest path cost is
greater than the minimum cost plus the pruning threshold:
The same state set pruning is used for the -Beam and
Table 5: Effect of the coverage pruning threshold on
BLEU and the overall CPU time [secs]. To restrict the
overall search space the cardinality pruning is set to
and the cardinality histogram pruning is set to .
</bodyText>
<table confidence="0.996426818181818">
BLEU CPU BLEU CPU
-Beam [secs] -Beam [secs]
0.001 106 198
0.01 109 213
0.05 139 301
0.1 215 508
0.25 1018 1977
0.5 4527 6289
1.0 6623 8092
2.5 6797 8187
5.0 6810 8191
</table>
<bodyText confidence="0.998055896551724">
the -Beam search algorithms. Table 4 shows the ef-
fect of the skip size on the translation performance. The
pruning thresholds are set to conservatively large values:
and . Only if no block re-ordering
is allowed ( ), performance drops significantly.
The -Beam search is consistently faster than -Beam
search algorithm. Table 5 demonstrates the effect of the
coverage pruning threshold. Here, a conservatively large
cardinality pruning threshold of and the so-
called histogram pruning to restrict the overall number
of states in the beam to a maximum number of
are used to restrict the overall search space. The -
Beam search algorithm is consistently faster than the -
Beam search algorithm for the same pruning threshold,
but performance in terms of BLEU score drops signifi-
cantly for lower coverage pruning thresholds as
a smaller portion of the overall search space is searched
which leads to search errors. For larger pruning thresh-
olds , where the performance of the two algo-
rithms in terms of BLEU score is nearly identical, the
-Beam algorithm runs significantly faster. For a cover-
age threshold of , the -Beam algorithm is as
fast as the -Beam algorithm at , but obtains a
significantly higher BLEU score of versus for
the -Beam algorithm. The results in this section show
that the -Beam algorithm generally runs faster since the
beam search pruning is applied to all states simultane-
ously making more efficient use of the beam search con-
cept.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9985285">
The decoding algorithm shown here is most similar to
the decoding algorithms presented in (Koehn, 2004) and
(Och and Ney, 2004), the later being used for the Align-
ment Template Model for SMT. These algorithms also
</bodyText>
<page confidence="0.992464">
15
</page>
<bodyText confidence="0.999995851851852">
include an estimate of the path completion cost which
can easily be included into this work as well ((Tillmann,
2001)). (Knight, 1999) shows that the decoding problem
for SMT as well as some bilingual tiling problems are
NP-complete, so no efficient algorithm exists in the gen-
eral case. But using DP-based optimization techniques
and appropriate restrictions leads to efficient DP-based
decoding algorithms as shown in this paper.
The efficient block alignment algorithm in Section 4 is
related to the inversion transduction grammar approach to
bilingual parsing described in (Wu, 1997): in both cases
the number of alignments is drastically reduced by in-
troducing appropriate re-ordering restrictions. The list-
based decoding algorithms can also be compared to an
Earley-style parsing algorithm that processes list of parse
states in a single left-to-right run over the input sentence.
For this algorithm, the comparison in terms of a shortest-
path algorithm is less obvious: in the so-called comple-
tion step the parser re-visits states in previous stacks. But
it is interesting to note that there is no multiple lists vari-
ant of that parser. In phrase-based decoding, a multiple
list decoder is feasible only because exact phrase matches
occur. A block decoding algorithm that would allow for
a ’fuzzy’ match of source phrases, e.g. insertions or dele-
tions of some source phrase words are allowed, would
need to carry out its computations using two stacks since
the match end of a block is unknown.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="conclusions">
7 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999933">
This work was partially supported by DARPA and mon-
itored by SPAWAR under contract No. N66001-99-2-
8916. The author would like to thank the anonymous
reviewers for their detailed criticism on this paper.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931537313433">
Yaser Al-Onaizan, Niyu Ge, Young-Suk Lee, Kishore Pa-
pineni, Fei Xia, and Christoph Tillmann. 2004. IBM
Site Report. In NIST 2004 MT Workshop, Alexandria,
VA, June. IBM.
Adam L. Berger, Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and
Robert L. Mercer. 1996. Language Translation Ap-
paratus and Method of Using Context-Based Trans-
lation Models. United States Patent, Patent Number
5510981, April.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. MIT Press, Cambridge Massachusetts.
Stuart E. Dreyfus and Averill M. Law. 1977. The Art
and Theory ofDynamic Programming (Mathematics in
Science and Engineering; vol. 130). Acadamic Press,
New York, N.Y.
Held and Karp. 1962. A Dynamic Programming Ap-
proach to Sequencing Problems. SIAM, 10(1):196–
210.
Fred Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, MA.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. CL, 25(4):607–615.
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-Based Statistical Machine Translation Mod-
els. In Proceedings of AMTA 2004, Washington DC,
September-October.
Bruce Lowerre and Raj Reddy. 1980. The Harpy speech
understanding system, in Trends in Speech Recogni-
tion, W.A. Lea, Ed. Prentice Hall, EngleWood Cliffs,
NJ.
H. Ney, D. Mergel, A. Noll, and A. Paeseler. 1992. Data
Driven Search Organization for Continuous Speech
Recognition in the SPICOS System. IEEE Transac-
tion on Signal Processing, 40(2):272–281.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998.
A DP-Based Search Algorithm for Statistical Machine
Translation. In Proc. ofACL/COLING 98, pages 960–
967, Montreal, Canada, August.
Franz-Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417–450.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL’03, pages 160–167, Sapporo, Japan.
Christoph Tillmann and Hermann Ney. 2003. Word Re-
ordering and a DP Beam Search Algorithm for Statis-
tical Machine Translation. CL, 29(1):97–133.
Christoph Tillmann, Stefan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based Search Using
Monotone Alignments in Statistical Translation. In
Proc. ofACL 97, pages 289–296, Madrid,Spain, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, University
of Technology, Aachen, Germany.
Christoph Tillmann. 2003. A Projection Extension Al-
gorithm for Statistical Machine Translation. In Proc.
of EMNLP 03, pages 1–8, Sapporo, Japan, July.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.998702">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634811">
<title confidence="0.999747">Efficient Dynamic Programming Search Algorithms for Phrase-Based SMT</title>
<author confidence="0.995571">Christoph</author>
<affiliation confidence="0.8971305">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.999046">ctill@us.ibm.com</email>
<abstract confidence="0.9985863">This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT). The DP-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to DP-based decoding algorithms in speech recognition is demonstrated. The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently</abstract>
<note confidence="0.827292">that are generally NP complete (Knight, 1999).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Niyu Ge</author>
<author>Young-Suk Lee</author>
<author>Kishore Papineni</author>
<author>Fei Xia</author>
<author>Christoph Tillmann</author>
</authors>
<title>IBM Site Report.</title>
<date>2004</date>
<booktitle>In NIST 2004 MT Workshop,</booktitle>
<publisher>IBM.</publisher>
<location>Alexandria, VA,</location>
<contexts>
<context position="6120" citStr="Al-Onaizan et al., 2004" startWordPosition="918" endWordPosition="921">to swap some neighbor blocks. During decoding, we try to minimize the score of a block sequence under the restriction that the concatenated source phrases of the blocks yield a segmentation of the input sentence: (1) Here, is -dimensional feature vector with real-valued features and is the corresponding weight vector as described in Section 5. The fact that a given block covers some source interval is implicit in this notation. 1The multi-beam decoder is similar to the decoder presented in (Koehn, 2004) which is a standard decoder used in phrasebased SMT. A multi-beam decoder is also used in (Al-Onaizan et al., 2004) and (Berger et al., 1996). 2In that work, there is a distinction between within-word and between-word search, which is not relevant for phrase-based decoding where only exact phrase matches are searched. Figure 2: An Arabic-English block translation example, where the Arabic words are romanized. A sequence of blocks is generated. This paper is structured as follows: Section 2 introduces the multi-beam and the single-beam DP-based decoders. Section 3 presents an analysis of all the graphbased shortest-path finding algorithm mentioned above: a search algorithm for a directed acyclic graph (dag)</context>
<context position="26249" citStr="Al-Onaizan et al., 2004" startWordPosition="4310" endWordPosition="4313"> following: a block translation score derived from phrase occurrence statistics , a trigram language model to predict target words , a lexical weighting score for the block internal words , a distortion model as well as the negative target phrase length . The transition cost is computed as , where is a weight vector that sums up to : . The weights are trained using a procedure similar to (Och, 2003) on held-out test data. A block set of million blocks, which are not filtered according to any particular test set is used, which has been generated by a phrase-pair selection algorithm similar to (Al-Onaizan et al., 2004). The training data is sentence-aligned consisting of million training sentence pairs. Beam-search results are presented in terms of two pruning thresholds: the coverage pruning threshold and the cardinality pruning threshold (Tillmann and Ney, 2003). To carry out the pruning, the minimum cost with respect to each coverage set and cardinality are computed for a state set . For the coverage pruning, states are distinguished according to the subset of covered positions . The minimum cost is defined as: . For the cardinality pruning, states are distinguished according to the cardinality of subset</context>
</contexts>
<marker>Al-Onaizan, Ge, Lee, Papineni, Xia, Tillmann, 2004</marker>
<rawString>Yaser Al-Onaizan, Niyu Ge, Young-Suk Lee, Kishore Papineni, Fei Xia, and Christoph Tillmann. 2004. IBM Site Report. In NIST 2004 MT Workshop, Alexandria, VA, June. IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Andrew S Kehler</author>
<author>Robert L Mercer</author>
</authors>
<title>Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981,</title>
<date>1996</date>
<contexts>
<context position="6146" citStr="Berger et al., 1996" startWordPosition="923" endWordPosition="926">During decoding, we try to minimize the score of a block sequence under the restriction that the concatenated source phrases of the blocks yield a segmentation of the input sentence: (1) Here, is -dimensional feature vector with real-valued features and is the corresponding weight vector as described in Section 5. The fact that a given block covers some source interval is implicit in this notation. 1The multi-beam decoder is similar to the decoder presented in (Koehn, 2004) which is a standard decoder used in phrasebased SMT. A multi-beam decoder is also used in (Al-Onaizan et al., 2004) and (Berger et al., 1996). 2In that work, there is a distinction between within-word and between-word search, which is not relevant for phrase-based decoding where only exact phrase matches are searched. Figure 2: An Arabic-English block translation example, where the Arabic words are romanized. A sequence of blocks is generated. This paper is structured as follows: Section 2 introduces the multi-beam and the single-beam DP-based decoders. Section 3 presents an analysis of all the graphbased shortest-path finding algorithm mentioned above: a search algorithm for a directed acyclic graph (dag). Section 4 shows an effic</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Kehler, Mercer, 1996</marker>
<rawString>Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and Robert L. Mercer. 1996. Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="18860" citStr="Brown et al., 1993" startWordPosition="3087" endWordPosition="3090">product of source and target positions: Here, is the target sentence length and is the source sentence length. The phrase learning approach in (Och and Ney, 2004) takes two alignments: a source-to-target alignment and a target-to-source alignment . The intersection of these two alignments is computed to obtain a high-precision word alignment. Here, we note that if the intersection covers all source and target positions (as shown in Fig. 4), it constitutes a bijection between source and target sentence positions, since the intersecting alignments are functions according to their definition in (Brown et al., 1993) 3. In this paper, an algorithmic justification for restricting blocks based on word alignments is given. We assume that source and target sentence are given, and the task is to compute the lowest scoring block alignment. Such an algorithm might be important in some discriminative training procedure that relies on decoding the training data efficiently. To restrict the block selection based on word aligned training data, interval projection functions are defined as follows 4: is a source interval and is an target inter3(Tillmann, 2003) reports an intersection coverage of about % for Arabic-Eng</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge Massachusetts.</location>
<contexts>
<context position="15152" citStr="Cormen et al., 2001" startWordPosition="2465" endWordPosition="2469"> scanner procedure as described above. The predicate CLOSED is used to switch between matching and scanning states. The predicate CLOSED is true if the pointer is equal to the match end position (this is stored in ). At this point, the position-byposition match of the source phrase is completed and we can search for additional block matches. 3 DP Shortest Path Algorithm for dag This section analyzes the relationship between the block decoding algorithms in this paper and a single-source shortest path finding algorithm for a directed acyclic graphs (dag). We closely follow the presentation in (Cormen et al., 2001) and only sketch the algorithm here: a dag is a weighted graph for which a topological sort of its vertex set exists: all the vertexes can be enumerated in linear order. For such a weighted graph, the shortest path from a single source can be computed in time, where is the number of vertexes and number of edges in the graph. The dag search algorithm runs over all vertexes in topological order. Assuming an adjacency-list representation of the dag, for each vertex , we loop over all successor vertexes , where each vertex with its adjacency-list is processed exactly once. During the search, we ma</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms. MIT Press, Cambridge Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart E Dreyfus</author>
<author>Averill M Law</author>
</authors>
<date>1977</date>
<booktitle>The Art and Theory ofDynamic Programming (Mathematics in Science and Engineering;</booktitle>
<volume>130</volume>
<publisher>Acadamic Press,</publisher>
<location>New York, N.Y.</location>
<contexts>
<context position="2814" citStr="Dreyfus and Law, 1977" startWordPosition="413" endWordPosition="416">he search algorithm in (Niessen et al., 1998) is (target) position-synchronous or bottom-to-top, and the search algorithm in (Tillmann and Ney, 2003) is so-called cardinality-synchronous. Taking into account the different word order between source and target language sentences, it becomes less obvious that a SMT search algorithm can be described as a shortest path finding algorithm. But this has been shown by linking decoding to a dynamic-programming solution for the traveling salesman problem. This algorithm due to (Held and Karp, 1962) is a special case of a shortest path finding algorithm (Dreyfus and Law, 1977). The regularly structured search graph for this problem is illustrated in Fig. 1: all paths from the left-most to the right-most vertex correspond to a translation of the in((1),1) ((1,2),2) ((1,4),4) ((1,5),5) ((1,3),3) ((1,2,3),2) ((1,2,4),2) ((1,3,4),3) ((1,4,5),5) ((1,2,5),2) ((1,3,5),3) ((1,4,5),4) ((1,3,4),4) ((1,3,5),5) ((1,2,3),3) ((1,2,4),4) ((1,2,5),5) ((1,2,3,5),5) ((1,2,4,5),5) ((1,3,4,5),5) ((1,2,3,4),4) ((1,2,4,5),4) ((1,3,4,5),4) ((1,2,3,4),2) ((1,2,3,5),2) ((1,2,4,5),2) ((1,2,3,4),3) ((1,2,3,5),3) ((1,3,4,5),3) ((1,2,3,4,5),2) ((1,2,3,4,5),3) ((1,2,3,4,5),5) ((1,2,3,4,5),4) Fi</context>
</contexts>
<marker>Dreyfus, Law, 1977</marker>
<rawString>Stuart E. Dreyfus and Averill M. Law. 1977. The Art and Theory ofDynamic Programming (Mathematics in Science and Engineering; vol. 130). Acadamic Press, New York, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Held</author>
<author>Karp</author>
</authors>
<title>A Dynamic Programming Approach to Sequencing Problems.</title>
<date>1962</date>
<journal>SIAM,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>210</pages>
<contexts>
<context position="2735" citStr="Held and Karp, 1962" startWordPosition="399" endWordPosition="402"> (Tillmann et al., 1997) is (source) position-synchronous or left-to-right, the search algorithm in (Niessen et al., 1998) is (target) position-synchronous or bottom-to-top, and the search algorithm in (Tillmann and Ney, 2003) is so-called cardinality-synchronous. Taking into account the different word order between source and target language sentences, it becomes less obvious that a SMT search algorithm can be described as a shortest path finding algorithm. But this has been shown by linking decoding to a dynamic-programming solution for the traveling salesman problem. This algorithm due to (Held and Karp, 1962) is a special case of a shortest path finding algorithm (Dreyfus and Law, 1977). The regularly structured search graph for this problem is illustrated in Fig. 1: all paths from the left-most to the right-most vertex correspond to a translation of the in((1),1) ((1,2),2) ((1,4),4) ((1,5),5) ((1,3),3) ((1,2,3),2) ((1,2,4),2) ((1,3,4),3) ((1,4,5),5) ((1,2,5),2) ((1,3,5),3) ((1,4,5),4) ((1,3,4),4) ((1,3,5),5) ((1,2,3),3) ((1,2,4),4) ((1,2,5),5) ((1,2,3,5),5) ((1,2,4,5),5) ((1,3,4,5),5) ((1,2,3,4),4) ((1,2,4,5),4) ((1,3,4,5),4) ((1,2,3,4),2) ((1,2,3,5),2) ((1,2,4,5),2) ((1,2,3,4),3) ((1,2,3,5),3) (</context>
</contexts>
<marker>Held, Karp, 1962</marker>
<rawString>Held and Karp. 1962. A Dynamic Programming Approach to Sequencing Problems. SIAM, 10(1):196– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1278" citStr="Jelinek, 1998" startWordPosition="180" endWordPosition="181">d in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduction This paper deals with dynamic programming based decoding and alignment algorithms for phrase-based SMT. Dynamic Programming based search algorithms are being used in speech recognition (Jelinek, 1998; Ney et al., 1992) as well as in statistical machine translation (Tillmann et al., 1997; Niessen et al., 1998; Tillmann and Ney, 2003). Here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids. Under certain restrictions, e.g. start and end point restrictions for the path, the shortest path computed corresponds to a recognized word sequence or a generated target language translation. In these algorithms, a shortest-path search 9 Figure 1: Illustration of a DP-based algorithm to solve a traveling salesman problem with</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>Fred Jelinek. 1998. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding Complexity in WordReplacement Translation Models.</title>
<date>1999</date>
<journal>CL,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1062" citStr="Knight, 1999" startWordPosition="149" endWordPosition="150">the similarity to DP-based decoding algorithms in speech recognition is demonstrated. The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduction This paper deals with dynamic programming based decoding and alignment algorithms for phrase-based SMT. Dynamic Programming based search algorithms are being used in speech recognition (Jelinek, 1998; Ney et al., 1992) as well as in statistical machine translation (Tillmann et al., 1997; Niessen et al., 1998; Tillmann and Ney, 2003). Here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids. Under certain restrictions, e.g. start and end point restrictions for the path, the shortest path computed corre</context>
<context position="17749" citStr="Knight, 1999" startWordPosition="2916" endWordPosition="2917">finite and that all the states are actually reachable from the start state . This way for the single-word based search in (Tillmann and Ney, 2003), a complexity of is shown, where is the size of the target vocabulary and is the length of the input sentence. The complexity is dominated by the exponential number of coverage vectors that occur in the search, and the complexity of phrase-based decoding is higher yet since its hypotheses store a source interval rather than a single source position . In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999). Efficient search algorithms can be derived by the restricting the allowable coverage vectors (Tillmann, 2001) to local word re-ordering only. An efficient phrase alignment method that does not make use of re-ordering restriction is demonstrated in the following section. 4 Efficient Block Alignment Algorithm A common approach to phrase-based SMT is to learn phrasal translation pairs from word-aligned training data (Och and Ney, 2004). Here, a word alignment is a subset of the Cartesian product of source and target positions: Here, is the target sentence length and is the source sentence lengt</context>
<context position="29305" citStr="Knight, 1999" startWordPosition="4831" endWordPosition="4832">LEU score of versus for the -Beam algorithm. The results in this section show that the -Beam algorithm generally runs faster since the beam search pruning is applied to all states simultaneously making more efficient use of the beam search concept. 6 Discussion The decoding algorithm shown here is most similar to the decoding algorithms presented in (Koehn, 2004) and (Och and Ney, 2004), the later being used for the Alignment Template Model for SMT. These algorithms also 15 include an estimate of the path completion cost which can easily be included into this work as well ((Tillmann, 2001)). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. But using DP-based optimization techniques and appropriate restrictions leads to efficient DP-based decoding algorithms as shown in this paper. The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in (Wu, 1997): in both cases the number of alignments is drastically reduced by introducing appropriate re-ordering restrictions. The listbased decoding algorit</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding Complexity in WordReplacement Translation Models. CL, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA 2004,</booktitle>
<location>Washington DC, September-October.</location>
<contexts>
<context position="6004" citStr="Koehn, 2004" startWordPosition="899" endWordPosition="900">block at a time. In practice, a largely monotone block sequence is generated except for the possibility to swap some neighbor blocks. During decoding, we try to minimize the score of a block sequence under the restriction that the concatenated source phrases of the blocks yield a segmentation of the input sentence: (1) Here, is -dimensional feature vector with real-valued features and is the corresponding weight vector as described in Section 5. The fact that a given block covers some source interval is implicit in this notation. 1The multi-beam decoder is similar to the decoder presented in (Koehn, 2004) which is a standard decoder used in phrasebased SMT. A multi-beam decoder is also used in (Al-Onaizan et al., 2004) and (Berger et al., 1996). 2In that work, there is a distinction between within-word and between-word search, which is not relevant for phrase-based decoding where only exact phrase matches are searched. Figure 2: An Arabic-English block translation example, where the Arabic words are romanized. A sequence of blocks is generated. This paper is structured as follows: Section 2 introduces the multi-beam and the single-beam DP-based decoders. Section 3 presents an analysis of all t</context>
<context position="7784" citStr="Koehn, 2004" startWordPosition="1178" endWordPosition="1179">, since it is conceptually simpler. 2.1 Multi-Beam Decoder For the multi-beam decoder makes use of search states that are -tuples of the following type: (2) is the state history, that depends on the block generation model. In our case, , where is the interval where the most recent block matched the input sentence, and are the final two target words of the partial translation produced thus far. is the so-called coverage vector that ensures that a consistent block alignment is obtained during decoding and that the decoding 10 Table 1: Multi-beam ( -Beam) decoding algorithm, which is similar to (Koehn, 2004). The decoders differ in their pruning strategy: here, each state list is pruned only once, whereas the decoder in (Koehn, 2004) prunes a state list every time a new hypothesis is entered. input: source sentence with words can be carried out efficiently. It keeps track of the already processed input sentence positions. is the cost of the shortest path (distance) from some initial state to the current state . The baseline decoder maintains state lists with entries of the above type, where is the number of input words. The states are stored in lists or stacks that support lookup operations to ch</context>
<context position="12225" citStr="Koehn, 2004" startWordPosition="1956" endWordPosition="1957">s already in only the state with the lower path cost is kept. This inserting/updating operation is also called recombination or relaxation in the context of a dag search algorithm (cf. Section 3). The update procedure also stores for each state its predecessor state in a so-called back-pointer array (Ney et al., 1992). The final block alignment and target translation can be recovered from this back-pointer array once the final state set has been computed. is the source phrase length of the matching block when going from to . This algorithm is similar to the beam-search algorithm presented in (Koehn, 2004): it allows states to be added to a stack that is not the stack for the successor cardinality. is the initial decoder state, where no source position is covered: . For the final states in all source positions are covered. 2.2 Single-Beam Implementation The second implementation uses two lists to keep a single beam of active states. This corresponds to a beam-search decoder in speech recognition, where path hypotheses corresponding to word sequences are processed in a timesynchronous way and at a given time step only hypotheses within some percentage of the best hypothesis are kept (Lowerre and</context>
<context position="29057" citStr="Koehn, 2004" startWordPosition="4788" endWordPosition="4789">formance of the two algorithms in terms of BLEU score is nearly identical, the -Beam algorithm runs significantly faster. For a coverage threshold of , the -Beam algorithm is as fast as the -Beam algorithm at , but obtains a significantly higher BLEU score of versus for the -Beam algorithm. The results in this section show that the -Beam algorithm generally runs faster since the beam search pruning is applied to all states simultaneously making more efficient use of the beam search concept. 6 Discussion The decoding algorithm shown here is most similar to the decoding algorithms presented in (Koehn, 2004) and (Och and Ney, 2004), the later being used for the Alignment Template Model for SMT. These algorithms also 15 include an estimate of the path completion cost which can easily be included into this work as well ((Tillmann, 2001)). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. But using DP-based optimization techniques and appropriate restrictions leads to efficient DP-based decoding algorithms as shown in this paper. The efficient block alignment algorithm in Section 4 i</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proceedings of AMTA 2004, Washington DC, September-October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Lowerre</author>
<author>Raj Reddy</author>
</authors>
<title>The Harpy speech understanding system, in Trends in Speech Recognition,</title>
<date>1980</date>
<publisher>Prentice Hall,</publisher>
<location>W.A. Lea, Ed.</location>
<contexts>
<context position="9781" citStr="Lowerre and Reddy, 1980" startWordPosition="1515" endWordPosition="1518">orithm: the last visited position is replaced by an interval . The states are stored in lists or stacks that support lookup operations to check whether a given state tuple is already present in a list and what its score is. Extending the partial block translation that is represented by a state with a single block generates a new state . Here, is the source interval where block matches the input sentence. The state transition is defined as follows: (3) The state fields are updated on a component-bycomponent basis. is the coverage vecTable 2: Single-beam ( -Beam) decoding algorithm (related to (Lowerre and Reddy, 1980)). tor obtained by adding all the positions from the interval . The new state history is defined as where and are the final two target words of the target phrase of . Some special cases, e.g. where has less than two target words, are taken into account. The path cost is computed as , where the transition cost is computed from the history and the matching block as defined in Section 5. The decoder in Table 1 fills state sets . All the coverage vectors for states in the set cover the same number of source positions . When a state set is processed, the decoder has finished processing all states i</context>
<context position="12838" citStr="Lowerre and Reddy, 1980" startWordPosition="2056" endWordPosition="2059">oehn, 2004): it allows states to be added to a stack that is not the stack for the successor cardinality. is the initial decoder state, where no source position is covered: . For the final states in all source positions are covered. 2.2 Single-Beam Implementation The second implementation uses two lists to keep a single beam of active states. This corresponds to a beam-search decoder in speech recognition, where path hypotheses corresponding to word sequences are processed in a timesynchronous way and at a given time step only hypotheses within some percentage of the best hypothesis are kept (Lowerre and Reddy, 1980). The single-beam decoder processes hypotheses cardinality-synchronously, i.e. the states at stage generate new states at position . In order to make the use of a single beam possible, we slightly modify the state transitions in Eq. 3: Here, Eq. 5 corresponds to the matcher definition in Eq. 3. We add an additional field that is a pointer keeping track of how much of the recent source phrase match has been covered. In Eq. 5, when a block is matched to the input sentence, this pointer is set to position k where the most recent block match starts. We use a dot to indicate that when a block is ma</context>
</contexts>
<marker>Lowerre, Reddy, 1980</marker>
<rawString>Bruce Lowerre and Raj Reddy. 1980. The Harpy speech understanding system, in Trends in Speech Recognition, W.A. Lea, Ed. Prentice Hall, EngleWood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>D Mergel</author>
<author>A Noll</author>
<author>A Paeseler</author>
</authors>
<title>Data Driven Search Organization for Continuous Speech Recognition in the SPICOS System.</title>
<date>1992</date>
<journal>IEEE Transaction on Signal Processing,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="1297" citStr="Ney et al., 1992" startWordPosition="182" endWordPosition="185">ay to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduction This paper deals with dynamic programming based decoding and alignment algorithms for phrase-based SMT. Dynamic Programming based search algorithms are being used in speech recognition (Jelinek, 1998; Ney et al., 1992) as well as in statistical machine translation (Tillmann et al., 1997; Niessen et al., 1998; Tillmann and Ney, 2003). Here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids. Under certain restrictions, e.g. start and end point restrictions for the path, the shortest path computed corresponds to a recognized word sequence or a generated target language translation. In these algorithms, a shortest-path search 9 Figure 1: Illustration of a DP-based algorithm to solve a traveling salesman problem with cities. The visite</context>
<context position="4604" citStr="Ney et al., 1992" startWordPosition="661" endWordPosition="664">m decoder with increased translation speed 1. A common analysis of all the search algorithms above in terms of a shortest-path finding algorithm for a directed acyclic graph (dag) is presented. This analysis provides a simple way of analyzing the complexity of DP-based search algorithm. Generally, the regular search space can only be fully searched for small search grids under appropriate restrictions, i.e. the monotonicity restrictions in (Tillmann et al., 1997) or the inverted search graph in (Niessen et al., 1998). For larger search spaces as are required for continuous speech recognition (Ney et al., 1992) 2 or phrasebased decoding in SMT, the search space cannot be fully searched: suitably defined lists of path hypothesis are maintained that partially explore the search space. The number of hypotheses depends locally on the number hypotheses whose score is close to the top scoring hypothesis: this set of hypotheses is called the beam. The translation model used in this paper is a phrasebased model, where the translation units are so-called blocks: a block is a pair of phrases which are translations of each other. For example, Fig. 2 shows an ArabicEnglish translation example that uses blocks. </context>
<context position="11932" citStr="Ney et al., 1992" startWordPosition="1905" endWordPosition="1908">each update for end end output: translation from lowest cost state in output: translation from lowest cost state in input: source sentence with words for each do for each state in do if CLOSED? then matcher: for each else scanner: for single update for end Prune state set Swap , end end 11 state is already in only the state with the lower path cost is kept. This inserting/updating operation is also called recombination or relaxation in the context of a dag search algorithm (cf. Section 3). The update procedure also stores for each state its predecessor state in a so-called back-pointer array (Ney et al., 1992). The final block alignment and target translation can be recovered from this back-pointer array once the final state set has been computed. is the source phrase length of the matching block when going from to . This algorithm is similar to the beam-search algorithm presented in (Koehn, 2004): it allows states to be added to a stack that is not the stack for the successor cardinality. is the initial decoder state, where no source position is covered: . For the final states in all source positions are covered. 2.2 Single-Beam Implementation The second implementation uses two lists to keep a sin</context>
</contexts>
<marker>Ney, Mergel, Noll, Paeseler, 1992</marker>
<rawString>H. Ney, D. Mergel, A. Noll, and A. Paeseler. 1992. Data Driven Search Organization for Continuous Speech Recognition in the SPICOS System. IEEE Transaction on Signal Processing, 40(2):272–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>A DP-Based Search Algorithm for Statistical Machine Translation.</title>
<date>1998</date>
<booktitle>In Proc. ofACL/COLING 98,</booktitle>
<pages>960--967</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="1388" citStr="Niessen et al., 1998" startWordPosition="197" endWordPosition="200">esented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduction This paper deals with dynamic programming based decoding and alignment algorithms for phrase-based SMT. Dynamic Programming based search algorithms are being used in speech recognition (Jelinek, 1998; Ney et al., 1992) as well as in statistical machine translation (Tillmann et al., 1997; Niessen et al., 1998; Tillmann and Ney, 2003). Here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids. Under certain restrictions, e.g. start and end point restrictions for the path, the shortest path computed corresponds to a recognized word sequence or a generated target language translation. In these algorithms, a shortest-path search 9 Figure 1: Illustration of a DP-based algorithm to solve a traveling salesman problem with cities. The visited cities correspond to processed source positions. is carried out in one pass over some inp</context>
<context position="4509" citStr="Niessen et al., 1998" startWordPosition="645" endWordPosition="648">erent search graphs are presented: a multi-beam decoder reported in the literature and a single-beam decoder with increased translation speed 1. A common analysis of all the search algorithms above in terms of a shortest-path finding algorithm for a directed acyclic graph (dag) is presented. This analysis provides a simple way of analyzing the complexity of DP-based search algorithm. Generally, the regular search space can only be fully searched for small search grids under appropriate restrictions, i.e. the monotonicity restrictions in (Tillmann et al., 1997) or the inverted search graph in (Niessen et al., 1998). For larger search spaces as are required for continuous speech recognition (Ney et al., 1992) 2 or phrasebased decoding in SMT, the search space cannot be fully searched: suitably defined lists of path hypothesis are maintained that partially explore the search space. The number of hypotheses depends locally on the number hypotheses whose score is close to the top scoring hypothesis: this set of hypotheses is called the beam. The translation model used in this paper is a phrasebased model, where the translation units are so-called blocks: a block is a pair of phrases which are translations o</context>
</contexts>
<marker>Niessen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998. A DP-Based Search Algorithm for Statistical Machine Translation. In Proc. ofACL/COLING 98, pages 960– 967, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="18187" citStr="Och and Ney, 2004" startWordPosition="2979" endWordPosition="2982">ses store a source interval rather than a single source position . In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999). Efficient search algorithms can be derived by the restricting the allowable coverage vectors (Tillmann, 2001) to local word re-ordering only. An efficient phrase alignment method that does not make use of re-ordering restriction is demonstrated in the following section. 4 Efficient Block Alignment Algorithm A common approach to phrase-based SMT is to learn phrasal translation pairs from word-aligned training data (Och and Ney, 2004). Here, a word alignment is a subset of the Cartesian product of source and target positions: Here, is the target sentence length and is the source sentence length. The phrase learning approach in (Och and Ney, 2004) takes two alignments: a source-to-target alignment and a target-to-source alignment . The intersection of these two alignments is computed to obtain a high-precision word alignment. Here, we note that if the intersection covers all source and target positions (as shown in Fig. 4), it constitutes a bijection between source and target sentence positions, since the intersecting align</context>
<context position="19649" citStr="Och and Ney, 2004" startWordPosition="3214" endWordPosition="3217"> is to compute the lowest scoring block alignment. Such an algorithm might be important in some discriminative training procedure that relies on decoding the training data efficiently. To restrict the block selection based on word aligned training data, interval projection functions are defined as follows 4: is a source interval and is an target inter3(Tillmann, 2003) reports an intersection coverage of about % for Arabic-English parallel data, and a coverage of % for Chinese-English data. In the case of uncomplete coverage, the current algorithm can be extended as described in Section 4.1. 4(Och and Ney, 2004) defines the notion of consistency for the set of phrasal translations that are learned from wordFigure 3: Following the definition in Eq. 6, the left picture shows three admissible block links while the right picture shows three non-admissible block links. val. is the set of target positions such that the alignment point occurs in the alignment set and is covered by the source interval . is defined accordingly. Formally, the definitions look like this: In order to obtain a particularly simple block alignment algorithm, the allowed block links are restricted by an ADMISSIBILITY restriction, wh</context>
<context position="29081" citStr="Och and Ney, 2004" startWordPosition="4791" endWordPosition="4794">o algorithms in terms of BLEU score is nearly identical, the -Beam algorithm runs significantly faster. For a coverage threshold of , the -Beam algorithm is as fast as the -Beam algorithm at , but obtains a significantly higher BLEU score of versus for the -Beam algorithm. The results in this section show that the -Beam algorithm generally runs faster since the beam search pruning is applied to all states simultaneously making more efficient use of the beam search concept. 6 Discussion The decoding algorithm shown here is most similar to the decoding algorithms presented in (Koehn, 2004) and (Och and Ney, 2004), the later being used for the Alignment Template Model for SMT. These algorithms also 15 include an estimate of the path completion cost which can easily be included into this work as well ((Tillmann, 2001)). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. But using DP-based optimization techniques and appropriate restrictions leads to efficient DP-based decoding algorithms as shown in this paper. The efficient block alignment algorithm in Section 4 is related to the inversi</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz-Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="26027" citStr="Och, 2003" startWordPosition="4275" endWordPosition="4276">ource phrase matches is represented as a feature-vector . The feature-vector components are the negative logarithm of some probabilities as well as a word-penalty feature. The real-valued features include the following: a block translation score derived from phrase occurrence statistics , a trigram language model to predict target words , a lexical weighting score for the block internal words , a distortion model as well as the negative target phrase length . The transition cost is computed as , where is a weight vector that sums up to : . The weights are trained using a procedure similar to (Och, 2003) on held-out test data. A block set of million blocks, which are not filtered according to any particular test set is used, which has been generated by a phrase-pair selection algorithm similar to (Al-Onaizan et al., 2004). The training data is sentence-aligned consisting of million training sentence pairs. Beam-search results are presented in terms of two pruning thresholds: the coverage pruning threshold and the cardinality pruning threshold (Tillmann and Ney, 2003). To carry out the pruning, the minimum cost with respect to each coverage set and cardinality are computed for a state set . Fo</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL’03, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word Reordering and a DP Beam Search Algorithm for Statistical Machine Translation.</title>
<date>2003</date>
<journal>CL,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="654" citStr="Tillmann and Ney, 2003" startWordPosition="83" endWordPosition="86">earch Algorithms for Phrase-Based SMT Christoph Tillmann IBM T.J. Watson Research Center Yorktown Heights, NY 10598 ctill@us.ibm.com Abstract This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT). The DP-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to DP-based decoding algorithms in speech recognition is demonstrated. The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduction This paper deals with dynamic programming based decoding and alignment algorithms for phrase-based SMT. Dynamic Programming based search algorithms are being used in speech re</context>
<context position="2341" citStr="Tillmann and Ney, 2003" startWordPosition="339" endWordPosition="342">et language translation. In these algorithms, a shortest-path search 9 Figure 1: Illustration of a DP-based algorithm to solve a traveling salesman problem with cities. The visited cities correspond to processed source positions. is carried out in one pass over some input along a specific ’direction’: in speech recognition the search is timesynchronous, the single-word based search algorithm in (Tillmann et al., 1997) is (source) position-synchronous or left-to-right, the search algorithm in (Niessen et al., 1998) is (target) position-synchronous or bottom-to-top, and the search algorithm in (Tillmann and Ney, 2003) is so-called cardinality-synchronous. Taking into account the different word order between source and target language sentences, it becomes less obvious that a SMT search algorithm can be described as a shortest path finding algorithm. But this has been shown by linking decoding to a dynamic-programming solution for the traveling salesman problem. This algorithm due to (Held and Karp, 1962) is a special case of a shortest path finding algorithm (Dreyfus and Law, 1977). The regularly structured search graph for this problem is illustrated in Fig. 1: all paths from the left-most to the right-mo</context>
<context position="3751" citStr="Tillmann and Ney, 2003" startWordPosition="526" endWordPosition="529">,4) ((1,3,5),5) ((1,2,3),3) ((1,2,4),4) ((1,2,5),5) ((1,2,3,5),5) ((1,2,4,5),5) ((1,3,4,5),5) ((1,2,3,4),4) ((1,2,4,5),4) ((1,3,4,5),4) ((1,2,3,4),2) ((1,2,3,5),2) ((1,2,4,5),2) ((1,2,3,4),3) ((1,2,3,5),3) ((1,3,4,5),3) ((1,2,3,4,5),2) ((1,2,3,4,5),3) ((1,2,3,4,5),5) ((1,2,3,4,5),4) Final Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 9–16, New York City, New York, June 2006. c�2006 Association for Computational Linguistics put sentence, where each source position is processed exactly once. In this paper, the DP-based search algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrase-based translation. Two versions of a phrase-based decoder for SMT that search slightly different search graphs are presented: a multi-beam decoder reported in the literature and a single-beam decoder with increased translation speed 1. A common analysis of all the search algorithms above in terms of a shortest-path finding algorithm for a directed acyclic graph (dag) is presented. This analysis provides a simple way of analyzing the complexity of DP-based search algorithm. Generally, the regular search space can only be fully searched for small sea</context>
<context position="10610" citStr="Tillmann and Ney, 2003" startWordPosition="1671" endWordPosition="1674"> than two target words, are taken into account. The path cost is computed as , where the transition cost is computed from the history and the matching block as defined in Section 5. The decoder in Table 1 fills state sets . All the coverage vectors for states in the set cover the same number of source positions . When a state set is processed, the decoder has finished processing all states in the sets where . Before expanding a state set, the decoder prunes a state set based on its coverage vector and the path costs only: two different pruning strategies are used that have been introduced in (Tillmann and Ney, 2003): 1) coverage pruning prunes states that share the same coverage vector , 2) cardinality pruning prunes states according to the cardinality of covered positions: all states in the beam are compared with each other. Since the states are kept in separate lists, which are pruned independently of each others, this decoder version is called multi-beam decoder. The decoder uses a matcher function when expanding a state: for a state it looks for uncovered source positions to find source phrase matches for blocks. Updating a state in Table 1 includes adding the state if it is not yet present or updati</context>
<context position="17282" citStr="Tillmann and Ney, 2003" startWordPosition="2832" endWordPosition="2835">orithm. These involve costly lookup operations, e.g. language, distortion and translation model probability lookup. Typically the computation time for update operations on lists is negligible compared to these probability lookups. So, the search algorithm complexity is simply computed as the number of edges in the search graph: (this analysis is implicit in (Tillmann, 12 2001)). Without proof, for the search algorithm in Section 2.1 we observe that the number of states is finite and that all the states are actually reachable from the start state . This way for the single-word based search in (Tillmann and Ney, 2003), a complexity of is shown, where is the size of the target vocabulary and is the length of the input sentence. The complexity is dominated by the exponential number of coverage vectors that occur in the search, and the complexity of phrase-based decoding is higher yet since its hypotheses store a source interval rather than a single source position . In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999). Efficient search algorithms can be derived by the restricting the allowable coverage vectors (Tillmann, 2001) to local word re-orde</context>
<context position="24928" citStr="Tillmann and Ney, 2003" startWordPosition="4082" endWordPosition="4085">ed out during alignment. The efficiency of the block alignment algorithm depends on the alignment intersection coverage. 5 Beam-Search Results In this section, we present results for the beam-search algorithms introduced in Section 2. The MT03 ArabicEnglish NIST evaluation test set consisting of sentences with Arabic words is used for the experiments. Translation results in terms of uncased BLEU using reference translations are reported in Table 4 and Table 5 for the single-beam ( -Beam) and the multibeam ( -Beam) search algorithm. For all re-ordering experiments, the notion of skips is used (Tillmann and Ney, 2003) to restrict the phrase re-ordering: the number of skips restricts the number of holes in the coverage vector for a left-to-right traversal of the input sentence. All 14 Table 4: Effect of the skip parameter for the two search strategies. and window width . Skip BLEU CPU BLEU CPU -Beam [secs] -Beam [secs] 0 1 2 3 re-ordering takes place in a window of size , such that only local block re-ordering is handled. The following block bigram scoring is used: a block pair with corresponding source phrase matches is represented as a feature-vector . The feature-vector components are the negative logari</context>
<context position="26499" citStr="Tillmann and Ney, 2003" startWordPosition="4345" endWordPosition="4348">th . The transition cost is computed as , where is a weight vector that sums up to : . The weights are trained using a procedure similar to (Och, 2003) on held-out test data. A block set of million blocks, which are not filtered according to any particular test set is used, which has been generated by a phrase-pair selection algorithm similar to (Al-Onaizan et al., 2004). The training data is sentence-aligned consisting of million training sentence pairs. Beam-search results are presented in terms of two pruning thresholds: the coverage pruning threshold and the cardinality pruning threshold (Tillmann and Ney, 2003). To carry out the pruning, the minimum cost with respect to each coverage set and cardinality are computed for a state set . For the coverage pruning, states are distinguished according to the subset of covered positions . The minimum cost is defined as: . For the cardinality pruning, states are distinguished according to the cardinality of subsets of covered positions. The minimum cost is defined for all hypotheses with the same cardinality : . States in are pruned if the shortest path cost is greater than the minimum cost plus the pruning threshold: The same state set pruning is used for th</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word Reordering and a DP Beam Search Algorithm for Statistical Machine Translation. CL, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
</authors>
<title>A DP-based Search Using Monotone Alignments in Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proc. ofACL 97,</booktitle>
<pages>289--296</pages>
<location>Madrid,Spain,</location>
<contexts>
<context position="1366" citStr="Tillmann et al., 1997" startWordPosition="193" endWordPosition="196">translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduction This paper deals with dynamic programming based decoding and alignment algorithms for phrase-based SMT. Dynamic Programming based search algorithms are being used in speech recognition (Jelinek, 1998; Ney et al., 1992) as well as in statistical machine translation (Tillmann et al., 1997; Niessen et al., 1998; Tillmann and Ney, 2003). Here, the decoding algorithms are described as shortest path finding algorithms in regularly structured search graphs or search grids. Under certain restrictions, e.g. start and end point restrictions for the path, the shortest path computed corresponds to a recognized word sequence or a generated target language translation. In these algorithms, a shortest-path search 9 Figure 1: Illustration of a DP-based algorithm to solve a traveling salesman problem with cities. The visited cities correspond to processed source positions. is carried out in </context>
<context position="4454" citStr="Tillmann et al., 1997" startWordPosition="635" endWordPosition="638">a phrase-based decoder for SMT that search slightly different search graphs are presented: a multi-beam decoder reported in the literature and a single-beam decoder with increased translation speed 1. A common analysis of all the search algorithms above in terms of a shortest-path finding algorithm for a directed acyclic graph (dag) is presented. This analysis provides a simple way of analyzing the complexity of DP-based search algorithm. Generally, the regular search space can only be fully searched for small search grids under appropriate restrictions, i.e. the monotonicity restrictions in (Tillmann et al., 1997) or the inverted search graph in (Niessen et al., 1998). For larger search spaces as are required for continuous speech recognition (Ney et al., 1992) 2 or phrasebased decoding in SMT, the search space cannot be fully searched: suitably defined lists of path hypothesis are maintained that partially explore the search space. The number of hypotheses depends locally on the number hypotheses whose score is close to the top scoring hypothesis: this set of hypotheses is called the beam. The translation model used in this paper is a phrasebased model, where the translation units are so-called blocks</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>Christoph Tillmann, Stefan Vogel, Hermann Ney, and Alex Zubiaga. 1997. A DP-based Search Using Monotone Alignments in Statistical Translation. In Proc. ofACL 97, pages 289–296, Madrid,Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>Word Re-Ordering and Dynamic Programming based Search Algorithm for Statistical Machine Translation.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Technology,</institution>
<location>Aachen, Germany.</location>
<contexts>
<context position="17860" citStr="Tillmann, 2001" startWordPosition="2932" endWordPosition="2933">ed search in (Tillmann and Ney, 2003), a complexity of is shown, where is the size of the target vocabulary and is the length of the input sentence. The complexity is dominated by the exponential number of coverage vectors that occur in the search, and the complexity of phrase-based decoding is higher yet since its hypotheses store a source interval rather than a single source position . In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999). Efficient search algorithms can be derived by the restricting the allowable coverage vectors (Tillmann, 2001) to local word re-ordering only. An efficient phrase alignment method that does not make use of re-ordering restriction is demonstrated in the following section. 4 Efficient Block Alignment Algorithm A common approach to phrase-based SMT is to learn phrasal translation pairs from word-aligned training data (Och and Ney, 2004). Here, a word alignment is a subset of the Cartesian product of source and target positions: Here, is the target sentence length and is the source sentence length. The phrase learning approach in (Och and Ney, 2004) takes two alignments: a source-to-target alignment and a</context>
<context position="29288" citStr="Tillmann, 2001" startWordPosition="4829" endWordPosition="4830">nificantly higher BLEU score of versus for the -Beam algorithm. The results in this section show that the -Beam algorithm generally runs faster since the beam search pruning is applied to all states simultaneously making more efficient use of the beam search concept. 6 Discussion The decoding algorithm shown here is most similar to the decoding algorithms presented in (Koehn, 2004) and (Och and Ney, 2004), the later being used for the Alignment Template Model for SMT. These algorithms also 15 include an estimate of the path completion cost which can easily be included into this work as well ((Tillmann, 2001)). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. But using DP-based optimization techniques and appropriate restrictions leads to efficient DP-based decoding algorithms as shown in this paper. The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in (Wu, 1997): in both cases the number of alignments is drastically reduced by introducing appropriate re-ordering restrictions. The listbased</context>
</contexts>
<marker>Tillmann, 2001</marker>
<rawString>Christoph Tillmann. 2001. Word Re-Ordering and Dynamic Programming based Search Algorithm for Statistical Machine Translation. Ph.D. thesis, University of Technology, Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Projection Extension Algorithm for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP 03,</booktitle>
<pages>1--8</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="19401" citStr="Tillmann, 2003" startWordPosition="3174" endWordPosition="3176">nments are functions according to their definition in (Brown et al., 1993) 3. In this paper, an algorithmic justification for restricting blocks based on word alignments is given. We assume that source and target sentence are given, and the task is to compute the lowest scoring block alignment. Such an algorithm might be important in some discriminative training procedure that relies on decoding the training data efficiently. To restrict the block selection based on word aligned training data, interval projection functions are defined as follows 4: is a source interval and is an target inter3(Tillmann, 2003) reports an intersection coverage of about % for Arabic-English parallel data, and a coverage of % for Chinese-English data. In the case of uncomplete coverage, the current algorithm can be extended as described in Section 4.1. 4(Och and Ney, 2004) defines the notion of consistency for the set of phrasal translations that are learned from wordFigure 3: Following the definition in Eq. 6, the left picture shows three admissible block links while the right picture shows three non-admissible block links. val. is the set of target positions such that the alignment point occurs in the alignment set </context>
</contexts>
<marker>Tillmann, 2003</marker>
<rawString>Christoph Tillmann. 2003. A Projection Extension Algorithm for Statistical Machine Translation. In Proc. of EMNLP 03, pages 1–8, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="29758" citStr="Wu, 1997" startWordPosition="4899" endWordPosition="4900">se algorithms also 15 include an estimate of the path completion cost which can easily be included into this work as well ((Tillmann, 2001)). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. But using DP-based optimization techniques and appropriate restrictions leads to efficient DP-based decoding algorithms as shown in this paper. The efficient block alignment algorithm in Section 4 is related to the inversion transduction grammar approach to bilingual parsing described in (Wu, 1997): in both cases the number of alignments is drastically reduced by introducing appropriate re-ordering restrictions. The listbased decoding algorithms can also be compared to an Earley-style parsing algorithm that processes list of parse states in a single left-to-right run over the input sentence. For this algorithm, the comparison in terms of a shortestpath algorithm is less obvious: in the so-called completion step the parser re-visits states in previous stacks. But it is interesting to note that there is no multiple lists variant of that parser. In phrase-based decoding, a multiple list de</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>