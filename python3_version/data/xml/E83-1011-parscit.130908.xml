<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.319505">
AN EXPERIMENT WITH HEURISTIC PARSING OF SWEDISH
</note>
<author confidence="0.830578">
Benny Brodda
</author>
<affiliation confidence="0.990121">
Inst. of Linguistics
University of Stockholm
</affiliation>
<address confidence="0.685272">
S-106 91 Stockholm SWEDEN
</address>
<email confidence="0.536289">
ABSTRACT
</email>
<bodyText confidence="0.999867166666666">
Heuristic parsing is the art of doing parsing
in a haphazard and seemingly careless manner but
in such a way that the outcome is still &amp;quot;good&amp;quot;, at
least from a statistical point of view, or, hope-
fully, even from a more absolute point of view.
The idea is to find strategic shortcuts derived
from guesses about the structure of a sentence
based on scanty observations of linguistic units
in the sentence. If the guess comes out right much
parsing time can be saved, and if it does not,
many subobservations may still be valid for re-
vised guesses. In the (very preliminary) experi-
ment reported here the main idea is to make use of
(combinations of) surface phenomena as much as
possible as the base for the prediction of the
structure as a whole. In the parser to be deve-
loped along the lines sketched in this report main
stress is put on arriving at independently
working, parallel recognition procedures.
The work reported here is both aimed at simu-
lating certain aspects of human language per-
ception and at arriving at effective algorithms
for actual parsing of running text. There is,
indeed, a great need for fast such algorithms,
e.g. for the analysis of the literally millions of
words of running text that already today comprise
the data bases in various large information re-
trieval systems, and which can be expected to
expand several orders of magnitude both in im-
portance and in size in the foreseeable future.
</bodyText>
<sectionHeader confidence="0.679454" genericHeader="method">
I BACKGROUND
</sectionHeader>
<bodyText confidence="0.996743695652174">
The general idea behind the system for heu-
ristic parsing now being developed at our group in
Stockholm dates more than 15 years back, when I
was making an investigation (together with Hans
Karlgren, Stockholm) of the possibilities of
using computers for information retrieval purposes
for the Swedish Governmental Board for Rationali-
zation (Statskontoret). In the course of this
investigation we performed some psycholinguistic
experiments aimed at finding out to what extent
surface markers, such as endings, prepositions,
conjunctions and other (bound) elements from
typically closed categories of linguistic units,
could serve as a base for a syntactic analysis of
sentences. We sampled a couple of texts more or
less at random and prepared them in such a way
that stems of nouns, adjectives and (main) verbs -
these categories being thought of as the main
carriers of semantic information - were substi-
tuted for by a mere &amp;quot;-&amp;quot;, whereas other formatives
were left in their original shape and place. These
transformed texts were presented to subjects who
were asked to fill in the gaps in such a way that
the texts thus obtained were to be both syntacti-
cally correct and reasonably coherent.
The result of the experiment was rather
astonishing. It turned out that not only were the
syntactic structures mainly restored, in some few
cases also the original content was reestablished,
almost word by word. (It was beyond any possi-
bility that the subjects could have had access to
the original text.) Even in those cases when the
text itself was not restored to this remarkable
extent, the stylistic value of the various texts
was almost invariably reestablished; an originally
lively, narrative story came out as a lively,
narrative story , and a piece of rather dull,
factual text (from a school text book on socio-
logy) invariably came out as dull, factual prose.
This experiment showed quite clearly that at
least for Swedish the information contained in the
combinations of surface markers to a remarkably
high degree reflects the syntactic structure of
the original text; in almost all cases also the
stylistic value and in some few cases even the
semantic content was kept. (The extent to which
this is true is probably language dependent; Swe-
dish is rather rich in morphology, and this
property is certainly a contributing factor for an
experiment of this type to come out successful to
the extent it actually did.)
This type of experiment has since then been
repeated many times by many scholars; in fact, it
is one of the standard ways to demonstrate the
concept of redundancy in texts. But there are
several other important conclusions one could draw
from this type of experiments. First of all, of
course, the obvious conclusion that surface
signals do carry a lot of information about the
structure of sentences, probably much more than
one has been inclined to think, and, consequently,
it could be worth while to try to capture that
information in some kind of automatic analysis
system. This is the practical side of it. But
there is more to it. One must ask the question why
a language like Swedish is like this. What are the
theoretical implications?
Much interest has been devoted in later years
to theories (and speculations) about human per-
</bodyText>
<page confidence="0.983424">
66
</page>
<bodyText confidence="0.9999425625">
ception of linguistic stimuli, and I do not think
that one speculates too much if one assumes that
surface markers of the type that appeared in the
described experiment together constitute im-
portant clues concerning the gross syntactic
structure of sentences (or utterances), clues that
are probably much less consiously perceived than,
e.g., the actual words in the sentences/utteran-
ces. To the extent that such clues are actually
perceived they are obviously perceived simulta-
neously with, i.e. in parallel with, other units
(words, for instance).
The above way of looking upon perception as a
set of independently operating processes is, of
course, more or less generally accepted nowadays
(cf., e.g., Lindsay-Norman 1977), and it is also
generally accepted in computational linguistics
that any program that aims at simulating per-
ception in one way or other must have features
that simulates (or, even better, actually per-
forms) parallel processing, and the analysis
system to be described below has much emphasis on
exactly this feature.
Another common saying nowadays when dis-
cussing parsing techniques is that one should try
to incorporate &amp;quot;heuristic devices&amp;quot; (cf., e.g.,
the many subreports related to the big ARPA-
project concerning Speech Recognition and Under-
standing 1970-76), although there does not seem
to exist a very precise consensus of what exactly
that would mean. (In mathematics the term has
been traditionally used to refer to informal
reasoning, especially when used in classroom
situations. In a famous study the hungarian
mathematician Polys, 1945 put forth the thesis
that heuristics is one of the most important
psychological driving mechanisms behind mathe-
matical - or scientific - progress. In AI-
literature it is often used to refer to shortcut
search methods in semantic networks/spaces; c.f.
Lenat, 1982).
One reason for trying to adopt some kind of
heuristic device in the analysis procedures is
that one for mathematical reasons knows that
ordinary, &amp;quot;careful&amp;quot;, parsing algorithms inherently
seem to refuse to work in real time (i.e. in
linear time), whereas human beings, on the whole,
seem to be able to do exactly that (i.e. perceive
sentences or utterances simultaneously with their
production). Parallel processing may partly be an
answer to that dilemma, but still, any process
that claims to actually simulate some part of
human perception must in some way or other
simulate the remarkable abilities human beings
have in grasping complex patterns (&amp;quot;gestalts&amp;quot;)
seemingly in one single operation.
Ordinary, careful, parsing algorithms are
often organized according to some general
principle such as &amp;quot;top-down&amp;quot;, &amp;quot;bottom-to-top&amp;quot;,
&amp;quot;breadth first&amp;quot;, &amp;quot;depth first&amp;quot;, etc., these
headings referring to some specified type of
&amp;quot;strategy&amp;quot;. The heuristic model we are trying to
work out has no such preconceived strategy built
into it. Our philosophy is instead rather
anarchistic (The Heuristic Principle): Whatever
linguistic unit that can be identified at whatever
stage of the analysis, according to whatever means
there are, is identified, and the significance of
the fact that the unit in question has been
identified is made use of in all subsequent stages
of the analysis. At any time one must be prepared
to reconsider an already established analysis of a
unit on the ground that evidence against the
analysis may successively accumulate due to what
analyses other units arrive at.
In next section we give a brief description
of the analysis system for Swedish that is now
under development at our group in Stockholm. As
has been said, much effort is spent on trying to
make use of surface signals as much as possible.
Not that we believe that surface signals play a
more important role than any other type of
linguistic signals, but rather that we think it is
important to try to optimize each single sub-
process (in a parallel system) as much as
possible, and, as said, it might be worth while
to look careful into this level, because the im-
portance of surface signals might have been under-
estimated in previous research. Our experiments so
far seem to indicate that they constitute ex-
cellent units to base heuristic guesses on. An-
other reason for concentrating our efforts on this
level is that it takes time and requires much hard
computational work to get such an anarchistic
system to really work, and this surface level is
reasonably simple to handle.
</bodyText>
<sectionHeader confidence="0.8820725" genericHeader="method">
II AN OUTLINE OF AN ANALYZER BASED ON
THE HEURISTIC PRINCIPLE
</sectionHeader>
<bodyText confidence="0.999446433333334">
Figure 1 below shows the general outline of
the system. Each of the various boxes (or sub-
boxes) represents one specific process, usually a
complete computer program in itself, or, in some
cases, independent processes within a program. The
big &amp;quot;container&amp;quot;, labelled &amp;quot;The Pool&amp;quot;, contains
both the linguistic material as well as the
current analysis of it. Each program or process
looks into the Pool for things &amp;quot;it&amp;quot; can recognize,
and when the process finds anything it is trained
to recognize, it adds its observation to the ma-
terial in the Pool. This added material may (hope-
fully) help other processes in recognizing what
they are trained to recognize, which in its turn
may again help the first process to recognize more
of &amp;quot;its&amp;quot; units. And so on.
The system is now under development and
during this build-up phase each process is, as was
said above, essentially a complete, stand-alone
module, and the Pool exists simply as successively
updated text files on a disc storage. At the
moment some programs presuppose that other prog-
rams have already been run, but this state of
affairs will be valid just during this build-up
phase. At the end of the build-up phase each
program shall be able to run completely inde-
pendent of any other program in the system and in
arbitrary order relative to the others (but, of
course, usually perform better if more information
is available in the Pool).
</bodyText>
<page confidence="0.996991">
67
</page>
<bodyText confidence="0.998031880434783">
In the iecond phase superordinated control
programs are to be implemented. These programs
will function as &amp;quot;traffic rules&amp;quot; and via these
systems one shall be able to test various strate-
gies, i.e. to test which relative order between
the different subsystems that yields optimal re-
sult in some kind of &amp;quot;performance metric&amp;quot;, some
evaluation procedure that takes both speed and
quality into account.
The programs/processes shown in Figure 1 all
represent rather straightforward Finite State
Pattern Matching (FS/PM) procedures. It is rather
trivial to show mathematically that a set of
interacting FS/PM procedures of the type used in
our system together will yield a system that
formally has the power of a CF-parser; in practice
it will yield a system that in some sense is
stronger, at least from the point of view of
convenience. Congruence and similar phenomena will
be reduced to simple local observations. Trans-
formational variants of sentences will be re-
cognized directly - there will be no need for
performing some kind of backward transformational
operations. (In this respect a system like this
will resemble Gazdar&apos;s grammar concept; Gazdar
1980.)
The control structures later to be superim-
posed on the interacting FS/PM systems will also
be of a Finite State type. A system of the type
then obtained - a system of independent Finite
State Automatons controlled by another Finite
State Automaton - will in principle have rather
complex mathematical properties. It is, e.g.,
rather easy to see that such a system has stronger
capacity than a Type 2 device, but it will not
have the power of a full Type I system.
- - -
Now a few comments to Figure 1
The &amp;quot;balloons&amp;quot; in the figure represent inde-
pendent programs (later to be developed into inde-
pendent processes inside one &amp;quot;big&amp;quot; program). The
figure displays those programs that so far
(January 1983) have been implemented and tested
(to some extent). Other programs will successively
be entered into the system.
The big balloon labelled &amp;quot;The Closed Cat&amp;quot;
represents a program that recognizes closed word
classes such as prepositions, conjunctions, pro-
nouns, auxiliaries, and so on. The Closed Cat
recognizes full word forms directly. The SMURF
balloon represents the morphological component
(SMURF = &amp;quot;Swedish Murphology&amp;quot;). SMURF itself is
organized internally as a complex system of inde-
pendently nperating &amp;quot;demons&amp;quot; - SMURFs - each
knowing &amp;quot;its- little corner of Swedish word forma-
tion. (The name of the program is an allusion to
the powilar comic strip leprechauns &amp;quot;les
Schtroumpfs&amp;quot;, which in Swedish are called
&amp;quot;smurfar&amp;quot;0 Thus there is one little smurf recog-
nizing derivational morphemes, one recognizing
flectional endings, and so on. One special smurf,
Phonotax, has an important controlling function -
every other smurf must always consult Phonotax
before identifying one of &amp;quot;its&amp;quot; (potential) forma-
tives; the word minus this formative must still be
pronounceable, otherwise it cannot be a formative.
SMURF works entirely without stem lexicon; it
adheres completely to the &amp;quot;philosophy&amp;quot; of using
surface signals as far as possible.
NOMFRAS, VERBAL, IFIGEN, CLAUS and PREPPS are
other &amp;quot;demons&amp;quot; that recognize different phrases or
word groups within sentences, viz, noun phrases,
verbal complexes, infinitival constructions,
clauses and prepositional phrases, respectively.
V-lex and A-lex represent various (sub)-
lexicons; so far we have tried to do without them
as far as possible. One should observe that stem
lexicons are no prerequisites for the system to
work, adding them only enhances its performance.
The format of the material inside the Pool is
the original text, plus appropriate &amp;quot;labelled
brackets&amp;quot; enclosing words, word groups, phrases
and so on. In this way, the form of representation
is consistent throughout, no matter how many
different types of analyses have been applied to
it. Thus, various people can join our group and
write their own &amp;quot;demons&amp;quot; in whatever language they
prefer, as long as they can take sentences in text
format, be reasonably tolerant to what types of
-brackets&amp;quot; they find in there, do their analysis,
add their own brackets (in the specified format),
and put the result back into the Pool.
</bodyText>
<page confidence="0.997183">
68
</page>
<bodyText confidence="0.999981772727273">
Of the various programs SMURF, NOMFRAS and
IFIGEN are extensively tested (and, of course, The
Closed Cat, which is a simple lexical lookup
system), and various examples of analyses of these
programs will be demonstrated in the next section.
We hope to arrive at a crucial station in this
project during 1983, when CLAUS has been more
thoroughly tested. If CLAUS performs the way we
hope (and preliminary tests indicate that it
will), we will have means to identify very quickly
the clausal structures of the sentences in an
arbitrary running text, thus having a firm base
for entering higher hierarchies in the syntactic
domains.
The programs are written in the Beta language
developed by the present author; c.f. Brodda-
Karlsson, 1980, and Brodda, 1983, forthcoming. Of
the actual programs in the system, SMURF was
developed and extensively tested by B.B. during
1977-79 (Brodda, 1979), whereas the others are
(being) developed by B.B. and/or Gunnel Kallgren,
Stockholm (mostly &amp;quot;and&amp;quot;).
</bodyText>
<sectionHeader confidence="0.931836" genericHeader="method">
III EXPLODING SOME OF THE BALLOONS
</sectionHeader>
<bodyText confidence="0.999917141509434">
When a &amp;quot;fresh&amp;quot; text is entered into The Pool
it first passes through a preliminary one-pass-
program, INIT, (not shown in Fig. 1) that &amp;quot;normal-
izes&amp;quot; the text. The original text may be of any
type as long as it is regularly typed Swedish.
INIT transforms the text so that each graphic
sentence will make up exactly one physical record.
(Except in poetry, physical records, i.e. lines,
usually are of marginal linguistic interest.)
Paragraph ends will be represented by empty re-
cords. Periods used to indicate abbreviations are
just taken away and the abbreviation itself is
contracted to one graphic word, if necessary; thus
&amp;quot;t.ex.&amp;quot; (&amp;quot;e.g.&amp;quot;) is transformed into &amp;quot;tax&amp;quot;, and so
on. Otherwise, periods, commas, question marks and
other typographic characters are provided with
preceding blanks. Through this each word is
guaranteed to be surrounded by blanks, and de-
limiters like commas, periods and so on are
guaranteed to signal their &amp;quot;normal&amp;quot; textual func-
tions. Each record is also ended by a sentence
delimiter (preceded by a blank). Some manual post-
editing is sometimes needed in order to get the
text normalized according to the above. In the
INIT-phase no linguistic analysis whatsoever is
introduced (other than into what appears to be
orthographic sentences).
INIT also changes all letters in the original
text to their corresponding upper case variants.
(Originally capital letters are optionally pro-
vided with a prefixed &amp;quot;=&amp;quot;.) All subsequent ana-
lysis programs add their analyses in the form of
lower case letters or letter combinations. Thus
upper case letters or words will belong to the
object language, and lower case letters or letter
combinations will signal meta-language informa-
tion. In this way, strictly text (ASCII) format
can be kept for the text as well as for the va-
rious stages of its analysis; the &amp;quot;philosophy&amp;quot; to
use text input and text output for all programs
involved represents the computational solution to
the problem of how to make it possible for each
process to work independently of all other in the
system.
The Closed Cat (CC) has the important role to
mark words belonging to some well defined closed
categories of words. This program makes no in-
ternal analysis of the words, and only takes full
words into account. CC makes use of simple rewrite
rules of the type &amp;quot;PA =&gt; ePAe / (blank) (blank)&amp;quot;,
where the inserted es represent the &amp;quot;analysis&amp;quot;
(&amp;quot;e&amp;quot; stands for &amp;quot;preposition&amp;quot;; PA = &amp;quot;on&amp;quot;). A
sample output from The Closed Cat is shown in
illustration 2, where the various meta-symbols
also are explained.
The simple &apos;example above also shows the
format of inserted meta-information. Each identi-
fied constituent is &amp;quot;tagged&amp;quot; with surrounding
lower case letters, which then can be conceived of
as labelled brackets. This format is used
throughout the system, also for complex constit-
uents. Thus the nominal phrase &amp;quot;DEN LILLA FLICKAN&amp;quot;
(&amp;quot;the little girl&amp;quot;) will be tagged as
&amp;quot;nDEN+LILLA+FLICKANn&amp;quot; by NOMFRAS (cf. below; the
pluses are inserted to make the constituent one
continuous string). We have reserved the letters
n, v and s for the major categories nouns or noun
phrases, verbs or verbal groups, and sentences,
respectively, whereas other more or less transpar-
ent letters are used for other categories. (A list
of used category symbols is presented in the
Appendix: Printout Illustrations.)
The program SWEMRF (or SMURF, as it is called
here) has been extensively described elsewhere
(Brodda, 1979). It makes a rather intricate
morphological analysis word-by-word in running
text (i.e. SMURF analyzes each word in itself,
disregarding the context it appears in). SMURF can
be run in two modes, in &amp;quot;segmentation&amp;quot; mode and
&amp;quot;analysis&amp;quot; mode. In its segmentation mode SMURF
simply strips off the possible affixes from each
word; it makes no use of any stem lexicon. (The
affixes it recognizes are common prefixes, suf-
fixes - i.e. derivational morphemes - and flex-
ional endings.) In analysis mode it also tries to
make an optimal guess of the word class of the
word under inspection, based on what (combinations
of) word formation elements it finds in the word.
SMURF in itself is organized entirely according to
the heuristic principles as they are conceived
here, i.e. as a set of independently operating
processes that interactively work on each others
output. The SMURF system has been the test bench
for testing out the methods now being used
throughout the entire Heuristic Parsing Project.
In its segmentation mode SMURF functions
formally as a set of interactive transformations,
where the structural changes happen to be ex-
tremely simple, viz, simple segmentation rules of
the type 1P=&gt;P-&amp;quot;, &amp;quot;S=&gt; -S&amp;quot; and &amp;quot;E=&gt;-E&amp;quot; for an
arbitrary Prefix, Suffix and Ending, respectively,
but where the &amp;quot;job&amp;quot; essentially consists of
establishing the corresponding structural de-
scriptions. These are shown in Ill. 1, below,
together with sample analyses. It should be noted
that phonotactic constraints play a central role
</bodyText>
<page confidence="0.998507">
69
</page>
<bodyText confidence="0.999905090909091">
in the SMURF system; in fact, one of the main
objectives in designing the SMURF system was to
find out how much information actually was carried
by the phonotactic component in Swedish. (It
turned out to be quite much; cf. Brodda 1979. This
probably holds for other Germanic languages as
well, which all have a rather elaborated phono-
taxis.)
NOMFRAS is the next program to be commented
on. The present version recognizes structures of
the type
</bodyText>
<equation confidence="0.795341">
det/quant + (adj)g + noun;
</equation>
<bodyText confidence="0.987837824074075">
where the &amp;quot;det/quant&amp;quot; categories (i.e. determiners
or quantifiers) are defined explicitly through
enumeration - they are supposed to belong to the
class of &amp;quot;surface markers&amp;quot; and are as such identi-
fied by The Closed Cat. Adjectives and nouns on
the other hand are identified solely on the ground
of their &amp;quot;cadences&amp;quot;, i.e. what kind of (formally)
ending-like strings they happen to end with. The
number of adjectives that are accepted (n in the
formula above) varies depending on what (probable)
type of construction is under inspection. In inde-
finite noun phrases the substantial content of the
expected endings is, to say the least, meager, as
both nouns and adjectives in many situations only
have 0-endings. In definite noun phrases the noun
mostly - but not always - has a more substantial
and recognizable ending and all intervening ad-
jectives have either the cadence -A or a cadence
from a small but characteristic set. In a (sup-
posed) definite noun phrase all words ending in
any of the mentioned cadences are assumed to be
adjectives, but in (supposed) indefinite noun
phrases not more than one adjective is assumed
unless other types of morphological support are
present.
The Finite State Scheme behind NOMFRAS is
presented in Ill. 2, together with sample outputs;
In this case the text has been preprocessed by The
Closed Cat, and it appears that these two programs
in cooperation are able to recognize noun phrases
of the discussed type correctly to well over 95%
in running text (at a speed of about 5 sentences
per second, CPU-time); the errors were shared
about 50% each between over- and undergenerations.
Preliminary experiments aiming at including also
SMURF and PREPPS (Prepositional Phrases) seem to
indicate that about the same recall and precision
rate could be kept for arbitrary types of (non-
sentential) noun phrases (cf. Ill. 6). (The sys-
tems are not yet trimmed to the extent that they
can be operatively run together.)
IFIGEN (Infinitive Generator) is another
rather straightforward Finite State Pattern
Matcher (developed by Gunnel Kallgren). It recog-
nizes (groups of) nonfinite verbs. Somewhat
simplified it can be represented by the following
diagram (remember the conventions for upper and
lower case):
IFIGEN parsing diagram (simplified):
Aux nXn
2
ATT
where &amp;quot;Aux&amp;quot; and &amp;quot;Adv&amp;quot; are categories recognized by
The Closed Cat (tagged &amp;quot;g&amp;quot; and &amp;quot;a&amp;quot;, respectively),
and &amp;quot;nXn&amp;quot; are structures recognized by either
NOMFRAS or, in the case of personal pronouns, by
CC (It should be worth mentioning that the class
of auxiliaries in Swedish is more open than the
corresponding word class in English; besides the
&amp;quot;ordinary&amp;quot; VARA (&amp;quot;to be&amp;quot;), HA (&amp;quot;to have&amp;quot;) and the
modals, there is a fuzzy class of semi-auxiliaries
like BoRJA (&amp;quot;begin&amp;quot;) and others; IFIGEN makes use
of about 20 of these in the present version.) The
supine cadence -(A/I)-T is supposed to appear only
once in an infinitival group. A sample output of
IFIGEN is given in Ill. 3. Also for IFIGEN we have
reached a recognition level around 95%, which,
again, is rather astonishing, considering how
little information actually is made use of in the
system.
The IFIGEN case illustrates very clearly one
of the central points in our heuristic approach,
namely the following: The information that a word
has a specific cadence, in this case the cadence
-A, is usually of very little significance in
itself in Swedish. Certainly it is a typical infi-
nitival cadence (at least 90% of all infinitives
in Swedish have it), but on the other hand, it is
certainly a very typical cadence for other types
of words as well: FLICKA (noun), HELA (adjective),
DENNA/DETTA/DESSA (determiners or pronouns) and so
on, and these other types are by no comparison the
dominant group having this specific cadence in
running text. But, in connection with an &amp;quot;infini-
tive warner&amp;quot; - an auxiliary, or the word ATT - the
situation changes dramatically. This can be demon-
strated by the following figures: In running text
words having the cadance -A represents infinitives
in about 30% of the cases. ATT is an infinitive
marker (equivalent to &amp;quot;to&amp;quot;) in quite exactly 50%
of its occurences (the other 50% it is a subordi-
nating conjunction). The conditional probability
that the configuration ATT ..-A represents an
infinitve is, however, greater than 99%, pro-
vided that characteristic cadences like -ARNA/-
ORNA and quantifiers/determiners like ALLA and
DESSA are disregarded (In our system they are
marked by SMURF and The Closed Cat, respectively,
and thereby &amp;quot;saved&amp;quot; from being classified as infi-
nitives.) Given this, there is almost no over-
generation in IFIGEN, but Swedish allows for split
infinitives to some extent. Quite much material
can be put in between the infinitive warner and
the infinitive, and this gives rise to some under-
generation (presenzly). (Similar observations re-
garding conditional probabilities in configura-
tions of linguistic units has been made by Mats
Eeg-Olofson, Lund, 1982).
</bodyText>
<figure confidence="0.884355">
-A
# (C)CV
-(A/I)T 1
</figure>
<page confidence="0.984423">
70
</page>
<bodyText confidence="0.989852454545455">
IV REFERENCES
Brodda, B. &amp;quot;Nagot om de svenska ordens fonotax och
morfotax&amp;quot;, Papers from the Institute of
Linguistics (PILUS) No. 38, University of Stock-
holm, 1979.
Brodda, B. -fttreltriterier f6r igenkgnning av
sammansgttningar&amp;quot; in Saari, M. and Tandefelt, M.
(eds.) F6rhandlingar r6rande svenskans beskriv-
ning - Hanaholmen 1981, Meddelanden fran Insti-
tutionen f6r Nordiska Sprak, Helsingfors Univer-
sitet, 1981
</bodyText>
<reference confidence="0.83083825">
Brodda, B. &amp;quot;The BETA System, and some Applica-
tions&amp;quot;, Data Linguistics, Gothenburg, 1983
(forthcoming).
Brodda, B. and Karlsson, F. &amp;quot;An experiment with
Automatic Morphological Analysis of Finnish&amp;quot;,
Publications No. 7, Dept. of Linguistics, Uni-
versity of Helsinki, 1981.
Cazdar, G. &amp;quot;Phrase Structure&amp;quot; in Jacobson, P. and
Pullam G. (eds.), Nature of Syntactic Represen-
tation, Reidel, 1982
Lenat, D.P. &amp;quot;The Nature of Heuristics&amp;quot;, Artifi-
cial Intelligence, Vol 19(2), 1982.
</reference>
<figureCaption confidence="0.538166875">
Eeg-Olofsson, M. &amp;quot;En sprakstatistisk modell f6r
ordklassmgrkning i 16pande text&amp;quot; in Kgllgren, G.
(ed.) TAGGNING, F6redrag frail 3:e svenska kollo-
kviet i spraklig databehandling i maj 1982,
PILUS 47, Stockholm 1982.
Polya, G. &amp;quot;How to Solve it&amp;quot;, Princeton University
Press, 1945. Also Doubleday Anchor Press, New
York, N.Y. (several editions).
</figureCaption>
<bodyText confidence="0.958385875">
APPENDIX: Some computer illustrations
The following three pages illustrate some of the parsing diagrams used in
the system: Ill. 1, SMURF, and III. 2, NOMFRAS, together with sample analyses.
IF1GEN is represented by sample analyses (Ill. 3; the diagram is given in the
text). The samples are all taken from running text analysis (from a novel by
Ivar Lo-Johansson), and &amp;quot;pruned&amp;quot; only in the way that trivial, recurrent examples
are omitted. Some typical erroneous analyses are also shown (prefixed by **).
In Ill. 1 SMURF is run in segmentation mode only, and the existing tags are
inserted by the Closed Cat. &apos;A and &apos;E in word final position indicates the
corresponding cadences (fullfilling the pattern &apos;:..V-M-A/E&amp;quot;, where M denotes a
set of admissible medial clusters).
The tags inserted by CC are: a=(sentence) adverbials, b=particles, d=determiners,
e=prepositions, g=auxiliaries, h=(forms of) HA(VA), i=infinitives, j=adjectives,
n=nouns, K=conjunctions, q=quantifiers, r=pronouns, u=supine verb form, v=verbal
(group).
(For space reasons, Ill. 3 is given first, then I and II.)
</bodyText>
<tableCaption confidence="0.192595">
III. 3: PATTERN: aux/ATT&amp;quot;(pron)&amp;quot;(adv)&amp;quot;(adv)inf&apos;inf&apos;...:
</tableCaption>
<footnote confidence="0.622016142857143">
FRAMATOJD HELA DAGEN..
ciETTq KXDSTRECK eIe
eTILLe ikATTk+iSSi
clENq KARL INUTI?
VIPPEN?
HEM eMEDe SKAMMEN
e0Me WARSOMHELST.
ePAe rDETr.
NAT eMEDe rDENr, kSAk
eUPPe POTATISEN.
BALLONGEN FYLLD.
SEJ OPPE.
STILLA eUNDERe OSS.
SITT MAL.
</footnote>
<bodyText confidence="0.993315333333334">
..PLOCKNINGEN eEFTER..ikATTk+iHAi+uGATTui
rDETr vVARv ORIMLIGT ikATTk+iFINNAi
rJAGr gSKAg aBARAa iHJXLPAi
- rDETr gKANg iLIGGAi
gSKAg rVIr iVAGAi
- rVIr gKANg aINTEa iGki
...ORNA vHoLLv SIG FXRDIGA ikATTk+iKASTAi
rDEr gVAGADEg aXNTLIGENa iLYFTAi
gSKAg rNIr alloDVANDIGTVISa iGORAi
..rVIr hHADEh aANNUa aINTEa uHUNNITu iFAi
..BECKMoRKRET eMEDe ikATTk+iFoRSoKAi+iFAi
eMEDe VXTGAS eFoRe ikATTk+iKUNNAi+iHALLAi
SKOGEN, LANDEN gTYCKTESg iSTAi
rDENr hHADEh MISSLYCKATS eIe ikATTk+iNAi
*** clENq As gVAGADEg iKVINNORNA+STANNAi
</bodyText>
<page confidence="0.987506">
71
</page>
<figure confidence="0.78754375">
Ill. 1: SMURF - PARSING DIAGRAM FOR SWEDISH MORPHOLOGY
Structural
changes
PATTERNS (&amp;quot;Structural Descriptions&amp;quot;):
1) ENDINGS (El: 1/vS_ m
2) PREFIXES (P): 1#
P - V - X ;
X - V . F (s) —
E=&gt;=E
P =&gt; (-)P&gt;
3) SUFFIXES (S): (s) I V - XI
X &apos;V F-S-1E# ; S =&gt;
</figure>
<bodyText confidence="0.9807198">
where I = (admissible) initial cluster, F = final cluster, Me = morph-
eme-internal cluster, V = vowel, (s) the &amp;quot;gluon&amp;quot;S (cf. TIDNINGSMAN),
# = word boundary, (=,&gt;,/,-) = earlier accepted affix segmentations, and
&amp;quot;, finallay, denotes ordinary concatenation. (It is the enhanced ele-
ment in each pattern that is tested for its segmentability).
</bodyText>
<sectionHeader confidence="0.752222" genericHeader="method">
BAGG&apos;E vDROGv . REP=ET SLINGR=ADE MELLAN STEN=AR , FoR&gt;BI
TALLSTAMM=AR , MELLAN RDA L/NGONTUV=OR eIe GRN IN&gt;FATT/NING.
clETTq STORT FoRE&gt;MAL hHADEh uRoRTu ePAe SIG BORT&apos;A eIe
SLANT=EN . FoRE&gt;MAL=ET NARM=ADE SIG HOTFULL&apos;T . dDETd KNASTR=
=ADE eIe SKOG=EN . - SPRING .
B.AGG&apos;E SLAPP=TE kOCHk vSPRANGv . rDEr LANG&apos;A KJOL=ARNA
VIRV1=ADE etiVERe O&lt;PLOCK=ADE LINGONTUV=OR . BAGG&apos;E KVINNO=RNA
hHADEh STRUMPEBAND FoR&gt;FARDIG=ADE eAVe SOCKERTOPPSSOR=EN ,
KNUT=NA NEDAN&gt;FoR KNAN&apos;A .
</sectionHeader>
<bodyText confidence="0.705224238095238">
aFoRSTa bUPPEb ePAe cIENq As VAG=ADE KVINNO=RNA STANN&apos;A .
rDEr vSTODv kOCHk STRACK=TE ePAe HALS=ARNA . ciENq FRAN
UT&gt;DUNST/NING eAVe SKRACK SIPPR=ADE bFRAMb . rDEr vHEILLv
BE&gt;sVARJ/ANDE HAND=ERNA FRAM&gt;FoR SIN&apos;A SKoT=EN .
- dDETd vSERv STORT kOCHk eRUNTe bUTb , vSAv dDENd KORT&apos;A
e0Me FoRE&gt;MAL=ET . dDETd vARv aVALa aINTEa ciNAGOTq IN&gt;UT&gt;I ?
- dDETd gKANg LIGG&apos;A cIENq KARL IN&gt;UT&gt;I ? dDETd vVETv rMANr
aV.ALa kVADk rHANr vGoRv eMEDe OSS .
- rJAGr TYCK=TE dDETd RoR=DE ePAe SEJ . gSKAg rVIr iVAGAi
VIPP=EN ? - JA ? gSKAg rVIr 1VAGAl VIPP=EN ?
BAGGE vSMoGv SIG ePAe GLAPP&apos;A KNAN UT&gt;FoR BRANT=EN . kNARk
rDEr NARM=ADE SIG rDEr FLAT=ADE POTATISKORG=ARNA eMEDe LINGON
kSOMk vSTODv ePAe LUT eVIDe VARSIN TUVA , vVARv rDEr aREDANa
UT&gt;OM SIG eAVe SKRACK . oDERASo SANS vVARv BORT&apos;A .
- PASS ePAe . rVIr KANHAND&apos;A aINTEa vToRSv NARM=ARE ? vSAv
dDENd MAGR&apos;A HUSTRUN .
- rVIr gKANg aINTEa GA HEM eMEDe SKAMM=EN aHELLERa . rVIr
gMASTEg aJUa iHAi WARKORG=ARNA eMEDe .
- JAVISST , BARKORG=ARNA .
kMENk kNARk rDEr uKOMMITu bNERb eTILLe STALL=ET I&lt;GEN
uVARTu rDEr NYFIK=NA . rDEr vDROGSv eTILLe FoRE&gt;MAL=ET eIe
</bodyText>
<page confidence="0.985168">
72
</page>
<figure confidence="0.995562272727272">
Ill. 2: NOMFRAS. - FS-DIAGRAM FOR SWEDISH NOUN PHRASE PARSING
noun
—ITTOT
NAG 01
11611ETT
-T
EGEN
EGET
A/ E/ 0
-0/-E
MIN
MITT
-S
IDENNA1
IDETTA
DESSA1
-0R/-AR/-ER/-N1-01
MINA
-S
EGNA
-AN/-EN
-ET
IDEN
IDET
ALLA
BADA
-(0R/AR/ER)-NA/-ENI
ToT
sa4L_It + det + &amp;quot;OWN&amp;quot; + adj
-AD(E)
-ANDE
-ENDE
-S
</figure>
<bodyText confidence="0.997925045454546">
- PYTT , vSAv nDEN+LANGAn
kVADk vVARv NU nDET+DARn
nDET+OMFANGSRIKA+,+SIDENLATTA+TYGETn
nDEn GJORDE nEN+STOR+PACKEn
eMEDe SIG SJALVA e0Me kATTk nDET+HELAn
nDET+HELAn aINTEa uVARITu nETT+DUGGn
nDET+FoRMENTA+KLADSTRECKETn
GRoN eMEDe HANG8JoRKAR kSOMk nALLAn
.. MODERN , nDEN+LANGA+EGNAHEMSHUSTRUNn
STORA BOKSTAVER nETT+SVENSKT+FIRMANAMNn
ePAe nDEN+ANDRA+,+FRANVANDAn
nDETn vVARv nEN+LUFTENS+SPILLFRUKTn
kOCHk nDEN+ANDRA+EGNAHEMSHUSTRUNS+6GONn
nETT+STORT+MOSSIGT+BERGn
.. SIG eMOTe SKYN eMEDe nEN+DISIG+MANEn
eVIDe nDET+STALLEn
SAGA HONOM kATTk nALLA+DESSA+FdREMALn
..ARNA kSOMk nEN+AVIGT+SKRUBBANDE+HANDn
kSOMk nEN+OFORMLIG+MASSAn
- nEN+RIKTIG+BALLONGn
**nDETn aINTEa vLAGv nNAGON+KROPP+GoMDn
*** TVA kSOMk BARGADE nDEN+TILLSAMMANSn
</bodyText>
<figure confidence="0.998519555555556">
•
kATTk VARA RADD eFoRe ?
eAVe dDETd .
aINTEa uVARITu ciETTq
FARLIGT .
vVARv kDAk SNOTT FLE..
FYLLDE FUNKTIONER
kSOMk uVARITu eIe SKO..
, vSTODv ORDEN ..
kSOMk hHADEh uRAMLAT..
VATTNADES eAVe omsom
HoJDE SIG eMOTe SKYN..
kSOMk cIENq RUND LYKTA
kDKRk LANDNINGSLINAN
aANDIa aINTEa FoRMAD..
VALTRADE SIG BALLONG..
gSKAg VARA FYLLD eMEDe..
INUNDER .
</figure>
<page confidence="0.974556">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000045">
<title confidence="0.996001">AN EXPERIMENT WITH HEURISTIC PARSING OF SWEDISH</title>
<author confidence="0.999896">Benny Brodda</author>
<affiliation confidence="0.999804">Inst. of Linguistics University of Stockholm</affiliation>
<address confidence="0.984835">S-106 91 Stockholm SWEDEN</address>
<abstract confidence="0.997767968586388">Heuristic parsing is the art of doing parsing in a haphazard and seemingly careless manner but in such a way that the outcome is still &amp;quot;good&amp;quot;, at least from a statistical point of view, or, hopefully, even from a more absolute point of view. The idea is to find strategic shortcuts derived from guesses about the structure of a sentence based on scanty observations of linguistic units in the sentence. If the guess comes out right much parsing time can be saved, and if it does not, many subobservations may still be valid for revised guesses. In the (very preliminary) experiment reported here the main idea is to make use of (combinations of) surface phenomena as much as possible as the base for the prediction of the structure as a whole. In the parser to be developed along the lines sketched in this report main stress is put on arriving at independently working, parallel recognition procedures. The work reported here is both aimed at simulating certain aspects of human language perception and at arriving at effective algorithms for actual parsing of running text. There is, indeed, a great need for fast such algorithms, e.g. for the analysis of the literally millions of words of running text that already today comprise the data bases in various large information retrieval systems, and which can be expected to expand several orders of magnitude both in importance and in size in the foreseeable future. I BACKGROUND The general idea behind the system for heuristic parsing now being developed at our group in Stockholm dates more than 15 years back, when I was making an investigation (together with Hans Karlgren, Stockholm) of the possibilities of using computers for information retrieval purposes for the Swedish Governmental Board for Rationalization (Statskontoret). In the course of this investigation we performed some psycholinguistic experiments aimed at finding out to what extent surface markers, such as endings, prepositions, conjunctions and other (bound) elements from typically closed categories of linguistic units, could serve as a base for a syntactic analysis of sentences. We sampled a couple of texts more or less at random and prepared them in such a way that stems of nouns, adjectives and (main) verbs these categories being thought of as the main carriers of semantic information were substituted for by a mere &amp;quot;-&amp;quot;, whereas other formatives were left in their original shape and place. These transformed texts were presented to subjects who were asked to fill in the gaps in such a way that the texts thus obtained were to be both syntactically correct and reasonably coherent. The result of the experiment was rather astonishing. It turned out that not only were the syntactic structures mainly restored, in some few cases also the original content was reestablished, almost word by word. (It was beyond any possibility that the subjects could have had access to the original text.) Even in those cases when the text itself was not restored to this remarkable the valueof the various texts was almost invariably reestablished; an originally lively, narrative story came out as a lively, narrative story , and a piece of rather dull, factual text (from a school text book on sociology) invariably came out as dull, factual prose. This experiment showed quite clearly that at least for Swedish the information contained in the combinations of surface markers to a remarkably high degree reflects the syntactic structure of the original text; in almost all cases also the stylistic value and in some few cases even the semantic content was kept. (The extent to which this is true is probably language dependent; Swedish is rather rich in morphology, and this property is certainly a contributing factor for an experiment of this type to come out successful to the extent it actually did.) This type of experiment has since then been repeated many times by many scholars; in fact, it is one of the standard ways to demonstrate the concept of redundancy in texts. But there are several other important conclusions one could draw from this type of experiments. First of all, of course, the obvious conclusion that surface signals do carry a lot of information about the structure of sentences, probably much more than one has been inclined to think, and, consequently, it could be worth while to try to capture that information in some kind of automatic analysis system. This is the practical side of it. But there is more to it. One must ask the question why a language like Swedish is like this. What are the theoretical implications? Much interest has been devoted in later years theories (and speculations) about human per- 66 ception of linguistic stimuli, and I do not think that one speculates too much if one assumes that surface markers of the type that appeared in the described experiment together constitute important clues concerning the gross syntactic structure of sentences (or utterances), clues that are probably much less consiously perceived than, e.g., the actual words in the sentences/utterances. To the extent that such clues are actually perceived they are obviously perceived simultaneously with, i.e. in parallel with, other units (words, for instance). The above way of looking upon perception as a set of independently operating processes is, of course, more or less generally accepted nowadays (cf., e.g., Lindsay-Norman 1977), and it is also generally accepted in computational linguistics that any program that aims at simulating perception in one way or other must have features that simulates (or, even better, actually performs) parallel processing, and the analysis system to be described below has much emphasis on exactly this feature. Another common saying nowadays when discussing parsing techniques is that one should try to incorporate &amp;quot;heuristic devices&amp;quot; (cf., e.g., the many subreports related to the big ARPAproject concerning Speech Recognition and Understanding 1970-76), although there does not seem to exist a very precise consensus of what exactly that would mean. (In mathematics the term has been traditionally used to refer to informal reasoning, especially when used in classroom situations. In a famous study the hungarian mathematician Polys, 1945 put forth the thesis that heuristics is one of the most important psychological driving mechanisms behind mathematical or scientific progress. In AIliterature it is often used to refer to shortcut search methods in semantic networks/spaces; c.f. Lenat, 1982). One reason for trying to adopt some kind of heuristic device in the analysis procedures is that one for mathematical reasons knows that ordinary, &amp;quot;careful&amp;quot;, parsing algorithms inherently seem to refuse to work in real time (i.e. in linear time), whereas human beings, on the whole, seem to be able to do exactly that (i.e. perceive sentences or utterances simultaneously with their production). Parallel processing may partly be an answer to that dilemma, but still, any process that claims to actually simulate some part of human perception must in some way or other simulate the remarkable abilities human beings have in grasping complex patterns (&amp;quot;gestalts&amp;quot;) seemingly in one single operation. Ordinary, careful, parsing algorithms are often organized according to some general principle such as &amp;quot;top-down&amp;quot;, &amp;quot;bottom-to-top&amp;quot;, &amp;quot;breadth first&amp;quot;, &amp;quot;depth first&amp;quot;, etc., these headings referring to some specified type of &amp;quot;strategy&amp;quot;. The heuristic model we are trying to work out has no such preconceived strategy built into it. Our philosophy is instead rather anarchistic (The Heuristic Principle): Whatever linguistic unit that can be identified at whatever stage of the analysis, according to whatever means there are, is identified, and the significance of the fact that the unit in question has been identified is made use of in all subsequent stages of the analysis. At any time one must be prepared to reconsider an already established analysis of a on the ground that evidence againstthe analysis may successively accumulate due to what analyses other units arrive at. In next section we give a brief description of the analysis system for Swedish that is now under development at our group in Stockholm. As has been said, much effort is spent on trying to make use of surface signals as much as possible. Not that we believe that surface signals play a more important role than any other type of linguistic signals, but rather that we think it is important to try to optimize each single subprocess (in a parallel system) as much as possible, and, as said, it might be worth while to look careful into this level, because the importance of surface signals might have been underestimated in previous research. Our experiments so far seem to indicate that they constitute excellent units to base heuristic guesses on. Another reason for concentrating our efforts on this level is that it takes time and requires much hard computational work to get such an anarchistic system to really work, and this surface level is reasonably simple to handle. II AN OUTLINE OF AN ANALYZER BASED ON THE HEURISTIC PRINCIPLE Figure 1 below shows the general outline of the system. Each of the various boxes (or subboxes) represents one specific process, usually a complete computer program in itself, or, in some cases, independent processes within a program. The big &amp;quot;container&amp;quot;, labelled &amp;quot;The Pool&amp;quot;, contains both the linguistic material as well as the current analysis of it. Each program or process looks into the Pool for things &amp;quot;it&amp;quot; can recognize, and when the process finds anything it is trained to recognize, it adds its observation to the material in the Pool. This added material may (hopefully) help other processes in recognizing what theyare trained to recognize, which in its turn may again help the first process to recognize more of &amp;quot;its&amp;quot; units. And so on. The system is now under development and during this build-up phase each process is, as was said above, essentially a complete, stand-alone module, and the Pool exists simply as successively updated text files on a disc storage. At the moment some programs presuppose that other programs have already been run, but this state of affairs will be valid just during this build-up phase. At the end of the build-up phase each program shall be able to run completely independent of any other program in the system and in arbitrary order relative to the others (but, of course, usually perform better if more information is available in the Pool). 67 In the iecond phase superordinated control programs are to be implemented. These programs will function as &amp;quot;traffic rules&amp;quot; and via these systems one shall be able to test various strategies, i.e. to test which relative order between the different subsystems that yields optimal result in some kind of &amp;quot;performance metric&amp;quot;, some evaluation procedure that takes both speed and quality into account. The programs/processes shown in Figure 1 all represent rather straightforward Finite State Pattern Matching (FS/PM) procedures. It is rather trivial to show mathematically that a set of interacting FS/PM procedures of the type used in our system together will yield a system that formally has the power of a CF-parser; in practice it will yield a system that in some sense is stronger, at least from the point of view of convenience. Congruence and similar phenomena will be reduced to simple local observations. Transformational variants of sentences will be redirectly there will be no performing some kind of backward transformational operations. (In this respect a system like this will resemble Gazdar&apos;s grammar concept; Gazdar 1980.) The control structures later to be superimposed on the interacting FS/PM systems will also be of a Finite State type. A system of the type then obtained a system of independent Finite State Automatons controlled by another Finite State Automaton will in principle have rather complex mathematical properties. It is, e.g., rather easy to see that such a system has stronger capacity than a Type 2 device, but it will not have the power of a full Type I system. - - - Now a few comments to Figure 1 The &amp;quot;balloons&amp;quot; in the figure represent independent programs (later to be developed into independent processes inside one &amp;quot;big&amp;quot; program). The figure displays those programs that so far (January 1983) have been implemented and tested (to some extent). Other programs will successively be entered into the system. The big balloon labelled &amp;quot;The Closed Cat&amp;quot; represents a program that recognizes closed word classes such as prepositions, conjunctions, pronouns, auxiliaries, and so on. The Closed Cat recognizes full word forms directly. The SMURF balloon represents the morphological component (SMURF = &amp;quot;Swedish Murphology&amp;quot;). SMURF itself is organized internally as a complex system of independently nperating &amp;quot;demons&amp;quot; - SMURFs each little corner of Swedish word formation. (The name of the program is an allusion to the powilar comic strip leprechauns &amp;quot;les Schtroumpfs&amp;quot;, which in Swedish are called &amp;quot;smurfar&amp;quot;0 Thus there is one little smurf recognizing derivational morphemes, one recognizing flectional endings, and so on. One special smurf, Phonotax, has an important controlling function every other smurf must always consult Phonotax before identifying one of &amp;quot;its&amp;quot; (potential) formatives; the word minus this formative must still be pronounceable, otherwise it cannot be a formative. SMURF works entirely without stem lexicon; it adheres completely to the &amp;quot;philosophy&amp;quot; of using surface signals as far as possible. NOMFRAS, VERBAL, IFIGEN, CLAUS and PREPPS are other &amp;quot;demons&amp;quot; that recognize different phrases or word groups within sentences, viz, noun phrases, verbal complexes, infinitival constructions, clauses and prepositional phrases, respectively. V-lex and A-lex represent various (sub)lexicons; so far we have tried to do without them as far as possible. One should observe that stem lexicons are no prerequisites for the system to work, adding them only enhances its performance. The format of the material inside the Pool is the original text, plus appropriate &amp;quot;labelled brackets&amp;quot; enclosing words, word groups, phrases and so on. In this way, the form of representation is consistent throughout, no matter how many different types of analyses have been applied to it. Thus, various people can join our group and write their own &amp;quot;demons&amp;quot; in whatever language they prefer, as long as they can take sentences in text format, be reasonably tolerant to what types of they find in there, do their analysis, add their own brackets (in the specified format), and put the result back into the Pool. 68 Of the various programs SMURF, NOMFRAS and IFIGEN are extensively tested (and, of course, The Closed Cat, which is a simple lexical lookup system), and various examples of analyses of these programs will be demonstrated in the next section. We hope to arrive at a crucial station in this project during 1983, when CLAUS has been more thoroughly tested. If CLAUS performs the way we hope (and preliminary tests indicate that it will), we will have means to identify very quickly the clausal structures of the sentences in an arbitrary running text, thus having a firm base for entering higher hierarchies in the syntactic domains. The programs are written in the Beta language developed by the present author; c.f. Brodda- Karlsson, 1980, and Brodda, 1983, forthcoming. Of the actual programs in the system, SMURF was developed and extensively tested by B.B. during 1977-79 (Brodda, 1979), whereas the others are (being) developed by B.B. and/or Gunnel Kallgren, Stockholm (mostly &amp;quot;and&amp;quot;). III EXPLODING SOME OF THE BALLOONS When a &amp;quot;fresh&amp;quot; text is entered into The Pool it first passes through a preliminary one-passprogram, INIT, (not shown in Fig. 1) that &amp;quot;normalizes&amp;quot; the text. The original text may be of any type as long as it is regularly typed Swedish. INIT transforms the text so that each graphic sentence will make up exactly one physical record. (Except in poetry, physical records, i.e. lines, usually are of marginal linguistic interest.) Paragraph ends will be represented by empty records. Periods used to indicate abbreviations are just taken away and the abbreviation itself is contracted to one graphic word, if necessary; thus &amp;quot;t.ex.&amp;quot; (&amp;quot;e.g.&amp;quot;) is transformed into &amp;quot;tax&amp;quot;, and so on. Otherwise, periods, commas, question marks and other typographic characters are provided with preceding blanks. Through this each word is guaranteed to be surrounded by blanks, and delimiters like commas, periods and so on are guaranteed to signal their &amp;quot;normal&amp;quot; textual functions. Each record is also ended by a sentence delimiter (preceded by a blank). Some manual postediting is sometimes needed in order to get the text normalized according to the above. In the INIT-phase no linguistic analysis whatsoever is introduced (other than into what appears to be orthographic sentences). INIT also changes all letters in the original text to their corresponding upper case variants. (Originally capital letters are optionally provided with a prefixed &amp;quot;=&amp;quot;.) All subsequent analysis programs add their analyses in the form of lower case letters or letter combinations. Thus upper case letters or words will belong to the object language, and lower case letters or letter combinations will signal meta-language information. In this way, strictly text (ASCII) format can be kept for the text as well as for the various stages of its analysis; the &amp;quot;philosophy&amp;quot; to use text input and text output for all programs involved represents the computational solution to the problem of how to make it possible for each process to work independently of all other in the system. The Closed Cat (CC) has the important role to mark words belonging to some well defined closed categories of words. This program makes no internal analysis of the words, and only takes full words into account. CC makes use of simple rewrite rules of the type &amp;quot;PA =&gt; ePAe / (blank) (blank)&amp;quot;, where the inserted es represent the &amp;quot;analysis&amp;quot; (&amp;quot;e&amp;quot; stands for &amp;quot;preposition&amp;quot;; PA = &amp;quot;on&amp;quot;). A sample output from The Closed Cat is shown in illustration 2, where the various meta-symbols also are explained. The simple &apos;example above also shows the format of inserted meta-information. Each identified constituent is &amp;quot;tagged&amp;quot; with surrounding lower case letters, which then can be conceived of as labelled brackets. This format is used throughout the system, also for complex constituents. Thus the nominal phrase &amp;quot;DEN LILLA FLICKAN&amp;quot; (&amp;quot;the little girl&amp;quot;) will be tagged as &amp;quot;nDEN+LILLA+FLICKANn&amp;quot; by NOMFRAS (cf. below; the pluses are inserted to make the constituent one continuous string). We have reserved the letters n, v and s for the major categories nouns or noun phrases, verbs or verbal groups, and sentences, respectively, whereas other more or less transparent letters are used for other categories. (A list of used category symbols is presented in the Appendix: Printout Illustrations.) The program SWEMRF (or SMURF, as it is called here) has been extensively described elsewhere (Brodda, 1979). It makes a rather intricate morphological analysis word-by-word in running text (i.e. SMURF analyzes each word in itself, disregarding the context it appears in). SMURF can be run in two modes, in &amp;quot;segmentation&amp;quot; mode and &amp;quot;analysis&amp;quot; mode. In its segmentation mode SMURF simply strips off the possible affixes from each word; it makes no use of any stem lexicon. (The affixes it recognizes are common prefixes, suffixes i.e. derivational morphemes and flexional endings.) In analysis mode it also tries to make an optimal guess of the word class of the word under inspection, based on what (combinations of) word formation elements it finds in the word. SMURF in itself is organized entirely according to the heuristic principles as they are conceived here, i.e. as a set of independently operating processes that interactively work on each others output. The SMURF system has been the test bench for testing out the methods now being used throughout the entire Heuristic Parsing Project. In its segmentation mode SMURF functions formally as a set of interactive transformations, where the structural changes happen to be extremely simple, viz, simple segmentation rules of the type 1P=&gt;P-&amp;quot;, &amp;quot;S=&gt; -S&amp;quot; and &amp;quot;E=&gt;-E&amp;quot; for an arbitrary Prefix, Suffix and Ending, respectively, but where the &amp;quot;job&amp;quot; essentially consists of establishing the corresponding structural descriptions. These are shown in Ill. 1, below, together with sample analyses. It should be noted that phonotactic constraints play a central role 69 in the SMURF system; in fact, one of the main objectives in designing the SMURF system was to find out how much information actually was carried by the phonotactic component in Swedish. (It turned out to be quite much; cf. Brodda 1979. This probably holds for other Germanic languages as well, which all have a rather elaborated phonotaxis.) NOMFRAS is the next program to be commented on. The present version recognizes structures of the type det/quant + (adj)g + noun; where the &amp;quot;det/quant&amp;quot; categories (i.e. determiners or quantifiers) are defined explicitly through enumeration they are supposed to belong to the class of &amp;quot;surface markers&amp;quot; and are as such identified by The Closed Cat. Adjectives and nouns on the other hand are identified solely on the ground of their &amp;quot;cadences&amp;quot;, i.e. what kind of (formally) ending-like strings they happen to end with. The number of adjectives that are accepted (n in the formula above) varies depending on what (probable) type of construction is under inspection. In indefinite noun phrases the substantial content of the expected endings is, to say the least, meager, as both nouns and adjectives in many situations only have 0-endings. In definite noun phrases the noun mostly but not always has a more substantial and recognizable ending and all intervening adjectives have either the cadence -A or a cadence from a small but characteristic set. In a (supposed) definite noun phrase all words ending in any of the mentioned cadences are assumed to be adjectives, but in (supposed) indefinite noun phrases not more than one adjective is assumed unless other types of morphological support are present. The Finite State Scheme behind NOMFRAS is presented in Ill. 2, together with sample outputs; In this case the text has been preprocessed by The Closed Cat, and it appears that these two programs in cooperation are able to recognize noun phrases of the discussed type correctly to well over 95% in running text (at a speed of about 5 sentences per second, CPU-time); the errors were shared about 50% each between overand undergenerations. Preliminary experiments aiming at including also SMURF and PREPPS (Prepositional Phrases) seem to indicate that about the same recall and precision rate could be kept for arbitrary types of (nonsentential) noun phrases (cf. Ill. 6). (The systems are not yet trimmed to the extent that they can be operatively run together.) IFIGEN (Infinitive Generator) is another rather straightforward Finite State Pattern Matcher (developed by Gunnel Kallgren). It recognizes (groups of) nonfinite verbs. Somewhat simplified it can be represented by the following diagram (remember the conventions for upper and lower case): diagram (simplified): Aux nXn 2 ATT where &amp;quot;Aux&amp;quot; and &amp;quot;Adv&amp;quot; are categories recognized by The Closed Cat (tagged &amp;quot;g&amp;quot; and &amp;quot;a&amp;quot;, respectively), and &amp;quot;nXn&amp;quot; are structures recognized by either NOMFRAS or, in the case of personal pronouns, by CC (It should be worth mentioning that the class of auxiliaries in Swedish is more open than the corresponding word class in English; besides the &amp;quot;ordinary&amp;quot; VARA (&amp;quot;to be&amp;quot;), HA (&amp;quot;to have&amp;quot;) and the modals, there is a fuzzy class of semi-auxiliaries like BoRJA (&amp;quot;begin&amp;quot;) and others; IFIGEN makes use of about 20 of these in the present version.) The cadence supposed to appear only once in an infinitival group. A sample output of IFIGEN is given in Ill. 3. Also for IFIGEN we have reached a recognition level around 95%, which, again, is rather astonishing, considering how little information actually is made use of in the system. The IFIGEN case illustrates very clearly one of the central points in our heuristic approach, namely the following: The information that a word has a specific cadence, in this case the cadence usually of very little significance in itself in Swedish. Certainly it is a typical infinitival cadence (at least 90% of all infinitives in Swedish have it), but on the other hand, it is certainly a very typical cadence for other types of words as well: FLICKA (noun), HELA (adjective), DENNA/DETTA/DESSA (determiners or pronouns) and so on, and these other types are by no comparison the dominant group having this specific cadence in text. connection with an &amp;quot;infinitive warner&amp;quot; an auxiliary, or the word ATT the situation changes dramatically. This can be demonstrated by the following figures: In running text having the cadance infinitives in about 30% of the cases. ATT is an infinitive marker (equivalent to &amp;quot;to&amp;quot;) in quite exactly 50% of its occurences (the other 50% it is a subordinating conjunction). The conditional probability the configuration ..-A an infinitve is, however, greater than 99%, prothat characteristic cadences like ORNA and quantifiers/determiners like ALLA and DESSA are disregarded (In our system they are marked by SMURF and The Closed Cat, respectively, and thereby &amp;quot;saved&amp;quot; from being classified as infinitives.) Given this, there is almost no overgeneration in IFIGEN, but Swedish allows for split infinitives to some extent. Quite much material can be put in between the infinitive warner and the infinitive, and this gives rise to some undergeneration (presenzly). (Similar observations regarding conditional probabilities in configurations of linguistic units has been made by Mats</abstract>
<note confidence="0.737180580645161">Eeg-Olofson, Lund, 1982). -A 70 IV REFERENCES Brodda, B. &amp;quot;Nagot om de svenska ordens fonotax och morfotax&amp;quot;, Papers from the Institute of Linguistics (PILUS) No. 38, University of Stockholm, 1979. B. f6r igenkgnning av sammansgttningar&amp;quot; in Saari, M. and Tandefelt, M. (eds.) F6rhandlingar r6rande svenskans beskrivning - Hanaholmen 1981, Meddelanden fran Institutionen f6r Nordiska Sprak, Helsingfors Universitet, 1981 Brodda, B. &amp;quot;The BETA System, and some Applications&amp;quot;, Data Linguistics, Gothenburg, 1983 (forthcoming). Brodda, B. and Karlsson, F. &amp;quot;An experiment with Automatic Morphological Analysis of Finnish&amp;quot;, No. 7, Dept. of Linguistics, University of Helsinki, 1981. Cazdar, G. &amp;quot;Phrase Structure&amp;quot; in Jacobson, P. and Pullam G. (eds.), Nature of Syntactic Representation, Reidel, 1982 Lenat, D.P. &amp;quot;The Nature of Heuristics&amp;quot;, Artificial Intelligence, Vol 19(2), 1982. Eeg-Olofsson, M. &amp;quot;En sprakstatistisk modell f6r ordklassmgrkning i 16pande text&amp;quot; in Kgllgren, G. (ed.) TAGGNING, F6redrag frail 3:e svenska kollokviet i spraklig databehandling i maj 1982, PILUS 47, Stockholm 1982.</note>
<affiliation confidence="0.977514">Polya, G. &amp;quot;How to Solve it&amp;quot;, Princeton University</affiliation>
<address confidence="0.8697245">Press, 1945. Also Doubleday Anchor Press, New York, N.Y. (several editions).</address>
<abstract confidence="0.853352835365854">APPENDIX: Some computer illustrations The following three pages illustrate some of the parsing diagrams used in the system: Ill. 1, SMURF, and III. 2, NOMFRAS, together with sample analyses. IF1GEN is represented by sample analyses (Ill. 3; the diagram is given in the text). The samples are all taken from running text analysis (from a novel by Ivar Lo-Johansson), and &amp;quot;pruned&amp;quot; only in the way that trivial, recurrent examples are omitted. Some typical erroneous analyses are also shown (prefixed by **). In Ill. 1 SMURF is run in segmentation mode only, and the existing tags are inserted by the Closed Cat. &apos;A and &apos;E in word final position indicates the cadences (fullfilling the pattern where M denotes a set of admissible medial clusters). The tags inserted by CC are: a=(sentence) adverbials, b=particles, d=determiners, e=prepositions, g=auxiliaries, h=(forms of) HA(VA), i=infinitives, j=adjectives, n=nouns, K=conjunctions, q=quantifiers, r=pronouns, u=supine verb form, v=verbal (group). space reasons, Ill. 3 is given first, then I and 3: FRAMATOJD HELA DAGEN.. ciETTq KXDSTRECK eIe eTILLe ikATTk+iSSi clENq KARL INUTI? VIPPEN? HEM eMEDe SKAMMEN e0Me WARSOMHELST. ePAe rDETr. NAT eMEDe rDENr, kSAk eUPPe POTATISEN. BALLONGEN FYLLD. SEJ OPPE. STILLA eUNDERe OSS. SITT MAL. ..PLOCKNINGEN eEFTER..ikATTk+iHAi+uGATTui rDETr vVARv ORIMLIGT ikATTk+iFINNAi rJAGr gSKAg aBARAa iHJXLPAi rDETr gKANg iLIGGAi gSKAg rVIr iVAGAi rVIr gKANg aINTEa iGki ...ORNA vHoLLv SIG FXRDIGA ikATTk+iKASTAi rDEr gVAGADEg aXNTLIGENa iLYFTAi gSKAg rNIr alloDVANDIGTVISa iGORAi ..rVIr hHADEh aANNUa aINTEa uHUNNITu iFAi ..BECKMoRKRET eMEDe ikATTk+iFoRSoKAi+iFAi eMEDe VXTGAS eFoRe ikATTk+iKUNNAi+iHALLAi SKOGEN, LANDEN gTYCKTESg iSTAi rDENr hHADEh MISSLYCKATS eIe ikATTk+iNAi clENq iKVINNORNA+STANNAi 71 1: - PARSING DIAGRAM FOR SWEDISH MORPHOLOGY Structural changes Descriptions&amp;quot;): ENDINGS (El:m PREFIXES (P):1# ; X - V . F (s) — E=&gt;=E P =&gt; (-)P&gt; (S): (s) I V - &apos;V ; S =&gt; I = (admissible) initial cluster, F = final cluster, Me = morphcluster, V = vowel, (s) the &amp;quot;gluon&amp;quot;S (cf. TIDNINGSMAN), &amp;quot;, finallay, denotes ordinary concatenation. (It is the enhanced element in each pattern that is tested for its segmentability). BAGG&apos;E vDROGv . REP=ET SLINGR=ADE MELLAN STEN=AR , FoR&gt;BI TALLSTAMM=AR , MELLAN RDA L/NGONTUV=OR eIe GRN IN&gt;FATT/NING. clETTq STORT FoRE&gt;MAL hHADEh uRoRTu ePAe SIG BORT&apos;A eIe SLANT=EN . FoRE&gt;MAL=ET NARM=ADE SIG HOTFULL&apos;T . dDETd KNASTR= =ADE eIe SKOG=EN . - SPRING . SLAPP=TE kOCHk vSPRANGv . rDEr LANG&apos;A KJOL=ARNA VIRV1=ADE etiVERe O&lt;PLOCK=ADE LINGONTUV=OR . BAGG&apos;E KVINNO=RNA hHADEh STRUMPEBAND FoR&gt;FARDIG=ADE eAVe SOCKERTOPPSSOR=EN , KNUT=NA NEDAN&gt;FoR KNAN&apos;A . bUPPEb ePAe cIENq KVINNO=RNA STANN&apos;A . rDEr vSTODv kOCHk STRACK=TE ePAe HALS=ARNA . ciENq FRAN UT&gt;DUNST/NING eAVe SKRACK SIPPR=ADE bFRAMb . rDEr vHEILLv BE&gt;sVARJ/ANDE HAND=ERNA FRAM&gt;FoR SIN&apos;A SKoT=EN . dDETd vSERv STORT kOCHk eRUNTe bUTb , vSAv dDENd KORT&apos;A e0Me FoRE&gt;MAL=ET . dDETd vARv aVALa aINTEa ciNAGOTq IN&gt;UT&gt;I ? dDETd gKANg LIGG&apos;A cIENq KARL IN&gt;UT&gt;I ? dDETd vVETv rMANr kVADk rHANr vGoRv eMEDe OSS . rJAGr TYCK=TE dDETd RoR=DE ePAe SEJ . gSKAg rVIr iVAGAi VIPP=EN ? - JA ? gSKAg rVIr 1VAGAl VIPP=EN ? BAGGE vSMoGv SIG ePAe GLAPP&apos;A KNAN UT&gt;FoR BRANT=EN . kNARk rDEr NARM=ADE SIG rDEr FLAT=ADE POTATISKORG=ARNA eMEDe LINGON kSOMk vSTODv ePAe LUT eVIDe VARSIN TUVA , vVARv rDEr aREDANa UT&gt;OM SIG eAVe SKRACK . oDERASo SANS vVARv BORT&apos;A . PASS ePAe . rVIr KANHAND&apos;A aINTEa vToRSv NARM=ARE ? dDENd MAGR&apos;A HUSTRUN . rVIr gKANg aINTEa GA HEM eMEDe SKAMM=EN aHELLERa . rVIr gMASTEg aJUa iHAi WARKORG=ARNA eMEDe . - JAVISST , BARKORG=ARNA . kMENk kNARk rDEr uKOMMITu bNERb eTILLe STALL=ET I&lt;GEN uVARTu rDEr NYFIK=NA . rDEr vDROGSv eTILLe FoRE&gt;MAL=ET eIe 72 2: - FS-DIAGRAM FOR SWEDISH NOUN PHRASE PARSING noun NAG 01 -T EGEN EGET A/ E/ 0 -0/-E MIN MITT -S IDENNA1 IDETTA DESSA1 -0R/-AR/-ER/-N1-01 MINA -S EGNA -AN/-EN -ET IDEN IDET ALLA BADA ToT + + &amp;quot;OWN&amp;quot; + -AD(E) -ANDE -ENDE -S - PYTT , vSAv nDEN+LANGAn kVADk vVARv NU nDET+DARn nDET+OMFANGSRIKA+,+SIDENLATTA+TYGETn nDEn GJORDE nEN+STOR+PACKEn eMEDe SIG SJALVA e0Me kATTk nDET+HELAn nDET+HELAn aINTEa uVARITu nETT+DUGGn nDET+FoRMENTA+KLADSTRECKETn GRoN eMEDe HANG8JoRKAR kSOMk nALLAn .. MODERN , nDEN+LANGA+EGNAHEMSHUSTRUNn STORA BOKSTAVER nETT+SVENSKT+FIRMANAMNn ePAe nDEN+ANDRA+,+FRANVANDAn nDETn vVARv nEN+LUFTENS+SPILLFRUKTn kOCHk nDEN+ANDRA+EGNAHEMSHUSTRUNS+6GONn nETT+STORT+MOSSIGT+BERGn .. SIG eMOTe SKYN eMEDe nEN+DISIG+MANEn eVIDe nDET+STALLEn SAGA HONOM kATTk nALLA+DESSA+FdREMALn ..ARNA kSOMk nEN+AVIGT+SKRUBBANDE+HANDn kSOMk nEN+OFORMLIG+MASSAn nEN+RIKTIG+BALLONGn **nDETn aINTEa vLAGv nNAGON+KROPP+GoMDn *** TVA kSOMk BARGADE nDEN+TILLSAMMANSn • kATTk VARA RADD eFoRe ? eAVe dDETd . aINTEa uVARITu ciETTq FARLIGT . vVARv kDAk SNOTT FLE.. FYLLDE FUNKTIONER kSOMk uVARITu eIe SKO.. vSTODv .. kSOMk hHADEh uRAMLAT.. eAVe HoJDE SIG eMOTe SKYN.. kSOMk cIENq RUND LYKTA kDKRk LANDNINGSLINAN aANDIa aINTEa FoRMAD.. VALTRADE SIG BALLONG.. gSKAg VARA FYLLD eMEDe.. INUNDER .</abstract>
<intro confidence="0.723353">73</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Brodda</author>
</authors>
<title>The BETA System, and some Applications&amp;quot;, Data Linguistics, Gothenburg,</title>
<date>1983</date>
<contexts>
<context position="15627" citStr="Brodda, 1983" startWordPosition="2565" endWordPosition="2566">ystem), and various examples of analyses of these programs will be demonstrated in the next section. We hope to arrive at a crucial station in this project during 1983, when CLAUS has been more thoroughly tested. If CLAUS performs the way we hope (and preliminary tests indicate that it will), we will have means to identify very quickly the clausal structures of the sentences in an arbitrary running text, thus having a firm base for entering higher hierarchies in the syntactic domains. The programs are written in the Beta language developed by the present author; c.f. BroddaKarlsson, 1980, and Brodda, 1983, forthcoming. Of the actual programs in the system, SMURF was developed and extensively tested by B.B. during 1977-79 (Brodda, 1979), whereas the others are (being) developed by B.B. and/or Gunnel Kallgren, Stockholm (mostly &amp;quot;and&amp;quot;). III EXPLODING SOME OF THE BALLOONS When a &amp;quot;fresh&amp;quot; text is entered into The Pool it first passes through a preliminary one-passprogram, INIT, (not shown in Fig. 1) that &amp;quot;normalizes&amp;quot; the text. The original text may be of any type as long as it is regularly typed Swedish. INIT transforms the text so that each graphic sentence will make up exactly one physical record.</context>
</contexts>
<marker>Brodda, 1983</marker>
<rawString>Brodda, B. &amp;quot;The BETA System, and some Applications&amp;quot;, Data Linguistics, Gothenburg, 1983 (forthcoming).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Brodda</author>
<author>F Karlsson</author>
</authors>
<title>An experiment with Automatic Morphological Analysis of Finnish&amp;quot;,</title>
<date>1981</date>
<journal>Publications</journal>
<volume>7</volume>
<institution>Dept. of Linguistics, University of Helsinki,</institution>
<marker>Brodda, Karlsson, 1981</marker>
<rawString>Brodda, B. and Karlsson, F. &amp;quot;An experiment with Automatic Morphological Analysis of Finnish&amp;quot;, Publications No. 7, Dept. of Linguistics, University of Helsinki, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cazdar</author>
</authors>
<title>Phrase Structure&amp;quot;</title>
<date>1982</date>
<booktitle>Nature of Syntactic Representation,</booktitle>
<editor>in Jacobson, P. and Pullam G. (eds.),</editor>
<location>Reidel,</location>
<marker>Cazdar, 1982</marker>
<rawString>Cazdar, G. &amp;quot;Phrase Structure&amp;quot; in Jacobson, P. and Pullam G. (eds.), Nature of Syntactic Representation, Reidel, 1982</rawString>
</citation>
<citation valid="false">
<authors>
<author>D P Lenat</author>
</authors>
<title>The Nature of Heuristics&amp;quot;,</title>
<journal>Artificial Intelligence, Vol</journal>
<volume>19</volume>
<issue>2</issue>
<pages>1982</pages>
<marker>Lenat, </marker>
<rawString>Lenat, D.P. &amp;quot;The Nature of Heuristics&amp;quot;, Artificial Intelligence, Vol 19(2), 1982.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>