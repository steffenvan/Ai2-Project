<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000376">
<title confidence="0.975531">
Robust Extraction of Named Entity Including Unfamiliar Word
</title>
<author confidence="0.996143">
Masatoshi Tsuchiya† Shinya Hida$ Seiichi Nakagawa$
</author>
<affiliation confidence="0.99875">
†Information and Media Center / $Department of Information and Computer Sciences,
Toyohashi University of Technology
</affiliation>
<email confidence="0.994231">
tsuchiya@imc.tut.ac.jp, {hida,nakagawa}@slp.ics.tut.ac.jp
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905214285714">
This paper proposes a novel method to extract
named entities including unfamiliar words
which do not occur or occur few times in a
training corpus using a large unannotated cor-
pus. The proposed method consists of two
steps. The first step is to assign the most simi-
lar and familiar word to each unfamiliar word
based on their context vectors calculated from
a large unannotated corpus. After that, tra-
ditional machine learning approaches are em-
ployed as the second step. The experiments of
extracting Japanese named entities from IREX
corpus and NHK corpus show the effective-
ness of the proposed method.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935739130435">
It is widely agreed that extraction of named entity
(henceforth, denoted as NE) is an important sub-
task for various NLP applications. Various ma-
chine learning approaches such as maximum en-
tropy(Uchimoto et al., 2000), decision list(Sassano
and Utsuro, 2000; Isozaki, 2001), and Support
Vector Machine(Yamada et al., 2002; Isozaki and
Kazawa, 2002) were investigated for extracting NEs.
All of them require a corpus whose NEs are an-
notated properly as training data. However, it is dif-
ficult to obtain an enough corpus in the real world,
because there are increasing the number of NEs like
personal names and company names. For example,
a large database of organization names(Nichigai As-
sociates, 2007) already contains 171,708 entries and
is still increasing. Therefore, a robust method to ex-
tract NEs including unfamiliar words which do not
occur or occur few times in a training corpus is nec-
essary.
This paper proposes a novel method of extract-
ing NEs which contain unfamiliar morphemes us-
ing a large unannotated corpus, in order to resolve
the above problem. The proposed method consists
</bodyText>
<tableCaption confidence="0.999492">
Table 1: Statistics of NE Types of IREX Corpus
</tableCaption>
<table confidence="0.9951978">
NE Type Frequency (%)
ARTIFACT 747 (4.0)
DATE 3567 (19.1)
LOCATION 5463 (29.2)
MONEY 390 (2.1)
ORGANIZATION 3676 (19.7)
PERCENT 492 (2.6)
PERSON 3840 (20.6)
TIME 502 (2.7)
Total 18677
</table>
<bodyText confidence="0.9995182">
of two steps. The first step is to assign the most
similar and familiar morpheme to each unfamiliar
morpheme based on their context vectors calculated
from a large unannotated corpus. The second step is
to employ traditional machine learning approaches
using both features of original morphemes and fea-
tures of similar morphemes. The experiments of
extracting Japanese NEs from IREX corpus and
NHK corpus show the effectiveness of the proposed
method.
</bodyText>
<subsectionHeader confidence="0.72407">
2 Extraction of Japanese Named Entity
2.1 Task of the IREX Workshop
</subsectionHeader>
<bodyText confidence="0.99996275">
The task of NE extraction of the IREX workshop
(Sekine and Eriguchi, 2000) is to recognize eight
NE types in Table 1. The organizer of the IREX
workshop provided a training corpus, which consists
of 1,174 newspaper articles published from January
1st 1995 to 10th which include 18,677 NEs. In the
Japanese language, no other corpus whose NEs are
annotated is publicly available as far as we know.1
</bodyText>
<subsectionHeader confidence="0.999966">
2.2 Chunking of Named Entities
</subsectionHeader>
<bodyText confidence="0.998681333333333">
It is quite common that the task of extracting
Japanese NEs from a sentence is formalized as
a chunking problem against a sequence of mor-
</bodyText>
<footnote confidence="0.997450333333333">
1The organizer of the IREX workshop also provides the test-
ing data to its participants, however, we cannot see it because
we did not join it.
</footnote>
<page confidence="0.953676">
125
</page>
<reference confidence="0.2174725">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 125–128,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.980790933333333">
phemes. For representing proper chunks, we em-
ploy IOB2 representation, one of those which have
been studied well in various chunking tasks of
NLP (Tjong Kim Sang, 1999). This representation
uses the following three labels.
B Current token is the beginning of a chunk.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
Actually, we prepare the 16 derived labels from the
label B and the label I for eight NE types, in order
to distinguish them.
When the task of extracting Japanese NEs from
a sentence is formalized as a chunking problem of a
sequence of morphemes, the segmentation boundary
problem arises as widely known. For example, the
NE definition of IREX tells that a Chinese character
“米 (bei)” must be extracted as an NE means Amer-
ica from a morpheme “訪米 (hou-bei)” which means
visiting America. A naive chunker using a mor-
pheme as a chunking unit cannot extract such kind of
NEs. In order to cope this problem, (Uchimoto et al.,
2000) proposed employing translation rules to mod-
ify problematic morphemes, and (Asahara and Mat-
sumoto, 2003; Nakano and Hirai, 2004) formalized
the task of extracting NEs as a chunking problem
of a sequence of characters instead of a sequence of
morphemes. In this paper, we keep the naive formal-
ization, because it is still enough to compare perfor-
mances of proposed methods and baseline methods.
</bodyText>
<sectionHeader confidence="0.933585" genericHeader="method">
3 Robust Extraction of Named Entities
Including Unfamiliar Words
</sectionHeader>
<bodyText confidence="0.999965888888889">
The proposed method of extracting NEs consists
of two steps. Its first step is to assign the most
similar and familiar morpheme to each unfamiliar
morpheme based on their context vectors calculated
from a large unannotated corpus. The second step is
to employ traditional machine learning approaches
using both features of original morphemes and fea-
tures of similar morphemes. The following sub-
sections describe these steps respectively.
</bodyText>
<subsectionHeader confidence="0.999936">
3.1 Assignment of Similar Morpheme
</subsectionHeader>
<bodyText confidence="0.975234666666667">
A context vector Vm of a morpheme m is a vector
consisting of frequencies of all possible unigrams
and bigrams,
</bodyText>
<equation confidence="0.994341">
� �
f(m, m0), · · · f(m, mN),
� �
�f(m, m0, m0), ··· f(m, mN, mN), �
� �
�f(m0, m), · · · f(mN, m), �
f(m0, m0, m), ··· f(mN, mN, m)
</equation>
<bodyText confidence="0.999115692307692">
where M = {m0, m1, ... , mN} is a set of all mor-
phemes of the unannotated corpus, f(mi, mj) is a
frequency that a sequence of a morpheme mi and
a morpheme mj occurs in the unannotated corpus,
and f(mi, mj, mk) is a frequency that a sequence
of morphemes mi, mj and mk occurs in the unan-
notated corpus.
Suppose an unfamiliar morpheme mu E MnMF,
where MF is a set of familiar morphemes that occur
frequently in the annotated corpus. The most sim-
ilar morpheme rhu to the morpheme mu measured
with their context vectors is given by the following
equation,
</bodyText>
<equation confidence="0.992226">
mu = argmax sim(Vmu, Vm), (1)
mEMF
</equation>
<bodyText confidence="0.999831333333333">
where sim(Vi,Vj) is a similarity function between
context vectors. In this paper, the cosine function is
employed as it.
</bodyText>
<subsectionHeader confidence="0.952491">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.945787375">
The feature set Fi at i-th position is defined as a tuple
of the morpheme feature MF(mi) of the i-th mor-
pheme mi, the similar morpheme feature SF (mi),
and the character type feature CF(mi).
Fi = (MF(mi), SF(mi), CF(mi) )
The morpheme feature MF(mi) is a pair of the sur-
face string and the part-of-speech of mi. The similar
morpheme feature SF(mi) is defined as
</bodyText>
<equation confidence="0.99497525">
r MF( mi) if mi E M n MF
SF(mi) =Sl
,
MF(mi) otherwise
</equation>
<bodyText confidence="0.843569571428571">
where
is the most similar and familiar morpheme
to
given by Equation (1). The character type fea-
ture
is a set of four binary flags to indi-
cate that the surface string of
</bodyText>
<subsectionHeader confidence="0.814017">
contains a Chinese
</subsectionHeader>
<bodyText confidence="0.89481125">
character, a hiragana character, a katakana charac-
ter, and an English alphabet respectively.
When we identify the chunk label
for the i-
th morpheme
the surrounding five feature sets
Fi,
and the preceding two
</bodyText>
<equation confidence="0.928881666666667">
chunk labels
i
mi
CF(mi)
mi
ci
mi,
Fi−2,Fi−1,
Fi+1,Fi+2
ci−2,ci−1 are refered.
Vm =
,
</equation>
<page confidence="0.985066">
126
</page>
<table confidence="0.998178615384615">
Morpheme Feature Similar Morpheme Feature Character Chunk Label
Type
Feature
(English POS (English POS
translation) translation)
今日 (kyou) (today) Noun–Adverbial 今日 (kyou) (today) Noun–Adverbial (1, 0, 0, 0) O
の (no) gen Particle の (no) gen Particle (0, 1, 0, 0) O
Tint (Ishikari) (Ishikari) Noun–Proper PAX (Kantou) (Kantou) Noun–Proper (1, 0, 0, 0) B-LOCATION
平野 (heiya) (plain) Noun–Generic 平野 (heiya) (plain) Noun–Generic (1, 0, 0, 0) I-LOCATION
の (no) gen Particle の (no) gen Particle (0, 1, 0, 0) O
天気 (tenki) (weather) Noun–Generic 天気 (tenki) (weather) Noun–Generic (1, 0, 0, 0) O
は (ha) top Particle は (ha) top Particle (0, 1, 0, 0) O
晴れ (hare) (fine) Noun–Generic 晴れ (hare) (fine) Noun–Generic (1, 1, 0, 0) O
</table>
<figureCaption confidence="0.965527">
Figure 1: Example of Training Instance for Proposed Method
</figureCaption>
<table confidence="0.370088">
−&gt; Parsing Direction −&gt;
Feature set Fi−2 Fi−1 Fi Fi+1 Fi+2
Chunk label Ci−2 Ci−1 Ci
</table>
<bodyText confidence="0.796462285714286">
Figure 1 shows an example of training instance of
the proposed method for the sentence “今日 (kyou)
の (no) 石狩 (Ishikari) 平野 (heiya) の (no) 天気
(tenki) は (ha) 晴れ (hare)” which means “It is fine at
Ishikari-plain, today”. “関東 (Kantou)” is assigned
as the most similar and familiar morpheme to “石狩
(Ishikari)” which is unfamiliar in the training corpus.
</bodyText>
<sectionHeader confidence="0.997381" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.943315">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997741615384615">
IREX Corpus is used as the annotated corpus to train
statistical NE chunkers, and MF is defined experi-
mentally as a set of all morphemes which occur five
or more times in IREX corpus. Mainichi News-
paper Corpus (1993–1995), which contains 3.5M
sentences consisting of 140M words, is used as
the unannotated corpus to calculate context vectors.
MeCab2(Kudo et al., 2004) is used as a preprocess-
ing morphological analyzer through experiments.
In this paper, either Conditional Random
Fields(CRF)3(Lafferty et al., 2001) or Support Vec-
tor Machine(SVM)4(Cristianini and Shawe-Taylor,
2000) is employed to train a statistical NE chunker.
</bodyText>
<subsectionHeader confidence="0.998669">
4.2 Experiment of IREX Corpus
</subsectionHeader>
<bodyText confidence="0.999987333333333">
Table 2 shows the results of extracting NEs of IREX
corpus, which are measured with F-measure through
5-fold cross validation. The columns of “Proposed”
show the results with 5F, and the ones of “Base-
line” show the results without 5F. The column of
“NExT” shows the result of using NExT(Masui et
</bodyText>
<footnote confidence="0.99544325">
2http://mecab.sourceforge.net/
3http://chasen.org/-taku/software/CRF++/
4http://chasen.org/-taku/software/
yamcha/
</footnote>
<tableCaption confidence="0.970575">
Table 2: NE Extraction Performance of IREX Corpus
</tableCaption>
<table confidence="0.999958181818182">
Proposed Baseline NExT
CRF SVM CRF SVM
ARTIFACT 0.487 0.518 0.458 0.457 -
DATE 0.921 0.909 0.916 0.916 0.682
LOCATION 0.866 0.863 0.847 0.846 0.696
MONEY 0.951 0.610 0.937 0.937 0.895
ORGANIZATION 0.774 0.766 0.744 0.742 0.506
PERCENT 0.936 0.863 0.928 0.928 0.821
PERSON 0.825 0.842 0.788 0.787 0.672
TIME 0.901 0.903 0.902 0.901 0.800
Total 0.842 0.834 0.821 0.820 0.732
</table>
<tableCaption confidence="0.993283">
Table 3: Statistics of NE Types of NHK Corpus
</tableCaption>
<table confidence="0.997831333333333">
NE Type Frequency (%)
DATE 755 (19%)
LOCATION 1465 (36%)
MONEY 124 (3%)
ORGANIZATION 1056 (26%)
PERCENT 55 (1%)
PERSON 516 (13%)
TIME 101 (2%)
Total 4072
</table>
<bodyText confidence="0.990571230769231">
al., 2002), an NE chunker based on hand-crafted
rules, without 5-fold cross validation.
As shown in Table 2, machine learning ap-
proaches with 5F outperform ones without 5F.
Please note that the result of SVM without 5F and
the result of (Yamada et al., 2002) are comparable,
because our using feature set without 5F is quite
similar to their feature set. This fact suggests that
5F is effective to achieve better performances than
the previous research. CRF with 5F achieves better
performance than SVM with 5F, although CRF and
SVM are comparable in the case without 5F. NExT
achieves poorer performance than CRF and SVM.
</bodyText>
<subsectionHeader confidence="0.99987">
4.3 Experiment of NHK Corpus
</subsectionHeader>
<bodyText confidence="0.9998988">
Nippon Housou Kyoukai (NHK) corpus is a set of
transcriptions of 30 broadcast news programs which
were broadcasted from June 1st 1996 to 12th. Ta-
ble 3 shows the statistics of NEs of NHK corpus
which were annotated by a graduate student except
</bodyText>
<page confidence="0.998417">
127
</page>
<tableCaption confidence="0.998236">
Table 4: NE Extraction Performance of NHK Corpus
</tableCaption>
<table confidence="0.9999539">
Proposed Baseline NExT
CRF SVM CRF SVM
DATE 0.630 0.595 0.571 0.569 0.523
LOCATION 0.837 0.825 0.797 0.811 0.741
MONEY 0.988 0.660 0.971 0.623 0.996
ORGANIZATION 0.662 0.636 0.601 0.598 0.612
PERCENT 0.538 0.430 0.539 0.435 0.254
PERSON 0.794 0.813 0.752 0.787 0.622
TIME 0.250 0.224 0.200 0.247 0.260
Total 0.746 0.719 0.702 0.697 0.615
</table>
<tableCaption confidence="0.989345">
Table 5: Extraction of Familiar/Unfamiliar NEs
</tableCaption>
<table confidence="0.999327333333333">
Familiar Unfamiliar Other
CRF (Proposed) 0.789 0.654 0.621
CRF (Baseline) 0.757 0.556 0.614
</table>
<bodyText confidence="0.990735045454545">
for ARTIFACT in accordance with the NE definition
of IREX. Because all articles of IREX corpus had
been published earlier than broadcasting programs
of NHK corpus, we can suppose that NHK corpus
contains unfamiliar NEs like real input texts.
Table 4 shows the results of chunkers trained from
whole IREX corpus against NHK corpus. The meth-
ods with SF outperform the ones without SF. Fur-
thermore, performance improvements between the
ones with SF and the ones without SF are greater
than Table 2.
The performance of CRF with SF and one of
CRF without SF are compared in Table 5. The col-
umn “Familiar” shows the results of extracting NEs
which consist of familiar morphemes, as well as the
column “Unfamiliar” shows the results of extracting
NEs which consist of unfamiliar morphemes. The
column “Other” shows the results of extracting NEs
which contain both familiar morpheme and unfa-
miliar one. These results indicate that SF is espe-
cially effective to extract NEs consisting of unfamil-
iar morphemes.
</bodyText>
<sectionHeader confidence="0.984332" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999892833333333">
This paper proposes a novel method to extract NEs
including unfamiliar morphemes which do not occur
or occur few times in a training corpus using a large
unannotated corpus. The experimental results show
that SF is effective for robust extracting NEs which
consist of unfamiliar morphemes. There are other
effective features of extracting NEs like N-best mor-
pheme sequences described in (Asahara and Mat-
sumoto, 2003) and features of surrounding phrases
described in (Nakano and Hirai, 2004). We will in-
vestigate incorporating SF and these features in the
near future.
</bodyText>
<sectionHeader confidence="0.989438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989145">
Masayuki Asahara and Yuji Matsumoto. 2003. Japanese
named entity extraction with redundant morphological
analysis. In Proc. ofHLT–NAACL ’03, pages 8–15.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proc. of the 19th COLING, pages 1–7.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision tree
learning. In Proc. ofACL ’01, pages 314–321.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to japanese
morphological analysis. In Proc. of EMNLP2004,
pages 230–237.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings ofICML, pages 282–289.
Fumito Masui, Shinya Suzuki, and Junichi Fukumoto.
2002. Development of named entity extraction tool
NExT for text processing. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 176–179. (in Japanese).
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tions of Information Processing Society of Japan,
45(3):934–941, Mar. (in Japanese).
Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho.
Nichigai Associates. (in Japanese).
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning for
japanese named entity recognition. In Proc. ofthe 18th
COLING, pages 705–711.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation: analysis of results.
In Proc. of the 18th COLING, pages 1106–1110.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. of the 9th EACL, pages 173–179.
Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi
Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000.
Named entity extraction based on a maximum entropy
model and transformation rules. Journal of Natural
Language Processing, 7(2):63–90, Apr. (in Japanese).
Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.
2002. Japanese named entity extraction using support
vector machine. Transactions ofInformation Process-
ing Society ofJapan, 43(1):44–53, Jan. (in Japanese).
</reference>
<page confidence="0.996611">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516432">
<title confidence="0.551559">Robust Extraction of Named Entity Including Unfamiliar Word</title>
<affiliation confidence="0.992039">and Media Center / of Information and Computer Sciences, Toyohashi University of Technology</affiliation>
<abstract confidence="0.997434466666667">This paper proposes a novel method to extract named entities including unfamiliar words which do not occur or occur few times in a training corpus using a large unannotated corpus. The proposed method consists of two steps. The first step is to assign the most similar and familiar word to each unfamiliar word based on their context vectors calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>125--128</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 125–128,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese named entity extraction with redundant morphological analysis.</title>
<date>2003</date>
<booktitle>In Proc. ofHLT–NAACL ’03,</booktitle>
<pages>8--15</pages>
<marker>Asahara, Matsumoto, 2003</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2003. Japanese named entity extraction with redundant morphological analysis. In Proc. ofHLT–NAACL ’03, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th COLING,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1261" citStr="Isozaki and Kazawa, 2002" startWordPosition="184" endWordPosition="187"> calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. All of them require a corpus whose NEs are annotated properly as training data. However, it is difficult to obtain an enough corpus in the real world, because there are increasing the number of NEs like personal names and company names. For example, a large database of organization names(Nichigai Associates, 2007) already contains 171,708 entries and is still increasing. Therefore, a robust method to extract NEs including unfamiliar words which do not occur or occur few times in a training corpus is necessary. This paper proposes a novel method of extract</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In Proc. of the 19th COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
</authors>
<title>Japanese named entity recognition based on a simple rule generator and decision tree learning.</title>
<date>2001</date>
<booktitle>In Proc. ofACL ’01,</booktitle>
<pages>314--321</pages>
<contexts>
<context position="1186" citStr="Isozaki, 2001" startWordPosition="175" endWordPosition="176">liar word to each unfamiliar word based on their context vectors calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. All of them require a corpus whose NEs are annotated properly as training data. However, it is difficult to obtain an enough corpus in the real world, because there are increasing the number of NEs like personal names and company names. For example, a large database of organization names(Nichigai Associates, 2007) already contains 171,708 entries and is still increasing. Therefore, a robust method to extract NEs including unfamiliar words which do not occur or occur few times in a </context>
</contexts>
<marker>Isozaki, 2001</marker>
<rawString>Hideki Isozaki. 2001. Japanese named entity recognition based on a simple rule generator and decision tree learning. In Proc. ofACL ’01, pages 314–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Appliying conditional random fields to japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP2004,</booktitle>
<pages>230--237</pages>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Appliying conditional random fields to japanese morphological analysis. In Proc. of EMNLP2004, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings ofICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fumito Masui</author>
<author>Shinya Suzuki</author>
<author>Junichi Fukumoto</author>
</authors>
<title>Development of named entity extraction tool NExT for text processing.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>176--179</pages>
<note>(in Japanese).</note>
<marker>Masui, Suzuki, Fukumoto, 2002</marker>
<rawString>Fumito Masui, Shinya Suzuki, and Junichi Fukumoto. 2002. Development of named entity extraction tool NExT for text processing. In Proceedings of the 8th Annual Meeting of the Association for Natural Language Processing, pages 176–179. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keigo Nakano</author>
<author>Yuzo Hirai</author>
</authors>
<title>Japanese named entity extraction with bunsetsu features.</title>
<date>2004</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<volume>45</volume>
<issue>3</issue>
<note>(in Japanese).</note>
<marker>Nakano, Hirai, 2004</marker>
<rawString>Keigo Nakano and Yuzo Hirai. 2004. Japanese named entity extraction with bunsetsu features. Transactions of Information Processing Society of Japan, 45(3):934–941, Mar. (in Japanese).</rawString>
</citation>
<citation valid="false">
<editor>Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho. Nichigai Associates. (in Japanese).</editor>
<marker></marker>
<rawString>Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho. Nichigai Associates. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
<author>Takehito Utsuro</author>
</authors>
<title>Named entity chunking techniques in supervised learning for japanese named entity recognition.</title>
<date>2000</date>
<booktitle>In Proc. ofthe 18th COLING,</booktitle>
<pages>705--711</pages>
<contexts>
<context position="1170" citStr="Sassano and Utsuro, 2000" startWordPosition="171" endWordPosition="174"> the most similar and familiar word to each unfamiliar word based on their context vectors calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. All of them require a corpus whose NEs are annotated properly as training data. However, it is difficult to obtain an enough corpus in the real world, because there are increasing the number of NEs like personal names and company names. For example, a large database of organization names(Nichigai Associates, 2007) already contains 171,708 entries and is still increasing. Therefore, a robust method to extract NEs including unfamiliar words which do not occur or occur</context>
</contexts>
<marker>Sassano, Utsuro, 2000</marker>
<rawString>Manabu Sassano and Takehito Utsuro. 2000. Named entity chunking techniques in supervised learning for japanese named entity recognition. In Proc. ofthe 18th COLING, pages 705–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Yoshio Eriguchi</author>
</authors>
<title>Japanese named entity extraction evaluation: analysis of results.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th COLING,</booktitle>
<pages>1106--1110</pages>
<contexts>
<context position="2831" citStr="Sekine and Eriguchi, 2000" startWordPosition="443" endWordPosition="446">0.6) TIME 502 (2.7) Total 18677 of two steps. The first step is to assign the most similar and familiar morpheme to each unfamiliar morpheme based on their context vectors calculated from a large unannotated corpus. The second step is to employ traditional machine learning approaches using both features of original morphemes and features of similar morphemes. The experiments of extracting Japanese NEs from IREX corpus and NHK corpus show the effectiveness of the proposed method. 2 Extraction of Japanese Named Entity 2.1 Task of the IREX Workshop The task of NE extraction of the IREX workshop (Sekine and Eriguchi, 2000) is to recognize eight NE types in Table 1. The organizer of the IREX workshop provided a training corpus, which consists of 1,174 newspaper articles published from January 1st 1995 to 10th which include 18,677 NEs. In the Japanese language, no other corpus whose NEs are annotated is publicly available as far as we know.1 2.2 Chunking of Named Entities It is quite common that the task of extracting Japanese NEs from a sentence is formalized as a chunking problem against a sequence of mor1The organizer of the IREX workshop also provides the testing data to its participants, however, we cannot s</context>
</contexts>
<marker>Sekine, Eriguchi, 2000</marker>
<rawString>Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese named entity extraction evaluation: analysis of results. In Proc. of the 18th COLING, pages 1106–1110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proc. of the 9th EACL,</booktitle>
<pages>173--179</pages>
<marker>Sang, 1999</marker>
<rawString>E. Tjong Kim Sang. 1999. Representing text chunks. In Proc. of the 9th EACL, pages 173–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Ma Qing</author>
<author>Masaki Murata</author>
<author>Hiromi Ozaku</author>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Named entity extraction based on a maximum entropy model and transformation rules.</title>
<date>2000</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>7</volume>
<issue>2</issue>
<note>(in Japanese).</note>
<contexts>
<context position="1130" citStr="Uchimoto et al., 2000" startWordPosition="165" endWordPosition="169">two steps. The first step is to assign the most similar and familiar word to each unfamiliar word based on their context vectors calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. All of them require a corpus whose NEs are annotated properly as training data. However, it is difficult to obtain an enough corpus in the real world, because there are increasing the number of NEs like personal names and company names. For example, a large database of organization names(Nichigai Associates, 2007) already contains 171,708 entries and is still increasing. Therefore, a robust method to extract NEs including unfa</context>
</contexts>
<marker>Uchimoto, Qing, Murata, Ozaku, Utiyama, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000. Named entity extraction based on a maximum entropy model and transformation rules. Journal of Natural Language Processing, 7(2):63–90, Apr. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese named entity extraction using support vector machine.</title>
<date>2002</date>
<journal>Transactions ofInformation Processing Society ofJapan,</journal>
<volume>43</volume>
<issue>1</issue>
<note>(in Japanese).</note>
<contexts>
<context position="1234" citStr="Yamada et al., 2002" startWordPosition="180" endWordPosition="183">their context vectors calculated from a large unannotated corpus. After that, traditional machine learning approaches are employed as the second step. The experiments of extracting Japanese named entities from IREX corpus and NHK corpus show the effectiveness of the proposed method. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. All of them require a corpus whose NEs are annotated properly as training data. However, it is difficult to obtain an enough corpus in the real world, because there are increasing the number of NEs like personal names and company names. For example, a large database of organization names(Nichigai Associates, 2007) already contains 171,708 entries and is still increasing. Therefore, a robust method to extract NEs including unfamiliar words which do not occur or occur few times in a training corpus is necessary. This paper propose</context>
</contexts>
<marker>Yamada, Kudo, Matsumoto, 2002</marker>
<rawString>Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto. 2002. Japanese named entity extraction using support vector machine. Transactions ofInformation Processing Society ofJapan, 43(1):44–53, Jan. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>