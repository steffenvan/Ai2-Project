<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004508">
<title confidence="0.992178">
Classification of semantic relations by humans and machines ∗
</title>
<author confidence="0.977224">
Erwin Marsi and Emiel Krahmer
</author>
<affiliation confidence="0.96692">
Communication and Cognition
Tilburg University, The Netherlands
</affiliation>
<email confidence="0.988947">
{e.c.marsi, e.j.krahmer}@uvt.nl
</email>
<sectionHeader confidence="0.997294" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987795">
This paper addresses the classification of
semantic relations between pairs of sen-
tences extracted from a Dutch parallel cor-
pus at the word, phrase and sentence level.
We first investigate the performance of hu-
man annotators on the task of manually
aligning dependency analyses of the re-
spective sentences and of assigning one
of five semantic relations to the aligned
phrases (equals, generalizes, specifies, re-
states and intersects). Results indicate that
humans can perform this task well, with
an F-score of .98 on alignment and an F-
score of .95 on semantic relations (after
correction). We then describe and evalu-
ate a combined alignment and classifica-
tion algorithm, which achieves an F-score
on alignment of .85 (using EuroWordNet)
and an F-score of .80 on semantic relation
classification.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925727272727">
An automatic method that can determine how two
sentences relate to each other in terms of seman-
tic overlap or textual entailment (e.g., (Dagan and
Glickman, 2004)) would be a very useful thing to
have for robust natural language applications. A
summarizer, for instance, could use it to extract
the most informative sentences, while a question-
answering system – to give a second example –
could use it to select potential answer string (Pun-
yakanok et al., 2004), perhaps preferring more spe-
cific answers over more general ones. In general, it
</bodyText>
<footnote confidence="0.580862333333333">
∗This work was carried out within the IMIX-IMOGEN (In-
teractive Multimodal Output Generation) project, sponsored by
the Netherlands Organization of Scientific Research (NWO).
</footnote>
<bodyText confidence="0.999729078947368">
is very useful to know whether some sentence S is
more specific (entails) or more general than (is en-
tailed by) an alternative sentence S&apos;, or whether the
two sentences express essentially the same informa-
tion albeit in a different way (paraphrasing).
Research on automatic methods for recognizing
semantic relations between sentences is still rela-
tively new, and many basic issues need to be re-
solved. In this paper we address two such related is-
sues: (1) to what extent can human annotators label
semantic overlap relations between words, phrases
and sentences, and (2) what is the added value of
linguistically informed analyses.
It is generally assumed that pure string overlap
is not sufficient for recognizing semantic relations;
and that using some form of syntactic analysis may
be beneficial (e.g., (Herrera et al., 2005), (Vander-
wende et al., 2005)). Our working hypothesis is that
semantic overlap at the word and phrase levels may
provide a good basis for deciding the semantic re-
lation between sentences. Recognising semantic re-
lations between sentences then becomes a two-step
procedure: first, the words and phrases in the re-
spective sentences need to be aligned, after which
the relations between the pairs of aligned words and
phrases should be labeled in terms of semantic rela-
tions.
Various alignment algorithms have been devel-
oped for data-driven approaches to machine trans-
lation (e.g. (Och and Ney, 2000)). Initially work
focused on word-based alignment, but more and
more work is also addressing alignment at the higher
levels (substrings, syntactic phrases or trees), e.g.,
(Meyers et al., 1996), (Gildea, 2003). For our pur-
poses, an additional advantage of aligning syntac-
tic structures is that it keeps the alignment feasible
(as the number of arbitrary substrings that may be
aligned grows exponentially to the number of words
</bodyText>
<page confidence="0.772262">
1
</page>
<note confidence="0.8000825">
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999951">
in the sentence). Here, following (Herrera et al.,
2005) and (Barzilay, 2003), we will align sentences
at the level of dependency structures. In addition,
we will label the alignments in terms of five basic
semantic relations to be defined below. We will per-
form this task both manually and automatically, so
that we can address both of the issues raised above.
Section 2 describes a monolingual parallel cor-
pus consisting of two Dutch translations, and for-
malizes the alignment-classification task to be per-
formed. In section 3 we report the results on align-
ment, first describing interannotator agreement on
this task and then the results on automatic alignment.
In section 4, then, we address the semantic relation
classification; again, first describing interannotator
results, followed by results obtained using memory-
based machine learning techniques. We end with a
general discussion.
</bodyText>
<sectionHeader confidence="0.939415" genericHeader="introduction">
2 Corpus and Task definition
</sectionHeader>
<subsectionHeader confidence="0.963268">
2.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999994393939394">
We have developed a parallel monolingual corpus
consisting of two different Dutch translations of the
French book “Le petit prince” (the little prince) by
Antoine de Saint-Exup´ery (published 1943), one by
Laetitia de Beaufort-van Hamel (1966) and one by
Ernst Altena (2000). For our purposes, this proved
to be a good way to quickly find a large enough
set of related sentence pairs, which differ semanti-
cally in interesting and subtle ways. In this work,
we used the first five chapters, with 290 sentences
and 3600 words in the first translation, and 277 sen-
tences and 3358 words in the second translation.
The texts were automatically tokenized and split into
sentences, after which errors were manually cor-
rected. Corresponding sentences from both trans-
lations were manually aligned; in most cases this
was a one-to-one mapping, but occasionally a sin-
gle sentence in one translation mapped onto two or
more sentences in the other: this occurred 23 times
in all five chapters. Next, the Alpino parser for
Dutch (e.g., (Bouma et al., 2001)) was used for part-
of-speech tagging and lemmatizing all words, and
for assigning a dependency analysis to all sentences.
The POS labels indicate the major word class (e.g.
verb, noun, adj, and adv). The dependency rela-
tions hold between tokens and are identical to those
used in the Spoken Dutch Corpus. These include de-
pendencies such as head/subject, head/modifier and
coordination/conjunction. If a full parse could not
be obtained, Alpino produced partial analyses col-
lected under a single root node. Errors in lemmati-
zation, POS tagging, and syntactic dependency pars-
ing were not subject to manual correction.
</bodyText>
<subsectionHeader confidence="0.998702">
2.2 Task definition
</subsectionHeader>
<bodyText confidence="0.9999520625">
The task to be performed can be described infor-
mally as follows: given two dependency analyses,
align those nodes that are semantically related. More
precisely: For each node v in the dependency struc-
ture for a sentence 5, we define STR(v) as the sub-
string of all tokens under v (i.e., the composition of
the tokens of all nodes reachable from v). An align-
ment between sentences 5 and 50 pairs nodes from
the dependency graphs for both sentences. Aligning
node v from the dependency graph D of sentence
5 with node v0 from the graph D0 of 50 indicates
that there is a semantic relation between STR(v) and
STR(v0), that is, between the respective substrings
associated with v and v0. We distinguish five po-
tential, mutually exclusive, relations between nodes
(with illustrative examples):
</bodyText>
<listItem confidence="0.99232815">
1. v equals v0 iff STR(v) and STR(v0) are literally
identical (abstracting from case). Example: “a
small and a large boa-constrictor” equals “a
large and a small boa-constrictor”;
2. v restates v0 iff STR(v) is a paraphrase of
STR(v0) (same information content but differ-
ent wording). Example: “a drawing of a boa-
constrictor snake” restates “a drawing of a boa-
constrictor”;
3. v specifies v0 iff STR(v) is more specific than
STR(v0). Example: “the planet B 612” specifies
“the planet”;
4. v generalizes v0 iff STR(v0) is more specific
than STR(v). Example: “the planet” general-
izes “the planet B 612”;
5. v intersects v0 iff STR(v) and STR(v0) share
some informational content, but also each ex-
press some piece of information not expressed
in the other. Example: “Jupiter and Mars” in-
tersects “Mars and Venus”
</listItem>
<bodyText confidence="0.981955">
Figure 1 shows an example alignment with seman-
tic relations between the dependency structures of
</bodyText>
<page confidence="0.984899">
2
</page>
<figure confidence="0.803993">
hebben
</figure>
<figureCaption confidence="0.981472833333333">
Figure 1: Dependency structures and alignment for the sentences Zo heb ik in de loop van mijn leven heel
veel contacten gehad met heel veel serieuze personen. (lit. ‘Thus have I in the course of my life very
many contacts had with very many serious persons’) and Op die manier kwam ik in het leven met massa’s
gewichtige mensen in aanraking.. (lit. ‘In that way came I in the life with mass-of weighty/important people
in touch’). The alignment relations are equals (dotted gray), restates (solid gray), specifies (dotted black),
and intersects (dashed gray). For the sake of transparency, dependency relations have been omitted.
</figureCaption>
<figure confidence="0.9848723125">
hebben
zo contact met in de loop van
ik
leven
mijn
persoon
komen
ik op in in aanraking met
heel
manier leven
massa
die het mens
gewichtig
serieus veel
veel
heel
</figure>
<bodyText confidence="0.997104714285714">
two sentences. Note that there is an intuitive rela-
tion with entailment here: both equals and restates
can be understood as mutual entailment (i.e., if the
root nodes of the analyses corresponding 5 and 5&apos;
stand in an equal or restate relation, 5 entails 5&apos; and
5&apos; entails 5), if 5 specifies 5&apos; then 5 also entails 5&apos;
and if 5 generalizes 5&apos; then 5 is entailed by 5&apos;.
In remainder of this paper, we will distinguish two
aspects of this task: alignment is the subtask of pair-
ing related nodes – or more precise, pairing the to-
ken strings corresponding to these nodes; classifica-
tion of semantic relations is the subtask of labeling
these alignments in terms of the five types of seman-
tic relations.
</bodyText>
<subsectionHeader confidence="0.9997">
2.3 Annotation procedure
</subsectionHeader>
<bodyText confidence="0.999993642857143">
For creating manual alignments, we developed a
special-purpose annotation tool which shows, side
by side, two sentences, as well as their respective
dependency graphs. When the user clicks on a node
v in the graph, the corresponding string (STR(v)) is
shown at the bottom. The tool enables the user to
manually construct an alignment graph on the basis
of the respective dependency graphs. This is done by
focusing on a node in the structure for one sentence,
and then selecting a corresponding node (if possible)
in the other structure, after which the user can select
the relevant alignment relation. The tool offers addi-
tional support for folding parts of the graphs, high-
lighting unaligned nodes and hiding dependency re-
lation labels.
All text material was aligned by the two authors.
They started with annotating the first ten sentences
of chapter one together in order to get a feel for
the task. They continued with the remaining sen-
tences from chapter one individually (35 sentences
and 521 in the first translation, and 35 sentences and
481 words in the second translation). Next, both
annotators discussed annotation differences, which
triggered some revisions in their respective annota-
tion. They also agreed on a single consensus annota-
tion. Interannotator agreement will be discussed in
the next two sections. Finally, each author annotated
two additional chapters, bringing the total to five.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
3 Alignment
</sectionHeader>
<subsectionHeader confidence="0.99397">
3.1 Interannotator agreement
</subsectionHeader>
<bodyText confidence="0.997771">
Interannotator agreement was calculated in terms of
precision, recall and F-score (with Q = 1) on aligned
</bodyText>
<page confidence="0.989965">
3
</page>
<table confidence="0.999438857142857">
(A1,A2) (A1&apos;,A2&apos;) (A.,A11) (A,,A2&apos;)
#real: 322 323 322 322
#pred: 312 321 323 321
#correct: 293 315 317 318
precision: .94 .98 .98 .99
recall: .91 .98 .98 .99
F-score: .92 .98 .98 .99
</table>
<tableCaption confidence="0.998771">
Table 1: Interannotator agreement with respect
</tableCaption>
<bodyText confidence="0.87424188">
to alignment between annotators 1 and 2 before
(A1, A2) and after (A10, A20) revision, and between
the consensus and annotator 1 (Ac, A10) and annota-
tor 2 (Ac, A20) respectively.
node pairs as follows:
precision =  |Areal n Apred  |/  |Apred |
recall =  |Areal n Apred  |/  |Areal |
F-score = (2 x prec x rec) / (prec + rec)
where Areal is the set of all real alignments (the ref-
erence or golden standard), Apred is the set of all
predicted alignments, and Apred nAreal is the set all
correctly predicted alignments. For the purpose of
calculating interannotator agreement, one of the an-
notations (A1) was considered the ‘real’ alignment,
the other (A2) the ‘predicted’. The results are sum-
marized in Table 1 in column (A1, A2).1
As explained in section 2.3, both annotators re-
vised their initial annotations. This improved their
agreement, as shown in column (A10, A20). In ad-
dition, they agreed on a single consensus annotation
(Ac). The last two columns of Table 1 show the re-
sults of evaluating each of the revised annotations
against this consensus annotation. The F-score of
.98 can therefore be regarded as the upper bound on
the alignment task.
</bodyText>
<subsectionHeader confidence="0.999428">
3.2 Automatic alignment
</subsectionHeader>
<bodyText confidence="0.985719076923077">
Our tree alignment algorithm is based on the dy-
namic programming algorithm in (Meyers et al.,
1996), and similar to that used in (Barzilay, 2003).
It calculates the match between each node in de-
pendency tree D against each node in dependency
tree D&apos;. The score for each pair of nodes only de-
pends on the similarity of the words associated with
the nodes and, recursively, on the scores of the best
1Note that since there are no classes, we can not calculate
change agreement rethe Kappa statistic.
matching pairs of their descendants. The node simi-
larity function relies either on identity of the lemmas
or on synonym, hyperonym, and hyponym relations
between them, as retrieved from EuroWordNet.
Automatic alignment was evaluated with the con-
sensus alignment of the first chapter as the gold
standard. A baseline was constructed by aligning
those nodes which stand in an equals relation to each
other, i.e., a node v in D is aligned to a node v&apos;
in D&apos; iff STR(v) =STR(v&apos;). This baseline already
achieves a relatively high score (an F-score of .56),
which may be attributed to the nature of our mate-
rial: the translated sentence pairs are relatively close
to each other and may show a sizeable amount of lit-
eral string overlap. In order to test the contribution
of synonym and hyperonym information for node
matching, performance is measured with and with-
out the use of EuroWordNet. The results for auto-
matic alignment are shown in Table 2. In compari-
son with the baseline, the alignment algorithm with-
out use of EuroWordnet loses a few points on preci-
sion, but improves a lot on recall (a 200% increase),
which in turn leads to a substantial improvement on
the overall F-score. The use of EurWordNet leads to
a small increase (two points) on both precision and
recall, and thus to small increase in F-score. How-
ever, in comparison with the gold standard human
score for this task (.95), there is clearly room for
further improvement.
</bodyText>
<sectionHeader confidence="0.911229" genericHeader="method">
4 Classification of semantic relations
</sectionHeader>
<subsectionHeader confidence="0.916479">
4.1 Interannotator agreement
</subsectionHeader>
<bodyText confidence="0.999971">
In addition to alignment, the annotation procedure
for the first chapter of The little prince by two anno-
tators (cf. section 2.3) also involved labeling of the
semantic relation between aligned nodes. Interanno-
tator agreement on this task is shown Table 3, before
and after revision. The measures are weighted preci-
sion, recall and F-score. For instance, the precision
is the weighted sum of the separate precision scores
for each of the five relations. The table also shows
the κ-score. The F-score of .97 can be regarded as
the upper bound on the relation labeling task. We
think these numbers indicate that the classification
of semantic relations is a well defined task which
can be accomplished with a high level of interanno-
tator agreement.
</bodyText>
<page confidence="0.99527">
4
</page>
<table confidence="0.698201">
Alignment: Prec : Rec : F-score:
</table>
<tableCaption confidence="0.8804535">
Table 2: Precision, recall and F-score on automatic
alignment
</tableCaption>
<table confidence="0.999844">
(A1, A2) (A1&apos;, A2&apos;) (A,, A1&apos;) (A,, A2&apos;)
precision: .86 .96 .98 .97
recall: .86 .95 .97 .97
F-score: .85 .95 .97 .97
κ: .77 .92 .96 .96
</table>
<tableCaption confidence="0.999065">
Table 3: Interannotator agreement with respect to se-
</tableCaption>
<bodyText confidence="0.801792">
mantic relation labeling between annotators 1 and 2
before (A1, A2) and after (A1&apos;, A2&apos;) revision , and
between the consensus and annotator 1 (A,, A1&apos;)
and annotator 2 (A,, A2&apos;) respectively.
</bodyText>
<subsectionHeader confidence="0.9943">
4.2 Automatic classification
</subsectionHeader>
<bodyText confidence="0.9997605">
For the purpose of automatic semantic relation la-
beling, we approach the task as a classification prob-
lem to be solved by machine learning. Alignments
between node pairs are classified on the basis of the
lexical-semantic relation between the nodes, their
corresponding strings, and – recursively – on previ-
ous decisions about the semantic relations of daugh-
ter nodes. The input features used are:
</bodyText>
<listItem confidence="0.997914625">
• a boolean feature representing string identity
between the strings corresponding to the nodes
• a boolean feature for each of the five semantic
relations indicating whether the relation holds
for at least one of the daughter nodes;
• a boolean feature indicating whether at least
one of the daughter nodes is not aligned;
• a categorical feature representing the lexical se-
</listItem>
<bodyText confidence="0.967927666666667">
mantic relation between the nodes (i.e. the
lemmas and their part-of-speech) as found in
EuroWordNet, which can be synonym, hyper-
onym, or hyponym.2
To allow for the use of previous decisions, the
nodes of the dependency analyses are traversed in
a bottom-up fashion. Whenever a node is aligned,
the classifier assigns a semantic label to the align-
ment. Taking previous decisions into account may
</bodyText>
<footnote confidence="0.997347">
2These three form the bulk of all relations in Dutch Eu-
roWordnet. Since no word sense disambiguation was involved,
we simply used all word senses.
</footnote>
<table confidence="0.998668142857143">
Prec : Rec : F-score:
equals .93 ± .06 .95 ± .04 .94 ± .02
restates .56 ± .08 .78 ± .04 .65 ± .05
specifies n.a. 0 n.a.
generalizes .19 ± .06 .37 ± .09 .24 ± .05
intersects n.a. 0 n.a.
Combined: .62 ± .01 .70 ± .02 .64 ± .02
</table>
<tableCaption confidence="0.948837666666667">
Table 4: Average precision, recall and F-score (and
SD) over all 5 folds on automatic classification of
semantic relations
</tableCaption>
<bodyText confidence="0.999645459459459">
cause a proliferation of errors: wrong classification
of daughter nodes may in turn cause wrong classifi-
cation of the mother node. To investigate this risk,
classification experiments were run both with and
without (i.e. using the annotation) previous deci-
sions.
Since our amount of data is limited, we used
a memory-based classifier, which – in contrast to
most other machine learning algorithms – performs
no abstraction, allowing it to deal with productive
but low-frequency exceptions typically occurring in
NLP tasks(Daelemans et al., 1999). All memory-
based learning was performed with TiMBL, version
5.1 (Daelemans et al., 2004), with its default set-
tings (overlap distance function, gain-ratio feature
weighting, k = 1).
The five first chapters of The little prince were
used to run a 5-fold cross-validated classification ex-
periment. The first chapter is the consensus align-
ment and relation labeling, while the other four were
done by one out of two annotators. The alignments
to be classified are those from to the human align-
ment. The baseline of always guessing equals – the
majority class – gives a precision of 0.26, a recall of
0.51, and an F-score of 0.36. Table 4 presents the re-
sults broken down to relation type. The combined F-
score of 0.64 is almost twice the baseline score. As
expected, the highest score goes to equals, followed
by a reasonable score on restates. Performance on
the other relation types is rather poor, with even no
predictions of specifies and intersects at all.
Faking perfect previous decisions by using the
annotation gives a considerable improvement, as
shown in Table 5, especially on specifies, general-
izes and intersects. This reveals that the prolifera-
tion of classification errors is indeed a problem that
should be addressed.
</bodyText>
<figure confidence="0.9752415">
baseline
algorithm without wordnet
algorithm with wordnet
.87 .41 .56
.84 .82 .83
.86 .84 .85
</figure>
<page confidence="0.870725">
5
</page>
<table confidence="0.996235857142857">
Prec : Rec : F-score:
equals .99 ± .02 .97 ± .02 .98 ± .01
restates .65 ± .04 .82 ± .04 .73 ± .03
specifies .60 ± .12 .48 ± .10 .53 ± .09
generalizes .50 ± .11 .52 ± .10 .50 ± .09
intersects .69 ± .27 .35 ± .12 .46 ± .16
Combined: .82 ± .02 .81 ± .02 .80 ± .02
</table>
<tableCaption confidence="0.852290666666667">
Table 5: Average precision, recall and F-score (and
SD) over all 5 folds on automatic classification of
semantic relations without using previous decisions.
</tableCaption>
<bodyText confidence="0.9997922">
In sum, these results show that automatic classifi-
cation of semantic relations is feasible and promis-
ing – especially when the proliferation of classifica-
tion errors can be prevented – but still not nearly as
good as human performance.
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="conclusions">
5 Discussion and Future work
</sectionHeader>
<bodyText confidence="0.999981384615385">
This paper presented an approach to detecting se-
mantic relations at the word, phrase and sentence
level on the basis of dependency analyses. We inves-
tigated the performance of human annotators on the
tasks of manually aligning dependency analyses and
of labeling the semantic relations between aligned
nodes. Results indicate that humans can perform this
task well, with an F-score of .98 on alignment and an
F-score of .92 on semantic relations (after revision).
We also described and evaluated automatic methods
addressing these tasks: a dynamic programming tree
alignment algorithm which achieved an F-score on
alignment of .85 (using lexical semantic information
from EuroWordNet), and a memory-based seman-
tic relation classifier which achieved F-scores of .64
and .80 with and without using real previous deci-
sions respectively.
One of the issues that remains to be addressed
in future work is the effect of parsing errors. Such
errors were not corrected, but during manual align-
ment, we sometimes found that substrings could not
be properly aligned because the parser had failed to
identify them as syntactic constituents. As far as
classification of semantic relations is concerned, the
proliferation of classification errors is an issue that
needs to be solved. Classification performance may
be further improved with additional features (e.g.
phrase length information), optimization, and more
data. Also, we have not yet tried to combine au-
tomatic alignment and classification. Yet another
point concerns the type of text material. The sen-
tence pairs from our current corpus are relatively
close, in the sense that both translations more or less
convey the same information. Although this seems a
good starting point to study alignment, we intend to
continue with other types of text material in future
work. For instance, in extending our work to the ac-
tual output of a QA system, we expect to encounter
sentences with far less overlap.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998963136363636">
R. Barzilay. 2003. Information Fusion for Multidocu-
ment Summarization. Ph.D. Thesis, Columbia Univer-
sity.
G. Bouma, G. van Noord, and R. Malouf. 2001. Alpino:
Wide-coverage computational analysis of Dutch. In
Computational Linguistics in The Netherlands 2000,
pages 45–59.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11–41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory
based learner, version 5.1, reference guide. ILK Tech-
nical Report 04-02, Tilburg University.
I. Dagan and O. Glickman. 2004. Probabilistic textual
entailment: Generic applied modelling of language
variability. In Learning Methods for Text Understand-
ing and Mining, Grenoble.
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the 41st Annual
Meeting of the ACL, Sapporo, Japan.
J. Herrera, A. Pe nas, and F. Verdejo. 2005. Textual
entailment recognition based on dependency analy-
sis and wordnet. In Proceedings of the 1st. PASCAL
Recognision Textual Entailment Challenge Workshop.
Pattern Analysis, Statistical Modelling and Computa-
tional Learning, PASCAL.
A. Meyers, R. Yangarber, and R. Grisham. 1996. Align-
ment of shared forests for bilingual corpora. In Pro-
ceedings of 16th International Conference on Com-
putational Linguistics (COLING-96), pages 460–465,
Copenhagen, Denmark.
F.J. Och and H. Ney. 2000. Statistical machine trans-
lation. In EAMT Workshop, pages 39–46, Ljubljana,
Slovenia.
V. Punyakanok, D. Roth, and W. Yih. 2004. Natural lan-
guage inference via dependency tree mapping: An ap-
plication to question answering. Computational Lin-
guistics, 6(9).
L. Vanderwende, D. Coughlin, and W. Dolan. 2005.
What syntax can contribute in entailment task. In Pro-
ceedings of the 1st. PASCAL Recognision Textual En-
tailment Challenge Workshop, Southampton, U.K.
</reference>
<page confidence="0.998779">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.769761">
<title confidence="0.997046">of semantic relations by humans and machines</title>
<author confidence="0.911281">Marsi</author>
<affiliation confidence="0.934273">Communication and Tilburg University, The</affiliation>
<abstract confidence="0.993393238095238">This paper addresses the classification of semantic relations between pairs of sentences extracted from a Dutch parallel corpus at the word, phrase and sentence level. We first investigate the performance of human annotators on the task of manually aligning dependency analyses of the respective sentences and of assigning one of five semantic relations to the aligned phrases (equals, generalizes, specifies, restates and intersects). Results indicate that humans can perform this task well, with F-score of alignment and an Fof semantic relations (after correction). We then describe and evaluate a combined alignment and classification algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
</authors>
<title>Information Fusion for Multidocument Summarization.</title>
<date>2003</date>
<tech>Ph.D. Thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="3836" citStr="Barzilay, 2003" startWordPosition="599" endWordPosition="600"> work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in the sentence). Here, following (Herrera et al., 2005) and (Barzilay, 2003), we will align sentences at the level of dependency structures. In addition, we will label the alignments in terms of five basic semantic relations to be defined below. We will perform this task both manually and automatically, so that we can address both of the issues raised above. Section 2 describes a monolingual parallel corpus consisting of two Dutch translations, and formalizes the alignment-classification task to be performed. In section 3 we report the results on alignment, first describing interannotator agreement on this task and then the results on automatic alignment. In section 4</context>
<context position="12690" citStr="Barzilay, 2003" startWordPosition="2072" endWordPosition="2073">1 in column (A1, A2).1 As explained in section 2.3, both annotators revised their initial annotations. This improved their agreement, as shown in column (A10, A20). In addition, they agreed on a single consensus annotation (Ac). The last two columns of Table 1 show the results of evaluating each of the revised annotations against this consensus annotation. The F-score of .98 can therefore be regarded as the upper bound on the alignment task. 3.2 Automatic alignment Our tree alignment algorithm is based on the dynamic programming algorithm in (Meyers et al., 1996), and similar to that used in (Barzilay, 2003). It calculates the match between each node in dependency tree D against each node in dependency tree D&apos;. The score for each pair of nodes only depends on the similarity of the words associated with the nodes and, recursively, on the scores of the best 1Note that since there are no classes, we can not calculate change agreement rethe Kappa statistic. matching pairs of their descendants. The node similarity function relies either on identity of the lemmas or on synonym, hyperonym, and hyponym relations between them, as retrieved from EuroWordNet. Automatic alignment was evaluated with the conse</context>
</contexts>
<marker>Barzilay, 2003</marker>
<rawString>R. Barzilay. 2003. Information Fusion for Multidocument Summarization. Ph.D. Thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G van Noord</author>
<author>R Malouf</author>
</authors>
<title>Alpino: Wide-coverage computational analysis of Dutch.</title>
<date>2001</date>
<booktitle>In Computational Linguistics in The</booktitle>
<pages>45--59</pages>
<marker>Bouma, van Noord, Malouf, 2001</marker>
<rawString>G. Bouma, G. van Noord, and R. Malouf. 2001. Alpino: Wide-coverage computational analysis of Dutch. In Computational Linguistics in The Netherlands 2000, pages 45–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning, Special issue on Natural Language Learning,</booktitle>
<pages>34--11</pages>
<marker>Daelemans, Van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, Special issue on Natural Language Learning, 34:11–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 5.1, reference guide. ILK</title>
<date>2004</date>
<tech>Technical Report 04-02,</tech>
<institution>Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, van den Bosch, 2004</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg memory based learner, version 5.1, reference guide. ILK Technical Report 04-02, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modelling of language variability.</title>
<date>2004</date>
<booktitle>In Learning Methods for Text Understanding and Mining,</booktitle>
<location>Grenoble.</location>
<contexts>
<context position="1172" citStr="Dagan and Glickman, 2004" startWordPosition="176" endWordPosition="179">ons to the aligned phrases (equals, generalizes, specifies, restates and intersects). Results indicate that humans can perform this task well, with an F-score of .98 on alignment and an Fscore of .95 on semantic relations (after correction). We then describe and evaluate a combined alignment and classification algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classification. 1 Introduction An automatic method that can determine how two sentences relate to each other in terms of semantic overlap or textual entailment (e.g., (Dagan and Glickman, 2004)) would be a very useful thing to have for robust natural language applications. A summarizer, for instance, could use it to extract the most informative sentences, while a questionanswering system – to give a second example – could use it to select potential answer string (Punyakanok et al., 2004), perhaps preferring more specific answers over more general ones. In general, it ∗This work was carried out within the IMIX-IMOGEN (Interactive Multimodal Output Generation) project, sponsored by the Netherlands Organization of Scientific Research (NWO). is very useful to know whether some sentence </context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>I. Dagan and O. Glickman. 2004. Probabilistic textual entailment: Generic applied modelling of language variability. In Learning Methods for Text Understanding and Mining, Grenoble.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3362" citStr="Gildea, 2003" startWordPosition="526" endWordPosition="527">gnising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in the sentence). Here, following (Herrera et al., 2005) and (Barzilay, 2003), we will align sentences at the level of dependency structures. In addition, we will label the alignments in terms of five ba</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>D. Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41st Annual Meeting of the ACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Herrera</author>
<author>A Pe nas</author>
<author>F Verdejo</author>
</authors>
<title>Textual entailment recognition based on dependency analysis and wordnet.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st. PASCAL Recognision Textual Entailment Challenge Workshop. Pattern Analysis, Statistical Modelling and Computational Learning,</booktitle>
<location>PASCAL.</location>
<contexts>
<context position="2557" citStr="Herrera et al., 2005" startWordPosition="397" endWordPosition="400"> albeit in a different way (paraphrasing). Research on automatic methods for recognizing semantic relations between sentences is still relatively new, and many basic issues need to be resolved. In this paper we address two such related issues: (1) to what extent can human annotators label semantic overlap relations between words, phrases and sentences, and (2) what is the added value of linguistically informed analyses. It is generally assumed that pure string overlap is not sufficient for recognizing semantic relations; and that using some form of syntactic analysis may be beneficial (e.g., (Herrera et al., 2005), (Vanderwende et al., 2005)). Our working hypothesis is that semantic overlap at the word and phrase levels may provide a good basis for deciding the semantic relation between sentences. Recognising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). I</context>
<context position="3815" citStr="Herrera et al., 2005" startWordPosition="594" endWordPosition="597">lignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in the sentence). Here, following (Herrera et al., 2005) and (Barzilay, 2003), we will align sentences at the level of dependency structures. In addition, we will label the alignments in terms of five basic semantic relations to be defined below. We will perform this task both manually and automatically, so that we can address both of the issues raised above. Section 2 describes a monolingual parallel corpus consisting of two Dutch translations, and formalizes the alignment-classification task to be performed. In section 3 we report the results on alignment, first describing interannotator agreement on this task and then the results on automatic al</context>
</contexts>
<marker>Herrera, nas, Verdejo, 2005</marker>
<rawString>J. Herrera, A. Pe nas, and F. Verdejo. 2005. Textual entailment recognition based on dependency analysis and wordnet. In Proceedings of the 1st. PASCAL Recognision Textual Entailment Challenge Workshop. Pattern Analysis, Statistical Modelling and Computational Learning, PASCAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Yangarber</author>
<author>R Grisham</author>
</authors>
<title>Alignment of shared forests for bilingual corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>460--465</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="3346" citStr="Meyers et al., 1996" startWordPosition="522" endWordPosition="525">between sentences. Recognising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics in the sentence). Here, following (Herrera et al., 2005) and (Barzilay, 2003), we will align sentences at the level of dependency structures. In addition, we will label the alignments in </context>
<context position="12644" citStr="Meyers et al., 1996" startWordPosition="2062" endWordPosition="2065">e ‘predicted’. The results are summarized in Table 1 in column (A1, A2).1 As explained in section 2.3, both annotators revised their initial annotations. This improved their agreement, as shown in column (A10, A20). In addition, they agreed on a single consensus annotation (Ac). The last two columns of Table 1 show the results of evaluating each of the revised annotations against this consensus annotation. The F-score of .98 can therefore be regarded as the upper bound on the alignment task. 3.2 Automatic alignment Our tree alignment algorithm is based on the dynamic programming algorithm in (Meyers et al., 1996), and similar to that used in (Barzilay, 2003). It calculates the match between each node in dependency tree D against each node in dependency tree D&apos;. The score for each pair of nodes only depends on the similarity of the words associated with the nodes and, recursively, on the scores of the best 1Note that since there are no classes, we can not calculate change agreement rethe Kappa statistic. matching pairs of their descendants. The node similarity function relies either on identity of the lemmas or on synonym, hyperonym, and hyponym relations between them, as retrieved from EuroWordNet. Au</context>
</contexts>
<marker>Meyers, Yangarber, Grisham, 1996</marker>
<rawString>A. Meyers, R. Yangarber, and R. Grisham. 1996. Alignment of shared forests for bilingual corpora. In Proceedings of 16th International Conference on Computational Linguistics (COLING-96), pages 460–465, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Statistical machine translation.</title>
<date>2000</date>
<booktitle>In EAMT Workshop,</booktitle>
<pages>39--46</pages>
<location>Ljubljana, Slovenia.</location>
<contexts>
<context position="3153" citStr="Och and Ney, 2000" startWordPosition="493" endWordPosition="496">(Herrera et al., 2005), (Vanderwende et al., 2005)). Our working hypothesis is that semantic overlap at the word and phrase levels may provide a good basis for deciding the semantic relation between sentences. Recognising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, Ann Arbor, June 2005. c�2005 Association for Computational Lingui</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F.J. Och and H. Ney. 2000. Statistical machine translation. In EAMT Workshop, pages 39–46, Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Natural language inference via dependency tree mapping: An application to question answering.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>6</volume>
<issue>9</issue>
<contexts>
<context position="1471" citStr="Punyakanok et al., 2004" startWordPosition="226" endWordPosition="230">ssification algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classification. 1 Introduction An automatic method that can determine how two sentences relate to each other in terms of semantic overlap or textual entailment (e.g., (Dagan and Glickman, 2004)) would be a very useful thing to have for robust natural language applications. A summarizer, for instance, could use it to extract the most informative sentences, while a questionanswering system – to give a second example – could use it to select potential answer string (Punyakanok et al., 2004), perhaps preferring more specific answers over more general ones. In general, it ∗This work was carried out within the IMIX-IMOGEN (Interactive Multimodal Output Generation) project, sponsored by the Netherlands Organization of Scientific Research (NWO). is very useful to know whether some sentence S is more specific (entails) or more general than (is entailed by) an alternative sentence S&apos;, or whether the two sentences express essentially the same information albeit in a different way (paraphrasing). Research on automatic methods for recognizing semantic relations between sentences is still </context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2004. Natural language inference via dependency tree mapping: An application to question answering. Computational Linguistics, 6(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>D Coughlin</author>
<author>W Dolan</author>
</authors>
<title>What syntax can contribute in entailment task.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st. PASCAL Recognision Textual Entailment Challenge Workshop,</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="2585" citStr="Vanderwende et al., 2005" startWordPosition="401" endWordPosition="405">ay (paraphrasing). Research on automatic methods for recognizing semantic relations between sentences is still relatively new, and many basic issues need to be resolved. In this paper we address two such related issues: (1) to what extent can human annotators label semantic overlap relations between words, phrases and sentences, and (2) what is the added value of linguistically informed analyses. It is generally assumed that pure string overlap is not sufficient for recognizing semantic relations; and that using some form of syntactic analysis may be beneficial (e.g., (Herrera et al., 2005), (Vanderwende et al., 2005)). Our working hypothesis is that semantic overlap at the word and phrase levels may provide a good basis for deciding the semantic relation between sentences. Recognising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on wor</context>
</contexts>
<marker>Vanderwende, Coughlin, Dolan, 2005</marker>
<rawString>L. Vanderwende, D. Coughlin, and W. Dolan. 2005. What syntax can contribute in entailment task. In Proceedings of the 1st. PASCAL Recognision Textual Entailment Challenge Workshop, Southampton, U.K.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>