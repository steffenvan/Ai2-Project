<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000202">
<title confidence="0.99799">
Recognizing Authority in Dialogue with an Integer Linear Programming
Constrained Model
</title>
<author confidence="0.995289">
Elijah Mayfield
</author>
<affiliation confidence="0.981776">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.612256">
Pittsburgh, PA 15213
</address>
<email confidence="0.998672">
elijah@cmu.edu
</email>
<author confidence="0.987064">
Carolyn Penstein Ros´e
</author>
<affiliation confidence="0.9791595">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.612067">
Pittsburgh, PA 15213
</address>
<email confidence="0.998776">
cprose@cs.cmu.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998619">
We present a novel computational formula-
tion of speaker authority in discourse. This
notion, which focuses on how speakers posi-
tion themselves relative to each other in dis-
course, is first developed into a reliable cod-
ing scheme (0.71 agreement between human
annotators). We also provide a computational
model for automatically annotating text using
this coding scheme, using supervised learning
enhanced by constraints implemented with In-
teger Linear Programming. We show that this
constrained model’s analyses of speaker au-
thority correlates very strongly with expert hu-
man judgments (r2 coefficient of 0.947).
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895529411765">
In this work, we seek to formalize the ways speak-
ers position themselves in discourse. We do this in
a way that maintains a notion of discourse structure,
and which can be aggregated to evaluate a speaker’s
overall stance in a dialogue. We define the body of
work in positioning to include any attempt to formal-
ize the processes by which speakers attempt to influ-
ence or give evidence of their relations to each other.
Constructs such as Initiative and Control (Whittaker
and Stenton, 1988), which attempt to operationalize
the authority over a discourse’s structure, fall under
the umbrella of positioning. As we construe posi-
tioning, it also includes work on detecting certainty
and confusion in speech (Liscombe et al., 2005),
which models a speaker’s understanding of the in-
formation in their statements. Work in dialogue act
tagging is also relevant, as it seeks to describe the ac-
</bodyText>
<page confidence="0.910738">
1018
</page>
<bodyText confidence="0.999770588235294">
tions and moves with which speakers display these
types of positioning (Stolcke et al., 2000).
To complement these bodies of work, we choose
to focus on the question of how speakers position
themselves as authoritative in a discourse. This
means that we must describe the way speakers intro-
duce new topics or discussions into the discourse;
the way they position themselves relative to that
topic; and how these functions interact with each
other. While all of the tasks mentioned above focus
on specific problems in the larger rhetorical question
of speaker positioning, none explicitly address this
framing of authority. Each does have valuable ties
to the work that we would like to do, and in section
2, we describe prior work in each of those areas, and
elaborate on how each relates to our questions.
We measure this as an authoritativeness ratio. Of
the contentful dialogue moves made by a speaker,
in what fraction of those moves is the speaker po-
sitioned as the primary authority on that topic? To
measure this quantitatively, we introduce the Nego-
tiation framework, a construct from the field of sys-
temic functional linguistics (SFL), which addresses
specifically the concepts that we are interested in.
We present a reproducible formulation of this so-
ciolinguistics research in section 3, along with our
preliminary findings on reliability between human
coders, where we observe inter-rater agreement of
0.71. Applying this coding scheme to data, we see
strong correlations with important motivational con-
structs such as Self-Efficacy (Bandura, 1997) as well
as learning gains.
Next, we address automatic coding of the Ne-
gotiation framework, which we treat as a two-
</bodyText>
<note confidence="0.961981">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1018–1026,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999849733333333">
dimensional classification task. One dimension is
a set of codes describing the authoritative status of
a contribution1. The other dimension is a segmen-
tation task. We impose constraints on both of these
models based on the structure observed in the work
of SFL. These constraints are formulated as boolean
statements describing what a correct label sequence
looks like, and are imposed on our model using an
Integer Linear Programming formulation (Roth and
Yih, 2004). In section 5, this model is evaluated
on a subset of the MapTask corpus (Anderson et
al., 1991) and shows a high correlation with human
judgements of authoritativeness (r2 = 0.947). After
a detailed error analysis, we will conclude the paper
in section 6 with a discussion of our future work.
</bodyText>
<sectionHeader confidence="0.987652" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999903965517241">
The Negotiation framework, as formulated by the
SFL community, places a special emphasis on how
speakers function in a discourse as sources or recip-
ients of information or action. We break down this
concept into a set of codes, one code per contribu-
tion. Before we break down the coding scheme more
concretely in section 3, it is important to understand
why we have chosen to introduce a new framework,
rather than reusing existing computational work.
Much work has examined the emergence of dis-
course structure from the choices speakers make at
the linguistic and intentional level (Grosz and Sid-
ner, 1986). For instance, when a speaker asks a
question, it is expected to be followed with an an-
swer. In discourse analysis, this notion is described
through dialogue games (Carlson, 1983), while con-
versation analysis frames the structure in terms of
adjacency pairs (Schegloff, 2007). These expec-
tations can be viewed under the umbrella of con-
ditional relevance (Levinson, 2000), and the ex-
changes can be labelled discourse segments.
In prior work, the way that people influence dis-
course structure is described through the two tightly-
related concepts of initiative and control. A speaker
who begins a discourse segment is said to have ini-
tiative, while control accounts for which speaker is
being addressed in a dialogue (Whittaker and Sten-
ton, 1988). As initiative passes back and forth be-
tween discourse participants, control over the con-
</bodyText>
<footnote confidence="0.550602">
1We treat each line in our corpus as a single contribution.
</footnote>
<bodyText confidence="0.999490229166667">
versation similarly transfers from one speaker to an-
other (Walker and Whittaker, 1990). This relation is
often considered synchronous, though evidence sug-
gests that the reality is not straightforward (Jordan
and Di Eugenio, 1997).
Research in initiative and control has been ap-
plied in the form of mixed-initiative dialogue sys-
tems (Smith, 1992). This is a large and ac-
tive field, with applications in tutorial dialogues
(Core, 2003), human-robot interactions (Peltason
and Wrede, 2010), and more general approaches to
effective turn-taking (Selfridge and Heeman, 2010).
However, that body of work focuses on influenc-
ing discourse structure through positioning. The
question that we are asking instead focuses on how
speakers view their authority as a source of informa-
tion about the topic of the discourse.
In particular, consider questioning in discourse.
In mixed-initiative analysis of discourse, asking a
question always gives you control of a discourse.
There is an expectation that your question will be
followed by an answer. A speaker might already
know the answer to a question they asked - for
instance, when a teacher is verifying a student’s
knowledge. However, in most cases asking a ques-
tion represents a lack of authority, treating the other
speakers as a source for that knowledge. While there
have been preliminary attempts to separate out these
specific types of positioning in initiative, such as
Chu-Carroll and Brown (1998), it has not been stud-
ied extensively in a computational setting.
Another similar thread of research is to identify
a speaker’s certainty, that is, the confidence of a
speaker and how that self-evaluation affects their
language (Pon-Barry and Shieber, 2010). Substan-
tial work has gone into automatically identifying
levels of speaker certainty, for example in Liscombe
et al. (2005) and Litman et al. (2009). The major
difference between our work and this body of liter-
ature is that work on certainty has rarely focused on
how state translates into interaction between speak-
ers (with some exceptions, such as the application
of certainty to tutoring dialogues (Forbes-Riley and
Litman, 2009)). Instead, the focus is on the person’s
self-evaluation, independent of the influence on the
speaker’s positioning within a discourse.
Dialogue act tagging seeks to describe the moves
people make to express themselves in a discourse.
</bodyText>
<page confidence="0.995013">
1019
</page>
<bodyText confidence="0.999948333333333">
This task involves defining the role of each contri-
bution based on its function (Stolcke et al., 2000).
We know that there are interesting correlations be-
tween these acts and other factors, such as learning
gains (Litman and Forbes-Riley, 2006) and the rel-
evance of a contribution for summarization (Wrede
and Shriberg, 2003). However, adapting dialogue
act tags to the question of how speakers position
themselves is not straightforward. In particular,
the granularity of these tagsets, which is already a
highly debated topic (Popescu-Belis, 2008), is not
ideal for the task we have set for ourselves. Many
dialogue acts can be used in authoritative or non-
authoritative ways, based on context, and can posi-
tion a speaker as either giver or receiver of informa-
tion. Thus these more general tagsets are not specific
enough to the role of authority in discourse.
Each of these fields of prior work is highly valu-
able. However, none were designed to specifically
describe how people present themselves as a source
or recipient of knowledge in a discourse. Thus, we
have chosen to draw on a different field of soci-
olinguistics. Our formalization of that theory is de-
scribed in the next section.
</bodyText>
<sectionHeader confidence="0.988871" genericHeader="method">
3 The Negotiation Framework
</sectionHeader>
<bodyText confidence="0.9867535">
We now present the Negotiation framework2, which
we use to answer the questions left unanswered in
the previous section. Within the field of SFL, this
framework has been continually refined over the last
three decades (Berry, 1981; Martin, 1992; Martin,
2003). It attempts to describe how speakers use their
role as a source of knowledge or action to position
themselves relative to others in a discourse.
Applications of the framework include distin-
guishing between focus on teacher knowledge and
student reasoning (Veel, 1999) and distribution of
authority in juvenile trials (Martin et al., 2008). The
framework can also be applied to problems similar
to those studied through the lens of initiative, such
as the distinction between authority over discourse
structure and authority over content (Martin, 2000).
A challenge of applying this work to language
technologies is that it has historically been highly
2All examples are drawn from the MapTask corpus and in-
volve an instruction giver (g) and follower (f). Within examples,
discourse segment boundaries are shown by horizontal lines.
qualitative, with little emphasis placed on repro-
ducibility. We have formulated a pared-down, repro-
ducible version of the framework, presented in Sec-
tion 3.1. Evidence of the usefulness of that formu-
lation for identifying authority, and of correlations
that we can study based on these codes, is presented
briefly in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.998786">
3.1 Our Formulation of Negotiation
</subsectionHeader>
<bodyText confidence="0.999210481481482">
The codes that we can apply to a contribution us-
ing the Negotiation framework are comprised of four
main codes, K1, K2, A1, and A2, and two additional
codes, ch and o. This is a reduction over the many
task-specific or highly contextual codes used in the
original work. This was done to ensure that a ma-
chine learning classification task would not be over-
whelmed with many infrequent classes.
The main codes are divided by two questions.
First, is the contribution related to exchanging infor-
mation, or to exchanging services and actions? If the
former, then it is a K move (knowledge); if the latter,
then an A move (action). Second, is the contribution
acting as a primary actor, or secondary? In the case
of knowledge, this often correlates to the difference
between assertions (K1) and queries (K2). For in-
stance, a statement of fact or opinion is a K1:
g K1 well i’ve got a great viewpoint
here just below the east lake
By contrast, asking for someone else’s knowledge
or opinion is a K2:
g K2 what have you got underneath the
east lake
f K1 rocket launch
In the case of action, the codes usually corre-
spond to narrating action (A1) and giving instruc-
tions (A2), as below:
</bodyText>
<equation confidence="0.8513245">
g A2 go almost to the edge of the lake
f A1 yeah
</equation>
<bodyText confidence="0.9991367">
A challenge move (ch) is one which directly con-
tradicts the content or assertion of the previous line,
or makes that previous contribution irrelevant. For
instance, consider the exchange below, where an in-
struction is rejected because its presuppositions are
broken by the challenging statement.
g A2 then head diagonally down to-
wards the bottom of the dead tree
f ch i have don’t have a dead tree i
have a dutch elm
</bodyText>
<page confidence="0.960042">
1020
</page>
<bodyText confidence="0.98880428">
All moves that do not fit into one of these cate-
gories are classified as other (o). This includes back-
channel moves, floor-grabbing moves, false starts,
and any other non-contentful contributions.
This theory makes use of discourse segmenta-
tion. Research in the SFL community has focused
on intra-segment structure, and empirical evidence
from this research has shown that exchanges be-
tween speakers follow a very specific pattern:
o* X2? o* X1+ o*
That is to say, each segment contains a primary
move (a K1 or an A1) and an optional preceding
secondary move, with other non-contentful moves
interspersed throughout. A single statement of fact
would be a K1 move comprising an entire segment,
while a single question/answer pair would be a K2
move followed by a K1. Longer exchanges of many
lines obviously also occur.
We iteratively developed a coding manual which
describes, in a reproducible way, how to apply the
codes listed above. The six codes we use, along with
their frequency in our corpus, are given in Table 1.
In the next section, we evaluate the reliability and
utility of hand-coded data, before moving on to au-
tomation in section 4.
</bodyText>
<subsectionHeader confidence="0.999299">
3.2 Preliminary Evaluation
</subsectionHeader>
<bodyText confidence="0.999840210526316">
This coding scheme was evaluated for reliability on
two corpora using Cohen’s kappa (Cohen, 1960).
Within the social sciences community, a kappa
above 0.7 is considered acceptable. Two conversa-
tions were each coded by hand by two trained anno-
tators. The first conversation was between three stu-
dents in a collaborative learning task; inter-rater re-
liability kappa for Negotiation labels was 0.78. The
second conversation was from the MapTask corpus,
and kappa was 0.71. Further data was labelled by
hand by one trained annotator.
In our work, we label conversations using the cod-
ing scheme above. To determine how well these
codes correlate with other interesting factors, we
choose to assign a quantitative measure of authori-
tativeness to each speaker. This measure can then
be compared to other features of a speaker. To do
this, we use the coded labels to assign an Authori-
tativeness Ratio to each speaker. First, we define a
</bodyText>
<table confidence="0.999819">
Code Meaning Count Percent
K1 Primary Knower 984 22.5
K2 Secondary Knower 613 14.0
A1 Primary Actor 471 10.8
A2 Secondary Actor 708 16.2
ch Challenge 129 2.9
o Other 1469 33.6
Total 4374 100.0
</table>
<tableCaption confidence="0.9917685">
Table 1: The six codes in our coding scheme, along with
their frequency in our corpus of twenty conversations.
</tableCaption>
<bodyText confidence="0.471008">
function A(5, c, L) for a speaker, a contribution, and
a set of labels L C {K1, K2, A1, A2, o, ch} as:
</bodyText>
<equation confidence="0.745366">
� 1 c spoken by 5 with label l E L
A(5, c, L) _
0 otherwise.
</equation>
<bodyText confidence="0.999550333333333">
We then define the Authoritativeness ratio
Auth(5) for a speaker 5 in a dialogue consisting
of contributions c1...cn as:
</bodyText>
<equation confidence="0.96252475">
n
A(5, ci, {K1, A2})
Auth(5) _ i=1
A(5, ci, {K1, K2, A1, A2})
</equation>
<bodyText confidence="0.999919">
The intuition behind this ratio is that we are only
interested in the four main label types in our analy-
sis - at least for an initial description of authority, we
do not consider the non-contentful o moves. Within
these four main labels, there are clearly two that ap-
pear “dominant” - statements of fact or opinion, and
commands or instructions - and two that appear less
dominant - questions or requests for information,
and narration of an action. We sum these together
to reach a single numeric value for each speaker’s
projection of authority in the dialogue.
The full details of our external validations of this
approach are available in Howley et al. (2011). To
summarize, we considered two data sets involving
student collaborative learning. The first data set con-
sisted of pairs of students interacting over two days,
and was annotated for aggressive behavior, to assess
warning factors in social interactions. Our analysis
</bodyText>
<equation confidence="0.658167">
n
i=1
</equation>
<page confidence="0.890292">
1021
</page>
<bodyText confidence="0.999986076923077">
showed that aggressive behavior correlated with au-
thoritativeness ratio (p &lt; .05), and that less aggres-
sive students became less authoritative in the second
day (p &lt; .05, effect size .15Q). The second data
set was analyzed for Self-Efficacy - the confidence
of each student in their own ability (Bandura, 1997)
- as well as actual learning gains based on pre- and
post-test scores. We found that the Authoritativeness
ratio was a significant predictor of learning gains
(r2 = .41, p &lt; .04). Furthermore, in a multiple re-
gression, we determined that the Authoritativeness
ratio of both students in a group predict the average
Self-Efficacy of the pair (r2 = .12, p &lt; .01).
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="method">
4 Computational Model
</sectionHeader>
<bodyText confidence="0.999983454545455">
We know that our coding scheme is useful for mak-
ing predictions about speakers. We now judge
whether it can be reproduced fully automatically.
Our model must select, for each contribution cz in a
dialogue, the most likely classification label lz from
{K1, K2, A1, A2, o, ch}. We also build in paral-
lel a segmentation model to select sz from the set
{new, same}. Our baseline approach to both prob-
lems is to use a bag-of-words model of the contribu-
tion, and use machine learning for classification.
Certain types of interactions, explored in section
4.1, are difficult or impossible to classify without
context. We build a contextual feature space, de-
scribed in section 4.2, to enhance our baseline bag-
of-words model. We can also describe patterns that
appear in discourse segments, as detailed in section
3.1. In our coding manual, these instructions are
given as rules for how segments should be coded by
humans. Our hypothesis is that by enforcing these
rules in the output of our automatic classifier, per-
formance will increase. In section 4.3 we formalize
these constraints using Integer Linear Programming.
</bodyText>
<subsectionHeader confidence="0.998187">
4.1 Challenging cases
</subsectionHeader>
<bodyText confidence="0.970071444444444">
We want to distinguish between phenomena such as
in the following two examples.
f K2 so I’m like on the bank on the
bank of the east lake
g K1 yeah
In this case, a one-token contribution is indis-
putably a K1 move, answering a yes/no question.
However, in the dialogue below, it is equally inar-
guable that the same move is an A1:
</bodyText>
<equation confidence="0.9117815">
g A2 go almost to the edge of the lake
f A1 yeah
</equation>
<bodyText confidence="0.999861">
Without this context, these moves would be indis-
tinguishable to a model. With it, they are both easily
classified correctly.
We also observed that markers for segmentation
of a segment vary between contentful initiations and
non-contentful ones. For instance, filler noises can
often initiate segments:
</bodyText>
<equation confidence="0.98656">
g o hmm...
g K2 do you have a farmer’s gate?
f K1 no
</equation>
<bodyText confidence="0.9688175">
Situations such as this are common. This is also a
challenge for segmentation, as demonstrated below:
</bodyText>
<equation confidence="0.965734166666667">
g K1 oh oh it’s on the right-hand side
of my great viewpoint
f o okay yeah
g o right eh
g A2 go almost to the edge of the lake
f A1 yeah
</equation>
<bodyText confidence="0.998069444444444">
A long statement or instruction from one speaker
is followed up with a terse response (in the same
segment) from the listener. However, after that back-
channel move, a short floor-grabbing move is often
made to start the next segment. This is a distinc-
tion that a bag-of-words model would have difficulty
with. This is markedly different from contentful seg-
ment initiations:
g A2 come directly down below the
stone circle and we come up
f ch I don’t have a stone circle
g o you don’t have a stone circle
All three of these lines look like statements, which
often initiate new segments. However, only the first
should be marked as starting a new segment. The
other two are topically related, in the second line by
contradicting the instruction, and in the third by re-
peating the previous person’s statement.
</bodyText>
<subsectionHeader confidence="0.995647">
4.2 Contextual Feature Space Additions
</subsectionHeader>
<bodyText confidence="0.99997425">
To incorporate the insights above into our model, we
append features to our bag-of-words model. First,
in our classification model we include both lexical
bigrams and part-of-speech bigrams to encode fur-
ther lexical knowledge and some notion of syntac-
tic structure. To account for restatements and topic
shifts, we add a feature based on cosine similarity
(using term vectors weighted by TF-IDF calculated
</bodyText>
<page confidence="0.992183">
1022
</page>
<bodyText confidence="0.9999593125">
over training data). We then add a feature for the
predicted label of the previous contribution - after
each contribution is classified, the next contribution
adds a feature for the automatic label. This requires
our model to function as an on-line classifier.
We build two segmentation models, one trained
on contributions of less than four tokens, and an-
other trained on contributions of four or more to-
kens, to distinguish between characteristics of con-
tentful and non-contentful contributions. To the
short-contribution model, we add two additional fea-
tures. The first represents the ratio between the
length of the current contribution and the length of
the previous contribution. The second represents
whether a change in speaker has occurred between
the current and previous contribution.
</bodyText>
<subsectionHeader confidence="0.984671">
4.3 Constraints using Integer Linear
Programming
</subsectionHeader>
<bodyText confidence="0.981014892857143">
We formulate our constraints using Integer Linear
Programming (ILP). This formulation has an ad-
vantage over other sequence labelling formulations,
such as Viterbi decoding, in its ability to enforce
structure through constraints. We then enhance this
classifier by adding constraints, which allow expert
knowledge of discourse structure to be enforced in
classification. We can use these constraints to elim-
inate label options which would violate the rules for
a segment outlined in our coding manual.
Each classification decision is made at the contri-
bution level, jointly optimizing the Negotiation la-
bel and segmentation label for a single contribution,
then treating those labels as given for the next con-
tribution classification.
To define our objective function for optimization,
for each possible label, we train a one vs. all SVM,
and use the resulting regression for each label as
�
a score, giving us six values li for our Negotiation
label and two values si for our segmentation label.
Then, subject to the constraints below, we optimize:
Recall from section 3.1 that our discourse seg-
ments follow strict rules related to ordering and rep-
etition of contributions. Below, we list the con-
straints that we used in our model to enforce that
pattern, along with a brief explanation of the intu-
ition behind each.
</bodyText>
<equation confidence="0.97502125">
Vci E s, (li = K2) =�- (1)
Vj &lt; i,cj E t =�&apos; (lj =�K1)
Vci E s, (li = A2) ==�- (2)
Vj &lt; i,cj E t ==�- (lj =�A1)
</equation>
<bodyText confidence="0.99959975">
The first constraints enforce the rule that a pri-
mary move cannot occur before a secondary move
in the same segment. For instance, a question must
initiate a new segment if it follows a statement.
</bodyText>
<equation confidence="0.9934655">
Vci E s, (li E {A1, A2}) ==&gt;. (3)
Vj &lt; i, cj E s ==&gt;. (lj V {K1, K2})
Vci E s, (li E {K1, K2}) ==�- (4)
Vj &lt; i, cj E s ==�- (lj V {A1, A2})
</equation>
<bodyText confidence="0.9974855">
These constraints specify that A moves and K
moves cannot cooccur in a segment. An instruc-
tion for action and a question requesting information
must be considered separate segments.
</bodyText>
<equation confidence="0.94536625">
Vci E s, (li = A1) ==&gt;. ((li_1 = A1) V (5)
Vj &lt; i, cj E s ==�- (lj =� A1))
Vci E s, (li = K1) ==&gt;. ((li_1 = K1) V (6)
Vj &lt; i,cj E s ==�- (lj =�K1))
</equation>
<bodyText confidence="0.999253">
This pair states that two primary moves cannot oc-
cur in the same segment unless they are contiguous,
in rapid succession.
</bodyText>
<equation confidence="0.60535775">
Vci E s, (li = A1) =�- (7)
Vj &lt; i, cj E s, (lj = A2) =&gt;. (Si 7� Sj )
arg max l + s Vci E s, (li = K1) =&gt;. (8)
lE�li,sEsi Vj &lt; i,cj E s,(lj = K2) =&gt;. (Si 7�Sj)
</equation>
<bodyText confidence="0.999887375">
Thus, at each contribution, if the highest-scoring
Negotiation label breaks a constraint, the model can
optimize whether to drop to the next-most-likely la-
bel, or start a new segment.
The last set of constraints enforce the intuitive
notion that a speaker cannot follow their own sec-
ondary move with a primary move in that segment
(such as answering their own question).
</bodyText>
<page confidence="0.970113">
1023
</page>
<bodyText confidence="0.999951888888889">
Computationally, an advantage of these con-
straints is that they do not extend past the current
segment in history. This means that they usually
are only enforced over the past few moves, and do
not enforce any global constraint over the structure
of the whole dialogue. This allows the constraints
to be flexible to various conversational styles, and
tractable for fast computation independent of the
length of the dialogue.
</bodyText>
<sectionHeader confidence="0.994797" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99992">
We test our models on a twenty conversation sub-
set of the MapTask corpus detailed in Table 1. We
compare the use of four models in our results.
</bodyText>
<listItem confidence="0.8870704">
• Baseline: This model uses a bag-of-words fea-
ture space as input to an SVM classifier. No
segmentation model is used and no ILP con-
straints are enforced.
• Baseline+ILP: This model uses the baseline
feature space as input to both classification and
segmentation models. ILP constraints are en-
forced between these models.
• Contextual: This model uses our enhanced
feature space from section 4.2, with no segmen-
tation model and no ILP constraints enforced.
• Contextual+ILP: This model uses the en-
hanced feature spaces for both Negotiation la-
bels and segment boundaries from section 4.2
to enforce ILP constraints.
</listItem>
<bodyText confidence="0.9980836">
For segmentation, we evaluate our models using
exact-match accuracy. We use multiple evaluation
metrics to judge classification. The first and most
basic is accuracy - the percentage of accurately cho-
sen Negotiation labels. Secondly, we use Cohen’s
Kappa (Cohen, 1960) to judge improvement in ac-
curacy over chance. The final evaluation is the r2
coefficient computed between predicted and actual
Authoritativeness ratios per speaker. This represents
how much variance in authoritativeness is accounted
for in the predicted ratios. This final metric is the
most important for measuring reproducibility of hu-
man analyses of speaker authority in conversation.
We use SIDE for feature extraction (Mayfield
and Ros´e, 2010), SVM-Light for machine learning
</bodyText>
<table confidence="0.999477714285714">
Model Accuracy Kappa r2
Baseline 59.7% 0.465 0.354
Baseline+ILP 61.6% 0.488 0.663
Segmentation 72.3%
Contextual 66.7% 0.565 0.908
Contextual+ILP 68.4% 0.584 0.947
Segmentation 74.9%
</table>
<tableCaption confidence="0.924938666666667">
Table 2: Performance evaluation for our models. Each
line is significantly improved in both accuracy and r2 er-
ror from the previous line (p &lt; .01).
</tableCaption>
<bodyText confidence="0.99956025">
(Joachims, 1999), and Learning-Based Java for ILP
inference (Rizzolo and Roth, 2010). Performance
is evaluated by 20-fold cross-validation, where each
fold is trained on 19 conversations and tested on the
remaining one. Statistical significance was calcu-
lated using a student’s paired t-test. For accuracy
and kappa, n = 20 (one data point per conversation)
and for r2, n = 40 (one data point per speaker).
</bodyText>
<subsectionHeader confidence="0.65806">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.998837727272727">
All classification results are given in Table 2 and
charts showing correlation between predicted and
actual speaker Authoritativeness ratios are shown in
Figure 1. We observe that the baseline bag-of-words
model performs well above random chance (kappa
of 0.465); however, its accuracy is still very low
and its ability to predict Authoritativeness ratio of
a speaker is not particularly high (r2 of 0.354 with
ratios from manually labelled data). We observe a
significant improvement when ILP constraints are
applied to this model.
The contextual model described in section 4.2
performs better than our baseline constrained model.
However, the gains found in the contextual model
are somewhat orthogonal to the gains from using
ILP constraints, as applying those constraints to
the contextual model results in further performance
gains (and a high r2 coefficient of 0.947).
Our segmentation model was evaluated based on
exact matches in boundaries. Switching from base-
line to contextual features, we observe an improve-
ment in accuracy of 2.6%.
</bodyText>
<subsectionHeader confidence="0.893882">
5.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9892385">
An error analysis of model predictions explains the
large effect on correlation despite relatively smaller
</bodyText>
<page confidence="0.996893">
1024
</page>
<figureCaption confidence="0.992761">
Figure 1: Plots of predicted (x axis) and actual (y axis) Authoritativeness ratios for speakers across 20 conversations,
for the Baseline (left), Baseline+Constraints (center), and Contextual+Constraints (right) models.
</figureCaption>
<bodyText confidence="0.998907321428572">
changes in accuracy. Our Authoritativeness ratio
does not take into account moves labelled o or
ch. What we find is that the most advanced model
still makes many mistakes at determining whether a
move should be labelled as o or a core move. This er-
ror rate is, however, fairly consistent across the four
core move codes. When a move is determined (cor-
rectly) to not be an o move, the system is highly ac-
curate in distinguishing between the four core labels.
The one systematic confusion that continues to
appear most frequently in our results is the inabil-
ity to distinguish between a segment containing an
A2 move followed by an A1 move, and a segment
containing a K1 move followed by an o move. The
surface structure of these types of exchanges is very
similar. Consider the following two exchanges:
g A2 if you come down almost to the
bottom of the map that I’ve got
f A1 uh-huh
f K1 but the meadow’s below my bro-
ken gate
g o right yes
These two exchanges on a surface level are highly
similar. Out of context, making this distinction is
very hard even for human coders, so it is not surpris-
ing then that this pattern is the most difficult one to
recognize in this corpus. It contributes most of the
remaining confusion between the four core codes.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999988352941176">
In this work we have presented one formulation of
authority in dialogue. This formulation allows us
to describe positioning in discourse in a way that
is complementary to prior work in mixed-initiative
dialogue systems and analysis of speaker certainty.
Our model includes a simple understanding of dis-
course structure while also encoding information
about the types of moves used, and the certainty of a
speaker as a source of information. This formulation
is reproducible by human coders, with an inter-rater
reliability of 0.71.
We have then presented a computational model
for automatically applying these codes per contribu-
tion. In our best model, we see a good 68.4% accu-
racy on a six-way individual contribution labelling
task. More importantly, this model replicates human
analyses of authoritativeness very well, with an r2
coefficient of 0.947.
There is room for improvement in our model in
future work. Further use of contextual features will
more thoroughly represent the information we want
our model to take into account. Our segmentation
accuracy is also fairly low, and further examination
of segmentation accuracy using a more sophisticated
evaluation metric, such as WindowDiff (Pevzner and
Hearst, 2002), would be helpful.
In general, however, we now have an automated
model that is reliable in reproducing human judg-
ments of authoritativeness. We are now interested in
how we can apply this to the larger questions of po-
sitioning we began this paper by asking, especially
in describing speaker positioning at various instants
throughout a single discourse. This will be the main
thrust of our future work.
</bodyText>
<sectionHeader confidence="0.996386" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.7300595">
This research was supported by NSF grants SBE-
0836012 and HCC-0803482.
</bodyText>
<page confidence="0.993845">
1025
</page>
<sectionHeader confidence="0.985595" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999456714285714">
Anne Anderson, Miles Bader, Ellen Bard, Elizabeth
Boyle, Gwyneth Doherty, Simon Garrod, et al. 1991.
The HCRC Map Task Corpus. In Language and
Speech.
Albert Bandura. 1997. Self-efficacy: The Exercise of
Control
Margaret Berry. 1981. Towards Layers of Exchange
Structure for Directive Exchanges. In Network 2.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis.
Jennifer Chu-Carroll and Michael Brown. 1998. An Ev-
idential Model for Tracking Initiative in Collaborative
Dialogue Interactions. In User Modeling and User-
Adapted Interaction.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. In Educational and Psychological
Measurement.
Mark Core and Johanna Moore and Claus Zinn. 2003.
The Role of Initiative in Tutorial Dialogue. In Pro-
ceedings of EACL.
Kate Forbes-Riley and Diane Litman. 2009. Adapting to
Student Uncertainty Improves Tutoring Dialogues. In
Proceedings of Artificial Intelligence in Education.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. In Compu-
tational Linguistics.
Iris Howley and Elijah Mayfield and Carolyn Penstein
Ros´e. 2011. Missing Something? Authority in Col-
laborative Learning. In Proceedings of Computer-
Supported Collaborative Learning.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In Advances in Kernel Methods
- Support Vector Learning.
Pamela Jordan and Barbara Di Eugenio. 1997. Control
and Initiative in Collaborative Problem Solving Dia-
logues. In Proceedings of AAAI Spring Symposium
on Computational Models for Mixed Initiative Inter-
actions.
Stephen Levinson. 2000. Pragmatics.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting Certainness in Spoken Tutorial
Dialogues. In Proceedings of Interspeech.
Diane Litman and Kate Forbes-Riley. 2006. Correlations
betweeen Dialogue Acts and Learning in Spoken Tu-
toring Dialogue. In Natural Language Engineering.
Diane Litman, Mihai Rotaru, and Greg Nicholas. 2009.
Classifying Turn-Level Uncertainty Using Word-Level
Prosody. In Proceedings of Interspeech.
James Martin. 1992. English Text: System and Structure.
James Martin. 2000. Factoring out Exchange: Types of
Structure. In Working with Dialogue.
James Martin and David Rose. 2003. Working with Dis-
course: Meaning Beyond the Clause.
James Martin, Michele Zappavigna, and Paul Dwyer.
2008. Negotiating Shame: Exchange and Genre Struc-
ture in Youth Justice Conferencing. In Proceedings of
European Systemic Functional Linguistics.
Elijah Mayfield and Carolyn Penstein Ros´e. 2010. An
Interactive Tool for Supporting Error Analysis for Text
Mining. In Proceedings of Demo Session at NAACL.
Julia Peltason and Britta Wrede. 2010. Modeling
Human-Robot Interaction Based on Generic Interac-
tion Patterns. In AAAI Report on Dialog with Robots.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. In Computational Linguistics.
Heather Pon-Barry and Stuart Shieber. 2010. Assessing
Self-awareness and Transparency when Classifying a
Speakers Level of Certainty. In Speech Prosody.
Andrei Popescu-Belis. 2008. Dimensionality of Dia-
logue Act Tagsets: An Empirical Analysis of Large
Corpora. In Language Resources and Evaluation.
Nick Rizzolo and Dan Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Language
Resources and Evaluation.
Dan Roth and Wen-Tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural Lan-
guage Tasks. In Proceedings of CoNLL.
Emanuel Schegloff. 2007. Sequence Organization in In-
teraction: A Primer in Conversation Analysis.
Ethan Selfridge and Peter Heeman. 2010. Importance-
Driven Turn-Bidding for Spoken Dialogue Systems.
In Proceedings of ACL.
Ronnie Smith. 1992. A computational model of
expectation-driven mixed-initiative dialog processing.
Ph.D. Dissertation.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, et al. 2000.
Dialogue Act Modeling for Automatic Tagging and
Recognition of Conversational Speech. In Computa-
tional Linguistics.
Robert Veel. 1999. Language, Knowledge, and Author-
ity in School Mathematics. In Pedagogy and the Shap-
ing of Consciousness: Linguistics and Social Pro-
cesses
Marilyn Walker and Steve Whittaker. 1990. Mixed Ini-
tiative in Dialogue: An Investigation into Discourse
Structure. In Proceedings of ACL.
Steve Whittaker and Phil Stenton. 1988. Cues and Con-
trol in Expert-Client Dialogues. In Proceedings of
ACL.
Britta Wrede and Elizabeth Shriberg. 2003. The Re-
lationship between Dialogue Acts and Hot Spots in
Meetings. In IEEE Workshop on Automatic Speech
Recognition and Understanding.
</reference>
<page confidence="0.993469">
1026
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.281446">
<title confidence="0.996861">Recognizing Authority in Dialogue with an Integer Linear Constrained Model</title>
<author confidence="0.94045">Elijah</author>
<affiliation confidence="0.917715">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.857674">Pittsburgh, PA</address>
<email confidence="0.999214">elijah@cmu.edu</email>
<author confidence="0.969592">Carolyn Penstein</author>
<affiliation confidence="0.853962">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.811873">Pittsburgh, PA</address>
<email confidence="0.999238">cprose@cs.cmu.edu</email>
<abstract confidence="0.979149333333333">We present a novel computational formulaof speaker discourse. This notion, which focuses on how speakers position themselves relative to each other in discourse, is first developed into a reliable coding scheme (0.71 agreement between human annotators). We also provide a computational model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with Integer Linear Programming. We show that this constrained model’s analyses of speaker authority correlates very strongly with expert hujudgments coefficient of 0.947).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Anderson</author>
<author>Miles Bader</author>
<author>Ellen Bard</author>
<author>Elizabeth Boyle</author>
<author>Gwyneth Doherty</author>
<author>Simon Garrod</author>
</authors>
<title>The HCRC Map Task Corpus. In Language and Speech.</title>
<date>1991</date>
<contexts>
<context position="4279" citStr="Anderson et al., 1991" startWordPosition="663" endWordPosition="666">, 2011. c�2011 Association for Computational Linguistics dimensional classification task. One dimension is a set of codes describing the authoritative status of a contribution1. The other dimension is a segmentation task. We impose constraints on both of these models based on the structure observed in the work of SFL. These constraints are formulated as boolean statements describing what a correct label sequence looks like, and are imposed on our model using an Integer Linear Programming formulation (Roth and Yih, 2004). In section 5, this model is evaluated on a subset of the MapTask corpus (Anderson et al., 1991) and shows a high correlation with human judgements of authoritativeness (r2 = 0.947). After a detailed error analysis, we will conclude the paper in section 6 with a discussion of our future work. 2 Background The Negotiation framework, as formulated by the SFL community, places a special emphasis on how speakers function in a discourse as sources or recipients of information or action. We break down this concept into a set of codes, one code per contribution. Before we break down the coding scheme more concretely in section 3, it is important to understand why we have chosen to introduce a n</context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, 1991</marker>
<rawString>Anne Anderson, Miles Bader, Ellen Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, et al. 1991. The HCRC Map Task Corpus. In Language and Speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bandura</author>
</authors>
<title>Self-efficacy: The Exercise of Control</title>
<date>1997</date>
<contexts>
<context position="3408" citStr="Bandura, 1997" startWordPosition="528" endWordPosition="529">eaker positioned as the primary authority on that topic? To measure this quantitatively, we introduce the Negotiation framework, a construct from the field of systemic functional linguistics (SFL), which addresses specifically the concepts that we are interested in. We present a reproducible formulation of this sociolinguistics research in section 3, along with our preliminary findings on reliability between human coders, where we observe inter-rater agreement of 0.71. Applying this coding scheme to data, we see strong correlations with important motivational constructs such as Self-Efficacy (Bandura, 1997) as well as learning gains. Next, we address automatic coding of the Negotiation framework, which we treat as a twoProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1018–1026, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics dimensional classification task. One dimension is a set of codes describing the authoritative status of a contribution1. The other dimension is a segmentation task. We impose constraints on both of these models based on the structure observed in the work of SFL. These constraints are formulate</context>
<context position="16703" citStr="Bandura, 1997" startWordPosition="2732" endWordPosition="2733"> in Howley et al. (2011). To summarize, we considered two data sets involving student collaborative learning. The first data set consisted of pairs of students interacting over two days, and was annotated for aggressive behavior, to assess warning factors in social interactions. Our analysis n i=1 1021 showed that aggressive behavior correlated with authoritativeness ratio (p &lt; .05), and that less aggressive students became less authoritative in the second day (p &lt; .05, effect size .15Q). The second data set was analyzed for Self-Efficacy - the confidence of each student in their own ability (Bandura, 1997) - as well as actual learning gains based on pre- and post-test scores. We found that the Authoritativeness ratio was a significant predictor of learning gains (r2 = .41, p &lt; .04). Furthermore, in a multiple regression, we determined that the Authoritativeness ratio of both students in a group predict the average Self-Efficacy of the pair (r2 = .12, p &lt; .01). 4 Computational Model We know that our coding scheme is useful for making predictions about speakers. We now judge whether it can be reproduced fully automatically. Our model must select, for each contribution cz in a dialogue, the most l</context>
</contexts>
<marker>Bandura, 1997</marker>
<rawString>Albert Bandura. 1997. Self-efficacy: The Exercise of Control</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Berry</author>
</authors>
<title>Towards Layers of Exchange Structure for Directive Exchanges.</title>
<date>1981</date>
<journal>In Network</journal>
<volume>2</volume>
<contexts>
<context position="9817" citStr="Berry, 1981" startWordPosition="1558" endWordPosition="1559">uthority in discourse. Each of these fields of prior work is highly valuable. However, none were designed to specifically describe how people present themselves as a source or recipient of knowledge in a discourse. Thus, we have chosen to draw on a different field of sociolinguistics. Our formalization of that theory is described in the next section. 3 The Negotiation Framework We now present the Negotiation framework2, which we use to answer the questions left unanswered in the previous section. Within the field of SFL, this framework has been continually refined over the last three decades (Berry, 1981; Martin, 1992; Martin, 2003). It attempts to describe how speakers use their role as a source of knowledge or action to position themselves relative to others in a discourse. Applications of the framework include distinguishing between focus on teacher knowledge and student reasoning (Veel, 1999) and distribution of authority in juvenile trials (Martin et al., 2008). The framework can also be applied to problems similar to those studied through the lens of initiative, such as the distinction between authority over discourse structure and authority over content (Martin, 2000). A challenge of a</context>
</contexts>
<marker>Berry, 1981</marker>
<rawString>Margaret Berry. 1981. Towards Layers of Exchange Structure for Directive Exchanges. In Network 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Carlson</author>
</authors>
<title>Dialogue Games: An Approach to Discourse Analysis.</title>
<date>1983</date>
<contexts>
<context position="5277" citStr="Carlson, 1983" startWordPosition="832" endWordPosition="833">ction. We break down this concept into a set of codes, one code per contribution. Before we break down the coding scheme more concretely in section 3, it is important to understand why we have chosen to introduce a new framework, rather than reusing existing computational work. Much work has examined the emergence of discourse structure from the choices speakers make at the linguistic and intentional level (Grosz and Sidner, 1986). For instance, when a speaker asks a question, it is expected to be followed with an answer. In discourse analysis, this notion is described through dialogue games (Carlson, 1983), while conversation analysis frames the structure in terms of adjacency pairs (Schegloff, 2007). These expectations can be viewed under the umbrella of conditional relevance (Levinson, 2000), and the exchanges can be labelled discourse segments. In prior work, the way that people influence discourse structure is described through the two tightlyrelated concepts of initiative and control. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988). As initiative passes back and fort</context>
</contexts>
<marker>Carlson, 1983</marker>
<rawString>Lauri Carlson. 1983. Dialogue Games: An Approach to Discourse Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Michael Brown</author>
</authors>
<title>An Evidential Model for Tracking Initiative in Collaborative Dialogue Interactions. In User Modeling and UserAdapted Interaction.</title>
<date>1998</date>
<contexts>
<context position="7436" citStr="Chu-Carroll and Brown (1998)" startWordPosition="1172" endWordPosition="1175">ular, consider questioning in discourse. In mixed-initiative analysis of discourse, asking a question always gives you control of a discourse. There is an expectation that your question will be followed by an answer. A speaker might already know the answer to a question they asked - for instance, when a teacher is verifying a student’s knowledge. However, in most cases asking a question represents a lack of authority, treating the other speakers as a source for that knowledge. While there have been preliminary attempts to separate out these specific types of positioning in initiative, such as Chu-Carroll and Brown (1998), it has not been studied extensively in a computational setting. Another similar thread of research is to identify a speaker’s certainty, that is, the confidence of a speaker and how that self-evaluation affects their language (Pon-Barry and Shieber, 2010). Substantial work has gone into automatically identifying levels of speaker certainty, for example in Liscombe et al. (2005) and Litman et al. (2009). The major difference between our work and this body of literature is that work on certainty has rarely focused on how state translates into interaction between speakers (with some exceptions,</context>
</contexts>
<marker>Chu-Carroll, Brown, 1998</marker>
<rawString>Jennifer Chu-Carroll and Michael Brown. 1998. An Evidential Model for Tracking Initiative in Collaborative Dialogue Interactions. In User Modeling and UserAdapted Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales.</title>
<date>1960</date>
<booktitle>In Educational and Psychological Measurement.</booktitle>
<contexts>
<context position="13967" citStr="Cohen, 1960" startWordPosition="2258" endWordPosition="2259">e comprising an entire segment, while a single question/answer pair would be a K2 move followed by a K1. Longer exchanges of many lines obviously also occur. We iteratively developed a coding manual which describes, in a reproducible way, how to apply the codes listed above. The six codes we use, along with their frequency in our corpus, are given in Table 1. In the next section, we evaluate the reliability and utility of hand-coded data, before moving on to automation in section 4. 3.2 Preliminary Evaluation This coding scheme was evaluated for reliability on two corpora using Cohen’s kappa (Cohen, 1960). Within the social sciences community, a kappa above 0.7 is considered acceptable. Two conversations were each coded by hand by two trained annotators. The first conversation was between three students in a collaborative learning task; inter-rater reliability kappa for Negotiation labels was 0.78. The second conversation was from the MapTask corpus, and kappa was 0.71. Further data was labelled by hand by one trained annotator. In our work, we label conversations using the coding scheme above. To determine how well these codes correlate with other interesting factors, we choose to assign a qu</context>
<context position="25514" citStr="Cohen, 1960" startWordPosition="4269" endWordPosition="4270">ILP constraints are enforced between these models. • Contextual: This model uses our enhanced feature space from section 4.2, with no segmentation model and no ILP constraints enforced. • Contextual+ILP: This model uses the enhanced feature spaces for both Negotiation labels and segment boundaries from section 4.2 to enforce ILP constraints. For segmentation, we evaluate our models using exact-match accuracy. We use multiple evaluation metrics to judge classification. The first and most basic is accuracy - the percentage of accurately chosen Negotiation labels. Secondly, we use Cohen’s Kappa (Cohen, 1960) to judge improvement in accuracy over chance. The final evaluation is the r2 coefficient computed between predicted and actual Authoritativeness ratios per speaker. This represents how much variance in authoritativeness is accounted for in the predicted ratios. This final metric is the most important for measuring reproducibility of human analyses of speaker authority in conversation. We use SIDE for feature extraction (Mayfield and Ros´e, 2010), SVM-Light for machine learning Model Accuracy Kappa r2 Baseline 59.7% 0.465 0.354 Baseline+ILP 61.6% 0.488 0.663 Segmentation 72.3% Contextual 66.7%</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. In Educational and Psychological Measurement.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Core</author>
<author>Johanna Moore</author>
<author>Claus Zinn</author>
</authors>
<title>The Role of Initiative in Tutorial Dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL.</booktitle>
<marker>Core, Moore, Zinn, 2003</marker>
<rawString>Mark Core and Johanna Moore and Claus Zinn. 2003. The Role of Initiative in Tutorial Dialogue. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
</authors>
<title>Adapting to Student Uncertainty Improves Tutoring Dialogues.</title>
<date>2009</date>
<booktitle>In Proceedings of Artificial Intelligence in Education.</booktitle>
<contexts>
<context position="8127" citStr="Forbes-Riley and Litman, 2009" startWordPosition="1281" endWordPosition="1284">ing. Another similar thread of research is to identify a speaker’s certainty, that is, the confidence of a speaker and how that self-evaluation affects their language (Pon-Barry and Shieber, 2010). Substantial work has gone into automatically identifying levels of speaker certainty, for example in Liscombe et al. (2005) and Litman et al. (2009). The major difference between our work and this body of literature is that work on certainty has rarely focused on how state translates into interaction between speakers (with some exceptions, such as the application of certainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to describe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contribution based on its function (Stolcke et al., 2000). We know that there are interesting correlations between these acts and other factors, such as learning gains (Litman and Forbes-Riley, 2006) and the relevance of a contribution for summarization (Wrede and Shriberg, 2003). However, adapting dialogue act ta</context>
</contexts>
<marker>Forbes-Riley, Litman, 2009</marker>
<rawString>Kate Forbes-Riley and Diane Litman. 2009. Adapting to Student Uncertainty Improves Tutoring Dialogues. In Proceedings of Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intentions, and the Structure of Discourse. In Computational Linguistics.</booktitle>
<contexts>
<context position="5097" citStr="Grosz and Sidner, 1986" startWordPosition="799" endWordPosition="803">k. 2 Background The Negotiation framework, as formulated by the SFL community, places a special emphasis on how speakers function in a discourse as sources or recipients of information or action. We break down this concept into a set of codes, one code per contribution. Before we break down the coding scheme more concretely in section 3, it is important to understand why we have chosen to introduce a new framework, rather than reusing existing computational work. Much work has examined the emergence of discourse structure from the choices speakers make at the linguistic and intentional level (Grosz and Sidner, 1986). For instance, when a speaker asks a question, it is expected to be followed with an answer. In discourse analysis, this notion is described through dialogue games (Carlson, 1983), while conversation analysis frames the structure in terms of adjacency pairs (Schegloff, 2007). These expectations can be viewed under the umbrella of conditional relevance (Levinson, 2000), and the exchanges can be labelled discourse segments. In prior work, the way that people influence discourse structure is described through the two tightlyrelated concepts of initiative and control. A speaker who begins a disco</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attention, Intentions, and the Structure of Discourse. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Howley</author>
<author>Elijah Mayfield</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Missing Something? Authority in Collaborative Learning.</title>
<date>2011</date>
<booktitle>In Proceedings of ComputerSupported Collaborative Learning.</booktitle>
<marker>Howley, Mayfield, Ros´e, 2011</marker>
<rawString>Iris Howley and Elijah Mayfield and Carolyn Penstein Ros´e. 2011. Missing Something? Authority in Collaborative Learning. In Proceedings of ComputerSupported Collaborative Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods - Support Vector Learning.</booktitle>
<contexts>
<context position="26343" citStr="Joachims, 1999" startWordPosition="4392" endWordPosition="4393">veness is accounted for in the predicted ratios. This final metric is the most important for measuring reproducibility of human analyses of speaker authority in conversation. We use SIDE for feature extraction (Mayfield and Ros´e, 2010), SVM-Light for machine learning Model Accuracy Kappa r2 Baseline 59.7% 0.465 0.354 Baseline+ILP 61.6% 0.488 0.663 Segmentation 72.3% Contextual 66.7% 0.565 0.908 Contextual+ILP 68.4% 0.584 0.947 Segmentation 74.9% Table 2: Performance evaluation for our models. Each line is significantly improved in both accuracy and r2 error from the previous line (p &lt; .01). (Joachims, 1999), and Learning-Based Java for ILP inference (Rizzolo and Roth, 2010). Performance is evaluated by 20-fold cross-validation, where each fold is trained on 19 conversations and tested on the remaining one. Statistical significance was calculated using a student’s paired t-test. For accuracy and kappa, n = 20 (one data point per conversation) and for r2, n = 40 (one data point per speaker). 5.1 Results All classification results are given in Table 2 and charts showing correlation between predicted and actual speaker Authoritativeness ratios are shown in Figure 1. We observe that the baseline bag-</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. In Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Jordan</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Control and Initiative in Collaborative Problem Solving Dialogues.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Computational Models for Mixed Initiative Interactions.</booktitle>
<marker>Jordan, Di Eugenio, 1997</marker>
<rawString>Pamela Jordan and Barbara Di Eugenio. 1997. Control and Initiative in Collaborative Problem Solving Dialogues. In Proceedings of AAAI Spring Symposium on Computational Models for Mixed Initiative Interactions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Levinson</author>
</authors>
<date>2000</date>
<publisher>Pragmatics.</publisher>
<contexts>
<context position="5468" citStr="Levinson, 2000" startWordPosition="861" endWordPosition="862">ve chosen to introduce a new framework, rather than reusing existing computational work. Much work has examined the emergence of discourse structure from the choices speakers make at the linguistic and intentional level (Grosz and Sidner, 1986). For instance, when a speaker asks a question, it is expected to be followed with an answer. In discourse analysis, this notion is described through dialogue games (Carlson, 1983), while conversation analysis frames the structure in terms of adjacency pairs (Schegloff, 2007). These expectations can be viewed under the umbrella of conditional relevance (Levinson, 2000), and the exchanges can be labelled discourse segments. In prior work, the way that people influence discourse structure is described through the two tightlyrelated concepts of initiative and control. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988). As initiative passes back and forth between discourse participants, control over the con1We treat each line in our corpus as a single contribution. versation similarly transfers from one speaker to another (Walker and Whittak</context>
</contexts>
<marker>Levinson, 2000</marker>
<rawString>Stephen Levinson. 2000. Pragmatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackson Liscombe</author>
<author>Julia Hirschberg</author>
<author>Jennifer Venditti</author>
</authors>
<title>Detecting Certainness in Spoken Tutorial Dialogues.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="1684" citStr="Liscombe et al., 2005" startWordPosition="248" endWordPosition="251"> that maintains a notion of discourse structure, and which can be aggregated to evaluate a speaker’s overall stance in a dialogue. We define the body of work in positioning to include any attempt to formalize the processes by which speakers attempt to influence or give evidence of their relations to each other. Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse’s structure, fall under the umbrella of positioning. As we construe positioning, it also includes work on detecting certainty and confusion in speech (Liscombe et al., 2005), which models a speaker’s understanding of the information in their statements. Work in dialogue act tagging is also relevant, as it seeks to describe the ac1018 tions and moves with which speakers display these types of positioning (Stolcke et al., 2000). To complement these bodies of work, we choose to focus on the question of how speakers position themselves as authoritative in a discourse. This means that we must describe the way speakers introduce new topics or discussions into the discourse; the way they position themselves relative to that topic; and how these functions interact with e</context>
<context position="7818" citStr="Liscombe et al. (2005)" startWordPosition="1231" endWordPosition="1234">n represents a lack of authority, treating the other speakers as a source for that knowledge. While there have been preliminary attempts to separate out these specific types of positioning in initiative, such as Chu-Carroll and Brown (1998), it has not been studied extensively in a computational setting. Another similar thread of research is to identify a speaker’s certainty, that is, the confidence of a speaker and how that self-evaluation affects their language (Pon-Barry and Shieber, 2010). Substantial work has gone into automatically identifying levels of speaker certainty, for example in Liscombe et al. (2005) and Litman et al. (2009). The major difference between our work and this body of literature is that work on certainty has rarely focused on how state translates into interaction between speakers (with some exceptions, such as the application of certainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to describe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contri</context>
</contexts>
<marker>Liscombe, Hirschberg, Venditti, 2005</marker>
<rawString>Jackson Liscombe, Julia Hirschberg, and Jennifer Venditti. 2005. Detecting Certainness in Spoken Tutorial Dialogues. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Kate Forbes-Riley</author>
</authors>
<date>2006</date>
<booktitle>Correlations betweeen Dialogue Acts and Learning in Spoken Tutoring Dialogue. In Natural Language Engineering.</booktitle>
<contexts>
<context position="8611" citStr="Litman and Forbes-Riley, 2006" startWordPosition="1357" endWordPosition="1360">nto interaction between speakers (with some exceptions, such as the application of certainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to describe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contribution based on its function (Stolcke et al., 2000). We know that there are interesting correlations between these acts and other factors, such as learning gains (Litman and Forbes-Riley, 2006) and the relevance of a contribution for summarization (Wrede and Shriberg, 2003). However, adapting dialogue act tags to the question of how speakers position themselves is not straightforward. In particular, the granularity of these tagsets, which is already a highly debated topic (Popescu-Belis, 2008), is not ideal for the task we have set for ourselves. Many dialogue acts can be used in authoritative or nonauthoritative ways, based on context, and can position a speaker as either giver or receiver of information. Thus these more general tagsets are not specific enough to the role of author</context>
</contexts>
<marker>Litman, Forbes-Riley, 2006</marker>
<rawString>Diane Litman and Kate Forbes-Riley. 2006. Correlations betweeen Dialogue Acts and Learning in Spoken Tutoring Dialogue. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Mihai Rotaru</author>
<author>Greg Nicholas</author>
</authors>
<title>Classifying Turn-Level Uncertainty Using Word-Level Prosody.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="7843" citStr="Litman et al. (2009)" startWordPosition="1236" endWordPosition="1239">ority, treating the other speakers as a source for that knowledge. While there have been preliminary attempts to separate out these specific types of positioning in initiative, such as Chu-Carroll and Brown (1998), it has not been studied extensively in a computational setting. Another similar thread of research is to identify a speaker’s certainty, that is, the confidence of a speaker and how that self-evaluation affects their language (Pon-Barry and Shieber, 2010). Substantial work has gone into automatically identifying levels of speaker certainty, for example in Liscombe et al. (2005) and Litman et al. (2009). The major difference between our work and this body of literature is that work on certainty has rarely focused on how state translates into interaction between speakers (with some exceptions, such as the application of certainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to describe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contribution based on its funct</context>
</contexts>
<marker>Litman, Rotaru, Nicholas, 2009</marker>
<rawString>Diane Litman, Mihai Rotaru, and Greg Nicholas. 2009. Classifying Turn-Level Uncertainty Using Word-Level Prosody. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Martin</author>
</authors>
<title>English Text: System</title>
<date>1992</date>
<contexts>
<context position="9831" citStr="Martin, 1992" startWordPosition="1560" endWordPosition="1561">iscourse. Each of these fields of prior work is highly valuable. However, none were designed to specifically describe how people present themselves as a source or recipient of knowledge in a discourse. Thus, we have chosen to draw on a different field of sociolinguistics. Our formalization of that theory is described in the next section. 3 The Negotiation Framework We now present the Negotiation framework2, which we use to answer the questions left unanswered in the previous section. Within the field of SFL, this framework has been continually refined over the last three decades (Berry, 1981; Martin, 1992; Martin, 2003). It attempts to describe how speakers use their role as a source of knowledge or action to position themselves relative to others in a discourse. Applications of the framework include distinguishing between focus on teacher knowledge and student reasoning (Veel, 1999) and distribution of authority in juvenile trials (Martin et al., 2008). The framework can also be applied to problems similar to those studied through the lens of initiative, such as the distinction between authority over discourse structure and authority over content (Martin, 2000). A challenge of applying this w</context>
</contexts>
<marker>Martin, 1992</marker>
<rawString>James Martin. 1992. English Text: System and Structure. James Martin. 2000. Factoring out Exchange: Types of Structure. In Working with Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Martin</author>
<author>David Rose</author>
</authors>
<title>Working with Discourse: Meaning Beyond the Clause.</title>
<date>2003</date>
<marker>Martin, Rose, 2003</marker>
<rawString>James Martin and David Rose. 2003. Working with Discourse: Meaning Beyond the Clause.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Martin</author>
<author>Michele Zappavigna</author>
<author>Paul Dwyer</author>
</authors>
<title>Negotiating Shame: Exchange and Genre Structure in Youth Justice Conferencing.</title>
<date>2008</date>
<booktitle>In Proceedings of European Systemic Functional Linguistics.</booktitle>
<contexts>
<context position="10186" citStr="Martin et al., 2008" startWordPosition="1613" endWordPosition="1616">egotiation Framework We now present the Negotiation framework2, which we use to answer the questions left unanswered in the previous section. Within the field of SFL, this framework has been continually refined over the last three decades (Berry, 1981; Martin, 1992; Martin, 2003). It attempts to describe how speakers use their role as a source of knowledge or action to position themselves relative to others in a discourse. Applications of the framework include distinguishing between focus on teacher knowledge and student reasoning (Veel, 1999) and distribution of authority in juvenile trials (Martin et al., 2008). The framework can also be applied to problems similar to those studied through the lens of initiative, such as the distinction between authority over discourse structure and authority over content (Martin, 2000). A challenge of applying this work to language technologies is that it has historically been highly 2All examples are drawn from the MapTask corpus and involve an instruction giver (g) and follower (f). Within examples, discourse segment boundaries are shown by horizontal lines. qualitative, with little emphasis placed on reproducibility. We have formulated a pared-down, reproducible</context>
</contexts>
<marker>Martin, Zappavigna, Dwyer, 2008</marker>
<rawString>James Martin, Michele Zappavigna, and Paul Dwyer. 2008. Negotiating Shame: Exchange and Genre Structure in Youth Justice Conferencing. In Proceedings of European Systemic Functional Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elijah Mayfield</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>An Interactive Tool for Supporting Error Analysis for Text Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of Demo Session at NAACL.</booktitle>
<marker>Mayfield, Ros´e, 2010</marker>
<rawString>Elijah Mayfield and Carolyn Penstein Ros´e. 2010. An Interactive Tool for Supporting Error Analysis for Text Mining. In Proceedings of Demo Session at NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Peltason</author>
<author>Britta Wrede</author>
</authors>
<title>Modeling Human-Robot Interaction Based on Generic Interaction Patterns.</title>
<date>2010</date>
<booktitle>In AAAI Report on Dialog with Robots.</booktitle>
<contexts>
<context position="6477" citStr="Peltason and Wrede, 2010" startWordPosition="1020" endWordPosition="1023">tive passes back and forth between discourse participants, control over the con1We treat each line in our corpus as a single contribution. versation similarly transfers from one speaker to another (Walker and Whittaker, 1990). This relation is often considered synchronous, though evidence suggests that the reality is not straightforward (Jordan and Di Eugenio, 1997). Research in initiative and control has been applied in the form of mixed-initiative dialogue systems (Smith, 1992). This is a large and active field, with applications in tutorial dialogues (Core, 2003), human-robot interactions (Peltason and Wrede, 2010), and more general approaches to effective turn-taking (Selfridge and Heeman, 2010). However, that body of work focuses on influencing discourse structure through positioning. The question that we are asking instead focuses on how speakers view their authority as a source of information about the topic of the discourse. In particular, consider questioning in discourse. In mixed-initiative analysis of discourse, asking a question always gives you control of a discourse. There is an expectation that your question will be followed by an answer. A speaker might already know the answer to a questio</context>
</contexts>
<marker>Peltason, Wrede, 2010</marker>
<rawString>Julia Peltason and Britta Wrede. 2010. Modeling Human-Robot Interaction Based on Generic Interaction Patterns. In AAAI Report on Dialog with Robots.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<booktitle>In Computational Linguistics.</booktitle>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Pon-Barry</author>
<author>Stuart Shieber</author>
</authors>
<title>Assessing Self-awareness and Transparency when Classifying a Speakers Level of Certainty. In Speech Prosody.</title>
<date>2010</date>
<contexts>
<context position="7693" citStr="Pon-Barry and Shieber, 2010" startWordPosition="1212" endWordPosition="1215">to a question they asked - for instance, when a teacher is verifying a student’s knowledge. However, in most cases asking a question represents a lack of authority, treating the other speakers as a source for that knowledge. While there have been preliminary attempts to separate out these specific types of positioning in initiative, such as Chu-Carroll and Brown (1998), it has not been studied extensively in a computational setting. Another similar thread of research is to identify a speaker’s certainty, that is, the confidence of a speaker and how that self-evaluation affects their language (Pon-Barry and Shieber, 2010). Substantial work has gone into automatically identifying levels of speaker certainty, for example in Liscombe et al. (2005) and Litman et al. (2009). The major difference between our work and this body of literature is that work on certainty has rarely focused on how state translates into interaction between speakers (with some exceptions, such as the application of certainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to </context>
</contexts>
<marker>Pon-Barry, Shieber, 2010</marker>
<rawString>Heather Pon-Barry and Stuart Shieber. 2010. Assessing Self-awareness and Transparency when Classifying a Speakers Level of Certainty. In Speech Prosody.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Dimensionality of Dialogue Act Tagsets: An Empirical Analysis of Large Corpora. In Language Resources and Evaluation.</title>
<date>2008</date>
<contexts>
<context position="8916" citStr="Popescu-Belis, 2008" startWordPosition="1404" endWordPosition="1405">cribe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contribution based on its function (Stolcke et al., 2000). We know that there are interesting correlations between these acts and other factors, such as learning gains (Litman and Forbes-Riley, 2006) and the relevance of a contribution for summarization (Wrede and Shriberg, 2003). However, adapting dialogue act tags to the question of how speakers position themselves is not straightforward. In particular, the granularity of these tagsets, which is already a highly debated topic (Popescu-Belis, 2008), is not ideal for the task we have set for ourselves. Many dialogue acts can be used in authoritative or nonauthoritative ways, based on context, and can position a speaker as either giver or receiver of information. Thus these more general tagsets are not specific enough to the role of authority in discourse. Each of these fields of prior work is highly valuable. However, none were designed to specifically describe how people present themselves as a source or recipient of knowledge in a discourse. Thus, we have chosen to draw on a different field of sociolinguistics. Our formalization of tha</context>
</contexts>
<marker>Popescu-Belis, 2008</marker>
<rawString>Andrei Popescu-Belis. 2008. Dimensionality of Dialogue Act Tagsets: An Empirical Analysis of Large Corpora. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Rizzolo</author>
<author>Dan Roth</author>
</authors>
<title>Learning Based Java for Rapid Development of NLP Systems.</title>
<date>2010</date>
<booktitle>In Language Resources and Evaluation.</booktitle>
<contexts>
<context position="26411" citStr="Rizzolo and Roth, 2010" startWordPosition="4400" endWordPosition="4403">metric is the most important for measuring reproducibility of human analyses of speaker authority in conversation. We use SIDE for feature extraction (Mayfield and Ros´e, 2010), SVM-Light for machine learning Model Accuracy Kappa r2 Baseline 59.7% 0.465 0.354 Baseline+ILP 61.6% 0.488 0.663 Segmentation 72.3% Contextual 66.7% 0.565 0.908 Contextual+ILP 68.4% 0.584 0.947 Segmentation 74.9% Table 2: Performance evaluation for our models. Each line is significantly improved in both accuracy and r2 error from the previous line (p &lt; .01). (Joachims, 1999), and Learning-Based Java for ILP inference (Rizzolo and Roth, 2010). Performance is evaluated by 20-fold cross-validation, where each fold is trained on 19 conversations and tested on the remaining one. Statistical significance was calculated using a student’s paired t-test. For accuracy and kappa, n = 20 (one data point per conversation) and for r2, n = 40 (one data point per speaker). 5.1 Results All classification results are given in Table 2 and charts showing correlation between predicted and actual speaker Authoritativeness ratios are shown in Figure 1. We observe that the baseline bag-of-words model performs well above random chance (kappa of 0.465); h</context>
</contexts>
<marker>Rizzolo, Roth, 2010</marker>
<rawString>Nick Rizzolo and Dan Roth. 2010. Learning Based Java for Rapid Development of NLP Systems. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-Tau Yih</author>
</authors>
<title>A Linear Programming Formulation for Global Inference in Natural Language Tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="4182" citStr="Roth and Yih, 2004" startWordPosition="645" endWordPosition="648">f the Association for Computational Linguistics, pages 1018–1026, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics dimensional classification task. One dimension is a set of codes describing the authoritative status of a contribution1. The other dimension is a segmentation task. We impose constraints on both of these models based on the structure observed in the work of SFL. These constraints are formulated as boolean statements describing what a correct label sequence looks like, and are imposed on our model using an Integer Linear Programming formulation (Roth and Yih, 2004). In section 5, this model is evaluated on a subset of the MapTask corpus (Anderson et al., 1991) and shows a high correlation with human judgements of authoritativeness (r2 = 0.947). After a detailed error analysis, we will conclude the paper in section 6 with a discussion of our future work. 2 Background The Negotiation framework, as formulated by the SFL community, places a special emphasis on how speakers function in a discourse as sources or recipients of information or action. We break down this concept into a set of codes, one code per contribution. Before we break down the coding schem</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-Tau Yih. 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel Schegloff</author>
</authors>
<title>Sequence Organization in Interaction: A Primer in Conversation Analysis.</title>
<date>2007</date>
<contexts>
<context position="5373" citStr="Schegloff, 2007" startWordPosition="846" endWordPosition="847">eak down the coding scheme more concretely in section 3, it is important to understand why we have chosen to introduce a new framework, rather than reusing existing computational work. Much work has examined the emergence of discourse structure from the choices speakers make at the linguistic and intentional level (Grosz and Sidner, 1986). For instance, when a speaker asks a question, it is expected to be followed with an answer. In discourse analysis, this notion is described through dialogue games (Carlson, 1983), while conversation analysis frames the structure in terms of adjacency pairs (Schegloff, 2007). These expectations can be viewed under the umbrella of conditional relevance (Levinson, 2000), and the exchanges can be labelled discourse segments. In prior work, the way that people influence discourse structure is described through the two tightlyrelated concepts of initiative and control. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988). As initiative passes back and forth between discourse participants, control over the con1We treat each line in our corpus as a sin</context>
</contexts>
<marker>Schegloff, 2007</marker>
<rawString>Emanuel Schegloff. 2007. Sequence Organization in Interaction: A Primer in Conversation Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ethan Selfridge</author>
<author>Peter Heeman</author>
</authors>
<title>ImportanceDriven Turn-Bidding for Spoken Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6560" citStr="Selfridge and Heeman, 2010" startWordPosition="1031" endWordPosition="1034">e treat each line in our corpus as a single contribution. versation similarly transfers from one speaker to another (Walker and Whittaker, 1990). This relation is often considered synchronous, though evidence suggests that the reality is not straightforward (Jordan and Di Eugenio, 1997). Research in initiative and control has been applied in the form of mixed-initiative dialogue systems (Smith, 1992). This is a large and active field, with applications in tutorial dialogues (Core, 2003), human-robot interactions (Peltason and Wrede, 2010), and more general approaches to effective turn-taking (Selfridge and Heeman, 2010). However, that body of work focuses on influencing discourse structure through positioning. The question that we are asking instead focuses on how speakers view their authority as a source of information about the topic of the discourse. In particular, consider questioning in discourse. In mixed-initiative analysis of discourse, asking a question always gives you control of a discourse. There is an expectation that your question will be followed by an answer. A speaker might already know the answer to a question they asked - for instance, when a teacher is verifying a student’s knowledge. How</context>
</contexts>
<marker>Selfridge, Heeman, 2010</marker>
<rawString>Ethan Selfridge and Peter Heeman. 2010. ImportanceDriven Turn-Bidding for Spoken Dialogue Systems. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronnie Smith</author>
</authors>
<title>A computational model of expectation-driven mixed-initiative dialog processing.</title>
<date>1992</date>
<tech>Ph.D. Dissertation.</tech>
<contexts>
<context position="6336" citStr="Smith, 1992" startWordPosition="1001" endWordPosition="1002">e initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988). As initiative passes back and forth between discourse participants, control over the con1We treat each line in our corpus as a single contribution. versation similarly transfers from one speaker to another (Walker and Whittaker, 1990). This relation is often considered synchronous, though evidence suggests that the reality is not straightforward (Jordan and Di Eugenio, 1997). Research in initiative and control has been applied in the form of mixed-initiative dialogue systems (Smith, 1992). This is a large and active field, with applications in tutorial dialogues (Core, 2003), human-robot interactions (Peltason and Wrede, 2010), and more general approaches to effective turn-taking (Selfridge and Heeman, 2010). However, that body of work focuses on influencing discourse structure through positioning. The question that we are asking instead focuses on how speakers view their authority as a source of information about the topic of the discourse. In particular, consider questioning in discourse. In mixed-initiative analysis of discourse, asking a question always gives you control o</context>
</contexts>
<marker>Smith, 1992</marker>
<rawString>Ronnie Smith. 1992. A computational model of expectation-driven mixed-initiative dialog processing. Ph.D. Dissertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. In Computational Linguistics.</title>
<date>2000</date>
<contexts>
<context position="1940" citStr="Stolcke et al., 2000" startWordPosition="291" endWordPosition="294">ence or give evidence of their relations to each other. Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse’s structure, fall under the umbrella of positioning. As we construe positioning, it also includes work on detecting certainty and confusion in speech (Liscombe et al., 2005), which models a speaker’s understanding of the information in their statements. Work in dialogue act tagging is also relevant, as it seeks to describe the ac1018 tions and moves with which speakers display these types of positioning (Stolcke et al., 2000). To complement these bodies of work, we choose to focus on the question of how speakers position themselves as authoritative in a discourse. This means that we must describe the way speakers introduce new topics or discussions into the discourse; the way they position themselves relative to that topic; and how these functions interact with each other. While all of the tasks mentioned above focus on specific problems in the larger rhetorical question of speaker positioning, none explicitly address this framing of authority. Each does have valuable ties to the work that we would like to do, and</context>
<context position="8469" citStr="Stolcke et al., 2000" startWordPosition="1335" endWordPosition="1338"> major difference between our work and this body of literature is that work on certainty has rarely focused on how state translates into interaction between speakers (with some exceptions, such as the application of certainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to describe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contribution based on its function (Stolcke et al., 2000). We know that there are interesting correlations between these acts and other factors, such as learning gains (Litman and Forbes-Riley, 2006) and the relevance of a contribution for summarization (Wrede and Shriberg, 2003). However, adapting dialogue act tags to the question of how speakers position themselves is not straightforward. In particular, the granularity of these tagsets, which is already a highly debated topic (Popescu-Belis, 2008), is not ideal for the task we have set for ourselves. Many dialogue acts can be used in authoritative or nonauthoritative ways, based on context, and ca</context>
</contexts>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, et al. 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Veel</author>
</authors>
<title>Language, Knowledge, and Authority in School Mathematics.</title>
<date>1999</date>
<booktitle>In Pedagogy and the Shaping of Consciousness: Linguistics and Social Processes</booktitle>
<contexts>
<context position="10115" citStr="Veel, 1999" startWordPosition="1604" endWordPosition="1605">ation of that theory is described in the next section. 3 The Negotiation Framework We now present the Negotiation framework2, which we use to answer the questions left unanswered in the previous section. Within the field of SFL, this framework has been continually refined over the last three decades (Berry, 1981; Martin, 1992; Martin, 2003). It attempts to describe how speakers use their role as a source of knowledge or action to position themselves relative to others in a discourse. Applications of the framework include distinguishing between focus on teacher knowledge and student reasoning (Veel, 1999) and distribution of authority in juvenile trials (Martin et al., 2008). The framework can also be applied to problems similar to those studied through the lens of initiative, such as the distinction between authority over discourse structure and authority over content (Martin, 2000). A challenge of applying this work to language technologies is that it has historically been highly 2All examples are drawn from the MapTask corpus and involve an instruction giver (g) and follower (f). Within examples, discourse segment boundaries are shown by horizontal lines. qualitative, with little emphasis p</context>
</contexts>
<marker>Veel, 1999</marker>
<rawString>Robert Veel. 1999. Language, Knowledge, and Authority in School Mathematics. In Pedagogy and the Shaping of Consciousness: Linguistics and Social Processes</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
</authors>
<title>Mixed Initiative in Dialogue: An Investigation into Discourse Structure.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6077" citStr="Walker and Whittaker, 1990" startWordPosition="959" endWordPosition="962">e (Levinson, 2000), and the exchanges can be labelled discourse segments. In prior work, the way that people influence discourse structure is described through the two tightlyrelated concepts of initiative and control. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988). As initiative passes back and forth between discourse participants, control over the con1We treat each line in our corpus as a single contribution. versation similarly transfers from one speaker to another (Walker and Whittaker, 1990). This relation is often considered synchronous, though evidence suggests that the reality is not straightforward (Jordan and Di Eugenio, 1997). Research in initiative and control has been applied in the form of mixed-initiative dialogue systems (Smith, 1992). This is a large and active field, with applications in tutorial dialogues (Core, 2003), human-robot interactions (Peltason and Wrede, 2010), and more general approaches to effective turn-taking (Selfridge and Heeman, 2010). However, that body of work focuses on influencing discourse structure through positioning. The question that we are</context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>Marilyn Walker and Steve Whittaker. 1990. Mixed Initiative in Dialogue: An Investigation into Discourse Structure. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Whittaker</author>
<author>Phil Stenton</author>
</authors>
<title>Cues and Control in Expert-Client Dialogues.</title>
<date>1988</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1446" citStr="Whittaker and Stenton, 1988" startWordPosition="212" endWordPosition="215">ined model’s analyses of speaker authority correlates very strongly with expert human judgments (r2 coefficient of 0.947). 1 Introduction In this work, we seek to formalize the ways speakers position themselves in discourse. We do this in a way that maintains a notion of discourse structure, and which can be aggregated to evaluate a speaker’s overall stance in a dialogue. We define the body of work in positioning to include any attempt to formalize the processes by which speakers attempt to influence or give evidence of their relations to each other. Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse’s structure, fall under the umbrella of positioning. As we construe positioning, it also includes work on detecting certainty and confusion in speech (Liscombe et al., 2005), which models a speaker’s understanding of the information in their statements. Work in dialogue act tagging is also relevant, as it seeks to describe the ac1018 tions and moves with which speakers display these types of positioning (Stolcke et al., 2000). To complement these bodies of work, we choose to focus on the question of how speakers position themselv</context>
<context position="5841" citStr="Whittaker and Stenton, 1988" startWordPosition="920" endWordPosition="924"> this notion is described through dialogue games (Carlson, 1983), while conversation analysis frames the structure in terms of adjacency pairs (Schegloff, 2007). These expectations can be viewed under the umbrella of conditional relevance (Levinson, 2000), and the exchanges can be labelled discourse segments. In prior work, the way that people influence discourse structure is described through the two tightlyrelated concepts of initiative and control. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988). As initiative passes back and forth between discourse participants, control over the con1We treat each line in our corpus as a single contribution. versation similarly transfers from one speaker to another (Walker and Whittaker, 1990). This relation is often considered synchronous, though evidence suggests that the reality is not straightforward (Jordan and Di Eugenio, 1997). Research in initiative and control has been applied in the form of mixed-initiative dialogue systems (Smith, 1992). This is a large and active field, with applications in tutorial dialogues (Core, 2003), human-robot int</context>
</contexts>
<marker>Whittaker, Stenton, 1988</marker>
<rawString>Steve Whittaker and Phil Stenton. 1988. Cues and Control in Expert-Client Dialogues. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Wrede</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>The Relationship between Dialogue Acts and Hot Spots in Meetings.</title>
<date>2003</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="8692" citStr="Wrede and Shriberg, 2003" startWordPosition="1370" endWordPosition="1373">tainty to tutoring dialogues (Forbes-Riley and Litman, 2009)). Instead, the focus is on the person’s self-evaluation, independent of the influence on the speaker’s positioning within a discourse. Dialogue act tagging seeks to describe the moves people make to express themselves in a discourse. 1019 This task involves defining the role of each contribution based on its function (Stolcke et al., 2000). We know that there are interesting correlations between these acts and other factors, such as learning gains (Litman and Forbes-Riley, 2006) and the relevance of a contribution for summarization (Wrede and Shriberg, 2003). However, adapting dialogue act tags to the question of how speakers position themselves is not straightforward. In particular, the granularity of these tagsets, which is already a highly debated topic (Popescu-Belis, 2008), is not ideal for the task we have set for ourselves. Many dialogue acts can be used in authoritative or nonauthoritative ways, based on context, and can position a speaker as either giver or receiver of information. Thus these more general tagsets are not specific enough to the role of authority in discourse. Each of these fields of prior work is highly valuable. However,</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>Britta Wrede and Elizabeth Shriberg. 2003. The Relationship between Dialogue Acts and Hot Spots in Meetings. In IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>