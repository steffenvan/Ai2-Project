<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028176">
<title confidence="0.964743">
Knowledge-based Automatic Topic Identification
</title>
<author confidence="0.92924">
Chin-Yew Lin
</author>
<affiliation confidence="0.9765625">
Department of Electrical Engineering/System
University of Southern California
</affiliation>
<address confidence="0.902817">
Los Angeles, CA 90089-2562, USA
</address>
<email confidence="0.998317">
chinyew@pollux.usc.edu
</email>
<sectionHeader confidence="0.996946" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966307692308">
As the first step in an automated text sum-
marization algorithm, this work presents
a new method for automatically identi-
fying the central ideas in a text based
on a knowledge-based concept counting
paradigm. To represent and generalize
concepts, we use the hierarchical concept
taxonomy WordNet. By setting appropri-
ate cutoff values for such parameters as
concept generality and child-to-parent fre-
quency ratio, we control the amount and
level of generality of concepts extracted
from the text.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983428571429">
As the amount of text available online keeps grow-
ing, it becomes increasingly difficult for people to
keep track of and locate the information of inter-
est to them. To remedy the problem of information
overload, a robust and automated text summarizer
or information extrator is needed. Topic identifica-
tion is one of two very important steps in the process
of summarizing a text; the second step is summary
text generation.
A topic is a particular subject that we write about
or discuss. (Sinclair et al., 1987). To identify
the topics of texts, Information Retrieval (IR) re-
searchers use word frequency, cue word, location,
and title-keyword techniques (Paice, 1990). Among
these techniques, only word frequency counting can
be used robustly across different domains; the other
techniques rely on stereotypical text structure or the
functional structures of specific domains.
Underlying the use of word frequency is the as-
sumption that the more a word is used in a text,
the more important it is in that text. This method
</bodyText>
<footnote confidence="0.92183275">
&apos;This research was funded in part by ARPA under or-
der number 8073, issued as Maryland Procurement Con-
tract # MDA904-91-C-5224 and in part by the National
Science Foundation Grant No. MIP 8902426.
</footnote>
<bodyText confidence="0.99970247826087">
recognizes only the literal word forms and noth-
ing else. Some morphological processing may help,
but pronominalization and other forms of coreferen-
tiality defeat simple word counting. Furthermore,
straightforward word counting can be misleading
since it misses conceptual generalizations. For exam-
ple: &amp;quot;John bought some vegetables, fruit, bread, and
milk.&amp;quot; What would be the topic of this sentence?
We can draw no conclusion by using word counting
method; where the topic actually should be: &amp;quot;John
bought some groceries.&amp;quot; The problem is that word
counting method misses the important concepts be-
hind those words: vegetables, fruit, etc. relates to
groceries at the deeper level of semantics. In rec-
ognizing the inherent problem of the word counting
method, recently people have started to use artifi-
cial intelligence techniques (Jacobs and Rau, 1990;
Mauldin, 1991) and statistical techniques (Salton
et al., 1994; Grefenstette, 1994) to incorporate the
sementic relations among words into their applica-
tions. Following this trend, we have developed a new
way to identify topics by counting concepts instead
of words.
</bodyText>
<sectionHeader confidence="0.936533" genericHeader="method">
2 The Power of Generalization
</sectionHeader>
<bodyText confidence="0.999604055555556">
In order to count concept frequency, we employ a
concept generalization taxonomy. Figure 1 shows a
possible hierarchy for the concept digital computer.
According to this hierarchy, if we find laptop and
hand-held computer, in a text, we can infer that the
text is about portable computers, which is their par-
ent concept. And if in addition, the text also men-
tions workstation and mainframe, it is reasonable to
say that the topic of the text is related to digital
computer.
Using a hierarchy, the question is now how to find
the most appropriate generalization. Clearly we can-
not just use the leaf concepts — since at this level we
have gained no power from generalization. On the
other hand, neither can we use the very top concept
— everything is a thing. We need a method of iden-
tifying the most appropriate concepts somewhere in
middle of the taxonomy. Our current solution uses
</bodyText>
<page confidence="0.978286">
308
</page>
<figure confidence="0.95761275">
Digital computer (a) ratio It = 0.70 Computer Company(10)
Personal computer
PC
Microcomputer
Workstation Minicomputer
Mainframe
Portable computer Desktop computer Toshiba(0) NEC(1) Compaq(1) Apple(7) IBM(1)
Hand-held computer Laptop computer (b) ratio R = 0.30 Computer Company(10)
</figure>
<figureCaption confidence="0.999722">
Figure 1: A sample hierarchy for computer
</figureCaption>
<bodyText confidence="0.79088">
concept frequency ratio and starting depth.
</bodyText>
<subsectionHeader confidence="0.996595">
2.1 Branch Ratio Threshold
</subsectionHeader>
<bodyText confidence="0.998269041666667">
We call the frequency of occurrence of a concept C
and it&apos;s subconcepts in a text the concept&apos;s weight&apos;.
We then define the ratio R.,at any concept C, as fol-
lows:
= MAX(weight of all the direct children of C)
SUM(weight of all the direct children of C)
R is a way to identify the degree of summarization
informativeness. The higher the ratio, the less con-
cept C generalizes over many children, i.e., the more
it reflects only one child. Consider Figure 2. In case
(a) the parent concept&apos;s ratio is 0.70, and in case (b),
it is 0.3 by the definition of R. To generate a sum-
mary for case (a), we should simply choose Apple
as the main idea instead of its parent concept, since
it is by far the most mentioned. In contrast, in case
(b), we should use the parent concept Computer
Company as the concept of interest. Its small ra-
tio, 0.30, tells us that if we go down to its children,
we will lose too much important information. We
define the branch ratio threshold (R4) to serve as a
cutoff point for the determination of interestingness,
i.e., the degree of generalization. We define that if a
concept&apos;s ratio R is less than 711 it is an interesting
concept.
</bodyText>
<subsectionHeader confidence="0.999806">
2.2 Starting Depth
</subsectionHeader>
<bodyText confidence="0.986799058823529">
We can use the ratio to find all the possible inter-
esting concepts in a hierarchical concept taxonomy.
If we start from the top of a hierarchy and pro-
ceed downward along each child branch whenever
the branch ratio is greater than or equal to 724, we
will eventually stop with a list of interesting con-
cepts. We call these interesting concepts the inter-
esting wavefront. We can start another exploration
of interesting concepts downward from this interest-
ing wavefront resulting in a second, lower, wavefront,
and so on. By repeating this process until we reach
the leaf concepts of the hierarchy, we can get a set
of interesting wavefronts. Among these interesting
&apos;According to this, a parent concept always has
weight greater or equal to its maximum weighted direct
children. A concept itself is considered as its own direct
child.
</bodyText>
<equation confidence="0.788547">
Toshiba(2) NEC(2) Compaq(3) Apple(2) IBM(1)
</equation>
<figureCaption confidence="0.999184">
Figure 2: Ratio and degree of generalization
</figureCaption>
<bodyText confidence="0.9999762">
wavefronts, which one is the most appropriate for
generation of topics? It is obvious that using the
concept counting technique we have suggested so
far, a concept higher in the hierarchy tends to be
more general. On the other hand, a concept lower
in the hierarchy tends to be more specific. In order
to choose an adequate wavefront with appropriate
generalization, we introduce the parameter starting
depth, V. We require that the branch ratio criterion
defined in the previous section can only take effect
after the wavefront exceeds the starting depth; the
first subsequent interesting wavefront generated will
be our collection of topic concepts. The appropri-
ate D, is determined by experimenting with different
values and choosing the best one.
</bodyText>
<sectionHeader confidence="0.998079" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.999915272727273">
We have implemented a prototype system to test
the automatic topic identification algorithm. As the
concept hierarchy, we used the noun taxonomy from
WordNet3 (Miller et al., 1990). WordNet has been
used for other similar tasks, such as (Resnik, 1993)
For input texts, we selected articles about informa-
tion processing of average 750 words each out of
Business Week (93-94). We ran the algorithm on
50 texts, and for each text extracted eight sentences
containing the most interesting concepts.
How now to evaluate the results? For each text,
we obtained a professional&apos;s abstract from an online
service. Each abstract contains 7 to 8 sentences on
average. In order to compare the system&apos;s selection
with the professional&apos;s, we identified in the text the
sentences that contain the main concepts mentioned
in the professional&apos;s abstract. We scored how many
sentences were selected by both the system and the
professional abstractor. We are aware that this eval-
uation scheme is not very accurate, but it serves as
a rough indicator for our initital investigation.
We developed three variations to score the text
</bodyText>
<footnote confidence="0.993745">
3WordNet is a concept taxnonmy which consists of
synonym sets instead of individual words
</footnote>
<page confidence="0.998517">
309
</page>
<bodyText confidence="0.959451">
sentences on weights of the concepts in the interest-
ing wavefront.
</bodyText>
<listItem confidence="0.969769">
1. the weight of a sentence is equal to the sum
of weights of parent concepts of words in the
sentence.
2. the weight of a sentence is the sum of weights
of words in the sentence.
3. similar to one, but counts only one concept in-
stance per sentence.
</listItem>
<bodyText confidence="0.943505222222222">
To evaluate the system&apos;s performance, we defined
three counts: (1) hits, sentences identified by the
algorithm and referenced by the professional&apos;s ab-
stract; (2) mistakes, sentences identified by the al-
gorithm but not referenced by the professional&apos;s ab-
stract; (3) misses, sentences in the professional&apos;s ab-
stract not identified by the algorithm. We then bor-
rowed two measures from Information Retrieval re-
search:
</bodyText>
<figure confidence="0.3870825">
Recall: hits /(hits + misses)
Precision: hits l(hits + mistakes)
</figure>
<bodyText confidence="0.9999565">
The closer these two measures are to unity, the bet-
ter the algorithm&apos;s performance. The precision mea-
sure plays a central role in the text summarization
problem: the higher the precision score, the higher
probability that the algorithm would identify the
true topics of a text. We also implemented a simple
plain word counting algorithm and a random selec-
tion algorithm for comparision.
The average result of 50 input texts with branch
ratio threshold4 0.68 and starting depth 6. The aver-
age scores&apos; for the three sentence scoring variations
are 0.32 recall and 0.35 precision when the system
produces extracts of 8 sentences; while the random
selection method has 0.18 recall and 0.22 precision
in the same experimental setting and the plain word
counting method has 0.23 recall and 0.28 precision.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999924777777778">
The system achieves its current performance without
using linguistic tools such as a part-of-speech tag-
ger, syntactic parser, pronoun resoultion algorithm,
or discourse analyzer. Hence we feel that the con-
cept counting paradigm is a robust method which
can serve as a basis upon which to build an au-
tomated text summarization system. The current
system draws a performance lower bound for future
systems.
</bodyText>
<footnote confidence="0.615730666666667">
4This threshold and the starting depth are deter-
mined by running the system through different parame-
ter setting. We test ratio = 0.95,0.68,0.45,0.25 and depth
</footnote>
<bodyText confidence="0.963013535714286">
= 3,6,9,12. Among them, 12t = 0.68 and D, = 6 give
the best result.
&apos;The recall (R) and precision (P) for the three varia-
tions are: varl(R=0.32,P=0.37), var2(R=0.30,P=0.34),
and var3(R=0.28,P=0.33) when the system picks 8
sentences.
We have not yet been able to compare the perfor-
mance of our system against IR and commerically
available extraction packages, but since they do not
employ concept counting, we feel that our method
can make a significant contribution.
We plan to improve the system&apos;s extraction re-
sults by incorporating linguistic tools. Our next
goal is generating a summary instead of just extract-
ing sentences. Using a part-of-speech tagger and
syntatic parser to distinguish different syntatic cat-
egories and relations among concepts; we can find
appropriate concept types on the interesting wave-
front, and compose them into summary. For exam-
ple, if a noun concept is selected, we can find its
accompanying verb; if verb is selected, we find its
subject noun. For a set of selected concepts, we then
generalize their matching concepts using the taxon-
omy and generate the list of {selected concepts +
matching generalization) pairs as English sentences.
There are other possibilities. With a robust work-
ing prototype system in hand, we are encouraged to
look for new interesting results.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995339">
Gregory Grefenstette. 1994. Explorations in Au-
tomatic Thesaurus Discovery. Kluwer Academic
Publishers, Boston.
Paul S. Jacobs and Lisa F. Rau. 1990. SCISOR:
Extracting information from on-line news. Com-
munication of the ACM, 33(11):88-97, November.
Michael L. Mauldin. 1991. Conceptual Information
Retrieval — A Case Study in Adaptive Partial
Parsing. Kluwer Academic Publishers, Boston.
George Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Five papers on wordnet. CSL Report 43, Congni-
tive Science Labortory, Princeton University, New
Haven, July.
Chris D. Paice. 1990. Constructing litera-
ture abstracts by computer: Techinques and
prospects. Information Processing and Manage-
ment, 26(1):171-186.
Philip Stuart Resnik. 1993. Selection and Informa-
tion: A Class-Based Approach to Lexical Relation-
ships. Ph.D. thesis, University of Pennsylvania,
University of Pennsylvania.
Gerard Salton, James Allan, Chris Buckley, and
Amit Singhal. 1994. Automatic analysis,
theme generation, and summarization of machine-
readable texts. Science, 264:1421-1426, June.
John Sinclair, Patrick Hanks, Gwyneth Fox,
Rosamuna Moon, and Penny Stock. 1987. Collins
COB UILD English Language Dictionary. William
Collins Sons &amp; Co. Ltd., Glasgow, UK.
</reference>
<page confidence="0.998612">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.971464">
<title confidence="0.999578">Knowledge-based Automatic Topic Identification</title>
<author confidence="0.999513">Chin-Yew Lin</author>
<affiliation confidence="0.9995865">Department of Electrical Engineering/System University of Southern California</affiliation>
<address confidence="0.999742">Los Angeles, CA 90089-2562, USA</address>
<email confidence="0.999741">chinyew@pollux.usc.edu</email>
<abstract confidence="0.997963285714286">As the first step in an automated text summarization algorithm, this work presents a new method for automatically identifying the central ideas in a text based on a knowledge-based concept counting paradigm. To represent and generalize concepts, we use the hierarchical concept taxonomy WordNet. By setting appropriate cutoff values for such parameters as concept generality and child-to-parent frequency ratio, we control the amount and level of generality of concepts extracted from the text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="2864" citStr="Grefenstette, 1994" startWordPosition="440" endWordPosition="441">some vegetables, fruit, bread, and milk.&amp;quot; What would be the topic of this sentence? We can draw no conclusion by using word counting method; where the topic actually should be: &amp;quot;John bought some groceries.&amp;quot; The problem is that word counting method misses the important concepts behind those words: vegetables, fruit, etc. relates to groceries at the deeper level of semantics. In recognizing the inherent problem of the word counting method, recently people have started to use artificial intelligence techniques (Jacobs and Rau, 1990; Mauldin, 1991) and statistical techniques (Salton et al., 1994; Grefenstette, 1994) to incorporate the sementic relations among words into their applications. Following this trend, we have developed a new way to identify topics by counting concepts instead of words. 2 The Power of Generalization In order to count concept frequency, we employ a concept generalization taxonomy. Figure 1 shows a possible hierarchy for the concept digital computer. According to this hierarchy, if we find laptop and hand-held computer, in a text, we can infer that the text is about portable computers, which is their parent concept. And if in addition, the text also mentions workstation and mainfr</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
<author>Lisa F Rau</author>
</authors>
<title>SCISOR: Extracting information from on-line news.</title>
<date>1990</date>
<journal>Communication of the ACM,</journal>
<pages>33--11</pages>
<contexts>
<context position="2779" citStr="Jacobs and Rau, 1990" startWordPosition="427" endWordPosition="430">n be misleading since it misses conceptual generalizations. For example: &amp;quot;John bought some vegetables, fruit, bread, and milk.&amp;quot; What would be the topic of this sentence? We can draw no conclusion by using word counting method; where the topic actually should be: &amp;quot;John bought some groceries.&amp;quot; The problem is that word counting method misses the important concepts behind those words: vegetables, fruit, etc. relates to groceries at the deeper level of semantics. In recognizing the inherent problem of the word counting method, recently people have started to use artificial intelligence techniques (Jacobs and Rau, 1990; Mauldin, 1991) and statistical techniques (Salton et al., 1994; Grefenstette, 1994) to incorporate the sementic relations among words into their applications. Following this trend, we have developed a new way to identify topics by counting concepts instead of words. 2 The Power of Generalization In order to count concept frequency, we employ a concept generalization taxonomy. Figure 1 shows a possible hierarchy for the concept digital computer. According to this hierarchy, if we find laptop and hand-held computer, in a text, we can infer that the text is about portable computers, which is th</context>
</contexts>
<marker>Jacobs, Rau, 1990</marker>
<rawString>Paul S. Jacobs and Lisa F. Rau. 1990. SCISOR: Extracting information from on-line news. Communication of the ACM, 33(11):88-97, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Mauldin</author>
</authors>
<title>Conceptual Information Retrieval — A Case Study in Adaptive Partial Parsing.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="2795" citStr="Mauldin, 1991" startWordPosition="431" endWordPosition="432">it misses conceptual generalizations. For example: &amp;quot;John bought some vegetables, fruit, bread, and milk.&amp;quot; What would be the topic of this sentence? We can draw no conclusion by using word counting method; where the topic actually should be: &amp;quot;John bought some groceries.&amp;quot; The problem is that word counting method misses the important concepts behind those words: vegetables, fruit, etc. relates to groceries at the deeper level of semantics. In recognizing the inherent problem of the word counting method, recently people have started to use artificial intelligence techniques (Jacobs and Rau, 1990; Mauldin, 1991) and statistical techniques (Salton et al., 1994; Grefenstette, 1994) to incorporate the sementic relations among words into their applications. Following this trend, we have developed a new way to identify topics by counting concepts instead of words. 2 The Power of Generalization In order to count concept frequency, we employ a concept generalization taxonomy. Figure 1 shows a possible hierarchy for the concept digital computer. According to this hierarchy, if we find laptop and hand-held computer, in a text, we can infer that the text is about portable computers, which is their parent conce</context>
</contexts>
<marker>Mauldin, 1991</marker>
<rawString>Michael L. Mauldin. 1991. Conceptual Information Retrieval — A Case Study in Adaptive Partial Parsing. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1990</date>
<tech>CSL Report 43,</tech>
<institution>Congnitive Science Labortory, Princeton University,</institution>
<location>New Haven,</location>
<contexts>
<context position="7403" citStr="Miller et al., 1990" startWordPosition="1200" endWordPosition="1203">wavefront with appropriate generalization, we introduce the parameter starting depth, V. We require that the branch ratio criterion defined in the previous section can only take effect after the wavefront exceeds the starting depth; the first subsequent interesting wavefront generated will be our collection of topic concepts. The appropriate D, is determined by experimenting with different values and choosing the best one. 3 Experiment We have implemented a prototype system to test the automatic topic identification algorithm. As the concept hierarchy, we used the noun taxonomy from WordNet3 (Miller et al., 1990). WordNet has been used for other similar tasks, such as (Resnik, 1993) For input texts, we selected articles about information processing of average 750 words each out of Business Week (93-94). We ran the algorithm on 50 texts, and for each text extracted eight sentences containing the most interesting concepts. How now to evaluate the results? For each text, we obtained a professional&apos;s abstract from an online service. Each abstract contains 7 to 8 sentences on average. In order to compare the system&apos;s selection with the professional&apos;s, we identified in the text the sentences that contain th</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Five papers on wordnet. CSL Report 43, Congnitive Science Labortory, Princeton University, New Haven, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<title>Constructing literature abstracts by computer: Techinques and prospects.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>26--1</pages>
<contexts>
<context position="1378" citStr="Paice, 1990" startWordPosition="207" endWordPosition="208">it becomes increasingly difficult for people to keep track of and locate the information of interest to them. To remedy the problem of information overload, a robust and automated text summarizer or information extrator is needed. Topic identification is one of two very important steps in the process of summarizing a text; the second step is summary text generation. A topic is a particular subject that we write about or discuss. (Sinclair et al., 1987). To identify the topics of texts, Information Retrieval (IR) researchers use word frequency, cue word, location, and title-keyword techniques (Paice, 1990). Among these techniques, only word frequency counting can be used robustly across different domains; the other techniques rely on stereotypical text structure or the functional structures of specific domains. Underlying the use of word frequency is the assumption that the more a word is used in a text, the more important it is in that text. This method &apos;This research was funded in part by ARPA under order number 8073, issued as Maryland Procurement Contract # MDA904-91-C-5224 and in part by the National Science Foundation Grant No. MIP 8902426. recognizes only the literal word forms and nothi</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Chris D. Paice. 1990. Constructing literature abstracts by computer: Techinques and prospects. Information Processing and Management, 26(1):171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stuart Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, University of Pennsylvania.</institution>
<contexts>
<context position="7474" citStr="Resnik, 1993" startWordPosition="1214" endWordPosition="1215"> depth, V. We require that the branch ratio criterion defined in the previous section can only take effect after the wavefront exceeds the starting depth; the first subsequent interesting wavefront generated will be our collection of topic concepts. The appropriate D, is determined by experimenting with different values and choosing the best one. 3 Experiment We have implemented a prototype system to test the automatic topic identification algorithm. As the concept hierarchy, we used the noun taxonomy from WordNet3 (Miller et al., 1990). WordNet has been used for other similar tasks, such as (Resnik, 1993) For input texts, we selected articles about information processing of average 750 words each out of Business Week (93-94). We ran the algorithm on 50 texts, and for each text extracted eight sentences containing the most interesting concepts. How now to evaluate the results? For each text, we obtained a professional&apos;s abstract from an online service. Each abstract contains 7 to 8 sentences on average. In order to compare the system&apos;s selection with the professional&apos;s, we identified in the text the sentences that contain the main concepts mentioned in the professional&apos;s abstract. We scored how</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Stuart Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>James Allan</author>
<author>Chris Buckley</author>
<author>Amit Singhal</author>
</authors>
<title>Automatic analysis, theme generation, and summarization of machinereadable texts.</title>
<date>1994</date>
<journal>Science,</journal>
<pages>264--1421</pages>
<contexts>
<context position="2843" citStr="Salton et al., 1994" startWordPosition="436" endWordPosition="439">xample: &amp;quot;John bought some vegetables, fruit, bread, and milk.&amp;quot; What would be the topic of this sentence? We can draw no conclusion by using word counting method; where the topic actually should be: &amp;quot;John bought some groceries.&amp;quot; The problem is that word counting method misses the important concepts behind those words: vegetables, fruit, etc. relates to groceries at the deeper level of semantics. In recognizing the inherent problem of the word counting method, recently people have started to use artificial intelligence techniques (Jacobs and Rau, 1990; Mauldin, 1991) and statistical techniques (Salton et al., 1994; Grefenstette, 1994) to incorporate the sementic relations among words into their applications. Following this trend, we have developed a new way to identify topics by counting concepts instead of words. 2 The Power of Generalization In order to count concept frequency, we employ a concept generalization taxonomy. Figure 1 shows a possible hierarchy for the concept digital computer. According to this hierarchy, if we find laptop and hand-held computer, in a text, we can infer that the text is about portable computers, which is their parent concept. And if in addition, the text also mentions w</context>
</contexts>
<marker>Salton, Allan, Buckley, Singhal, 1994</marker>
<rawString>Gerard Salton, James Allan, Chris Buckley, and Amit Singhal. 1994. Automatic analysis, theme generation, and summarization of machinereadable texts. Science, 264:1421-1426, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
<author>Patrick Hanks</author>
<author>Gwyneth Fox</author>
<author>Rosamuna Moon</author>
<author>Penny Stock</author>
</authors>
<date>1987</date>
<journal>Collins COB UILD English Language Dictionary. William Collins Sons &amp; Co. Ltd.,</journal>
<location>Glasgow, UK.</location>
<contexts>
<context position="1222" citStr="Sinclair et al., 1987" startWordPosition="183" endWordPosition="186">quency ratio, we control the amount and level of generality of concepts extracted from the text. 1 Introduction As the amount of text available online keeps growing, it becomes increasingly difficult for people to keep track of and locate the information of interest to them. To remedy the problem of information overload, a robust and automated text summarizer or information extrator is needed. Topic identification is one of two very important steps in the process of summarizing a text; the second step is summary text generation. A topic is a particular subject that we write about or discuss. (Sinclair et al., 1987). To identify the topics of texts, Information Retrieval (IR) researchers use word frequency, cue word, location, and title-keyword techniques (Paice, 1990). Among these techniques, only word frequency counting can be used robustly across different domains; the other techniques rely on stereotypical text structure or the functional structures of specific domains. Underlying the use of word frequency is the assumption that the more a word is used in a text, the more important it is in that text. This method &apos;This research was funded in part by ARPA under order number 8073, issued as Maryland Pr</context>
</contexts>
<marker>Sinclair, Hanks, Fox, Moon, Stock, 1987</marker>
<rawString>John Sinclair, Patrick Hanks, Gwyneth Fox, Rosamuna Moon, and Penny Stock. 1987. Collins COB UILD English Language Dictionary. William Collins Sons &amp; Co. Ltd., Glasgow, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>