<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.986869">
Utterance-Level Multimodal Sentiment Analysis
</title>
<author confidence="0.994172">
Ver´onica P´erez-Rosas and Rada Mihalcea Louis-Philippe Morency
</author>
<affiliation confidence="0.998795">
Computer Science and Engineering Institute for Creative Technologies
University of North Texas University of Southern California
</affiliation>
<email confidence="0.997285">
veronicaperezrosas@my.unt.edu, rada@cs.unt.edu morency@ict.usc.edu
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999705772727273">
During real-life interactions, people are
naturally gesturing and modulating their
voice to emphasize specific points or to
express their emotions. With the recent
growth of social websites such as YouTube,
Facebook, and Amazon, video reviews are
emerging as a new source of multimodal
and natural opinions that has been left al-
most untapped by automatic opinion anal-
ysis techniques. This paper presents a
method for multimodal sentiment classi-
fication, which can identify the sentiment
expressed in utterance-level visual datas-
treams. Using a new multimodal dataset
consisting of sentiment annotated utter-
ances extracted from video reviews, we
show that multimodal sentiment analysis
can be effectively performed, and that the
joint use of visual, acoustic, and linguistic
modalities can lead to error rate reductions
of up to 10.5% as compared to the best
performing individual modality.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952821428572">
Video reviews represent a growing source of con-
sumer information that gained increasing interest
from companies, researchers, and consumers. Pop-
ular web platforms such as YouTube, Amazon,
Facebook, and ExpoTV have reported a signifi-
cant increase in the number of consumer reviews
in video format over the past five years. Compared
to traditional text reviews, video reviews provide a
more natural experience as they allow the viewer to
better sense the reviewer’s emotions, beliefs, and
intentions through richer channels such as intona-
tions, facial expressions, and body language.
Much of the work to date on opinion analysis has
focused on textual data, and a number of resources
have been created including lexicons (Wiebe and
Riloff, 2005; Esuli and Sebastiani, 2006) or large
annotated datasets (Maas et al., 2011). Given the
accelerated growth of other media on the Web and
elsewhere, which includes massive collections of
videos (e.g., YouTube, Vimeo, VideoLectures), im-
ages (e.g., Flickr, Picasa), audio clips (e.g., pod-
casts), the ability to address the identification of
opinions in the presence of diverse modalities is be-
coming increasingly important. This has motivated
researchers to start exploring multimodal clues for
the detection of sentiment and emotions in video
content (Morency et al., 2011; Wagner et al., 2011).
In this paper, we explore the addition of speech
and visual modalities to text analysis in order to
identify the sentiment expressed in video reviews.
Given the non homogeneous nature of full-video
reviews, which typically include a mixture of posi-
tive, negative, and neutral statements, we decided
to perform our experiments and analyses at the ut-
terance level. This is in line with earlier work on
text-based sentiment analysis, where it has been
observed that full-document reviews often contain
both positive and negative comments, which led to
a number of methods addressing opinion analysis
at sentence level. Our results show that relying
on the joint use of linguistic, acoustic, and visual
modalities allows us to better sense the sentiment
being expressed as compared to the use of only one
modality at a time.
Another important aspect of this paper is the in-
troduction of a new multimodal opinion database
annotated at the utterance level which is, to our
knowledge, the first of its kind. In our work, this
dataset enabled a wide range of multimodal senti-
ment analysis experiments, addressing the relative
importance of modalities and individual features.
The following section presents related work
in text-based sentiment analysis and audio-visual
emotion recognition. Section 3 describes our new
multimodal datasets with utterance-level sentiment
annotations. Section 4 presents our multimodal sen-
</bodyText>
<page confidence="0.985372">
973
</page>
<note confidence="0.91406">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973–982,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999848">
timent analysis approach, including details about
our linguistic, acoustic, and visual features. Our
experiments and results on multimodal sentiment
classification are presented in Section 5, with a
detailed discussion and analysis in Section 6.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999893666666667">
In this section we provide a brief overview of re-
lated work in text-based sentiment analysis, as well
as audio-visual emotion analysis.
</bodyText>
<subsectionHeader confidence="0.987957">
2.1 Text-based Subjectivity and Sentiment
Analysis
</subsectionHeader>
<bodyText confidence="0.998603230769231">
The techniques developed so far for subjectivity
and sentiment analysis have focused primarily on
the processing of text, and consist of either rule-
based classifiers that make use of opinion lexicons,
or data-driven methods that assume the availability
of a large dataset annotated for polarity. These tools
and resources have been already used in a large
number of applications, including expressive text-
to-speech synthesis (Alm et al., 2005), tracking
sentiment timelines in on-line forums and news
(Balog et al., 2006), analysis of political debates
(Carvalho et al., 2011), question answering (Oh et
al., 2012), conversation summarization (Carenini et
al., 2008), and citation sentiment detection (Athar
and Teufel, 2012).
One of the first lexicons used in sentiment anal-
ysis is the General Inquirer (Stone, 1968). Since
then, many methods have been developed to auto-
matically identify opinion words and their polarity
(Hatzivassiloglou and McKeown, 1997; Turney,
2002; Hu and Liu, 2004; Taboada et al., 2011), as
well as n-gram and more linguistically complex
phrases (Yang and Cardie, 2012).
For data-driven methods, one of the most widely
used datasets is the MPQA corpus (Wiebe et al.,
2005), which is a collection of news articles manu-
ally annotated for opinions. Other datasets are also
available, including two polarity datasets consist-
ing of movie reviews (Pang and Lee, 2004; Maas et
al., 2011), and a collection of newspaper headlines
annotated for polarity (Strapparava and Mihalcea,
2007).
While difficult problems such as cross-domain
(Blitzer et al., 2007; Li et al., 2012) or cross-
language (Mihalcea et al., 2007; Wan, 2009; Meng
et al., 2012) portability have been addressed, not
much has been done in terms of extending the ap-
plicability of sentiment analysis to other modalities,
such as speech or facial expressions.
The only exceptions that we are aware of are the
findings reported in (Somasundaran et al., 2006;
Raaijmakers et al., 2008; Mairesse et al., 2012;
Metze et al., 2009), where speech and text have
been analyzed jointly for the purpose of subjectiv-
ity or sentiment identification, without, however,
addressing other modalities such as visual cues;
and the work reported in (Morency et al., 2011;
Perez-Rosas et al., 2013), where multimodal cues
have been used for the analysis of sentiment in
product reviews, but where the analysis was done
at the much coarser level of full videos rather than
individual utterances as we do in our work.
</bodyText>
<subsectionHeader confidence="0.999154">
2.2 Audio-Visual Emotion Analysis.
</subsectionHeader>
<bodyText confidence="0.996981942857143">
Also related to our work is the research done on
emotion analysis. Emotion analysis of speech sig-
nals aims to identify the emotional or physical
states of a person by analyzing his or her voice
(Ververidis and Kotropoulos, 2006). Proposed
methods for emotion recognition from speech fo-
cus both on what is being said and how is be-
ing said, and rely mainly on the analysis of the
speech signal by sampling the content at utterance
or frame level (Bitouk et al., 2010). Several re-
searchers used prosody (e.g., pitch, speaking rate,
Mel frequency coefficients) for speech-based emo-
tion recognition (Polzin and Waibel, 1996; Tato et
al., 2002; Ayadi et al., 2011).
There are also studies that analyzed the visual
cues, such as facial expressions and body move-
ments (Calder et al., 2001; Rosenblum et al., 1996;
Essa and Pentland, 1997). Facial expressions are
among the most powerful and natural means for
human beings to communicate their emotions and
intentions (Tian et al., 2001). Emotions can be
also expressed unconsciously, through subtle move-
ments of facial muscles such as smiling or eyebrow
raising, often measured and described using the
Facial Action Coding System (FACS) (Ekman et
al., 2002).
De Silva et. al. (De Silva et al., 1997) and Chen
et. al. (Chen et al., 1998) presented one of the
early works that integrate both acoustic and visual
information for emotion recognition. In addition to
work that considered individual modalities, there
is also a growing body of work concerned with
multimodal emotion analysis (Silva et al., 1997;
Sebe et al., 2006; Zhihong et al., 2009; Wollmer et
al., 2010).
</bodyText>
<page confidence="0.997581">
974
</page>
<bodyText confidence="0.914374090909091">
Utterance transcription Label
En este color, creo que era el color frambuesa. neu
In this color, I think it was raspberry
Pinta hermosisimo. pos
It looks beautiful.
Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos
Honestly, talking about how they looks and hydrates, yes they are very hydrant.
Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg
But the problem with those lipsticks is that when you apply them, they leave a very nasty taste
Sinceramente, es no es que sea el olor sino que es mas bien el gusto. neg
Honestly, is not the smell, it is the taste.
</bodyText>
<tableCaption confidence="0.997447">
Table 1: Sample utterance-level annotations. The labels used are: pos(itive), neg(ative), neu(tral).
</tableCaption>
<bodyText confidence="0.999770454545454">
More recently, two challenges have been or-
ganized focusing on the recognition of emotions
using audio and visual cues (Schuller et al.,
2011a; Schuller et al., 2011b), which included sub-
challenges on audio-only, video-only, and audio-
video, and drew the participation of many teams
from around the world. Note however that most of
the previous work on audio-visual emotion analy-
sis has focused exclusively on the audio and video
modalities, and did not consider textual features, as
we do in our work.
</bodyText>
<sectionHeader confidence="0.8723135" genericHeader="method">
3 MOUD: Multimodal Opinion
Utterances Dataset
</sectionHeader>
<bodyText confidence="0.999957956521739">
For our experiments, we created a dataset of ut-
terances (named MOUD) containing product opin-
ions expressed in Spanish.1 We chose to work with
Spanish because it is a widely used language, and
it is the native language of the main author of this
paper.
We started by collecting a set of videos from
the social media web site YouTube, using several
keywords likely to lead to a product review or rec-
ommendation. Starting with the YouTube search
page, videos were found using the following key-
words: mis products favoritos (my favorite prod-
ucts), products que no recomiendo (non recom-
mended products), mis perfumes favoritos (my fa-
vorite perfumes), peliculas recomendadas (recom-
mended movies), peliculas que no recomiendo (non
recommended movies) and libros recomendados
(recommended books), libros que no recomiendo
(non recommended books). Notice that the key-
words are not targeted at a specific product type;
rather, we used a variety of product names, so that
the dataset has some degree of generality within
the broad domain of product reviews.
</bodyText>
<footnote confidence="0.516497">
1Publicly available from the authors webpage.
</footnote>
<bodyText confidence="0.999984444444444">
Among all the videos returned by the YouTube
search, we selected only videos that respected the
following guidelines: the speaker should be in front
of the camera; her face should be clearly visible,
with a minimum amount of face occlusion during
the recording; there should not be any background
music or animation. The final video set includes 80
videos randomly selected from the videos retrieved
from YouTube that also met the guidelines above.
The dataset includes 15 male and 65 female speak-
ers, with their age approximately ranging from 20
to 60 years.
All the videos were first pre-processed to elimi-
nate introductory titles and advertisements. Since
the reviewers often switched topics when express-
ing their opinions, we manually selected a 30 sec-
onds opinion segment from each video to avoid
having multiple topics in a single review.
</bodyText>
<subsectionHeader confidence="0.999639">
3.1 Segmentation and Transcription
</subsectionHeader>
<bodyText confidence="0.9998810625">
All the video clips were manually processed to
transcribe the verbal statements and also to extract
the start and end time of each utterance. Since the
reviewers utter expressive sentences that are nat-
urally segmented by speech pauses, we decided
to use these pauses (&gt;0.5seconds) to identify the
beginning and the end of each utterance. The tran-
scription and segmentation were performed using
the Transcriber software.
Each video was segmented into an average of
six utterances, resulting in a final dataset of 498
utterances. Each utterance is linked to the corre-
sponding audio and video stream, as well as its
manual transcription. The utterances have an aver-
age duration of 5 seconds, with a standard deviation
of 1.2 seconds.
</bodyText>
<page confidence="0.997618">
975
</page>
<figureCaption confidence="0.999348">
Figure 1: Multimodal feature extraction
</figureCaption>
<subsectionHeader confidence="0.99982">
3.2 Sentiment Annotation
</subsectionHeader>
<bodyText confidence="0.9999199">
To enable the use of this dataset for sentiment de-
tection, we performed sentiment annotations at ut-
terance level. Annotations were done using Elan,2
which is a widely used tool for the annotation of
video and audio resources. Two annotators indepen-
dently labeled each utterance as positive, negative,
or neutral. The annotation was done after seeing
the video corresponding to an utterance (along with
the corresponding audio source). The transcription
of the utterance was also made available. Thus, the
annotation process included all three modalities: vi-
sual, acoustic, and linguistic. The annotators were
allowed to watch the video segment and their cor-
responding transcription as many times as needed.
The inter-annotator agreement was measured at
88%, with a Kappa of 0.81, which represents good
agreement. All the disagreements were reconciled
through discussions.
Table 1 shows the five utterances obtained from a
video in our dataset, along with their corresponding
</bodyText>
<footnote confidence="0.738638">
2http://tla.mpi.nl/tools/tla-tools/elan/
</footnote>
<bodyText confidence="0.997555625">
sentiment annotations. As this example illustrates,
a video can contain a mix of positive, negative, and
neutral utterances. Note also that sentiment is not
always explicit in the text: for example, the last
utterance “Honestly, it is not the smell, it is the
taste” has an implicit reference to the “nasty taste”
expressed in the previous utterance, and thus it was
also labeled as negative by both annotators.
</bodyText>
<sectionHeader confidence="0.987937" genericHeader="method">
4 Multimodal Sentiment Analysis
</sectionHeader>
<bodyText confidence="0.999978333333333">
The main advantage that comes with the analysis of
video opinions, as compared to their textual coun-
terparts, is the availability of visual and speech cues.
In textual opinions, the only source of information
consists of words and their dependencies, which
may sometime prove insufficient to convey the ex-
act sentiment of the user. Instead, video opinions
naturally contain multiple modalities, consisting of
visual, acoustic, and linguistic datastreams. We hy-
pothesize that the simultaneous use of these three
modalities will help create a better opinion analysis
model.
</bodyText>
<page confidence="0.993062">
976
</page>
<subsectionHeader confidence="0.993947">
4.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.986645888888889">
This section describes the process of automatically
extracting linguistic, acoustic and visual features
from the video reviews. First, we obtain the stream
corresponding to each modality, followed by the
extraction of a representative set of features for
each modality, as described in the following sub-
sections. These features are then used as cues to
build a classifier of positive or negative sentiment.
Figure 1 illustrates this process.
</bodyText>
<subsectionHeader confidence="0.717589">
4.1.1 Linguistic Features
</subsectionHeader>
<bodyText confidence="0.999770941176471">
We use a bag-of-words representation of the video
transcriptions of each utterance to derive unigram
counts, which are then used as linguistic features.
First, we build a vocabulary consisting of all the
words, including stopwords, occurring in the tran-
scriptions of the training set. We then remove
those words that have a frequency below 10 (value
determined empirically on a small development
set). The remaining words represent the unigram
features, which are then associated with a value
corresponding to the frequency of the unigram in-
side each utterance transcription. These simple
weighted unigram features have been successfully
used in the past to build sentiment classifiers on
text, and in conjunction with Support Vector Ma-
chines (SVM) have been shown to lead to state-of-
the-art performance (Maas et al., 2011).
</bodyText>
<subsectionHeader confidence="0.54941">
4.1.2 Acoustic Features
</subsectionHeader>
<bodyText confidence="0.9999405">
Acoustic features are automatically extracted from
the speech signal of each utterance. We used the
open source software OpenEAR (Schuller, 2009)
to automatically compute a set of acoustic features.
We include prosody, energy, voicing probabilities,
spectrum, and cepstral features.
</bodyText>
<listItem confidence="0.8838008125">
• Prosody features. These include intensity,
loudness, and pitch that describe the speech
signal in terms of amplitude and frequency.
• Energy features. These features describe the
human loudness perception.
• Voice probabilities. These are probabilities
that represent an estimate of the percentage of
voiced and unvoiced energy in the speech.
• Spectral features. The spectral features are
based on the characteristics of the human ear,
which uses a nonlinear frequency unit to simu-
late the human auditory system. These fea-
tures describe the speech formants, which
model spoken content and represent speaker
characteristics.
• Cepstral features. These features emphasize
</listItem>
<bodyText confidence="0.951988428571429">
changes or periodicity in the spectrum fea-
tures measured by frequencies; we model
them using 12 Mel-frequency cepstral coeffi-
cients that are calculated based on the Fourier
transform of a speech frame.
Overall, we have a set of 28 acoustic features.
During the feature extraction, we use a frame sam-
pling of 25ms. Speaker normalization is performed
using z-standardization. The voice intensity is
thresholded to identify samples with and without
speech, with the same threshold being used for all
the experiments and all the speakers. The features
are averaged over all the frames in an utterance, to
obtain one feature vector for each utterance.
</bodyText>
<subsubsectionHeader confidence="0.456175">
4.1.3 Facial Features
</subsubsectionHeader>
<bodyText confidence="0.997361052631579">
Facial expressions can provide important clues for
affect recognition, which we use to complement
the linguistic and acoustic features extracted from
the speech stream.
The most widely used system for measuring and
describing facial behaviors is the Facial Action
Coding System (FACS), which allows for the de-
scription of face muscle activities through the use
of a set of Action Units (AUs). According with
(Ekman, 1993), there are 64 AUs that involve the
upper and lower face, including several face posi-
tions and movements.3 AUs can occur either by
themselves or in combination, and can be used to
identify a variety of emotions. While AUs are fre-
quently annotated by certified human annotators,
automatic tools are also available. In our work, we
use the Computer Expression Recognition Toolbox
(CERT) (Littlewort et al., 2011), which allows us to
automatically extract the following visual features:
</bodyText>
<listItem confidence="0.9656588">
• Smile and head pose estimates. The smile
feature is an estimate for smiles. Head pose
detection consists of three-dimensional esti-
mates of the head orientation, i.e., yaw, pitch,
and roll. These features provide information
about changes in smiles and face positions
while uttering positive and negative opinions.
• Facial AUs. These features are the raw es-
timates for 30 facial AUs related to muscle
movements for the eyes, eyebrows, nose, lips,
</listItem>
<footnote confidence="0.961885">
3http://www.cs.cmu.edu/afs/cs/project/face/www/facs.htm
</footnote>
<page confidence="0.992986">
977
</page>
<bodyText confidence="0.99085716">
and chin. They provide detailed information
about facial behaviors from which we expect
to find differences between positive and nega-
tive states.
• Eight basic emotions. These are estimates
for the following emotions: anger, contempt,
disgust, fear, joy, sad, surprise, and neutral.
These features describe the presence of two or
more AUs that define a specific emotion. For
example, the unit A12 describes the pulling
of lip corners movement, which usually sug-
gests a smile but when associated with a
check raiser movement (unit A6), represents
a marker for the emotion of happiness.
We extract a total of 40 visual features, each
of them obtained at frame level. Since only one
person is present in each video clip, most of the
time facing the camera, the facial tracking was
successfully applied for most of our data. For the
analysis, we use a sampling rate of 30 frames per
second. The features extracted for each utterance
are averaged over all the valid frames, which are
automatically identified using the output of CERT.4
Segments with more than 60% of invalid frames
are simply discarded.
</bodyText>
<sectionHeader confidence="0.99682" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9999861">
We run our sentiment classification experiments
on the MOUD dataset introduced earlier. From
the dataset, we remove utterances labeled as neu-
tral, thus keeping only the positive and negative
utterances with valid visual features. The removal
of neutral utterances is done for two main reasons.
First, the number of neutral utterances in the dataset
is rather small. Second, previous work in subjec-
tivity and sentiment analysis has demonstrated that
a layered approach (where neutral statements are
first separated from opinion statements followed
by a separation between positive and negative state-
ments) works better than a single three-way classifi-
cation. After this process, we are left with an exper-
imental dataset of 412 utterances, 182 of which are
labeled as positive, and 231 are labeled as negative.
From each utterance, we extract the linguis-
tic, acoustic, and visual features described above,
which are then combined using the early fusion
(or feature-level fusion) approach (Hall and Llinas,
</bodyText>
<footnote confidence="0.985320666666667">
4There is a small number of frames that CERT could not
process, mostly due to the brief occlusions that occur when
the speaker is showing the product she is reviewing.
</footnote>
<figure confidence="0.740805666666667">
Modality Accuracy
Baseline 55.93%
One modality at a time
Linguistic 70.94%
Acoustic 64.85%
Visual 67.31%
Two modalities at a time
Linguistic + Acoustic 72.88%
Linguistic + Visual 72.39%
Acoustic + Visual 68.86%
Three modalities at a time
Linguistic+Acoustic+Visual 74.09%
</figure>
<tableCaption confidence="0.6596665">
Table 2: Utterance-level sentiment classification
with linguistic, acoustic, and visual features.
</tableCaption>
<bodyText confidence="0.997101392857143">
1997; Atrey et al., 2010). In this approach, the fea-
tures collected from all the multimodal streams are
combined into a single feature vector, thus result-
ing in one vector for each utterance in the dataset
which is used to make a decision about the senti-
ment orientation of the utterance.
We run several comparative experiments, using
one, two, and three modalities at a time. We use
the entire set of 412 utterances and run ten fold
cross validations using an SVM classifier, as imple-
mented in the Weka toolkit.5 In line with previous
work on emotion recognition in speech (Haq and
Jackson, 2009; Anagnostopoulos and Vovoli, 2010)
where utterances are selected in a speaker depen-
dent manner (i.e., utterances from the same speaker
are included in both training and test), as well as
work on sentence-level opinion classification where
document boundaries are not considered in the split
performed between the training and test sets (Wil-
son et al., 2004; Wiegand and Klakow, 2009), the
training/test split for each fold is performed at ut-
terance level regardless of the video they belong
to.
Table 2 shows the results of the utterance-level
sentiment classification experiments. The baseline
is obtained using the ZeroR classifier, which as-
signs the most frequent label by default, averaged
over the ten folds.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999692">
The experimental results show that sentiment clas-
sification can be effectively performed on multi-
modal datastreams. Moreover, the integration of
</bodyText>
<footnote confidence="0.975546">
5http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.993988">
978
</page>
<figureCaption confidence="0.90585125">
Figure 2: Visual and acoustic feature weights. This
graph shows the relative importance of the infor-
mation gain weights associated with the top most
informative acoustic-visual features.
</figureCaption>
<bodyText confidence="0.989052023809524">
visual, acoustic, and linguistic features can improve
significantly over the use of one modality at a time,
with incremental improvements observed for each
added modality.
Among the individual classifiers, the linguistic
classifier appears to be the most accurate, followed
by the classifier that relies on visual clues, and by
the audio classifier. Compared to the best indi-
vidual classifier, the relative error rate reduction
obtained with the tri-modal classifier is 10.5%.
The results obtained with this multimodal utter-
ance classifier are found to be significantly better
than the best individual results (obtained with the
text modality), with significance being tested with
a t-test (p=0.05).
Feature analysis.
To determine the role played by each of the vi-
sual and acoustic features, we compare the fea-
ture weights assigned by the learning algorithm,
as shown in Figure 2. Interestingly, a distressed
brow is the strongest indicator of sentiment, fol-
lowed, this time not surprisingly, by the smile fea-
ture. Other informative features for sentiment clas-
sification are the voice probability, representing the
energy in speech, the combined visual features that
represent an angry face, and two of the cepstral
coefficients.
To reach a better understanding of the relation
between features, we also calculate the Pearson
correlation between the visual and acoustic fea-
tures. Table 3 shows a subset of these correlation
figures. As we expected, correlations between fea-
tures of the same type are higher. For example,
the correlation between features AU6 and AU12
or the correlation between intensity and loudness
is higher than the correlation between AU6 and in-
tensity. Nonetheless, we still find some significant
correlations between features of different types, for
instance AU12 and AU45 which are both signifi-
cantly correlated with the intensity and loudness
features. This give us confidence about using them
for further analysis.
</bodyText>
<subsectionHeader confidence="0.999267">
Video-level sentiment analysis.
</subsectionHeader>
<bodyText confidence="0.999973388888889">
To understand the role played by the size of the
video-segments considered in the sentiment classi-
fication experiments, as well as the potential effect
of a speaker-independence assumption, we also run
a set of experiments where we use full videos for
the classification.
In these experiments, once again the sentiment
annotation is done by two independent annotators,
using the same protocol as in the utterance-based
annotations. Videos that were ambivalent about
the general sentiment were either labeled as neu-
tral (and thus removed from the experiments), or
labeled with the dominant sentiment. The inter-
annotator agreement for this annotation was mea-
sured at 96.1%. As before, the linguistic, acoustic,
and visual features are averaged over the entire
video, and we use an SVM classifier in ten-fold
cross validation experiments.
Table 4 shows the results obtained in these
video-level experiments. While the combination of
modalities still helps, the improvement is smaller
than the one obtained during the utterance-level
classification. Specifically, the combined effect of
acoustic and visual features improves significantly
over the individual modalities. However, the com-
bination of linguistic features with other modalities
does not lead to clear improvements. This may be
due to the smaller number of feature vectors used
in the experiments (only 80, as compared to the
412 used in the previous setup). Another possi-
ble reason is the fact that the acoustic and visual
modalities are significantly weaker than the lin-
guistic modality, most likely due to the fact that
the feature vectors are now speaker-independent,
which makes it harder to improve over the linguis-
tic modality alone.
</bodyText>
<sectionHeader confidence="0.996057" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99906">
In this paper, we presented a multimodal approach
for utterance-level sentiment classification. We
introduced a new multimodal dataset consisting
</bodyText>
<page confidence="0.996211">
979
</page>
<table confidence="0.991498111111111">
AU6 AU12 AU45 AUs 1,1+4 Pitch Voice probability Intensity Loudness
AU6 1.00 0.46* -0.03 -0.05 0.06 -0.14* -0.04 -0.02
AU12 1.00 -0.23* -0.33* 0.04 0.05 0.15* 0.16*
AU45 1.00 0.05 -0.05 -0.11* -.163* 0.16*
AUs 1,1+4 1.00 -0.11* -0.16* 0.06 0.07
Pitch 1.00 -0.04 -0.01 -0.08
Voice probability 1.00 0.19* 0.38*
Intensity 1.00 0.85*
Loudness 1.00
</table>
<tableCaption confidence="0.973958">
Table 3: Correlations between several visual and acoustic features. Visual features: AU6 Cheek raise,
AU12 Lip corner pull, AU45 Blink eye and closure, AU1,1+4 Distress brow. Acoustic features: Pitch,
Voice probability, Intensity, Energy. *Correlation is significant at the 0.05 level (1-tailed)
</tableCaption>
<figure confidence="0.830706846153846">
.
Modality Accuracy
Baseline 55.93%
One modality at a time
Linguistic 73.33%
Acoustic 53.33%
Visual 50.66%
Two modalities at a time
Linguistic + Acoustic 72.00%
Linguistic + Visual 74.66%
Acoustic + Visual 61.33%
Three modalities at a time
Linguistic+Acoustic+Visual 74.66%
</figure>
<tableCaption confidence="0.705038">
Table 4: Video-level sentiment classification with
linguistic, acoustic, and visual features.
</tableCaption>
<bodyText confidence="0.999684833333333">
of sentiment annotated utterances extracted from
video reviews, where each utterance is associated
with a video, acoustic, and linguistic datastream.
Our experiments show that sentiment annotation
of utterance-level visual datastreams can be ef-
fectively performed, and that the use of multiple
modalities can lead to error rate reductions of up to
10.5% as compared to the use of one modality at a
time. In future work, we plan to explore alternative
multimodal fusion methods, such as decision-level
and meta-level fusion, to improve the integration
of the visual, acoustic, and linguistic modalities.
</bodyText>
<sectionHeader confidence="0.989774" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996982333333333">
We would like to thank Alberto Castro for his help
with the sentiment annotations. This material is
based in part upon work supported by National Sci-
ence Foundation awards #0917170 and #1118018,
by DARPA-BAA-12-47 DEFT grant #12475008,
and by a grant from U.S. RDECOM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation, the Defense Advanced
Research Projects Agency, or the U.S. Army Re-
search, Development, and Engineering Command.
</bodyText>
<sectionHeader confidence="0.990586" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988848533333334">
C. Alm, D. Roth, and R. Sproat. 2005. Emotions
from text: Machine learning for text-based emotion
prediction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 347–354, Vancouver, Canada.
C. Anagnostopoulos and E. Vovoli. 2010. Sound pro-
cessing features for speaker-dependent and phrase-
independent emotion recognition in berlin database.
In Information Systems Development, pages 413–
421. Springer.
A. Athar and S. Teufel. 2012. Context-enhanced cita-
tion sentiment detection. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Montr´eal, Canada, June.
P. K. Atrey, M. A. Hossain, A. El Saddik, and
M. Kankanhalli. 2010. Multimodal fusion for mul-
timedia analysis: a survey. Multimedia Systems, 16.
M. El Ayadi, M. Kamel, and F. Karray. 2011. Survey
on speech emotion recognition: Features, classifica-
tion schemes, and databases. Pattern Recognition,
44(3):572 – 587.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in
blog mood levels. In Proceedings of the 11th Meet-
ing of the European Chapter of the As sociation for
Computational Linguistics (EACL-2006).
Dmitri Bitouk, Ragini Verma, and Ani Nenkova. 2010.
Class-level spectral features for emotion recognition.
Speech Commun., 52(7-8):613–625, July.
</reference>
<page confidence="0.988136">
980
</page>
<reference confidence="0.999173367924529">
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In As-
sociation for Computational Linguistics.
A. J. Calder, A. M. Burton, P. Miller, A. W. Young, and
S. Akamatsu. 2001. A principal component analysis
of facial expressions. Vision research, 41(9):1179–
1208, April.
G. Carenini, R. Ng, and X. Zhou. 2008. Summarizing
emails with conversational cohesion and subjectivity.
In Proceedings of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT 2008), Columbus, Ohio.
P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva.
2011. Liars and saviors in a sentiment annotated
corpus of comments to political debates. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL 2011), Portland, OR.
L. S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu.
1998. Multimodal human emotion/expression recog-
nition. In Proceedings of the 3rd. International Con-
ference on Face &amp; Gesture Recognition, pages 366–,
Washington, DC, USA. IEEE Computer Society.
L C De Silva, T Miyasato, and R Nakatsu, 1997. Facial
emotion recognition using multi-modal information,
volume 1, page 397401. IEEE Signal Processing So-
ciety.
P. Ekman, W. Friesen, and J. Hager. 2002. Facial ac-
tion coding system.
P. Ekman. 1993. Facial expression of emotion. Ameri-
can Psychologist, 48:384–392.
I.A. Essa and A.P. Pentland. 1997. Coding, analy-
sis, interpretation, and recognition of facial expres-
sions. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 19(7):757 –763, jul.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation (LREC 2006), Gen-
ova, IT.
D.L. Hall and J. Llinas. 1997. An introduction to mul-
tisensor fusion. IEEE Special Issue on Data Fusion,
85(1).
S. Haq and P. Jackson. 2009. Speaker-dependent
audio-visual emotion recognition. In International
Conference on Audio-Visual Speech Processing.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 174–181.
M. Hu and B. Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Seattle, Washing-
ton.
F. Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu. 2012.
Cross-domain co-extraction of sentiment and topic
lexicons. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
Jeju Island, Korea.
G. Littlewort, J. Whitehill, Tingfan Wu, I. Fasel,
M. Frank, J. Movellan, and M. Bartlett. 2011. The
computer expression recognition toolbox (cert). In
Automatic Face Gesture Recognition and Workshops
(FG 2011), 2011 IEEE International Conference on,
pages 298 –305, march.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and
C. Potts. 2011. Learning word vectors for sentiment
analysis. In Proceedings of the Association for Com-
putational Linguistics (ACL 2011), Portland, OR.
F. Mairesse, J. Polifroni, and G. Di Fabbrizio. 2012.
Can prosody inform sentiment analysis? experi-
ments on short spoken reviews. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE Inter-
national Conference on, pages 5093 –5096, march.
X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang.
2012. Cross-lingual mixture model for sentiment
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea.
F. Metze, T. Polzehl, and M. Wagner. 2009. Fusion
of acoustic and linguistic features for emotion detec-
tion. In Semantic Computing, 2009. ICSC ’09. IEEE
International Conference on, pages 153 –160, sept.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In Proceedings of the Association for
Computational Linguistics, Prague, Czech Republic.
L.P. Morency, R. Mihalcea, and P. Doshi. 2011. To-
wards multimodal sentiment analysis: Harvesting
opinions from the web. In Proceedings of the In-
ternational Conference on Multimodal Computing,
Alicante, Spain.
J. Oh, K. Torisawa, C. Hashimoto, T. Kawada,
S. De Saeger, J. Kazama, and Y. Wang. 2012.
Why question answering using sentiment analysis
and word classes. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, Jeju Island, Korea.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, July.
</reference>
<page confidence="0.979446">
981
</page>
<reference confidence="0.999921278846154">
V. Perez-Rosas, R. Mihalcea, and L.-P. Morency. 2013.
Multimodal sentiment analysis of spanish online
videos. IEEE Intelligent Systems.
T. Polzin and A. Waibel. 1996. Recognizing emotions
in speech. In In ICSLP.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
466–474, Honolulu, Hawaii.
M. Rosenblum, Y. Yacoob, and L.S. Davis. 1996. Hu-
man expression recognition from motion using a ra-
dial basis function network architecture. Neural Net-
works, IEEE Transactions on, 7(5):1121 –1138, sep.
B. Schuller, M. Valstar, R. Cowie, and M. Pantic, edi-
tors. 2011a. Audio/Visual Emotion Challenge and
Workshop (AVEC 2011).
B. Schuller, M. Valstar, F. Eyben, R. Cowie, and
M. Pantic, editors. 2011b. Audio/Visual Emotion
Challenge and Workshop (AVEC 2011).
F. Eyben M. Wollmer B. Schuller. 2009. Openear in-
troducing the munich open-source emotion and af-
fect recognition toolkit. In ACII.
N. Sebe, I. Cohen, T. Gevers, and T.S. Huang. 2006.
Emotion recognition based on joint visual and audio
cues. In ICPR.
D. Silva, T. Miyasato, and R. Nakatsu. 1997. Facial
emotion recognition using multi-modal information.
In Proceedings of the International Conference on
Information and Communications Security.
S. Somasundaran, J. Wiebe, P. Hoffmann, and D. Lit-
man. 2006. Manual annotation of opinion cate-
gories in meetings. In Proceedings of the Work-
shop on Frontiers in Linguistically Annotated Cor-
pora 2006.
P. Stone. 1968. General Inquirer: Computer Approach
to Content Analysis. MIT Press.
C. Strapparava and R. Mihalcea. 2007. Semeval-2007
task 14: Affective text. In Proceedings of the 4th In-
ternational Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic.
M. Taboada, J. Brooke, M. Tofiloski, K. Voli, and
M. Stede. 2011. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, 37(3).
R. Tato, R. Santos, R. Kompe, and J. M. Pardo. 2002.
Emotional space improves emotion recognition. In
In Proc. ICSLP 2002, pages 2029–2032.
Y.-I. Tian, T. Kanade, and J.F. Cohn. 2001. Recogniz-
ing action units for facial expression analysis. Pat-
tern Analysis and Machine Intelligence, IEEE Trans-
actions on, 23(2):97 –115, feb.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417–424, Philadelphia.
D. Ververidis and C. Kotropoulos. 2006. Emotional
speech recognition: Resources, features, and meth-
ods. Speech Communication, 48(9):1162–1181,
September.
J. Wagner, E. Andre, F. Lingenfelser, and Jonghwa
Kim. 2011. Exploring fusion methods for multi-
modal emotion recognition with missing data. Af-
fective Computing, IEEE Transactions on, 2(4):206
–218, oct.-dec.
X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the Association of Computational Linguistics
and the International Joint Conference on Natural
Language Processing, Singapore, August.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2005) (invited paper), Mexico
City, Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2-3):165–
210.
M. Wiegand and D. Klakow. 2009. The role of
knowledge-based features in polarity classification
at sentence level. In Proceedings of the Interna-
tional Conference of the Florida Artificial Intelli-
gence Research Society.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? finding strong and weak opinion clauses.
In Proceedings of the American Association forArti-
ficial Intelligence.
M. Wollmer, B. Schuller, F. Eyben, and G. Rigoll.
2010. Combining long short-term memory and dy-
namic bayesian networks for incremental emotion-
sensitive artificial listening. IEEE Journal of Se-
lected Topics in Signal Processing, 4(5), October.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
Jeju Island, Korea.
Z. Zhihong, M. Pantic G.I. Roisman, and T.S. Huang.
2009. A survey of affect recognition methods: Au-
dio, visual, and spontaneous expressions. PAMI,
31(1).
</reference>
<page confidence="0.997162">
982
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968110">
<title confidence="0.998798">Utterance-Level Multimodal Sentiment Analysis</title>
<author confidence="0.996699">Ver´onica P´erez-Rosas</author>
<author confidence="0.996699">Rada Mihalcea Louis-Philippe Morency</author>
<affiliation confidence="0.9968505">Computer Science and Engineering Institute for Creative Technologies University of North Texas University of Southern California</affiliation>
<email confidence="0.99976">veronicaperezrosas@my.unt.edu,rada@cs.unt.edumorency@ict.usc.edu</email>
<abstract confidence="0.999042956521739">During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5051" citStr="Alm et al., 2005" startWordPosition="746" endWordPosition="749">on we provide a brief overview of related work in text-based sentiment analysis, as well as audio-visual emotion analysis. 2.1 Text-based Subjectivity and Sentiment Analysis The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more l</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>C. Alm, D. Roth, and R. Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Anagnostopoulos</author>
<author>E Vovoli</author>
</authors>
<title>Sound processing features for speaker-dependent and phraseindependent emotion recognition in berlin database.</title>
<date>2010</date>
<booktitle>In Information Systems Development,</booktitle>
<pages>413--421</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="22536" citStr="Anagnostopoulos and Vovoli, 2010" startWordPosition="3519" endWordPosition="3522">. 1997; Atrey et al., 2010). In this approach, the features collected from all the multimodal streams are combined into a single feature vector, thus resulting in one vector for each utterance in the dataset which is used to make a decision about the sentiment orientation of the utterance. We run several comparative experiments, using one, two, and three modalities at a time. We use the entire set of 412 utterances and run ten fold cross validations using an SVM classifier, as implemented in the Weka toolkit.5 In line with previous work on emotion recognition in speech (Haq and Jackson, 2009; Anagnostopoulos and Vovoli, 2010) where utterances are selected in a speaker dependent manner (i.e., utterances from the same speaker are included in both training and test), as well as work on sentence-level opinion classification where document boundaries are not considered in the split performed between the training and test sets (Wilson et al., 2004; Wiegand and Klakow, 2009), the training/test split for each fold is performed at utterance level regardless of the video they belong to. Table 2 shows the results of the utterance-level sentiment classification experiments. The baseline is obtained using the ZeroR classifier,</context>
</contexts>
<marker>Anagnostopoulos, Vovoli, 2010</marker>
<rawString>C. Anagnostopoulos and E. Vovoli. 2010. Sound processing features for speaker-dependent and phraseindependent emotion recognition in berlin database. In Information Systems Development, pages 413– 421. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Athar</author>
<author>S Teufel</author>
</authors>
<title>Context-enhanced citation sentiment detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5333" citStr="Athar and Teufel, 2012" startWordPosition="786" endWordPosition="789">processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two pol</context>
</contexts>
<marker>Athar, Teufel, 2012</marker>
<rawString>A. Athar and S. Teufel. 2012. Context-enhanced citation sentiment detection. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P K Atrey</author>
<author>M A Hossain</author>
<author>A El Saddik</author>
<author>M Kankanhalli</author>
</authors>
<title>Multimodal fusion for multimedia analysis: a survey.</title>
<date>2010</date>
<journal>Multimedia Systems,</journal>
<volume>16</volume>
<marker>Atrey, Hossain, El Saddik, Kankanhalli, 2010</marker>
<rawString>P. K. Atrey, M. A. Hossain, A. El Saddik, and M. Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey. Multimedia Systems, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M El Ayadi</author>
<author>M Kamel</author>
<author>F Karray</author>
</authors>
<title>Survey on speech emotion recognition: Features, classification schemes, and databases.</title>
<date>2011</date>
<journal>Pattern Recognition,</journal>
<volume>44</volume>
<issue>3</issue>
<pages>587</pages>
<marker>El Ayadi, Kamel, Karray, 2011</marker>
<rawString>M. El Ayadi, M. Kamel, and F. Karray. 2011. Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern Recognition, 44(3):572 – 587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Balog</author>
<author>G Mishne</author>
<author>M de Rijke</author>
</authors>
<title>Why are they excited? identifying and explaining spikes in blog mood levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Meeting of the European Chapter of the As sociation for Computational Linguistics (EACL-2006).</booktitle>
<marker>Balog, Mishne, de Rijke, 2006</marker>
<rawString>K. Balog, G. Mishne, and M. de Rijke. 2006. Why are they excited? identifying and explaining spikes in blog mood levels. In Proceedings of the 11th Meeting of the European Chapter of the As sociation for Computational Linguistics (EACL-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitri Bitouk</author>
<author>Ragini Verma</author>
<author>Ani Nenkova</author>
</authors>
<title>Class-level spectral features for emotion recognition.</title>
<date>2010</date>
<journal>Speech Commun.,</journal>
<pages>52--7</pages>
<contexts>
<context position="7584" citStr="Bitouk et al., 2010" startWordPosition="1158" endWordPosition="1161"> the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial musc</context>
</contexts>
<marker>Bitouk, Verma, Nenkova, 2010</marker>
<rawString>Dmitri Bitouk, Ragini Verma, and Ani Nenkova. 2010. Class-level spectral features for emotion recognition. Speech Commun., 52(7-8):613–625, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="6181" citStr="Blitzer et al., 2007" startWordPosition="920" endWordPosition="923"> 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as v</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Calder</author>
<author>A M Burton</author>
<author>P Miller</author>
<author>A W Young</author>
<author>S Akamatsu</author>
</authors>
<title>A principal component analysis of facial expressions.</title>
<date>2001</date>
<journal>Vision research,</journal>
<volume>41</volume>
<issue>9</issue>
<pages>1208</pages>
<contexts>
<context position="7899" citStr="Calder et al., 2001" startWordPosition="1208" endWordPosition="1211">on by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion reco</context>
</contexts>
<marker>Calder, Burton, Miller, Young, Akamatsu, 2001</marker>
<rawString>A. J. Calder, A. M. Burton, P. Miller, A. W. Young, and S. Akamatsu. 2001. A principal component analysis of facial expressions. Vision research, 41(9):1179– 1208, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R Ng</author>
<author>X Zhou</author>
</authors>
<title>Summarizing emails with conversational cohesion and subjectivity.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Human Language Technologies (ACLHLT</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5274" citStr="Carenini et al., 2008" startWordPosition="778" endWordPosition="781">vity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opi</context>
</contexts>
<marker>Carenini, Ng, Zhou, 2008</marker>
<rawString>G. Carenini, R. Ng, and X. Zhou. 2008. Summarizing emails with conversational cohesion and subjectivity. In Proceedings of the Association for Computational Linguistics: Human Language Technologies (ACLHLT 2008), Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Carvalho</author>
<author>L Sarmento</author>
<author>J Teixeira</author>
<author>M Silva</author>
</authors>
<title>Liars and saviors in a sentiment annotated corpus of comments to political debates.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL 2011),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="5184" citStr="Carvalho et al., 2011" startWordPosition="766" endWordPosition="769">ext-based Subjectivity and Sentiment Analysis The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corp</context>
</contexts>
<marker>Carvalho, Sarmento, Teixeira, Silva, 2011</marker>
<rawString>P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva. 2011. Liars and saviors in a sentiment annotated corpus of comments to political debates. In Proceedings of the Association for Computational Linguistics (ACL 2011), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Chen</author>
<author>T S Huang</author>
<author>T Miyasato</author>
<author>R Nakatsu</author>
</authors>
<title>Multimodal human emotion/expression recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd. International Conference on Face &amp; Gesture Recognition,</booktitle>
<pages>366</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="8397" citStr="Chen et al., 1998" startWordPosition="1290" endWordPosition="1293">There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010). 974 Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos Ho</context>
</contexts>
<marker>Chen, Huang, Miyasato, Nakatsu, 1998</marker>
<rawString>L. S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu. 1998. Multimodal human emotion/expression recognition. In Proceedings of the 3rd. International Conference on Face &amp; Gesture Recognition, pages 366–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L C De Silva</author>
<author>T Miyasato</author>
<author>R Nakatsu</author>
</authors>
<title>Facial emotion recognition using multi-modal information,</title>
<date>1997</date>
<volume>1</volume>
<pages>397401</pages>
<publisher>IEEE Signal Processing Society.</publisher>
<marker>De Silva, Miyasato, Nakatsu, 1997</marker>
<rawString>L C De Silva, T Miyasato, and R Nakatsu, 1997. Facial emotion recognition using multi-modal information, volume 1, page 397401. IEEE Signal Processing Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W Friesen</author>
<author>J Hager</author>
</authors>
<title>Facial action coding system.</title>
<date>2002</date>
<contexts>
<context position="8318" citStr="Ekman et al., 2002" startWordPosition="1273" endWordPosition="1276">n recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010). 974 Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con</context>
</contexts>
<marker>Ekman, Friesen, Hager, 2002</marker>
<rawString>P. Ekman, W. Friesen, and J. Hager. 2002. Facial action coding system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>Facial expression of emotion.</title>
<date>1993</date>
<journal>American Psychologist,</journal>
<pages>48--384</pages>
<contexts>
<context position="18245" citStr="Ekman, 1993" startWordPosition="2841" endWordPosition="2842">being used for all the experiments and all the speakers. The features are averaged over all the frames in an utterance, to obtain one feature vector for each utterance. 4.1.3 Facial Features Facial expressions can provide important clues for affect recognition, which we use to complement the linguistic and acoustic features extracted from the speech stream. The most widely used system for measuring and describing facial behaviors is the Facial Action Coding System (FACS), which allows for the description of face muscle activities through the use of a set of Action Units (AUs). According with (Ekman, 1993), there are 64 AUs that involve the upper and lower face, including several face positions and movements.3 AUs can occur either by themselves or in combination, and can be used to identify a variety of emotions. While AUs are frequently annotated by certified human annotators, automatic tools are also available. In our work, we use the Computer Expression Recognition Toolbox (CERT) (Littlewort et al., 2011), which allows us to automatically extract the following visual features: • Smile and head pose estimates. The smile feature is an estimate for smiles. Head pose detection consists of three-</context>
</contexts>
<marker>Ekman, 1993</marker>
<rawString>P. Ekman. 1993. Facial expression of emotion. American Psychologist, 48:384–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Essa</author>
<author>A P Pentland</author>
</authors>
<title>Coding, analysis, interpretation, and recognition of facial expressions.</title>
<date>1997</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>19</volume>
<issue>7</issue>
<pages>763</pages>
<contexts>
<context position="7949" citStr="Essa and Pentland, 1997" startWordPosition="1216" endWordPosition="1219">and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered indiv</context>
</contexts>
<marker>Essa, Pentland, 1997</marker>
<rawString>I.A. Essa and A.P. Pentland. 1997. Coding, analysis, interpretation, and recognition of facial expressions. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 19(7):757 –763, jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genova, IT.</location>
<contexts>
<context position="1991" citStr="Esuli and Sebastiani, 2006" startWordPosition="283" endWordPosition="286">ular web platforms such as YouTube, Amazon, Facebook, and ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addi</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC 2006), Genova, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Hall</author>
<author>J Llinas</author>
</authors>
<title>An introduction to multisensor fusion.</title>
<date>1997</date>
<journal>IEEE Special Issue on Data Fusion,</journal>
<volume>85</volume>
<issue>1</issue>
<marker>Hall, Llinas, 1997</marker>
<rawString>D.L. Hall and J. Llinas. 1997. An introduction to multisensor fusion. IEEE Special Issue on Data Fusion, 85(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Haq</author>
<author>P Jackson</author>
</authors>
<title>Speaker-dependent audio-visual emotion recognition.</title>
<date>2009</date>
<booktitle>In International Conference on Audio-Visual Speech Processing.</booktitle>
<contexts>
<context position="22501" citStr="Haq and Jackson, 2009" startWordPosition="3515" endWordPosition="3518">ic, and visual features. 1997; Atrey et al., 2010). In this approach, the features collected from all the multimodal streams are combined into a single feature vector, thus resulting in one vector for each utterance in the dataset which is used to make a decision about the sentiment orientation of the utterance. We run several comparative experiments, using one, two, and three modalities at a time. We use the entire set of 412 utterances and run ten fold cross validations using an SVM classifier, as implemented in the Weka toolkit.5 In line with previous work on emotion recognition in speech (Haq and Jackson, 2009; Anagnostopoulos and Vovoli, 2010) where utterances are selected in a speaker dependent manner (i.e., utterances from the same speaker are included in both training and test), as well as work on sentence-level opinion classification where document boundaries are not considered in the split performed between the training and test sets (Wilson et al., 2004; Wiegand and Klakow, 2009), the training/test split for each fold is performed at utterance level regardless of the video they belong to. Table 2 shows the results of the utterance-level sentiment classification experiments. The baseline is o</context>
</contexts>
<marker>Haq, Jackson, 2009</marker>
<rawString>S. Haq and P. Jackson. 2009. Speaker-dependent audio-visual emotion recognition. In International Conference on Audio-Visual Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="5566" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="822" endWordPosition="825">ve been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitz</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="5598" citStr="Hu and Liu, 2004" startWordPosition="828" endWordPosition="831">ions, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
<author>S J Pan</author>
<author>O Jin</author>
<author>Q Yang</author>
<author>X Zhu</author>
</authors>
<title>Cross-domain co-extraction of sentiment and topic lexicons.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="6199" citStr="Li et al., 2012" startWordPosition="924" endWordPosition="927">u and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and th</context>
</contexts>
<marker>Li, Pan, Jin, Yang, Zhu, 2012</marker>
<rawString>F. Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu. 2012. Cross-domain co-extraction of sentiment and topic lexicons. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Littlewort</author>
<author>J Whitehill</author>
<author>Tingfan Wu</author>
<author>I Fasel</author>
<author>M Frank</author>
<author>J Movellan</author>
<author>M Bartlett</author>
</authors>
<title>The computer expression recognition toolbox (cert).</title>
<date>2011</date>
<booktitle>In Automatic Face Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on,</booktitle>
<pages>298--305</pages>
<contexts>
<context position="18655" citStr="Littlewort et al., 2011" startWordPosition="2906" endWordPosition="2909">ing and describing facial behaviors is the Facial Action Coding System (FACS), which allows for the description of face muscle activities through the use of a set of Action Units (AUs). According with (Ekman, 1993), there are 64 AUs that involve the upper and lower face, including several face positions and movements.3 AUs can occur either by themselves or in combination, and can be used to identify a variety of emotions. While AUs are frequently annotated by certified human annotators, automatic tools are also available. In our work, we use the Computer Expression Recognition Toolbox (CERT) (Littlewort et al., 2011), which allows us to automatically extract the following visual features: • Smile and head pose estimates. The smile feature is an estimate for smiles. Head pose detection consists of three-dimensional estimates of the head orientation, i.e., yaw, pitch, and roll. These features provide information about changes in smiles and face positions while uttering positive and negative opinions. • Facial AUs. These features are the raw estimates for 30 facial AUs related to muscle movements for the eyes, eyebrows, nose, lips, 3http://www.cs.cmu.edu/afs/cs/project/face/www/facs.htm 977 and chin. They pr</context>
</contexts>
<marker>Littlewort, Whitehill, Wu, Fasel, Frank, Movellan, Bartlett, 2011</marker>
<rawString>G. Littlewort, J. Whitehill, Tingfan Wu, I. Fasel, M. Frank, J. Movellan, and M. Bartlett. 2011. The computer expression recognition toolbox (cert). In Automatic Face Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on, pages 298 –305, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maas</author>
<author>R Daly</author>
<author>P Pham</author>
<author>D Huang</author>
<author>A Ng</author>
<author>C Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL 2011),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="2039" citStr="Maas et al., 2011" startWordPosition="291" endWordPosition="294"> ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addition of speech and visual modalities to text ana</context>
<context position="6015" citStr="Maas et al., 2011" startWordPosition="897" endWordPosition="900">neral Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009),</context>
<context position="16173" citStr="Maas et al., 2011" startWordPosition="2524" endWordPosition="2527">ords, including stopwords, occurring in the transcriptions of the training set. We then remove those words that have a frequency below 10 (value determined empirically on a small development set). The remaining words represent the unigram features, which are then associated with a value corresponding to the frequency of the unigram inside each utterance transcription. These simple weighted unigram features have been successfully used in the past to build sentiment classifiers on text, and in conjunction with Support Vector Machines (SVM) have been shown to lead to state-ofthe-art performance (Maas et al., 2011). 4.1.2 Acoustic Features Acoustic features are automatically extracted from the speech signal of each utterance. We used the open source software OpenEAR (Schuller, 2009) to automatically compute a set of acoustic features. We include prosody, energy, voicing probabilities, spectrum, and cepstral features. • Prosody features. These include intensity, loudness, and pitch that describe the speech signal in terms of amplitude and frequency. • Energy features. These features describe the human loudness perception. • Voice probabilities. These are probabilities that represent an estimate of the pe</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the Association for Computational Linguistics (ACL 2011), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mairesse</author>
<author>J Polifroni</author>
<author>G Di Fabbrizio</author>
</authors>
<title>Can prosody inform sentiment analysis? experiments on short spoken reviews.</title>
<date>2012</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on,</booktitle>
<pages>5093--5096</pages>
<marker>Mairesse, Polifroni, Di Fabbrizio, 2012</marker>
<rawString>F. Mairesse, J. Polifroni, and G. Di Fabbrizio. 2012. Can prosody inform sentiment analysis? experiments on short spoken reviews. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 5093 –5096, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Meng</author>
<author>F Wei</author>
<author>X Liu</author>
<author>M Zhou</author>
<author>G Xu</author>
<author>H Wang</author>
</authors>
<title>Cross-lingual mixture model for sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="6270" citStr="Meng et al., 2012" startWordPosition="937" endWordPosition="940">nguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), wh</context>
</contexts>
<marker>Meng, Wei, Liu, Zhou, Xu, Wang, 2012</marker>
<rawString>X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang. 2012. Cross-lingual mixture model for sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Metze</author>
<author>T Polzehl</author>
<author>M Wagner</author>
</authors>
<title>Fusion of acoustic and linguistic features for emotion detection.</title>
<date>2009</date>
<booktitle>In Semantic Computing,</booktitle>
<pages>153--160</pages>
<contexts>
<context position="6614" citStr="Metze et al., 2009" startWordPosition="994" endWordPosition="997">; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech s</context>
</contexts>
<marker>Metze, Polzehl, Wagner, 2009</marker>
<rawString>F. Metze, T. Polzehl, and M. Wagner. 2009. Fusion of acoustic and linguistic features for emotion detection. In Semantic Computing, 2009. ICSC ’09. IEEE International Conference on, pages 153 –160, sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Banea</author>
<author>J Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6239" citStr="Mihalcea et al., 2007" startWordPosition="931" endWordPosition="934">11), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L P Morency</author>
<author>R Mihalcea</author>
<author>P Doshi</author>
</authors>
<title>Towards multimodal sentiment analysis: Harvesting opinions from the web.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Multimodal Computing,</booktitle>
<location>Alicante,</location>
<contexts>
<context position="2533" citStr="Morency et al., 2011" startWordPosition="365" endWordPosition="368">reated including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addition of speech and visual modalities to text analysis in order to identify the sentiment expressed in video reviews. Given the non homogeneous nature of full-video reviews, which typically include a mixture of positive, negative, and neutral statements, we decided to perform our experiments and analyses at the utterance level. This is in line with earlier work on text-based sentiment analysis, where it has been observed that full-document reviews often contain both positive and negative comments, which led to a number of methods address</context>
<context position="6839" citStr="Morency et al., 2011" startWordPosition="1029" endWordPosition="1032">ihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and </context>
</contexts>
<marker>Morency, Mihalcea, Doshi, 2011</marker>
<rawString>L.P. Morency, R. Mihalcea, and P. Doshi. 2011. Towards multimodal sentiment analysis: Harvesting opinions from the web. In Proceedings of the International Conference on Multimodal Computing, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Oh</author>
<author>K Torisawa</author>
<author>C Hashimoto</author>
<author>T Kawada</author>
<author>S De Saeger</author>
<author>J Kazama</author>
<author>Y Wang</author>
</authors>
<title>Why question answering using sentiment analysis and word classes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Jeju Island,</location>
<marker>Oh, Torisawa, Hashimoto, Kawada, De Saeger, Kazama, Wang, 2012</marker>
<rawString>J. Oh, K. Torisawa, C. Hashimoto, T. Kawada, S. De Saeger, J. Kazama, and Y. Wang. 2012. Why question answering using sentiment analysis and word classes. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5995" citStr="Pang and Lee, 2004" startWordPosition="893" endWordPosition="896">t analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; </context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Perez-Rosas</author>
<author>R Mihalcea</author>
<author>L-P Morency</author>
</authors>
<title>Multimodal sentiment analysis of spanish online videos.</title>
<date>2013</date>
<journal>IEEE Intelligent Systems.</journal>
<contexts>
<context position="6866" citStr="Perez-Rosas et al., 2013" startWordPosition="1033" endWordPosition="1036">Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely</context>
</contexts>
<marker>Perez-Rosas, Mihalcea, Morency, 2013</marker>
<rawString>V. Perez-Rosas, R. Mihalcea, and L.-P. Morency. 2013. Multimodal sentiment analysis of spanish online videos. IEEE Intelligent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Polzin</author>
<author>A Waibel</author>
</authors>
<title>Recognizing emotions in speech. In</title>
<date>1996</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="7737" citStr="Polzin and Waibel, 1996" startWordPosition="1179" endWordPosition="1182">sis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. </context>
</contexts>
<marker>Polzin, Waibel, 1996</marker>
<rawString>T. Polzin and A. Waibel. 1996. Recognizing emotions in speech. In In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raaijmakers</author>
<author>K Truong</author>
<author>T Wilson</author>
</authors>
<title>Multimodal subjectivity analysis of multiparty conversation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>466--474</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="6570" citStr="Raaijmakers et al., 2008" startWordPosition="986" endWordPosition="989">s consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on em</context>
</contexts>
<marker>Raaijmakers, Truong, Wilson, 2008</marker>
<rawString>S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multimodal subjectivity analysis of multiparty conversation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 466–474, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosenblum</author>
<author>Y Yacoob</author>
<author>L S Davis</author>
</authors>
<title>Human expression recognition from motion using a radial basis function network architecture. Neural Networks,</title>
<date>1996</date>
<journal>IEEE Transactions on,</journal>
<volume>7</volume>
<issue>5</issue>
<pages>1138</pages>
<contexts>
<context position="7923" citStr="Rosenblum et al., 1996" startWordPosition="1212" endWordPosition="1215">r her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to </context>
</contexts>
<marker>Rosenblum, Yacoob, Davis, 1996</marker>
<rawString>M. Rosenblum, Y. Yacoob, and L.S. Davis. 1996. Human expression recognition from motion using a radial basis function network architecture. Neural Networks, IEEE Transactions on, 7(5):1121 –1138, sep.</rawString>
</citation>
<citation valid="false">
<booktitle>2011a. Audio/Visual Emotion Challenge and Workshop</booktitle>
<editor>B. Schuller, M. Valstar, R. Cowie, and M. Pantic, editors.</editor>
<marker></marker>
<rawString>B. Schuller, M. Valstar, R. Cowie, and M. Pantic, editors. 2011a. Audio/Visual Emotion Challenge and Workshop (AVEC 2011).</rawString>
</citation>
<citation valid="false">
<booktitle>2011b. Audio/Visual Emotion Challenge and Workshop</booktitle>
<editor>B. Schuller, M. Valstar, F. Eyben, R. Cowie, and M. Pantic, editors.</editor>
<marker></marker>
<rawString>B. Schuller, M. Valstar, F. Eyben, R. Cowie, and M. Pantic, editors. 2011b. Audio/Visual Emotion Challenge and Workshop (AVEC 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Eyben M Wollmer B Schuller</author>
</authors>
<title>Openear introducing the munich open-source emotion and affect recognition toolkit.</title>
<date>2009</date>
<booktitle>In ACII.</booktitle>
<contexts>
<context position="16344" citStr="Schuller, 2009" startWordPosition="2550" endWordPosition="2551">mall development set). The remaining words represent the unigram features, which are then associated with a value corresponding to the frequency of the unigram inside each utterance transcription. These simple weighted unigram features have been successfully used in the past to build sentiment classifiers on text, and in conjunction with Support Vector Machines (SVM) have been shown to lead to state-ofthe-art performance (Maas et al., 2011). 4.1.2 Acoustic Features Acoustic features are automatically extracted from the speech signal of each utterance. We used the open source software OpenEAR (Schuller, 2009) to automatically compute a set of acoustic features. We include prosody, energy, voicing probabilities, spectrum, and cepstral features. • Prosody features. These include intensity, loudness, and pitch that describe the speech signal in terms of amplitude and frequency. • Energy features. These features describe the human loudness perception. • Voice probabilities. These are probabilities that represent an estimate of the percentage of voiced and unvoiced energy in the speech. • Spectral features. The spectral features are based on the characteristics of the human ear, which uses a nonlinear </context>
</contexts>
<marker>Schuller, 2009</marker>
<rawString>F. Eyben M. Wollmer B. Schuller. 2009. Openear introducing the munich open-source emotion and affect recognition toolkit. In ACII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sebe</author>
<author>I Cohen</author>
<author>T Gevers</author>
<author>T S Huang</author>
</authors>
<title>Emotion recognition based on joint visual and audio cues.</title>
<date>2006</date>
<booktitle>In ICPR.</booktitle>
<contexts>
<context position="8685" citStr="Sebe et al., 2006" startWordPosition="1335" endWordPosition="1338">ntentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010). 974 Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos Honestly, talking about how they looks and hydrates, yes they are very hydrant. Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg But the problem with those lipsticks is that when you apply them, they leave a very nasty taste Si</context>
</contexts>
<marker>Sebe, Cohen, Gevers, Huang, 2006</marker>
<rawString>N. Sebe, I. Cohen, T. Gevers, and T.S. Huang. 2006. Emotion recognition based on joint visual and audio cues. In ICPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Silva</author>
<author>T Miyasato</author>
<author>R Nakatsu</author>
</authors>
<title>Facial emotion recognition using multi-modal information.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Information and Communications Security.</booktitle>
<contexts>
<context position="8360" citStr="Silva et al., 1997" startWordPosition="1282" endWordPosition="1285">to et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010). 974 Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con respecto a lo que pinta y a que son hidra</context>
</contexts>
<marker>Silva, Miyasato, Nakatsu, 1997</marker>
<rawString>D. Silva, T. Miyasato, and R. Nakatsu. 1997. Facial emotion recognition using multi-modal information. In Proceedings of the International Conference on Information and Communications Security.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
<author>D Litman</author>
</authors>
<title>Manual annotation of opinion categories in meetings.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora</booktitle>
<contexts>
<context position="6544" citStr="Somasundaran et al., 2006" startWordPosition="982" endWordPosition="985">luding two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work </context>
</contexts>
<marker>Somasundaran, Wiebe, Hoffmann, Litman, 2006</marker>
<rawString>S. Somasundaran, J. Wiebe, P. Hoffmann, and D. Litman. 2006. Manual annotation of opinion categories in meetings. In Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stone</author>
</authors>
<title>General Inquirer: Computer Approach to Content Analysis.</title>
<date>1968</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5425" citStr="Stone, 1968" startWordPosition="804" endWordPosition="805">ata-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a co</context>
</contexts>
<marker>Stone, 1968</marker>
<rawString>P. Stone. 1968. General Inquirer: Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Semeval-2007 task 14: Affective text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on the Semantic Evaluations (SemEval</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6112" citStr="Strapparava and Mihalcea, 2007" startWordPosition="910" endWordPosition="913">tically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment id</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>C. Strapparava and R. Mihalcea. 2007. Semeval-2007 task 14: Affective text. In Proceedings of the 4th International Workshop on the Semantic Evaluations (SemEval 2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
<author>J Brooke</author>
<author>M Tofiloski</author>
<author>K Voli</author>
<author>M Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="5621" citStr="Taboada et al., 2011" startWordPosition="832" endWordPosition="835">pressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mih</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voli, Stede, 2011</marker>
<rawString>M. Taboada, J. Brooke, M. Tofiloski, K. Voli, and M. Stede. 2011. Lexicon-based methods for sentiment analysis. Computational Linguistics, 37(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tato</author>
<author>R Santos</author>
<author>R Kompe</author>
<author>J M Pardo</author>
</authors>
<title>Emotional space improves emotion recognition. In</title>
<date>2002</date>
<booktitle>In Proc. ICSLP</booktitle>
<pages>2029--2032</pages>
<contexts>
<context position="7756" citStr="Tato et al., 2002" startWordPosition="1183" endWordPosition="1186">work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1</context>
</contexts>
<marker>Tato, Santos, Kompe, Pardo, 2002</marker>
<rawString>R. Tato, R. Santos, R. Kompe, and J. M. Pardo. 2002. Emotional space improves emotion recognition. In In Proc. ICSLP 2002, pages 2029–2032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-I Tian</author>
<author>T Kanade</author>
<author>J F Cohn</author>
</authors>
<title>Recognizing action units for facial expression analysis.</title>
<date>2001</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>115</pages>
<contexts>
<context position="8097" citStr="Tian et al., 2001" startWordPosition="1239" endWordPosition="1242">e analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997). Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et</context>
</contexts>
<marker>Tian, Kanade, Cohn, 2001</marker>
<rawString>Y.-I. Tian, T. Kanade, and J.F. Cohn. 2001. Recognizing action units for facial expression analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(2):97 –115, feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>417--424</pages>
<location>Philadelphia.</location>
<contexts>
<context position="5580" citStr="Turney, 2002" startWordPosition="826" endWordPosition="827">er of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 200</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 417–424, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ververidis</author>
<author>C Kotropoulos</author>
</authors>
<title>Emotional speech recognition: Resources, features, and methods.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<volume>48</volume>
<issue>9</issue>
<contexts>
<context position="7347" citStr="Ververidis and Kotropoulos, 2006" startWordPosition="1114" endWordPosition="1117">tification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010). Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011). There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 199</context>
</contexts>
<marker>Ververidis, Kotropoulos, 2006</marker>
<rawString>D. Ververidis and C. Kotropoulos. 2006. Emotional speech recognition: Resources, features, and methods. Speech Communication, 48(9):1162–1181, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wagner</author>
<author>E Andre</author>
<author>F Lingenfelser</author>
<author>Jonghwa Kim</author>
</authors>
<title>Exploring fusion methods for multimodal emotion recognition with missing data. Affective Computing,</title>
<date>2011</date>
<journal>IEEE Transactions on,</journal>
<volume>2</volume>
<issue>4</issue>
<pages>218</pages>
<contexts>
<context position="2555" citStr="Wagner et al., 2011" startWordPosition="369" endWordPosition="372">ons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addition of speech and visual modalities to text analysis in order to identify the sentiment expressed in video reviews. Given the non homogeneous nature of full-video reviews, which typically include a mixture of positive, negative, and neutral statements, we decided to perform our experiments and analyses at the utterance level. This is in line with earlier work on text-based sentiment analysis, where it has been observed that full-document reviews often contain both positive and negative comments, which led to a number of methods addressing opinion analysis a</context>
</contexts>
<marker>Wagner, Andre, Lingenfelser, Kim, 2011</marker>
<rawString>J. Wagner, E. Andre, F. Lingenfelser, and Jonghwa Kim. 2011. Exploring fusion methods for multimodal emotion recognition with missing data. Affective Computing, IEEE Transactions on, 2(4):206 –218, oct.-dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association of Computational Linguistics and the International Joint Conference on Natural Language Processing,</booktitle>
<location>Singapore,</location>
<contexts>
<context position="6250" citStr="Wan, 2009" startWordPosition="935" endWordPosition="936">and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Ros</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>X. Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the Association of Computational Linguistics and the International Joint Conference on Natural Language Processing, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005) (invited paper),</booktitle>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="1962" citStr="Wiebe and Riloff, 2005" startWordPosition="279" endWordPosition="282">hers, and consumers. Popular web platforms such as YouTube, Amazon, Facebook, and ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In th</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>J. Wiebe and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of the 6th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005) (invited paper), Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="5807" citStr="Wiebe et al., 2005" startWordPosition="863" endWordPosition="866">estion answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalitie</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wiegand</author>
<author>D Klakow</author>
</authors>
<title>The role of knowledge-based features in polarity classification at sentence level.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference of the Florida Artificial Intelligence Research Society.</booktitle>
<contexts>
<context position="22885" citStr="Wiegand and Klakow, 2009" startWordPosition="3575" endWordPosition="3578">modalities at a time. We use the entire set of 412 utterances and run ten fold cross validations using an SVM classifier, as implemented in the Weka toolkit.5 In line with previous work on emotion recognition in speech (Haq and Jackson, 2009; Anagnostopoulos and Vovoli, 2010) where utterances are selected in a speaker dependent manner (i.e., utterances from the same speaker are included in both training and test), as well as work on sentence-level opinion classification where document boundaries are not considered in the split performed between the training and test sets (Wilson et al., 2004; Wiegand and Klakow, 2009), the training/test split for each fold is performed at utterance level regardless of the video they belong to. Table 2 shows the results of the utterance-level sentiment classification experiments. The baseline is obtained using the ZeroR classifier, which assigns the most frequent label by default, averaged over the ten folds. 6 Discussion The experimental results show that sentiment classification can be effectively performed on multimodal datastreams. Moreover, the integration of 5http://www.cs.waikato.ac.nz/ml/weka/ 978 Figure 2: Visual and acoustic feature weights. This graph shows the r</context>
</contexts>
<marker>Wiegand, Klakow, 2009</marker>
<rawString>M. Wiegand and D. Klakow. 2009. The role of knowledge-based features in polarity classification at sentence level. In Proceedings of the International Conference of the Florida Artificial Intelligence Research Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of the American Association forArtificial Intelligence.</booktitle>
<contexts>
<context position="22858" citStr="Wilson et al., 2004" startWordPosition="3570" endWordPosition="3574"> one, two, and three modalities at a time. We use the entire set of 412 utterances and run ten fold cross validations using an SVM classifier, as implemented in the Weka toolkit.5 In line with previous work on emotion recognition in speech (Haq and Jackson, 2009; Anagnostopoulos and Vovoli, 2010) where utterances are selected in a speaker dependent manner (i.e., utterances from the same speaker are included in both training and test), as well as work on sentence-level opinion classification where document boundaries are not considered in the split performed between the training and test sets (Wilson et al., 2004; Wiegand and Klakow, 2009), the training/test split for each fold is performed at utterance level regardless of the video they belong to. Table 2 shows the results of the utterance-level sentiment classification experiments. The baseline is obtained using the ZeroR classifier, which assigns the most frequent label by default, averaged over the ten folds. 6 Discussion The experimental results show that sentiment classification can be effectively performed on multimodal datastreams. Moreover, the integration of 5http://www.cs.waikato.ac.nz/ml/weka/ 978 Figure 2: Visual and acoustic feature weig</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? finding strong and weak opinion clauses. In Proceedings of the American Association forArtificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wollmer</author>
<author>B Schuller</author>
<author>F Eyben</author>
<author>G Rigoll</author>
</authors>
<title>Combining long short-term memory and dynamic bayesian networks for incremental emotionsensitive artificial listening.</title>
<date>2010</date>
<journal>IEEE Journal of Selected Topics in Signal Processing,</journal>
<volume>4</volume>
<issue>5</issue>
<contexts>
<context position="8730" citStr="Wollmer et al., 2010" startWordPosition="1343" endWordPosition="1346">an be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010). 974 Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos Honestly, talking about how they looks and hydrates, yes they are very hydrant. Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg But the problem with those lipsticks is that when you apply them, they leave a very nasty taste Sinceramente, es no es que sea el olor sino que</context>
</contexts>
<marker>Wollmer, Schuller, Eyben, Rigoll, 2010</marker>
<rawString>M. Wollmer, B. Schuller, F. Eyben, and G. Rigoll. 2010. Combining long short-term memory and dynamic bayesian networks for incremental emotionsensitive artificial listening. IEEE Journal of Selected Topics in Signal Processing, 4(5), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Yang</author>
<author>C Cardie</author>
</authors>
<title>Extracting opinion expressions with semi-markov conditional random fields.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="5704" citStr="Yang and Cardie, 2012" startWordPosition="845" endWordPosition="848">in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, </context>
</contexts>
<marker>Yang, Cardie, 2012</marker>
<rawString>B. Yang and C. Cardie. 2012. Extracting opinion expressions with semi-markov conditional random fields. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhihong</author>
<author>M Pantic G I Roisman</author>
<author>T S Huang</author>
</authors>
<title>A survey of affect recognition methods: Audio, visual, and spontaneous expressions.</title>
<date>2009</date>
<journal>PAMI,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="8707" citStr="Zhihong et al., 2009" startWordPosition="1339" endWordPosition="1342">al., 2001). Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002). De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010). 974 Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos Honestly, talking about how they looks and hydrates, yes they are very hydrant. Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg But the problem with those lipsticks is that when you apply them, they leave a very nasty taste Sinceramente, es no es q</context>
</contexts>
<marker>Zhihong, Roisman, Huang, 2009</marker>
<rawString>Z. Zhihong, M. Pantic G.I. Roisman, and T.S. Huang. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. PAMI, 31(1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>