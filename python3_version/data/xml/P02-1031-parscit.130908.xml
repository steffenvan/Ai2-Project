<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001433">
<note confidence="0.9505315">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 239-246.
</note>
<title confidence="0.979164">
The Necessity of Parsing for Predicate Argument Recognition
</title>
<author confidence="0.988292">
Daniel Gildea and Martha Palmer
</author>
<affiliation confidence="0.996263">
University of Pennsylvania
</affiliation>
<email confidence="0.998281">
dgildea,mpalmer@cis.upenn.edu
</email>
<sectionHeader confidence="0.980103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943692307692">
Broad-coverage corpora annotated with
semantic role, or argument structure, in-
formation are becoming available for the
first time. Statistical systems have been
trained to automatically label seman-
tic roles from the output of statistical
parsers on unannotated text. In this pa-
per, we quantify the effect of parser accu-
racy on these systems&apos; performance, and
examine the question of whether a flat-
ter &amp;quot;chunked&amp;quot; representation of the in-
put can be as effective for the purposes
of semantic role identification.
</bodyText>
<sectionHeader confidence="0.996301" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999565076923077">
Over the past decade, most work in the field of
information extraction has shifted from complex
rule-based, systems designed to handle a wide
variety of semantic phenomena including quan-
tification, anaphora, aspect and modality (e.g.
Alshawi (1992)), to simpler finite-state or sta-
tistical systems such as Hobbs et al. (1997) and
Miller et al. (1998). Much of the evaluation of
these systems has been conducted on extracting
relations for specific semantic domains such as
corporate acquisitions or terrorist events in the
framework of the DARPA Message Understand-
ing Conferences.
Recently, attention has turned to creating cor-
pora annotated for argument structure for a
broader range of predicates. The Propbank
project at the University of Pennsylvania (Kings-
bury and Palmer, 2002) and the FrameNet project
at the International Computer Science Institute
(Baker et al., 1998) share the goal of document-
ing the syntactic realization of arguments of the
predicates of the general English lexicon by an-
notating a corpus with semantic roles. Even for
a single predicate, semantic arguments often have
multiple syntactic realizations, as shown by the
following paraphrases:
</bodyText>
<listItem confidence="0.9358416">
(1) John will meet with Mary.
John will meet Mary.
John and Mary will meet.
(2) The door opened.
Mary opened the door.
</listItem>
<bodyText confidence="0.9997845">
Correctly identifying the semantic roles of the
sentence constituents is a crucial part of interpret-
ing text, and in addition to forming an important
part of the information extraction problem, can
serve as an intermediate step in machine trans-
lation or automatic summarization. In this pa-
per, we examine how the information provided by
modern statistical parsers such as Collins (1997)
and Charniak (1997) contributes to solving this
problem. We measure the effect of parser accu-
racy on semantic role prediction from parse trees,
and determine whether a complete tree is indeed
necessary for accurate role prediction.
Gildea and Jurafsky (2002) describe a statisti-
cal system trained on the data from the FrameNet
project to automatically assign semantic roles.
The system first passed sentences through an au-
tomatic parser, extracted syntactic features from
the parses, and estimated probabilities for seman-
tic roles from the syntactic and lexical features.
Both training and test sentences were automat-
ically parsed, as no hand-annotated parse trees
were available for the corpus. While the errors
introduced by the parser no doubt negatively af-
fected the results obtained, there was no direct
way of quantifying this effect. Of the systems
evaluated for the Message Understanding Confer-
ence task, Miller et al. (1998) made use of an inte-
grated syntactic and semantic model producing a
full parse tree, and achieved results comparable to
other systems that did not make use of a complete
parse. As in the FrameNet case, the parser was
not trained on the corpus for which semantic an-
notations were available, and the effect of better,
or even perfect, parses could not be measured.
One of the differences between the two semantic
annotation projects is that the sentences chosen
for annotation for Propbank are from the same
Wall Street Journal corpus chosen for annotation
for the original Penn Treebank project, and thus
hand-checked syntactic parse trees are available
for the entire dataset. In this paper, we com-
pare the performance of a system based on gold-
standard parses with one using automatically gen-
erated parser output. We also examine whether it
is possible that the additional information con-
tained in a full parse tree is negated by the errors
present in automatic parser output, by testing a
role-labeling system based on a flat or &amp;quot;chunked&amp;quot;
representation of the input.
</bodyText>
<sectionHeader confidence="0.946124" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.999980784090909">
The results in this paper are primarily derived
from the Propbank corpus, and will be compared
to earlier results from the FrameNet corpus. Be-
fore proceeding to the experiments, this section
will briefly describe the similarities and differences
between the two sets of data.
While the goals of the two projects are similar in
many respects, their methodologies are quite dif-
ferent. FrameNet is focused on semantic frames,
which are defined as schematic representation of
situations involving various participants, props,
and other conceptual roles (Fillmore, 1976). The
project methodology has proceeded on a frame-
by-frame basis, that is by first choosing a semantic
frame, defining the frame and its participants or
frame elements, and listing the various lexical
predicates which invoke the frame, and then find-
ing example sentences of each predicate in the cor-
pus (the British National Corpus was used) and
annotating each frame element. The example sen-
tences were chosen primarily for coverage of all
the syntactic realizations of the frame elements,
and simple examples of these realizations were
preferred over those involving complex syntactic
structure not immediate relevant to the lexical
predicate itself. From the perspective of an auto-
matic classification system, the overrepresentation
of rare syntactic realizations may cause the system
to perform more poorly than it might on more sta-
tistically representative data. On the other hand,
the exclusion of complex examples may make the
task artificially easy. Only sentences where the
lexical predicate was used &amp;quot;in frame&amp;quot; were anno-
tated. A word with multiple distinct senses would
generally be analyzed as belonging to different
frames in each sense, but may only be found in the
FrameNet corpus in the sense for which a frame
has been defined. It is interesting to note that the
semantic frames are a helpful way of generalizing
between predicates; words in the same frame have
been found frequently to share the same syntactic
argument structure. A more complete description
of the FrameNet project can be found in (Baker
et al., 1998; Johnson et al., 2001), and the rami-
fications for automatic classification are discussed
more thoroughly in (Gildea and Jurafsky, 2002).
The philosophy of the Propbank project can be
likened to FrameNet without frames. While the
semantic roles of FrameNet are defined at the level
of the frame, in Propbank, roles are defined on a
per-predicate basis. The core arguments of each
predicate are simply numbered, while remaining
arguments are given labels such as &amp;quot;temporal&amp;quot; or
&amp;quot;locative&amp;quot;. While the two types of label names are
reminiscent of the traditional argument/adjunct
distinction, this is primarily as a convenience in
defining roles, and no claims are intended as to
optionality or other traditional argument/adjunct
tests. To date, Propbank has addressed only
verbs, where FrameNet includes nouns and ad-
jectives. Propbank&apos;s annotation process has pro-
ceeded from the most to least common verbs, and
all examples of each verb from the corpus are an-
notated. Thus, the data for each predicate are
statistically representative of the corpus, as are
the frequencies of the predicates themselves. An-
notation takes place with reference to the Penn
Treebank trees not only are annotators shown
the trees when analyzing a sentence, they are con-
strained to assign the semantic labels to portions
of the sentence corresponding to nodes in the tree.
Propbank annotators tag all examples of a given
verb, regardless of word sense. The tagging guide-
lines for a verb may contain many &amp;quot;rolesets&amp;quot;, cor-
responding to word sense at a relatively coarse-
grained level. The need for multiple rolesets is
determined by the roles themselves, that is, uses
of the verb with different arguments are given sep-
arate rolesets. However, the preliminary version
of the data used in the experiments below are
not tagged for word sense, or for the roleset used.
Sense tagging is planned for a second pass through
the data. In many cases the roleset can be deter-
mined from the argument annotations themselves.
However, we did not make any attempt to distin-
guish sense in our experiments, and simply at-
tempted to predict argument labels based on the
identity of the lexical predicate.
</bodyText>
<sectionHeader confidence="0.959479" genericHeader="method">
3 The Experiments
</sectionHeader>
<bodyText confidence="0.99754508">
In previous work using the FrameNet corpus,
Gildea and Jurafsky (2002) developed a system to
predict semantic roles from sentences and their
parse trees as determined by the statistical parser
of Collins (1997). We will briefly review their
probability model before adapting the system to
handle unparsed data.
Probabilities of a parse constituent belonging
to a given semantic role were calculated from the
following features:
Phrase Type: This feature indicates the syntac-
tic type of the phrase expressing the semantic
roles: examples include noun phrase (NP),
verb phrase (VP), and clause (S). Phrase
types were derived automatically from parse
trees generated by the parser, as shown in
Figure 1. The parse constituent spanning
each set of words annotated as an argument
was found, and the constituent&apos;s nonterminal
label was taken as the phrase type. As an
example of how this feature is useful, in com-
munication frames, the SpEAKER is likely to
appear as a noun phrase, Topic as a prepo-
sitional phrase or noun phrase, and MEDiUM
as a prepositional phrase, as in: &amp;quot;We talked
about the proposal over the phone.&amp;quot; When
no parse constituent was found with bound-
aries matching those of an argument during
testing, the largest constituent beginning at
the argument&apos;s left boundary and lying en-
tirely within the element was used to calcu-
late the features.
Parse Tree Path: This feature is designed to
capture the syntactic relation of a constituent
to the predicate. It is defined as the path
from the predicate through the parse tree
to the constituent in question, represented
as a string of parse tree nonterminals linked
by symbols indicating upward or downward
movement through the tree, as shown in Fig-
ure 2. Although the path is composed as a
string of symbols, our systems will treat the
string as an atomic value. The path includes,
as the first element of the string, the part of
speech of the predicate, and, as the last ele-
ment, the phrase type or syntactic category
of the sentence constituent marked as an ar-
gument.
Position: This feature simply indicates whether
the constituent to be labeled occurs before
or after the predicate defining the semantic
frame. This feature is highly correlated with
grammatical function, since subjects will gen-
erally appear before a verb, and objects after.
This feature may overcome the shortcom-
ings of reading grammatical function from the
parse tree, as well as errors in the parser out-
put.
He ate some pancakes
Figure 2: In this example, the path from the pred-
icate ate to the argument He can be represented
as VBfVPfS�NP, with f indicating upward move-
ment in the parse tree and � downward movement.
Voice: The distinction between active and pas-
sive verbs plays an important role in the con-
nection between semantic role and grammat-
ical function, since direct objects of active
verbs correspond to subjects of passive verbs.
From the parser output, verbs were classified
as active or passive by building a set of 10
passive-identifying patterns. Each of the pat-
terns requires both a passive auxiliary (some
form of &amp;quot;to be&amp;quot; or &amp;quot;to get&amp;quot;) and a past par-
ticiple.
Head Word: Lexical dependencies provide im-
portant information in labeling semantic
roles, as one might expect from their use
in statistical models for parsing. Since the
parser used assigns each constituent a head
word as an integral part of the parsing model,
the head words of the constituents can be
read from the parser output. For example, in
a communication frame, noun phrases headed
by &amp;quot;Bill&amp;quot;, &amp;quot;brother&amp;quot;, or &amp;quot;he&amp;quot; are more likely
to be the SpEAKER, while those headed by
&amp;quot;proposal&amp;quot;, &amp;quot;story&amp;quot;, or &amp;quot;question&amp;quot; are more
likely to be the Topic.
To predict argument roles in new data, we
wish to estimate the probability of each role
given these five features and the predicate p:
P(rlpt, path, position, voice, hw, p). Due to the
sparsity of the data, it is not possible to estimate
this probability from the counts in the training.
Instead, we estimate probabilities from various
subsets of the features, and interpolate a linear
combination of the resulting distributions. The
interpolation is performed over the most specific
distributions for which data are available, which
can be thought of as choosing the topmost distri-
butions available from a backoff lattice, shown in
</bodyText>
<figureCaption confidence="0.616129">
Figure 3.
</figureCaption>
<figure confidence="0.998637">
S
VP
NP
NP
DT NN
PRP
VB
</figure>
<figureCaption confidence="0.9298875">
Figure 1: A sample sentence with parser output (above) and argument structure annotation (below).
Parse constituents corresponding to frame elements are highlighted.
</figureCaption>
<figure confidence="0.9996996">
him
as
Farrell
behind
from
the sound of liquid slurping in a metal container
approached
heard
He
S
VP
NP
PRP
NP
VBD
SBAR
IN
S
VP
NP
NNP
VBD
NP
PP
PRP
IN
NP
NN
predicate Goal Source
Theme
</figure>
<figureCaption confidence="0.9923315">
Figure 3: Backoff lattice with more specific distri-
butions towards the top.
</figureCaption>
<bodyText confidence="0.999892042553192">
We applied the same system, using the same
features to a preliminary release of the Propbank
data. The dataset used contained annotations for
26,138 predicate-argument structures containing
65,364 individual arguments and containing exam-
ples from 1,527 lexical predicates (types). In order
to provide results comparable with the statistical
parsing literature, annotations from Section 23 of
the Treebank were used as the test set; all other
sections were included in the training set.
The system was tested under two conditions,
one in which it is given the constituents which
are arguments to the predicate and merely has to
predict the correct role, and one in which it has to
both find the arguments in the sentence and label
them correctly. Results are shown in Tables 1 and
2.
Although results for Propbank are lower than
for FrameNet, this appears to be primarily due to
the smaller number of training examples for each
predicate, rather than the difference in annotation
style between the two corpora. The FrameNet
data contained at least ten examples from each
predicate, while 17% of the Propbank data had
fewer than ten training examples. Removing these
examples from the test set gives 84.1% accuracy
with gold-standard parses and 80.5% accuracy
with automatic parses.
As our path feature is a somewhat unusual way
of looking at parse trees, its behavior in the sys-
tem warrants a closer look. The path feature is
most useful as a way of finding arguments in the
unknown boundary condition. Removing the path
feature from the known-boundary system results
in only a small degradation in performance, from
82.3% to 81.7%. One reason for the relatively
small impact may be sparseness of the feature �
7% of paths in the test set are unseen in training
data. The most common values of the feature are
shown in Table 3, where the first two rows cor-
respond to standard subject and object positions.
One reason for sparsity is seen in the third row:
in the Treebank, the adjunction of an adverbial
phrase or modal verb can cause an additional VP
node to appear in our path feature. We tried two
variations of the path feature to address this prob-
lem. The first collapses sequences of nodes with
</bodyText>
<equation confidence="0.995896875">
P(r  |h)
P(r  |p)
P(r  |h, pt, p)
P(r  |pt, path, p)
P(r  |pt, voice, p)
P(r  |h, p)
P(r  |pt, p)
P(r  |pt, voice)
</equation>
<figure confidence="0.950916714285714">
Accuracy
FrameNet Propbank Propbank
&gt; 10 ex.
Gold-standard parses
Automatic parses
82.8 84.1
82.0 79.2 80.5
</figure>
<tableCaption confidence="0.6904155">
Table 1: Accuracy of semantic role prediction for known boundaries the system is given the con-
stituents to classify.
</tableCaption>
<figure confidence="0.957581363636363">
FrameNet Propbank Propbank &gt; 10
Precision Recall
Precision Recall
Precision Recall
64.6 61.2
71.1 64.4
57.7 50.0
Gold-standard parses
Automatic parses
73.5 71.7
59.0 55.4
</figure>
<tableCaption confidence="0.947065">
Table 2: Accuracy of semantic role prediction for unknown boundaries the system must identify the
correct constituents as arguments and give them the correct roles.
</tableCaption>
<table confidence="0.9998397">
Path Frequency
VB&amp;quot;VP#NP 17.6%
VB&amp;quot;VP&amp;quot;S#NP 16.4
VB&amp;quot;VP&amp;quot;VP&amp;quot;S#NP 7.8
VB&amp;quot;VP#PP 7.6
VB&amp;quot;VP#PP#NP 7.3
VB&amp;quot;VP#SBAR#S 4.3
VB&amp;quot;VP#S 4.3
VB&amp;quot;VP#ADVP 2.4
1031 others 76.0
</table>
<tableCaption confidence="0.987453">
Table 3: Common values for parse tree path in
Propbank data, using gold-standard parses.
</tableCaption>
<bodyText confidence="0.999167230769231">
the same label, for example combining rows 2 and
3 of Table 3. The second variation uses only two
values for the feature: NP under S (subject posi-
tion), and NP under VP (object position). Nei-
ther variation improved performance in the known
boundary condition. As a gauge of how closely the
Propbank argument labels correspond to the path
feature overall, we note that by always assigning
the most common role for each path, for example
always assigning ARG0 to the subject position,
and using no other features, we obtain the correct
role 69.4% of the time, vs. 82.3% for the complete
system.
</bodyText>
<sectionHeader confidence="0.963536" genericHeader="method">
4 Is Parsing Necessary?
</sectionHeader>
<bodyText confidence="0.985967915662651">
Many recent information extraction systems for
limited domains have relied on finite-state systems
that do not build a full parse tree for the sentence
being analyzed. Among such systems, (Hobbs et
al., 1997) built finite-state recognizers for vari-
ous entities, which were then cascaded to form
recognizers for higher-level relations, while (Ray
and Craven, 2001) used low-level &amp;quot;chunks&amp;quot; from
a general-purpose syntactic analyzer as observa-
tions in a trained Hidden Markov Model. Such
an approach has a large advantage in speed, as
the extensive search of modern statistical parsers
is avoided. It is also possible that this approach
may be more robust to error than parsers. Al-
though we expect the attachment decisions made
by a parser to be relevant to determining whether
a constituent of a sentence is an argument of a
particular predicate, and what its relation to the
predicate is, those decisions may be so frequently
incorrect that a much simpler system can do just
as well. In this section we test this hypothesis
by comparing a system which is given only a flat,
&amp;quot;chunked&amp;quot; representation of the input sentence to
the parse-tree-based systems described above. In
this representation, base-level constituent bound-
aries and labels are present, but there are no de-
pendencies between constituents, as shown by the
following sample sentence:
(3) [NP Big investment banks] [V P refused to
step] [ADVP up] [PP to] [NP the plate]
[V P to support] [NP the beleaguered floor
traders] [PP by] [V P buying] [NP big blocks]
[PP of] [NP stock] , [NP traders] [V P say] .
Our chunks were derived from the Tree-
bank trees using the conversion described by
Tjong Kim Sang and Buchholz (2000). Thus,
the experiments were carried out using &amp;quot;gold-
standard&amp;quot; rather than automatically derived
chunk boundaries, which we believe will provide
an upper bound on the performance of a chunk-
based system.
The information provided by the parse tree can
be decomposed into three pieces: the constituent
boundaries, the grammatical relationship between
predicate and argument, expressed by our path
feature, and the head word of each candidate con-
stituent. We will examine the contribution of each
of these information sources, beginning with the
problem of assigning the correct role in the case
where the boundaries of the arguments in the sen-
tence are known, and then turning to the problem
of finding arguments in the sentence.
When the argument boundaries are known, the
grammatical relationship of the the constituent
to the predicate turns out to be of little value.
Removing the path feature from the system de-
scribed above results in only a small degradation
in performance, from 82.3% to 81.7%. While the
path feature serves to distinguish subjects from
objects, the combination of the constituent po-
sition before or after the predicate and the ac-
tive/passive voice feature serves the same purpose.
However, this result still makes use of the parser
output for finding the constituent&apos;s head word.
We implemented a simple algorithm to guess the
argument&apos;s head word from the chunked output: if
the argument begins at a chunk boundary, taking
the last word of the chunk, and in all other cases,
taking the first word of the argument. This heuris-
tic matches the head word read from the parse tree
77% of the the time, as it correctly identifies the
final word of simple noun phrases as the head, the
preposition as the head of prepositional phrases,
and the complementizer as the head of sentential
complements. Using this process for determining
head words, the system drops to 77.0% accuracy,
indicating that identifying the relevant head word
from semantic role prediction is in itself an impor-
tant function of the parser. This chunker-based re-
sult is only slightly lower than the 79.2% obtained
using automatic parses in the known boundary
condition. These results for the known boundary
condition are summarized in Table 4.
</bodyText>
<subsubsectionHeader confidence="0.569375">
Path Head Accuracy
</subsubsectionHeader>
<bodyText confidence="0.934422676923077">
gold parse
auto parse
gold parse
chunks
Table 4: Summary of results for known boundary
condition
We might expect the information provided by
the parser to be more important in identifying the
arguments in the sentence than in assigning them
the correct role. While it is easy to guess whether
a noun phrase is a subject or object given only
its position relative to the predicate, identifying
complex noun phrases and determining whether
they are arguments of a verb may be more difficult
without the attachment information provided by
the parser.
To test this, we implemented a system in which
the argument labels were assigned to chunks, with
the path feature used by the parse-tree-based sys-
tem replaced by a number expressing the distance
in chunks to the left or right of the predicate.
Of the 3990 arguments in our test set, only
39.8% correspond to a single chunk in the flat-
tened sentence representation, giving an upper
bound to the performance of this system. In par-
ticular, sentential complements (which comprise
11% of the data) and prepositional phrases (which
comprise 10%) always correspond to more than
one chunk, and therefore cannot be correctly la-
beled by our system which assigns roles to single
chunks. In fact, this system achieves 27.6% preci-
sion and 22.0% recall.
In order to see how much of the performance
degradation is caused by the difficulty of finding
exact argument boundaries in the chunked rep-
resentation, we can relax the scoring criteria to
count as correct all cases where the system cor-
rectly identifies the first chunk belonging to an
argument. For example, if the system assigns the
correct label to the preposition beginning a prepo-
sitional phrase, the argument will be counted as
correct, even though the system does not find
the argument&apos;s righthand boundary. With this
scoring regime, the chunk-based system performs
at 49.5% precision and 35.1% recall, still signifi-
cantly lower than the 57.7% precision/50.0% recall
for exact matches using automatically generated
parses. Results for the unknown boundary condi-
tion are summarized in Table 5.
gold parse
auto parse
chunk
chunk, relaxed scoring
Table 5: Summary of results for unknown bound-
ary condition
As an example for comparing the behavior of
the tree-based and chunk-based systems, consider
the following sentence, with human annotations
showing the arguments of the predicate support:
(4) [ARG0 Big investment banks] refused to step
up to the plate to support [ARG1 the
beleaguered floor traders] [MNR by buying
big blocks of stock] , traders say .
Our tree-based system assigned the following anal-
ysis:
</bodyText>
<listItem confidence="0.606476">
(5) Big investment banks refused to step up to
the plate to support [ARG1 the beleaguered
</listItem>
<figure confidence="0.991906923076923">
not used
not used
gold parse
auto parse
82.3
79.2
81.7
77.0
Precision Recall
71.1 64.4
57.7 50.0
27.6 22.0
49.5 35.1
</figure>
<bodyText confidence="0.999768672413793">
floor traders] [MNR by buying big blocks of
stock] , traders say .
In this case, the system failed to find the predi-
cate&apos;s ARG0 relation, because it is syntactically
distant from the verb support. The original Tree-
bank syntactic tree contains a trace which would
allow one to recover this relation, co-indexing the
empty subject position of support with the noun
phrase &amp;quot;Big investment banks&amp;quot;. However, our
automatic parser output does not include such
traces, nor does our system make use of them.
The chunk-based system assigns the following ar-
gument labels:
(6) Big investment banks refused to step up to
[ARG0 the plate] to support [ARG1 the
beleaguered floor traders] by buying big
blocks of stock , traders say .
Here, as before, the true ARG0 relation is not
found, and it would be difficult to imagine iden-
tifying it without building a complete syntactic
parse of the sentence. But now, unlike in the
tree-based output, the ARG0 label is mistakenly
attached to a noun phrase immediately before the
predicate. The ARG1 relation in direct object po-
sition is fairly easily identifiable in the chunked
representation as a noun phrase directly follow-
ing the verb. The prepositional phrase expressing
the Manner relation, however, is not identified by
the chunk-based system. The tree-based system&apos;s
path feature for this constituent is VBTVP�PP,
which identifies the prepositional phrase as at-
taching to the verb, and increases its probability
of being assigned an argument label. The chunk-
based system sees this as a prepositional phrase
appearing as the second chunk after the predi-
cate. Although this may be a typical position for
the Manner relation, the fact that the preposition
attaches to the predicate rather than to its direct
object is not represented.
In interpreting these results, it is important to
keep in mind the differences between this task
and other information extraction datasets. In
comparison to the domain-specific relations eval-
uated by the Message Understanding Conference
(MUC) tasks, we have a wider variety of relations
but fewer training instances for each. The rela-
tions may themselves be less susceptible to finite
state methods. For example, a named-entity sys-
tem which indentifies corporation names can go
a long way towards finding the &amp;quot;employment&amp;quot; re-
lation of MUC, and similarly systems for tagging
genes and proteins help a great deal for relations
in the biomedical domain. Both Propbank and
FrameNet tend to include longer arguments with
internal syntactic structure, making parsing deci-
sions more important in finding argument bound-
aries. They also involve abstract relations, with a
wide variety of possible fillers for each role.
</bodyText>
<sectionHeader confidence="0.992118" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999953633333333">
Our chunk-based system takes the last word of
the chunk as its head word for the purposes of
predicting roles, but does not make use of the
identities of the chunk&apos;s other words or the inter-
vening words between a chunk and the predicate,
unlike Hidden Markov Model-like systems such as
Bikel et al. (1997), McCallum et al. (2000) and
Lafferty et al. (2001). While a more elaborate
finite-state system might do better, it is possible
that additional features would not be helpful given
the small amount of data for each predicate. By
using a gold-standard chunking representation, we
have obtained higher performance over what could
be expected from an entirely automatic system
based on a flat representation of the data.
We feel that our results show that statistical
parsers, although computationally expensive, do
a good job of providing relevant information for
semantic interpretation. Not only the constituent
structure but also head word information, pro-
duced as a side product, are important features.
Parsers, however, still have a long way to go.
Our results using hand-annotated parse trees show
that improvements in parsing should translate di-
rectly into better semantic interpretations.
Acknowledgments This work was undertaken
with funding from the Institute for Research in
Cognitive Science at the University of Pennsylva-
nia and from the Propbank project, DoD Grant
MDA904-00C-2136.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999523093333333">
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press, Cambridge, MA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Proceedings of COLING/ACL, pages 86{90,
Montreal, Canada.
D. M. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI-97, pages 598{603, Menlo Park, August.
AAAI Press.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 16{23, Madrid, Spain.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
for information extraction. In Seventeenth In-
ternational Joint Conference on Artificial Intel-
ligence (IJCAI-01), Seattle, Washington.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the conll-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, Lisbon, Portugal.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, in press.
Jerry R. Hobbs, Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark E.
Stickel, and Mabry Tyson. 1997. FASTUS:
A cascaded finite-state transducer for extract-
ing information from natural-language text. In
Emmanuel Roche and Yves Schabes, editors,
Finite-State Language Processing, pages 383{
406. MIT Press, Cambridge, MA.
Christopher R. Johnson, Charles J. Fillmore,
Esther J. Wood, Josef Ruppenhofer, Mar-
garet Urban, Miriam R. L. Petruk, and
Collin F. Baker. 2001. The FrameNet
project: Tools for lexicon building. Version 0.7,
http://www.icsi.berkeley.edu/~framenet/book.html.
Paul Kingsbury and Martha Palmer. 2002. From
Treebank to PropBank. In Proceedings of the
3rd International Conference on Language Re-
sources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and label-
ing sequence data. In Machine Learning: Pro-
ceedings of the Eighteenth International Confer-
ence (ICML 2001), Stanford, California.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmenta-
tion. In Machine Learning: Proceedings of the
Seventeenth International Conference (ICML
2000), pages 591{598, Stanford, California.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion — BBN: Description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
April.
Soumya Ray and Mark Craven. 2001. Represent-
ing sentence structure in hidden markov model
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985610">
<note confidence="0.997685">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 239-246.</note>
<title confidence="0.999315">The Necessity of Parsing for Predicate Argument Recognition</title>
<author confidence="0.999975">Daniel Gildea</author>
<author confidence="0.999975">Martha Palmer</author>
<affiliation confidence="0.999514">University of Pennsylvania</affiliation>
<email confidence="0.999866">dgildea,mpalmer@cis.upenn.edu</email>
<abstract confidence="0.999335142857143">Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems&apos; performance, and examine the question of whether a flatter &amp;quot;chunked&amp;quot; representation of the input can be as effective for the purposes of semantic role identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine.</title>
<date>1992</date>
<editor>Hiyan Alshawi, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1063" citStr="(1992)" startWordPosition="154" endWordPosition="154">ed to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems&apos; performance, and examine the question of whether a flatter &amp;quot;chunked&amp;quot; representation of the input can be as effective for the purposes of semantic role identification. 1 Introduction Over the past decade, most work in the field of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quantification, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science I</context>
</contexts>
<marker>1992</marker>
<rawString>Hiyan Alshawi, editor. 1992. The Core Language Engine. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1692" citStr="Baker et al., 1998" startWordPosition="248" endWordPosition="251">ler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles. Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction pr</context>
<context position="6605" citStr="Baker et al., 1998" startWordPosition="1037" endWordPosition="1040">les may make the task artificially easy. Only sentences where the lexical predicate was used &amp;quot;in frame&amp;quot; were annotated. A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure. A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002). The philosophy of the Propbank project can be likened to FrameNet without frames. While the semantic roles of FrameNet are defined at the level of the frame, in Propbank, roles are defined on a per-predicate basis. The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &amp;quot;temporal&amp;quot; or &amp;quot;locative&amp;quot;. While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is prim</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of COLING/ACL, pages 86{90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="26996" citStr="Bikel et al. (1997)" startWordPosition="4413" endWordPosition="4416"> for relations in the biomedical domain. Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in finding argument boundaries. They also involve abstract relations, with a wide variety of possible fillers for each role. 5 Conclusion Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk&apos;s other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al. (2000) and Lafferty et al. (2001). While a more elaborate finite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate. By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data. We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation. Not only the c</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a highperformance learning name-finder. In Proceedings of the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In AAAI-97,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park,</location>
<contexts>
<context position="2511" citStr="Charniak (1997)" startWordPosition="382" endWordPosition="383">uments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatic</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In AAAI-97, pages 598{603, Menlo Park, August. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="2491" citStr="Collins (1997)" startWordPosition="379" endWordPosition="380">icate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sent</context>
<context position="8989" citStr="Collins (1997)" startWordPosition="1426" endWordPosition="1427">ts below are not tagged for word sense, or for the roleset used. Sense tagging is planned for a second pass through the data. In many cases the roleset can be determined from the argument annotations themselves. However, we did not make any attempt to distinguish sense in our experiments, and simply attempted to predict argument labels based on the identity of the lexical predicate. 3 The Experiments In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). We will briefly review their probability model before adapting the system to handle unparsed data. Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S). Phrase types were derived automatically from parse trees generated by the parser, as shown in Figure 1. The parse constituent spanning each set of words annotated as an argument was found, and the constituent&apos;s no</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL, pages 16{23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language.</title>
<date>1976</date>
<booktitle>In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<volume>280</volume>
<pages>pages</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="5070" citStr="Fillmore, 1976" startWordPosition="792" endWordPosition="793">&amp;quot;chunked&amp;quot; representation of the input. 2 The Data The results in this paper are primarily derived from the Propbank corpus, and will be compared to earlier results from the FrameNet corpus. Before proceeding to the experiments, this section will briefly describe the similarities and differences between the two sets of data. While the goals of the two projects are similar in many respects, their methodologies are quite different. FrameNet is focused on semantic frames, which are defined as schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore, 1976). The project methodology has proceeded on a frameby-frame basis, that is by first choosing a semantic frame, defining the frame and its participants or frame elements, and listing the various lexical predicates which invoke the frame, and then finding example sentences of each predicate in the corpus (the British National Corpus was used) and annotating each frame element. The example sentences were chosen primarily for coverage of all the syntactic realizations of the frame elements, and simple examples of these realizations were preferred over those involving complex syntactic structure not</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J. Fillmore. 1976. Frame semantics and the nature of language. In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, volume 280, pages 20{32. for information extraction. In Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-01), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="18939" citStr="Sang and Buchholz (2000)" startWordPosition="3093" endWordPosition="3096">n only a flat, &amp;quot;chunked&amp;quot; representation of the input sentence to the parse-tree-based systems described above. In this representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (3) [NP Big investment banks] [V P refused to step] [ADVP up] [PP to] [NP the plate] [V P to support] [NP the beleaguered floor traders] [PP by] [V P buying] [NP big blocks] [PP of] [NP stock] , [NP traders] [V P say] . Our chunks were derived from the Treebank trees using the conversion described by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried out using &amp;quot;goldstandard&amp;quot; rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunkbased system. The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate constituent. We will examine the contribution of each of these information sources, beginning with the problem of assigning the correct role in the case where the</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles. Computational Linguistics,</title>
<date>2002</date>
<note>in press.</note>
<contexts>
<context position="2750" citStr="Gildea and Jurafsky (2002)" startWordPosition="417" endWordPosition="420">ng the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect. Of the systems ev</context>
<context position="6741" citStr="Gildea and Jurafsky, 2002" startWordPosition="1057" endWordPosition="1060">ith multiple distinct senses would generally be analyzed as belonging to different frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure. A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002). The philosophy of the Propbank project can be likened to FrameNet without frames. While the semantic roles of FrameNet are defined at the level of the frame, in Propbank, roles are defined on a per-predicate basis. The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &amp;quot;temporal&amp;quot; or &amp;quot;locative&amp;quot;. While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a convenience in defining roles, and no claims are intended as to optionality or other traditional argument/adjunct tests. To d</context>
<context position="8849" citStr="Gildea and Jurafsky (2002)" startWordPosition="1402" endWordPosition="1405">lves, that is, uses of the verb with different arguments are given separate rolesets. However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used. Sense tagging is planned for a second pass through the data. In many cases the roleset can be determined from the argument annotations themselves. However, we did not make any attempt to distinguish sense in our experiments, and simply attempted to predict argument labels based on the identity of the lexical predicate. 3 The Experiments In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). We will briefly review their probability model before adapting the system to handle unparsed data. Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S). Phrase types were derived automatically from parse trees generated by the</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Douglas Appelt</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Mark E Stickel</author>
<author>Mabry Tyson</author>
</authors>
<title>FASTUS: A cascaded finite-state transducer for extracting information from natural-language text.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing,</booktitle>
<pages>383--406</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1140" citStr="Hobbs et al. (1997)" startWordPosition="164" endWordPosition="167">istical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems&apos; performance, and examine the question of whether a flatter &amp;quot;chunked&amp;quot; representation of the input can be as effective for the purposes of semantic role identification. 1 Introduction Over the past decade, most work in the field of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quantification, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic rea</context>
<context position="17465" citStr="Hobbs et al., 1997" startWordPosition="2846" endWordPosition="2849">improved performance in the known boundary condition. As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system. 4 Is Parsing Necessary? Many recent information extraction systems for limited domains have relied on finite-state systems that do not build a full parse tree for the sentence being analyzed. Among such systems, (Hobbs et al., 1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while (Ray and Craven, 2001) used low-level &amp;quot;chunks&amp;quot; from a general-purpose syntactic analyzer as observations in a trained Hidden Markov Model. Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided. It is also possible that this approach may be more robust to error than parsers. Although we expect the attachment decisions made by a parser to be relevant to determining whether a constituent of a sentence is an</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kameyama, Stickel, Tyson, 1997</marker>
<rawString>Jerry R. Hobbs, Douglas Appelt, John Bear, David Israel, Megumi Kameyama, Mark E. Stickel, and Mabry Tyson. 1997. FASTUS: A cascaded finite-state transducer for extracting information from natural-language text. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, pages 383{ 406. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christopher R Johnson</author>
<author>Charles J Fillmore</author>
<author>Esther J Wood</author>
<author>Josef Ruppenhofer</author>
<author>Margaret Urban</author>
<author>Miriam R L Petruk</author>
</authors>
<location>and</location>
<marker>Johnson, Fillmore, Wood, Ruppenhofer, Urban, Petruk, </marker>
<rawString>Christopher R. Johnson, Charles J. Fillmore, Esther J. Wood, Josef Ruppenhofer, Margaret Urban, Miriam R. L. Petruk, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
</authors>
<title>The FrameNet project: Tools for lexicon building.</title>
<date>2001</date>
<note>Version 0.7, http://www.icsi.berkeley.edu/~framenet/book.html.</note>
<marker>Baker, 2001</marker>
<rawString>Collin F. Baker. 2001. The FrameNet project: Tools for lexicon building. Version 0.7, http://www.icsi.berkeley.edu/~framenet/book.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="1598" citStr="Kingsbury and Palmer, 2002" startWordPosition="233" endWordPosition="237">antic phenomena including quantification, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles. Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of in</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001),</booktitle>
<location>Stanford, California.</location>
<contexts>
<context position="27047" citStr="Lafferty et al. (2001)" startWordPosition="4422" endWordPosition="4425">opbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in finding argument boundaries. They also involve abstract relations, with a wide variety of possible fillers for each role. 5 Conclusion Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk&apos;s other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al. (2000) and Lafferty et al. (2001). While a more elaborate finite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate. By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data. We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation. Not only the constituent structure but also head word information</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001), Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Machine Learning: Proceedings of the Seventeenth International Conference (ICML</booktitle>
<pages>591--598</pages>
<location>Stanford, California.</location>
<contexts>
<context position="27020" citStr="McCallum et al. (2000)" startWordPosition="4417" endWordPosition="4420"> biomedical domain. Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in finding argument boundaries. They also involve abstract relations, with a wide variety of possible fillers for each role. 5 Conclusion Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk&apos;s other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al. (2000) and Lafferty et al. (2001). While a more elaborate finite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate. By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data. We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation. Not only the constituent structure but</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Machine Learning: Proceedings of the Seventeenth International Conference (ICML 2000), pages 591{598, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<title>and the Annotation Group.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<contexts>
<context position="1165" citStr="Miller et al. (1998)" startWordPosition="169" endWordPosition="172">otated text. In this paper, we quantify the effect of parser accuracy on these systems&apos; performance, and examine the question of whether a flatter &amp;quot;chunked&amp;quot; representation of the input can be as effective for the purposes of semantic role identification. 1 Introduction Over the past decade, most work in the field of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quantification, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of </context>
<context position="3425" citStr="Miller et al. (1998)" startWordPosition="522" endWordPosition="525"> the FrameNet project to automatically assign semantic roles. The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect. Of the systems evaluated for the Message Understanding Conference task, Miller et al. (1998) made use of an integrated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse. As in the FrameNet case, the parser was not trained on the corpus for which semantic annotations were available, and the effect of better, or even perfect, parses could not be measured. One of the differences between the two semantic annotation projects is that the sentences chosen for annotation for Propbank are from the same Wall Street Journal corpus chosen for annotation for the original Penn Treebank project, and t</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, Ralph Weischedel, and the Annotation Group. 1998. Algorithms that learn to extract information — BBN: Description of the SIFT system as used for MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumya Ray</author>
<author>Mark Craven</author>
</authors>
<date>2001</date>
<note>Representing sentence structure in hidden markov model</note>
<contexts>
<context position="17620" citStr="Ray and Craven, 2001" startWordPosition="2868" endWordPosition="2871">ote that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system. 4 Is Parsing Necessary? Many recent information extraction systems for limited domains have relied on finite-state systems that do not build a full parse tree for the sentence being analyzed. Among such systems, (Hobbs et al., 1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while (Ray and Craven, 2001) used low-level &amp;quot;chunks&amp;quot; from a general-purpose syntactic analyzer as observations in a trained Hidden Markov Model. Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided. It is also possible that this approach may be more robust to error than parsers. Although we expect the attachment decisions made by a parser to be relevant to determining whether a constituent of a sentence is an argument of a particular predicate, and what its relation to the predicate is, those decisions may be so frequently incorrect that a much simpler system c</context>
</contexts>
<marker>Ray, Craven, 2001</marker>
<rawString>Soumya Ray and Mark Craven. 2001. Representing sentence structure in hidden markov model</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>