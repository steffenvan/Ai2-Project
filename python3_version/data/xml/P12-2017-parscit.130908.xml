<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.112380">
<title confidence="0.9844325">
Using Rejuvenation to Improve Particle Filtering for Bayesian Word
Segmentation
</title>
<author confidence="0.991628">
Benjamin B¨orschinger*† Mark Johnson*
</author>
<email confidence="0.665029">
benjamin.borschinger@mq.edu.au mark.johnson@mq.edu.au
</email>
<affiliation confidence="0.977351">
*Department of Computing †Department of Computational Linguistics
Macquarie University Heidelberg University
Sydney, Australia Heidelberg, Germany
</affiliation>
<sectionHeader confidence="0.986995" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999127625">
We present a novel extension to a recently pro-
posed incremental learning algorithm for the
word segmentation problem originally intro-
duced in Goldwater (2006). By adding rejuve-
nation to a particle filter, we are able to consid-
erably improve its performance, both in terms
of finding higher probability and higher accu-
racy solutions.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918535714286">
The goal of word segmentation is to segment a
stream of segments, e.g. characters or phonemes,
into words. For example, given the sequence
“youwanttoseethebook”, the goal is to recover the
segmented string “you want to see the book”. The
models introduced in Goldwater (2006) solve this
problem in a fully unsupervised way by defining a
generative process for word sequences, making use
of the Dirichlet Process (DP) prior.
Until recently, the only inference algorithm
applied to these models were batch Markov
Chain Monte Carlo (MCMC) sampling algorithms.
B¨orschinger and Johnson (2011) proposed a strictly
incremental particle filter algorithm that, however,
performed considerably worse than the standard
batch algorithms, in particular for the Bigram model.
We extend that algorithm by adding rejuvenation
steps and show that this leads to considerable im-
provements, thus strengthening the case for particle
filters as another tool for Bayesian inference in com-
putational linguistics.
The rest of the paper is structured as follows. Sec-
tions 2 and 3 provide the relevant background about
word segmentation and previous work. Section 4 de-
scribes our algorithm. Section 5 reports on an ex-
perimental evaluation of our algorithm, and section
6 concludes and suggests possible directions for fu-
ture research.
</bodyText>
<sectionHeader confidence="0.942432" genericHeader="method">
2 Model description
</sectionHeader>
<bodyText confidence="0.999955481481482">
The Unigram model assumes that words in a se-
quence are generated independently whereas the Bi-
gram model models dependencies between adjacent
words. This has been shown by Goldwater (2006) to
markedly improve segmentation performance. We
perform experiments on both models but, for rea-
sons of space, only give an overview of the Unigram
model, referring the reader to the original papers for
more detailed descriptions. (Goldwater, 2006; Gold-
water et al., 2009)
A sequence of words or utterance is generated by
making independent draws from a discrete distribu-
tion over words, G. As neither the actual “true”
words nor their number is known in advance, G is
modelled as a draw from a DP. A DP is parametrized
by a base distribution Po and a concentration param-
eter α. Here, Po assigns a probability to every possi-
ble word, i.e. sequence of segments, and α controls
the sparsity of G; the smaller α, the sparser G tends
to be.
To computationally cope with the unbounded
nature of draws from a DP, they can be “inte-
grated out”, yielding the Chinese Restaurant Process
(CRP), an infinitely exchangeable conditional pre-
dictive distribution. The CRP also provides an in-
tuitive generative story for the observed data. Each
generated word token corresponds to a customer sit-
</bodyText>
<page confidence="0.997372">
85
</page>
<note confidence="0.6840605">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 85–89,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999931421052632">
ting at one of the unboundedly many tables in an
imaginary Chinese restaurant. Customers choose
their seats sequentially, and they sit either at an al-
ready occupied or a new table. The former hap-
pens with probability proportional to the number of
customers already sitting at a table and corresponds
to generating one more token of the word type all
customers at a table instantiate. The latter happens
with probability proportional to α and corresponds
to generating a token by sampling from the base dis-
tribution, thus also determining the type for all po-
tential future customers at the new table.
Given this generative process, word segmentation
can be cast as a probabilistic inference problem. For
a fixed input, in our case a sequence of phonemes,
our goal is to determine the posterior distribution
over segmentations. This is usually infeasible to do
exactly, leading to the use of approximate inference
methods.
</bodyText>
<sectionHeader confidence="0.998285" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.999984628571429">
The “standard” inference algorithms for the Uni-
gram and Bigram model are MCMC samplers that
are batch algorithms making multiple iterations over
the data to non-deterministically explore the state
space of possible segmentations. If an MCMC algo-
rithm runs long enough, the probability of it visiting
any specific segmentation is the probability of that
segmentation under the target posterior distribution,
here, the distribution over segmentations given the
observed data.
The MCMC algorithm of Goldwater et al. (2009)
is a Gibbs sampler that makes very small moves
through the state space by changing individual word
boundaries one at a time. An alternative MCMC al-
gorithm that samples segmentations for entire utter-
ances was proposed by Mochihashi et al. (2009).
Below, we correct a minor error in the algorithm, re-
casting it as a Metropolis-within-Gibbs sampler.
Moving beyond MCMC algorithms, Pearl et al.
(2010) describe an algorithm that can be seen as
a degenerate limiting case of a particle filter with
only one particle. Their Dynamic Programming
Sampling algorithm makes a single pass through the
data, processing one utterance at a time by sampling
a segmentation given the choices made for all pre-
vious utterances. While their algorithm comes with
no guarantee that it converges on the intended pos-
terior distribution, B¨orschinger and Johnson (2011)
showed how to construct a particle filter that is
asymptotically correct, although experiments sug-
gested that the number of particles required for good
performance is impractically large.
This paper shows how their algorithm can be im-
proved by adding rejuvenation steps, which we will
describe in the next section.
</bodyText>
<sectionHeader confidence="0.972238" genericHeader="method">
4 A Particle Filter with Rejuvenation
</sectionHeader>
<bodyText confidence="0.999970375">
The core idea of a particle filter is to sequentially
approximate a target posterior distribution P by N
weighted point samples or “particles”. Each parti-
cle is updated one observation at a time, exploiting
the insight that Bayes’ Theorem can be applied re-
cursively, as illustratively shown for the case of cal-
culating the posterior probability of a hypothesis H
given two observations O1 and O2:
</bodyText>
<equation confidence="0.9999875">
P(H|O1) ∝ P(O1|H)P(H) (1)
P(H|O1, O2) ∝ P(O2|H)P(H|O1) (2)
</equation>
<bodyText confidence="0.99992892">
If the observations are conditionally independent
given the hypothesis, one can simply take the poste-
rior at time step t as the prior for the posterior update
at time step t + 1.
Here, each particle corresponds to a specific seg-
mentation of the data observed so far, or more pre-
cisely, the specific CRP seating of word tokens in
this segmentation; we refer to this as its history. Its
weight indicates how well a particle is supported by
the data, and each observation corresponds to an un-
segmented utterance. With this, the basic particle
filter algorithm can be described as follows: Begin
with N “empty” particles. To get the particles at time
t+1 from the particles at time t, update each particle
using the observation at time t+1 as follows: sample
a segmentation for this observation, given the parti-
cle’s history, then add the words in this segmentation
to that history. After each particle has been updated,
their weights are adjusted to reflect how well they
are now supported by the observations. The set of
updated and reweighted particles constitutes the ap-
proximation of the posterior at time t + 1.
To overcome the problem of degeneracy (the sit-
uation where only very few particles have non-
negligible weights), B¨orschinger and Johnson use
</bodyText>
<page confidence="0.984497">
86
</page>
<bodyText confidence="0.999764551724138">
resampling; basically, high-probability particles are
permitted to have multiple descendants that can
replace low-probability particles. For reasons of
space, we refer the reader to B¨orschinger and John-
son (2011) for the details of these steps.
While necessary to address the degeneracy prob-
lem, resampling leads to a loss of sample diversity;
very quickly, almost all particles have an identical
history, descending from only a small number of
(previously) high probability particles. With a strict
online learning constraint, this can only be counter-
acted by using an extremely large number of parti-
cles. An alternative strategy which we explore here
is to use rejuvenation; the core idea is to restore
sample diversity after each resampling step by per-
forming MCMC resampling steps on each particle’s
history, thus leading to particles with different his-
tories in each generation, even if they all have the
same parent. (e.g., Canini et al. (2009)) This makes
it necessary to store previously processed observa-
tions and thus no longer qualifies as online learn-
ing in a strict sense, but it still yields an incremental
algorithm that learns as the observations arrive se-
quentially, instead of delaying learning until all ob-
servations are available.
In our setting, rejuvenation works as follows. Af-
ter each resampling step, for each particle the algo-
rithm performs a fixed number of the following re-
juvenation steps:
</bodyText>
<listItem confidence="0.91121025">
1. randomly choose a previously observed utter-
ance
2. resample the segmentation for this utterance
and update the particle accordingly
</listItem>
<bodyText confidence="0.999668142857143">
For the resampling step, we use Mochihashi et al.
(2009)’s algorithm to efficiently sample segmenta-
tions for an unsegmented utterance o, given a se-
quence of n previously observed words W1:n. As
the CRP is exchangeable, during resampling we can
treat every utterance as if it were the last, making
it possible to use this algorithm for any utterance,
irrespective of its actual position in the data. Cru-
cially, however, the distribution over segmentations
that this algorithm samples from is not the true pos-
terior distribution P(·|o, α, W1:n) as defined by the
CRP, but a slightly different proposal distribution
Q(·|o, α, W1:n) that does not take into account the
intra-sentential word dependencies for a segmenta-
tion of o. It is precisely because we ignore these de-
pendencies that an efficient dynamic programming
algorithm is possible, but because Q is different
from the target conditional distribution P, our algo-
rithm that uses Q instead of P needs to correct for
this. In a particle filter, this is done when the par-
ticle weights are calculated (B¨orschinger and John-
son, 2011). For an MCMC algorithm or our rejuve-
nation step, a Metropolis-Hastings accept/reject step
is required, as described in detail by Johnson et al.
(2007) in the context of grammatical inference.1
In our case, during rejuvenation an utterance u
with current segmentation s is reanalyzed as fol-
lows:
</bodyText>
<listItem confidence="0.956452875">
• remove all the words contained in s from the
particle’s current state L, yielding state L*
• sample a proposal segmentation s&apos; for u from
Q(·|u, L*, α), using Mochihashi et al. (2009)’s
dynamic programming algorithm
P(s&apos;|L*,α)Q(s |L*,α) }
• calculate m = min{ f P s L* α s L* α
• with probability m, accept the nQw� sample and
</listItem>
<bodyText confidence="0.990737166666667">
update L* accordingly, else keep the original
segmentation and set the particle’s state back
to L
This completes the description of our extension to
the algorithm. The remainder of the paper empiri-
cally evaluates the particle filter with rejuvenation.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99994725">
We compare the performance of a batch Metropolis-
Hastings sampler for the Unigram and Bigram
model with that of particle filter learners both with
and without rejuvenation, as described in the previ-
ous section. For the batch samplers, we use simu-
lated annealing to facilitate the finding of high prob-
ability solutions, and for the particle filters, we com-
pare the performance of a ‘degenerate’ 1-particle
learner with a 16-particle learner in the rejuvenation
setting.
To get an impression of the contribution of par-
ticle number and rejuvenation steps, we compare
</bodyText>
<footnote confidence="0.928223875">
1Because Mochihashi et al. (2009)’s algorithm samples di-
rectly from the proposal distribution without the accept-reject
step, it is not actually sampling from the intended posterior dis-
tribution. Because Q approaches the true conditional distribu-
tion as the size of the training data increases, however, there
may be almost no noticeable difference between using and not
using the accept/reject step, though strictly speaking, it is re-
quired to guarantee convergence to the the target posterior.
</footnote>
<page confidence="0.994992">
87
</page>
<table confidence="0.999803555555556">
Unigram Bigram
TF logProb TF logProb
MHS 50.39 -196.74 70.93 -237.24
PF1 55.82 -248.21 49.43 -265.40
PF16 62.34 -239.22 50.14 -262.34
PF1000 64.11 -234.87 57.88 -254.17
PF1,100 63.17 -245.32 66.88 -257.65
PF16,100 68.05 -235.71 70.05 -251.66
PF1,1600 77.06 -228.79 74.47 -249.78
</table>
<tableCaption confidence="0.997974">
Table 1: Results for both the Unigram and the Bigram
</tableCaption>
<bodyText confidence="0.995380363636364">
model. MHS is a Metropolis-Hastings batch sampler.
PFx is a particle filter with x particles and no rejuve-
nation. PFx,3 is a particle filter with x particles and s
rejuvenation steps. TF is token f-score, logProb is the
log-probability (x103) of the training-data at the end of
learning. Less negative logProb indicates a better solu-
tion according to the model, higher TF indicates a better
quality segmentation. All results are averaged across 4
runs. Results for the 1000 particle setting are taken from
B¨orschinger and Johnson (2011).
the 16-particle learner with rejuvenation with a 1-
particle learner that performs 16 times as many re-
juvenation samples. For comparison, we also cite
previous results for the 1000-particle learners with-
out rejuvenation reported in B¨orschinger and John-
son (2011), using their choice of parameters to allow
for a direct comparison: α = 20 for the Unigram
model, α0 = 3000, α1 = 100 for the Bigram model,
and we use their base-distribution which differs from
the one described in Goldwater et al. (2009) in that it
doesn’t assume a uniform distribution over segments
in the base-distribution but puts a Dirichlet Prior on
it.
We apply each learner to the Bernstein-Ratner
corpus (Brent, 1999) that is standardly used in
the word segmentation literature, which consists
of 9790 unsegmented and phonemically transcribed
child-directed speech utterances. We evaluate each
algorithm in two ways: inference performance, for
which the final log-probability of the training data
is the criterion, and segmentation performance, for
which we consider token f-score to be the best mea-
sure, since it indicates how well the actual word to-
kens in the data are recovered.Note that these two
measures can diverge, as previously documented for
the Unigram model (Goldwater, 2006) and, less so,
for the Bigram model (Pearl et al., 2010). Table 1
gives the results for our experiments.
For both models, adding rejuvenation always
improves performance markedly as compared to
the corresponding run without rejuvenation both in
terms of log-probability and segmentation f-score.
Note in particular that for the Bigram model, us-
ing 16 particles with 100 rejuvenation steps leads to
an improvement in token f-score of more than 10%
points over 1000 particles without rejuvenation.
Comparing the 1-particle learner with 1600 reju-
venation steps to the 16-particle learner with 100 re-
juvenation steps, for both models the former outper-
forms the latter in both log-probability and token f-
score. This suggests that if one has to trade-off par-
ticle number against rejuvenation steps, one may be
better off favouring the latter.
Despite the dramatic improvement over not us-
ing rejuvenation, there is still a considerable gap
between all the incremental learners and the batch
sampling algorithm in terms of log-probability. A
similar observation was made by Johnson and Gold-
water (2009) for incremental initialisation in word
segmentation using adaptor grammars. Their batch
sampler converged on higher token f-score but lower
probability solutions in some settings when initial-
ized in an incremental fashion as opposed to ran-
domly. We agree with their suggestion that this may
be due to the “greedy” character of an incremental
learner.
</bodyText>
<sectionHeader confidence="0.996125" genericHeader="conclusions">
6 Conclusion and outlook
</sectionHeader>
<bodyText confidence="0.999980882352941">
We have shown that adding rejuvenation to a par-
ticle filter improves segmentation scores and log-
probabilities. Yet, our incremental algorithm still
finds lower probability but high quality token f-
scores compared to its batch counterpart. While
in principle, increasing the number of rejuvenation
steps and particles will make this gap smaller and
smaller, we believe the existence of the gap to be
interesting in its own right, suggesting a general dif-
ference in learning behaviour between batch and in-
cremental learners, especially given the similar re-
sults in Johnson and Goldwater (2009). Further
research into incremental learning algorithms may
help us better understand how processing limitations
can affect learning and why this may be beneficial
for language acquisition, as suggested, for example,
in Newport (1988).
</bodyText>
<page confidence="0.998443">
88
</page>
<sectionHeader confidence="0.990277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99941772">
Benjamin B¨orschinger and Mark Johnson. 2011. A parti-
cle filter algorithm for bayesian wordsegmentation. In
Proceedings of the Australasian Language Technology
Association Workshop 2011, pages 10–18, Canberra,
Australia, December.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34(1-3):71–105.
Kevin R. Canini, Lei Shi, and Thomas L. Griffiths. 2009.
Online inference of topics with latent Dirichlet alloca-
tion. In David van Dyk and Max Welling, editors, Pro-
ceeings of the 12th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages 65–
72.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.
Sharon Goldwater. 2006. Nonparametric Bayesian Mod-
els of Lexical Acquisition. Ph.D. thesis, Brown Uni-
versity.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparametric bayesian inference: Experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the NorthAmeri-
can Chapter of the Association for Computational Lin-
guistics, Boulder, Colorado.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for pcfgs via markov
chain monte carlo. In Proceedings of Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
100–108, Suntec, Singapore, August. Association for
Computational Linguistics.
Elissa L Newport. 1988. Constraints on learning and
their role in language acquisition: Studies of the acqui-
sition of american sign language. Language Sciences,
10:147–172.
Lisa Pearl, Sharon Goldwater, and Mark Steyvers. 2010.
Online learning mechanisms for bayesian models of
word segmentation. Research on Language and Com-
putation, 8(2):107–132.
</reference>
<page confidence="0.99975">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814121">
<title confidence="0.9961085">Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</title>
<author confidence="0.882359">benjamin borschingermq edu au mark johnsonmq edu au</author>
<affiliation confidence="0.9914485">of Computing of Computational Linguistics Macquarie University Heidelberg University</affiliation>
<address confidence="0.965072">Sydney, Australia Heidelberg, Germany</address>
<abstract confidence="0.996227">We present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introin Goldwater (2006). By adding rejuvea particle filter, we are able to considerably improve its performance, both in terms of finding higher probability and higher accuracy solutions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Mark Johnson</author>
</authors>
<title>A particle filter algorithm for bayesian wordsegmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>10--18</pages>
<location>Canberra, Australia,</location>
<marker>B¨orschinger, Johnson, 2011</marker>
<rawString>Benjamin B¨orschinger and Mark Johnson. 2011. A particle filter algorithm for bayesian wordsegmentation. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 10–18, Canberra, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="13989" citStr="Brent, 1999" startWordPosition="2235" endWordPosition="2236">rms 16 times as many rejuvenation samples. For comparison, we also cite previous results for the 1000-particle learners without rejuvenation reported in B¨orschinger and Johnson (2011), using their choice of parameters to allow for a direct comparison: α = 20 for the Unigram model, α0 = 3000, α1 = 100 for the Bigram model, and we use their base-distribution which differs from the one described in Goldwater et al. (2009) in that it doesn’t assume a uniform distribution over segments in the base-distribution but puts a Dirichlet Prior on it. We apply each learner to the Bernstein-Ratner corpus (Brent, 1999) that is standardly used in the word segmentation literature, which consists of 9790 unsegmented and phonemically transcribed child-directed speech utterances. We evaluate each algorithm in two ways: inference performance, for which the final log-probability of the training data is the criterion, and segmentation performance, for which we consider token f-score to be the best measure, since it indicates how well the actual word tokens in the data are recovered.Note that these two measures can diverge, as previously documented for the Unigram model (Goldwater, 2006) and, less so, for the Bigram</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34(1-3):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin R Canini</author>
<author>Lei Shi</author>
<author>Thomas L Griffiths</author>
</authors>
<title>Online inference of topics with latent Dirichlet allocation.</title>
<date>2009</date>
<booktitle>In David van Dyk and Max Welling, editors, Proceeings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>65--72</pages>
<contexts>
<context position="8786" citStr="Canini et al. (2009)" startWordPosition="1388" endWordPosition="1391">le diversity; very quickly, almost all particles have an identical history, descending from only a small number of (previously) high probability particles. With a strict online learning constraint, this can only be counteracted by using an extremely large number of particles. An alternative strategy which we explore here is to use rejuvenation; the core idea is to restore sample diversity after each resampling step by performing MCMC resampling steps on each particle’s history, thus leading to particles with different histories in each generation, even if they all have the same parent. (e.g., Canini et al. (2009)) This makes it necessary to store previously processed observations and thus no longer qualifies as online learning in a strict sense, but it still yields an incremental algorithm that learns as the observations arrive sequentially, instead of delaying learning until all observations are available. In our setting, rejuvenation works as follows. After each resampling step, for each particle the algorithm performs a fixed number of the following rejuvenation steps: 1. randomly choose a previously observed utterance 2. resample the segmentation for this utterance and update the particle accordin</context>
</contexts>
<marker>Canini, Shi, Griffiths, 2009</marker>
<rawString>Kevin R. Canini, Lei Shi, and Thomas L. Griffiths. 2009. Online inference of topics with latent Dirichlet allocation. In David van Dyk and Max Welling, editors, Proceeings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 65– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="2465" citStr="Goldwater et al., 2009" startWordPosition="359" endWordPosition="363">ection 5 reports on an experimental evaluation of our algorithm, and section 6 concludes and suggests possible directions for future research. 2 Model description The Unigram model assumes that words in a sequence are generated independently whereas the Bigram model models dependencies between adjacent words. This has been shown by Goldwater (2006) to markedly improve segmentation performance. We perform experiments on both models but, for reasons of space, only give an overview of the Unigram model, referring the reader to the original papers for more detailed descriptions. (Goldwater, 2006; Goldwater et al., 2009) A sequence of words or utterance is generated by making independent draws from a discrete distribution over words, G. As neither the actual “true” words nor their number is known in advance, G is modelled as a draw from a DP. A DP is parametrized by a base distribution Po and a concentration parameter α. Here, Po assigns a probability to every possible word, i.e. sequence of segments, and α controls the sparsity of G; the smaller α, the sparser G tends to be. To computationally cope with the unbounded nature of draws from a DP, they can be “integrated out”, yielding the Chinese Restaurant Pro</context>
<context position="4922" citStr="Goldwater et al. (2009)" startWordPosition="758" endWordPosition="761"> usually infeasible to do exactly, leading to the use of approximate inference methods. 3 Previous Work The “standard” inference algorithms for the Unigram and Bigram model are MCMC samplers that are batch algorithms making multiple iterations over the data to non-deterministically explore the state space of possible segmentations. If an MCMC algorithm runs long enough, the probability of it visiting any specific segmentation is the probability of that segmentation under the target posterior distribution, here, the distribution over segmentations given the observed data. The MCMC algorithm of Goldwater et al. (2009) is a Gibbs sampler that makes very small moves through the state space by changing individual word boundaries one at a time. An alternative MCMC algorithm that samples segmentations for entire utterances was proposed by Mochihashi et al. (2009). Below, we correct a minor error in the algorithm, recasting it as a Metropolis-within-Gibbs sampler. Moving beyond MCMC algorithms, Pearl et al. (2010) describe an algorithm that can be seen as a degenerate limiting case of a particle filter with only one particle. Their Dynamic Programming Sampling algorithm makes a single pass through the data, proc</context>
<context position="13800" citStr="Goldwater et al. (2009)" startWordPosition="2203" endWordPosition="2206">esults are averaged across 4 runs. Results for the 1000 particle setting are taken from B¨orschinger and Johnson (2011). the 16-particle learner with rejuvenation with a 1- particle learner that performs 16 times as many rejuvenation samples. For comparison, we also cite previous results for the 1000-particle learners without rejuvenation reported in B¨orschinger and Johnson (2011), using their choice of parameters to allow for a direct comparison: α = 20 for the Unigram model, α0 = 3000, α1 = 100 for the Bigram model, and we use their base-distribution which differs from the one described in Goldwater et al. (2009) in that it doesn’t assume a uniform distribution over segments in the base-distribution but puts a Dirichlet Prior on it. We apply each learner to the Bernstein-Ratner corpus (Brent, 1999) that is standardly used in the word segmentation literature, which consists of 9790 unsegmented and phonemically transcribed child-directed speech utterances. We evaluate each algorithm in two ways: inference performance, for which the final log-probability of the training data is the criterion, and segmentation performance, for which we consider token f-score to be the best measure, since it indicates how </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian Models of Lexical Acquisition.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="951" citStr="Goldwater (2006)" startWordPosition="129" endWordPosition="130">ent a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in Goldwater (2006). By adding rejuvenation to a particle filter, we are able to considerably improve its performance, both in terms of finding higher probability and higher accuracy solutions. 1 Introduction The goal of word segmentation is to segment a stream of segments, e.g. characters or phonemes, into words. For example, given the sequence “youwanttoseethebook”, the goal is to recover the segmented string “you want to see the book”. The models introduced in Goldwater (2006) solve this problem in a fully unsupervised way by defining a generative process for word sequences, making use of the Dirichlet Process (DP) prior. Until recently, the only inference algorithm applied to these models were batch Markov Chain Monte Carlo (MCMC) sampling algorithms. B¨orschinger and Johnson (2011) proposed a strictly incremental particle filter algorithm that, however, performed considerably worse than the standard batch algorithms, in particular for the Bigram model. We extend that algorithm by adding rejuvenation steps and show that this leads to considerable improvements, thu</context>
<context position="2192" citStr="Goldwater (2006)" startWordPosition="319" endWordPosition="320">for particle filters as another tool for Bayesian inference in computational linguistics. The rest of the paper is structured as follows. Sections 2 and 3 provide the relevant background about word segmentation and previous work. Section 4 describes our algorithm. Section 5 reports on an experimental evaluation of our algorithm, and section 6 concludes and suggests possible directions for future research. 2 Model description The Unigram model assumes that words in a sequence are generated independently whereas the Bigram model models dependencies between adjacent words. This has been shown by Goldwater (2006) to markedly improve segmentation performance. We perform experiments on both models but, for reasons of space, only give an overview of the Unigram model, referring the reader to the original papers for more detailed descriptions. (Goldwater, 2006; Goldwater et al., 2009) A sequence of words or utterance is generated by making independent draws from a discrete distribution over words, G. As neither the actual “true” words nor their number is known in advance, G is modelled as a draw from a DP. A DP is parametrized by a base distribution Po and a concentration parameter α. Here, Po assigns a p</context>
<context position="14560" citStr="Goldwater, 2006" startWordPosition="2321" endWordPosition="2322"> to the Bernstein-Ratner corpus (Brent, 1999) that is standardly used in the word segmentation literature, which consists of 9790 unsegmented and phonemically transcribed child-directed speech utterances. We evaluate each algorithm in two ways: inference performance, for which the final log-probability of the training data is the criterion, and segmentation performance, for which we consider token f-score to be the best measure, since it indicates how well the actual word tokens in the data are recovered.Note that these two measures can diverge, as previously documented for the Unigram model (Goldwater, 2006) and, less so, for the Bigram model (Pearl et al., 2010). Table 1 gives the results for our experiments. For both models, adding rejuvenation always improves performance markedly as compared to the corresponding run without rejuvenation both in terms of log-probability and segmentation f-score. Note in particular that for the Bigram model, using 16 particles with 100 rejuvenation steps leads to an improvement in token f-score of more than 10% points over 1000 particles without rejuvenation. Comparing the 1-particle learner with 1600 rejuvenation steps to the 16-particle learner with 100 rejuve</context>
</contexts>
<marker>Goldwater, 2006</marker>
<rawString>Sharon Goldwater. 2006. Nonparametric Bayesian Models of Lexical Acquisition. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparametric bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="15654" citStr="Johnson and Goldwater (2009)" startWordPosition="2491" endWordPosition="2495">rticles without rejuvenation. Comparing the 1-particle learner with 1600 rejuvenation steps to the 16-particle learner with 100 rejuvenation steps, for both models the former outperforms the latter in both log-probability and token fscore. This suggests that if one has to trade-off particle number against rejuvenation steps, one may be better off favouring the latter. Despite the dramatic improvement over not using rejuvenation, there is still a considerable gap between all the incremental learners and the batch sampling algorithm in terms of log-probability. A similar observation was made by Johnson and Goldwater (2009) for incremental initialisation in word segmentation using adaptor grammars. Their batch sampler converged on higher token f-score but lower probability solutions in some settings when initialized in an incremental fashion as opposed to randomly. We agree with their suggestion that this may be due to the “greedy” character of an incremental learner. 6 Conclusion and outlook We have shown that adding rejuvenation to a particle filter improves segmentation scores and logprobabilities. Yet, our incremental algorithm still finds lower probability but high quality token fscores compared to its batc</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparametric bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for pcfgs via markov chain monte carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10625" citStr="Johnson et al. (2007)" startWordPosition="1688" endWordPosition="1691">(·|o, α, W1:n) that does not take into account the intra-sentential word dependencies for a segmentation of o. It is precisely because we ignore these dependencies that an efficient dynamic programming algorithm is possible, but because Q is different from the target conditional distribution P, our algorithm that uses Q instead of P needs to correct for this. In a particle filter, this is done when the particle weights are calculated (B¨orschinger and Johnson, 2011). For an MCMC algorithm or our rejuvenation step, a Metropolis-Hastings accept/reject step is required, as described in detail by Johnson et al. (2007) in the context of grammatical inference.1 In our case, during rejuvenation an utterance u with current segmentation s is reanalyzed as follows: • remove all the words contained in s from the particle’s current state L, yielding state L* • sample a proposal segmentation s&apos; for u from Q(·|u, L*, α), using Mochihashi et al. (2009)’s dynamic programming algorithm P(s&apos;|L*,α)Q(s |L*,α) } • calculate m = min{ f P s L* α s L* α • with probability m, accept the nQw� sample and update L* accordingly, else keep the original segmentation and set the particle’s state back to L This completes the descripti</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for pcfgs via markov chain monte carlo. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested pitman-yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5167" citStr="Mochihashi et al. (2009)" startWordPosition="799" endWordPosition="802">over the data to non-deterministically explore the state space of possible segmentations. If an MCMC algorithm runs long enough, the probability of it visiting any specific segmentation is the probability of that segmentation under the target posterior distribution, here, the distribution over segmentations given the observed data. The MCMC algorithm of Goldwater et al. (2009) is a Gibbs sampler that makes very small moves through the state space by changing individual word boundaries one at a time. An alternative MCMC algorithm that samples segmentations for entire utterances was proposed by Mochihashi et al. (2009). Below, we correct a minor error in the algorithm, recasting it as a Metropolis-within-Gibbs sampler. Moving beyond MCMC algorithms, Pearl et al. (2010) describe an algorithm that can be seen as a degenerate limiting case of a particle filter with only one particle. Their Dynamic Programming Sampling algorithm makes a single pass through the data, processing one utterance at a time by sampling a segmentation given the choices made for all previous utterances. While their algorithm comes with no guarantee that it converges on the intended posterior distribution, B¨orschinger and Johnson (2011)</context>
<context position="9446" citStr="Mochihashi et al. (2009)" startWordPosition="1494" endWordPosition="1497">reviously processed observations and thus no longer qualifies as online learning in a strict sense, but it still yields an incremental algorithm that learns as the observations arrive sequentially, instead of delaying learning until all observations are available. In our setting, rejuvenation works as follows. After each resampling step, for each particle the algorithm performs a fixed number of the following rejuvenation steps: 1. randomly choose a previously observed utterance 2. resample the segmentation for this utterance and update the particle accordingly For the resampling step, we use Mochihashi et al. (2009)’s algorithm to efficiently sample segmentations for an unsegmented utterance o, given a sequence of n previously observed words W1:n. As the CRP is exchangeable, during resampling we can treat every utterance as if it were the last, making it possible to use this algorithm for any utterance, irrespective of its actual position in the data. Crucially, however, the distribution over segmentations that this algorithm samples from is not the true posterior distribution P(·|o, α, W1:n) as defined by the CRP, but a slightly different proposal distribution Q(·|o, α, W1:n) that does not take into acc</context>
<context position="10955" citStr="Mochihashi et al. (2009)" startWordPosition="1745" endWordPosition="1748">ad of P needs to correct for this. In a particle filter, this is done when the particle weights are calculated (B¨orschinger and Johnson, 2011). For an MCMC algorithm or our rejuvenation step, a Metropolis-Hastings accept/reject step is required, as described in detail by Johnson et al. (2007) in the context of grammatical inference.1 In our case, during rejuvenation an utterance u with current segmentation s is reanalyzed as follows: • remove all the words contained in s from the particle’s current state L, yielding state L* • sample a proposal segmentation s&apos; for u from Q(·|u, L*, α), using Mochihashi et al. (2009)’s dynamic programming algorithm P(s&apos;|L*,α)Q(s |L*,α) } • calculate m = min{ f P s L* α s L* α • with probability m, accept the nQw� sample and update L* accordingly, else keep the original segmentation and set the particle’s state back to L This completes the description of our extension to the algorithm. The remainder of the paper empirically evaluates the particle filter with rejuvenation. 5 Experiments We compare the performance of a batch MetropolisHastings sampler for the Unigram and Bigram model with that of particle filter learners both with and without rejuvenation, as described in th</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested pitman-yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 100–108, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elissa L Newport</author>
</authors>
<title>Constraints on learning and their role in language acquisition: Studies of the acquisition of american sign language. Language Sciences,</title>
<date>1988</date>
<pages>10--147</pages>
<marker>Newport, 1988</marker>
<rawString>Elissa L Newport. 1988. Constraints on learning and their role in language acquisition: Studies of the acquisition of american sign language. Language Sciences, 10:147–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Pearl</author>
<author>Sharon Goldwater</author>
<author>Mark Steyvers</author>
</authors>
<title>Online learning mechanisms for bayesian models of word segmentation.</title>
<date>2010</date>
<journal>Research on Language and Computation,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="5320" citStr="Pearl et al. (2010)" startWordPosition="823" endWordPosition="826">g any specific segmentation is the probability of that segmentation under the target posterior distribution, here, the distribution over segmentations given the observed data. The MCMC algorithm of Goldwater et al. (2009) is a Gibbs sampler that makes very small moves through the state space by changing individual word boundaries one at a time. An alternative MCMC algorithm that samples segmentations for entire utterances was proposed by Mochihashi et al. (2009). Below, we correct a minor error in the algorithm, recasting it as a Metropolis-within-Gibbs sampler. Moving beyond MCMC algorithms, Pearl et al. (2010) describe an algorithm that can be seen as a degenerate limiting case of a particle filter with only one particle. Their Dynamic Programming Sampling algorithm makes a single pass through the data, processing one utterance at a time by sampling a segmentation given the choices made for all previous utterances. While their algorithm comes with no guarantee that it converges on the intended posterior distribution, B¨orschinger and Johnson (2011) showed how to construct a particle filter that is asymptotically correct, although experiments suggested that the number of particles required for good </context>
<context position="14616" citStr="Pearl et al., 2010" startWordPosition="2330" endWordPosition="2333"> standardly used in the word segmentation literature, which consists of 9790 unsegmented and phonemically transcribed child-directed speech utterances. We evaluate each algorithm in two ways: inference performance, for which the final log-probability of the training data is the criterion, and segmentation performance, for which we consider token f-score to be the best measure, since it indicates how well the actual word tokens in the data are recovered.Note that these two measures can diverge, as previously documented for the Unigram model (Goldwater, 2006) and, less so, for the Bigram model (Pearl et al., 2010). Table 1 gives the results for our experiments. For both models, adding rejuvenation always improves performance markedly as compared to the corresponding run without rejuvenation both in terms of log-probability and segmentation f-score. Note in particular that for the Bigram model, using 16 particles with 100 rejuvenation steps leads to an improvement in token f-score of more than 10% points over 1000 particles without rejuvenation. Comparing the 1-particle learner with 1600 rejuvenation steps to the 16-particle learner with 100 rejuvenation steps, for both models the former outperforms the</context>
</contexts>
<marker>Pearl, Goldwater, Steyvers, 2010</marker>
<rawString>Lisa Pearl, Sharon Goldwater, and Mark Steyvers. 2010. Online learning mechanisms for bayesian models of word segmentation. Research on Language and Computation, 8(2):107–132.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>