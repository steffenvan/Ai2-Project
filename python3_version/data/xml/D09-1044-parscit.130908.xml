<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.987437">
Multi-Document Summarisation Using Generic Relation Extraction
</title>
<author confidence="0.997472">
Ben Hachey
</author>
<affiliation confidence="0.97742">
Centre for Languate Tecnology Capital Markets CRC Limited
Macquarie University GPO Box 970
</affiliation>
<note confidence="0.722998">
NSW 2109 Australia Sydney NSW 2001
</note>
<email confidence="0.992113">
bhachey@cmcrc.com
</email>
<sectionHeader confidence="0.9937" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997497">
Experiments are reported that investi-
gate the effect of various source docu-
ment representations on the accuracy of
the sentence extraction phase of a multi-
document summarisation task. A novel
representation is introduced based on
generic relation extraction (GRE), which
aims to build systems for relation iden-
tification and characterisation that can be
transferred across domains and tasks with-
out modification of model parameters. Re-
sults demonstrate performance that is sig-
nificantly higher than a non-trivial base-
line that uses tf*idf-weighted words and at
least as good as a comparable but less gen-
eral approach from the literature. Anal-
ysis shows that the representations com-
pared are complementary, suggesting that
extraction performance could be further
improved through system combination.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960509803922">
The goal of summarisation is to take an informa-
tion source, extract content from it, and present
the most important content in a condensed form
(Mani, 2001). The field of automatic summarisa-
tion (Mani, 2001; Sp¨arck Jones, 2007) aims to cre-
ate tools that address various summarisation tasks
with minimal human intervention. Extractive ap-
proaches to automatic summarisation create rep-
resentations of the source document that are gen-
erally based on an easily identified text sub-unit
such as sentences or paragraphs. These represen-
tations are then used to identify representative or
otherwise important snippets of text to place in the
summary.
Following Sp¨arck Jones (2007), summarisation
systems can be characterised with respect to their
approach to three main sub-tasks: 1) interpreta-
tion, 2) transformation and 3) generation. The
input consists of the source document (or a col-
lection of source documents in the case of multi-
document summarisation). The first step (interpre-
tation) creates a representation of the source doc-
ument by performing some level of interpretation.
A simple approach here represents sentences by
their tokens (i.e., as an unordered bag-of-words).
The next step (transformation) is the compaction
step where the source representation is converted
into the summary representation, e.g. by identify-
ing sentences whose words are most representative
of the full text. Finally, in the last step (genera-
tion), the output summary is created. In the case
of sentence extraction, this includes various opera-
tions to maximise coherence such as ensuring that
entity references are comprehensible and arrang-
ing the sentences in a sensible order.
The current work investigates several represen-
tations of source documents. In particular, an ap-
proach from the literature based on atomic events
(Filatova and Hatzivassiloglou, 2004) is compared
to a novel approach based on generic relation ex-
traction (GRE), which aims to build systems for
relation identification and characterisation that can
be transferred across domains and tasks without
modification of model parameters (Hachey, 2009).
The various representations are substituted in the
interpretation phase of a multi-document sum-
marisation task and used as the basis for extract-
ing sentences to be placed in the summary. Sys-
tem summaries are compared by calculating term
overlap with reference summaries created by hu-
man analysts.
</bodyText>
<sectionHeader confidence="0.993396" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.9996615">
In seminal work on automatic summarisation,
Luhn (1958) introduces a representation based
on content words. These are defined as non-
function words from the source document that are
neither too frequent nor too infrequent. Luhn
uses frequency to weight content words and ex-
</bodyText>
<page confidence="0.973167">
420
</page>
<note confidence="0.996604">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 420–429,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999933229166667">
tracts sentences with the highest combined con-
tent scores to form the summary. Subsequent work
adapted the tf*idf weighting scheme, where term
frequency (tf) is combined with inverse document
frequency (idf), an inverse measure of term oc-
currence across documents that serves to down-
weight common words (Sp¨arck Jones, 1972). In
modern work, tf*idf representations are often used
as simple but non-trivial baselines. The problem
is that these shallow features often break down
where underlying linguistic content needs to be
compared rather than just surface structure.
The use of representations based on informa-
tion extraction (IE) has been suggested as one ap-
proach to capturing deeper semantic information.
This is based on the notion that IE definitions of
types for entities, relations and events provide a
level of abstraction that is appropriate for auto-
matic summarisation. Several approaches in the
literature have explored the use of IE-based rep-
resentations for extractive summarisation: McK-
eown et al. (1998) incorporate patient character-
istic templates for matching potential treatments
to specific patients in a medical summarisation
system; White and Cardie (2002) incorporate a
bootstrapped IE system based on Autoslog (Riloff,
1996) for filling event templates; and Harabagiu
and Maiorano (2002) incorporate a hybrid ap-
proach that uses conventional supervised IE tech-
niques for known topics and a more general ap-
proach based on WordNet for unknown topics.1
The problem with these systems is that they all
use supervised approaches to IE that require that
the IE templates be known in advance and addi-
tionally require significant investment in writing
extraction rules or in annotating data for train-
ing. Where more general techniques are used, they
still require domain-specific resources, e.g. White
and Cardie (2002) bootstrapping approach still re-
quires that the extraction templates be known in
advance and Harabagiu and Maiorano (2002) ap-
proach depends on the WordNet lexical database,
for which coverage is not guaranteed for arbitrary
domains.
Filatova and Hatzivassiloglou (2004) intro-
duce methods using more general IE represen-
tations that are not based on supervised learn-
ing. Given a named entity recogniser, the rep-
</bodyText>
<footnote confidence="0.98715175">
1Comparable approaches using IE in the context of
abstractive–as opposed to extractive–summarisation include
work by DeJong (1982), Hahn and Reimer (1999), White et
al. (2001) and Saggion and Lapalme (2002).
</footnote>
<bodyText confidence="0.99989162">
resentation is automatically derived and consists
of &lt;Ent, Connector, Ent&gt; event triples, where
connectors are verbs or action nouns that occur in
between the two NEs. Thus, the approach aims to
perform a generic IE task that the authors refer to
as atomic event extraction. This representation is
shown to outperform a tf*idf baseline on a multi-
document summarisation task. As we will see in
Section 4.3 below, Filatova and Hatzivassiloglou’s
approach has three main shortcomings. First, it fo-
cuses exclusively on simple atomic events (i.e., en-
tity mention pairs with an intervening verbal con-
nector), meaning that it will not be able to ad-
dress tasks where relations are at least as impor-
tant as events (e.g., biographical summarisation).
Second, it relies on exact matching between con-
nectors, which is not capable of capturing latent
semantic similarities (e.g., between ‘work for’ and
‘employed by’). Third, its performance is subject
to the coverage of WordNet, which is used to iden-
tify action nouns.
Generic relation extraction (GRE) aims to build
systems that can be transferred across domains
and tasks without modification of model param-
eters (Hachey, 2009). For relation identification
(i.e., extraction of relation forming entity mention
pairs), this is achieved by using general rule-based
approaches and, for relation characterisation (i.e.,
assignment of types to relation mentions), this is
achieved by using unsupervised machine learning.
Hachey (2009) introduces a GRE approach that
addresses the shortcomings of the atomic event ap-
proach mentioned above. First, it models a type of
IE that includes relations. Second, it uses a con-
nector model based on latent Dirichlet allocation
(Blei et al., 2003), which provides a mechanism
for capturing latent semantic similarities between
connectors. Third, it does not rely on domain-
specific resource like WordNet. The GRE models
used here do rely on dependency parsing. How-
ever, they still generalise across formal domains
as the relation identification and characterisation
systems, developed on news data, achieve compa-
rable performance when applied directly to a rela-
tion extraction task in the biomedical domain (see
Hachey (2009) for details). Furthermore, gram-
matical relations obtained from dependency pars-
ing provide a means for constraining relation iden-
tification and supplying more linguistically mean-
ingful features for relation characterisation.
</bodyText>
<page confidence="0.91418">
421
</page>
<equation confidence="0.9815906">
c1 c2 c3 c4 c5
t1 1 1 0 1 1
t2 1 0 0 1 0
t3 0 1 0 0 1
t4 1 0 1 1 1
</equation>
<tableCaption confidence="0.958083666666667">
Table 1: Text x concept matrix for set cover ap-
proach to automatic summarisation (Filatova and
Hatzivassiloglou, 2004).
</tableCaption>
<sectionHeader confidence="0.982858" genericHeader="method">
3 Algorithm for Set Cover Extraction
</sectionHeader>
<bodyText confidence="0.982536404761905">
For the sake of comparison, the current evaluation
adopts the Filatova and Hatzivassiloglou (2004)
summarisation framework. This defines an extrac-
tion approach based on a mapping between textual
units and concepts. To illustrate, consider the ma-
trix in Table 1 where rows represent textual units
(e.g., sentences, paragraphs) and columns repre-
sent concepts (e.g., words, events, relations) in the
input text. Each concept is either absent or present
in a given textual unit. Additionally, each con-
cept has a weight associated with it. Looking at
the problem in this way makes it natural to for-
mulate it as follows: the summary should select
textual units such that there is maximal coverage
of the salient conceptual units.2 This is essentially
the maximum coverage problem, which has been
shown to be reducible to the set covering problem,
for which there are approximation algorithms in
the literature that run in polynomial time or better
(Hochbaum, 1997; Bienstock and Iyengar, 2004).
Filatova and Hatzivassiloglou define several
greedy algorithms that can be parametrised in
terms of the general SUMMARISE function in Fig-
ure 1, which takes the text x concept matrix D
and the maximum summary length k as input. The
SUMMARISE function first initialises the summary
S to the empty set. Then it enters a loop that
continues until the summary reaches the desired
length. Within the loop, a text unit is extracted and
added to the summary after which the text x con-
cept matrix is updated The output of the algorithm
is a set S comprising the text units that make up
the summary. For the experiments reported here,
the text units t are sentences and LENGTH(ti) re-
2While not considered in the current experiments, a more
discourse-oriented approach could be derived within the set
cover framework by down-weighting conceptual units that
occur e.g. in portions of the source documents that describe
background information, where text segments containing
background information could be identified using a sentence-
level rhetorical status classifier like that developed by Teufel
and Moens (2002).
</bodyText>
<figure confidence="0.909794714285714">
SUMMARISE: D, k
1 S ← {}
2 while Et.csLENGTH(ti) &lt; k
3 tj ←EXTRACT(D)
4 S ← S ∪ tj
5 D ← UPDATE(D, tj)
6 return S
</figure>
<figureCaption confidence="0.978349">
Figure 1: Generalised function for Filatova and
Hatzivassiloglou (2004) approach to extractive
summarisation.
</figureCaption>
<figure confidence="0.968728333333333">
EXTRACT: D
1 cj ← arg maxcj Ecols(D) D[ti, cj]
tiErows(D)
2 tk ← arg maxtkErows(D)&amp;D[tk,cj]&gt;0 SCORE(D, tk)
3 return tk
UPDATE: D, ti
1 for each cj E cols(D)
2 if D[ti, cj] &gt; 0
3 for each tk E rows(D)
4 D[tk, cj] ← 0
5 D ← DELETE(D, ti)
6 return D
</figure>
<figureCaption confidence="0.794909666666667">
Figure 2: Extraction and update functions for Fi-
latova and Hatzivassiloglou (2004) modified adap-
tive algorithm.
turns the count of word tokens in sentence ti.
Figure 2 contains the EXTRACT and UPDATE
functions used here.3 The EXTRACT function first
</figureCaption>
<bodyText confidence="0.975732714285714">
identifies the concept cj not yet covered in the
summary that has the highest overall weight in the
text x concept matrix D. Then it selects the text
unit tk with the highest score from among the text
units that contain concept cj. The SCORE func-
tion is the sum of concept weights for the given
text unit, i.e.:
</bodyText>
<equation confidence="0.9955625">
�SCORE : D, ti �� return D[ti, cj] (1)
cj∈cols(D)
</equation>
<bodyText confidence="0.961025538461538">
The UPDATE function in Figure 2 aims to min-
imise redundancy in the summary by globally
maximising the number of conceptual units cov-
ered in the output. In addition to removing the row
representing the extracted text unit from the text x
concept matrix D, it iterates through the remaining
text units and assigns zero weights to all concepts
that are covered by the extracted text unit.
3The EXTRACT and UPDATE functions in Figure 2 corre-
spond to Filatova and Hatzivassiloglou (2004) modified adap-
tive algorithm and were found in preliminary experiments to
be the better than the simple greedy and adaptive greedy al-
gorithms (see Hachey (2009) for details).
</bodyText>
<page confidence="0.993148">
422
</page>
<bodyText confidence="0.992881">
Bush worked as an oil lease negotiator for Amoco in
Denver and later started his own oil company, JNB.
</bodyText>
<equation confidence="0.659271222222222">
tf*idf (TF)
jnb:3.55, amoco:3.13, oil:3.05,
negotiator:3.04, lease:2.58, denver:2.45,
bush:2.44, worked:2.28, started:2.21,
later:2.13, own:1.96, company:1.94,
...
event (EV)
&lt;PER bush,worked,XFN oil&gt;:0.00023,
&lt;PER bush,worked,ORG amoco&gt;:0.00011,
&lt;PER bush,worked,LOC denver&gt;:0.00011,
&lt;XFN oil,started,ORG jnb&gt;:0.00011,
...
relation (RL)
&lt;ORG amoco,rd94,LOC denver&gt;:0.00039,
&lt;ORG amoco,rd505,LOC denver&gt;:0.00039,
&lt;XFN oil,rd92,ORG jnb&gt;:0.00002,
&lt;XFN oil,rd712,ORG jnb&gt;:0.00002,
...
entity pairev (EE)
&lt;PER bush,XFN oil&gt;:0.00244,
&lt;PER bush,LOC denver&gt;:0.00122,
&lt;PER bush,ORG jnb&gt;:0.00044,
&lt;LOC denver,XFN oil&gt;:0.00033,
...
entity pairrl (ER)
&lt;ORG amoco,LOC denver&gt;:0.00311,
&lt;ORG jnb,XFN oil&gt;:0.00155
</equation>
<figureCaption confidence="0.9911325">
Figure 3: Example sentence and various represen-
tations of sentence content.
</figureCaption>
<sectionHeader confidence="0.988053" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.99951525">
Figure 3 contains an example sentence and its rep-
resentations corresponding to the various models
of sentence content explored here.4 These are de-
scribed in detail in the rest of this section.
</bodyText>
<subsectionHeader confidence="0.993637">
4.1 Baseline tf*idf Representation (TF)
</subsectionHeader>
<bodyText confidence="0.850328714285714">
The baseline model represents sentences as tf*idf-
weighted bags-of-words (TF). Document frequen-
cies for terms are derived from the same resource
used by Filatova and Hatzivassiloglou (2004)–a
frequency list compiled from a large sample of
web pages. Term weighting is calculated using
tf*idf as:
</bodyText>
<equation confidence="0.968945">
w(i,j) = 1I (1 + log (tfi,j)) ∗ log ( N)
(2)
�VVVIdfi
</equation>
<bodyText confidence="0.9998195">
where tfi,j is the number of times term i occurs in
sentence j and dfi is the number of documents in
which term i occurs. An example sentence and its
tf*idf representation can be seen in Figure 3.
</bodyText>
<footnote confidence="0.8954684">
4The sentence was selected from document set d47 (from
the data set described in Section 5.1 below), which contains
articles about Neil Bush and his role in the collapse of Sil-
verado Savings and Loan during the U.S. Savings and Loan
crisis of the 1980s and 1990s.
</footnote>
<subsectionHeader confidence="0.958855">
4.2 Event Representation (EV)
</subsectionHeader>
<bodyText confidence="0.999642625">
We also compare to Filatova and Hatzivassiloglou
(2004) atomic events (EV). This consists of
&lt;Enti, Connectorj, Entk&gt; event triples, where
connectors are verbs or action nouns (i.e., nouns
that are hyponyms of event or activity in Word-
Net) that occur in between the two entity men-
tions. Given a named entity recogniser and a lex-
ical resource (WordNet), these are derived auto-
matically from the text as follows. In the first step,
all pairs of entity mentions that occur together in a
sentence are identified. Next, the algorithm char-
acterises the entity mention pairs using the con-
nector words from the intervening context and dis-
cards pairs without an intervening connector word.
Event triple weighting is calculated by combin-
ing entity pair and connector weights as:
</bodyText>
<equation confidence="0.805953">
wev(i,j, k) = wne(i, k) ∗ wcn(j, i, k) (3)
</equation>
<bodyText confidence="0.9999678">
where wne(i, k) is the weight of the entity
pair &lt;i, k&gt; consisting of entities i and k and
wcn(j, i, k) is the weight of connector j in the con-
text of entity pair &lt;i, j&gt;. wne(i, k) is calculated
as the normalised entity pair count, i.e.:
</bodyText>
<equation confidence="0.999607">
Cne(&lt; i, k &gt;)
wne(i, k) = (4)
Cne(&lt; ∗, ∗ &gt;)
</equation>
<bodyText confidence="0.9999748">
where Cne(&lt;i, k&gt;) is the count of mentions of
entity pair &lt;i, k&gt;5 and Cne(&lt;∗, ∗&gt;) is the total
count of entity mention pairs. And, wcn(j, i, k) is
calculated as the normalised count of connector j
in the context of the entity pair, i.e.:
</bodyText>
<equation confidence="0.992291">
C&lt;i,k&gt;
cn (j) wcn(j, i, k) =(5)
C&lt;i,k&gt;
cn (∗)
</equation>
<bodyText confidence="0.621033">
where C&lt;i,k&gt;
</bodyText>
<equation confidence="0.91982875">
cn (j) is the count of occurrences of
connector j in the context of entity pair &lt;i, k&gt;
and C&lt;i,k&gt;
cn (∗) is the total count of connectors in
</equation>
<bodyText confidence="0.984878571428571">
the context of entity pair &lt;i, k&gt;. An example
sentence and its event representation can be seen
in Figure 3. Event triples generated include
&lt;PER bush,worked,ORG amoco&gt; and
&lt;PER bush,started,ORG jnb&gt;.
Some erroneous event triples are also generated.
The first error has to do with the fact that entities
</bodyText>
<footnote confidence="0.993232">
5Coreference between entity mentions is computed by ex-
act string match after removing punctuation, converting to
all lower case, and prefixing the entity type. For example,
the entity mention string “JNB” with type ORGANISATION is
normalised to ORG jnb.
</footnote>
<page confidence="0.998441">
423
</page>
<bodyText confidence="0.999302047619048">
include named entities identified in the pre-
processing as well as the ten most frequent nouns
in the document set. In the example sentence from
Figure 3, the most frequent nouns include ‘oil’
but not ‘negotiator’ or ‘company’. Therefore, ‘oil’
is labelled as an entity and extracted in a num-
ber of triples such as &lt;PER bush,worked,
XFN oil&gt; (as opposed to &lt;PER bush,
worked,XFN negotiator&gt;). Another
problem illustrated by the example sentence has
to do with the noisy nature of the surface-level
approach to identifying entity mention pairs and
connectors which tends to generate many false
positive events, e.g. &lt;ORG amoco,started,
ORG jnb&gt;. If the algorithm was constrained
based on the underlying grammatical structure,
it should be able to identity that the arguments
of ‘worked’ are ‘Bush’ and ‘Amoco’ (i.e.,
&lt;PER bush,worked,ORG amoco&gt;) and that
‘worked’ does not describe an event involving
‘Amoco’ and ‘JNB’.
</bodyText>
<subsectionHeader confidence="0.993073">
4.3 Relation Representation (RL)
</subsectionHeader>
<bodyText confidence="0.999965910714286">
The focus of the current evaluation is a novel rep-
resentation based on generic relation extraction
(GRE). As mentioned above, GRE is a minimally
supervised approach to the relation extraction task
that aims to build systems for relation identifica-
tion and characterisation that can be transferred
across domains and tasks without modification of
model parameters. Relation mentions are identi-
fied by taking pairs of entity mentions that have ei-
ther 1) no more than two intervening words in the
surface order of the sentence or 2) no more than
one edge intervening on the shortest path through
a dependency parse (see Hachey (2009) for details
and experiments comparing different window con-
figurations). This is stricter than the Filatova and
Hatzivassiloglou approach in that entity mentions
have to occur much closer or be connected by a
single dependency relation. At the same time, it
is less strict in the sense that an action- or event-
denoting word is not required in the context, which
makes it a more general model of IE.
Relation connectors are derived from a model of
relation types based on latent Dirichlet allocation
(Blei et al., 2003) that incorporates word, entity
and dependency path features from the context of
a relation-forming entity mention pair (see Hachey
(2009) for details). This outputs a topic distribu-
tion for each entity mention pair that corresponds
to the type of relation that is described. This rep-
resentation 1) models a type of generic IE that in-
cludes relations, 2) uses a connector model that ab-
stracts away from surface-level event descriptors
used by Filatova and Hatzivassiloglou (2004) and
3) does not rely on domain-specific resources like
WordNet.6 For the purpose of comparison, rela-
tion triples are weighted in the same way as event
triples using Equations 3 and 4 above. However,
the connector pair weighting is modified to use the
distribution over topics given by the LDA output.7
Relation triples generated for the example
sentence in Figure 3 include &lt;ORG amoco,
rd94,LOC denver&gt; and &lt;ORG amoco,
rd505,LOC denver&gt;, where the connectors
(i.e., rd94 and rd505) are identifiers that index
particular topics from the LDA output. Here,
rd94 and rd505 index topics that correspond
to located-in relations so the respective triples
both describe located-in relations between Amoco
and Denver. Relation triples generated for the
example sentence also include &lt;XFN oil,
rd92,ORG jnb&gt; and &lt;XFN oil,rd712,
ORG jnb&gt;. These are erroneous for the same
reason as some of the event triples above (i.e., due
to the noise inherent in the approach to identifying
nominal entity mentions by identifying the ten
most frequent nouns in the document set).
</bodyText>
<subsectionHeader confidence="0.978328">
4.4 Entity Pair Representations (EE, ER)
</subsectionHeader>
<bodyText confidence="0.999764461538462">
Finally, we investigate the performance of rep-
resentations that do not model event or re-
lation type information. These are identical
to the EV and RL representations above, ex-
cept they are &lt;Ent, Ent&gt; 2-tuples instead of
&lt;Ent, Connector, Ent&gt; 3-tuples. That is, entity
pairs are included here provided that they meet the
relation mention identification constraints. They
are weighted using the normalised entity pair
count (Equation 4 above). Relation-based entity
pairs generated for the example sentence in Fig-
ure 3 include &lt;LOC denver,ORG amoco&gt; and
&lt;ORG jnb,XFN oil&gt;.
</bodyText>
<footnote confidence="0.779196222222222">
6The GRE representation here does rely on dependency
parsing, however, Hachey (2009) shows that it is still directly
portable between the news and biomedical domains without
modification of model parameters.
7Distributions for entity mention pairs tend to have a long
uniform tail and only a few topics with higher probability. In
converting to a weighting scheme, topic representations here
are converted to a sparse representation where all topics in
the uniform tail are removed.
</footnote>
<page confidence="0.998926">
424
</page>
<sectionHeader confidence="0.997847" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.951261">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999986322580645">
The experiments here use the multi-document
summarisation data from the 2001 Document Un-
derstanding Conference (DUC),8 which is the
same data used by Filatova and Hatzivassiloglou
(2004). This comprises 30 test document sets,
each of which include approximately 10 news sto-
ries. Each document set is collected by a human
and focuses on a particular topic. Example topics
include the nomination of Clarence Thomas to the
American Supreme Court, Neil Bush’s role in the
collapse of Silverado Savings and Loan and the
Exxon Valdez oil spill. Gold standard summaries
are provided for each document set for summary
lengths of 50, 100, 200 and 400 words. This helps
to ensure that the systems are not over-tuned to
specific summary lengths. For each summary task
(i.e., all 120 document set × summary length com-
binations), there are three distinct gold standard
summaries created by different human analysts.
Pre-processing includes sentence boundary
identification, segmentation of words (tokenisa-
tion), labelling words with part-of-speech tags,
identification of noninflected base word forms
(lemmatisation) from the LT-TTT tools (Grover et
al., 2000). It also includes dependency parsing us-
ing Minipar (Lin, 1998) and automatic named en-
tity recognition using the C&amp;C tagger (Curran and
Clark, 2003) trained on the data from the MUC-7
shared task (Chinchor, 1998). Weights for the var-
ious IE-based representations are calculated over
each input document set.
</bodyText>
<subsectionHeader confidence="0.992788">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999719444444444">
The evaluation uses Rouge9 to determine which
representation selects content that overlaps most
with human summaries. Rouge estimates the
coverage of appropriate concepts (Lin, 2004) in
a summary by comparing it to several human-
created reference summaries. Rouge-1 does so
by computing recall based on macro-averaged un-
igram overlap. Rouge-SU4 does so by calculating
skip-bigram overlap where bigrams are allowed to
</bodyText>
<footnote confidence="0.99387775">
8http://www-nlpir.nist.gov/projects/
duc/index.html
9Rouge stands for recall-oriented understudy for gisting
evaluations. While current versions also compute precision
and f-score of system summaries, the evaluation here uses
recall alone, which is sufficient when the length of the sum-
maries being compared is the same. Rouge can be obtained
from http://haydn.isi.edu/ROUGE/.
</footnote>
<table confidence="0.9998065">
1 50 100 200 400
TF 0.0797 0.1113 0.1742 0.2467
EV 0.1360 0.1776 0.2315 0.3019
RL 0.1360 0.1766 0.2412 0.3014
SU4 50 100 200 400
TF 0.0173 0.0259 0.0442 0.0693
EV 0.0376 0.0494 0.0692 0.0950
RL 0.0356 0.0491 0.0701 0.0939
</table>
<tableCaption confidence="0.9348455">
Table 2: Comparison of Rouge scores for the tf*idf
(TF), event (EV) and relation (RL).
</tableCaption>
<bodyText confidence="0.998347578947369">
be composed of non-contiguous words (with as
many as four words intervening). Rouge-SU4 also
includes unigrams to decrease the chances of zero
scores where there is no skip-bigram overlap.
The configuration is based on comparisons be-
tween Rouge and human judgements of content
coverage (Lin, 2004), which suggest that Rouge-
1 and Rouge-SU4 with stemming and removal
of stop words are good measures for evaluating
multi-document summarisation tasks, consistently
achieving Pearson’s correlation scores above 0.72
and as high as 0.9 for longer summaries. Paired
Wilcoxon signed ranks tests across document sets
are used to check for significant differences be-
tween systems. The paired Wilcoxon signed ranks
test is a non-parametric analogue of the paired t
test. The null hypothesis is that the two popula-
tions from which the scores are sampled are iden-
tical.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.9992465">
Can extractive summarisation be improved us-
ing representations based on generic informa-
tion extraction? Table 2 contains results for
tf*idf (TF), event (EV) and relation (RL) repre-
sentations. Columns contain results for different
lengths of summary (50, 100, 200 and 400 words).
The best representation for each summary length
is in bold and representations that are statistically
distinguishable from the best (i.e., p ≤ 0.05) are
underlined. The results demonstrate unambigu-
ously that the event and relation representations
outperform the tf*idf representation, with strongly
significant p-values less than 0.001 for both Rouge
measures and all summary lengths. The event and
relation representations are indistinguishable for
both Rouge measures and all summary lengths.
</bodyText>
<page confidence="0.997989">
425
</page>
<table confidence="0.999779666666667">
1 50 100 200 400
ER 0.1497 0.1929 0.2527 0.3123
EE 0.1442 0.1705 0.2288 0.3061
SU4 50 100 200 400
ER 0.0419 0.0537 0.0786 0.1008
EE 0.0364 0.0447 0.0643 0.0963
</table>
<tableCaption confidence="0.986829">
Table 3: Comparison of Rouge scores for entity
pairs based on relations (ER) and events (EE).
</tableCaption>
<bodyText confidence="0.888310538461539">
How does entity pair identification for generic
relations compare to entity pair identification
for atomic events? Table 3 contains results for
the representations described in Section 4.4. Rows
correspond to entity pair identification for rela-
tions (ER) and events (EE).10 Results suggest that
the entity pair model based on GRE data out-
performs the entity pair model based on atomic
events, at least for medium sized summaries of
100 and 200 words where ER is significantly better
than EE for both Rouge measures.
How do the event and relation representations
perform with respect to corresponding entity
pair representations? The scores for the entity
pair representations reported in Table 3 are statisti-
cally indistinguishable from those for correspond-
ing relation and event representations in Table 2
above. This appears to be a mixed result for both
the relation representation introduced here and
the Filatova and Hatzivassiloglou event represen-
tation. And, while GRE is shown to have a positive
effect on Rouge scores when compared to atomic
events, the same cannot be said of approaches
to characterising relation and event types. How-
ever, as the correlation analysis (Section 7.1 be-
low) demonstrates, RL and ER do not necessar-
ily perform well on the same document sets. This
suggests that they are actually complementary to
some degree, meaning that a combined system
based on both representations would outperform
RL and ER on their own.
10In contrast to the results for the tf*idf, relation and event
representations which use the modified adaptive algorithm
described above, results for entity pair representations use a
simplified version of the EXTRACT function that picks the
text unit that has the highest score. This performed signifi-
cantly better than the modified adaptive algorithm (p ≤ 0.01)
for all summary lengths for ER and was indistinguishable for
EE. See Hachey (2009) for details.
</bodyText>
<sectionHeader confidence="0.4881" genericHeader="evaluation">
7 Analysis and Comparison
</sectionHeader>
<subsectionHeader confidence="0.971929">
7.1 Complementarity
</subsectionHeader>
<bodyText confidence="0.999975326086957">
Figure 4 contains results for a correlation analy-
sis comparing the various representations. This
also includes a comparison to the human upper
bound (HU), computed by leave-one-out cross val-
idation. Cells in the matrix contain the correla-
tions values measured across document set Rouge-
SU4 scores11 using Spearman’s p rank correlation
coefficient (rs). Here, high values mean that two
representations tend to perform well on the same
document sets such that an ordering of document
sets by Rouge scores is similar for the representa-
tions being compared. In the figure, correlation
strength is represented by shading where light-
toned squares indicate strong correlation (and the
darkest squares indicate weak negative correla-
tion). For example, the upper left cell contains rs
between the TF and EV representations. The four
squares correspond to rs values of -0.085, 0.199,
0.245 and 0.267 respectively for summaries of 50,
100, 200 and 400 words.
The analysis illustrates a number of interest-
ing points. First, it demonstrates that none of
the representations correlate highly with the hu-
man upper bound, meaning that the automatic sys-
tems do not necessarily do well on the document
sets that may be considered easier as measured
by human agreement using Rouge. This suggests
that task difficulty does not need to be considered
as a possible underlying cause of correlation be-
tween the automatic systems. The analysis also
illustrates that there is no clear and consistent re-
lationship between summary length and correla-
tion values. Some cells suggest that correlation
may have a monotonic linear relationship increas-
ing with length (e.g., TF*EV) while others seem
to suggest inverse linear (e.g., TF*RL), quadratic
(e.g., EV*HU) and invariant (e.g., EV*EE) rela-
tionships with length.
Looking at correlation between automatic sys-
tems (i.e., TF, EV, RL, EE and ER), correla-
tion values closer to zero suggest that the sys-
tems do well on different document sets and that
a combined system might therefore be better.
By this reasoning, the largest gains would come
from combining TF with any other representation.
Among the other automatic systems, the relation
</bodyText>
<footnote confidence="0.788286">
11Correlation across document set Rouge-1 scores shows
similar trends.
</footnote>
<page confidence="0.996915">
426
</page>
<figure confidence="0.746361875">
Rouge-SU4
TF 0.7
EV 0.5
RL 0.3
EE 0.1
ER -0.1
50 100 200 400 50 100 200 400 50 100 200 400 50 100 200 400 50 100 200 400
EV RL EE ER HU
</figure>
<figureCaption confidence="0.992708333333333">
Figure 4: Comparison of representations using Spearman’s rs. Row and column labels correspond to
tf*idf (TF), event (EV), relation (RL), event entity pair (EE), relation entity pair (ER) and human (HU)
representations. Lighter toned squares indicate stronger correlation.
</figureCaption>
<bodyText confidence="0.999980909090909">
representation (RL) shows moderately high poten-
tial for combination with its corresponding entity
pair representation (ER) with Spearman’s rs val-
ues in the range from 0.348 to 0.476. This sug-
gests that ER should not necessarily be consid-
ered a simpler representation of the same infor-
mation captured by RL when comparing results.
The event representation (EV), by contrast, shows
the strongest correlation of any comparison with
its corresponding entity pair representation (EE)
with rs values in the range from 0.541 to 0.725.
</bodyText>
<subsectionHeader confidence="0.982194">
7.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999890545454546">
Four document sets were considered for error
analysis. These were selected to cover different
relative rankings of representations. Rows in Ta-
ble 4 give the document set ID and list the repre-
sentations in order of their Rouge-SU4 scores. In-
spection of the corresponding document sets sug-
gests that the different approaches compared here
are appropriate for different types of summary
tasks. Specifically, it suggests that relation and
event representations perform poorly on summari-
sation tasks that are oriented towards sentiment,
description or analysis. However, they do well
on document sets that are oriented towards fac-
tual information typical of information extraction
tasks (though current representations do not cap-
ture date, time or numeric information). This sup-
ports the notion from the previous section that the
different representations evaluated here are com-
plementary.
The document set (d06) for the summaries in
Figure 5 illustrates a case where the relation and
event representations perform well with respect
</bodyText>
<note confidence="0.697073">
Set Rank 1 Rank 2 Rank 3
d15 TF (0.046) RL (0.035) EV (0.023)
d39 TF (0.033) EV (0.024) RL (0.014)
d06 RL (0.094) EV (0.060) TF (0.016)
d53 RL (0.078) TF (0.035) EV (0.020)
</note>
<tableCaption confidence="0.806801">
Table 4: DUC 2001 document sets chosen for er-
</tableCaption>
<bodyText confidence="0.990715695652174">
ror analysis and corresponding Rouge-SU4 scores.
to tf*idf. The gold standard summary describes
a beating event, addressing the basic facts of the
Rodney King beating by Los Angeles police as
well as the political aftermath which consists pri-
marily of an investigation and a summary of re-
lated police brutality events. The difference in
performance seems to be due to the fact that re-
lations and events are central to all aspects of
this summary and the relation and event represen-
tations clearly do better than tf*idf at capturing
this information. This summary also illustrates an
unintended side-effect of the relation representa-
tion where the generic relation identification algo-
rithm finds relations between components of lex-
ical compounds or multi-word phrases. The rep-
resentation for the third sentence in the RL sum-
mary, for example, includes a relation between
ORG police and XFN chief in addition to
true positive relations e.g. between ORG police
and PER darylgates and false positive re-
lations e.g. between PER tombradley and
ORG police.
</bodyText>
<subsectionHeader confidence="0.96868">
7.3 Comparison to Supervised Extraction
</subsectionHeader>
<bodyText confidence="0.826223">
Related work by Wong et al. (2008) also com-
pares representations for sentence extraction on
</bodyText>
<page confidence="0.994983">
427
</page>
<bodyText confidence="0.985911857142857">
TF [S1] Mr. Williams likened the report to the Knapp Commission, a 1970s blue-ribbon study that exposed
(0.016) widespread corruption in the New York Police Department and led to significant improvements there. [S2]
(20/29) “There’s no doubt in our mind that the only reason they stopped Joe Morgan was because he is black and
he was the first black who happened to come by,” said William Barnes, one of the attorneys representing
the former ballplayer. [S3] Joseph McNamara, retired chief of San Jose’s department and now a fellow
at Stanford University’s Hoover Institution, said he has been getting calls all summer from [END] cities
around the country about racism and brutality in their departments.
</bodyText>
<figure confidence="0.532225">
EV [S1] A high-ranking commission appointed after the beating, under the chairmanship of Mr Warren
(0.060) Christopher, a lawyer and former deputy secretary of state, concluded that the Los Angeles police de-
(9/29) partment got results, in terms of arrests, but had developed a ‘siege mentality that alienates the officer
</figure>
<figureCaption confidence="0.404645166666667">
from the community’. [S2] The images of Los Angeles police swinging nightsticks at King as he lay on
the ground, played repeatedly on national news programs, were burned into the national conscience and
led to widespread calls for investigation of police brutality. [S3] Besides recommending that Mr Gates
should go, the Christopher commission urged a policy [END] of community policing with more foot pa-
trols, as well as measures to discipline racist police officers and to improve the investigation of complaints
about police brutality.
</figureCaption>
<reference confidence="0.756490714285714">
RL [S1] Mr. Gates opposed the Police Corps because its members would not be professionals. [S2] Shortly
(0.094) after Rodney King’s beating, a news program on ABC illustrating police brutality showed a still photo
(3/29) of police using a martial-arts weapon against a person being arrested, but there was no mention that the
episode involved Operation Rescue. [S3] The report was issued yesterday by a commission appointed by
Mayor Tom Bradley and Police Chief Daryl Gates in the wake of the videotaped beating March 3 of a
black motorist, Rodney King, by Los Angeles police. [S4] Investigations have been launched by the FBI,
the Los [END] Angeles County district attorney’s office and the Long Beach Police Department.
HU The most important of the many cases of police brutality reported in southern California 1989-1992, was
(0.400) the beating of Rodney King by four Los Angeles officers on March 3, 1991. An investigating commission
(15/29) outlined steps for improvement of the police department and called for the resignation of Chief Gates.
Gates did not resign until the following year after the acquittal of the four officers caused massive rioting.
Other cases of police brutality arose in Minneapolis, Chicago and Kansas City. Operation Rescue claimed
that its non-violent anti-abortion demonstrators were seriously injured by excessive police tactics in more
than [END] 50 cities.
</reference>
<figureCaption confidence="0.8734855">
Figure 5: Example system and human (HU) summaries where relation (RL) and event (EV) representa-
tions perform well with respect to the tf*idf (TF) representation: Police Brutality Document Set (d06).
</figureCaption>
<bodyText confidence="0.99981364">
the DUC 2001 data. However, it uses supervised
machine learning (probabilistic support vector ma-
chines) to derive a salience function while we fo-
cus on unsupervised approaches that can be ported
to new domains and tasks without annotation or
training. Interestingly, Wong et al.’s results sug-
gest that adding events to a word-based feature
set increases the precision of supervised sentence
extraction but reduces the recall. By contrast,
the current results and analysis provide evidence
that word and generic IE-based representations are
complementary when using unsupervised salience
functions for sentence extraction.
The Wong et al. (2008) paper also provides use-
ful results for comparison to state-of-the art. On
the 200 word summarisation task, Wong et al. re-
port Rouge-1 scores of 0.352 and 0.344 respec-
tively for word-based and event-based represen-
tations. On the same task, our unsupervised ap-
proach achieves Rouge-1 scores of 0.174, 0.232,
0.229, 0.241 and 0.253 respectively for the tf*idf,
event, event entity pair, relation and relation en-
tity pair representations. Wong et al.’s best overall
score is 0.396 using a representation that combines
surface, content and relevance features.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999972869565217">
Experiments were presented that compare the ef-
fect of various source document representations
on the accuracy of automatic summarisation. This
serves as an extrinsic evaluation of generic relation
extraction, a domain-neutral and fully portable ap-
proach to relation identification and characterisa-
tion. Results demonstrate that GRE is an effective
representation for sentence extraction for multi-
document summarisation. Performance for the re-
lation representation is significantly better than a
non-trivial tf*idf baseline across the range of sum-
mary lengths explored. Performance is also at
least as good as a comparable but less general rep-
resentation based on event extraction. Correlation
analysis suggests that different representations are
complementary due to the fact that they perform
well on different document sets. Error analysis
supports this conclusion, suggesting that the rela-
tion and event representations perform poorly on
summarisation tasks that are oriented towards e.g.
sentiment, description or analysis while they per-
form well on tasks that focus on fact-oriented in-
formation.
</bodyText>
<page confidence="0.998486">
428
</page>
<sectionHeader confidence="0.998818" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996222">
This work was supported by Scottish Enterprise
Edinburgh-Stanford Link grant R37588 as part of
the EASIE project at the University of Edinburgh.
It would not have been possible without the guid-
ance of Claire Grover and Mirella Lapata.
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999930577319587">
Daniel Bienstock and Garud Iyengar. 2004. Faster
approximation algorithms for packing and covering
problems. Technical Report TR-2004-09, Columbia
University.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Nancy Chinchor. 1998. Overview of MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence, Fairfax, VA, USA.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, Edmonton, Alberta, Canada.
Gerald DeJong. 1982. An overview of the FRUMP
system. In Wendy G. Lehnert and Martin H. Ringle,
editors, Strategies for Natural Language Process-
ing, pages 149–176. Lawrence Erlbaum Associates,
Hillsdale, NJ.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Proceed-
ings of the ACL Text Summarization Branches Out
Workshop, Barcelona, Spain.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT—a flexible tokeni-
sation tool. In Proceedings of the 2nd International
Conference on Language Resources and Evaluation,
Athens, Greece.
Ben Hachey. 2009. Towards Generic Relation Extrac-
tion. Ph.D. thesis, University of Edinburgh.
Udo Hahn and Ulrich Reimer. 1999. Knowledge-
based text summarization: Salience and general-
ization operators for knowledge base abstraction.
In Inderjeet Mani and Mark T. Maybury, editors,
Advances in Automatic Text Summarization, pages
215–232. MIT Press, Cambridge, MA.
Sanda M. Harabagiu and Steven J. Maiorano. 2002.
Multi-document summarization with GISTexter. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Las Palmas,
Spain.
Dorit S. Hochbaum. 1997. Approximating covering
and packing problems: set cover, vertex cover, in-
dependent set and related problems. In Dorit S.
Hochbaum, editor, Approximation Algorithms for
NP-Hard Problems, pages 94–143. PWS Publishing
Company, Boston, MA.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the LREC Workshop
Evaluation of Parsing Systems, Granada, Spain.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the ACL Text Summarization Branches Out Work-
shop, Barcelona, Spain.
Hans P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM Journal of Research and Devel-
opment, 2(2).
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins, Amsterdam/Philadelphia.
Kathleen R. McKeown, Desmond A. Jordan, and
Vasileios Hatzivassiloglou. 1998. Generating
patient-specific summaries of online literature. In
Proceedings of the AAAI Spring Symposium on In-
telligent Text Summarization, Stanford, CA, USA.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of the 14th National Conference on Artificial Intelli-
gence, Portland, OR, USA.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational Linguistics, 28(4):497–526.
Karen Sp¨arck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28(1):11–21.
Karen Sp¨arck Jones. 2007. Automatic summarising:
The state of the art. Information Processing and
Management, 43:1449–1481.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles – experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
Michael White and Claire Cardie. 2002. Selecting
sentences for multidocument summaries using ran-
domized local search. In Proceedings of the ACL
Workshop on Automatic Summarization, Philadel-
phia, PA, USA.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Mul-
tidocument summarization via information extrac-
tion. In Proceedings of the 1st International Con-
ference on Human Language Technology Research,
San Diego, CA, USA.
Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Ex-
tractive summarization using supervised and semi-
supervised learning. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics, Manchester, UK.
</reference>
<page confidence="0.999243">
429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539737">
<title confidence="0.999968">Multi-Document Summarisation Using Generic Relation Extraction</title>
<author confidence="0.999983">Ben Hachey</author>
<affiliation confidence="0.999951">Centre for Languate Tecnology Capital Markets CRC Limited</affiliation>
<address confidence="0.7700785">Macquarie University GPO Box 970 NSW 2109 Australia Sydney NSW</address>
<email confidence="0.996791">bhachey@cmcrc.com</email>
<abstract confidence="0.999184857142857">Experiments are reported that investigate the effect of various source document representations on the accuracy of the sentence extraction phase of a multidocument summarisation task. A novel representation is introduced based on generic relation extraction (GRE), which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters. Results demonstrate performance that is significantly higher than a non-trivial basethat uses words and at least as good as a comparable but less general approach from the literature. Analysis shows that the representations compared are complementary, suggesting that extraction performance could be further improved through system combination.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>RL Mr</author>
</authors>
<title>Gates opposed the Police Corps because its members would not be professionals. [S2] Shortly (0.094) after Rodney King’s beating, a news program on ABC illustrating police brutality showed a still photo (3/29) of police using a martial-arts weapon against a person being arrested, but there was no mention that the episode involved Operation Rescue. [S3] The report was issued yesterday by a commission appointed by Mayor Tom Bradley and Police Chief Daryl Gates in the wake of the videotaped beating March 3 of a black motorist, Rodney King, by Los Angeles police. [S4] Investigations have been launched by the FBI, the Los [END] Angeles County district attorney’s office and the Long Beach Police Department.</title>
<marker>Mr, </marker>
<rawString>RL [S1] Mr. Gates opposed the Police Corps because its members would not be professionals. [S2] Shortly (0.094) after Rodney King’s beating, a news program on ABC illustrating police brutality showed a still photo (3/29) of police using a martial-arts weapon against a person being arrested, but there was no mention that the episode involved Operation Rescue. [S3] The report was issued yesterday by a commission appointed by Mayor Tom Bradley and Police Chief Daryl Gates in the wake of the videotaped beating March 3 of a black motorist, Rodney King, by Los Angeles police. [S4] Investigations have been launched by the FBI, the Los [END] Angeles County district attorney’s office and the Long Beach Police Department.</rawString>
</citation>
<citation valid="false">
<authors>
<author>HU</author>
</authors>
<title>The most important of the many cases of police brutality reported in southern California 1989-1992, was (0.400) the beating of Rodney King by four Los Angeles officers on</title>
<date>1991</date>
<marker>HU, 1991</marker>
<rawString>HU The most important of the many cases of police brutality reported in southern California 1989-1992, was (0.400) the beating of Rodney King by four Los Angeles officers on March 3, 1991. An investigating commission (15/29) outlined steps for improvement of the police department and called for the resignation of Chief Gates. Gates did not resign until the following year after the acquittal of the four officers caused massive rioting. Other cases of police brutality arose in Minneapolis, Chicago and Kansas City. Operation Rescue claimed that its non-violent anti-abortion demonstrators were seriously injured by excessive police tactics in more than [END] 50 cities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bienstock</author>
<author>Garud Iyengar</author>
</authors>
<title>Faster approximation algorithms for packing and covering problems.</title>
<date>2004</date>
<tech>Technical Report TR-2004-09,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="9973" citStr="Bienstock and Iyengar, 2004" startWordPosition="1535" endWordPosition="1538">.g., words, events, relations) in the input text. Each concept is either absent or present in a given textual unit. Additionally, each concept has a weight associated with it. Looking at the problem in this way makes it natural to formulate it as follows: the summary should select textual units such that there is maximal coverage of the salient conceptual units.2 This is essentially the maximum coverage problem, which has been shown to be reducible to the set covering problem, for which there are approximation algorithms in the literature that run in polynomial time or better (Hochbaum, 1997; Bienstock and Iyengar, 2004). Filatova and Hatzivassiloglou define several greedy algorithms that can be parametrised in terms of the general SUMMARISE function in Figure 1, which takes the text x concept matrix D and the maximum summary length k as input. The SUMMARISE function first initialises the summary S to the empty set. Then it enters a loop that continues until the summary reaches the desired length. Within the loop, a text unit is extracted and added to the summary after which the text x concept matrix is updated The output of the algorithm is a set S comprising the text units that make up the summary. For the </context>
</contexts>
<marker>Bienstock, Iyengar, 2004</marker>
<rawString>Daniel Bienstock and Garud Iyengar. 2004. Faster approximation algorithms for packing and covering problems. Technical Report TR-2004-09, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8056" citStr="Blei et al., 2003" startWordPosition="1224" endWordPosition="1227"> and tasks without modification of model parameters (Hachey, 2009). For relation identification (i.e., extraction of relation forming entity mention pairs), this is achieved by using general rule-based approaches and, for relation characterisation (i.e., assignment of types to relation mentions), this is achieved by using unsupervised machine learning. Hachey (2009) introduces a GRE approach that addresses the shortcomings of the atomic event approach mentioned above. First, it models a type of IE that includes relations. Second, it uses a connector model based on latent Dirichlet allocation (Blei et al., 2003), which provides a mechanism for capturing latent semantic similarities between connectors. Third, it does not rely on domainspecific resource like WordNet. The GRE models used here do rely on dependency parsing. However, they still generalise across formal domains as the relation identification and characterisation systems, developed on news data, achieve comparable performance when applied directly to a relation extraction task in the biomedical domain (see Hachey (2009) for details). Furthermore, grammatical relations obtained from dependency parsing provide a means for constraining relatio</context>
<context position="18975" citStr="Blei et al., 2003" startWordPosition="3026" endWordPosition="3029">no more than one edge intervening on the shortest path through a dependency parse (see Hachey (2009) for details and experiments comparing different window configurations). This is stricter than the Filatova and Hatzivassiloglou approach in that entity mentions have to occur much closer or be connected by a single dependency relation. At the same time, it is less strict in the sense that an action- or eventdenoting word is not required in the context, which makes it a more general model of IE. Relation connectors are derived from a model of relation types based on latent Dirichlet allocation (Blei et al., 2003) that incorporates word, entity and dependency path features from the context of a relation-forming entity mention pair (see Hachey (2009) for details). This outputs a topic distribution for each entity mention pair that corresponds to the type of relation that is described. This representation 1) models a type of generic IE that includes relations, 2) uses a connector model that abstracts away from surface-level event descriptors used by Filatova and Hatzivassiloglou (2004) and 3) does not rely on domain-specific resources like WordNet.6 For the purpose of comparison, relation triples are wei</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Overview of MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference,</booktitle>
<location>Fairfax, VA, USA.</location>
<contexts>
<context position="22999" citStr="Chinchor, 1998" startWordPosition="3657" endWordPosition="3658">summary task (i.e., all 120 document set × summary length combinations), there are three distinct gold standard summaries created by different human analysts. Pre-processing includes sentence boundary identification, segmentation of words (tokenisation), labelling words with part-of-speech tags, identification of noninflected base word forms (lemmatisation) from the LT-TTT tools (Grover et al., 2000). It also includes dependency parsing using Minipar (Lin, 1998) and automatic named entity recognition using the C&amp;C tagger (Curran and Clark, 2003) trained on the data from the MUC-7 shared task (Chinchor, 1998). Weights for the various IE-based representations are calculated over each input document set. 5.2 Evaluation The evaluation uses Rouge9 to determine which representation selects content that overlaps most with human summaries. Rouge estimates the coverage of appropriate concepts (Lin, 2004) in a summary by comparing it to several humancreated reference summaries. Rouge-1 does so by computing recall based on macro-averaged unigram overlap. Rouge-SU4 does so by calculating skip-bigram overlap where bigrams are allowed to 8http://www-nlpir.nist.gov/projects/ duc/index.html 9Rouge stands for rec</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy Chinchor. 1998. Overview of MUC-7. In Proceedings of the 7th Message Understanding Conference, Fairfax, VA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="22935" citStr="Curran and Clark, 2003" startWordPosition="3644" endWordPosition="3647">at the systems are not over-tuned to specific summary lengths. For each summary task (i.e., all 120 document set × summary length combinations), there are three distinct gold standard summaries created by different human analysts. Pre-processing includes sentence boundary identification, segmentation of words (tokenisation), labelling words with part-of-speech tags, identification of noninflected base word forms (lemmatisation) from the LT-TTT tools (Grover et al., 2000). It also includes dependency parsing using Minipar (Lin, 1998) and automatic named entity recognition using the C&amp;C tagger (Curran and Clark, 2003) trained on the data from the MUC-7 shared task (Chinchor, 1998). Weights for the various IE-based representations are calculated over each input document set. 5.2 Evaluation The evaluation uses Rouge9 to determine which representation selects content that overlaps most with human summaries. Rouge estimates the coverage of appropriate concepts (Lin, 2004) in a summary by comparing it to several humancreated reference summaries. Rouge-1 does so by computing recall based on macro-averaged unigram overlap. Rouge-SU4 does so by calculating skip-bigram overlap where bigrams are allowed to 8http://w</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Language independent NER using a maximum entropy tagger. In Proceedings of the 7th Conference on Natural Language Learning, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald DeJong</author>
</authors>
<title>An overview of the FRUMP system.</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing,</booktitle>
<pages>149--176</pages>
<editor>In Wendy G. Lehnert and Martin H. Ringle, editors,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="6258" citStr="DeJong (1982)" startWordPosition="944" endWordPosition="945">they still require domain-specific resources, e.g. White and Cardie (2002) bootstrapping approach still requires that the extraction templates be known in advance and Harabagiu and Maiorano (2002) approach depends on the WordNet lexical database, for which coverage is not guaranteed for arbitrary domains. Filatova and Hatzivassiloglou (2004) introduce methods using more general IE representations that are not based on supervised learning. Given a named entity recogniser, the rep1Comparable approaches using IE in the context of abstractive–as opposed to extractive–summarisation include work by DeJong (1982), Hahn and Reimer (1999), White et al. (2001) and Saggion and Lapalme (2002). resentation is automatically derived and consists of &lt;Ent, Connector, Ent&gt; event triples, where connectors are verbs or action nouns that occur in between the two NEs. Thus, the approach aims to perform a generic IE task that the authors refer to as atomic event extraction. This representation is shown to outperform a tf*idf baseline on a multidocument summarisation task. As we will see in Section 4.3 below, Filatova and Hatzivassiloglou’s approach has three main shortcomings. First, it focuses exclusively on simple </context>
</contexts>
<marker>DeJong, 1982</marker>
<rawString>Gerald DeJong. 1982. An overview of the FRUMP system. In Wendy G. Lehnert and Martin H. Ringle, editors, Strategies for Natural Language Processing, pages 149–176. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Text Summarization Branches Out Workshop,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2885" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="428" endWordPosition="431">s the compaction step where the source representation is converted into the summary representation, e.g. by identifying sentences whose words are most representative of the full text. Finally, in the last step (generation), the output summary is created. In the case of sentence extraction, this includes various operations to maximise coherence such as ensuring that entity references are comprehensible and arranging the sentences in a sensible order. The current work investigates several representations of source documents. In particular, an approach from the literature based on atomic events (Filatova and Hatzivassiloglou, 2004) is compared to a novel approach based on generic relation extraction (GRE), which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters (Hachey, 2009). The various representations are substituted in the interpretation phase of a multi-document summarisation task and used as the basis for extracting sentences to be placed in the summary. System summaries are compared by calculating term overlap with reference summaries created by human analysts. 2 Motivation In seminal work on automatic s</context>
<context position="5988" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="901" endWordPosition="904">ics.1 The problem with these systems is that they all use supervised approaches to IE that require that the IE templates be known in advance and additionally require significant investment in writing extraction rules or in annotating data for training. Where more general techniques are used, they still require domain-specific resources, e.g. White and Cardie (2002) bootstrapping approach still requires that the extraction templates be known in advance and Harabagiu and Maiorano (2002) approach depends on the WordNet lexical database, for which coverage is not guaranteed for arbitrary domains. Filatova and Hatzivassiloglou (2004) introduce methods using more general IE representations that are not based on supervised learning. Given a named entity recogniser, the rep1Comparable approaches using IE in the context of abstractive–as opposed to extractive–summarisation include work by DeJong (1982), Hahn and Reimer (1999), White et al. (2001) and Saggion and Lapalme (2002). resentation is automatically derived and consists of &lt;Ent, Connector, Ent&gt; event triples, where connectors are verbs or action nouns that occur in between the two NEs. Thus, the approach aims to perform a generic IE task that the authors refer to as at</context>
<context position="8947" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="1372" endWordPosition="1375">al domains as the relation identification and characterisation systems, developed on news data, achieve comparable performance when applied directly to a relation extraction task in the biomedical domain (see Hachey (2009) for details). Furthermore, grammatical relations obtained from dependency parsing provide a means for constraining relation identification and supplying more linguistically meaningful features for relation characterisation. 421 c1 c2 c3 c4 c5 t1 1 1 0 1 1 t2 1 0 0 1 0 t3 0 1 0 0 1 t4 1 0 1 1 1 Table 1: Text x concept matrix for set cover approach to automatic summarisation (Filatova and Hatzivassiloglou, 2004). 3 Algorithm for Set Cover Extraction For the sake of comparison, the current evaluation adopts the Filatova and Hatzivassiloglou (2004) summarisation framework. This defines an extraction approach based on a mapping between textual units and concepts. To illustrate, consider the matrix in Table 1 where rows represent textual units (e.g., sentences, paragraphs) and columns represent concepts (e.g., words, events, relations) in the input text. Each concept is either absent or present in a given textual unit. Additionally, each concept has a weight associated with it. Looking at the problem in </context>
<context position="11264" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="1752" endWordPosition="1755">es and LENGTH(ti) re2While not considered in the current experiments, a more discourse-oriented approach could be derived within the set cover framework by down-weighting conceptual units that occur e.g. in portions of the source documents that describe background information, where text segments containing background information could be identified using a sentencelevel rhetorical status classifier like that developed by Teufel and Moens (2002). SUMMARISE: D, k 1 S ← {} 2 while Et.csLENGTH(ti) &lt; k 3 tj ←EXTRACT(D) 4 S ← S ∪ tj 5 D ← UPDATE(D, tj) 6 return S Figure 1: Generalised function for Filatova and Hatzivassiloglou (2004) approach to extractive summarisation. EXTRACT: D 1 cj ← arg maxcj Ecols(D) D[ti, cj] tiErows(D) 2 tk ← arg maxtkErows(D)&amp;D[tk,cj]&gt;0 SCORE(D, tk) 3 return tk UPDATE: D, ti 1 for each cj E cols(D) 2 if D[ti, cj] &gt; 0 3 for each tk E rows(D) 4 D[tk, cj] ← 0 5 D ← DELETE(D, ti) 6 return D Figure 2: Extraction and update functions for Filatova and Hatzivassiloglou (2004) modified adaptive algorithm. turns the count of word tokens in sentence ti. Figure 2 contains the EXTRACT and UPDATE functions used here.3 The EXTRACT function first identifies the concept cj not yet covered in the summary that has</context>
<context position="12643" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="2001" endWordPosition="2004">hat contain concept cj. The SCORE function is the sum of concept weights for the given text unit, i.e.: �SCORE : D, ti �� return D[ti, cj] (1) cj∈cols(D) The UPDATE function in Figure 2 aims to minimise redundancy in the summary by globally maximising the number of conceptual units covered in the output. In addition to removing the row representing the extracted text unit from the text x concept matrix D, it iterates through the remaining text units and assigns zero weights to all concepts that are covered by the extracted text unit. 3The EXTRACT and UPDATE functions in Figure 2 correspond to Filatova and Hatzivassiloglou (2004) modified adaptive algorithm and were found in preliminary experiments to be the better than the simple greedy and adaptive greedy algorithms (see Hachey (2009) for details). 422 Bush worked as an oil lease negotiator for Amoco in Denver and later started his own oil company, JNB. tf*idf (TF) jnb:3.55, amoco:3.13, oil:3.05, negotiator:3.04, lease:2.58, denver:2.45, bush:2.44, worked:2.28, started:2.21, later:2.13, own:1.96, company:1.94, ... event (EV) &lt;PER bush,worked,XFN oil&gt;:0.00023, &lt;PER bush,worked,ORG amoco&gt;:0.00011, &lt;PER bush,worked,LOC denver&gt;:0.00011, &lt;XFN oil,started,ORG jnb&gt;:0.00011</context>
<context position="14130" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="2196" endWordPosition="2199">,ORG jnb&gt;:0.00044, &lt;LOC denver,XFN oil&gt;:0.00033, ... entity pairrl (ER) &lt;ORG amoco,LOC denver&gt;:0.00311, &lt;ORG jnb,XFN oil&gt;:0.00155 Figure 3: Example sentence and various representations of sentence content. 4 Models Figure 3 contains an example sentence and its representations corresponding to the various models of sentence content explored here.4 These are described in detail in the rest of this section. 4.1 Baseline tf*idf Representation (TF) The baseline model represents sentences as tf*idfweighted bags-of-words (TF). Document frequencies for terms are derived from the same resource used by Filatova and Hatzivassiloglou (2004)–a frequency list compiled from a large sample of web pages. Term weighting is calculated using tf*idf as: w(i,j) = 1I (1 + log (tfi,j)) ∗ log ( N) (2) �VVVIdfi where tfi,j is the number of times term i occurs in sentence j and dfi is the number of documents in which term i occurs. An example sentence and its tf*idf representation can be seen in Figure 3. 4The sentence was selected from document set d47 (from the data set described in Section 5.1 below), which contains articles about Neil Bush and his role in the collapse of Silverado Savings and Loan during the U.S. Savings and Loan crisis of</context>
<context position="19454" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="3102" endWordPosition="3105"> makes it a more general model of IE. Relation connectors are derived from a model of relation types based on latent Dirichlet allocation (Blei et al., 2003) that incorporates word, entity and dependency path features from the context of a relation-forming entity mention pair (see Hachey (2009) for details). This outputs a topic distribution for each entity mention pair that corresponds to the type of relation that is described. This representation 1) models a type of generic IE that includes relations, 2) uses a connector model that abstracts away from surface-level event descriptors used by Filatova and Hatzivassiloglou (2004) and 3) does not rely on domain-specific resources like WordNet.6 For the purpose of comparison, relation triples are weighted in the same way as event triples using Equations 3 and 4 above. However, the connector pair weighting is modified to use the distribution over topics given by the LDA output.7 Relation triples generated for the example sentence in Figure 3 include &lt;ORG amoco, rd94,LOC denver&gt; and &lt;ORG amoco, rd505,LOC denver&gt;, where the connectors (i.e., rd94 and rd505) are identifiers that index particular topics from the LDA output. Here, rd94 and rd505 index topics that correspond t</context>
<context position="21826" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="3472" endWordPosition="3475">er, Hachey (2009) shows that it is still directly portable between the news and biomedical domains without modification of model parameters. 7Distributions for entity mention pairs tend to have a long uniform tail and only a few topics with higher probability. In converting to a weighting scheme, topic representations here are converted to a sparse representation where all topics in the uniform tail are removed. 424 5 Experimental Setup 5.1 Data The experiments here use the multi-document summarisation data from the 2001 Document Understanding Conference (DUC),8 which is the same data used by Filatova and Hatzivassiloglou (2004). This comprises 30 test document sets, each of which include approximately 10 news stories. Each document set is collected by a human and focuses on a particular topic. Example topics include the nomination of Clarence Thomas to the American Supreme Court, Neil Bush’s role in the collapse of Silverado Savings and Loan and the Exxon Valdez oil spill. Gold standard summaries are provided for each document set for summary lengths of 50, 100, 200 and 400 words. This helps to ensure that the systems are not over-tuned to specific summary lengths. For each summary task (i.e., all 120 document set ×</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. Event-based extractive summarization. In Proceedings of the ACL Text Summarization Branches Out Workshop, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Colin Matheson</author>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
</authors>
<title>LT TTT—a flexible tokenisation tool.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="22787" citStr="Grover et al., 2000" startWordPosition="3620" endWordPosition="3623"> oil spill. Gold standard summaries are provided for each document set for summary lengths of 50, 100, 200 and 400 words. This helps to ensure that the systems are not over-tuned to specific summary lengths. For each summary task (i.e., all 120 document set × summary length combinations), there are three distinct gold standard summaries created by different human analysts. Pre-processing includes sentence boundary identification, segmentation of words (tokenisation), labelling words with part-of-speech tags, identification of noninflected base word forms (lemmatisation) from the LT-TTT tools (Grover et al., 2000). It also includes dependency parsing using Minipar (Lin, 1998) and automatic named entity recognition using the C&amp;C tagger (Curran and Clark, 2003) trained on the data from the MUC-7 shared task (Chinchor, 1998). Weights for the various IE-based representations are calculated over each input document set. 5.2 Evaluation The evaluation uses Rouge9 to determine which representation selects content that overlaps most with human summaries. Rouge estimates the coverage of appropriate concepts (Lin, 2004) in a summary by comparing it to several humancreated reference summaries. Rouge-1 does so by c</context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>Claire Grover, Colin Matheson, Andrei Mikheev, and Marc Moens. 2000. LT TTT—a flexible tokenisation tool. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
</authors>
<title>Towards Generic Relation Extraction.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="3143" citStr="Hachey, 2009" startWordPosition="468" endWordPosition="469">traction, this includes various operations to maximise coherence such as ensuring that entity references are comprehensible and arranging the sentences in a sensible order. The current work investigates several representations of source documents. In particular, an approach from the literature based on atomic events (Filatova and Hatzivassiloglou, 2004) is compared to a novel approach based on generic relation extraction (GRE), which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters (Hachey, 2009). The various representations are substituted in the interpretation phase of a multi-document summarisation task and used as the basis for extracting sentences to be placed in the summary. System summaries are compared by calculating term overlap with reference summaries created by human analysts. 2 Motivation In seminal work on automatic summarisation, Luhn (1958) introduces a representation based on content words. These are defined as nonfunction words from the source document that are neither too frequent nor too infrequent. Luhn uses frequency to weight content words and ex420 Proceedings </context>
<context position="7504" citStr="Hachey, 2009" startWordPosition="1144" endWordPosition="1145">on pairs with an intervening verbal connector), meaning that it will not be able to address tasks where relations are at least as important as events (e.g., biographical summarisation). Second, it relies on exact matching between connectors, which is not capable of capturing latent semantic similarities (e.g., between ‘work for’ and ‘employed by’). Third, its performance is subject to the coverage of WordNet, which is used to identify action nouns. Generic relation extraction (GRE) aims to build systems that can be transferred across domains and tasks without modification of model parameters (Hachey, 2009). For relation identification (i.e., extraction of relation forming entity mention pairs), this is achieved by using general rule-based approaches and, for relation characterisation (i.e., assignment of types to relation mentions), this is achieved by using unsupervised machine learning. Hachey (2009) introduces a GRE approach that addresses the shortcomings of the atomic event approach mentioned above. First, it models a type of IE that includes relations. Second, it uses a connector model based on latent Dirichlet allocation (Blei et al., 2003), which provides a mechanism for capturing laten</context>
<context position="12803" citStr="Hachey (2009)" startWordPosition="2029" endWordPosition="2030"> 2 aims to minimise redundancy in the summary by globally maximising the number of conceptual units covered in the output. In addition to removing the row representing the extracted text unit from the text x concept matrix D, it iterates through the remaining text units and assigns zero weights to all concepts that are covered by the extracted text unit. 3The EXTRACT and UPDATE functions in Figure 2 correspond to Filatova and Hatzivassiloglou (2004) modified adaptive algorithm and were found in preliminary experiments to be the better than the simple greedy and adaptive greedy algorithms (see Hachey (2009) for details). 422 Bush worked as an oil lease negotiator for Amoco in Denver and later started his own oil company, JNB. tf*idf (TF) jnb:3.55, amoco:3.13, oil:3.05, negotiator:3.04, lease:2.58, denver:2.45, bush:2.44, worked:2.28, started:2.21, later:2.13, own:1.96, company:1.94, ... event (EV) &lt;PER bush,worked,XFN oil&gt;:0.00023, &lt;PER bush,worked,ORG amoco&gt;:0.00011, &lt;PER bush,worked,LOC denver&gt;:0.00011, &lt;XFN oil,started,ORG jnb&gt;:0.00011, ... relation (RL) &lt;ORG amoco,rd94,LOC denver&gt;:0.00039, &lt;ORG amoco,rd505,LOC denver&gt;:0.00039, &lt;XFN oil,rd92,ORG jnb&gt;:0.00002, &lt;XFN oil,rd712,ORG jnb&gt;:0.00002, </context>
<context position="18457" citStr="Hachey (2009)" startWordPosition="2941" endWordPosition="2942">current evaluation is a novel representation based on generic relation extraction (GRE). As mentioned above, GRE is a minimally supervised approach to the relation extraction task that aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters. Relation mentions are identified by taking pairs of entity mentions that have either 1) no more than two intervening words in the surface order of the sentence or 2) no more than one edge intervening on the shortest path through a dependency parse (see Hachey (2009) for details and experiments comparing different window configurations). This is stricter than the Filatova and Hatzivassiloglou approach in that entity mentions have to occur much closer or be connected by a single dependency relation. At the same time, it is less strict in the sense that an action- or eventdenoting word is not required in the context, which makes it a more general model of IE. Relation connectors are derived from a model of relation types based on latent Dirichlet allocation (Blei et al., 2003) that incorporates word, entity and dependency path features from the context of a</context>
<context position="21207" citStr="Hachey (2009)" startWordPosition="3379" endWordPosition="3380"> representations that do not model event or relation type information. These are identical to the EV and RL representations above, except they are &lt;Ent, Ent&gt; 2-tuples instead of &lt;Ent, Connector, Ent&gt; 3-tuples. That is, entity pairs are included here provided that they meet the relation mention identification constraints. They are weighted using the normalised entity pair count (Equation 4 above). Relation-based entity pairs generated for the example sentence in Figure 3 include &lt;LOC denver,ORG amoco&gt; and &lt;ORG jnb,XFN oil&gt;. 6The GRE representation here does rely on dependency parsing, however, Hachey (2009) shows that it is still directly portable between the news and biomedical domains without modification of model parameters. 7Distributions for entity mention pairs tend to have a long uniform tail and only a few topics with higher probability. In converting to a weighting scheme, topic representations here are converted to a sparse representation where all topics in the uniform tail are removed. 424 5 Experimental Setup 5.1 Data The experiments here use the multi-document summarisation data from the 2001 Document Understanding Conference (DUC),8 which is the same data used by Filatova and Hatz</context>
<context position="28008" citStr="Hachey (2009)" startWordPosition="4433" endWordPosition="4434">s suggests that they are actually complementary to some degree, meaning that a combined system based on both representations would outperform RL and ER on their own. 10In contrast to the results for the tf*idf, relation and event representations which use the modified adaptive algorithm described above, results for entity pair representations use a simplified version of the EXTRACT function that picks the text unit that has the highest score. This performed significantly better than the modified adaptive algorithm (p ≤ 0.01) for all summary lengths for ER and was indistinguishable for EE. See Hachey (2009) for details. 7 Analysis and Comparison 7.1 Complementarity Figure 4 contains results for a correlation analysis comparing the various representations. This also includes a comparison to the human upper bound (HU), computed by leave-one-out cross validation. Cells in the matrix contain the correlations values measured across document set RougeSU4 scores11 using Spearman’s p rank correlation coefficient (rs). Here, high values mean that two representations tend to perform well on the same document sets such that an ordering of document sets by Rouge scores is similar for the representations bei</context>
</contexts>
<marker>Hachey, 2009</marker>
<rawString>Ben Hachey. 2009. Towards Generic Relation Extraction. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>Ulrich Reimer</author>
</authors>
<title>Knowledgebased text summarization: Salience and generalization operators for knowledge base abstraction.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>215--232</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6282" citStr="Hahn and Reimer (1999)" startWordPosition="946" endWordPosition="949">ire domain-specific resources, e.g. White and Cardie (2002) bootstrapping approach still requires that the extraction templates be known in advance and Harabagiu and Maiorano (2002) approach depends on the WordNet lexical database, for which coverage is not guaranteed for arbitrary domains. Filatova and Hatzivassiloglou (2004) introduce methods using more general IE representations that are not based on supervised learning. Given a named entity recogniser, the rep1Comparable approaches using IE in the context of abstractive–as opposed to extractive–summarisation include work by DeJong (1982), Hahn and Reimer (1999), White et al. (2001) and Saggion and Lapalme (2002). resentation is automatically derived and consists of &lt;Ent, Connector, Ent&gt; event triples, where connectors are verbs or action nouns that occur in between the two NEs. Thus, the approach aims to perform a generic IE task that the authors refer to as atomic event extraction. This representation is shown to outperform a tf*idf baseline on a multidocument summarisation task. As we will see in Section 4.3 below, Filatova and Hatzivassiloglou’s approach has three main shortcomings. First, it focuses exclusively on simple atomic events (i.e., ent</context>
</contexts>
<marker>Hahn, Reimer, 1999</marker>
<rawString>Udo Hahn and Ulrich Reimer. 1999. Knowledgebased text summarization: Salience and generalization operators for knowledge base abstraction. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, pages 215–232. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Steven J Maiorano</author>
</authors>
<title>Multi-document summarization with GISTexter.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="5196" citStr="Harabagiu and Maiorano (2002)" startWordPosition="778" endWordPosition="781">eper semantic information. This is based on the notion that IE definitions of types for entities, relations and events provide a level of abstraction that is appropriate for automatic summarisation. Several approaches in the literature have explored the use of IE-based representations for extractive summarisation: McKeown et al. (1998) incorporate patient characteristic templates for matching potential treatments to specific patients in a medical summarisation system; White and Cardie (2002) incorporate a bootstrapped IE system based on Autoslog (Riloff, 1996) for filling event templates; and Harabagiu and Maiorano (2002) incorporate a hybrid approach that uses conventional supervised IE techniques for known topics and a more general approach based on WordNet for unknown topics.1 The problem with these systems is that they all use supervised approaches to IE that require that the IE templates be known in advance and additionally require significant investment in writing extraction rules or in annotating data for training. Where more general techniques are used, they still require domain-specific resources, e.g. White and Cardie (2002) bootstrapping approach still requires that the extraction templates be known</context>
</contexts>
<marker>Harabagiu, Maiorano, 2002</marker>
<rawString>Sanda M. Harabagiu and Steven J. Maiorano. 2002. Multi-document summarization with GISTexter. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dorit S Hochbaum</author>
</authors>
<title>Approximating covering and packing problems: set cover, vertex cover, independent set and related problems.</title>
<date>1997</date>
<booktitle>Approximation Algorithms for NP-Hard Problems,</booktitle>
<pages>94--143</pages>
<editor>In Dorit S. Hochbaum, editor,</editor>
<publisher>PWS Publishing Company,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="9943" citStr="Hochbaum, 1997" startWordPosition="1533" endWordPosition="1534">sent concepts (e.g., words, events, relations) in the input text. Each concept is either absent or present in a given textual unit. Additionally, each concept has a weight associated with it. Looking at the problem in this way makes it natural to formulate it as follows: the summary should select textual units such that there is maximal coverage of the salient conceptual units.2 This is essentially the maximum coverage problem, which has been shown to be reducible to the set covering problem, for which there are approximation algorithms in the literature that run in polynomial time or better (Hochbaum, 1997; Bienstock and Iyengar, 2004). Filatova and Hatzivassiloglou define several greedy algorithms that can be parametrised in terms of the general SUMMARISE function in Figure 1, which takes the text x concept matrix D and the maximum summary length k as input. The SUMMARISE function first initialises the summary S to the empty set. Then it enters a loop that continues until the summary reaches the desired length. Within the loop, a text unit is extracted and added to the summary after which the text x concept matrix is updated The output of the algorithm is a set S comprising the text units that</context>
</contexts>
<marker>Hochbaum, 1997</marker>
<rawString>Dorit S. Hochbaum. 1997. Approximating covering and packing problems: set cover, vertex cover, independent set and related problems. In Dorit S. Hochbaum, editor, Approximation Algorithms for NP-Hard Problems, pages 94–143. PWS Publishing Company, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC Workshop Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="22850" citStr="Lin, 1998" startWordPosition="3632" endWordPosition="3633">r summary lengths of 50, 100, 200 and 400 words. This helps to ensure that the systems are not over-tuned to specific summary lengths. For each summary task (i.e., all 120 document set × summary length combinations), there are three distinct gold standard summaries created by different human analysts. Pre-processing includes sentence boundary identification, segmentation of words (tokenisation), labelling words with part-of-speech tags, identification of noninflected base word forms (lemmatisation) from the LT-TTT tools (Grover et al., 2000). It also includes dependency parsing using Minipar (Lin, 1998) and automatic named entity recognition using the C&amp;C tagger (Curran and Clark, 2003) trained on the data from the MUC-7 shared task (Chinchor, 1998). Weights for the various IE-based representations are calculated over each input document set. 5.2 Evaluation The evaluation uses Rouge9 to determine which representation selects content that overlaps most with human summaries. Rouge estimates the coverage of appropriate concepts (Lin, 2004) in a summary by comparing it to several humancreated reference summaries. Rouge-1 does so by computing recall based on macro-averaged unigram overlap. Rouge-</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Proceedings of the LREC Workshop Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Text Summarization Branches Out Workshop,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="23292" citStr="Lin, 2004" startWordPosition="3698" endWordPosition="3699">dentification of noninflected base word forms (lemmatisation) from the LT-TTT tools (Grover et al., 2000). It also includes dependency parsing using Minipar (Lin, 1998) and automatic named entity recognition using the C&amp;C tagger (Curran and Clark, 2003) trained on the data from the MUC-7 shared task (Chinchor, 1998). Weights for the various IE-based representations are calculated over each input document set. 5.2 Evaluation The evaluation uses Rouge9 to determine which representation selects content that overlaps most with human summaries. Rouge estimates the coverage of appropriate concepts (Lin, 2004) in a summary by comparing it to several humancreated reference summaries. Rouge-1 does so by computing recall based on macro-averaged unigram overlap. Rouge-SU4 does so by calculating skip-bigram overlap where bigrams are allowed to 8http://www-nlpir.nist.gov/projects/ duc/index.html 9Rouge stands for recall-oriented understudy for gisting evaluations. While current versions also compute precision and f-score of system summaries, the evaluation here uses recall alone, which is sufficient when the length of the summaries being compared is the same. Rouge can be obtained from http://haydn.isi.e</context>
<context position="24509" citStr="Lin, 2004" startWordPosition="3883" endWordPosition="3884">E/. 1 50 100 200 400 TF 0.0797 0.1113 0.1742 0.2467 EV 0.1360 0.1776 0.2315 0.3019 RL 0.1360 0.1766 0.2412 0.3014 SU4 50 100 200 400 TF 0.0173 0.0259 0.0442 0.0693 EV 0.0376 0.0494 0.0692 0.0950 RL 0.0356 0.0491 0.0701 0.0939 Table 2: Comparison of Rouge scores for the tf*idf (TF), event (EV) and relation (RL). be composed of non-contiguous words (with as many as four words intervening). Rouge-SU4 also includes unigrams to decrease the chances of zero scores where there is no skip-bigram overlap. The configuration is based on comparisons between Rouge and human judgements of content coverage (Lin, 2004), which suggest that Rouge1 and Rouge-SU4 with stemming and removal of stop words are good measures for evaluating multi-document summarisation tasks, consistently achieving Pearson’s correlation scores above 0.72 and as high as 0.9 for longer summaries. Paired Wilcoxon signed ranks tests across document sets are used to check for significant differences between systems. The paired Wilcoxon signed ranks test is a non-parametric analogue of the paired t test. The null hypothesis is that the two populations from which the scores are sampled are identical. 6 Results Can extractive summarisation b</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of the ACL Text Summarization Branches Out Workshop, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="3510" citStr="Luhn (1958)" startWordPosition="525" endWordPosition="526">o a novel approach based on generic relation extraction (GRE), which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters (Hachey, 2009). The various representations are substituted in the interpretation phase of a multi-document summarisation task and used as the basis for extracting sentences to be placed in the summary. System summaries are compared by calculating term overlap with reference summaries created by human analysts. 2 Motivation In seminal work on automatic summarisation, Luhn (1958) introduces a representation based on content words. These are defined as nonfunction words from the source document that are neither too frequent nor too infrequent. Luhn uses frequency to weight content words and ex420 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 420–429, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tracts sentences with the highest combined content scores to form the summary. Subsequent work adapted the tf*idf weighting scheme, where term frequency (tf) is combined with inverse document frequency (idf), an inverse measure</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Hans P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<location>John Benjamins, Amsterdam/Philadelphia.</location>
<contexts>
<context position="1197" citStr="Mani, 2001" startWordPosition="175" endWordPosition="176">ferred across domains and tasks without modification of model parameters. Results demonstrate performance that is significantly higher than a non-trivial baseline that uses tf*idf-weighted words and at least as good as a comparable but less general approach from the literature. Analysis shows that the representations compared are complementary, suggesting that extraction performance could be further improved through system combination. 1 Introduction The goal of summarisation is to take an information source, extract content from it, and present the most important content in a condensed form (Mani, 2001). The field of automatic summarisation (Mani, 2001; Sp¨arck Jones, 2007) aims to create tools that address various summarisation tasks with minimal human intervention. Extractive approaches to automatic summarisation create representations of the source document that are generally based on an easily identified text sub-unit such as sentences or paragraphs. These representations are then used to identify representative or otherwise important snippets of text to place in the summary. Following Sp¨arck Jones (2007), summarisation systems can be characterised with respect to their approach to thre</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic Summarization. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Desmond A Jordan</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Generating patient-specific summaries of online literature.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Intelligent Text Summarization,</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="4904" citStr="McKeown et al. (1998)" startWordPosition="737" endWordPosition="741">ivial baselines. The problem is that these shallow features often break down where underlying linguistic content needs to be compared rather than just surface structure. The use of representations based on information extraction (IE) has been suggested as one approach to capturing deeper semantic information. This is based on the notion that IE definitions of types for entities, relations and events provide a level of abstraction that is appropriate for automatic summarisation. Several approaches in the literature have explored the use of IE-based representations for extractive summarisation: McKeown et al. (1998) incorporate patient characteristic templates for matching potential treatments to specific patients in a medical summarisation system; White and Cardie (2002) incorporate a bootstrapped IE system based on Autoslog (Riloff, 1996) for filling event templates; and Harabagiu and Maiorano (2002) incorporate a hybrid approach that uses conventional supervised IE techniques for known topics and a more general approach based on WordNet for unknown topics.1 The problem with these systems is that they all use supervised approaches to IE that require that the IE templates be known in advance and additio</context>
</contexts>
<marker>McKeown, Jordan, Hatzivassiloglou, 1998</marker>
<rawString>Kathleen R. McKeown, Desmond A. Jordan, and Vasileios Hatzivassiloglou. 1998. Generating patient-specific summaries of online literature. In Proceedings of the AAAI Spring Symposium on Intelligent Text Summarization, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence,</booktitle>
<location>Portland, OR, USA.</location>
<contexts>
<context position="5133" citStr="Riloff, 1996" startWordPosition="771" endWordPosition="772"> been suggested as one approach to capturing deeper semantic information. This is based on the notion that IE definitions of types for entities, relations and events provide a level of abstraction that is appropriate for automatic summarisation. Several approaches in the literature have explored the use of IE-based representations for extractive summarisation: McKeown et al. (1998) incorporate patient characteristic templates for matching potential treatments to specific patients in a medical summarisation system; White and Cardie (2002) incorporate a bootstrapped IE system based on Autoslog (Riloff, 1996) for filling event templates; and Harabagiu and Maiorano (2002) incorporate a hybrid approach that uses conventional supervised IE techniques for known topics and a more general approach based on WordNet for unknown topics.1 The problem with these systems is that they all use supervised approaches to IE that require that the IE templates be known in advance and additionally require significant investment in writing extraction rules or in annotating data for training. Where more general techniques are used, they still require domain-specific resources, e.g. White and Cardie (2002) bootstrapping</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the 14th National Conference on Artificial Intelligence, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Guy Lapalme</author>
</authors>
<title>Generating indicative-informative summaries with SumUM.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="6334" citStr="Saggion and Lapalme (2002)" startWordPosition="955" endWordPosition="958">rdie (2002) bootstrapping approach still requires that the extraction templates be known in advance and Harabagiu and Maiorano (2002) approach depends on the WordNet lexical database, for which coverage is not guaranteed for arbitrary domains. Filatova and Hatzivassiloglou (2004) introduce methods using more general IE representations that are not based on supervised learning. Given a named entity recogniser, the rep1Comparable approaches using IE in the context of abstractive–as opposed to extractive–summarisation include work by DeJong (1982), Hahn and Reimer (1999), White et al. (2001) and Saggion and Lapalme (2002). resentation is automatically derived and consists of &lt;Ent, Connector, Ent&gt; event triples, where connectors are verbs or action nouns that occur in between the two NEs. Thus, the approach aims to perform a generic IE task that the authors refer to as atomic event extraction. This representation is shown to outperform a tf*idf baseline on a multidocument summarisation task. As we will see in Section 4.3 below, Filatova and Hatzivassiloglou’s approach has three main shortcomings. First, it focuses exclusively on simple atomic events (i.e., entity mention pairs with an intervening verbal connect</context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>Horacio Saggion and Guy Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational Linguistics, 28(4):497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="4207" citStr="Jones, 1972" startWordPosition="633" endWordPosition="634"> words from the source document that are neither too frequent nor too infrequent. Luhn uses frequency to weight content words and ex420 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 420–429, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tracts sentences with the highest combined content scores to form the summary. Subsequent work adapted the tf*idf weighting scheme, where term frequency (tf) is combined with inverse document frequency (idf), an inverse measure of term occurrence across documents that serves to downweight common words (Sp¨arck Jones, 1972). In modern work, tf*idf representations are often used as simple but non-trivial baselines. The problem is that these shallow features often break down where underlying linguistic content needs to be compared rather than just surface structure. The use of representations based on information extraction (IE) has been suggested as one approach to capturing deeper semantic information. This is based on the notion that IE definitions of types for entities, relations and events provide a level of abstraction that is appropriate for automatic summarisation. Several approaches in the literature have</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sp¨arck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>Automatic summarising: The state of the art.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--1449</pages>
<contexts>
<context position="1269" citStr="Jones, 2007" startWordPosition="186" endWordPosition="187">s. Results demonstrate performance that is significantly higher than a non-trivial baseline that uses tf*idf-weighted words and at least as good as a comparable but less general approach from the literature. Analysis shows that the representations compared are complementary, suggesting that extraction performance could be further improved through system combination. 1 Introduction The goal of summarisation is to take an information source, extract content from it, and present the most important content in a condensed form (Mani, 2001). The field of automatic summarisation (Mani, 2001; Sp¨arck Jones, 2007) aims to create tools that address various summarisation tasks with minimal human intervention. Extractive approaches to automatic summarisation create representations of the source document that are generally based on an easily identified text sub-unit such as sentences or paragraphs. These representations are then used to identify representative or otherwise important snippets of text to place in the summary. Following Sp¨arck Jones (2007), summarisation systems can be characterised with respect to their approach to three main sub-tasks: 1) interpretation, 2) transformation and 3) generation</context>
</contexts>
<marker>Jones, 2007</marker>
<rawString>Karen Sp¨arck Jones. 2007. Automatic summarising: The state of the art. Information Processing and Management, 43:1449–1481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles – experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="11077" citStr="Teufel and Moens (2002)" startWordPosition="1714" endWordPosition="1717">t matrix is updated The output of the algorithm is a set S comprising the text units that make up the summary. For the experiments reported here, the text units t are sentences and LENGTH(ti) re2While not considered in the current experiments, a more discourse-oriented approach could be derived within the set cover framework by down-weighting conceptual units that occur e.g. in portions of the source documents that describe background information, where text segments containing background information could be identified using a sentencelevel rhetorical status classifier like that developed by Teufel and Moens (2002). SUMMARISE: D, k 1 S ← {} 2 while Et.csLENGTH(ti) &lt; k 3 tj ←EXTRACT(D) 4 S ← S ∪ tj 5 D ← UPDATE(D, tj) 6 return S Figure 1: Generalised function for Filatova and Hatzivassiloglou (2004) approach to extractive summarisation. EXTRACT: D 1 cj ← arg maxcj Ecols(D) D[ti, cj] tiErows(D) 2 tk ← arg maxtkErows(D)&amp;D[tk,cj]&gt;0 SCORE(D, tk) 3 return tk UPDATE: D, ti 1 for each cj E cols(D) 2 if D[ti, cj] &gt; 0 3 for each tk E rows(D) 4 D[tk, cj] ← 0 5 D ← DELETE(D, ti) 6 return D Figure 2: Extraction and update functions for Filatova and Hatzivassiloglou (2004) modified adaptive algorithm. turns the count</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles – experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Claire Cardie</author>
</authors>
<title>Selecting sentences for multidocument summaries using randomized local search.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Automatic Summarization,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5063" citStr="White and Cardie (2002)" startWordPosition="759" endWordPosition="762">e structure. The use of representations based on information extraction (IE) has been suggested as one approach to capturing deeper semantic information. This is based on the notion that IE definitions of types for entities, relations and events provide a level of abstraction that is appropriate for automatic summarisation. Several approaches in the literature have explored the use of IE-based representations for extractive summarisation: McKeown et al. (1998) incorporate patient characteristic templates for matching potential treatments to specific patients in a medical summarisation system; White and Cardie (2002) incorporate a bootstrapped IE system based on Autoslog (Riloff, 1996) for filling event templates; and Harabagiu and Maiorano (2002) incorporate a hybrid approach that uses conventional supervised IE techniques for known topics and a more general approach based on WordNet for unknown topics.1 The problem with these systems is that they all use supervised approaches to IE that require that the IE templates be known in advance and additionally require significant investment in writing extraction rules or in annotating data for training. Where more general techniques are used, they still require</context>
</contexts>
<marker>White, Cardie, 2002</marker>
<rawString>Michael White and Claire Cardie. 2002. Selecting sentences for multidocument summaries using randomized local search. In Proceedings of the ACL Workshop on Automatic Summarization, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Tanya Korelsky</author>
<author>Claire Cardie</author>
<author>Vincent Ng</author>
<author>David Pierce</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Multidocument summarization via information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the 1st International Conference on Human Language Technology Research,</booktitle>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="6303" citStr="White et al. (2001)" startWordPosition="950" endWordPosition="953">urces, e.g. White and Cardie (2002) bootstrapping approach still requires that the extraction templates be known in advance and Harabagiu and Maiorano (2002) approach depends on the WordNet lexical database, for which coverage is not guaranteed for arbitrary domains. Filatova and Hatzivassiloglou (2004) introduce methods using more general IE representations that are not based on supervised learning. Given a named entity recogniser, the rep1Comparable approaches using IE in the context of abstractive–as opposed to extractive–summarisation include work by DeJong (1982), Hahn and Reimer (1999), White et al. (2001) and Saggion and Lapalme (2002). resentation is automatically derived and consists of &lt;Ent, Connector, Ent&gt; event triples, where connectors are verbs or action nouns that occur in between the two NEs. Thus, the approach aims to perform a generic IE task that the authors refer to as atomic event extraction. This representation is shown to outperform a tf*idf baseline on a multidocument summarisation task. As we will see in Section 4.3 below, Filatova and Hatzivassiloglou’s approach has three main shortcomings. First, it focuses exclusively on simple atomic events (i.e., entity mention pairs wit</context>
</contexts>
<marker>White, Korelsky, Cardie, Ng, Pierce, Wagstaff, 2001</marker>
<rawString>Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff. 2001. Multidocument summarization via information extraction. In Proceedings of the 1st International Conference on Human Language Technology Research, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kam-Fai Wong</author>
<author>Mingli Wu</author>
<author>Wenjie Li</author>
</authors>
<title>Extractive summarization using supervised and semisupervised learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="33626" citStr="Wong et al. (2008)" startWordPosition="5333" endWordPosition="5336"> than tf*idf at capturing this information. This summary also illustrates an unintended side-effect of the relation representation where the generic relation identification algorithm finds relations between components of lexical compounds or multi-word phrases. The representation for the third sentence in the RL summary, for example, includes a relation between ORG police and XFN chief in addition to true positive relations e.g. between ORG police and PER darylgates and false positive relations e.g. between PER tombradley and ORG police. 7.3 Comparison to Supervised Extraction Related work by Wong et al. (2008) also compares representations for sentence extraction on 427 TF [S1] Mr. Williams likened the report to the Knapp Commission, a 1970s blue-ribbon study that exposed (0.016) widespread corruption in the New York Police Department and led to significant improvements there. [S2] (20/29) “There’s no doubt in our mind that the only reason they stopped Joe Morgan was because he is black and he was the first black who happened to come by,” said William Barnes, one of the attorneys representing the former ballplayer. [S3] Joseph McNamara, retired chief of San Jose’s department and now a fellow at Sta</context>
</contexts>
<marker>Wong, Wu, Li, 2008</marker>
<rawString>Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive summarization using supervised and semisupervised learning. In Proceedings of the 22nd International Conference on Computational Linguistics, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>