<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023117">
<title confidence="0.8722365">
Improving nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor grammars
</title>
<author confidence="0.995485">
Mark Johnson Sharon Goldwater
</author>
<affiliation confidence="0.999883">
Brown University University of Edinburgh
</affiliation>
<address confidence="0.907705">
Providence, RI Edinburgh EH8 9AB
</address>
<email confidence="0.989064">
Mark Johnson@Brown.edu sgwater@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942">
One of the reasons nonparametric Bayesian
inference is attracting attention in computa-
tional linguistics is because it provides a prin-
cipled way of learning the units of generaliza-
tion together with their probabilities. Adaptor
grammars are a framework for defining a va-
riety of hierarchical nonparametric Bayesian
models. This paper investigates some of
the choices that arise in formulating adap-
tor grammars and associated inference proce-
dures, and shows that they can have a dra-
matic impact on performance in an unsuper-
vised word segmentation task. With appro-
priate adaptor grammars and inference proce-
dures we achieve an 87% word token f-score
on the standard Brent version of the Bernstein-
Ratner corpus, which is an error reduction of
over 35% over the best previously reported re-
sults for this corpus.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907">
Most machine learning algorithms used in computa-
tional linguistics are parametric, i.e., they learn a nu-
merical weight (e.g., a probability) associated with
each feature, where the set of features is fixed be-
fore learning begins. Such procedures can be used
to learn features or structural units by embedding
them in a “propose-and-prune” algorithm: a feature
proposal component proposes potentially useful fea-
tures (e.g., combinations of the currently most useful
features), which are then fed to a parametric learner
that estimates their weights. After estimating fea-
ture weights and pruning “useless” low-weight fea-
tures, the cycle repeats. While such algorithms can
achieve impressive results (Stolcke and Omohundro,
1994), their effectiveness depends on how well the
feature proposal step relates to the overall learning
objective, and it can take considerable insight and
experimentation to devise good feature proposals.
One of the main reasons for the recent interest in
nonparametric Bayesian inference is that it offers a
systematic framework for structural inference, i.e.,
inferring the features relevant to a particular prob-
lem as well as their weights. (Here “nonparamet-
ric” means that the models do not have a fixed set of
parameters; our nonparametric models do have pa-
rameters, but the particular parameters in a model
are learned along with their values). Dirichlet Pro-
cesses and their associated predictive distributions,
Chinese Restaurant Processes, are one kind of non-
parametric Bayesian model that has received consid-
erable attention recently, in part because they can be
composed in hierarchical fashion to form Hierarchi-
cal Dirichlet Processes (HDP) (Teh et al., 2006).
Lexical acquisition is an ideal test-bed for explor-
ing methods for inferring structure, where the fea-
tures learned are the words of the language. (Even
the most hard-core nativists agree that the words of a
language must be learned). We use the unsupervised
word segmentation problem as a test case for eval-
uating structural inference in this paper. Nonpara-
metric Bayesian methods produce state-of-the-art
performance on this task (Goldwater et al., 2006a;
Goldwater et al., 2007; Johnson, 2008).
In a computational linguistics setting it is natu-
ral to try to align the HDP hierarchy with the hi-
erarchy defined by a grammar. Adaptor grammars,
which are one way of doing this, make it easy to ex-
plore a wide variety of HDP grammar-based mod-
els. Given an appropriate adaptor grammar, the fea-
</bodyText>
<page confidence="0.972136">
317
</page>
<note confidence="0.8907405">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999974272727273">
tures learned by adaptor grammars can correspond
to linguistic units such as words, syllables and col-
locations. Different adaptor grammars encode dif-
ferent assumptions about the structure of these units
and how they relate to each other. A generic adaptor
grammar inference program infers these units from
training data, making it easy to investigate how these
assumptions affect learning (Johnson, 2008).1
However, there are a number of choices in the de-
sign of adaptor grammars and the associated infer-
ence procedure. While this paper studies the im-
pact of these on the word segmentation task, these
choices arise in other nonparametric Bayesian infer-
ence problems as well, so our results should be use-
ful more generally. The rest of this paper is orga-
nized as follows. The next section reviews adaptor
grammars and presents three different adaptor gram-
mars for word segmentation that serve as running
examples in this paper. Adaptor grammars contain
a large number of adjustable parameters, and Sec-
tion 3 discusses how these can be estimated using
Bayesian techniques. Section 4 examines several
implementation options within the adaptor grammar
inference algorithm and shows that they can make
a significant impact on performance. Cumulatively
these changes make a significant difference in word
segmentation accuracy: our final adaptor grammar
performs unsupervised word segmentation with an
87% token f-score on the standard Brent version
of the Bernstein-Ratner corpus (Bernstein-Ratner,
1987; Brent and Cartwright, 1996), which is an er-
ror reduction of over 35% compared to the best pre-
viously reported results on this corpus.
</bodyText>
<sectionHeader confidence="0.960895" genericHeader="method">
2 Adaptor grammars
</sectionHeader>
<bodyText confidence="0.99825275">
This section informally introduces adaptor gram-
mars using unsupervised word segmentation as a
motivating application; see Johnson et al. (2007b)
for a formal definition of adaptor grammars.
Consider the problem of learning language from
continuous speech: segmenting each utterance into
words is a nontrivial problem that language learn-
ers must solve. Elman (1990) introduced an ideal-
ized version of this task, and Brent and Cartwright
(1996) presented a version of it where the data
consists of unsegmented phonemic representations
of the sentences in the Bernstein-Ratner corpus of
</bodyText>
<footnote confidence="0.999172">
1The adaptor grammar inference program is available for
download at http://www.cog.brown.edu/˜mj/Software.htm.
</footnote>
<bodyText confidence="0.980769923076923">
child-directed speech (Bernstein-Ratner, 1987). Be-
cause these phonemic representations are obtained
by looking up orthographic forms in a pronounc-
ing dictionary and appending the results, identifying
the word tokens is equivalent to finding the locations
of the word boundaries. For example, the phoneme
string corresponding to “you want to see the book”
(with its correct segmentation indicated) is as fol-
lows:
y△uNw△a△n△tNt△uNs △iND△6Nb△U△k
We can represent any possible segmentation of any
possible sentence as a tree generated by the follow-
ing unigram grammar.
</bodyText>
<equation confidence="0.8785905">
Sentence → Word+
Word → Phoneme+
</equation>
<bodyText confidence="0.995532666666667">
The nonterminal Phoneme expands to each pos-
sible phoneme; the underlining, which identifies
“adapted nonterminals”, will be explained below. In
this paper “+” abbreviates right-recursion through a
dummy nonterminal, i.e., the unigram grammar ac-
tually is:
</bodyText>
<figure confidence="0.728557">
Sentence → Word
Sentence → Word Sentence
Word → Phonemes
Phonemes → Phoneme
</figure>
<subsectionHeader confidence="0.444665">
Phonemes → Phoneme Phonemes
</subsectionHeader>
<bodyText confidence="0.9866654375">
A PCFG with these productions can represent all
possible segmentations of any Sentence into a se-
quence of Words. But because it assumes that the
probability of a word is determined purely by mul-
tiplying together the probability of its individual
phonemes, it has no way to encode the fact that cer-
tain strings of phonemes (the words of the language)
have much higher probabilities than other strings
containing the same phonemes. In order to do this,
a PCFG would need productions like the following
one, which encodes the fact that “want” is a Word.
Word → w a n t
Adaptor grammars can be viewed as a way of for-
malizing this idea. Adaptor grammars learn the
probabilities of entire subtrees, much as in tree sub-
stitution grammar (Joshi, 2003) and DOP (Bod,
</bodyText>
<page confidence="0.99815">
318
</page>
<bodyText confidence="0.9994655625">
1998). (For computational efficiency reasons adap-
tor grammars require these subtrees to expand to ter-
minals). The set of possible adapted tree fragments
is the set of all subtrees generated by the CFG whose
root label is a member of the set of adapted non-
terminals A (adapted nonterminals are indicated by
underlining in this paper). For example, in the uni-
gram adaptor grammar A = {Word}, which means
that the adaptor grammar inference procedure learns
the probability of each possible Word subtree. Thus
adaptor grammars are simple models of structure
learning in which adapted subtrees are the units of
generalization.
One might try to reduce adaptor grammar infer-
ence to PCFG parameter estimation by introducing
a context-free rule for each possible adapted subtree,
but such an attempt would fail because the number
of such adapted subtrees, and hence the number of
corresponding rules, is unbounded. However non-
parametric Bayesian inference techniques permit us
to sample from this infinite set of adapted subtrees,
and only require us to instantiate the finite number
of them needed to analyse the finite training data.
An adaptor grammar is a 7-tuple
(N, W, R, 5, 0, A, C) where (N, W, R, 5, 0) is
a PCFG with nonterminals N, terminals W, rules
R, start symbol 5 E N and rule probabilities 0,
where θr is the probability of rule r E R, A C_ N is
the set of adapted nonterminals and C is a vector
of adaptors indexed by elements of A, so CX is the
adaptor for adapted nonterminal X E A.
Informally, an adaptor CX nondeterministically
maps a stream of trees from a base distribution HX
whose support is TX (the set of subtrees whose root
node is X E N generated by the grammar’s rules)
into another stream of trees whose support is also
TX. In adaptor grammars the base distributions HX
are determined by the PCFG rules expanding X and
the other adapted distributions, as explained in John-
son et al. (2007b). When called upon to generate an-
other sample tree, the adaptor either generates and
returns a fresh tree from HX or regenerates a tree
it has previously emitted, so in general the adapted
distribution differs from the base distribution.
This paper uses adaptors based on Chinese
Restaurant Processes (CRPs) or Pitman-Yor Pro-
cesses (PYPs) (Pitman, 1995; Pitman and Yor, 1997;
Ishwaran and James, 2003). CRPs and PYPs non-
deterministically generate infinite sequences of nat-
ural numbers z1, z2,. . ., where z1 = 1 and each
zn+1 &lt; m + 1 where m = max(z1, ... , zn). In the
“Chinese Restaurant” metaphor samples produced
by the adaptor are viewed as “customers” and zn
is the index of the “table” that the nth customer is
seated at. In adaptor grammars each table in the
adaptor CX is labeled with a tree sampled from the
base distribution HX that is shared by all customers
at that table; thus the nth sample tree from the adap-
tor CX is the znth sample from HX.
CRPs and PYPs differ in exactly how the
sequence {zk} is generated. Suppose z =
(z1, ... , zn) have already been generated and m =
max(z). Then a CRP generates the next table index
zn+1 according to the following distribution:
</bodyText>
<equation confidence="0.982074">
P(Z k I z) oc { nk(z) if k &lt; m
n+1— α if k = m + 1
</equation>
<bodyText confidence="0.999866133333333">
where nk(z) is the number of times table k appears
in z and α &gt; 0 is an adjustable parameter that deter-
mines how often a new table is chosen. This means
that if CX is a CRP adaptor then the next tree tn+1
it generates is the same as a previously generated
tree t′ with probability proportional to the number
of times CX has generated t′ before, and is a “fresh”
tree t sampled from HX with probability propor-
tional to αXHX(t). This leads to a powerful “rich-
get-richer” effect in which popular trees are gener-
ated with increasingly high probabilities.
Pitman-Yor Processes can control the strength of
this effect somewhat by moving mass from existing
tables to the base distribution. The PYP predictive
distribution is:
</bodyText>
<equation confidence="0.9967215">
_ r nk(z) − a if k &lt; m
P(Zn+1 k�z) °� S ma + b ifk=m+1
</equation>
<bodyText confidence="0.999985769230769">
where a E [0, 1] and b &gt; 0 are adjustable parame-
ters. It’s easy to see that the CRP is a special case of
the PRP where a = 0 and b = α.
Each adaptor in an adaptor grammar can be
viewed as estimating the probability of each adapted
subtree t; this probability can differ substantially
from t’s probability HX(t) under the base distribu-
tion. Because Words are adapted in the unigram
adaptor grammar it effectively estimates the proba-
bility of each Word tree separately; the sampling es-
timators described in section 4 only instantiate those
Words actually used in the analysis of Sentences in
the corpus. While the Word adaptor will generally
</bodyText>
<page confidence="0.996344">
319
</page>
<bodyText confidence="0.999560884615385">
prefer to reuse Words that have been used elsewhere
in the corpus, it is always possible to generate a fresh
Word using the CFG rules expanding Word into a
string of Phonemes.
We assume for now that all CFG rules RX ex-
panding the nonterminal X ∈ N have the same
probability (although we will explore estimating θ
below), so the base distribution HWord is a “mon-
keys banging on typewriters” model. That means the
unigram adaptor grammar implements the Goldwa-
ter et al. (2006a) unigram word segmentation model,
and in fact it produces segmentations of similar ac-
curacies, and exhibits the same characteristic under-
segmentation errors. As Goldwater et al. point out,
because Words are the only units of generalization
available to a unigram model it tends to misanal-
yse collocations as words, resulting in a marked ten-
dancy to undersegment.
Goldwater et al. demonstrate that modelling bi-
gram dependencies mitigates this undersegmenta-
tion. While adaptor grammars cannot express the
Goldwater et al. bigram model, they can get much
the same effect by directly modelling collocations
(Johnson, 2008). A collocation adaptor grammar
generates a Sentence as a sequence of Collocations,
each of which expands to a sequence of Words.
</bodyText>
<equation confidence="0.921107666666667">
Sentence → Colloc+
Colloc → Word+
Word → Phoneme+
</equation>
<bodyText confidence="0.999524892857143">
Because Colloc is adapted, the collocation adap-
tor grammar learns Collocations as well as Words.
(Presumably these approximate syntactic, semantic
and pragmatic interword dependencies). Johnson
reported that the collocation adaptor grammar seg-
ments as well as the Goldwater et al. bigram model,
which we confirm here.
Recently other researchers have emphasised the
utility of phonotactic constraints (i.e., modeling
the allowable phoneme sequences at word onsets
and endings) for word segmentation (Blanchard and
Heinz, 2008; Fleck, 2008). Johnson (2008) points
out that adaptor grammars that model words as se-
quences of syllables can learn and exploit these con-
straints, significantly improving segmentation accu-
racy. Here we present an adaptor grammar that mod-
els collocations together with these phonotactic con-
straints. This grammar is quite complex, permitting
us to study the effects of the various model and im-
plementation choices described below on a complex
hierarchical nonparametric Bayesian model.
The collocation-syllable adaptor grammar gen-
erates a Sentence in terms of three levels of
Collocations (enabling it to capture a wider range
of interword dependencies), and generates Words as
sequences of 1 to 4 Syllables. Syllables are subcat-
egorized as to whether they are initial (I), final (F) or
both (IF).
</bodyText>
<equation confidence="0.971368176470588">
Sentence → Colloc3+
Colloc3 → Colloc2+
Colloc2 → Colloc1+
Colloc1 → Word+
Word → SyllableIF
Word → SyllableI (Syllable) (Syllable) SyllableF
Syllable → Onset Rhyme
Onset → Consonant+
Rhyme → Nucleus Coda
Nucleus → Vowel+
Coda → Consonant+
SyllableIF → OnsetI RhymeF
OnsetI → Consonant+
RhymeF → Nucleus CodaF
CodaF → Consonant+
SyllableI → OnsetI Rhyme
SyllableF → Onset RhymeF
</equation>
<bodyText confidence="0.999963076923077">
Here Consonant and Vowel expand to all possible
consonants and vowels respectively, and the paren-
theses in the expansion of Word indicate optional-
ity. Because Onsets and Codas are adapted, the
collocation-syllable adaptor grammar learns the pos-
sible consonant sequences that begin and end syl-
lables. Moreover, because Onsets and Codas are
subcategorized based on whether they are word-
peripheral, the adaptor grammar learns which con-
sonant clusters typically appear at word boundaries,
even though the input contains no explicit word
boundary information (apart from what it can glean
from the sentence boundaries).
</bodyText>
<sectionHeader confidence="0.9730485" genericHeader="method">
3 Bayesian estimation of adaptor
grammar parameters
</sectionHeader>
<bodyText confidence="0.999160666666667">
Adaptor grammars as defined in section 2 have a
large number of free parameters that have to be
chosen by the grammar designer; a rule probabil-
ity θr for each PCFG rule r ∈ R and either one or
two hyperparameters for each adapted nonterminal
X ∈ A, depending on whether Chinese Restaurant
</bodyText>
<page confidence="0.986945">
320
</page>
<bodyText confidence="0.999430166666667">
or Pitman-Yor Processes are used as adaptors. It’s
difficult to have intuitions about the appropriate set-
tings for the latter parameters, and finding the opti-
mal values for these parameters by some kind of ex-
haustive search is usually computationally impracti-
cal. Previous work has adopted an expedient such as
parameter tying. For example, Johnson (2008) set
θ by requiring all productions expanding the same
nonterminal to have the same probability, and used
Chinese Restaurant Process adaptors with tied pa-
rameters αX, which was set using a grid search.
We now describe two methods of dealing with the
large number of parameters in these models that are
both more principled and more practical than the ap-
proaches described above. First, we can integrate
out θ, and second, we can infer values for the adap-
tor hyperparameters using sampling. These meth-
ods (the latter in particular) make it practical to use
Pitman-Yor Process adaptors in complex grammars
such as the collocation-syllable adaptor grammar,
where it is impractical to try to find optimal parame-
ter values by grid search. As we will show, they also
improve segmentation accuracy, sometimes dramat-
ically.
</bodyText>
<subsectionHeader confidence="0.998254">
3.1 Integrating out θ
</subsectionHeader>
<bodyText confidence="0.999945222222222">
Johnson et al. (2007a) describe Gibbs samplers for
Bayesian inference of PCFG rule probabilities θ,
and these techniques can be used directly with adap-
tor grammars as well. Just as in that paper, we
place Dirichlet priors on θ: here θX is the subvector
of θ corresponding to rules expanding nonterminal
X E N, and βX is a corresponding vector of posi-
tive real numbers specifying the hyperparameters of
the corresponding Dirichlet distributions:
</bodyText>
<equation confidence="0.991391">
P(θ  |β) = � Dir(θX  |βX)
X∈N
</equation>
<bodyText confidence="0.999897">
Because the Dirichlet distribution is conjugate to the
multinomial distribution, it is possible to integrate
out the rule probabilities θ, producing the “collapsed
sampler” described in Johnson et al. (2007a).
In our experiments we chose an uniform prior
Qr = 1 for all rules r E R. As Table 1 shows,
integrating out θ only has a major effect on re-
sults when the adaptor hyperparameters themselves
are not sampled, and even then it did not have
a large effect on the collocation-syllable adaptor
grammar. This is not too surprising: because the
Onset, Nucleus and Coda adaptors in this gram-
mar learn the probabilities of these building blocks
of words, the phoneme probabilities (which is most
of what θ encodes) play less important a role.
</bodyText>
<subsectionHeader confidence="0.999236">
3.2 Slice sampling adaptor hyperparameters
</subsectionHeader>
<bodyText confidence="0.999276473684211">
As far as we know, there are no conjugate priors for
the adaptor hyperparameters aX or bX (which cor-
responds to αX in a Chinese Restaurant Process),
so it is not possible to integrate them out as we did
with the rule probabilities θ. However, it is possible
to perform Bayesian inference by putting a prior on
them and sampling their values.
Because we have no strong intuitions about the
values of these parameters we chose uninformative
priors. We chose a uniform Beta(1,1) prior on aX,
and a “vague” Gamma(10, 0.1) prior on bX = αX
(MacKay, 2003). (We experimented with other pa-
rameters in the Gamma prior, but found no signifi-
cant difference in performance).
After each Gibbs sweep through the parse trees t
we resampled each of the adaptor parameters from
the posterior distribution of the parameter using a
slice sampler 10 times. For example, we resample
each bX from:
</bodyText>
<equation confidence="0.990695">
P(bX  |t) a P(t  |bX) Gamma(bX  |10, 0.1)
</equation>
<bodyText confidence="0.999803190476191">
Here P(t  |bX) is the likelihood of the current se-
quence of sample parse trees (we only need the fac-
tors that depend on bX) and Gamma(bX  |10, 0.1)
is the prior. The same formula is used for sampling
aX, except that the prior is now a flat Beta(1,1) dis-
tribution.
In general we cannot even compute the normaliz-
ing constants for these posterior distributions, so we
chose a sampler that does not require this. We use a
slice sampler here because it does not require a pro-
posal distribution (Neal, 2003). (We initially tried
a Metropolis-Hastings sampler but were unable to
find a proposal distribution that had reasonable ac-
ceptance ratios for all of our adaptor grammars).
As Table 1 makes clear, sampling the adaptor pa-
rameters makes a significant difference, especially
on the collocation-syllable adaptor grammar. This
is not surprising, as the adaptors in that grammar
play many different roles and there is no reason to
to expect the optimal values of their parameters to
be similar.
</bodyText>
<page confidence="0.989826">
321
</page>
<figure confidence="0.999461192307692">
Word token f-scores
Sample average
Max. Marginal
colloc
colloc-syll
unigram
Condition
Batch initialization
Table label resampling
Integrate out θ
Sample αX = bX
Sample aX
colloc
colloc-syll
unigram
• • • • • 0.55 0.74 0.85
0.56 0.76 0.87
• • • •
• • •
• •
• • • •
• • • •
• • • •
• • •
• •
• • •
</figure>
<bodyText confidence="0.959164222222222">
0.55 0.72 0.84
0.55 0.72 0.78
0.54 0.66 0.75
0.54 0.70 0.87
0.55 0.42 0.54
0.74 0.83 0.88
0.75 0.43 0.74
0.71 0.41 0.76
0.71 0.73 0.87
0.56 0.74 0.84
0.57 0.75 0.78
0.56 0.69 0.76
0.56 0.74 0.88
0.57 0.51 0.55
0.81 0.86 0.89
0.80 0.56 0.82
0.77 0.49 0.82
0.77 0.75 0.88
</bodyText>
<tableCaption confidence="0.7520575">
Table 1: Word segmentation accuracy measured by word token f-scores on Brent’s version of the Bernstein-Ratner
corpus as a function of adaptor grammar, adaptor and estimation procedure. Pitman-Yor Process adaptors were used
when ax was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where θ was not integrated
out it was set uniformly, and all αX = bx were set to 100 they were not sampled.
</tableCaption>
<sectionHeader confidence="0.999414" genericHeader="method">
4 Inference for adaptor grammars
</sectionHeader>
<bodyText confidence="0.999944588235294">
Johnson et al. (2007b) describe the basic adaptor
grammar inference procedure that we use here. That
paper leaves unspecified a number of implemen-
tation details, which we show can make a crucial
difference to segmentation accuracy. The adaptor
grammar algorithm is basically a Gibbs sampler of
the kind widely used for nonparametric Bayesian in-
ference (Blei et al., 2004; Goldwater et al., 2006b;
Goldwater et al., 2006a), so it seems reasonable to
expect that at least some of the details discussed be-
low will be relevant to other applications as well.
The inference algorithm maintains a vector t =
(ti, ... , tr,,) of sample parses, where tz E TS is a
parse for the ith sentence wz. It repeatedly chooses a
sentence wz at random and resamples the parse tree
tz for wz from P(tz  |t−z, wz), i.e., conditioned on wz
and the parses t−z of all sentences except wz.
</bodyText>
<subsectionHeader confidence="0.988202">
4.1 Maximum marginal decoding
</subsectionHeader>
<bodyText confidence="0.999974">
Sampling algorithms like ours produce a stream of
samples from the posterior distribution over parses
of the training data. It is standard to take the out-
put of the algorithm to be the last sample produced,
and evaluate those parses. In some other applica-
tions of nonparametric Bayesian inference involv-
ing latent structure (e.g., clustering) it is difficult to
usefully exploit multiple samples, but that is not the
case here.
In maximum marginal decoding we map each
sample parse tree t onto its corresponding word seg-
mentation s, marginalizing out irrelevant detail in
t. (For example, the collocation-syllable adaptor
grammar contains a syllabification and collocational
structure that is irrelevant for word segmentation).
Given a set of sample parse trees for a sentence we
compute the set of corresponding word segmenta-
tions, and return the one that occurs most frequently
(this is a sampling approximation to the maximum
probability marginal structure).
For each setting in the experiments described in
Table 1 we ran 8 samplers for 2,000 iterations (i.e.,
passes through the training data), and kept the sam-
ple parse trees from every 10th iteration after itera-
tion 1000, resulting in 800 sample parses for every
sentence. (An examination of the posterior proba-
bilities suggests that all of the samplers using batch
initialization and table label resampling had “burnt
</bodyText>
<page confidence="0.991878">
322
</page>
<figure confidence="0.994148333333333">
220000
215000
210000
205000
200000
195000
190000
185000
0 500 1000 1500 2000
</figure>
<figureCaption confidence="0.999787">
Figure 1: Negative log posterior probability (lower is bet-
</figureCaption>
<bodyText confidence="0.95643725">
ter) as a function of iteration for 24 runs of the collo-
cation adaptor grammar samplers with Pitman-Yor adap-
tors. The upper 8 runs use batch initialization but no ta-
ble label resampling, the middle 8 runs use incremental
initialization and table label resampling, while the lower
8 runs use batch initialization and table label resampling.
in” by iteration 1000). We evaluated the word to-
ken f-score of the most frequent marginal word seg-
mentation, and compared that to average of the word
token f-score for the 800 samples, which is also re-
ported in Table 1. For each grammar and setting we
tried, the maximum marginal segmentation was bet-
ter than the sample average, sometimes by a large
margin. Given its simplicity, this suggests that max-
imum marginal decoding is probably worth trying
when applicable.
</bodyText>
<subsectionHeader confidence="0.997293">
4.2 Batch initialization
</subsectionHeader>
<bodyText confidence="0.99982675">
The Gibbs sampling algorithm is initialized with a
set of sample parses t for each sentence in the train-
ing data. While the fundamental theorem of Markov
Chain Monte Carlo guarantees that eventually sam-
ples will converge to the posterior distribution, it
says nothing about how long the “burn in” phase
might last (Robert and Casella, 2004). In practice
initialization can make a huge difference to the per-
formance of Gibbs samplers (just as it can with other
unsupervised estimation procedures such as Expec-
tation Maximization).
There are many different ways in which we could
generate the initial trees t; we only study two of the
obvious methods here. Batch initialization assigns
every sentence a random parse tree in parallel. In
more detail, the initial parse tree ti for sentence wi
is sampled from P(t I wi, G′), where G′ is the PCFG
obtained from the adaptor grammar by ignoring its
last two components A and C (i.e., the adapted non-
terminals and their adaptors), and seated at a new
table. This means that in batch initialization each
initial parse tree is randomly generated without any
adaptation at all.
Incremental initialization assigns the initial parse
trees ti to sentences wi in order, updating the adaptor
grammar as it goes. That is, ti is sampled from P(t
wi,t1, ... , ti−1). This is easy to do in the context
of Gibbs sampling, since this distribution is a minor
variant of the distribution P(ti I t−i, wi) used during
Gibbs sampling itself.
Incremental initialization is greedier than batch
initialization, and produces initial sample trees with
much higher probability. As Table 1 shows, across
all grammars and conditions after 2,000 iterations
incremental initialization produces samples with
much better word segmentation token f-score than
does batch initialization, with the largest improve-
ment on the unigram adaptor grammar.
However, incremental initialization results in
sample parses with lower posterior probability for
the unigram and collocation adaptor grammars (but
not for the collocation-syllable adaptor grammar).
Figure 1 plots the posterior probabilities of the sam-
ple trees t at each iteration for the collocation adap-
tor grammar, showing that even after 2,000 itera-
tions incremental initialization results in trees that
are much less likely than those produced by batch
initialization. It seems that with incremental initial-
ization the Gibbs sampler gets stuck in a local op-
timum which it is extremely unlikely to move away
from.
It is interesting that incremental initialization re-
sults in more accurate word segmentation, even
though the trees it produces have lower posterior
probability. This seems to be because the most prob-
able analyses produced by the unigram and, to a
lesser extent, the collocation adaptor grammars tend
to undersegment. Incremental initialization greed-
ily searches for common substrings, and because
such substrings are more likely to be short rather
than long, it tends to produce analyses with shorter
words than batch initialization does. Goldwater et
al. (2006a) show that Brent’s incremental segmenta-
tion algorithm (Brent, 1999) has a similar property.
We favor batch initialization because we are in-
batch initialization, no table label resampling
incremental initialization, table label resampling
batch initialization, table label resampling
</bodyText>
<page confidence="0.997513">
323
</page>
<bodyText confidence="0.999980666666667">
terested in understanding the properties of our mod-
els (expressed here as adaptor grammars), and batch
initialization does a better job of finding the most
probable analyses under these models. However, it
might be possible to justify incremental initializa-
tion as (say) cognitively more plausible.
</bodyText>
<subsectionHeader confidence="0.989758">
4.3 Table label resampling
</subsectionHeader>
<bodyText confidence="0.999972649122807">
Unlike the previous two implementation choices
which apply to a broad range of algorithms, table
label resampling is a specialized kind of Gibbs step
for adaptor grammars and similar hierarchical mod-
els that is designed to improve mobility. The adap-
tor grammar algorithm described in Johnson et al.
(2007b) repeatedly resamples parses for the sen-
tences of the training data. However, the adaptor
grammar sampler itself maintains of a hierarchy of
Chinese Restaurant Processes or Pitman-Yor Pro-
cesses, one per adapted nonterminal X E A, that
cache subtrees from TX. In general each of these
subtrees will occur many times in the parses for the
training data sentences. Table label resampling re-
samples the trees in these adaptors (i.e., the table
labels, to use the restaurant metaphor), potentially
changing the analysis of many sentences at once.
For example, each Collocation in the collocation
adaptor grammar can occur in many Sentences, and
each Word can occur in many Collocations. Resam-
pling a single Collocation can change the way it is
analysed into Words, thus changing the analysis of
all of the Sentences containing that Collocation.
Table label resampling is an additional resam-
pling step performed after each Gibbs sweep
through the training data in which we resample the
parse trees labeling the tables in the adaptor for each
X E A. Specifically, if the adaptor CX for X E A
currently contains m tables labeled with the trees
t = (ti, ... , tm) then table label resampling re-
places each tj, j E 1, ... , m in turn with a tree sam-
pled from P(t  |t−j,wj), where wj is the terminal
yield of tj. (Within each adaptor we actually resam-
ple all of the trees t in a randomly chosen order).
Table label resampling is a kind of Gibbs sweep,
but at a higher level in the Bayesian hierarchy than
the standard Gibbs sweep. It’s easy to show that ta-
ble label resampling preserves detailed balance for
the adaptor grammars presented in this paper, so in-
terposing table label resampling steps with the stan-
dard Gibbs steps also preserves detailed balance.
We expect table label resampling to have the
greatest impact on models with a rich hierarchi-
cal structure, and the experimental results in Ta-
ble 1 confirm this. The unigram adaptor grammar
does not involve nested adapted nonterminals, so
we would not expect table label resampling to have
any effect on its analyses. On the other hand, the
collocation-syllable adaptor grammar involves a rich
hierarchical structure, and in fact without table la-
bel resampling our sampler did not burn in or mix
within 2,000 iterations. As Figure 1 shows, table
label resampling produces parses with higher pos-
terior probability, and Table 1 shows that table la-
bel resampling makes a significant difference in the
word segmentation f-score of the collocation and
collocation-syllable adaptor grammars.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999881">
This paper has examined adaptor grammar infer-
ence procedures and their effect on the word seg-
mentation problem. Some of the techniques inves-
tigated here, such as batch versus incremental ini-
tialization, are quite general and may be applica-
ble to a wide range of other algorithms, but some
of the other techniques, such as table label resam-
pling, are specialized to nonparametric hierarchi-
cal Bayesian inference. We’ve shown that sampling
adaptor hyperparameters is feasible, and demon-
strated that this improves word segmentation accu-
racy of the collocation-syllable adaptor grammar by
almost 10%, corresponding to an error reduction of
over 35% compared to the best results presented in
Johnson (2008). We also described and investigated
table label resampling, which dramatically improves
the effectiveness of Gibbs sampling estimators for
complex adaptor grammars, and makes it possible
to work with adaptor grammars with complex hier-
archical structure.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9971568">
We thank Erik Sudderth for suggesting sampling the
Pitman-Yor hyperparameters and the ACL review-
ers for their insightful comments. This research was
funded by NSF awards 0544127 and 0631667 to
Mark Johnson.
</bodyText>
<page confidence="0.998479">
324
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99997683">
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children’s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Daniel Blanchard and Jeffrey Heinz. 2008. Improv-
ing word segmentation by simultaneously learning
phonotactics. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 65–72, Manchester, England,
August.
David Blei, Thomas L. Griffiths, Michael I. Jordan, and
Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process.
In Sebastian Thrun, Lawrence Saul, and Bernhard
Sch¨olkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93–125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71–105.
Jeffrey Elman. 1990. Finding structure in time. Cogni-
tive Science, 14:197–211.
Margaret M. Fleck. 2008. Lexicalized phonotactic
word segmentation. In Proceedings ofACL-08: HLT,
pages 130–138, Columbus, Ohio, June. Association
for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673–680, Sydney, Aus-
tralia. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459–466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239–250, Somerville,
MA. Cascadilla Press.
H. Ishwaran and L. F. James. 2003. Generalized
weighted Chinese restaurant processes for species
sampling mixture models. Statistica Sinica, 13:1211–
1235.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139–146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework
for specifying compositional nonparametric Bayesian
models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, ed-
itors, Advances in Neural Information Processing Sys-
tems 19, pages 641–648. MIT Press, Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Compu-
tational Linguistics, pages 483–501. Oxford Univer-
sity Press, Oxford, England.
David J.C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705–767.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855–900.
J. Pitman. 1995. Exchangeable and partially exchange-
able random partitions. Probability Theory and Re-
lated Fields, 102:145–158.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Andreas Stolcke and Stephen Omohundro. 1994. Induc-
ing probabilistic grammars by Bayesian model merg-
ing. In Rafael C. Carrasco and Jose Oncina, editors,
Grammatical Inference and Applications, pages 106–
118. Springer, New York.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566–1581.
</reference>
<page confidence="0.999132">
325
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728084">
<title confidence="0.992653">Improving nonparameteric Bayesian inference: experiments unsupervised word segmentation with adaptor grammars</title>
<author confidence="0.999989">Mark Johnson Sharon Goldwater</author>
<affiliation confidence="0.999936">Brown University University of Edinburgh</affiliation>
<address confidence="0.954705">Providence, RI Edinburgh EH8 9AB</address>
<author confidence="0.801883">Mark JohnsonBrown edu sgwaterinf ed ac uk</author>
<abstract confidence="0.9982214">One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein- Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parentchild speech.</title>
<date>1987</date>
<booktitle>Children’s Language,</booktitle>
<volume>6</volume>
<editor>In K. Nelson and A. van Kleeck, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="5272" citStr="Bernstein-Ratner, 1987" startWordPosition="806" endWordPosition="807">tion that serve as running examples in this paper. Adaptor grammars contain a large number of adjustable parameters, and Section 3 discusses how these can be estimated using Bayesian techniques. Section 4 examines several implementation options within the adaptor grammar inference algorithm and shows that they can make a significant impact on performance. Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus. 2 Adaptor grammars This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars. Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learners must solve. Elman (1990) introduced an idealized version of this task, and Brent and Cartwright (1996) pr</context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>N. Bernstein-Ratner. 1987. The phonology of parentchild speech. In K. Nelson and A. van Kleeck, editors, Children’s Language, volume 6. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Blanchard</author>
<author>Jeffrey Heinz</author>
</authors>
<title>Improving word segmentation by simultaneously learning phonotactics.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>65--72</pages>
<location>Manchester, England,</location>
<contexts>
<context position="14206" citStr="Blanchard and Heinz, 2008" startWordPosition="2315" endWordPosition="2318">ons, each of which expands to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as sequences of syllables can learn and exploit these constraints, significantly improving segmentation accuracy. Here we present an adaptor grammar that models collocations together with these phonotactic constraints. This grammar is quite complex, permitting us to study the effects of the various model and implementation choices described below on a complex hierarchical nonparametric Bayesian model. The collocation-syllable adaptor grammar generates a Sentence in terms of three levels of Collocations (enabling i</context>
</contexts>
<marker>Blanchard, Heinz, 2008</marker>
<rawString>Daniel Blanchard and Jeffrey Heinz. 2008. Improving word segmentation by simultaneously learning phonotactics. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 65–72, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems 16.</booktitle>
<editor>In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22085" citStr="Blei et al., 2004" startWordPosition="3640" endWordPosition="3643"> adaptors were used when ax was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where θ was not integrated out it was set uniformly, and all αX = bx were set to 100 they were not sampled. 4 Inference for adaptor grammars Johnson et al. (2007b) describe the basic adaptor grammar inference procedure that we use here. That paper leaves unspecified a number of implementation details, which we show can make a crucial difference to segmentation accuracy. The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well. The inference algorithm maintains a vector t = (ti, ... , tr,,) of sample parses, where tz E TS is a parse for the ith sentence wz. It repeatedly chooses a sentence wz at random and resamples the parse tree tz for wz from P(tz |t−z, wz), i.e., conditioned on wz and the parses t−z of all sentences except wz. 4.1 Maximum marginal decoding Sampling algorithms like ours produce a stream of samples from the posterior </context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>David Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond grammar: an experience-based theory of language.</title>
<date>1998</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, California.</location>
<marker>Bod, 1998</marker>
<rawString>Rens Bod. 1998. Beyond grammar: an experience-based theory of language. CSLI Publications, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
<author>T Cartwright</author>
</authors>
<title>Distributional regularity and phonotactic constraints are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--93</pages>
<contexts>
<context position="5301" citStr="Brent and Cartwright, 1996" startWordPosition="808" endWordPosition="811">ng examples in this paper. Adaptor grammars contain a large number of adjustable parameters, and Section 3 discusses how these can be estimated using Bayesian techniques. Section 4 examines several implementation options within the adaptor grammar inference algorithm and shows that they can make a significant impact on performance. Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus. 2 Adaptor grammars This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars. Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learners must solve. Elman (1990) introduced an idealized version of this task, and Brent and Cartwright (1996) presented a version of it where</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>M. Brent and T. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition, 61:93–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="28063" citStr="Brent, 1999" startWordPosition="4611" endWordPosition="4612">hat incremental initialization results in more accurate word segmentation, even though the trees it produces have lower posterior probability. This seems to be because the most probable analyses produced by the unigram and, to a lesser extent, the collocation adaptor grammars tend to undersegment. Incremental initialization greedily searches for common substrings, and because such substrings are more likely to be short rather than long, it tends to produce analyses with shorter words than batch initialization does. Goldwater et al. (2006a) show that Brent’s incremental segmentation algorithm (Brent, 1999) has a similar property. We favor batch initialization because we are inbatch initialization, no table label resampling incremental initialization, table label resampling batch initialization, table label resampling 323 terested in understanding the properties of our models (expressed here as adaptor grammars), and batch initialization does a better job of finding the most probable analyses under these models. However, it might be possible to justify incremental initialization as (say) cognitively more plausible. 4.3 Table label resampling Unlike the previous two implementation choices which a</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--197</pages>
<contexts>
<context position="5791" citStr="Elman (1990)" startWordPosition="886" endWordPosition="887"> f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus. 2 Adaptor grammars This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars. Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learners must solve. Elman (1990) introduced an idealized version of this task, and Brent and Cartwright (1996) presented a version of it where the data consists of unsegmented phonemic representations of the sentences in the Bernstein-Ratner corpus of 1The adaptor grammar inference program is available for download at http://www.cog.brown.edu/˜mj/Software.htm. child-directed speech (Bernstein-Ratner, 1987). Because these phonemic representations are obtained by looking up orthographic forms in a pronouncing dictionary and appending the results, identifying the word tokens is equivalent to finding the locations of the word bo</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey Elman. 1990. Finding structure in time. Cognitive Science, 14:197–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret M Fleck</author>
</authors>
<title>Lexicalized phonotactic word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>130--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="14220" citStr="Fleck, 2008" startWordPosition="2319" endWordPosition="2320">to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as sequences of syllables can learn and exploit these constraints, significantly improving segmentation accuracy. Here we present an adaptor grammar that models collocations together with these phonotactic constraints. This grammar is quite complex, permitting us to study the effects of the various model and implementation choices described below on a complex hierarchical nonparametric Bayesian model. The collocation-syllable adaptor grammar generates a Sentence in terms of three levels of Collocations (enabling it to capture a</context>
</contexts>
<marker>Fleck, 2008</marker>
<rawString>Margaret M. Fleck. 2008. Lexicalized phonotactic word segmentation. In Proceedings ofACL-08: HLT, pages 130–138, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="3242" citStr="Goldwater et al., 2006" startWordPosition="487" endWordPosition="490">ceived considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units </context>
<context position="12882" citStr="Goldwater et al. (2006" startWordPosition="2116" endWordPosition="2120">cribed in section 4 only instantiate those Words actually used in the analysis of Sentences in the corpus. While the Word adaptor will generally 319 prefer to reuse Words that have been used elsewhere in the corpus, it is always possible to generate a fresh Word using the CFG rules expanding Word into a string of Phonemes. We assume for now that all CFG rules RX expanding the nonterminal X ∈ N have the same probability (although we will explore estimating θ below), so the base distribution HWord is a “monkeys banging on typewriters” model. That means the unigram adaptor grammar implements the Goldwater et al. (2006a) unigram word segmentation model, and in fact it produces segmentations of similar accuracies, and exhibits the same characteristic undersegmentation errors. As Goldwater et al. point out, because Words are the only units of generalization available to a unigram model it tends to misanalyse collocations as words, resulting in a marked tendancy to undersegment. Goldwater et al. demonstrate that modelling bigram dependencies mitigates this undersegmentation. While adaptor grammars cannot express the Goldwater et al. bigram model, they can get much the same effect by directly modelling collocat</context>
<context position="22109" citStr="Goldwater et al., 2006" startWordPosition="3644" endWordPosition="3647"> when ax was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where θ was not integrated out it was set uniformly, and all αX = bx were set to 100 they were not sampled. 4 Inference for adaptor grammars Johnson et al. (2007b) describe the basic adaptor grammar inference procedure that we use here. That paper leaves unspecified a number of implementation details, which we show can make a crucial difference to segmentation accuracy. The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well. The inference algorithm maintains a vector t = (ti, ... , tr,,) of sample parses, where tz E TS is a parse for the ith sentence wz. It repeatedly chooses a sentence wz at random and resamples the parse tree tz for wz from P(tz |t−z, wz), i.e., conditioned on wz and the parses t−z of all sentences except wz. 4.1 Maximum marginal decoding Sampling algorithms like ours produce a stream of samples from the posterior distribution over parses</context>
<context position="27994" citStr="Goldwater et al. (2006" startWordPosition="4600" endWordPosition="4603">l optimum which it is extremely unlikely to move away from. It is interesting that incremental initialization results in more accurate word segmentation, even though the trees it produces have lower posterior probability. This seems to be because the most probable analyses produced by the unigram and, to a lesser extent, the collocation adaptor grammars tend to undersegment. Incremental initialization greedily searches for common substrings, and because such substrings are more likely to be short rather than long, it tends to produce analyses with shorter words than batch initialization does. Goldwater et al. (2006a) show that Brent’s incremental segmentation algorithm (Brent, 1999) has a similar property. We favor batch initialization because we are inbatch initialization, no table label resampling incremental initialization, table label resampling batch initialization, table label resampling 323 terested in understanding the properties of our models (expressed here as adaptor grammars), and batch initialization does a better job of finding the most probable analyses under these models. However, it might be possible to justify incremental initialization as (say) cognitively more plausible. 4.3 Table la</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006a. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3242" citStr="Goldwater et al., 2006" startWordPosition="487" endWordPosition="490">ceived considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units </context>
<context position="12882" citStr="Goldwater et al. (2006" startWordPosition="2116" endWordPosition="2120">cribed in section 4 only instantiate those Words actually used in the analysis of Sentences in the corpus. While the Word adaptor will generally 319 prefer to reuse Words that have been used elsewhere in the corpus, it is always possible to generate a fresh Word using the CFG rules expanding Word into a string of Phonemes. We assume for now that all CFG rules RX expanding the nonterminal X ∈ N have the same probability (although we will explore estimating θ below), so the base distribution HWord is a “monkeys banging on typewriters” model. That means the unigram adaptor grammar implements the Goldwater et al. (2006a) unigram word segmentation model, and in fact it produces segmentations of similar accuracies, and exhibits the same characteristic undersegmentation errors. As Goldwater et al. point out, because Words are the only units of generalization available to a unigram model it tends to misanalyse collocations as words, resulting in a marked tendancy to undersegment. Goldwater et al. demonstrate that modelling bigram dependencies mitigates this undersegmentation. While adaptor grammars cannot express the Goldwater et al. bigram model, they can get much the same effect by directly modelling collocat</context>
<context position="22109" citStr="Goldwater et al., 2006" startWordPosition="3644" endWordPosition="3647"> when ax was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where θ was not integrated out it was set uniformly, and all αX = bx were set to 100 they were not sampled. 4 Inference for adaptor grammars Johnson et al. (2007b) describe the basic adaptor grammar inference procedure that we use here. That paper leaves unspecified a number of implementation details, which we show can make a crucial difference to segmentation accuracy. The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well. The inference algorithm maintains a vector t = (ti, ... , tr,,) of sample parses, where tz E TS is a parse for the ith sentence wz. It repeatedly chooses a sentence wz at random and resamples the parse tree tz for wz from P(tz |t−z, wz), i.e., conditioned on wz and the parses t−z of all sentences except wz. 4.1 Maximum marginal decoding Sampling algorithms like ours produce a stream of samples from the posterior distribution over parses</context>
<context position="27994" citStr="Goldwater et al. (2006" startWordPosition="4600" endWordPosition="4603">l optimum which it is extremely unlikely to move away from. It is interesting that incremental initialization results in more accurate word segmentation, even though the trees it produces have lower posterior probability. This seems to be because the most probable analyses produced by the unigram and, to a lesser extent, the collocation adaptor grammars tend to undersegment. Incremental initialization greedily searches for common substrings, and because such substrings are more likely to be short rather than long, it tends to produce analyses with shorter words than batch initialization does. Goldwater et al. (2006a) show that Brent’s incremental segmentation algorithm (Brent, 1999) has a similar property. We favor batch initialization because we are inbatch initialization, no table label resampling incremental initialization, table label resampling batch initialization, table label resampling 323 terested in understanding the properties of our models (expressed here as adaptor grammars), and batch initialization does a better job of finding the most probable analyses under these models. However, it might be possible to justify incremental initialization as (say) cognitively more plausible. 4.3 Table la</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006b. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Distributional cues to word boundaries: Context is important.</title>
<date>2007</date>
<booktitle>Proceedings of the 31st Annual Boston University Conference on Language Development,</booktitle>
<pages>239--250</pages>
<editor>In David Bamman, Tatiana Magnitskaia, and Colleen Zaller, editors,</editor>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA.</location>
<contexts>
<context position="3267" citStr="Goldwater et al., 2007" startWordPosition="491" endWordPosition="494">tion recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units such as words, syllables </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2007</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2007. Distributional cues to word boundaries: Context is important. In David Bamman, Tatiana Magnitskaia, and Colleen Zaller, editors, Proceedings of the 31st Annual Boston University Conference on Language Development, pages 239–250, Somerville, MA. Cascadilla Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ishwaran</author>
<author>L F James</author>
</authors>
<title>Generalized weighted Chinese restaurant processes for species sampling mixture models.</title>
<date>2003</date>
<journal>Statistica Sinica,</journal>
<volume>13</volume>
<pages>1235</pages>
<contexts>
<context position="10155" citStr="Ishwaran and James, 2003" startWordPosition="1604" endWordPosition="1607">s) into another stream of trees whose support is also TX. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of natural numbers z1, z2,. . ., where z1 = 1 and each zn+1 &lt; m + 1 where m = max(z1, ... , zn). In the “Chinese Restaurant” metaphor samples produced by the adaptor are viewed as “customers” and zn is the index of the “table” that the nth customer is seated at. In adaptor grammars each table in the adaptor CX is labeled with a tree sampled from the base distribution HX that is shared by all customers at that table; thus the nth sample tree from the adaptor CX is the znth sample from HX. CRPs and PYPs differ in exactly how the se</context>
</contexts>
<marker>Ishwaran, James, 2003</marker>
<rawString>H. Ishwaran and L. F. James. 2003. Generalized weighted Chinese restaurant processes for species sampling mixture models. Statistica Sinica, 13:1211– 1235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="5569" citStr="Johnson et al. (2007" startWordPosition="851" endWordPosition="854">ws that they can make a significant impact on performance. Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus. 2 Adaptor grammars This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars. Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learners must solve. Elman (1990) introduced an idealized version of this task, and Brent and Cartwright (1996) presented a version of it where the data consists of unsegmented phonemic representations of the sentences in the Bernstein-Ratner corpus of 1The adaptor grammar inference program is available for download at http://www.cog.brown.edu/˜mj/Software.htm. child-directed speech (Bernstein-Ratner, 1987).</context>
<context position="9752" citStr="Johnson et al. (2007" startWordPosition="1539" endWordPosition="1543">lities 0, where θr is the probability of rule r E R, A C_ N is the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A, so CX is the adaptor for adapted nonterminal X E A. Informally, an adaptor CX nondeterministically maps a stream of trees from a base distribution HX whose support is TX (the set of subtrees whose root node is X E N generated by the grammar’s rules) into another stream of trees whose support is also TX. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of natural numbers z1, z2,. . ., where z1 = 1 and each zn+1 &lt; m + 1 where m = max(z1, ... , zn). In the “Chinese Restaurant” metapho</context>
<context position="17555" citStr="Johnson et al. (2007" startWordPosition="2850" endWordPosition="2853">ng with the large number of parameters in these models that are both more principled and more practical than the approaches described above. First, we can integrate out θ, and second, we can infer values for the adaptor hyperparameters using sampling. These methods (the latter in particular) make it practical to use Pitman-Yor Process adaptors in complex grammars such as the collocation-syllable adaptor grammar, where it is impractical to try to find optimal parameter values by grid search. As we will show, they also improve segmentation accuracy, sometimes dramatically. 3.1 Integrating out θ Johnson et al. (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities θ, and these techniques can be used directly with adaptor grammars as well. Just as in that paper, we place Dirichlet priors on θ: here θX is the subvector of θ corresponding to rules expanding nonterminal X E N, and βX is a corresponding vector of positive real numbers specifying the hyperparameters of the corresponding Dirichlet distributions: P(θ |β) = � Dir(θX |βX) X∈N Because the Dirichlet distribution is conjugate to the multinomial distribution, it is possible to integrate out the rule probabilities θ, producin</context>
<context position="21735" citStr="Johnson et al. (2007" startWordPosition="3585" endWordPosition="3588">.71 0.73 0.87 0.56 0.74 0.84 0.57 0.75 0.78 0.56 0.69 0.76 0.56 0.74 0.88 0.57 0.51 0.55 0.81 0.86 0.89 0.80 0.56 0.82 0.77 0.49 0.82 0.77 0.75 0.88 Table 1: Word segmentation accuracy measured by word token f-scores on Brent’s version of the Bernstein-Ratner corpus as a function of adaptor grammar, adaptor and estimation procedure. Pitman-Yor Process adaptors were used when ax was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where θ was not integrated out it was set uniformly, and all αX = bx were set to 100 they were not sampled. 4 Inference for adaptor grammars Johnson et al. (2007b) describe the basic adaptor grammar inference procedure that we use here. That paper leaves unspecified a number of implementation details, which we show can make a crucial difference to segmentation accuracy. The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well. The inference algorithm maintains a vector t = (ti, ... , tr,,) of</context>
<context position="28913" citStr="Johnson et al. (2007" startWordPosition="4736" endWordPosition="4739">n understanding the properties of our models (expressed here as adaptor grammars), and batch initialization does a better job of finding the most probable analyses under these models. However, it might be possible to justify incremental initialization as (say) cognitively more plausible. 4.3 Table label resampling Unlike the previous two implementation choices which apply to a broad range of algorithms, table label resampling is a specialized kind of Gibbs step for adaptor grammars and similar hierarchical models that is designed to improve mobility. The adaptor grammar algorithm described in Johnson et al. (2007b) repeatedly resamples parses for the sentences of the training data. However, the adaptor grammar sampler itself maintains of a hierarchy of Chinese Restaurant Processes or Pitman-Yor Processes, one per adapted nonterminal X E A, that cache subtrees from TX. In general each of these subtrees will occur many times in the parses for the training data sentences. Table label resampling resamples the trees in these adaptors (i.e., the table labels, to use the restaurant metaphor), potentially changing the analysis of many sentences at once. For example, each Collocation in the collocation adaptor</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007a. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5569" citStr="Johnson et al. (2007" startWordPosition="851" endWordPosition="854">ws that they can make a significant impact on performance. Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus. 2 Adaptor grammars This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al. (2007b) for a formal definition of adaptor grammars. Consider the problem of learning language from continuous speech: segmenting each utterance into words is a nontrivial problem that language learners must solve. Elman (1990) introduced an idealized version of this task, and Brent and Cartwright (1996) presented a version of it where the data consists of unsegmented phonemic representations of the sentences in the Bernstein-Ratner corpus of 1The adaptor grammar inference program is available for download at http://www.cog.brown.edu/˜mj/Software.htm. child-directed speech (Bernstein-Ratner, 1987).</context>
<context position="9752" citStr="Johnson et al. (2007" startWordPosition="1539" endWordPosition="1543">lities 0, where θr is the probability of rule r E R, A C_ N is the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A, so CX is the adaptor for adapted nonterminal X E A. Informally, an adaptor CX nondeterministically maps a stream of trees from a base distribution HX whose support is TX (the set of subtrees whose root node is X E N generated by the grammar’s rules) into another stream of trees whose support is also TX. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of natural numbers z1, z2,. . ., where z1 = 1 and each zn+1 &lt; m + 1 where m = max(z1, ... , zn). In the “Chinese Restaurant” metapho</context>
<context position="17555" citStr="Johnson et al. (2007" startWordPosition="2850" endWordPosition="2853">ng with the large number of parameters in these models that are both more principled and more practical than the approaches described above. First, we can integrate out θ, and second, we can infer values for the adaptor hyperparameters using sampling. These methods (the latter in particular) make it practical to use Pitman-Yor Process adaptors in complex grammars such as the collocation-syllable adaptor grammar, where it is impractical to try to find optimal parameter values by grid search. As we will show, they also improve segmentation accuracy, sometimes dramatically. 3.1 Integrating out θ Johnson et al. (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities θ, and these techniques can be used directly with adaptor grammars as well. Just as in that paper, we place Dirichlet priors on θ: here θX is the subvector of θ corresponding to rules expanding nonterminal X E N, and βX is a corresponding vector of positive real numbers specifying the hyperparameters of the corresponding Dirichlet distributions: P(θ |β) = � Dir(θX |βX) X∈N Because the Dirichlet distribution is conjugate to the multinomial distribution, it is possible to integrate out the rule probabilities θ, producin</context>
<context position="21735" citStr="Johnson et al. (2007" startWordPosition="3585" endWordPosition="3588">.71 0.73 0.87 0.56 0.74 0.84 0.57 0.75 0.78 0.56 0.69 0.76 0.56 0.74 0.88 0.57 0.51 0.55 0.81 0.86 0.89 0.80 0.56 0.82 0.77 0.49 0.82 0.77 0.75 0.88 Table 1: Word segmentation accuracy measured by word token f-scores on Brent’s version of the Bernstein-Ratner corpus as a function of adaptor grammar, adaptor and estimation procedure. Pitman-Yor Process adaptors were used when ax was sampled, otherwise Chinese Restaurant Process adaptors were used. In runs where θ was not integrated out it was set uniformly, and all αX = bx were set to 100 they were not sampled. 4 Inference for adaptor grammars Johnson et al. (2007b) describe the basic adaptor grammar inference procedure that we use here. That paper leaves unspecified a number of implementation details, which we show can make a crucial difference to segmentation accuracy. The adaptor grammar algorithm is basically a Gibbs sampler of the kind widely used for nonparametric Bayesian inference (Blei et al., 2004; Goldwater et al., 2006b; Goldwater et al., 2006a), so it seems reasonable to expect that at least some of the details discussed below will be relevant to other applications as well. The inference algorithm maintains a vector t = (ti, ... , tr,,) of</context>
<context position="28913" citStr="Johnson et al. (2007" startWordPosition="4736" endWordPosition="4739">n understanding the properties of our models (expressed here as adaptor grammars), and batch initialization does a better job of finding the most probable analyses under these models. However, it might be possible to justify incremental initialization as (say) cognitively more plausible. 4.3 Table label resampling Unlike the previous two implementation choices which apply to a broad range of algorithms, table label resampling is a specialized kind of Gibbs step for adaptor grammars and similar hierarchical models that is designed to improve mobility. The adaptor grammar algorithm described in Johnson et al. (2007b) repeatedly resamples parses for the sentences of the training data. However, the adaptor grammar sampler itself maintains of a hierarchy of Chinese Restaurant Processes or Pitman-Yor Processes, one per adapted nonterminal X E A, that cache subtrees from TX. In general each of these subtrees will occur many times in the parses for the training data sentences. Table label resampling resamples the trees in these adaptors (i.e., the table labels, to use the restaurant metaphor), potentially changing the analysis of many sentences at once. For example, each Collocation in the collocation adaptor</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007b. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3283" citStr="Johnson, 2008" startWordPosition="495" endWordPosition="496">ecause they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units such as words, syllables and collocations</context>
<context position="13502" citStr="Johnson, 2008" startWordPosition="2213" endWordPosition="2214">gram word segmentation model, and in fact it produces segmentations of similar accuracies, and exhibits the same characteristic undersegmentation errors. As Goldwater et al. point out, because Words are the only units of generalization available to a unigram model it tends to misanalyse collocations as words, resulting in a marked tendancy to undersegment. Goldwater et al. demonstrate that modelling bigram dependencies mitigates this undersegmentation. While adaptor grammars cannot express the Goldwater et al. bigram model, they can get much the same effect by directly modelling collocations (Johnson, 2008). A collocation adaptor grammar generates a Sentence as a sequence of Collocations, each of which expands to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the </context>
<context position="16697" citStr="Johnson (2008)" startWordPosition="2711" endWordPosition="2712">tion 2 have a large number of free parameters that have to be chosen by the grammar designer; a rule probability θr for each PCFG rule r ∈ R and either one or two hyperparameters for each adapted nonterminal X ∈ A, depending on whether Chinese Restaurant 320 or Pitman-Yor Processes are used as adaptors. It’s difficult to have intuitions about the appropriate settings for the latter parameters, and finding the optimal values for these parameters by some kind of exhaustive search is usually computationally impractical. Previous work has adopted an expedient such as parameter tying. For example, Johnson (2008) set θ by requiring all productions expanding the same nonterminal to have the same probability, and used Chinese Restaurant Process adaptors with tied parameters αX, which was set using a grid search. We now describe two methods of dealing with the large number of parameters in these models that are both more principled and more practical than the approaches described above. First, we can integrate out θ, and second, we can infer values for the adaptor hyperparameters using sampling. These methods (the latter in particular) make it practical to use Pitman-Yor Process adaptors in complex gramm</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Tree adjoining grammars.</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics,</booktitle>
<pages>483--501</pages>
<editor>In Ruslan Mikkov, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford, England.</location>
<contexts>
<context position="7822" citStr="Joshi, 2003" startWordPosition="1204" endWordPosition="1205">umes that the probability of a word is determined purely by multiplying together the probability of its individual phonemes, it has no way to encode the fact that certain strings of phonemes (the words of the language) have much higher probabilities than other strings containing the same phonemes. In order to do this, a PCFG would need productions like the following one, which encodes the fact that “want” is a Word. Word → w a n t Adaptor grammars can be viewed as a way of formalizing this idea. Adaptor grammars learn the probabilities of entire subtrees, much as in tree substitution grammar (Joshi, 2003) and DOP (Bod, 318 1998). (For computational efficiency reasons adaptor grammars require these subtrees to expand to terminals). The set of possible adapted tree fragments is the set of all subtrees generated by the CFG whose root label is a member of the set of adapted nonterminals A (adapted nonterminals are indicated by underlining in this paper). For example, in the unigram adaptor grammar A = {Word}, which means that the adaptor grammar inference procedure learns the probability of each possible Word subtree. Thus adaptor grammars are simple models of structure learning in which adapted s</context>
</contexts>
<marker>Joshi, 2003</marker>
<rawString>Aravind Joshi. 2003. Tree adjoining grammars. In Ruslan Mikkov, editor, The Oxford Handbook of Computational Linguistics, pages 483–501. Oxford University Press, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Information Theory, Inference, and Learning Algorithms.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="19341" citStr="MacKay, 2003" startWordPosition="3156" endWordPosition="3157">ss important a role. 3.2 Slice sampling adaptor hyperparameters As far as we know, there are no conjugate priors for the adaptor hyperparameters aX or bX (which corresponds to αX in a Chinese Restaurant Process), so it is not possible to integrate them out as we did with the rule probabilities θ. However, it is possible to perform Bayesian inference by putting a prior on them and sampling their values. Because we have no strong intuitions about the values of these parameters we chose uninformative priors. We chose a uniform Beta(1,1) prior on aX, and a “vague” Gamma(10, 0.1) prior on bX = αX (MacKay, 2003). (We experimented with other parameters in the Gamma prior, but found no significant difference in performance). After each Gibbs sweep through the parse trees t we resampled each of the adaptor parameters from the posterior distribution of the parameter using a slice sampler 10 times. For example, we resample each bX from: P(bX |t) a P(t |bX) Gamma(bX |10, 0.1) Here P(t |bX) is the likelihood of the current sequence of sample parse trees (we only need the factors that depend on bX) and Gamma(bX |10, 0.1) is the prior. The same formula is used for sampling aX, except that the prior is now a f</context>
</contexts>
<marker>MacKay, 2003</marker>
<rawString>David J.C. MacKay. 2003. Information Theory, Inference, and Learning Algorithms. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="20206" citStr="Neal, 2003" startWordPosition="3310" endWordPosition="3311">r using a slice sampler 10 times. For example, we resample each bX from: P(bX |t) a P(t |bX) Gamma(bX |10, 0.1) Here P(t |bX) is the likelihood of the current sequence of sample parse trees (we only need the factors that depend on bX) and Gamma(bX |10, 0.1) is the prior. The same formula is used for sampling aX, except that the prior is now a flat Beta(1,1) distribution. In general we cannot even compute the normalizing constants for these posterior distributions, so we chose a sampler that does not require this. We use a slice sampler here because it does not require a proposal distribution (Neal, 2003). (We initially tried a Metropolis-Hastings sampler but were unable to find a proposal distribution that had reasonable acceptance ratios for all of our adaptor grammars). As Table 1 makes clear, sampling the adaptor parameters makes a significant difference, especially on the collocation-syllable adaptor grammar. This is not surprising, as the adaptors in that grammar play many different roles and there is no reason to to expect the optimal values of their parameters to be similar. 321 Word token f-scores Sample average Max. Marginal colloc colloc-syll unigram Condition Batch initialization T</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="10128" citStr="Pitman and Yor, 1997" startWordPosition="1600" endWordPosition="1603"> by the grammar’s rules) into another stream of trees whose support is also TX. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of natural numbers z1, z2,. . ., where z1 = 1 and each zn+1 &lt; m + 1 where m = max(z1, ... , zn). In the “Chinese Restaurant” metaphor samples produced by the adaptor are viewed as “customers” and zn is the index of the “table” that the nth customer is seated at. In adaptor grammars each table in the adaptor CX is labeled with a tree sampled from the base distribution HX that is shared by all customers at that table; thus the nth sample tree from the adaptor CX is the znth sample from HX. CRPs and PYPs d</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
</authors>
<title>Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields,</title>
<date>1995</date>
<pages>102--145</pages>
<contexts>
<context position="10106" citStr="Pitman, 1995" startWordPosition="1598" endWordPosition="1599"> E N generated by the grammar’s rules) into another stream of trees whose support is also TX. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of natural numbers z1, z2,. . ., where z1 = 1 and each zn+1 &lt; m + 1 where m = max(z1, ... , zn). In the “Chinese Restaurant” metaphor samples produced by the adaptor are viewed as “customers” and zn is the index of the “table” that the nth customer is seated at. In adaptor grammars each table in the adaptor CX is labeled with a tree sampled from the base distribution HX that is shared by all customers at that table; thus the nth sample tree from the adaptor CX is the znth sample fr</context>
</contexts>
<marker>Pitman, 1995</marker>
<rawString>J. Pitman. 1995. Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields, 102:145–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian P Robert</author>
<author>George Casella</author>
</authors>
<title>Monte Carlo Statistical Methods.</title>
<date>2004</date>
<publisher>Springer.</publisher>
<contexts>
<context position="25296" citStr="Robert and Casella, 2004" startWordPosition="4173" endWordPosition="4176"> in Table 1. For each grammar and setting we tried, the maximum marginal segmentation was better than the sample average, sometimes by a large margin. Given its simplicity, this suggests that maximum marginal decoding is probably worth trying when applicable. 4.2 Batch initialization The Gibbs sampling algorithm is initialized with a set of sample parses t for each sentence in the training data. While the fundamental theorem of Markov Chain Monte Carlo guarantees that eventually samples will converge to the posterior distribution, it says nothing about how long the “burn in” phase might last (Robert and Casella, 2004). In practice initialization can make a huge difference to the performance of Gibbs samplers (just as it can with other unsupervised estimation procedures such as Expectation Maximization). There are many different ways in which we could generate the initial trees t; we only study two of the obvious methods here. Batch initialization assigns every sentence a random parse tree in parallel. In more detail, the initial parse tree ti for sentence wi is sampled from P(t I wi, G′), where G′ is the PCFG obtained from the adaptor grammar by ignoring its last two components A and C (i.e., the adapted n</context>
</contexts>
<marker>Robert, Casella, 2004</marker>
<rawString>Christian P. Robert and George Casella. 2004. Monte Carlo Statistical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by Bayesian model merging. In</title>
<date>1994</date>
<booktitle>Grammatical Inference and Applications,</booktitle>
<pages>106--118</pages>
<editor>Rafael C. Carrasco and Jose Oncina, editors,</editor>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="1819" citStr="Stolcke and Omohundro, 1994" startWordPosition="266" endWordPosition="269">n a numerical weight (e.g., a probability) associated with each feature, where the set of features is fixed before learning begins. Such procedures can be used to learn features or structural units by embedding them in a “propose-and-prune” algorithm: a feature proposal component proposes potentially useful features (e.g., combinations of the currently most useful features), which are then fed to a parametric learner that estimates their weights. After estimating feature weights and pruning “useless” low-weight features, the cycle repeats. While such algorithms can achieve impressive results (Stolcke and Omohundro, 1994), their effectiveness depends on how well the feature proposal step relates to the overall learning objective, and it can take considerable insight and experimentation to devise good feature proposals. One of the main reasons for the recent interest in nonparametric Bayesian inference is that it offers a systematic framework for structural inference, i.e., inferring the features relevant to a particular problem as well as their weights. (Here “nonparametric” means that the models do not have a fixed set of parameters; our nonparametric models do have parameters, but the particular parameters i</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In Rafael C. Carrasco and Jose Oncina, editors, Grammatical Inference and Applications, pages 106– 118. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M Jordan</author>
<author>M Beal</author>
<author>D Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="2786" citStr="Teh et al., 2006" startWordPosition="415" endWordPosition="418">e, i.e., inferring the features relevant to a particular problem as well as their weights. (Here “nonparametric” means that the models do not have a fixed set of parameters; our nonparametric models do have parameters, but the particular parameters in a model are learned along with their values). Dirichlet Processes and their associated predictive distributions, Chinese Restaurant Processes, are one kind of nonparametric Bayesian model that has received considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hiera</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>