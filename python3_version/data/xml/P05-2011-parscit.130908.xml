<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022901">
<title confidence="0.998627">
Towards an Optimal Lexicalization in a Natural-Sounding Portable
Natural Language Generator for Dialog Systems
</title>
<author confidence="0.997485">
Inge M. R. De Bleecker
</author>
<affiliation confidence="0.9981975">
Department of Linguistics
The University of Texas at Austin
</affiliation>
<address confidence="0.512432">
Austin, TX 78712, USA
</address>
<email confidence="0.9914">
imrdb@mail.utexas.edu
</email>
<sectionHeader confidence="0.998541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999740555555556">
In contrast to the latest progress in speech
recognition, the state-of-the-art in natural
language generation for spoken language
dialog systems is lagging behind. The
core dialog managers are now more so-
phisticated; and natural-sounding and
flexible output is expected, but not
achieved with current simple techniques
such as template-based systems. Portabil-
ity of systems across subject domains and
languages is another increasingly impor-
tant requirement in dialog systems. This
paper presents an outline of LEGEND, a
system that is both portable and generates
natural-sounding output. This goal is
achieved through the novel use of existing
lexical resources such as FrameNet and
WordNet.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840615384615">
Most of the natural language generation (NLG)
components in current dialog systems are imple-
mented through the use of simple techniques such
as a library of hand-crafted and pre-recorded utter-
ances, or a template-based system where the tem-
plates contain slots in which different values can
be inserted. These techniques are unmanageable if
the dialog system aims to provide variable, natural-
sounding output, because the number of pre-
recorded strings or different templates becomes
very large (Theune, 2003). These techniques also
make it difficult to port the system into another
subject domain or language.
</bodyText>
<page confidence="0.988882">
61
</page>
<bodyText confidence="0.99990745945946">
In order to be widely successful, natural lan-
guage generation components of future dialog sys-
tems need to provide natural-sounding output
while being relatively easy to port. This can be
achieved by developing more sophisticated tech-
niques based on concepts from deep linguistically-
based NLG and text generation, and through the
use of existing resources that facilitate both the
natural-sounding and the portability requirement.
We might wonder what exactly it means for a
computer to generate ‘natural-sounding’ output.
Computer-generated natural-sounding output
should not mimic the output a human would con-
struct, because spontaneous human dialog tends to
be teeming with disfluencies, interruptions, syntac-
tically incorrect and incomplete sentences among
others (Zue, 1997). Furthermore, Oberlander
(1998) points out that humans do not always take
the most efficient route in their reasoning and
communication. These observations lead us to
define natural-sounding computer-generated output
to consist of utterances that are free of disfluencies
and interruptions, and where complete and
syntactically correct sentences convey the meaning
in a concise yet clear manner.
Secondly we can define the portability
requirement to include both domain and language
independence. Domain-independence suggests that
the system must be easily portable between
different domains, while language-independence
requires that the system must be able to
accommodate a new natural language without any
changes to the core components.
Section 2 of this paper explains some prerequi-
sites, such as the NLG pipeline architecture our
system is based on, and the FrameNet and Word-
Net resources. Next an overview of the system ar-
</bodyText>
<note confidence="0.387167">
Proceedings of the ACL Student Research Workshop, pages 61–66,
</note>
<page confidence="0.394404">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<bodyText confidence="0.9977606">
chitecture and implementation, as well as an in-
depth analysis of the lexicalization component are
presented. Section 3 presents related work. Section
4 outlines a preliminary conclusion and lists some
outstanding issues.
</bodyText>
<sectionHeader confidence="0.97321" genericHeader="introduction">
2 System Architecture
</sectionHeader>
<subsectionHeader confidence="0.996386">
2.1 Three-Stage Pipeline Architecture
</subsectionHeader>
<bodyText confidence="0.995469166666667">
Our natural language generator architecture
follows the three-stage pipeline architecture, as
described in Reiter &amp; Dale (2000). In this
architecture, the generation component of a text
generation system consists of the following
subcomponents:
</bodyText>
<listItem confidence="0.995173090909091">
• The document planner determines what the
actual content of the output will be on an
abstract level and decides how pieces of
content should be grouped together.
• The microplanner includes lexicalization,
aggregation, and referring expression
generation tasks.
• The surface realizer takes the information
constructed by the microplanner and
generates a syntactically correct sentence in
a natural language.
</listItem>
<subsectionHeader confidence="0.999613">
2.2 Lexical Resources
</subsectionHeader>
<bodyText confidence="0.999891217391304">
The use of FrameNet and WordNet in our system
is critical to its success. The FrameNet database
(Baker et al., 1998) is a machine-readable lexico-
graphic database which can be found at
http://framenet.icsi.berkeley.edu/. It is based on the
principles of Frame Semantics (Fillmore, 1985).
The following quote explains the idea behind
Frame Semantics: “The central idea of Frame Se-
mantics is that word meanings must be described
in relation to semantic frames – schematic repre-
sentations of the conceptual structures and patterns
of beliefs, practices, institutions, images, etc. that
provide a foundation for meaningful interaction in
a given speech community.” (Fillmore et al., 2003,
p. 235). In FrameNet, lexical units are grouped in
frames; frame hierarchy information is provided
for each frame, in combination with a list of se-
mantically annotated corpus sentences and syntac-
tic valence patterns.
WordNet is a lexical database that uses conceptual-
semantic and lexical relations in order to group
lexical items and link them to other groups
(Fellbaum, 1998).
</bodyText>
<subsectionHeader confidence="0.999653">
2.3 System Overview
</subsectionHeader>
<bodyText confidence="0.999943666666667">
Our system, called LEGEND (LExicalization in
natural language GENeration for Dialog systems)
adapts the pipeline architecture presented in
section 2.1 by replacing the document planner with
the dialog manager. This makes it more suitable
for use in dialog systems, since the dialog manager
decides on the actual content of the output in
dialog systems. Figure 1 below shows an overview
of our system architecture.
</bodyText>
<figureCaption confidence="0.997405">
Figure 1. System Architecture
</figureCaption>
<bodyText confidence="0.999976">
As figure 1 shows, the dialog manager provides
the generator with a dialog manager meaning
representation (DM MR), which contains the
content information for the answer.
Our research focuses on the lexicalization sub-
component of the microplanner (number 1 in fig-
ure 1). Lexicalization is further divided into two
processes: lexical choice and lexical search. Based
on the DM MR, the lexical choice process (number
2 in figure 1) constructs a set of all potential output
candidates. Section 2.5 describes the lexical choice
process in detail. Lexical search (number 3 in fig-
ure 1) consists of the decision algorithm that de-
</bodyText>
<page confidence="0.995698">
62
</page>
<bodyText confidence="0.99996825">
cides which one of the set of possible candidates is
most appropriate in any situation. Lexical search is
also responsible for packaging up the most appro-
priate candidate information in an adapted F-
structure, which is subsequently processed through
aggregation and referring expression generation,
and finally sent to the surface realizer. Section 2.6
describes the details of the lexical search process.
</bodyText>
<subsectionHeader confidence="0.998363">
2.4 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999959384615385">
Given time and resource constraints, our imple-
mentation will consist of a prototype (written in
Python) of the lexical choice and lexical search
processes only of the microplanner. We take a DM
MR as our input. Aggregation and referring ex-
pression generation requirements are hard-coded
for each example; algorithm development, identi-
fication and implementation for these modules is
beyond the scope of this research.
Our system uses the LFG-based XLE system’s
generator component as a surface realizer. For
more information, refer to Shemtov (1997) and
Kaplan &amp; Wedekind (2000).
</bodyText>
<subsectionHeader confidence="0.996604">
2.5 Lexical Choice
</subsectionHeader>
<bodyText confidence="0.999919384615385">
The task of the lexical choice process is to take the
meaning representation presented by the dialog
manager (refer to figure 1), and to construct a set
of output candidates. We will illustrate this by tak-
ing a simple example through the entire dialog sys-
tem. The example question and answer are
deliberately kept simple in order to focus on the
workings of the system, rather than the specifics of
the example.
Assume this is a dialog system that helps the
consumer in buying camping equipment. The user
says to the dialog system: “Where can I buy a
tent?” The speech recognizer recognizes the utter-
ance, and feeds this information to the parser. The
semantic parser parses the input and builds the
meaning representation shown in figure 2. The
main event (main verb) is identified as the lexical
item buy. The parser looks up this lexical item in
FrameNet, and identifies it as belonging to the
commerce_buy frame. This frame is defined in
FrameNet as: “... describing a basic commercial
transaction involving a buyer and a seller exchang-
ing money and goods, taking the perspective of the
buyer.” (http://framenet.icsi.berkeley.edu/). All
other elements in the meaning representation are
extracted from the input utterance.
</bodyText>
<figure confidence="0.7724378">
Event: buy
Frame: commerce_buy
Query: location
Agent: 1st pers sing
Patient: tent
</figure>
<figureCaption confidence="0.999841">
Figure 2. Parser Meaning Representation
</figureCaption>
<bodyText confidence="0.998868307692308">
This meaning representation is then sent to the
dialog manager. The dialog manager consults the
domain model for help in the query resolution, and
subsequently composes a meaning representation
consisting of the answer to the user’s question
(figure 3). For our example, the domain model pre-
sents the query resolution as “Camping World”,
the name of a (fictitious) store selling tents. The
DM MR also shows that the Agent and the Patient
have been identified by their frame element names.
This DM MR serves as the input to the
microplanner, where the first task is that of lexical
choice.
</bodyText>
<figure confidence="0.8456956">
Event: buy
Frame: commerce_buy
Query Resolution: place “Camping World”
Agent: buyer (1st p.s. =&gt; 2nd p.s.)
Object: goods (“tent”)
</figure>
<figureCaption confidence="0.999173">
Figure 3. Dialog Mgr Meaning Representation
</figureCaption>
<bodyText confidence="0.9962306">
In order to construct the set of output candidates,
the lexical choice process mines the FrameNet and
WordNet databases in order to find acceptable
generation possibilities. This is done in several
steps:
</bodyText>
<listItem confidence="0.950317333333333">
• In step 1, lexicalization variations of the
main Event within the same frame are iden-
tified.
• Step 2 consists of the investigation of lexical
variation in the frames that are one link
away in the hierarchy, namely the frame the
current frame inherits from, and the sub-
frames, if any exist.
• Step 3 is concerned with special relations
within FrameNet, such as the ‘use’-relation
The lexical variation within these frames is
investigated.
</listItem>
<bodyText confidence="0.9992605">
We return to our example in figure 3 to clarify
these 3 steps.
In step 1, appropriate lexical variation within the
same frame is identified. This is done by listing all
</bodyText>
<page confidence="0.997521">
63
</page>
<bodyText confidence="0.999772894736842">
lexical units of same syntactic category as the
original word. The following verbs are lexical units
in commerce_buy: buy, lease, purchase, rent.
These verbs are not necessarily synonyms or near-
synonyms of each other, but do belong to the same
frame. In order to determine which of these lexical
items are synonyms or near-synonyms, we turn to
WordNet, and look at the entry for buy. The only
lexical item that is also listed in one of the senses
of buy is purchase. We thus conclude that buy and
purchase are both good verb candidates.
Step 2 investigates the lexical items in the frames
that are one link away from the commerce_buy
frame. Commerce_buy inherits from getting, and
has no subframes. The lexical items of the getting
frame are listed. The lexical items of the getting
frame are: acquire, gain, get, obtain, secure. For
each entry, WordNet is consulted as a first pruning
mechanism. This results in the following:
</bodyText>
<listItem confidence="0.9999908">
• Acquire: get
• Gain: acquire, win
• Get: acquire
• Obtain: get, find, receive, incur
• Secure: no items on the list
</listItem>
<bodyText confidence="0.999787678571428">
How exactly lexical choice determines that get
and acquire are possible candidates, while the oth-
ers are not (because they aren’t suitable in the con-
text in which we use them) is as of yet an open
issue. It is also an open issue whether WordNet is
the most appropriate resource to use for this goal;
we must consider other options, such as Thesaurus,
etc...
In step 3 we investigate the other relations that
FrameNet presents. To date, we have only investi-
gated the ‘use relation’. Other relations available
are the inchoative and causative relations. At this
point, it is not entirely clear how those relations
will prove to be of any value to our task. The
commerce_buy frame uses com-
merce_goods_transfer, which is also used by
commerce_sell. We find our frame elements goods
and buyer in the commerce_sell frame as well.
Lexical choice concludes that the use of the lexical
items in this frame might be valuable and repeats
step 1 on these lexical items.
After all 3 steps are completed, we assume our
set of output candidates to be complete. The set of
output candidates is presented to the lexical search
process, whose task it is to choose the most appro-
priate candidate. For the example we have been
using throughout this section, the set of output
candidates is as follows:
</bodyText>
<listItem confidence="0.9999584">
• You can buy a tent at Camping World.
• You can purchase a tent at Camping World.
• You can get a tent at Camping World.
• You can acquire a tent at Camping World.
• Camping World sells tents.
</listItem>
<bodyText confidence="0.999971714285714">
As mentioned at the beginning of this section,
this example is very simple. For this reason, one
can definitely argue that the first 4 output possibili-
ties could be constructed in much simpler ways
than the method used here, e.g. by simply taking
the question and making it an affirmative sentence
through a simple rule. However, it should be
pointed out that the last possibility on the list
would not be covered by this simple method.
While user studies would need to provide backup
for this assumption, we feel that possibility 5 is a
very good example of natural-sounding output, and
thus proves our method to be valuable, even for
simple examples.
</bodyText>
<subsectionHeader confidence="0.987713">
2.6 Lexical Search
</subsectionHeader>
<bodyText confidence="0.999772590909091">
The set of output candidates for the example above
contains 5 possibilities. The main task of the lexi-
cal search process is to choose the most optimal
candidate, thus the most natural-sounding candi-
date (or at least one of the most natural-sounding
candidates, if more than one candidate fits that cri-
terion). There are a number of directions we can
take for this implementation.
One option is to implement a rule-based system.
Every output candidate is matched against the
rules, and the most appropriate one comes out at
the top. Problems with rule-based systems are
well-known: they must be handcrafted, which is
very time-consuming, constructing the rule base
such that the desired rules fire in the desired cir-
cumstances is somewhat of a “black” art, and of
course a rule base is highly domain-dependent.
Extending and maintaining it is also a laborious
effort.
Next we can look at a corpus-based technique.
One suggestion is to construct a language model of
the corpus data, and use this model to statistically
</bodyText>
<page confidence="0.998602">
64
</page>
<bodyText confidence="0.999711315789474">
determine the most suitable candidate. Langkilde
(2000) uses this approach. However, the main
problem here is that one needs a large corpus in the
domain of the application. Rambow (2001) agrees
that most often, no suitable corpora are available
for dialog system development.
Another possibility is to use machine learning to
train the microplanner. Walker et al. (2002) use
this approach in the SPOT sentence planner. Their
ranker’s main purpose is to choose between differ-
ent aggregation possibilities. The authors suggest
that many generation problems can successfully be
treated as ranking problems. The advantage of this
approach is that no domain-dependent hand-crafted
rules need to be constructed, and no existence of a
corpus is needed.
Our current research idea is somewhat related to
option two. A relatively small domain-independent
corpus of spoken dialogue is semi-automatically
labeled with frames and semantic roles. For each
frame, all the occurrences in the corpus are ordered
according to their frequency for each separate va-
lence pattern. This model is then used as a com-
parator for all output candidates, and the most
optimal one (most frequent one) will be selected.
This approach is currently not implemented; fur-
ther work needs to determine the viability of the
approach.
Independent of the method used to find the most
suitable candidate, the output must be packaged up
to be sent to the surface realizer. The XLE system
expects a fairly detailed syntactic description of the
utterance’s argument structure. We construct this
through the use of FrameNet and its valence pat-
tern information. In returning to our example, let’s
assume the selected candidate is “Camping World
sells tents.” Its meaning representation is as fol-
lows:
</bodyText>
<figure confidence="0.950384">
Event: sell
Frame: commerce_sell
Seller: Camping World
Goods: tents
</figure>
<figureCaption confidence="0.997606">
Figure 4. “Camping World sells tents.”
</figureCaption>
<bodyText confidence="0.964512941176471">
FrameNet provides an overview of the frame
elements a given frame requires (“core elements”)
and those that are optional (“peripheral elements”).
For the commerce_sell frame, the two core
elements are Goods and Seller. It also provides an
overview of the valence patterns that were found in
the annotated sentences for this frame. FrameNet
does not include frequency information for each
annotation. We thus need to pick a valence pattern
at random. One way of doing this is to find a
pattern that includes all (both) frame elements in
our utterance, and then use the (non-statistical)
frequency information. Figure 5 shows that, for our
example above, this results in:
FE_Seller sell FE_goods
With the following syntactic pattern:
NP.Ext sell NP.Obj
</bodyText>
<figureCaption confidence="0.92367">
Figure 5. Valence Patterns “commerce_sell”
</figureCaption>
<bodyText confidence="0.999970125">
Thus our output to the surface realizer indicates
that the seller frame element fills the subject role
and consists of an NP, while the goods frame
element fills the object role and consists of an NP.
Given this syntactic pattern information that we
gather from FrameNet, we are able to construct an
F-structure that is suitable as the input to the
surface realizer.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999981384615385">
To date, only a limited amount of research has
dealt with deep linguistically-based natural lan-
guage generation for dialog systems. Theune
(2003) presents an extensive overview of different
NLG methods and systems. A number of stochas-
tic-based generation efforts have been undertaken
in recent years. These generators generally consist
of an architecture similar to ours, in which first a
set of possible candidates is constructed, followed
by a decision process to choose the most appropri-
ate output. Some examples are the Nitrogen system
(Langkilde and Knight, 1998) and the SPoT train-
able sentence planner (Walker et al., 2002).
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="conclusions">
4 Outlook and Future Work
</sectionHeader>
<bodyText confidence="0.999901666666667">
We propose a novel approach to lexicalization in
NLG in order to generate natural-sounding speech
in a portable environment. The use of existing
</bodyText>
<figure confidence="0.998826058823529">
No. Annotated Patterns
Goods Seller
3
2
27
4
27
--
NP.Comp
NP
NP.Ext
NP.Obj
NP.Ext
NP.Ext
--
PP[by].Comp
NP.Ext
</figure>
<page confidence="0.99798">
65
</page>
<bodyText confidence="0.999941466666667">
lexical resources allows a system to be more port-
able across subject domains and languages, as long
as those resources are available for the targeted
domains and languages. FrameNet in particular
allows us to generate multiple possibilities of natu-
ral-sounding output while WordNet helps in a first
step to prune this set. FrameNet is further applied
on an existing corpus to help with the final deci-
sion on choosing the most optimal candidate
among the presented possibilities. The valence pat-
tern information in FrameNet helps constructing
the detailed syntactic pattern required by the sur-
face realizer.
A number of issues need further consideration,
including the following:
</bodyText>
<listItem confidence="0.990518125">
• lexical choice: investigation of semantic dis-
tances (step 2 of algorithm), use of WordNet
and/or other resources for first-step pruning.
• lexical search: develop initial research ideas
further and implement
• a user study to assess whether the goals of
natural-sounding output and portability have
successfully been fulfilled.
</listItem>
<bodyText confidence="0.9998685">
Furthermore, for this generator to be used in a
real-life environment, the entire dialog system
must be developed; for our research purposes, we
have left out the construction of a semantic parser,
the dialog manager, and an appropriate domain
model. We have also not focused on the develop-
ment of the aggregation and referring expression
generation subtasks in the microplanner.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999927178571429">
Baker, Collin F. and Charles J. Fillmore and John B.
Lowe. 1998. The Berkeley FrameNet project. In Pro-
ceedings of the COLING-ACL, Montreal, Canada.
Dale, Robert and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science
18:233-263.
Fellbaum, Christiane. 1998. A Semantic Network of
English: The Mother of All WordNets. In Computers
and the Humanities, Kluwer, The Netherlands, 32:
209-220.
Fillmore, Charles J. and Christopher R. Johnson and
Miriam R.L. Petruck. 2003. Background to Frame-
Net. In International Journal of Lexicography. Vol.
16 No. 3. 2003. Oxford University Press. Oxford,
UK.
Fillmore, Charles J. 1985. Frames and the semantics of
understanding. In Quaderni di Semantica, Vol. 6.2:
222-254.
Oberlander, Jon. 1998. Do the Right Thing... but Ex-
pect the Unexpected. Computational Linguistics.
Volume 24, Number 3. September 1998, pp. 501-
507. The MIT Press, Cambridge, MA.
Shemtov, Hadar. 1997. Ambiguity Management in
Natural Language Generation, PhD Thesis, Stanford.
Kaplan, R. M. and J. Wedekind. 2000. LFG generation
produces context-free languages. In Proceedings of
COLING-2000, Saarbruecken, pp. 297-302.
Langkilde, Irene. 2000. Forest-based Statistical Sen-
tence Generation. In Proceedings of the North
American Meeting of the Association for Computa-
tional Linguistics (NAACL), 2000.
Langkilde, Irene and Kevin Knight. 1998. Generation
that Exploits Corpus-Based Statistical Knowledge. In
Proceedings of Coling-ACL 1998. Montréal, Canada.
Rambow, Owen, 2001. Corpus-based Methods in Natu-
ral Language Generation: Friend or Foe? Invited talk
at the European Workshop for Natural Language
Generation, Toulouse, France.
Reiter, Ehud and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press. Cambridge, UK.
Theune, Mariët. 2000. From data to speech: language
generation in context. Ph.D. thesis, Eindhoven Uni-
versity of Technology.
Theune, Mariët. 2003. Natural Language Generation for
Dialogue: System Survey. University of Twente.
Twente, the Netherlands.
Walker, Marilyn and Owen Rambow and Monica Ro-
gati. 2002. Training a Sentence Planner for Spoken
Dialogue Using Boosting. Computer Speech and
Language, Special Issue on Spoken Language Gen-
eration, July 2002.
Zue, Victor. 1997. Conversational Interfaces: Advances
and Challenges. Keynote in Proceedings of Eu-
rospeech 1997. Rhodes, Greece.
</reference>
<page confidence="0.989">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474906">
<title confidence="0.9995385">Towards an Optimal Lexicalization in a Natural-Sounding Portable Natural Language Generator for Dialog Systems</title>
<author confidence="0.999886">Inge M R De_Bleecker</author>
<affiliation confidence="0.999904">Department of Linguistics The University of Texas at Austin</affiliation>
<address confidence="0.999573">Austin, TX 78712, USA</address>
<email confidence="0.999763">imrdb@mail.utexas.edu</email>
<abstract confidence="0.999637611111111">In contrast to the latest progress in speech recognition, the state-of-the-art in natural language generation for spoken language dialog systems is lagging behind. The core dialog managers are now more sophisticated; and natural-sounding and flexible output is expected, but not achieved with current simple techniques such as template-based systems. Portability of systems across subject domains and languages is another increasingly important requirement in dialog systems. This paper presents an outline of LEGEND, a system that is both portable and generates natural-sounding output. This goal is achieved through the novel use of existing lexical resources such as FrameNet and</abstract>
<intro confidence="0.47788">WordNet.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4491" citStr="Baker et al., 1998" startWordPosition="654" endWordPosition="657">t of a text generation system consists of the following subcomponents: • The document planner determines what the actual content of the output will be on an abstract level and decides how pieces of content should be grouped together. • The microplanner includes lexicalization, aggregation, and referring expression generation tasks. • The surface realizer takes the information constructed by the microplanner and generates a syntactically correct sentence in a natural language. 2.2 Lexical Resources The use of FrameNet and WordNet in our system is critical to its success. The FrameNet database (Baker et al., 1998) is a machine-readable lexicographic database which can be found at http://framenet.icsi.berkeley.edu/. It is based on the principles of Frame Semantics (Fillmore, 1985). The following quote explains the idea behind Frame Semantics: “The central idea of Frame Semantics is that word meanings must be described in relation to semantic frames – schematic representations of the conceptual structures and patterns of beliefs, practices, institutions, images, etc. that provide a foundation for meaningful interaction in a given speech community.” (Fillmore et al., 2003, p. 235). In FrameNet, lexical un</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, Collin F. and Charles J. Fillmore and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science</journal>
<pages>18--233</pages>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, Robert and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science 18:233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>A Semantic Network of English: The Mother of All WordNets.</title>
<date>1998</date>
<journal>In Computers and the Humanities, Kluwer, The Netherlands,</journal>
<volume>32</volume>
<pages>209--220</pages>
<contexts>
<context position="5435" citStr="Fellbaum, 1998" startWordPosition="799" endWordPosition="800">ntic frames – schematic representations of the conceptual structures and patterns of beliefs, practices, institutions, images, etc. that provide a foundation for meaningful interaction in a given speech community.” (Fillmore et al., 2003, p. 235). In FrameNet, lexical units are grouped in frames; frame hierarchy information is provided for each frame, in combination with a list of semantically annotated corpus sentences and syntactic valence patterns. WordNet is a lexical database that uses conceptualsemantic and lexical relations in order to group lexical items and link them to other groups (Fellbaum, 1998). 2.3 System Overview Our system, called LEGEND (LExicalization in natural language GENeration for Dialog systems) adapts the pipeline architecture presented in section 2.1 by replacing the document planner with the dialog manager. This makes it more suitable for use in dialog systems, since the dialog manager decides on the actual content of the output in dialog systems. Figure 1 below shows an overview of our system architecture. Figure 1. System Architecture As figure 1 shows, the dialog manager provides the generator with a dialog manager meaning representation (DM MR), which contains the </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane. 1998. A Semantic Network of English: The Mother of All WordNets. In Computers and the Humanities, Kluwer, The Netherlands, 32: 209-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet. In</title>
<date>2003</date>
<journal>International Journal of Lexicography.</journal>
<volume>16</volume>
<publisher>Oxford University Press. Oxford, UK.</publisher>
<contexts>
<context position="5057" citStr="Fillmore et al., 2003" startWordPosition="737" endWordPosition="740">its success. The FrameNet database (Baker et al., 1998) is a machine-readable lexicographic database which can be found at http://framenet.icsi.berkeley.edu/. It is based on the principles of Frame Semantics (Fillmore, 1985). The following quote explains the idea behind Frame Semantics: “The central idea of Frame Semantics is that word meanings must be described in relation to semantic frames – schematic representations of the conceptual structures and patterns of beliefs, practices, institutions, images, etc. that provide a foundation for meaningful interaction in a given speech community.” (Fillmore et al., 2003, p. 235). In FrameNet, lexical units are grouped in frames; frame hierarchy information is provided for each frame, in combination with a list of semantically annotated corpus sentences and syntactic valence patterns. WordNet is a lexical database that uses conceptualsemantic and lexical relations in order to group lexical items and link them to other groups (Fellbaum, 1998). 2.3 System Overview Our system, called LEGEND (LExicalization in natural language GENeration for Dialog systems) adapts the pipeline architecture presented in section 2.1 by replacing the document planner with the dialog</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Fillmore, Charles J. and Christopher R. Johnson and Miriam R.L. Petruck. 2003. Background to FrameNet. In International Journal of Lexicography. Vol. 16 No. 3. 2003. Oxford University Press. Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frames and the semantics of understanding.</title>
<date>1985</date>
<booktitle>In Quaderni di Semantica,</booktitle>
<volume>6</volume>
<pages>222--254</pages>
<contexts>
<context position="4660" citStr="Fillmore, 1985" startWordPosition="679" endWordPosition="680">nd decides how pieces of content should be grouped together. • The microplanner includes lexicalization, aggregation, and referring expression generation tasks. • The surface realizer takes the information constructed by the microplanner and generates a syntactically correct sentence in a natural language. 2.2 Lexical Resources The use of FrameNet and WordNet in our system is critical to its success. The FrameNet database (Baker et al., 1998) is a machine-readable lexicographic database which can be found at http://framenet.icsi.berkeley.edu/. It is based on the principles of Frame Semantics (Fillmore, 1985). The following quote explains the idea behind Frame Semantics: “The central idea of Frame Semantics is that word meanings must be described in relation to semantic frames – schematic representations of the conceptual structures and patterns of beliefs, practices, institutions, images, etc. that provide a foundation for meaningful interaction in a given speech community.” (Fillmore et al., 2003, p. 235). In FrameNet, lexical units are grouped in frames; frame hierarchy information is provided for each frame, in combination with a list of semantically annotated corpus sentences and syntactic va</context>
</contexts>
<marker>Fillmore, 1985</marker>
<rawString>Fillmore, Charles J. 1985. Frames and the semantics of understanding. In Quaderni di Semantica, Vol. 6.2: 222-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Oberlander</author>
</authors>
<title>Do the Right Thing... but Expect the Unexpected. Computational Linguistics.</title>
<date>1998</date>
<volume>24</volume>
<pages>501--507</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2375" citStr="Oberlander (1998)" startWordPosition="346" endWordPosition="347">ed by developing more sophisticated techniques based on concepts from deep linguisticallybased NLG and text generation, and through the use of existing resources that facilitate both the natural-sounding and the portability requirement. We might wonder what exactly it means for a computer to generate ‘natural-sounding’ output. Computer-generated natural-sounding output should not mimic the output a human would construct, because spontaneous human dialog tends to be teeming with disfluencies, interruptions, syntactically incorrect and incomplete sentences among others (Zue, 1997). Furthermore, Oberlander (1998) points out that humans do not always take the most efficient route in their reasoning and communication. These observations lead us to define natural-sounding computer-generated output to consist of utterances that are free of disfluencies and interruptions, and where complete and syntactically correct sentences convey the meaning in a concise yet clear manner. Secondly we can define the portability requirement to include both domain and language independence. Domain-independence suggests that the system must be easily portable between different domains, while language-independence requires t</context>
</contexts>
<marker>Oberlander, 1998</marker>
<rawString>Oberlander, Jon. 1998. Do the Right Thing... but Expect the Unexpected. Computational Linguistics. Volume 24, Number 3. September 1998, pp. 501-507. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadar Shemtov</author>
</authors>
<title>Ambiguity Management in Natural Language Generation, PhD Thesis,</title>
<date>1997</date>
<location>Stanford.</location>
<contexts>
<context position="7507" citStr="Shemtov (1997)" startWordPosition="1120" endWordPosition="1121">etails of the lexical search process. 2.4 Implementation Details Given time and resource constraints, our implementation will consist of a prototype (written in Python) of the lexical choice and lexical search processes only of the microplanner. We take a DM MR as our input. Aggregation and referring expression generation requirements are hard-coded for each example; algorithm development, identification and implementation for these modules is beyond the scope of this research. Our system uses the LFG-based XLE system’s generator component as a surface realizer. For more information, refer to Shemtov (1997) and Kaplan &amp; Wedekind (2000). 2.5 Lexical Choice The task of the lexical choice process is to take the meaning representation presented by the dialog manager (refer to figure 1), and to construct a set of output candidates. We will illustrate this by taking a simple example through the entire dialog system. The example question and answer are deliberately kept simple in order to focus on the workings of the system, rather than the specifics of the example. Assume this is a dialog system that helps the consumer in buying camping equipment. The user says to the dialog system: “Where can I buy a</context>
</contexts>
<marker>Shemtov, 1997</marker>
<rawString>Shemtov, Hadar. 1997. Ambiguity Management in Natural Language Generation, PhD Thesis, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>J Wedekind</author>
</authors>
<title>LFG generation produces context-free languages.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000, Saarbruecken,</booktitle>
<pages>297--302</pages>
<contexts>
<context position="7536" citStr="Kaplan &amp; Wedekind (2000)" startWordPosition="1123" endWordPosition="1126">al search process. 2.4 Implementation Details Given time and resource constraints, our implementation will consist of a prototype (written in Python) of the lexical choice and lexical search processes only of the microplanner. We take a DM MR as our input. Aggregation and referring expression generation requirements are hard-coded for each example; algorithm development, identification and implementation for these modules is beyond the scope of this research. Our system uses the LFG-based XLE system’s generator component as a surface realizer. For more information, refer to Shemtov (1997) and Kaplan &amp; Wedekind (2000). 2.5 Lexical Choice The task of the lexical choice process is to take the meaning representation presented by the dialog manager (refer to figure 1), and to construct a set of output candidates. We will illustrate this by taking a simple example through the entire dialog system. The example question and answer are deliberately kept simple in order to focus on the workings of the system, rather than the specifics of the example. Assume this is a dialog system that helps the consumer in buying camping equipment. The user says to the dialog system: “Where can I buy a tent?” The speech recognizer</context>
</contexts>
<marker>Kaplan, Wedekind, 2000</marker>
<rawString>Kaplan, R. M. and J. Wedekind. 2000. LFG generation produces context-free languages. In Proceedings of COLING-2000, Saarbruecken, pp. 297-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based Statistical Sentence Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American Meeting of the Association for Computational Linguistics (NAACL),</booktitle>
<contexts>
<context position="14755" citStr="Langkilde (2000)" startWordPosition="2347" endWordPosition="2348">ed against the rules, and the most appropriate one comes out at the top. Problems with rule-based systems are well-known: they must be handcrafted, which is very time-consuming, constructing the rule base such that the desired rules fire in the desired circumstances is somewhat of a “black” art, and of course a rule base is highly domain-dependent. Extending and maintaining it is also a laborious effort. Next we can look at a corpus-based technique. One suggestion is to construct a language model of the corpus data, and use this model to statistically 64 determine the most suitable candidate. Langkilde (2000) uses this approach. However, the main problem here is that one needs a large corpus in the domain of the application. Rambow (2001) agrees that most often, no suitable corpora are available for dialog system development. Another possibility is to use machine learning to train the microplanner. Walker et al. (2002) use this approach in the SPOT sentence planner. Their ranker’s main purpose is to choose between different aggregation possibilities. The authors suggest that many generation problems can successfully be treated as ranking problems. The advantage of this approach is that no domain-d</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Langkilde, Irene. 2000. Forest-based Statistical Sentence Generation. In Proceedings of the North American Meeting of the Association for Computational Linguistics (NAACL), 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that Exploits Corpus-Based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of Coling-ACL</booktitle>
<location>Montréal, Canada.</location>
<contexts>
<context position="18307" citStr="Langkilde and Knight, 1998" startWordPosition="2907" endWordPosition="2910">table as the input to the surface realizer. 3 Related Work To date, only a limited amount of research has dealt with deep linguistically-based natural language generation for dialog systems. Theune (2003) presents an extensive overview of different NLG methods and systems. A number of stochastic-based generation efforts have been undertaken in recent years. These generators generally consist of an architecture similar to ours, in which first a set of possible candidates is constructed, followed by a decision process to choose the most appropriate output. Some examples are the Nitrogen system (Langkilde and Knight, 1998) and the SPoT trainable sentence planner (Walker et al., 2002). 4 Outlook and Future Work We propose a novel approach to lexicalization in NLG in order to generate natural-sounding speech in a portable environment. The use of existing No. Annotated Patterns Goods Seller 3 2 27 4 27 -- NP.Comp NP NP.Ext NP.Obj NP.Ext NP.Ext -- PP[by].Comp NP.Ext 65 lexical resources allows a system to be more portable across subject domains and languages, as long as those resources are available for the targeted domains and languages. FrameNet in particular allows us to generate multiple possibilities of natura</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, Irene and Kevin Knight. 1998. Generation that Exploits Corpus-Based Statistical Knowledge. In Proceedings of Coling-ACL 1998. Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
</authors>
<title>Corpus-based Methods in Natural Language Generation: Friend or Foe? Invited talk at the European Workshop for Natural Language Generation,</title>
<date>2001</date>
<location>Toulouse, France.</location>
<contexts>
<context position="14887" citStr="Rambow (2001)" startWordPosition="2370" endWordPosition="2371">handcrafted, which is very time-consuming, constructing the rule base such that the desired rules fire in the desired circumstances is somewhat of a “black” art, and of course a rule base is highly domain-dependent. Extending and maintaining it is also a laborious effort. Next we can look at a corpus-based technique. One suggestion is to construct a language model of the corpus data, and use this model to statistically 64 determine the most suitable candidate. Langkilde (2000) uses this approach. However, the main problem here is that one needs a large corpus in the domain of the application. Rambow (2001) agrees that most often, no suitable corpora are available for dialog system development. Another possibility is to use machine learning to train the microplanner. Walker et al. (2002) use this approach in the SPOT sentence planner. Their ranker’s main purpose is to choose between different aggregation possibilities. The authors suggest that many generation problems can successfully be treated as ranking problems. The advantage of this approach is that no domain-dependent hand-crafted rules need to be constructed, and no existence of a corpus is needed. Our current research idea is somewhat re</context>
</contexts>
<marker>Rambow, 2001</marker>
<rawString>Rambow, Owen, 2001. Corpus-based Methods in Natural Language Generation: Friend or Foe? Invited talk at the European Workshop for Natural Language Generation, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press. Cambridge, UK.</publisher>
<contexts>
<context position="3825" citStr="Reiter &amp; Dale (2000)" startWordPosition="554" endWordPosition="557">he FrameNet and WordNet resources. Next an overview of the system arProceedings of the ACL Student Research Workshop, pages 61–66, Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics chitecture and implementation, as well as an indepth analysis of the lexicalization component are presented. Section 3 presents related work. Section 4 outlines a preliminary conclusion and lists some outstanding issues. 2 System Architecture 2.1 Three-Stage Pipeline Architecture Our natural language generator architecture follows the three-stage pipeline architecture, as described in Reiter &amp; Dale (2000). In this architecture, the generation component of a text generation system consists of the following subcomponents: • The document planner determines what the actual content of the output will be on an abstract level and decides how pieces of content should be grouped together. • The microplanner includes lexicalization, aggregation, and referring expression generation tasks. • The surface realizer takes the information constructed by the microplanner and generates a syntactically correct sentence in a natural language. 2.2 Lexical Resources The use of FrameNet and WordNet in our system is c</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter, Ehud and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press. Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariët Theune</author>
</authors>
<title>From data to speech: language generation in context.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Eindhoven University of Technology.</institution>
<marker>Theune, 2000</marker>
<rawString>Theune, Mariët. 2000. From data to speech: language generation in context. Ph.D. thesis, Eindhoven University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariët Theune</author>
</authors>
<title>Natural Language Generation for Dialogue: System Survey.</title>
<date>2003</date>
<journal>University of Twente. Twente, the Netherlands.</journal>
<contexts>
<context position="1460" citStr="Theune, 2003" startWordPosition="214" endWordPosition="215"> achieved through the novel use of existing lexical resources such as FrameNet and WordNet. 1 Introduction Most of the natural language generation (NLG) components in current dialog systems are implemented through the use of simple techniques such as a library of hand-crafted and pre-recorded utterances, or a template-based system where the templates contain slots in which different values can be inserted. These techniques are unmanageable if the dialog system aims to provide variable, naturalsounding output, because the number of prerecorded strings or different templates becomes very large (Theune, 2003). These techniques also make it difficult to port the system into another subject domain or language. 61 In order to be widely successful, natural language generation components of future dialog systems need to provide natural-sounding output while being relatively easy to port. This can be achieved by developing more sophisticated techniques based on concepts from deep linguisticallybased NLG and text generation, and through the use of existing resources that facilitate both the natural-sounding and the portability requirement. We might wonder what exactly it means for a computer to generate </context>
<context position="17884" citStr="Theune (2003)" startWordPosition="2844" endWordPosition="2845">wing syntactic pattern: NP.Ext sell NP.Obj Figure 5. Valence Patterns “commerce_sell” Thus our output to the surface realizer indicates that the seller frame element fills the subject role and consists of an NP, while the goods frame element fills the object role and consists of an NP. Given this syntactic pattern information that we gather from FrameNet, we are able to construct an F-structure that is suitable as the input to the surface realizer. 3 Related Work To date, only a limited amount of research has dealt with deep linguistically-based natural language generation for dialog systems. Theune (2003) presents an extensive overview of different NLG methods and systems. A number of stochastic-based generation efforts have been undertaken in recent years. These generators generally consist of an architecture similar to ours, in which first a set of possible candidates is constructed, followed by a decision process to choose the most appropriate output. Some examples are the Nitrogen system (Langkilde and Knight, 1998) and the SPoT trainable sentence planner (Walker et al., 2002). 4 Outlook and Future Work We propose a novel approach to lexicalization in NLG in order to generate natural-sound</context>
</contexts>
<marker>Theune, 2003</marker>
<rawString>Theune, Mariët. 2003. Natural Language Generation for Dialogue: System Survey. University of Twente. Twente, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Training a Sentence Planner for Spoken Dialogue Using Boosting.</title>
<date>2002</date>
<journal>Computer Speech and Language, Special Issue on Spoken Language Generation,</journal>
<contexts>
<context position="15071" citStr="Walker et al. (2002)" startWordPosition="2396" endWordPosition="2399"> rule base is highly domain-dependent. Extending and maintaining it is also a laborious effort. Next we can look at a corpus-based technique. One suggestion is to construct a language model of the corpus data, and use this model to statistically 64 determine the most suitable candidate. Langkilde (2000) uses this approach. However, the main problem here is that one needs a large corpus in the domain of the application. Rambow (2001) agrees that most often, no suitable corpora are available for dialog system development. Another possibility is to use machine learning to train the microplanner. Walker et al. (2002) use this approach in the SPOT sentence planner. Their ranker’s main purpose is to choose between different aggregation possibilities. The authors suggest that many generation problems can successfully be treated as ranking problems. The advantage of this approach is that no domain-dependent hand-crafted rules need to be constructed, and no existence of a corpus is needed. Our current research idea is somewhat related to option two. A relatively small domain-independent corpus of spoken dialogue is semi-automatically labeled with frames and semantic roles. For each frame, all the occurrences i</context>
<context position="18369" citStr="Walker et al., 2002" startWordPosition="2918" endWordPosition="2921">nly a limited amount of research has dealt with deep linguistically-based natural language generation for dialog systems. Theune (2003) presents an extensive overview of different NLG methods and systems. A number of stochastic-based generation efforts have been undertaken in recent years. These generators generally consist of an architecture similar to ours, in which first a set of possible candidates is constructed, followed by a decision process to choose the most appropriate output. Some examples are the Nitrogen system (Langkilde and Knight, 1998) and the SPoT trainable sentence planner (Walker et al., 2002). 4 Outlook and Future Work We propose a novel approach to lexicalization in NLG in order to generate natural-sounding speech in a portable environment. The use of existing No. Annotated Patterns Goods Seller 3 2 27 4 27 -- NP.Comp NP NP.Ext NP.Obj NP.Ext NP.Ext -- PP[by].Comp NP.Ext 65 lexical resources allows a system to be more portable across subject domains and languages, as long as those resources are available for the targeted domains and languages. FrameNet in particular allows us to generate multiple possibilities of natural-sounding output while WordNet helps in a first step to prune</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2002</marker>
<rawString>Walker, Marilyn and Owen Rambow and Monica Rogati. 2002. Training a Sentence Planner for Spoken Dialogue Using Boosting. Computer Speech and Language, Special Issue on Spoken Language Generation, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
</authors>
<title>Conversational Interfaces: Advances and Challenges. Keynote</title>
<date>1997</date>
<booktitle>in Proceedings of Eurospeech</booktitle>
<location>Rhodes, Greece.</location>
<contexts>
<context position="2343" citStr="Zue, 1997" startWordPosition="343" endWordPosition="344"> port. This can be achieved by developing more sophisticated techniques based on concepts from deep linguisticallybased NLG and text generation, and through the use of existing resources that facilitate both the natural-sounding and the portability requirement. We might wonder what exactly it means for a computer to generate ‘natural-sounding’ output. Computer-generated natural-sounding output should not mimic the output a human would construct, because spontaneous human dialog tends to be teeming with disfluencies, interruptions, syntactically incorrect and incomplete sentences among others (Zue, 1997). Furthermore, Oberlander (1998) points out that humans do not always take the most efficient route in their reasoning and communication. These observations lead us to define natural-sounding computer-generated output to consist of utterances that are free of disfluencies and interruptions, and where complete and syntactically correct sentences convey the meaning in a concise yet clear manner. Secondly we can define the portability requirement to include both domain and language independence. Domain-independence suggests that the system must be easily portable between different domains, while </context>
</contexts>
<marker>Zue, 1997</marker>
<rawString>Zue, Victor. 1997. Conversational Interfaces: Advances and Challenges. Keynote in Proceedings of Eurospeech 1997. Rhodes, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>