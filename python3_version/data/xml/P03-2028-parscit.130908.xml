<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001905">
<title confidence="0.963657">
Spoken Interactive ODQA System: SPIQA
</title>
<author confidence="0.9730425">
Chiori Hori, Takaaki Hori, Hajime Tsukada,
Hideki Isozaki, Yutaka Sasaki and Eisaku Maeda
</author>
<affiliation confidence="0.731115">
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
</affiliation>
<address confidence="0.593481">
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<sectionHeader confidence="0.9753" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986615384615">
We have been investigating an interactive
approach for Open-domain QA (ODQA)
and have constructed a spoken interactive
ODQA system, SPIQA. The system de-
rives disambiguating queries (DQs) that
draw out additional information. To test
the efficiency of additional information re-
quested by the DQs, the system recon-
structs the user’s initial question by com-
bining the addition information with ques-
tion. The combination is then used for an-
swer extraction. Experimental results re-
vealed the potential of the generated DQs.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999579153846154">
Open-domain QA (ODQA), which extracts answers
from large text corpora, such as newspaper texts, has
been intensively investigated in the Text REtrieval
Conference (TREC). ODQA systems return an ac-
tual answer in response to a question written in a
natural language. However, the information in the
first question input by a user is not usually sufficient
to yield the desired answer. Interactions for col-
lecting additional information to accomplish QA are
needed. To construct more precise and user-friendly
ODQA systems, a speech interface is used for the
interaction between human beings and machines.
Our goal is to construct a spoken interactive
ODQA system that includes an automatic speech
recognition (ASR) system and an ODQA system.
To clarify the problems presented in building such
a system, the QA systems constructed so far have
been classified into a number of groups, depending
on their target domains, interfaces, and interactions
to draw out additional information from users to ac-
complish set tasks, as is shown in Table 1. In this
table, text and speech denote text input and speech
input, respectively. The term “addition” represents
additional information queried by the QA systems.
This additional information is separate to that de-
rived from the user’s initial questions.
</bodyText>
<tableCaption confidence="0.96993">
Table 1: Domain and data structure for QA systems
</tableCaption>
<bodyText confidence="0.508350833333333">
target domain specific open
data structure knowledge DB unstructured text
text without addition CHAT-80 SAIQA
with addition MYCIN (SPIQA∗)
speech without addition Harpy VAQA
with addition JUPITER (SPIQA∗)
</bodyText>
<equation confidence="0.507806">
∗ SPIQA is our system.
</equation>
<bodyText confidence="0.9991815">
To construct spoken interactive ODQA systems,
the following problems must be overcome: 1. Sys-
tem queries for additional information to extract an-
swers and effective interaction strategies using such
queries cannot be prepared before the user inputs the
question. 2. Recognition errors degrade the perfor-
mance of QA systems. Some information indispens-
able for extracting answers is deleted or substituted
with other words.
Our spoken interactive ODQA system, SPIQA,
copes with the first problem by adopting disam-
biguating users’ questions using system queries. In
addition, a speech summarization technique is ap-
plied to handle recognition errors.
</bodyText>
<sectionHeader confidence="0.554004" genericHeader="method">
2 Spoken Interactive QA system: SPIQA
</sectionHeader>
<bodyText confidence="0.982349375">
Figure 1 shows the components of our system, and
the data that flows through it. This system com-
prises an ASR system (SOLON), a screening filter
that uses a summarization method, and ODQA en-
gine (SAIQA) for a Japanese newspaper text corpus,
a Deriving Disambiguating Queries (DDQ) module,
and a Text-to-Speech Synthesis (TTS) engine (Fi-
nalFluet).
</bodyText>
<figureCaption confidence="0.99877">
Figure 1: Components and data flow in SPIQA.
</figureCaption>
<sectionHeader confidence="0.545091" genericHeader="method">
ASR system
</sectionHeader>
<bodyText confidence="0.99989">
Our ASR system is based on the Weighted Finite-
State Transducers (WFST) approach that is becom-
ing a promising alternative formulation for the tra-
ditional decoding approach. The WFST approach
offers a unified framework representing various
knowledge sources in addition to producing an op-
timized search network of HMM states. We com-
bined cross-word triphones and trigrams into a sin-
gle WFST and applied a one-pass search algorithm
to it.
</bodyText>
<subsectionHeader confidence="0.908434">
Screening filter
</subsectionHeader>
<bodyText confidence="0.982421833333333">
To alleviate degradation of the QA’s perfor-
mance by recognition errors, fillers, word fragments,
and other distractors in the transcribed question, a
screening filter that removes these redundant and
irrelevant information and extracts meaningful in-
formation is required. The speech summarization
approach (C. Hori et. al., 2003) is applied to the
screening process, wherein a set of words maximiz-
ing a summarization score that indicates the appro-
priateness of summarization is extracted automati-
cally from a transcribed question, and these words
are then concatenated together. The extraction pro-
cess is performed using a Dynamic Programming
(DP) technique.
ODQA engine
The ODQA engine, SAIQA, has four compo-
nents: question analysis, text retrieval, answer hy-
pothesis extraction, and answer selection.
DDQ module
When the ODQA engine cannot extract an appro-
priate answer to a user’s question, the question is
considered to be “ambiguous.” To disambiguate the
initial questions, the DDQ module automatically de-
rives disambiguating queries (DQs) that require in-
formation indispensable for answer extraction. The
situations in which a question is considered ambigu-
ous are those when users’ questions exclude indis-
pensable information or indispensable information
is lost through ASR errors. These instances of miss-
ing information should be compensated for by the
users.
To disambiguate a question, ambiguous phrases
within it should be identified. The ambiguity of
each phrase can be measured by using the struc-
tural ambiguity and generality score for the phrase.
The structural ambiguity is based on the dependency
structure of the sentence; phrase that is not modified
by other phrases is considered to be highly ambigu-
ous. Figure 2 has an example of a dependency struc-
ture, where the question is separated into phrases.
Each arrow represents the dependency between two
phrases. In this example, “the World Cup” has no
</bodyText>
<figureCaption confidence="0.996201">
Figure 2: Example of dependency structure.
</figureCaption>
<bodyText confidence="0.999204538461538">
modifiers and needs more information to be identi-
fied. “Southeast Asia” also has no modifiers. How-
ever, since “the World Cup”appears more frequently
than “Southeast Asia” in the retrieved corpus, “the
World Cup” is more difficult to identify. In other
words, words that frequently occur in a corpus rarely
help to extract answers in ODQA systems. There-
fore, it is adequate for the DDQ module to generate
questions relating to “World Cup” in this example,
such as “What kind of World Cup?” , “What year
was the World Cup held?”.
The structural ambiguity of the n-th phrase is de-
fined as
</bodyText>
<equation confidence="0.879780666666667">
N
AD(Pn) = log 1 − E
f i=1:i=j4n D(Pi, Pn) ,
</equation>
<figure confidence="0.992487515151515">
Additional
info.
Question
reconstructor
ODQA engine
(SAIQA)
Recognition
result
First
question
Screening
filter
ASR
Answer
sentence
Answer
sentence generator
Answer
Yes
Answer
derived?
No
DDQ
sentence
DDQ
module
TTS
Question/
Additional info.
User Answer/
DDQ speech
New question
Which country in Southeast Asia won the world cup ?
</figure>
<bodyText confidence="0.992894235294118">
where the complete question is separated into N
phrases, and D(Pi, Pn) is the probability that phrase
Pn will be modified by phrase Pi, which can be cal-
culated using Stochastic Dependency Context-Free
Grammar (SDCFG) (C. Hori et. al., 2003).
Using this SDCFG, only the number of non-
terminal symbols is determined and all combina-
tions of rules are applied recursively. The non-
terminal symbol has no specific function, such as
a noun phrase. All the probabilities of rules are
stochastically estimated based on data. Probabilities
for frequently used rules become greater, and those
for rarely used rules become smaller. Even though
transcription results given by a speech recognizer are
ill-formed, the dependency structure can be robustly
estimated by our SDCFG.
The generality score is defined as
</bodyText>
<equation confidence="0.80143">
AG(Pn) = Ew∈P,,,:w=cont log P(
</equation>
<bodyText confidence="0.9979176875">
where P(w) is the unigram probability of w based
on the corpus to be retrieved. Thus, “w = cont”
means that w is a content word such as a noun, verb
or adjective.
We generate the DQs using templates of interrog-
ative sentences. These templates contain an inter-
rogative and a phrase taken from the user’s question,
i.e., “What kind of * ?”, “What year was * held?”
and “Where is * ?”.
The DDQ module selects the best DQ based on its
linguistic appropriateness and the ambiguity of the
phrase. The linguistic appropriateness of DQs can
be measured by using a language model, N-gram.
Let Smn be a DQ generated by inserting the n-th
phrase into the m-th template. The DDQ module
selects the DQ that maximizes the DQ score:
</bodyText>
<equation confidence="0.970327">
H(Smn) = λLL(Smn)+λDAD(Pn)+λGAG(Pn),
</equation>
<bodyText confidence="0.999833428571429">
where L(·) is a linguistic score such as the loga-
rithm for trigram probability, and λL, λD, and λG
are weighting factors to balance the scores.
Hence, the module can generate a sentence that
is linguistically appropriate and asks the user to dis-
ambiguate the most ambiguous phrase in his or her
question.
</bodyText>
<sectionHeader confidence="0.99477" genericHeader="evaluation">
3 Evaluation Experiments
</sectionHeader>
<bodyText confidence="0.9998663">
Questions consisting of 69 sentences read aloud by
seven male speakers were transcribed by our ASR
system. The question transcriptions were processed
with a screening filter and input into the ODQA
engine. Each question consisted of about 19 mor-
phemes on average. The sentences were grammat-
ically correct, formally structured, and had enough
information for the ODQA engine to extract the cor-
rect answers. The mean word recognition accuracy
obtained by the ASR system was 76%.
</bodyText>
<subsectionHeader confidence="0.99994">
3.1 Screening filter
</subsectionHeader>
<bodyText confidence="0.9999809375">
Screening was performed by removing recognition
errors using a confidence measure as a threshold and
then summarizing it within an 80% to 100% com-
paction ratio. In this summarization technique, the
word significance and linguistic score for summa-
rization were calculated using text from Mainichi
newspapers published from 1994 to 2001, compris-
ing 13.6M sentences with 232M words. The SD-
CFG for the word concatenation score was calcu-
lated using the manually parsed corpus of Mainichi
newspapers published from 1996 to 1998, consist-
ing of approximately 4M sentences with 68M words.
The number of non-terminal symbols was 100. The
posterior probability of each transcribed word in a
word graph obtained by ASR was used as the confi-
dence score.
</bodyText>
<subsectionHeader confidence="0.998609">
3.2 DDQ module
</subsectionHeader>
<bodyText confidence="0.999983214285714">
The word generality score AG was computed using
the same Mainichi newspaper text described above,
while the SDCFG for the dependency ambiguity
score AD for each phrase was the same as that used
in (C. Hori et. al., 2003). Eighty-two types of inter-
rogative sentences were created as disambiguating
queries for each noun and noun-phrase in each ques-
tion and evaluated by the DDQ module. The linguis-
tic score L indicating the appropriateness of inter-
rogative sentences was calculated using 1000 ques-
tions and newspaper text extracted for three years.
The structural ambiguity score ADwas calculated
based on the SDCFG, which was used for the screen-
ing filter.
</bodyText>
<subsectionHeader confidence="0.995589">
3.3 Evaluation method
</subsectionHeader>
<bodyText confidence="0.990068703703704">
The DQs generated by the DDQ module were eval-
uated in comparison with manual disambiguation
queries. Although the questions read by the seven
speakers had sufficient information to extract ex-
act answers, some recognition errors resulted in a
w),
loss of information that was indispensable for ob-
taining the correct answers. The manual DQs were
made by five subjects based on a comparison of
the original written questions and the transcription
results given by the ASR system. The automatic
DQs were categorized into two classes: APPRO-
PRIATE when they had the same meaning as at
least one of the five manual DQs, and INAPPRO-
PRIATE when there was no match. The QA per-
formance in using recognized (REC) and screened
questions (SCRN) were evaluated by MRR (Mean
Reciprocal Rank) (http://trec.nist.gov/data/qa.html).
SCRN was compared with the transcribed question
that just had recognition errors removed (DEL). In
addition, the questions reconstructed manually by
merging these questions and additional information
requested the DQs generated by using SCRN, (DQ)
were also evaluated. The additional information was
extracted from the original users’ question without
recognition errors. In this study, adding information
by using the DQs was performed only once.
</bodyText>
<subsectionHeader confidence="0.960993">
3.4 Evaluation results
</subsectionHeader>
<bodyText confidence="0.99997325">
Table 2 shows the evaluation results in terms of
the appropriateness of the DQs and the QA-system
MRRs. The results indicate that roughly 50% of the
DQs generated by the DDQ module based on the
screened results were APPROPRIATE. The MRR
for manual transcription (TRS) with no recognition
errors was 0.43. In addition, we could improve the
MRR from 0.25 (REC) to 0.28 (DQ) by using the
DQs only once. Experimental results revealed the
potential of the generated DQs in compensating for
the degradation of the QA performance due to recog-
nition errors.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.994901909090909">
The proposed spoken interactive ODQA system,
SPIQA copes with missing information by adopt-
ing disambiguation of users’ questions by system
queries. In addition, a speech summarization tech-
nique was applied for handling recognition errors.
Although adding information was performed using
DQs only once, experimental results revealed the
potential of the generated DQs to acquire indispens-
able information that was lacking for extracting an-
swers. In addition, the screening filter helped to gen-
erate the appropriate DQs. Future research will in-
</bodyText>
<tableCaption confidence="0.906995">
Table 2: Evaluation results of disambiguating
queries generated by the DDQ module.
</tableCaption>
<table confidence="0.999547545454546">
SPK Word MRR w/o APP IN-
acc. errors APP
REC DEL SCRN DQ
A 70% 0.19 0.16 0.17 0.23 4 32 33
B 76% 0.31 0.24 0.29 0.31 8 36 25
C 79% 0.26 0.18 0.26 0.30 10 34 25
D 73% 0.27 0.21 0.24 0.30 4 35 30
E 78% 0.24 0.21 0.24 0.27 7 31 31
F 80% 0.28 0.25 0.30 0.33 8 34 27
G 74% 0.22 0.19 0.19 0.22 3 35 31
AVG 76% 0.25 0.21 0.24 0.28 9% 49% 42%
</table>
<bodyText confidence="0.932785454545454">
An integer without a % other than MRRs indicates number of
sentences. Word acc.:word accuracy, SPK:speaker, AVG: aver-
aged values, w/o errors: transcribed sentences without recog-
nition errors, APP: appropriate DQs and InAPP: inappropriate
DQs.
clude an evaluation of the appropriateness of DQs
derived repeatedly to obtain the final answers. In
addition, the interaction strategy automatically gen-
erated by the DDQ module should be evaluated in
terms of how much the DQs improve QA’s total per-
formance.
</bodyText>
<sectionHeader confidence="0.999089" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991076">
F. Pereira et. al., “Definite Clause Grammars for Language
Analysis –a Survey of the Formalism and a Comparison with
Augmented Transition Networks,” Artificial Intelligence, 13:
231-278, 1980.
E. H. Shortliffe, “Computer-Based Medical Consultations:
MYCIN,” Elsevier/North Holland, New York NY, 1976.
B. Lowerre et. al., “The Harpy speech understanding system,”
W. A. Lea (Ed.), Trends in Speech recognition, pp. 340, Pren-
tice Hall.
L. D. Erman et. al., “The Hearsay-II Speech-Understanding
System: Integrating Knowledge to Resolve Uncertainty,”
ACM computing Survays, Vol. 12, No. 2, pp. 213 – 253,
1980.
V. Zue, et al., “JUPITER: A Telephone-Based Conversational
Interface for Weather Information,” IEEE Transactions on
Speech and Audio Processing, Vol. 8, No. 1, 2000.
S. Harabagiu et. al., “Open-Domain Voice-Activated Ques-
tion Answering,” COLING2002, Vol.I, pp. 321–327, Taipei,
2002.
C. Hori et. al., ”A Statistical Approach for Automatic Speech
Summarization,” EURASIP Journal on Applied Signal Pro-
cessing (EURASIP), pp128–139, 2003.
Y. Sasaki et. al., “NTT’s QA Systems for NTCIR QAC-1,”
Working Notes of the Third NTCIR Workshop Meeting,
pp.63–70, 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765260">
<title confidence="0.99838">Spoken Interactive ODQA System: SPIQA</title>
<author confidence="0.895655">Chiori Hori</author>
<author confidence="0.895655">Takaaki Hori</author>
<author confidence="0.895655">Hajime Tsukada</author>
<author confidence="0.895655">Yutaka Sasaki Maeda Isozaki</author>
<affiliation confidence="0.9971265">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</affiliation>
<address confidence="0.982097">2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<abstract confidence="0.998924785714285">We have been investigating an interactive approach for Open-domain QA (ODQA) and have constructed a spoken interactive system, The system derives disambiguating queries (DQs) that draw out additional information. To test the efficiency of additional information requested by the DQs, the system reconstructs the user’s initial question by combining the addition information with question. The combination is then used for answer extraction. Experimental results revealed the potential of the generated DQs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Definite Clause Grammars for Language Analysis –a Survey of the Formalism and a Comparison with Augmented Transition Networks,”</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>231--278</pages>
<marker>Pereira, 1980</marker>
<rawString>F. Pereira et. al., “Definite Clause Grammars for Language Analysis –a Survey of the Formalism and a Comparison with Augmented Transition Networks,” Artificial Intelligence, 13: 231-278, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Shortliffe</author>
</authors>
<title>Computer-Based Medical Consultations: MYCIN,” Elsevier/North Holland,</title>
<date>1976</date>
<location>New York NY,</location>
<marker>Shortliffe, 1976</marker>
<rawString>E. H. Shortliffe, “Computer-Based Medical Consultations: MYCIN,” Elsevier/North Holland, New York NY, 1976.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Lowerre</author>
</authors>
<title>The Harpy speech understanding system,”</title>
<booktitle>Trends in Speech recognition,</booktitle>
<pages>340</pages>
<publisher>Prentice Hall.</publisher>
<marker>Lowerre, </marker>
<rawString>B. Lowerre et. al., “The Harpy speech understanding system,” W. A. Lea (Ed.), Trends in Speech recognition, pp. 340, Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L D Erman</author>
</authors>
<title>The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty,”</title>
<date>1980</date>
<journal>ACM computing Survays,</journal>
<volume>12</volume>
<pages>213--253</pages>
<marker>Erman, 1980</marker>
<rawString>L. D. Erman et. al., “The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty,” ACM computing Survays, Vol. 12, No. 2, pp. 213 – 253, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Zue</author>
</authors>
<title>JUPITER: A Telephone-Based Conversational Interface for Weather Information,”</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<marker>Zue, 2000</marker>
<rawString>V. Zue, et al., “JUPITER: A Telephone-Based Conversational Interface for Weather Information,” IEEE Transactions on Speech and Audio Processing, Vol. 8, No. 1, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
</authors>
<date>2002</date>
<booktitle>Open-Domain Voice-Activated Question Answering,” COLING2002, Vol.I,</booktitle>
<pages>321--327</pages>
<location>Taipei,</location>
<marker>Harabagiu, 2002</marker>
<rawString>S. Harabagiu et. al., “Open-Domain Voice-Activated Question Answering,” COLING2002, Vol.I, pp. 321–327, Taipei, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
</authors>
<title>A Statistical Approach for Automatic Speech Summarization,”</title>
<date>2003</date>
<booktitle>EURASIP Journal on Applied Signal Processing (EURASIP), pp128–139,</booktitle>
<marker>Hori, 2003</marker>
<rawString>C. Hori et. al., ”A Statistical Approach for Automatic Speech Summarization,” EURASIP Journal on Applied Signal Processing (EURASIP), pp128–139, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Sasaki</author>
</authors>
<date>2002</date>
<booktitle>NTT’s QA Systems for NTCIR QAC-1,” Working Notes of the Third NTCIR Workshop Meeting, pp.63–70,</booktitle>
<marker>Sasaki, 2002</marker>
<rawString>Y. Sasaki et. al., “NTT’s QA Systems for NTCIR QAC-1,” Working Notes of the Third NTCIR Workshop Meeting, pp.63–70, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>