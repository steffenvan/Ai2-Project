<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000802">
<title confidence="0.998903">
Evaluating Models of Latent Document Semantics
in the Presence of OCR Errors
</title>
<author confidence="0.998009">
Daniel D. Walker, William B. Lund, and Eric K. Ringger
</author>
<affiliation confidence="0.98802">
Brigham Young University
</affiliation>
<address confidence="0.645265">
Provo, Utah, USA
</address>
<email confidence="0.975221">
danl4@cs.byu.edu, bill lund@byu.edu, ringger@cs.byu.edu
</email>
<sectionHeader confidence="0.99244" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999441">
Models of latent document semantics such as
the mixture of multinomials model and La-
tent Dirichlet Allocation have received sub-
stantial attention for their ability to discover
topical semantics in large collections of text.
In an effort to apply such models to noisy
optical character recognition (OCR) text out-
put, we endeavor to understand the effect
that character-level noise can have on unsu-
pervised topic modeling. We show the ef-
fects both with document-level topic analy-
sis (document clustering) and with word-level
topic analysis (LDA) on both synthetic and
real-world OCR data. As expected, experi-
mental results show that performance declines
as word error rates increase. Common tech-
niques for alleviating these problems, such as
filtering low-frequency words, are successful
in enhancing model quality, but exhibit fail-
ure trends similar to models trained on unpro-
cessed OCR output in the case of LDA. To our
knowledge, this study is the first of its kind.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990531914894">
As text data becomes available in massive quanti-
ties, it becomes increasingly difficult for human cu-
rators to manually catalog and index modern docu-
ment collections. To aid in the automation of such
tasks, algorithms can be used to create models of
the latent semantics present in a given corpus. One
example of this type of analysis is document cluster-
ing, in which documents are grouped into clusters
by topic. Another type of topic analysis attempts
to discover finer-grained topics—labeling individual
words in a document as belonging to a particular
topic. This type of analysis has grown in popular-
ity recently as inference on models containing large
numbers of latent variables has become feasible.
The modern explosion of data includes vast
amounts of historical documents, made available
by means of Optical Character Recognition (OCR),
which can introduce significant numbers of er-
rors. Undertakings to produce such data include
the Google Books, Internet Archive, and HathiTrust
projects. In addition, researchers are having increas-
ing levels of success in digitizing hand-written man-
uscripts (Bunke, 2003), though error rates remain
much higher than for OCR. Due to their nature, these
collections often lack helpful meta-data or labels. In
the absence of such labels, unsupervised machine
learning methods can reveal patterns in the data.
Finding good estimates for the parameters of
models such as the mixture of multinomials docu-
ment model (Walker and Ringger, 2008) and the La-
tent Dirichlet Allocation (LDA) model (Blei et al.,
2003) requires accurate counts of the occurrences
and co-occurrences of words. Depending on the
age of a document and the way in which it was
created, the OCR process results in text containing
many types of noise, including character-level er-
rors, which distort these counts. It is obvious, there-
fore, that model quality must suffer, especially since
unsupervised methods are typically much more sen-
sitive to noise than supervised methods. Good su-
pervised learning algorithms are substantially more
immune to spurious patterns in the data created by
noise for the following reason: under the mostly
reasonable assumption that the process contributing
the noise operates independently from the class la-
bels, the noise in the features will not correlate well
with the class labels, and the algorithm will learn
</bodyText>
<page confidence="0.949547">
240
</page>
<note confidence="0.816265">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 240–250,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999976522727273">
to ignore those features arising from noise. Unsu-
pervised models, in contrast, have no grounding in
labels to prevent them from confusing patterns that
emerge by chance in the noise with the “true” pat-
terns of potential interest. For example, even on
clean data, LDA will often do poorly if the very sim-
ple feature selection step of removing stop-words is
not performed first. Though we expect model qual-
ity to decrease, it is not well understood how sensi-
tive these models are to OCR errors, or how quality
deteriorates as the level of OCR noise increases.
In this work we show how the performance of un-
supervised topic modeling algorithms degrades as
character-level noise is introduced. We demonstrate
the effect using both artificially corrupted data and
an existing real-world OCR corpus. The results are
promising, especially in the case of relatively low
word error rates (e.g. less than 20%). Though model
quality declines as errors increase, simple feature se-
lection techniques enable the learning of relatively
high quality models even as word error rates ap-
proach 50%. This result is particularly interesting
in that even humans find it difficult to make sense
of documents with error rates of that magnitude
(Munteanu et al., 2006).
Because of the difficulties in evaluating topic
models, even on clean data, these results should not
be interpreted as definitive answers, but they do offer
insight into prominent trends. For example, proper-
ties of the OCR data suggest measures that can be
taken to improve performance in future work. It is
our hope that this work will lead to an increase in
the usefulness of collections of OCRed texts, as doc-
ument clustering and topic modeling expose useful
patterns to historians and other interested parties.
The remainder of the paper is outlined as follows.
After an overview of related work in Section 2, Sec-
tion 3 introduces the data used in our experiments,
including an explanation of how the synthetic data
were created and of some of their properties. Sec-
tion 4 describes how the experiments were designed
and carried out, and gives an analysis of the results
both empirically and qualitatively. Finally, conclu-
sions and future work are presented in Section 5.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99989424">
Topic models have been used previously to process
documents digitized by OCR, including eighteenth-
century American newspapers (Newmann and
Block, 2006), OCRed editions of Science (Blei and
Lafferty, 2006), OCRed NIPS papers (Wang and
McCallum, 2006), and books digitized by the Open
Content Alliance (Mimno and Mccallum, 2007).
Most of this previous work ignores the presence of
OCR errors or attempts to remove corrupted tokens
with special pre-processing such as stop-word re-
moval and frequency cutoffs. Also, there are at least
two instances of using topic modeling to improve
the results of an OCR algorithm (Wick et al., 2007;
Farooq et al., 2009).
Similar evaluations to ours have been conducted
to assess the effect of OCR errors on supervised doc-
ument classification (Taghva et al., 2001; Agarwal et
al., 2007), information retrieval (Taghva et al., 1994;
Beitzel et al., 2003), and a more general set of natu-
ral language processing tasks (Lopresti, 2008). Re-
sults suggest that in these supervised tasks OCR er-
rors have a minimal impact on the performance of
the methods employed, though it has remained un-
clear how well these results transfer to unsupervised
methods.
</bodyText>
<sectionHeader confidence="0.99729" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999836947368421">
We conducted experiments on synthetic and real
OCR data. As a real-world dataset, we used a cor-
pus consisting of 604 of the Eisenhower World War
II communiqu´es (Jordan, 1945; Lund and Ringger,
2009). These documents relate the daily progress
of the Allied campaign from D-Day until the Ger-
man surrender. They were originally produced as
telegrams and were distributed as mimeographed
copies. The quality of the originals is often quite
poor, making them a challenging case for OCR en-
gines. The communiqu´es have been OCRed using
three popular OCR engines: ABBYY FineReader
(ABBYY, 2010), OmniPage Pro (Nuance Commu-
nications, Inc., 2010), and Tesseract (Google, Inc.,
2010). In addition, the curator of the collection has
created a “gold standard” transcription, from which
it is possible to obtain accurate measures of average
document word error rates (WER) for each engine,
which are: 19.9%, 30.4%, and 50.1% respectively.
</bodyText>
<page confidence="0.99597">
241
</page>
<bodyText confidence="0.999925020833333">
While the real-world data is attractive as an ex-
ample of just the sort of data that the questions ad-
dressed here apply to, it is limited in size and scope.
All of the documents in the Eisenhower corpus dis-
cuss the fairly narrow topic of troop movements and
battle developments taking place at the end of World
War II. Neither the subject matter nor the means of
conveyance allowed for a large or diverse vocabu-
lary of discourse.
In an attempt to generalize our results to larger
and more diverse data, we also ran experiments
using synthetic OCR data. This synthetic data
was created by corrupting “clean” datasets, adding
character-level noise. The synthetic data was cre-
ated by building a noise model based on mistakes
made by the worst performing OCR engine on the
Eisenhower dataset, Tesseract.
To construct the noise model, a character-level
alignment between the human transcribed Eisen-
hower documents and the OCR output was first com-
puted. From this alignment, the contingency table
Md was generated such that Md��y was the count of
the instances in which a character x in the transcript
was aligned with a y in the OCR output. The rows
in this matrix were then normalized so that each rep-
resented the parameters of a categorical distribution,
conditioned on x. To parameterize the amount of
noise being generated, the Md matrix was interpo-
lated with an identity matrix I using a parameter y so
that the final interpolated parameters MZ were cal-
culated with the formula MZ = yMd + (1 − y)I.
So that at y = 0, MZ = I and no errors were in-
troduced. At y = 1.0, MZ = Md, and we would
expect to see characters corrupted at the same rate
as in the output of the OCR engine.
We then iterated over each document, choosing a
new (possibly the same) character yl for each orig-
inal character xl according to the probability distri-
bution p(yl = w&apos;|xl = w) = MZw�w,. Our pro-
cess was a one-substitution algorithm, as we did not
include instances of insertions or deletions, conse-
quently words were changed but not split or deleted.
This allowed for a more straightforward calculation
of word error rate. Segmentation errors can still
occur in the learning stage, however, as the noise
model sometimes replaced alphabet characters with
punctuation characters, which were treated as delim-
iters by our tokenizer.
</bodyText>
<table confidence="0.9972846">
Dataset |D |K # Types # Tokens
20 News 19997 20 107211 2261805
Reuters 11367 81 29034 747458
Enron 4935 32 60495 2063667
Eisenhower 604 N/A 8039 76674
</table>
<tableCaption confidence="0.94261">
Table 1: Summary of test dataset characteristics. |D |is
the number of documents in the dataset. K is the number
of human-labeled classes provided with the dataset.
</tableCaption>
<bodyText confidence="0.998127416666667">
We chose three datasets to corrupt: 20 News-
groups (Lang, 1995), Reuters 21578 (Lewis, 1997),
and the LDC-annotated portion of the Enron e-mail
archive (Berry et al., 2007). Each of these datasets
were corrupted at values y = i*0.01 for i E (0,13).
At this point, the word error rate of the corrupted
data was near 50% and, since this was approxi-
mately the WER observed for the worst OCR engine
on the real-world data, we chose to stop there. The
word error rate was calculated during the corruption
process. Here is an example sentence corrupted at
two y values:
</bodyText>
<equation confidence="0.546726333333333">
y = 0.000 I am also attaching the RFP itself.
y = 0.02 I am also attachEng the RFP itself.
y = 0.10 I Jm alAo attaching the RFP itself.
</equation>
<bodyText confidence="0.999892136363637">
Table 3 shows some basic statistics for the
datasets. The values shown are for the “clean” ver-
sions of the data. For an example of how noise
and pre-processing techniques affect these counts
see Section 4.1.
It is interesting to note that the word error rates
produced by the noise model appear to be signif-
icantly higher than first expected. One might as-
sume that the WER should increase fairly steadily
from 0% at y = 0 to about 50% (the error rate
achieved by the Tesseract OCR engine on the Eisen-
hower dataset) at y = 1. There are at least two
sources for the discrepancy. First, the vocabulary
of the Eisenhower dataset does not match well with
that of any of the source datasets from which the
synthetic data were generated. This means that the
word and character distributions are different and so
the error rates will be as well. Secondly, whereas our
technique gives the same probability of corruption to
all instances of a given character, errors in true OCR
output are bursty and more likely to be concentrated
in specific tokens, or regions, of a document. This
</bodyText>
<page confidence="0.984148">
242
</page>
<figure confidence="0.350988">
Number of Features Culled
</figure>
<bodyText confidence="0.9996524375">
is because most sources of noise do not affect docu-
ment images uniformly. Also, modern OCR engines
do not operate at just the character level. They in-
corporate dictionaries and language models to pre-
vent them from positing words that are highly un-
likely. As a consequence, an OCR engine is much
more likely to either get a whole word correct, or to
miss it completely, concentrating multiple errors in
a single word. This is the difference between 10 er-
rors in a single word, which only contributes 1 to the
numerator of the WER formula and 10 errors spread
across 10 different words, which contributes 10 to
the numerator. Furthermore, because content bear-
ing words tend to be relatively rare, language mod-
els are poorer for them than for frequent function
words, meaning that the words most correlated with
semantics are also the most likely to be corrupted by
an OCR engine.
An example of this phenomenon is easy to find.
In the Enron corpus, there are 165,871 instances of
the word “the” and 102 instances of the string “thc”.
Since “c” has a high rate of confusion with “e”, we
would expect at least some instances of “the” to be
corrupted to “thc” by the error model. At -y = 0.03,
there are 156,663 instances of the word “the” and
513 instances of “thc”. So, the noise model converts
“the” to “thc” roughly 0.3% of the time. In con-
trast, there are no instances of “thc” in the Tesseract
OCR output even though there are 5186 instances
of “the” in the transcription text, and so we would
expect approximately 16 occurrences of “thc” if the
errors introduced by the noise model were truly rep-
resentative of the errors in the actual OCR output.
Another interesting property of the noise intro-
duced by actual OCR engines and our synthetic
noise model is the way in which this noise affects
words distributions. This is very important, since
word occurrence and co-occurrence counts are the
basis for model inference in both clustering and
topic modeling. As mentioned previously, one com-
mon way of lessening the impact of OCR noise
when training topic models over OCRed data is to
apply a frequency cutoff filter to cull words that oc-
cur fewer than a certain number of times. Figures 1
and 2 show the number of word types that are culled
from the synthetic 20 Newsgroups OCR data and the
Eisenhower OCR data, respectively, at various levels
of noise. Note that the cutoff filters use a strict “less
</bodyText>
<figure confidence="0.991846444444445">
1000000
2
5
10
600000
400000
200000
00 10 20 30 40 50
WER
</figure>
<figureCaption confidence="0.977401333333333">
Figure 1: The number of word types culled with fre-
quency cutoff filters applied to the 20 Newsgroups data
with various levels of errors introduced.
</figureCaption>
<bodyText confidence="0.99982136">
than”, so a frequency cutoff of 2 eliminates only
words that occur once in the entire dataset. Also,
these series are additive, as the words culled with
a frequency cutoff of 2 are a subset of those culled
with a frequency cutoff of j &gt; 2.
In both cases, it is apparent that by far the largest
impact that noise has is in the creation of single-
tons. It seems that the most common corruptions in
these scenarios is the creation of one-off word types
through a unique corruption of a (most likely rare)
word. This means that it is unlikely that enough evi-
dence will be available to associate, through similar
contexts, the original word and its corrupted forms.
Due to the fact that most clustering and topic
models ignore the forms of word tokens (the charac-
ters that make them up), and only take into account
word identities, we believe that the similarity in the
way in which real OCR engines and our synthetic
OCR noise model distort word distributions is suf-
ficient evidence to support the use of the synthetic
data until larger and better real-world OCR datasets
can be made available. Though the actual errors will
take a different form, the character-level details of
the errors are less relevant than the word distribution
alterations for the models in question.
</bodyText>
<sectionHeader confidence="0.996298" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999148">
We ran experiments on both the real and synthetic
OCR data. In this section we explain our experi-
</bodyText>
<page confidence="0.750133">
800000
243
</page>
<figureCaption confidence="0.959232">
Figure 2: The number of word types culled with fre-
quency cutoff filters applied to the transcript and three
OCR engine outputs for the Eisenhower data.
</figureCaption>
<bodyText confidence="0.8854565">
mental methodology and present both empirical and
qualitative analyses of the results.
</bodyText>
<subsectionHeader confidence="0.981252">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999962013698631">
For the synthetic OCR datasets, we ran clustering
experiments using EM on a mixture of multinomials
(c.f. (Walker and Ringger, 2008)). We specified
the number of clusters to be the same as the num-
ber of classes provided with the data. Clusters were
evaluated using several external cluster quality met-
rics which compare “gold standard” labels to those
created through clustering. The metrics used were
Variation of Information (VI) (Meil˘a, 2007), and
the Adjusted Rand Index (ARI) (Hubert and Arabie,
1985). Other metrics were also calculated (e.g. the
V-Measure (Rosenberg and Hirschberg, 2007), and
Average Entropy (Liu et al., 2003)), but these results
were excluded due to space constraints and the fact
that their plots are similar to those shown. We did
not cluster the Eisenhower data because of the ab-
sence of the class labels necessary for evaluation.
For both the synthetic and non-synthetic data we
also trained LDA topic models (Blei et al., 2003) us-
ing Gibbs sampling. We used the implementation
found in the MALLET software package (McCal-
lum, 2002) with the option enabled to learn the pri-
ors during sampling as discussed by Wallach et al.
(2009a). Each LDA model was trained on 90% of
the documents in each dataset. The trained model
was used to calculate an estimate of the marginal
log-likelihood of the remaining 10% of the docu-
ments using the left-to-right algorithm (Wallach et
al., 2009b). The number of topics used for each
dataset was adjusted a priori according to the num-
ber of documents it contained. We used 100 topics
for Enron and Eisenhower, 150 for Reuters, and 200
for 20 Newsgroups.
In addition to running experiments on the “raw”
synthetic data, we also applied simple unsupervised
feature selectors before training in order to evalu-
ate the effectiveness of such measures in mitigat-
ing problems caused by OCR errors. For the topic
modeling (LDA) experiments three feature selectors
were used. The first method employed was a simple
term frequency cutoff filter (TFCF), with a cutoff
of 5 as in (Wang and McCallum, 2006). The next
method employed was Term Contribution (TC), a
feature selection algorithm developed for document
clustering (Liu et al., 2003). Term contribution is
parameterized by the number of word types that are
to remain after selection. We attempted three val-
ues for this parameter, 10,000, 20,000, and 50,000.
The final method we employed was a method called
Top-N per Document (TNPD) (Walker and Ring-
ger, 2010), which is a simple feature selection al-
gorithm that first assigns each type in every doc-
ument a document-specific score (e.g. its TF-IDF
weight), and then selects words to include in the fi-
nal vocabulary by choosing the N words with the
highest score from each document in the corpus. We
found that N = 1 gave the best results at the great-
est reduction in word types. After the vocabulary is
built, all words not in the vocabulary are culled from
the documents. This does not mean that all docu-
ments contain only one word after feature selection,
as the top word in one document may occur in many
other documents, even if it is not the top word in
those documents. Likewise, if two documents would
both contribute the same word, then the second doc-
ument makes no contribution to the vocabulary. This
process can result in vocabularies with thousands of
words, leaving sufficient words in each document
for analysis. For the clustering experiments, initial
tests showed little difference in the performance of
the feature selectors, so only the TNPD selector was
used. Figures 3(a) and 3(b) show how the various
pre-processing methods affect word type and token
</bodyText>
<figure confidence="0.9944245">
30000
2
5
10
20000
15000
10000
50000 10 20 30 40 50 60
WER
Number of Words Culled
25000
244
Types
40000
60000
50000
30000
20000
10000
00 5 10 15 20 25 30 35 40 45
Word Error Rate
tc.10000
tc.20000
tc.50000
tfcf.5
tnpd.1
Tokens
2200000
2000000
1800000
1600000
1400000
1200000
1000000
800000
6000000 5 10 15 20 25 30 35 40 45
Word Error Rate
tc.10000
tc.20000
tc.50000
tfcf.5
tnpd.1
(a) The number of word types remaining after pre-processing.
(b) The number of word tokens remaining after pre-processing.
</figure>
<figureCaption confidence="0.999988">
Figure 3: The effect of pre-processing on token and type counts for the 20 Newsgroups dataset at various error rates.
</figureCaption>
<bodyText confidence="0.9999875">
counts, respectively, for the 20 Newsgroups dataset.
In contrast, without pre-processing the number of
types scales from 107,211 to 892,983 and the num-
ber of tokens from 2,261,805 to 3,073,208.
Because all of these procedures alter the number
of words and tokens in the final data, log-likelihood
measured on a held-out set cannot be used to accu-
rately compare the quality of topic models trained
on pre-processed data, as the held-out data will con-
tain many unknown words. If the held-out data is
also pre-processed to only include known words,
then the likelihood will be greater for those proce-
dures that remove the most tokens, as the product
that dominates the calculation will have fewer terms.
If unknown words are allowed to remain, even a
smoothed model will assign them very little prob-
ability and so models will be heavily penalized.
We use an alternative method for evaluating the
topic models, discussed in (Griffiths et al., 2005), to
avoid the aforementioned problems with an evalu-
ation based on log-likelihood. Since the synthetic
data is derived from datasets that have topical docu-
ment labels, we are able to use the output from LDA
in a classification problem with the word vectors
for each document being replaced by the assigned
topic vectors. This is equivalent to using LDA as a
dimensionality reduction pre-process for document
classification. A naive Bayes learner is trained on a
portion of the topic vectors, labeled with the origi-
nal document label, and then the classification accu-
racy on a held-out portion of the data is computed.
Ten-fold cross-validation is used to control for sam-
pling issues. The rationale behind this evaluation is
that, even though the topics discovered by LDA will
not correspond directly to the labels, there should at
least be a high degree of correlation. Models that
discover topical semantics that correlate well with
the labels applied by humans will yield higher clas-
sification accuracies and be considered better mod-
els according to this metric.
To compensate for the randomness inherent in
the algorithms, each experiment was replicated ten
times. The results of these runs were averaged to
produce the values reported here.
</bodyText>
<subsectionHeader confidence="0.990371">
4.2 Empirical Analysis
</subsectionHeader>
<bodyText confidence="0.999984066666667">
Both the mixture of multinomials document model
and LDA appear to be fairly resilient to character-
level noise. Figures 4 and 5 show the results of the
document clustering experiments with and without
feature selection, respectively. Memory issues pre-
vented the collection of results for the highest error
rates on the Enron and Reuters data without feature
selection.
With no pre-processing, the results are somewhat
mixed. The Enron dataset experiences almost no
quality degradation as the WER increases, yielding
remarkably constant results according to the metrics.
However, this may be an artifact of the relatively
poor starting performance for this dataset, a result
of the fact that the gold standard labels do not align
</bodyText>
<page confidence="0.992722">
245
</page>
<figure confidence="0.999103347826087">
0.5
0.4
0.3
0.2
0.1
0.00 5 10 15 20 25 30 35 40 45
Word Error Rate
ari
20_newsgroups
reuters21578
enron
vi
4.5
4.0
3.5
3.0
2.5
2.00 5 10 15 20 25 30 35 40 45
Word Error Rate
20_newsgroups
reuters21578
enron
(a) Adjusted Rand Index results (b) Variation of Information results (lower is better)
</figure>
<figureCaption confidence="0.999952">
Figure 4: Results for the clustering experiments on the three synthetic datasets without feature selection.
</figureCaption>
<bodyText confidence="0.999175145454546">
well with automatically discovered patterns because
they correspond to external events. In contrast, the
Reuters data appears to experience drastic degrada-
tion in performance. Once feature selection occurs,
however, performance remains much more stable as
error rates increase.
Figure 6(a) shows the results of running LDA on
the transcript and digitized versions of the Eisen-
hower dataset. Log-likelihoods of the held-out set
are plotted with respect to the WER observed for
each OCR engine. The results support the find-
ing that the WER of the OCR engine that produced
the data has a significant negative correlation with
model quality. Unfortunately, it was not possible
to compare the performance of the pre-processing
methods on this dataset, due to a lack of document
topic labels and the deficiencies of log-likelihood
mentioned previously.
Figure 6(b) shows the results of the LDA topic-
modeling experiments on the three “raw” synthetic
datasets. Similar trends are observed in this graph.
LDA experiences a marked degree of performance
degradation, with all of the trend lines indicating a
linear decrease in log-likelihood.
Figures 7(a) through 7(c) show the results of eval-
uating the various proposed pre-processing proce-
dures in the context of topic modeling. In the graph
“noop.0” represents no pre-processing, “tc.N” are
the Term Contribution method parameterized to se-
lect N word types, “tfcf.5” is the term frequency cut-
off filter with a cutoff of 5 and “tnpd.1” is the Top
N per Document method with N = 1. The y-axis is
the average of the results for 10 distinct trials, where
the output for each trial is the average of the ten ac-
curacies achieved using ten-fold cross-validation as
described in Section 4.1.
Here, the cross-validation accuracy metric reveals
a slightly different story. These results show that
topic quality on both the raw and pre-processed
noisy data degrades at a rate relative to the amount
of errors in the data. That is, the difference in perfor-
mance between two relatively low word error rates
(e.g. 5% and 7% on the Reuters data) is small,
whereas the differences between two high error rates
(e.g. 30% and 32% on the Reuters data) can be rela-
tively quite large.
While pre-processing does improve model qual-
ity, in the case of LDA this improvement amounts
to a nearly constant boost; at high error rates quality
is improved the same amount as at low error rates.
Degradations in model quality, therefore, follow the
same trends, occurring at mostly the same rate in
pre-processed data as in the raw noisy data. This is
in contrast to the clustering experiments where pre-
processing virtually eliminates failure trends.
</bodyText>
<subsectionHeader confidence="0.999173">
4.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999110333333333">
Higher values measured with automated metrics
such as log-likelihood on a held-out set and the
cross-validation classification metric discussed here
</bodyText>
<page confidence="0.993765">
246
</page>
<figure confidence="0.999062565217392">
0.5
0.4
0.3
0.2
0.1
0.00 10 20 30 40 50
Word Error Rate
ari
20_newsgroups
reuters21578
enron
vi
4.5
4.0
3.5
3.0
2.5
2.00 10 20 30 40 50
Word Error Rate
20_newsgroups
reuters21578
enron
(a) Adjusted Rand Index results (b) Variation of Information results (lower is better)
</figure>
<figureCaption confidence="0.999906">
Figure 5: Results for the clustering experiments on the three synthetic OCR datasets with TNPD feature selection.
</figureCaption>
<bodyText confidence="0.997262750000001">
do not necessarily indicate superior topics according
to human judgement (Chang et al., 2009). In order
to provide a more thorough discussion of the relative
quality of the topic models induced on the OCR data
versus those induced on clean data, we sampled the
results of several of the runs of the LDA algorithm.
In Tables 2 and 3 we show the top words for the
five topics with the highest learned topic prior (α in
the LDA literature) learned during Gibbs sampling.
This information is shown for the Reuters data first
with no corruption and then at the highest error rate
for which we have results for that data of 45% WER.
In general, there appears to be a surprisingly good
correlation between the topics learned on the clean
data and those learned on the corrupted data, given
the high level of noise involved. The topics are
generally cohesive, containing mostly terms relat-
ing to financial market news. However, the topics
trained on the clean data, though all related to finan-
cial markets, are fairly distinctive. Topic 3, for ex-
ample seems to be about fluctuations in stock prices,
and Topics 106 and 34 about business acquisitions
and mergers. The topics trained on the noisy data
are fairly homogeneous and the differences between
them are more difficult to identify.
In addition, it appears as though the first topic
(topic 93) is not very coherent at all. This topic is
significantly larger, in terms of the number of tokens
assigned to it than the other topics shown in either
table. In addition, the most probable words listed for
the topic seem less cohesive than for the other top-
ics. It contains many two-letter words that are likely
a mixture of valid terms (e.g., stock exchange and
ticker symbols, and parts of place names like “Rio
de Janeiro”) and corruptions of real words. For ex-
ample, even though there are no instances of “ts” as
a distinct token in the clean Reuters data, it is in the
list of the top 19 words for topic 93. This is perhaps
due to the fact that “is” can easily be converted to
“ts” by mistaking t for i.
It is also the case that, for most topics learned
on the corrupted data, the most probable words for
those topics tend to be shorter, on average, than for
topics learned on clean data. We believe this is due
to the fact that the processes used to add noise to the
data (both real OCR engines and our synthetic noise
model) are more likely to corrupt long words, es-
pecially in the case of the synthetic data which was
created using a character-level noise model.
Examination of the data tends to corroborate this
hypothesis, though even long words usually contain
only a few errors. For example, in the 20 News-
groups data there are 379 instances of the word “yes-
terday”, a long word that is not close to other English
words in edit distance. When the data has been cor-
rupted to a WER of 47.9%, however, there are only
109 instances of “yesterday” and 132 tokens that are
within an edit distance of 1 from “yesterday”.
To some extent, we would expect to observe sim-
ilar trends in real-world data. However, most OCR
</bodyText>
<page confidence="0.988423">
247
</page>
<figure confidence="0.999811214285714">
(a) Eisenhower Communiqu´es
(b) Synthetic Data
abbyy
omnipage
tesseract
transcript
10 0 10 20 30 40 50 60
Word Error Rate
70000
75000
90000
Log-likelihood of Held-out
95000
80000
85000
70000000 10 20 30 40 50
Word Error Rate
Log-likelihood of Held-out
4000000
5000000
6000000
0
1000000
2000000
3000000
20_newsgroups
reuters21578
enron
</figure>
<figureCaption confidence="0.998556">
Figure 6: Log-likelihood of heldout data for the LDA experiments.
</figureCaption>
<figure confidence="0.991497">
Avereage CV Accuracy
54
70
62
60
58
56
520 5 10 15 20 25 30 35 40 45
Word Error Rate
noop.0
tc.10000
tc.20000
tc.50000
tfcf.5
tnpd.1
68
66
64
(a) 20 Newsgroups Data
73
72
71
70
69
68
670 5 10 15 20 25 30 35 40 45
Word Error Rate
Avereage CV Accuracy
noop.0
tc.10000
tc.20000
tc.50000
tfcf.5
tnpd.1
Avereage CV Accuracy
40
43
42
41
39
38
340 5 10 15 20 25 30 35 40 45
Word Error Rate
noop.0
tc.10000
tc.20000
tc.50000
tfcf.5
tnpd.1
37
36
35
(b) Reuters Data (c) Enron Data
</figure>
<figureCaption confidence="0.9898445">
Figure 7: Average ten-fold cross-validation accuracy for the LDA pre-processing experiments on the synthetic OCR
data.
</figureCaption>
<page confidence="0.932197">
248
</page>
<bodyText confidence="0.61451185">
Topic Words Tokens
64 told, market, reuters, reuter, added, time, 67159
year, major, years, president, make, made,
march, world, today, officials, industry,
government, move
3 year, pct, prices, expected, rise, lower, 32907
higher, demand, increase, due, fall, de-
cline, current, high, end, added, level,
drop, market
106 reuter, corp, company, unit, sale, march, 22167
dlrs, mln, sell, subsidiary, acquisition,
terms, group, april, purchase, acquired,
products, division, business
34 shares, dlrs, company, mln, stock, pct, 22668
share, common, reuter, corp, agreement,
march, shareholders, buy, cash, outstand-
ing, merger, acquire, acquisition
7 mln, cts, net, shr, qtr, revs, reuter, avg, shrs, 18511
march, mths, dlrs, sales, st, corp, oct, note,
year, april
</bodyText>
<tableCaption confidence="0.975165">
Table 2: Top words for the five topics with the highest α
prior values found using MALLET for one run of LDA
on the uncorrupted Reuters data.
</tableCaption>
<bodyText confidence="0.9997875">
engines employ language models and dictionaries to
attempt to mitigate OCR errors. As a result, given
that a word recognition error has occurred in true
OCR output, it is more likely to be an error that lies
at an edit distance greater than one from the true
word, or else it would have been corrected inter-
nally. For example, there are 349 instances of the
word “yesterday” in the Eisenhower transcripts, and
284 instances in the Tesserect OCR output and only
5 tokens within an edit distance of one, meaning that
60 corruptions of this word contained more than one
error, making up 90% of the errors for that word.
However, many of these errors still contain most of
the letters from the original word (e.g. “yesterdj.”,
and “yestjkday”). In all cases, the corrupted versions
of a given word are very rare, occurring usually only
once or twice in the noisy output, making them use-
less features for informing a model.
</bodyText>
<sectionHeader confidence="0.998267" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.77492168">
The primary outcome of these experiments is an
understanding regarding when clustering and LDA
topic models can be expected to function well on
noisy OCR data. Our results imply that clustering
methods should perform almost as well on OCR data
as they do on clean data, provided that a reasonable
feature selection algorithm is employed. The LDA
topic model degraded less gracefully in performance
Topic Words Tokens
93 reuter, march, pct, year, april, ed, market, 258932
er, told, es, st, end, ts, al, de, ng, id, sa,
added
99 company, pct, corp, shares, stock, dlrs, 50377
share, offer, group, reuter, mln, march,
unit, stake, buy, cash, bid, sale, board
96 mln, cts, net, shr, qtr, dlrs, revs, reuter, 54659
note, oper, avg, march, shrs, year, mths, st,
sales, corp, oct
141 mln, dlrs, year, net, quarter, share, com- 40475
pany, billion, tax, sales, earnings, dlr,
profit, march, income, ln, results, sale, corp
53 pct, year, rose, rise, january, february, fell, 22556
march, index, december, month, figures,
compared, reuter, rate, earlier, show, ago,
base
</bodyText>
<tableCaption confidence="0.83459375">
Table 3: Top words for the five topics with the highest α
prior values found using MALLET for one run of LDA
on the Reuters data corrupted with the data-derived noise
model to a WER of 45%.
</tableCaption>
<bodyText confidence="0.9999505">
with the addition of character level errors to its in-
put, with higher error rates impacting model quality
in a way that was apparent empirically in the log-
likelihood and ten-fold cross-validation metrics as
well as through human inspection of the produced
topics. Pre-processing the data also helps model
quality for LDA, yet still yields failure trends sim-
ilar to those observed on unprocessed data.
We found it to be the case that even in data with
high word error rates, corrupted words often share
many characters in common with their uncorrupted
form. This suggests an approach in which word sim-
ilarities are used to cluster the unique corrupted ver-
sions of a word in order to increase the evidence
available to the topic model during training time and
improve model quality. As the quality of models in-
creases on these noisy datasets, we anticipate a con-
sequent rise in their usefulness to researchers and
historians as browsing the data and mining it for use-
ful patterns becomes more efficient and profitable.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999931666666667">
We would like to thank the Fulton Supercomput-
ing Center at BYU for providing the computing re-
sources required for experiments reported here.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9655145">
ABBYY. 2010. ABBYY finereader.
http://finereader.abbyy.com.
</reference>
<page confidence="0.98275">
249
</page>
<reference confidence="0.9998485">
S. Agarwal, S. Godbole, D. Punjani, and Shourya Roy. 2007.
How much noise is too much: A study in automatic text clas-
sification. In Proceedings of the Seventh IEEE Intl. Conf. on
Data Mining (ICDM 2007), pages 3–12.
Steven M. Beitzel, Eric C. Jensen, and David A. Grossman.
2003. A survey of retrieval strategies for ocr text collec-
tions. In In Proceedings of the Symposium on Document
Image Understanding Technologies.
Michael W. Berry, Murray Brown, and Ben Signer. 2007. 2001
topic annotated Enron email data set.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd Intl. Conf. on Machine
Learning (ICML 2006).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022.
Horst Bunke. 2003. Recognition of cursive roman
handwriting- past, present and future. In 7th International
Conference on Document Analysis and Recognition (ICDAR
2003), volume 1, pages 448–459.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong
Wang, and David Blei. 2009. Reading tea leaves: How
humans interpret topic models. In Advances in Neural Infor-
mation Processing Systems 22, pages 288–296.
Faisal Farooq, Anurag Bhardwaj, and Venu Govindaraju. 2009.
Using topic models for OCR correction. Intl. Journal
on Document Analysis and Recognition (IJDAR), 12(3),
September.
Google, Inc. 2010. Tesseract.
http://code.google.com/p/tesseract-ocr.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and syntax.
In In Advances in Neural Information Processing Systems
17, pages 537–544. MIT Press.
Lawrence Hubert and Phipps Arabie. 1985. Comparing parti-
tions. Journal of Classification, 2(1):193–218, December.
David Reed Jordan. 1945. Daily battle communiques, 1944-
1945. Harold B. Lee Library, L. Tom Perry Special Collec-
tions, MSS 2766.
Ken Lang. 1995. NewsWeeder: Learning to filter netnews.
In Proceedings of the Twelfth International Conference on
Machine Learning, pages 331–339.
D. Lewis. 1997. Reuters-21578 text categorization test collec-
tion. http://www.research.att.com/˜lewis.
Tao Liu, Shengping Liu, Zheng Chen, and Wei-Ying Ma. 2003.
An evaluation on feature selection for text clustering. In Pro-
ceedings of the Twentieth Intl. Conf. on Machine Learning
(ICML 2003), August.
Daniel Lopresti. 2008. Optical character recognition errors and
their effects on natural language processing. In Proceedings
of the second workshop on Analytics for noisy unstructured
text data (AND 2008), pages 9–16.
William B. Lund and Eric. K Ringger. 2009. Improving op-
tical character recognition through efficient multiple system
alignment. In Proceedings of the Joint Conf. on Digital Li-
braries (JCDL’09), June.
Andrew Kachites McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Marina Meil˘a. 2007. Comparing clusterings—an informa-
tion based distance. Journal of Multivariate Analysis,
98(5):873–895.
David Mimno and Andrew Mccallum. 2007. Organizing the
OCA: learning faceted subjects from a library of digital
books. In Proceedings of the Joint Conf. on Digital Libraries
(JCDL’07), pages 376–385.
Cosmin Munteanu, Ronald Baecker, Gerald Penn, Elaine Toms,
and David James. 2006. The effect of speech recognition
accuracy rates on the usefulness and usability of webcast
archives. In Proceedings of the SIGCHI conference on Hu-
man Factors in computing systems, pages 493–502.
David J. Newmann and Sharon Block. 2006. Probabilistic topic
decomposition of an eighteenth-century american newspa-
per. J. Am. Soc. Inf. Sci. Technol., 57(6):753–767, February.
Nuance Communications, Inc. 2010. OmniPage Pro.
http://www.nuance.com/imaging/products/omnipage.asp.
Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A
conditional entropy-based external cluster evaluation mea-
sure. In Joint Conference on Empirical Methods in Natural
Language Processing and Computational Language Learn-
ing (EMNLP-CoNLL 2007).
Kazem Taghva, Julie Borsack, and Allen Condit. 1994. Results
of applying probabilistic ir to ocr text. In in Proc. 17th Intl.
ACM/SIGIR Conf. on Research and Development in Infor-
mation Retrieval, pages 202–211.
Kazem Taghva, Tom Nartker, Julie Borsack, Steve Lumos,
Allen Condit, and Ron Young. 2001. Evaluating text catego-
rization in the presence of ocr errors. In In Proc. IS&amp;T/SPIE
2001 Intl. Symp. on Electronic Imaging Science and Tech-
nology, pages 68–74. SPIE.
Daniel Walker and Eric Ringger. 2008. Model-based document
clustering with a collapsed gibbs sampler. In Proceedings of
the 14th ACM SIGKDD Intl. Conf. on Knowledge Discovery
and Data Mining (KDD 2008).
Daniel Walker and Erik K. Ringger. 2010. Top N per docu-
ment: Fast and effective unsupervised feature selection for
document clustering. Technical Report 6, Brigham Young
University. http://nlp.cs.byu.edu/techreports/BYUNLP-
TR6.pdf.
Hanna Wallach, David Mimno, and Andrew McCallum. 2009a.
Rethinking LDA: Why priors matter. In Advances in Neural
Information Processing Systems 22, pages 1973–1981.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009b. Evaluation methods for topic models.
In Proceedings of the 26th Annual Intl. Conf. on Machine
Learning (ICML 2009), pages 1105–1112.
Xuerui Wang and Andrew McCallum. 2006. Topics over time:
A non-markov continuous-time model of topical trends. In
Proceedings of the 12th ACM SIGKDD Intl. Conf. on Knowl-
edge Discovery and Data Mining (KDD 2006).
Michael L. Wick, Michael G. Ross, and Erik G. Learned-
Miller. 2007. Context-sensitive error correction: Using
topic models to improve OCR. In Proceedings of the Ninth
Intl. Conf. on Document Analysis and Recognition (ICDAR
2007), pages 1168–1172.
</reference>
<page confidence="0.997155">
250
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.572098">
<title confidence="0.9996145">Evaluating Models of Latent Document in the Presence of OCR Errors</title>
<author confidence="0.8831365">Daniel D Walker</author>
<author confidence="0.8831365">William B Lund</author>
<author confidence="0.8831365">Eric K Brigham Young</author>
<address confidence="0.79992">Provo, Utah,</address>
<email confidence="0.99762">danl4@cs.byu.edu,billlund@byu.edu,ringger@cs.byu.edu</email>
<abstract confidence="0.999651217391304">Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unprocessed OCR output in the case of LDA. To our knowledge, this study is the first of its kind.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ABBYY</author>
</authors>
<date>2010</date>
<note>ABBYY finereader. http://finereader.abbyy.com.</note>
<contexts>
<context position="7782" citStr="ABBYY, 2010" startWordPosition="1240" endWordPosition="1241">pervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily progress of the Allied campaign from D-Day until the German surrender. They were originally produced as telegrams and were distributed as mimeographed copies. The quality of the originals is often quite poor, making them a challenging case for OCR engines. The communiqu´es have been OCRed using three popular OCR engines: ABBYY FineReader (ABBYY, 2010), OmniPage Pro (Nuance Communications, Inc., 2010), and Tesseract (Google, Inc., 2010). In addition, the curator of the collection has created a “gold standard” transcription, from which it is possible to obtain accurate measures of average document word error rates (WER) for each engine, which are: 19.9%, 30.4%, and 50.1% respectively. 241 While the real-world data is attractive as an example of just the sort of data that the questions addressed here apply to, it is limited in size and scope. All of the documents in the Eisenhower corpus discuss the fairly narrow topic of troop movements and </context>
</contexts>
<marker>ABBYY, 2010</marker>
<rawString>ABBYY. 2010. ABBYY finereader. http://finereader.abbyy.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Agarwal</author>
<author>S Godbole</author>
<author>D Punjani</author>
<author>Shourya Roy</author>
</authors>
<title>How much noise is too much: A study in automatic text classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the Seventh IEEE Intl. Conf. on Data Mining (ICDM</booktitle>
<pages>3--12</pages>
<contexts>
<context position="6829" citStr="Agarwal et al., 2007" startWordPosition="1082" endWordPosition="1085">, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily </context>
</contexts>
<marker>Agarwal, Godbole, Punjani, Roy, 2007</marker>
<rawString>S. Agarwal, S. Godbole, D. Punjani, and Shourya Roy. 2007. How much noise is too much: A study in automatic text classification. In Proceedings of the Seventh IEEE Intl. Conf. on Data Mining (ICDM 2007), pages 3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven M Beitzel</author>
<author>Eric C Jensen</author>
<author>David A Grossman</author>
</authors>
<title>A survey of retrieval strategies for ocr text collections. In</title>
<date>2003</date>
<booktitle>In Proceedings of the Symposium on Document Image Understanding Technologies.</booktitle>
<contexts>
<context position="6896" citStr="Beitzel et al., 2003" startWordPosition="1092" endWordPosition="1095">itized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily progress of the Allied campaign from D-Day until the German surrend</context>
</contexts>
<marker>Beitzel, Jensen, Grossman, 2003</marker>
<rawString>Steven M. Beitzel, Eric C. Jensen, and David A. Grossman. 2003. A survey of retrieval strategies for ocr text collections. In In Proceedings of the Symposium on Document Image Understanding Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Murray Brown</author>
<author>Ben Signer</author>
</authors>
<title>topic annotated Enron email data set.</title>
<date>2007</date>
<contexts>
<context position="10916" citStr="Berry et al., 2007" startWordPosition="1777" endWordPosition="1780"> noise model sometimes replaced alphabet characters with punctuation characters, which were treated as delimiters by our tokenizer. Dataset |D |K # Types # Tokens 20 News 19997 20 107211 2261805 Reuters 11367 81 29034 747458 Enron 4935 32 60495 2063667 Eisenhower 604 N/A 8039 76674 Table 1: Summary of test dataset characteristics. |D |is the number of documents in the dataset. K is the number of human-labeled classes provided with the dataset. We chose three datasets to corrupt: 20 Newsgroups (Lang, 1995), Reuters 21578 (Lewis, 1997), and the LDC-annotated portion of the Enron e-mail archive (Berry et al., 2007). Each of these datasets were corrupted at values y = i*0.01 for i E (0,13). At this point, the word error rate of the corrupted data was near 50% and, since this was approximately the WER observed for the worst OCR engine on the real-world data, we chose to stop there. The word error rate was calculated during the corruption process. Here is an example sentence corrupted at two y values: y = 0.000 I am also attaching the RFP itself. y = 0.02 I am also attachEng the RFP itself. y = 0.10 I Jm alAo attaching the RFP itself. Table 3 shows some basic statistics for the datasets. The values shown a</context>
</contexts>
<marker>Berry, Brown, Signer, 2007</marker>
<rawString>Michael W. Berry, Murray Brown, and Ben Signer. 2007. 2001 topic annotated Enron email data set.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd Intl. Conf. on Machine Learning (ICML</booktitle>
<contexts>
<context position="6215" citStr="Blei and Lafferty, 2006" startWordPosition="981" endWordPosition="984">r an overview of related work in Section 2, Section 3 introduces the data used in our experiments, including an explanation of how the synthetic data were created and of some of their properties. Section 4 describes how the experiments were designed and carried out, and gives an analysis of the results both empirically and qualitatively. Finally, conclusions and future work are presented in Section 5. 2 Related Work Topic models have been used previously to process documents digitized by OCR, including eighteenthcentury American newspapers (Newmann and Block, 2006), OCRed editions of Science (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M. Blei and John D. Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd Intl. Conf. on Machine Learning (ICML 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2771" citStr="Blei et al., 2003" startWordPosition="428" endWordPosition="431">include the Google Books, Internet Archive, and HathiTrust projects. In addition, researchers are having increasing levels of success in digitizing hand-written manuscripts (Bunke, 2003), though error rates remain much higher than for OCR. Due to their nature, these collections often lack helpful meta-data or labels. In the absence of such labels, unsupervised machine learning methods can reveal patterns in the data. Finding good estimates for the parameters of models such as the mixture of multinomials document model (Walker and Ringger, 2008) and the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) requires accurate counts of the occurrences and co-occurrences of words. Depending on the age of a document and the way in which it was created, the OCR process results in text containing many types of noise, including character-level errors, which distort these counts. It is obvious, therefore, that model quality must suffer, especially since unsupervised methods are typically much more sensitive to noise than supervised methods. Good supervised learning algorithms are substantially more immune to spurious patterns in the data created by noise for the following reason: under the mostly reaso</context>
<context position="17772" citStr="Blei et al., 2003" startWordPosition="2981" endWordPosition="2984"> to those created through clustering. The metrics used were Variation of Information (VI) (Meil˘a, 2007), and the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). Other metrics were also calculated (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors during sampling as discussed by Wallach et al. (2009a). Each LDA model was trained on 90% of the documents in each dataset. The trained model was used to calculate an estimate of the marginal log-likelihood of the remaining 10% of the documents using the left-to-right algorithm (Wallach et al., 2009b). The number of topics used for each dataset was adjusted a priori according to the number of documents it contained. We used 100 topics for Enron and</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horst Bunke</author>
</authors>
<title>Recognition of cursive roman handwriting- past, present and future.</title>
<date>2003</date>
<booktitle>In 7th International Conference on Document Analysis and Recognition (ICDAR</booktitle>
<volume>1</volume>
<pages>448--459</pages>
<contexts>
<context position="2339" citStr="Bunke, 2003" startWordPosition="360" endWordPosition="361"> words in a document as belonging to a particular topic. This type of analysis has grown in popularity recently as inference on models containing large numbers of latent variables has become feasible. The modern explosion of data includes vast amounts of historical documents, made available by means of Optical Character Recognition (OCR), which can introduce significant numbers of errors. Undertakings to produce such data include the Google Books, Internet Archive, and HathiTrust projects. In addition, researchers are having increasing levels of success in digitizing hand-written manuscripts (Bunke, 2003), though error rates remain much higher than for OCR. Due to their nature, these collections often lack helpful meta-data or labels. In the absence of such labels, unsupervised machine learning methods can reveal patterns in the data. Finding good estimates for the parameters of models such as the mixture of multinomials document model (Walker and Ringger, 2008) and the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) requires accurate counts of the occurrences and co-occurrences of words. Depending on the age of a document and the way in which it was created, the OCR process result</context>
</contexts>
<marker>Bunke, 2003</marker>
<rawString>Horst Bunke. 2003. Recognition of cursive roman handwriting- past, present and future. In 7th International Conference on Document Analysis and Recognition (ICDAR 2003), volume 1, pages 448–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>288--296</pages>
<contexts>
<context position="27727" citStr="Chang et al., 2009" startWordPosition="4629" endWordPosition="4632"> measured with automated metrics such as log-likelihood on a held-out set and the cross-validation classification metric discussed here 246 0.5 0.4 0.3 0.2 0.1 0.00 10 20 30 40 50 Word Error Rate ari 20_newsgroups reuters21578 enron vi 4.5 4.0 3.5 3.0 2.5 2.00 10 20 30 40 50 Word Error Rate 20_newsgroups reuters21578 enron (a) Adjusted Rand Index results (b) Variation of Information results (lower is better) Figure 5: Results for the clustering experiments on the three synthetic OCR datasets with TNPD feature selection. do not necessarily indicate superior topics according to human judgement (Chang et al., 2009). In order to provide a more thorough discussion of the relative quality of the topic models induced on the OCR data versus those induced on clean data, we sampled the results of several of the runs of the LDA algorithm. In Tables 2 and 3 we show the top words for the five topics with the highest learned topic prior (α in the LDA literature) learned during Gibbs sampling. This information is shown for the Reuters data first with no corruption and then at the highest error rate for which we have results for that data of 45% WER. In general, there appears to be a surprisingly good correlation be</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In Advances in Neural Information Processing Systems 22, pages 288–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faisal Farooq</author>
<author>Anurag Bhardwaj</author>
<author>Venu Govindaraju</author>
</authors>
<title>Using topic models for OCR correction.</title>
<date>2009</date>
<journal>Intl. Journal on Document Analysis and Recognition (IJDAR),</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="6663" citStr="Farooq et al., 2009" startWordPosition="1055" endWordPosition="1058">iously to process documents digitized by OCR, including eighteenthcentury American newspapers (Newmann and Block, 2006), OCRed editions of Science (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-w</context>
</contexts>
<marker>Farooq, Bhardwaj, Govindaraju, 2009</marker>
<rawString>Faisal Farooq, Anurag Bhardwaj, and Venu Govindaraju. 2009. Using topic models for OCR correction. Intl. Journal on Document Analysis and Recognition (IJDAR), 12(3), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inc Google</author>
</authors>
<date>2010</date>
<note>Tesseract. http://code.google.com/p/tesseract-ocr.</note>
<marker>Google, 2010</marker>
<rawString>Google, Inc. 2010. Tesseract. http://code.google.com/p/tesseract-ocr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22007" citStr="Griffiths et al., 2005" startWordPosition="3700" endWordPosition="3703">et cannot be used to accurately compare the quality of topic models trained on pre-processed data, as the held-out data will contain many unknown words. If the held-out data is also pre-processed to only include known words, then the likelihood will be greater for those procedures that remove the most tokens, as the product that dominates the calculation will have fewer terms. If unknown words are allowed to remain, even a smoothed model will assign them very little probability and so models will be heavily penalized. We use an alternative method for evaluating the topic models, discussed in (Griffiths et al., 2005), to avoid the aforementioned problems with an evaluation based on log-likelihood. Since the synthetic data is derived from datasets that have topical document labels, we are able to use the output from LDA in a classification problem with the word vectors for each document being replaced by the assigned topic vectors. This is equivalent to using LDA as a dimensionality reduction pre-process for document classification. A naive Bayes learner is trained on a portion of the topic vectors, labeled with the original document label, and then the classification accuracy on a held-out portion of the </context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hubert</author>
<author>Phipps Arabie</author>
</authors>
<title>Comparing partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="17319" citStr="Hubert and Arabie, 1985" startWordPosition="2906" endWordPosition="2909">enhower data. mental methodology and present both empirical and qualitative analyses of the results. 4.1 Methodology For the synthetic OCR datasets, we ran clustering experiments using EM on a mixture of multinomials (c.f. (Walker and Ringger, 2008)). We specified the number of clusters to be the same as the number of classes provided with the data. Clusters were evaluated using several external cluster quality metrics which compare “gold standard” labels to those created through clustering. The metrics used were Variation of Information (VI) (Meil˘a, 2007), and the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). Other metrics were also calculated (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of Classification, 2(1):193–218, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Reed Jordan</author>
</authors>
<title>Daily battle communiques,</title>
<date>1945</date>
<journal>MSS</journal>
<pages>1944--1945</pages>
<contexts>
<context position="7369" citStr="Jordan, 1945" startWordPosition="1175" endWordPosition="1176">ised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily progress of the Allied campaign from D-Day until the German surrender. They were originally produced as telegrams and were distributed as mimeographed copies. The quality of the originals is often quite poor, making them a challenging case for OCR engines. The communiqu´es have been OCRed using three popular OCR engines: ABBYY FineReader (ABBYY, 2010), OmniPage Pro (Nuance Communications, Inc., 2010), and Tesseract (Google, Inc., 2010). In addition, the curator of the collection has created a “gold standard” transcription, from which </context>
</contexts>
<marker>Jordan, 1945</marker>
<rawString>David Reed Jordan. 1945. Daily battle communiques, 1944-1945. Harold B. Lee Library, L. Tom Perry Special Collections, MSS 2766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Lang</author>
</authors>
<title>NewsWeeder: Learning to filter netnews.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>331--339</pages>
<contexts>
<context position="10807" citStr="Lang, 1995" startWordPosition="1762" endWordPosition="1763">lation of word error rate. Segmentation errors can still occur in the learning stage, however, as the noise model sometimes replaced alphabet characters with punctuation characters, which were treated as delimiters by our tokenizer. Dataset |D |K # Types # Tokens 20 News 19997 20 107211 2261805 Reuters 11367 81 29034 747458 Enron 4935 32 60495 2063667 Eisenhower 604 N/A 8039 76674 Table 1: Summary of test dataset characteristics. |D |is the number of documents in the dataset. K is the number of human-labeled classes provided with the dataset. We chose three datasets to corrupt: 20 Newsgroups (Lang, 1995), Reuters 21578 (Lewis, 1997), and the LDC-annotated portion of the Enron e-mail archive (Berry et al., 2007). Each of these datasets were corrupted at values y = i*0.01 for i E (0,13). At this point, the word error rate of the corrupted data was near 50% and, since this was approximately the WER observed for the worst OCR engine on the real-world data, we chose to stop there. The word error rate was calculated during the corruption process. Here is an example sentence corrupted at two y values: y = 0.000 I am also attaching the RFP itself. y = 0.02 I am also attachEng the RFP itself. y = 0.10</context>
</contexts>
<marker>Lang, 1995</marker>
<rawString>Ken Lang. 1995. NewsWeeder: Learning to filter netnews. In Proceedings of the Twelfth International Conference on Machine Learning, pages 331–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<date>1997</date>
<note>Reuters-21578 text categorization test collection. http://www.research.att.com/˜lewis.</note>
<contexts>
<context position="10836" citStr="Lewis, 1997" startWordPosition="1766" endWordPosition="1767">egmentation errors can still occur in the learning stage, however, as the noise model sometimes replaced alphabet characters with punctuation characters, which were treated as delimiters by our tokenizer. Dataset |D |K # Types # Tokens 20 News 19997 20 107211 2261805 Reuters 11367 81 29034 747458 Enron 4935 32 60495 2063667 Eisenhower 604 N/A 8039 76674 Table 1: Summary of test dataset characteristics. |D |is the number of documents in the dataset. K is the number of human-labeled classes provided with the dataset. We chose three datasets to corrupt: 20 Newsgroups (Lang, 1995), Reuters 21578 (Lewis, 1997), and the LDC-annotated portion of the Enron e-mail archive (Berry et al., 2007). Each of these datasets were corrupted at values y = i*0.01 for i E (0,13). At this point, the word error rate of the corrupted data was near 50% and, since this was approximately the WER observed for the worst OCR engine on the real-world data, we chose to stop there. The word error rate was calculated during the corruption process. Here is an example sentence corrupted at two y values: y = 0.000 I am also attaching the RFP itself. y = 0.02 I am also attachEng the RFP itself. y = 0.10 I Jm alAo attaching the RFP </context>
</contexts>
<marker>Lewis, 1997</marker>
<rawString>D. Lewis. 1997. Reuters-21578 text categorization test collection. http://www.research.att.com/˜lewis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Liu</author>
<author>Shengping Liu</author>
<author>Zheng Chen</author>
<author>Wei-Ying Ma</author>
</authors>
<title>An evaluation on feature selection for text clustering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth Intl. Conf. on Machine Learning (ICML</booktitle>
<contexts>
<context position="17448" citStr="Liu et al., 2003" startWordPosition="2925" endWordPosition="2928">CR datasets, we ran clustering experiments using EM on a mixture of multinomials (c.f. (Walker and Ringger, 2008)). We specified the number of clusters to be the same as the number of classes provided with the data. Clusters were evaluated using several external cluster quality metrics which compare “gold standard” labels to those created through clustering. The metrics used were Variation of Information (VI) (Meil˘a, 2007), and the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). Other metrics were also calculated (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors during sampling as discussed by Wallach et al. (2009a). Each LDA model was trained on 90% of the documents in each dataset. The </context>
<context position="19003" citStr="Liu et al., 2003" startWordPosition="3186" endWordPosition="3189">50 for Reuters, and 200 for 20 Newsgroups. In addition to running experiments on the “raw” synthetic data, we also applied simple unsupervised feature selectors before training in order to evaluate the effectiveness of such measures in mitigating problems caused by OCR errors. For the topic modeling (LDA) experiments three feature selectors were used. The first method employed was a simple term frequency cutoff filter (TFCF), with a cutoff of 5 as in (Wang and McCallum, 2006). The next method employed was Term Contribution (TC), a feature selection algorithm developed for document clustering (Liu et al., 2003). Term contribution is parameterized by the number of word types that are to remain after selection. We attempted three values for this parameter, 10,000, 20,000, and 50,000. The final method we employed was a method called Top-N per Document (TNPD) (Walker and Ringger, 2010), which is a simple feature selection algorithm that first assigns each type in every document a document-specific score (e.g. its TF-IDF weight), and then selects words to include in the final vocabulary by choosing the N words with the highest score from each document in the corpus. We found that N = 1 gave the best resu</context>
</contexts>
<marker>Liu, Liu, Chen, Ma, 2003</marker>
<rawString>Tao Liu, Shengping Liu, Zheng Chen, and Wei-Ying Ma. 2003. An evaluation on feature selection for text clustering. In Proceedings of the Twentieth Intl. Conf. on Machine Learning (ICML 2003), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lopresti</author>
</authors>
<title>Optical character recognition errors and their effects on natural language processing.</title>
<date>2008</date>
<booktitle>In Proceedings of the</booktitle>
<pages>9--16</pages>
<contexts>
<context position="6974" citStr="Lopresti, 2008" startWordPosition="1107" endWordPosition="1108">s work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily progress of the Allied campaign from D-Day until the German surrender. They were originally produced as telegrams and were distributed as mimeogr</context>
</contexts>
<marker>Lopresti, 2008</marker>
<rawString>Daniel Lopresti. 2008. Optical character recognition errors and their effects on natural language processing. In Proceedings of the second workshop on Analytics for noisy unstructured text data (AND 2008), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ringger</author>
</authors>
<title>Improving optical character recognition through efficient multiple system alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conf. on Digital Libraries (JCDL’09),</booktitle>
<contexts>
<context position="7394" citStr="Ringger, 2009" startWordPosition="1179" endWordPosition="1180">ation (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily progress of the Allied campaign from D-Day until the German surrender. They were originally produced as telegrams and were distributed as mimeographed copies. The quality of the originals is often quite poor, making them a challenging case for OCR engines. The communiqu´es have been OCRed using three popular OCR engines: ABBYY FineReader (ABBYY, 2010), OmniPage Pro (Nuance Communications, Inc., 2010), and Tesseract (Google, Inc., 2010). In addition, the curator of the collection has created a “gold standard” transcription, from which it is possible to obtain </context>
</contexts>
<marker>Ringger, 2009</marker>
<rawString>William B. Lund and Eric. K Ringger. 2009. Improving optical character recognition through efficient multiple system alignment. In Proceedings of the Joint Conf. on Digital Libraries (JCDL’09), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="17875" citStr="McCallum, 2002" startWordPosition="2999" endWordPosition="3001">and the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). Other metrics were also calculated (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors during sampling as discussed by Wallach et al. (2009a). Each LDA model was trained on 90% of the documents in each dataset. The trained model was used to calculate an estimate of the marginal log-likelihood of the remaining 10% of the documents using the left-to-right algorithm (Wallach et al., 2009b). The number of topics used for each dataset was adjusted a priori according to the number of documents it contained. We used 100 topics for Enron and Eisenhower, 150 for Reuters, and 200 for 20 Newsgroups. In addition to running experiments on the “raw</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
</authors>
<title>Comparing clusterings—an information based distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<volume>98</volume>
<issue>5</issue>
<marker>Meil˘a, 2007</marker>
<rawString>Marina Meil˘a. 2007. Comparing clusterings—an information based distance. Journal of Multivariate Analysis, 98(5):873–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Andrew Mccallum</author>
</authors>
<title>Organizing the OCA: learning faceted subjects from a library of digital books.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conf. on Digital Libraries (JCDL’07),</booktitle>
<pages>376--385</pages>
<contexts>
<context position="6337" citStr="Mimno and Mccallum, 2007" startWordPosition="1000" endWordPosition="1003">on of how the synthetic data were created and of some of their properties. Section 4 describes how the experiments were designed and carried out, and gives an analysis of the results both empirically and qualitatively. Finally, conclusions and future work are presented in Section 5. 2 Related Work Topic models have been used previously to process documents digitized by OCR, including eighteenthcentury American newspapers (Newmann and Block, 2006), OCRed editions of Science (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural langu</context>
</contexts>
<marker>Mimno, Mccallum, 2007</marker>
<rawString>David Mimno and Andrew Mccallum. 2007. Organizing the OCA: learning faceted subjects from a library of digital books. In Proceedings of the Joint Conf. on Digital Libraries (JCDL’07), pages 376–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Munteanu</author>
<author>Ronald Baecker</author>
<author>Gerald Penn</author>
<author>Elaine Toms</author>
<author>David James</author>
</authors>
<title>The effect of speech recognition accuracy rates on the usefulness and usability of webcast archives.</title>
<date>2006</date>
<booktitle>In Proceedings of the SIGCHI conference on Human Factors in computing systems,</booktitle>
<pages>493--502</pages>
<contexts>
<context position="5019" citStr="Munteanu et al., 2006" startWordPosition="786" endWordPosition="789">modeling algorithms degrades as character-level noise is introduced. We demonstrate the effect using both artificially corrupted data and an existing real-world OCR corpus. The results are promising, especially in the case of relatively low word error rates (e.g. less than 20%). Though model quality declines as errors increase, simple feature selection techniques enable the learning of relatively high quality models even as word error rates approach 50%. This result is particularly interesting in that even humans find it difficult to make sense of documents with error rates of that magnitude (Munteanu et al., 2006). Because of the difficulties in evaluating topic models, even on clean data, these results should not be interpreted as definitive answers, but they do offer insight into prominent trends. For example, properties of the OCR data suggest measures that can be taken to improve performance in future work. It is our hope that this work will lead to an increase in the usefulness of collections of OCRed texts, as document clustering and topic modeling expose useful patterns to historians and other interested parties. The remainder of the paper is outlined as follows. After an overview of related wor</context>
</contexts>
<marker>Munteanu, Baecker, Penn, Toms, James, 2006</marker>
<rawString>Cosmin Munteanu, Ronald Baecker, Gerald Penn, Elaine Toms, and David James. 2006. The effect of speech recognition accuracy rates on the usefulness and usability of webcast archives. In Proceedings of the SIGCHI conference on Human Factors in computing systems, pages 493–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Newmann</author>
<author>Sharon Block</author>
</authors>
<title>Probabilistic topic decomposition of an eighteenth-century american newspaper.</title>
<date>2006</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>57</volume>
<issue>6</issue>
<contexts>
<context position="6162" citStr="Newmann and Block, 2006" startWordPosition="973" endWordPosition="976">e remainder of the paper is outlined as follows. After an overview of related work in Section 2, Section 3 introduces the data used in our experiments, including an explanation of how the synthetic data were created and of some of their properties. Section 4 describes how the experiments were designed and carried out, and gives an analysis of the results both empirically and qualitatively. Finally, conclusions and future work are presented in Section 5. 2 Related Work Topic models have been used previously to process documents digitized by OCR, including eighteenthcentury American newspapers (Newmann and Block, 2006), OCRed editions of Science (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised </context>
</contexts>
<marker>Newmann, Block, 2006</marker>
<rawString>David J. Newmann and Sharon Block. 2006. Probabilistic topic decomposition of an eighteenth-century american newspaper. J. Am. Soc. Inf. Sci. Technol., 57(6):753–767, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuance Communications</author>
<author>Inc</author>
</authors>
<date>2010</date>
<note>OmniPage Pro. http://www.nuance.com/imaging/products/omnipage.asp.</note>
<marker>Communications, Inc, 2010</marker>
<rawString>Nuance Communications, Inc. 2010. OmniPage Pro. http://www.nuance.com/imaging/products/omnipage.asp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning</booktitle>
<contexts>
<context position="17408" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2918" endWordPosition="2921">es of the results. 4.1 Methodology For the synthetic OCR datasets, we ran clustering experiments using EM on a mixture of multinomials (c.f. (Walker and Ringger, 2008)). We specified the number of clusters to be the same as the number of classes provided with the data. Clusters were evaluated using several external cluster quality metrics which compare “gold standard” labels to those created through clustering. The metrics used were Variation of Information (VI) (Meil˘a, 2007), and the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). Other metrics were also calculated (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors during sampling as discussed by Wallach et al. (2009a). Each LDA model was trained on 90</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning (EMNLP-CoNLL 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazem Taghva</author>
<author>Julie Borsack</author>
<author>Allen Condit</author>
</authors>
<title>Results of applying probabilistic ir to ocr text.</title>
<date>1994</date>
<booktitle>In in Proc. 17th Intl. ACM/SIGIR Conf. on Research and Development in Information Retrieval,</booktitle>
<pages>202--211</pages>
<contexts>
<context position="6873" citStr="Taghva et al., 1994" startWordPosition="1088" endWordPosition="1091"> 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These documents relate the daily progress of the Allied campaign from D-Day u</context>
</contexts>
<marker>Taghva, Borsack, Condit, 1994</marker>
<rawString>Kazem Taghva, Julie Borsack, and Allen Condit. 1994. Results of applying probabilistic ir to ocr text. In in Proc. 17th Intl. ACM/SIGIR Conf. on Research and Development in Information Retrieval, pages 202–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazem Taghva</author>
<author>Tom Nartker</author>
<author>Julie Borsack</author>
<author>Steve Lumos</author>
<author>Allen Condit</author>
<author>Ron Young</author>
</authors>
<title>Evaluating text categorization in the presence of ocr errors.</title>
<date>2001</date>
<booktitle>In In Proc. IS&amp;T/SPIE 2001 Intl. Symp. on Electronic Imaging Science and Technology,</booktitle>
<pages>68--74</pages>
<publisher>SPIE.</publisher>
<contexts>
<context position="6806" citStr="Taghva et al., 2001" startWordPosition="1078" endWordPosition="1081">ce (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real OCR data. As a real-world dataset, we used a corpus consisting of 604 of the Eisenhower World War II communiqu´es (Jordan, 1945; Lund and Ringger, 2009). These docu</context>
</contexts>
<marker>Taghva, Nartker, Borsack, Lumos, Condit, Young, 2001</marker>
<rawString>Kazem Taghva, Tom Nartker, Julie Borsack, Steve Lumos, Allen Condit, and Ron Young. 2001. Evaluating text categorization in the presence of ocr errors. In In Proc. IS&amp;T/SPIE 2001 Intl. Symp. on Electronic Imaging Science and Technology, pages 68–74. SPIE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Walker</author>
<author>Eric Ringger</author>
</authors>
<title>Model-based document clustering with a collapsed gibbs sampler.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD</booktitle>
<contexts>
<context position="2703" citStr="Walker and Ringger, 2008" startWordPosition="416" endWordPosition="419">introduce significant numbers of errors. Undertakings to produce such data include the Google Books, Internet Archive, and HathiTrust projects. In addition, researchers are having increasing levels of success in digitizing hand-written manuscripts (Bunke, 2003), though error rates remain much higher than for OCR. Due to their nature, these collections often lack helpful meta-data or labels. In the absence of such labels, unsupervised machine learning methods can reveal patterns in the data. Finding good estimates for the parameters of models such as the mixture of multinomials document model (Walker and Ringger, 2008) and the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) requires accurate counts of the occurrences and co-occurrences of words. Depending on the age of a document and the way in which it was created, the OCR process results in text containing many types of noise, including character-level errors, which distort these counts. It is obvious, therefore, that model quality must suffer, especially since unsupervised methods are typically much more sensitive to noise than supervised methods. Good supervised learning algorithms are substantially more immune to spurious patterns in the da</context>
<context position="16944" citStr="Walker and Ringger, 2008" startWordPosition="2846" endWordPosition="2849">details of the errors are less relevant than the word distribution alterations for the models in question. 4 Experimental Results We ran experiments on both the real and synthetic OCR data. In this section we explain our experi800000 243 Figure 2: The number of word types culled with frequency cutoff filters applied to the transcript and three OCR engine outputs for the Eisenhower data. mental methodology and present both empirical and qualitative analyses of the results. 4.1 Methodology For the synthetic OCR datasets, we ran clustering experiments using EM on a mixture of multinomials (c.f. (Walker and Ringger, 2008)). We specified the number of clusters to be the same as the number of classes provided with the data. Clusters were evaluated using several external cluster quality metrics which compare “gold standard” labels to those created through clustering. The metrics used were Variation of Information (VI) (Meil˘a, 2007), and the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). Other metrics were also calculated (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are si</context>
</contexts>
<marker>Walker, Ringger, 2008</marker>
<rawString>Daniel Walker and Eric Ringger. 2008. Model-based document clustering with a collapsed gibbs sampler. In Proceedings of the 14th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Walker</author>
<author>Erik K Ringger</author>
</authors>
<title>Top N per document: Fast and effective unsupervised feature selection for document clustering.</title>
<date>2010</date>
<tech>Technical Report 6,</tech>
<institution>Brigham Young University.</institution>
<note>http://nlp.cs.byu.edu/techreports/BYUNLPTR6.pdf.</note>
<contexts>
<context position="19279" citStr="Walker and Ringger, 2010" startWordPosition="3231" endWordPosition="3235">R errors. For the topic modeling (LDA) experiments three feature selectors were used. The first method employed was a simple term frequency cutoff filter (TFCF), with a cutoff of 5 as in (Wang and McCallum, 2006). The next method employed was Term Contribution (TC), a feature selection algorithm developed for document clustering (Liu et al., 2003). Term contribution is parameterized by the number of word types that are to remain after selection. We attempted three values for this parameter, 10,000, 20,000, and 50,000. The final method we employed was a method called Top-N per Document (TNPD) (Walker and Ringger, 2010), which is a simple feature selection algorithm that first assigns each type in every document a document-specific score (e.g. its TF-IDF weight), and then selects words to include in the final vocabulary by choosing the N words with the highest score from each document in the corpus. We found that N = 1 gave the best results at the greatest reduction in word types. After the vocabulary is built, all words not in the vocabulary are culled from the documents. This does not mean that all documents contain only one word after feature selection, as the top word in one document may occur in many ot</context>
</contexts>
<marker>Walker, Ringger, 2010</marker>
<rawString>Daniel Walker and Erik K. Ringger. 2010. Top N per document: Fast and effective unsupervised feature selection for document clustering. Technical Report 6, Brigham Young University. http://nlp.cs.byu.edu/techreports/BYUNLPTR6.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>1973--1981</pages>
<contexts>
<context position="17972" citStr="Wallach et al. (2009" startWordPosition="3016" endWordPosition="3019">ted (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors during sampling as discussed by Wallach et al. (2009a). Each LDA model was trained on 90% of the documents in each dataset. The trained model was used to calculate an estimate of the marginal log-likelihood of the remaining 10% of the documents using the left-to-right algorithm (Wallach et al., 2009b). The number of topics used for each dataset was adjusted a priori according to the number of documents it contained. We used 100 topics for Enron and Eisenhower, 150 for Reuters, and 200 for 20 Newsgroups. In addition to running experiments on the “raw” synthetic data, we also applied simple unsupervised feature selectors before training in order </context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna Wallach, David Mimno, and Andrew McCallum. 2009a. Rethinking LDA: Why priors matter. In Advances in Neural Information Processing Systems 22, pages 1973–1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual Intl. Conf. on Machine Learning (ICML</booktitle>
<pages>1105--1112</pages>
<contexts>
<context position="17972" citStr="Wallach et al. (2009" startWordPosition="3016" endWordPosition="3019">ted (e.g. the V-Measure (Rosenberg and Hirschberg, 2007), and Average Entropy (Liu et al., 2003)), but these results were excluded due to space constraints and the fact that their plots are similar to those shown. We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models (Blei et al., 2003) using Gibbs sampling. We used the implementation found in the MALLET software package (McCallum, 2002) with the option enabled to learn the priors during sampling as discussed by Wallach et al. (2009a). Each LDA model was trained on 90% of the documents in each dataset. The trained model was used to calculate an estimate of the marginal log-likelihood of the remaining 10% of the documents using the left-to-right algorithm (Wallach et al., 2009b). The number of topics used for each dataset was adjusted a priori according to the number of documents it contained. We used 100 topics for Enron and Eisenhower, 150 for Reuters, and 200 for 20 Newsgroups. In addition to running experiments on the “raw” synthetic data, we also applied simple unsupervised feature selectors before training in order </context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009b. Evaluation methods for topic models. In Proceedings of the 26th Annual Intl. Conf. on Machine Learning (ICML 2009), pages 1105–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
</authors>
<title>Topics over time: A non-markov continuous-time model of topical trends.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD</booktitle>
<contexts>
<context position="6260" citStr="Wang and McCallum, 2006" startWordPosition="988" endWordPosition="991">ection 3 introduces the data used in our experiments, including an explanation of how the synthetic data were created and of some of their properties. Section 4 describes how the experiments were designed and carried out, and gives an analysis of the results both empirically and qualitatively. Finally, conclusions and future work are presented in Section 5. 2 Related Work Topic models have been used previously to process documents digitized by OCR, including eighteenthcentury American newspapers (Newmann and Block, 2006), OCRed editions of Science (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva</context>
<context position="18866" citStr="Wang and McCallum, 2006" startWordPosition="3166" endWordPosition="3169">cs used for each dataset was adjusted a priori according to the number of documents it contained. We used 100 topics for Enron and Eisenhower, 150 for Reuters, and 200 for 20 Newsgroups. In addition to running experiments on the “raw” synthetic data, we also applied simple unsupervised feature selectors before training in order to evaluate the effectiveness of such measures in mitigating problems caused by OCR errors. For the topic modeling (LDA) experiments three feature selectors were used. The first method employed was a simple term frequency cutoff filter (TFCF), with a cutoff of 5 as in (Wang and McCallum, 2006). The next method employed was Term Contribution (TC), a feature selection algorithm developed for document clustering (Liu et al., 2003). Term contribution is parameterized by the number of word types that are to remain after selection. We attempted three values for this parameter, 10,000, 20,000, and 50,000. The final method we employed was a method called Top-N per Document (TNPD) (Walker and Ringger, 2010), which is a simple feature selection algorithm that first assigns each type in every document a document-specific score (e.g. its TF-IDF weight), and then selects words to include in the</context>
</contexts>
<marker>Wang, McCallum, 2006</marker>
<rawString>Xuerui Wang and Andrew McCallum. 2006. Topics over time: A non-markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Wick</author>
<author>Michael G Ross</author>
<author>Erik G LearnedMiller</author>
</authors>
<title>Context-sensitive error correction: Using topic models to improve OCR.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth Intl. Conf. on Document Analysis and Recognition (ICDAR</booktitle>
<pages>1168--1172</pages>
<contexts>
<context position="6641" citStr="Wick et al., 2007" startWordPosition="1051" endWordPosition="1054">have been used previously to process documents digitized by OCR, including eighteenthcentury American newspapers (Newmann and Block, 2006), OCRed editions of Science (Blei and Lafferty, 2006), OCRed NIPS papers (Wang and McCallum, 2006), and books digitized by the Open Content Alliance (Mimno and Mccallum, 2007). Most of this previous work ignores the presence of OCR errors or attempts to remove corrupted tokens with special pre-processing such as stop-word removal and frequency cutoffs. Also, there are at least two instances of using topic modeling to improve the results of an OCR algorithm (Wick et al., 2007; Farooq et al., 2009). Similar evaluations to ours have been conducted to assess the effect of OCR errors on supervised document classification (Taghva et al., 2001; Agarwal et al., 2007), information retrieval (Taghva et al., 1994; Beitzel et al., 2003), and a more general set of natural language processing tasks (Lopresti, 2008). Results suggest that in these supervised tasks OCR errors have a minimal impact on the performance of the methods employed, though it has remained unclear how well these results transfer to unsupervised methods. 3 Data We conducted experiments on synthetic and real</context>
</contexts>
<marker>Wick, Ross, LearnedMiller, 2007</marker>
<rawString>Michael L. Wick, Michael G. Ross, and Erik G. LearnedMiller. 2007. Context-sensitive error correction: Using topic models to improve OCR. In Proceedings of the Ninth Intl. Conf. on Document Analysis and Recognition (ICDAR 2007), pages 1168–1172.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>