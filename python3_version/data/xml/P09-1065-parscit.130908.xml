<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.993786">
Joint Decoding with Multiple Translation Models
</title>
<author confidence="0.995882">
Yang Liu and Haitao Mi and Yang Feng and Qun Liu
</author>
<affiliation confidence="0.987189666666667">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.859857">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.999421">
{yliu,htmi,fengyang,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997946875">
Current SMT systems usually decode with
single translation models and cannot ben-
efit from the strengths of other models in
decoding phase. We instead propose joint
decoding, a method that combines multi-
ple translation models in one decoder. Our
joint decoder draws connections among
multiple models by integrating the trans-
lation hypergraphs they produce individu-
ally. Therefore, one model can share trans-
lations and even derivations with other
models. Comparable to the state-of-the-art
system combination technique, joint de-
coding achieves an absolute improvement
of 1.5 BLEU points over individual decod-
ing.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942807692307">
System combination aims to find consensus trans-
lations among different machine translation sys-
tems. It proves that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
Recent several years have witnessed the rapid
development of system combination methods
based on confusion networks (e.g., (Rosti et al.,
2007; He et al., 2008)), which show state-of-the-
art performance in MT benchmarks. A confusion
network consists of a sequence of sets of candidate
words. Each candidate word is associated with a
score. The optimal consensus translation can be
obtained by selecting one word from each set of
candidates to maximizing the overall score. While
it is easy and efficient to manipulate strings, cur-
rent methods usually have no access to most infor-
mation available in decoding phase, which might
be useful for obtaining further improvements.
In this paper, we propose a framework for com-
bining multiple translation models directly in de-
coding phase. 1 Based on max-translation decod-
ing and max-derivation decoding used in conven-
tional individual decoders (Section 2), we go fur-
ther to develop a joint decoder that integrates mul-
tiple models on a firm basis:
</bodyText>
<listItem confidence="0.92034735">
• Structuring the search space of each model
as a translation hypergraph (Section 3.1),
our joint decoder packs individual translation
hypergraphs together by merging nodes that
have identical partial translations (Section
3.2). Although such translation-level combi-
nation will not produce new translations, it
does change the way of selecting promising
candidates.
• Two models could even share derivations
with each other if they produce the same
structures on the target side (Section 3.3),
which we refer to as derivation-level com-
bination. This method enlarges the search
space by allowing for mixing different types
of translation rules within one derivation.
• As multiple derivations are used for finding
optimal translations, we extend the minimum
error rate training (MERT) algorithm (Och,
2003) to tune feature weights with respect
</listItem>
<bodyText confidence="0.975087714285714">
to BLEU score for max-translation decoding
(Section 4).
We evaluated our joint decoder that integrated
a hierarchical phrase-based model (Chiang, 2005;
Chiang, 2007) and a tree-to-string model (Liu et
al., 2006) on the NIST 2005 Chinese-English test-
set. Experimental results show that joint decod-
</bodyText>
<footnote confidence="0.711517888888889">
1It might be controversial to use the term “model”, which
usually has a very precise definition in the field. Some
researchers prefer to saying “phrase-based approaches” or
“phrase-based systems”. On the other hand, other authors
(e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007))
do use the expression “phrase-based models”. In this paper,
we use the term “model” to emphasize that we integrate dif-
ferent approaches directly in decoding phase rather than post-
processing system outputs.
</footnote>
<page confidence="0.917838">
576
</page>
<note confidence="0.999634">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.423474">
S (X1, X1)
X (fabiao X1, give a X1)
X (yanjiang, talk)
</equation>
<figureCaption confidence="0.770912333333333">
Figure 1: A derivation composed of SCFG rules
that translates a Chinese sentence “fabiao yan-
jiang” into an English sentence “give a talk”.
</figureCaption>
<bodyText confidence="0.953336">
ing with multiple models achieves an absolute im-
provement of 1.5 BLEU points over individual de-
coding with single models (Section 5).
</bodyText>
<sectionHeader confidence="0.996925" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999505416666667">
Statistical machine translation is a decision prob-
lem where we need decide on the best of target
sentence matching a source sentence. The process
of searching for the best translation is convention-
ally called decoding, which usually involves se-
quences of decisions that translate a source sen-
tence into a target sentence step by step.
For example, Figure 1 shows a sequence of
SCFG rules (Chiang, 2005; Chiang, 2007) that
translates a Chinese sentence “fabiao yanjiang”
into an English sentence “give a talk”. Such se-
quence of decisions is called a derivation. In
phrase-based models, a decision can be translating
a source phrase into a target phrase or reordering
the target phrases. In syntax-based models, deci-
sions usually correspond to transduction rules. Of-
ten, there are many derivations that are distinct yet
produce the same translation.
Blunsom et al. (2008) present a latent vari-
able model that describes the relationship between
translation and derivation clearly. Given a source
sentence f, the probability of a target sentence e
being its translation is the sum over all possible
derivations:
</bodyText>
<equation confidence="0.9970785">
Pr(e|f) = E Pr(d, e|f) (1)
d∈0(e,f)
</equation>
<bodyText confidence="0.992329">
where A(e, f) is the set of all possible derivations
that translate f into e and d is one such derivation.
They use a log-linear model to define the con-
ditional probability of a derivation d and corre-
sponding translation e conditioned on a source
sentence f:
</bodyText>
<equation confidence="0.938648333333333">
exp em Amhm(d, e, f)
Pr(d, e|f) = (2)
Z(f)
</equation>
<bodyText confidence="0.997528333333333">
where hm is a feature function, Am is the asso-
ciated feature weight, and Z(f) is a constant for
normalization:
</bodyText>
<equation confidence="0.9981725">
Z(f) = E E Eexp Amhm(d, e, f) (3)
e d∈0(e,f) m
</equation>
<bodyText confidence="0.9934">
A feature value is usually decomposed as the
product of decision probabilities: 2
</bodyText>
<equation confidence="0.914735">
h(d, e, f) = � p(d) (4)
d∈d
</equation>
<bodyText confidence="0.997196625">
where d is a decision in the derivation d.
Although originally proposed for supporting
large sets of non-independent and overlapping fea-
tures, the latent variable model is actually a more
general form of conventional linear model (Och
and Ney, 2002).
Accordingly, decoding for the latent variable
model can be formalized as
</bodyText>
<equation confidence="0.999403333333333">
e� = argmax
e {E exp E Amhm(d, e, f) } (5)
d∈0(e,f) m 1
</equation>
<bodyText confidence="0.998382">
where Z(f) is not needed in decoding because it
is independent of e.
Most SMT systems approximate the summa-
tion over all possible derivations by using 1-best
derivation for efficiency. They search for the 1-
best derivation and take its target yield as the best
translation:
</bodyText>
<equation confidence="0.997791666666667">
e� argmax
e,d { E }
m Amhm(d, e, f) (6)
</equation>
<bodyText confidence="0.999861">
We refer to Eq. (5) as max-translation decoding
and Eq. (6) as max-derivation decoding, which are
first termed by Blunsom et al. (2008).
By now, most current SMT systems, adopting
either max-derivation decoding or max-translation
decoding, have only used single models in decod-
ing phase. We refer to them as individual de-
coders. In the following section, we will present
a new method called joint decoding that includes
multiple models in one decoder.
</bodyText>
<sectionHeader confidence="0.987339" genericHeader="method">
3 Joint Decoding
</sectionHeader>
<bodyText confidence="0.996197333333333">
There are two major challenges for combining
multiple models directly in decoding phase. First,
they rely on different kinds of knowledge sources
</bodyText>
<footnote confidence="0.52649">
2There are also features independent of derivations, such
as language model and word penalty.
</footnote>
<page confidence="0.995182">
577
</page>
<figureCaption confidence="0.98450275">
Figure 2: (a) A translation hypergraph produced by one model; (b) a translation hypergraph produced by
another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote
the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating
that the two models produce the same translations.
</figureCaption>
<figure confidence="0.998397454545455">
give a talk
0-2
give talks
0-2
give a talk
0-2
make a speech
0-2
give
0-1
talk
1-2
give
0-1
speech
1-2
S
S
(a) packing (b)
give a talk
0-2
give talks
0-2
make a speech
0-2
give
0-1
talk
1-2
speech
1-2
S
(c)
</figure>
<bodyText confidence="0.997886033333333">
and thus need to collect different information dur-
ing decoding. For example, taking a source parse
as input, a tree-to-string decoder (e.g., (Liu et al.,
2006)) pattern-matches the source parse with tree-
to-string rules and produces a string on the tar-
get side. On the contrary, a string-to-tree decoder
(e.g., (Galley et al., 2006; Shen et al., 2008)) is a
parser that applies string-to-tree rules to obtain a
target parse for the source string. As a result, the
hypothesis structures of the two models are funda-
mentally different.
Second, translation models differ in decoding
algorithms. Depending on the generating order
of a target sentence, we distinguish between two
major categories: left-to-right and bottom-up. De-
coders that use rules with flat structures (e.g.,
phrase pairs) usually generate target sentences
from left to right while those using rules with hier-
archical structures (e.g., SCFG rules) often run in
a bottom-up style.
In response to the two challenges, we first ar-
gue that the search space of an arbitrary model can
be structured as a translation hypergraph, which
makes each model connectable to others (Section
3.1). Then, we show that a packed translation hy-
pergraph that integrates the hypergraphs of indi-
vidual models can be generated in a bottom-up
topological order, either integrated at the transla-
tion level (Section 3.2) or the derivation level (Sec-
tion 3.3).
</bodyText>
<subsectionHeader confidence="0.999329">
3.1 Translation Hypergraph
</subsectionHeader>
<bodyText confidence="0.98819525">
Despite the diversity of translation models, they all
have to produce partial translations for substrings
of input sentences. Therefore, we represent the
search space of a translation model as a structure
called translation hypergraph.
Figure 2(a) demonstrates a translation hyper-
graph for one model, for example, a hierarchical
phrase-based model. A node in a hypergraph de-
notes a partial translation for a source substring,
except for the starting node “S”. For example,
given the example source sentence
0 fabiao 1 yanjiang 2
the node (“give talks”, [0, 2]) in Figure 2(a) de-
notes that “give talks” is one translation of the
source string f21 = “fabiao yanjiang”.
The hyperedges between nodes denote the deci-
sion steps that produce head nodes from tail nodes.
For example, the incoming hyperedge of the node
(“give talks”, [0, 2]) could correspond to an SCFG
rule:
</bodyText>
<equation confidence="0.964861">
X —* (X1 yanjiang, X1 talks)
</equation>
<bodyText confidence="0.99893575">
Each hyperedge is associated with a number of
weights, which are the feature values of the corre-
sponding translation rules. A path of hyperedges
constitutes a derivation.
</bodyText>
<page confidence="0.980671">
578
</page>
<table confidence="0.724216">
Hypergraph Decoding
node translation
hyperedge rule
path derivation
</table>
<tableCaption confidence="0.983546">
Table 1: Correspondence between translation hy-
</tableCaption>
<bodyText confidence="0.986378275862069">
pergraph and decoding.
More formally, a hypergraph (Klein and Man-
ning., 2001; Huang and Chiang, 2005) is a tuple
(V, E, R), where V is a set of nodes, E is a set
of hyperedges, and R is a set of weights. For a
given source sentence f = f&apos;1 = f1 ... f,,,, each
node v E V is in the form of (t, [i, j]), which de-
notes the recognition of t as one translation of the
source substring spanning from i through j (that
is, fi+1 ... fj). Each hyperedge e E E is a tuple
e = (tails(e), head(e), w(e)), where head(e) E
V is the consequent node in the deductive step,
tails(e) E V ∗ is the list of antecedent nodes, and
w(e) is a weight function from R|tails(e) |to R.
As a general representation, a translation hyper-
graph is capable of characterizing the search space
of an arbitrary translation model. Furthermore,
it offers a graphic interpretation of decoding pro-
cess. A node in a hypergraph denotes a translation,
a hyperedge denotes a decision step, and a path
of hyperedges denotes a derivation. A translation
hypergraph is formally a semiring as the weight
of a path is the product of hyperedge weights and
the weight of a node is the sum of path weights.
While max-derivation decoding only retains the
single best path at each node, max-translation de-
coding sums up all incoming paths. Table 1 sum-
marizes the relationship between translation hy-
pergraph and decoding.
</bodyText>
<subsectionHeader confidence="0.994775">
3.2 Translation-Level Combination
</subsectionHeader>
<bodyText confidence="0.9999742">
The conventional interpretation of Eq. (1) is that
the probability of a translation is the sum over all
possible derivations coming from the same model.
Alternatively, we interpret Eq. (1) as that the
derivations could come from different models.3
This forms the theoretical basis of joint decoding.
Although the information inside a derivation
differs widely among translation models, the be-
ginning and end points (i.e., f and e, respectively)
must be identical. For example, a tree-to-string
</bodyText>
<footnote confidence="0.989251">
3The same for all d occurrences in Section 2. For exam-
ple, O(e, f) might include derivations from various models
now. Note that we still use Z for normalization.
</footnote>
<bodyText confidence="0.997838444444444">
model first parses f to obtain a source tree T(f)
and then transforms T(f) to the target sentence
e. Conversely, a string-to-tree model first parses
f into a target tree T(e) and then takes the surface
string e as the translation. Despite different inside,
their derivations must begin with f and end with e.
This situation remains the same for derivations
between a source substring fji and its partial trans-
lation t during joint decoding:
</bodyText>
<equation confidence="0.880568">
Pr(t fj i ) = � Pr(d, t fji ) (7)
d∈0(t,fi )
</equation>
<bodyText confidence="0.999885487179487">
where d might come from multiple models. In
other words, derivations from multiple models
could be brought together for computing the prob-
ability of one partial translation.
Graphically speaking, joint decoding creates a
packed translation hypergraph that combines in-
dividual hypergraphs by merging nodes that have
identical translations. For example, Figure 2 (a)
and (b) demonstrate two translation hypergraphs
generated by two models respectively and Fig-
ure 2 (c) is the resulting packed hypergraph. The
solid lines denote the hyperedges of the first model
and the dashed lines denote those of the second
model. The shaded nodes are shared by both mod-
els. Therefore, the two models are combined at the
translation level. Intuitively, shared nodes should
be favored in decoding because they offer consen-
sus translations among different models.
Now the question is how to decode with multi-
ple models jointly in just one decoder. We believe
that both left-to-right and bottom-up strategies can
be used for joint decoding. Although phrase-based
decoders usually produce translations from left to
right, they can adopt bottom-up decoding in prin-
ciple. Xiong et al. (2006) develop a bottom-up de-
coder for BTG (Wu, 1997) that uses only phrase
pairs. They treat reordering of phrases as a binary
classification problem. On the other hand, it is
possible for syntax-based models to decode from
left to right. Watanabe et al. (2006) propose left-
to-right target generation for hierarchical phrase-
based translation. Although left-to-right decod-
ing might enable a more efficient use of language
models and hopefully produce better translations,
we adopt bottom-up decoding in this paper just for
convenience.
Figure 3 demonstrates the search algorithm of
our joint decoder. The input is a source language
sentence f&apos;1 , and a set of translation models M
</bodyText>
<page confidence="0.98736">
579
</page>
<listItem confidence="0.964311636363636">
1: procedure JOINTDECODING(fi , M)
2: G + -0
3: for l +- 1 ... n do
4: for all i, j s.t. j − i = l do
5: for all m E M do
6: ADD(G, i, j, m)
7: end for
8: PRUNE(G, i, j)
9: end for
10: end for
11: end procedure
</listItem>
<figureCaption confidence="0.997766">
Figure 3: Search algorithm for joint decoding.
</figureCaption>
<bodyText confidence="0.998868217391304">
(line 1). After initializing the translation hyper-
graph G (line 2), the decoder runs in a bottom-
up style, adding nodes for each span [i, j] and for
each model m. For each span [i, j] (lines 3-5),
the procedure ADD(G, i, j, m) add nodes gener-
ated by the model m to the hypergraph G (line 6).
Each model searches for partial translations inde-
pendently: it uses its own knowledge sources and
visits its own antecedent nodes, just running like
a bottom-up individual decoder. After all mod-
els finishes adding nodes for span [i, j], the pro-
cedure PRUNE(G, i, j) merges identical nodes and
removes less promising nodes to control the search
space (line 8). The pruning strategy is similar to
that of individual decoders, except that we require
there must exist at least one node for each model
to ensure further inference.
Although translation-level combination will not
offer new translations as compared to single mod-
els, it changes the way of selecting promising can-
didates in a combined search space and might po-
tentially produce better translations than individ-
ual decoding.
</bodyText>
<subsectionHeader confidence="0.972578">
3.3 Derivation-Level Combination
</subsectionHeader>
<bodyText confidence="0.9999145">
In translation-level combination, different models
interact with each other only at the nodes. The
derivations of one model are unaccessible to other
models. However, if two models produce the same
structures on the target side, it is possible to com-
bine two models within one derivation, which we
refer to as derivation-level combination.
For example, although different on the source
side, both hierarchical phrase-based and tree-to-
string models produce strings of terminals and
nonterminals on the target side. Figure 4 shows
a derivation composed of both hierarchical phrase
</bodyText>
<equation confidence="0.995216">
IP(x1:VV, x2:NN) x1 x2
X (fabiao, give)
X (yanjiang, a talk)
</equation>
<figureCaption confidence="0.997668">
Figure 4: A derivation composed of both SCFG
and tree-to-string rules.
</figureCaption>
<bodyText confidence="0.99971709375">
pairs and tree-to-string rules. Hierarchical phrase
pairs are used for translating smaller units and
tree-to-string rules for bigger ones. It is appealing
to combine them in such a way because the hierar-
chical phrase-based model provides excellent rule
coverage while the tree-to-string model offers lin-
guistically motivated non-local reordering. Sim-
ilarly, Blunsom and Osborne (2008) use both hi-
erarchical phrase pairs and tree-to-string rules in
decoding, where source parse trees serve as condi-
tioning context rather than hard constraints.
Depending on the target side output, we dis-
tinguish between string-targeted and tree-targeted
models. String-targeted models include phrase-
based, hierarchical phrase-based, and tree-to-
string models. Tree-targeted models include
string-to-tree and tree-to-tree models. All models
can be combined at the translation level. Models
that share with same target output structure can be
further combined at the derivation level.
The joint decoder usually runs as max-
translation decoding because multiple derivations
from various models are used. However, if all
models involved belong to the same category, a
joint decoder can also adopt the max-derivation
fashion because all nodes and hyperedges are ac-
cessible now (Section 5.2).
Allowing derivations for comprising rules from
different models and integrating their strengths,
derivation-level combination could hopefully pro-
duce new and better translations as compared with
single models.
</bodyText>
<sectionHeader confidence="0.9678905" genericHeader="method">
4 Extended Minimum Error Rate
Training
</sectionHeader>
<bodyText confidence="0.9999566">
Minimum error rate training (Och, 2003) is widely
used to optimize feature weights for a linear model
(Och and Ney, 2002). The key idea of MERT is
to tune one feature weight to minimize error rate
each time while keep others fixed. Therefore, each
</bodyText>
<page confidence="0.997042">
580
</page>
<figureCaption confidence="0.999813">
Figure 5: Calculation of critical intersections.
</figureCaption>
<bodyText confidence="0.558371">
candidate translation can be represented as a line:
</bodyText>
<equation confidence="0.869978">
f(x) = a × x + b (8)
</equation>
<bodyText confidence="0.9999748">
where a is the feature value of current dimension,
x is the feature weight being tuned, and b is the
dotproduct of other dimensions. The intersection
of two lines is where the candidate translation will
change. Instead of computing all intersections,
Och (2003) only computes critical intersections
where highest-score translations will change. This
method reduces the computational overhead sig-
nificantly.
Unfortunately, minimum error rate training can-
not be directly used to optimize feature weights of
max-translation decoding because Eq. (5) is not a
linear model. However, if we also tune one dimen-
sion each time and keep other dimensions fixed,
we obtain a monotonic curve as follows:
</bodyText>
<equation confidence="0.995240333333333">
K
f(x) = E eak×x+bk (9)
k=1
</equation>
<bodyText confidence="0.999980823529412">
where K is the number of derivations for a can-
didate translation, ak is the feature value of cur-
rent dimension on the kth derivation and bk is the
dotproduct of other dimensions on the kth deriva-
tion. If we restrict that ak is always non-negative,
the curve shown in Eq. (9) will be a monotoni-
cally increasing function. Therefore, it is possible
to extend the MERT algorithm to handle situations
where multiple derivations are taken into account
for decoding.
The key difference is the calculation of criti-
cal intersections. The major challenge is that two
curves might have multiple intersections while
two lines have at most one intersection. Fortu-
nately, as the curve is monotonically increasing,
we need only to find the leftmost intersection of
a curve with other curves that have greater values
after the intersection as a candidate critical inter-
section.
Figure 5 demonstrates three curves: t1, t2, and
t3. Suppose that the left bound of x is 0, we com-
pute the function values for t1, t2, and t3 at x = 0
and find that t3 has the greatest value. As a result,
we choose x = 0 as the first critical intersection.
Then, we compute the leftmost intersections of t3
with t1 and t2 and choose the intersection closest
to x = 0, that is x1, as our new critical intersec-
tion. Similarly, we start from x1 and find x2 as the
next critical intersection. This iteration continues
until it reaches the right bound. The bold curve de-
notes the translations we will choose over different
ranges. For example, we will always choose t2 for
the range [x1, x2].
To compute the leftmost intersection of two
curves, we divide the range from current critical
intersection to the right bound into many bins (i.e.,
smaller ranges) and search the bins one by one
from left to right. We assume that there is at most
one intersection in each bin. As a result, we can
use the Bisection method for finding the intersec-
tion in each bin. The search process ends immedi-
ately once an intersection is found.
We divide max-translation decoding into three
phases: (1) build the translation hypergraphs, (2)
generate n-best translations, and (3) generate n′-
best derivations. We apply Algorithm 3 of Huang
and Chiang (2005) for n-best list generation. Ex-
tended MERT runs on n-best translations plus n′-
best derivations to optimize the feature weights.
Note that feature weights of various models are
tuned jointly in extended MERT.
</bodyText>
<sectionHeader confidence="0.99983" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999344">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999846416666667">
Our experiments were on Chinese-to-English
translation. We used the FBIS corpus (6.9M +
8.9M words) as the training corpus. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT Evaluation test
set as our development set, and used the NIST
2005 test set as test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al., 2002).
Our joint decoder included two models. The
</bodyText>
<equation confidence="0.952481166666667">
t1
t2
t3
f(x)
(0, 0) x1 x2
x
</equation>
<page confidence="0.991895">
581
</page>
<table confidence="0.999713833333333">
Model Combination Max-derivation Max-translation
Time BLEU Time BLEU
hierarchical N/A 40.53 30.11 44.87 29.82
tree-to-string N/A 6.13 27.23 6.69 27.11
both translation N/A N/A 55.89 30.79
derivation 48.45 31.63 54.91 31.49
</table>
<tableCaption confidence="0.9706555">
Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec-
onds/sentence) and BLEU score (case-insensitive).
</tableCaption>
<bodyText confidence="0.999751176470588">
first model was the hierarchical phrase-based
model (Chiang, 2005; Chiang, 2007). We obtained
word alignments of training data by first running
GIZA++ (Och and Ney, 2003) and then applying
the refinement rule “grow-diag-final-and” (Koehn
et al., 2003). About 2.6M hierarchical phrase pairs
extracted from the training corpus were used on
the test set.
Another model was the tree-to-string model
(Liu et al., 2006; Liu et al., 2007). Based on
the same word-aligned training corpus, we ran a
Chinese parser on the source side to obtain 1-best
parses. For 15,157 sentences we failed to obtain
1-best parses. Therefore, only 93.7% of the train-
ing corpus were used by the tree-to-string model.
About 578K tree-to-string rules extracted from the
training corpus were used on the test set.
</bodyText>
<subsectionHeader confidence="0.996599">
5.2 Individual Decoding Vs. Joint Decoding
</subsectionHeader>
<bodyText confidence="0.9998105">
Table 2 shows the results of comparing individ-
ual decoding and joint decoding on the test set.
With conventional max-derivation decoding, the
hierarchical phrase-based model achieved a BLEU
score of 30.11 on the test set, with an average de-
coding time of 40.53 seconds/sentence. We found
that accounting for all possible derivations in max-
translation decoding resulted in a small negative
effect on BLEU score (from 30.11 to 29.82), even
though the feature weights were tuned with respect
to BLEU score. One possible reason is that we
only used n-best derivations instead of all possi-
ble derivations for minimum error rate training.
Max-derivation decoding with the tree-to-string
model yielded much lower BLEU score (i.e.,
27.23) than the hierarchical phrase-based model.
One reason is that the tree-to-string model fails
to capture a large amount of linguistically unmo-
tivated mappings due to syntactic constraints. An-
other reason is that the tree-to-string model only
used part of the training data because of pars-
ing failure. Similarly, accounting for all possible
</bodyText>
<figure confidence="0.996261846153846">
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0 1 2 3 4 5 6 7 8 9 10 11
span width
</figure>
<figureCaption confidence="0.7143175">
Figure 6: Node sharing in max-translation de-
coding with varying span widths. We retain at
most 100 nodes for each source substring for each
model.
</figureCaption>
<bodyText confidence="0.99238324">
derivations in max-translation decoding failed to
bring benefits for the tree-to-string model (from
27.23 to 27.11).
When combining the two models at the trans-
lation level, the joint decoder achieved a BLEU
score of 30.79 that outperformed the best result
(i.e., 30.11) of individual decoding significantly
(p &lt; 0.05). This suggests that accounting for
all possible derivations from multiple models will
help discriminate among candidate translations.
Figure 6 demonstrates the percentages of nodes
shared by the two models over various span widths
in packed translation hypergraphs during max-
translation decoding. For one-word source strings,
89.33% nodes in the hypergrpah were shared by
both models. With the increase of span width, the
percentage decreased dramatically due to the di-
versity of the two models. However, there still ex-
ist nodes shared by two models even for source
substrings that contain 33 words.
When combining the two models at the deriva-
tion level using max-derivation decoding, the joint
decoder achieved a BLEU score of 31.63 that out-
performed the best result (i.e., 30.11) of individ-
percentage
</bodyText>
<page confidence="0.981393">
582
</page>
<table confidence="0.994594">
Method Model BLEU
hierarchical 30.11
individual decoding
tree-to-string 27.23
system combination both 31.50
joint decoding both 31.63
</table>
<tableCaption confidence="0.8396655">
Table 3: Comparison of individual decoding, sys-
tem combination, and joint decoding.
</tableCaption>
<bodyText confidence="0.999923416666667">
ual decoding significantly (p &lt; 0.01). This im-
provement resulted from the mixture of hierarchi-
cal phrase pairs and tree-to-string rules. To pro-
duce the result, the joint decoder made use of
8,114 hierarchical phrase pairs learned from train-
ing data, 6,800 glue rules connecting partial trans-
lations monotonically, and 16,554 tree-to-string
rules. While tree-to-string rules offer linguistically
motivated non-local reordering during decoding,
hierarchical phrase pairs ensure good rule cover-
age. Max-translation decoding still failed to sur-
pass max-derivation decoding in this case.
</bodyText>
<subsectionHeader confidence="0.999757">
5.3 Comparison with System Combination
</subsectionHeader>
<bodyText confidence="0.999971428571429">
We re-implemented a state-of-the-art system com-
bination method (Rosti et al., 2007). As shown
in Table 3, taking the translations of the two indi-
vidual decoders as input, the system combination
method achieved a BLEU score of 31.50, slightly
lower than that of joint decoding. But this differ-
ence is not significant statistically.
</bodyText>
<subsectionHeader confidence="0.935808">
5.4 Individual Training Vs. Joint Training
</subsectionHeader>
<bodyText confidence="0.999843444444445">
Table 4 shows the effects of individual training and
joint training. By individual, we mean that the two
models are trained independently. We concatenate
and normalize their feature weights for the joint
decoder. By joint, we mean that they are trained
together by the extended MERT algorithm. We
found that joint training outperformed individual
training significantly for both max-derivation de-
coding and max-translation decoding.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99731275">
System combination has benefited various NLP
tasks in recent years, such as products-of-experts
(e.g., (Smith and Eisner, 2005)) and ensemble-
based parsing (e.g., (Henderson and Brill, 1999)).
In machine translation, confusion-network based
combination techniques (e.g., (Rosti et al., 2007;
He et al., 2008)) have achieved the state-of-the-
art performance in MT evaluations. From a dif-
</bodyText>
<table confidence="0.949032333333333">
Training Max-derivation Max-translation
individual 30.70 29.95
joint 31.63 30.79
</table>
<tableCaption confidence="0.952021">
Table 4: Comparison of individual training and
joint training.
</tableCaption>
<bodyText confidence="0.999790692307692">
ferent perspective, we try to combine different ap-
proaches directly in decoding phase by using hy-
pergraphs. While system combination techniques
manipulate only the final translations of each sys-
tem, our method opens the possibility of exploit-
ing much more information.
Blunsom et al. (2008) first distinguish between
max-derivation decoding and max-translation de-
coding explicitly. They show that max-translation
decoding outperforms max-derivation decoding
for the latent variable model. While they train the
parameters using a maximum a posteriori estima-
tor, we extend the MERT algorithm (Och, 2003)
to take the evaluation metric into account.
Hypergraphs have been successfully used in
parsing (Klein and Manning., 2001; Huang and
Chiang, 2005; Huang, 2008) and machine trans-
lation (Huang and Chiang, 2007; Mi et al., 2008;
Mi and Huang, 2008). Both Mi et al. (2008) and
Blunsom et al. (2008) use a translation hyper-
graph to represent search space. The difference is
that their hypergraphs are specifically designed for
the forest-based tree-to-string model and the hier-
archical phrase-based model, respectively, while
ours is more general and can be applied to arbi-
trary models.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999950076923077">
We have presented a framework for including mul-
tiple translation models in one decoder. Repre-
senting search space as a translation hypergraph,
individual models are accessible to others via shar-
ing nodes and even hyperedges. As our decoder
accounts for multiple derivations, we extend the
MERT algorithm to tune feature weights with re-
spect to BLEU score for max-translation decod-
ing. In the future, we plan to optimize feature
weights for max-translation decoding directly on
the entire packed translation hypergraph rather
than on n-best derivations, following the lattice-
based MERT (Macherey et al., 2008).
</bodyText>
<page confidence="0.998284">
583
</page>
<sectionHeader confidence="0.992201" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9906377">
The authors were supported by National Natural
Science Foundation of China, Contracts 60873167
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done while
Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Yajuan L¨u, Liang Huang, Nguyen
Bach, Andreas Zollmann, Vamshi Ambati, and
Kevin Gimpel for their helpful feedback.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99928191954023">
Phil Blunsom and Mile Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP08.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ofACL08.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ofACL05.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ofANLP94.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ofACL06.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proc. of
EMNLP08.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proc. ofEMNLP99.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. ofIWPT05.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. ofACL07.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ofACL08.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proc. ofACL08.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL03.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ofACL06.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. ofACL07.
Wolfgang Macherey, Franz J. Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proc. of EMNLP08.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. ofEMNLP08.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ofACL08.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ofACL02.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ofACL03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. ofACL02.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proc. ofACL07.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ofACL08.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. ofACL05.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proc. ofICSLP02.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. ofACL06.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. ofACL06.
</reference>
<page confidence="0.998589">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721899">
<title confidence="0.994517">Joint Decoding with Multiple Translation Models</title>
<author confidence="0.991783">Liu Mi Feng Liu</author>
<affiliation confidence="0.948926666666667">Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.994625">P.O. Box 2704, Beijing 100190, China</address>
<abstract confidence="0.991427176470588">Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in phase. We instead propose a method that combines multiple translation models in one decoder. Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually. Therefore, one model can share translations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Mile Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP08.</booktitle>
<contexts>
<context position="17432" citStr="Blunsom and Osborne (2008)" startWordPosition="2861" endWordPosition="2864">rminals and nonterminals on the target side. Figure 4 shows a derivation composed of both hierarchical phrase IP(x1:VV, x2:NN) x1 x2 X (fabiao, give) X (yanjiang, a talk) Figure 4: A derivation composed of both SCFG and tree-to-string rules. pairs and tree-to-string rules. Hierarchical phrase pairs are used for translating smaller units and tree-to-string rules for bigger ones. It is appealing to combine them in such a way because the hierarchical phrase-based model provides excellent rule coverage while the tree-to-string model offers linguistically motivated non-local reordering. Similarly, Blunsom and Osborne (2008) use both hierarchical phrase pairs and tree-to-string rules in decoding, where source parse trees serve as conditioning context rather than hard constraints. Depending on the target side output, we distinguish between string-targeted and tree-targeted models. String-targeted models include phrasebased, hierarchical phrase-based, and tree-tostring models. Tree-targeted models include string-to-tree and tree-to-tree models. All models can be combined at the translation level. Models that share with same target output structure can be further combined at the derivation level. The joint decoder u</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Phil Blunsom and Mile Osborne. 2008. Probabilistic inference for machine translation. In Proc. of EMNLP08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL08.</booktitle>
<contexts>
<context position="5150" citStr="Blunsom et al. (2008)" startWordPosition="796" endWordPosition="799">ces of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence f, the probability of a target sentence e being its translation is the sum over all possible derivations: Pr(e|f) = E Pr(d, e|f) (1) d∈0(e,f) where A(e, f) is the set of all possible derivations that translate f into e and d is one such derivation. They use a log-linear model to define the conditional probability of a derivation d and corresponding translation e conditioned on a source sentence f: exp em Amhm(d, e, f) Pr(d, e|f) = (2) Z(f) where hm is a feature f</context>
<context position="6825" citStr="Blunsom et al. (2008)" startWordPosition="1096" endWordPosition="1099">ntional linear model (Och and Ney, 2002). Accordingly, decoding for the latent variable model can be formalized as e� = argmax e {E exp E Amhm(d, e, f) } (5) d∈0(e,f) m 1 where Z(f) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency. They search for the 1- best derivation and take its target yield as the best translation: e� argmax e,d { E } m Amhm(d, e, f) (6) We refer to Eq. (5) as max-translation decoding and Eq. (6) as max-derivation decoding, which are first termed by Blunsom et al. (2008). By now, most current SMT systems, adopting either max-derivation decoding or max-translation decoding, have only used single models in decoding phase. We refer to them as individual decoders. In the following section, we will present a new method called joint decoding that includes multiple models in one decoder. 3 Joint Decoding There are two major challenges for combining multiple models directly in decoding phase. First, they rely on different kinds of knowledge sources 2There are also features independent of derivations, such as language model and word penalty. 577 Figure 2: (a) A transl</context>
<context position="28743" citStr="Blunsom et al. (2008)" startWordPosition="4658" endWordPosition="4661">chine translation, confusion-network based combination techniques (e.g., (Rosti et al., 2007; He et al., 2008)) have achieved the state-of-theart performance in MT evaluations. From a difTraining Max-derivation Max-translation individual 30.70 29.95 joint 31.63 30.79 Table 4: Comparison of individual training and joint training. ferent perspective, we try to combine different approaches directly in decoding phase by using hypergraphs. While system combination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. ofACL08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL05.</booktitle>
<contexts>
<context position="3137" citStr="Chiang, 2005" startWordPosition="470" endWordPosition="471">uld even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase</context>
<context position="4682" citStr="Chiang, 2005" startWordPosition="725" endWordPosition="726">tence “fabiao yanjiang” into an English sentence “give a talk”. ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5). 2 Background Statistical machine translation is a decision problem where we need decide on the best of target sentence matching a source sentence. The process of searching for the best translation is conventionally called decoding, which usually involves sequences of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence</context>
<context position="10684" citStr="Chiang, 2005" startWordPosition="1725" endWordPosition="1726">eredges between nodes denote the decision steps that produce head nodes from tail nodes. For example, the incoming hyperedge of the node (“give talks”, [0, 2]) could correspond to an SCFG rule: X —* (X1 yanjiang, X1 talks) Each hyperedge is associated with a number of weights, which are the feature values of the corresponding translation rules. A path of hyperedges constitutes a derivation. 578 Hypergraph Decoding node translation hyperedge rule path derivation Table 1: Correspondence between translation hypergraph and decoding. More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple (V, E, R), where V is a set of nodes, E is a set of hyperedges, and R is a set of weights. For a given source sentence f = f&apos;1 = f1 ... f,,,, each node v E V is in the form of (t, [i, j]), which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 ... fj). Each hyperedge e E E is a tuple e = (tails(e), head(e), w(e)), where head(e) E V is the consequent node in the deductive step, tails(e) E V ∗ is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translation </context>
<context position="21851" citStr="Chiang (2005)" startWordPosition="3588" endWordPosition="3589">ction of two curves, we divide the range from current critical intersection to the right bound into many bins (i.e., smaller ranges) and search the bins one by one from left to right. We assume that there is at most one intersection in each bin. As a result, we can use the Bisection method for finding the intersection in each bin. The search process ends immediately once an intersection is found. We divide max-translation decoding into three phases: (1) build the translation hypergraphs, (2) generate n-best translations, and (3) generate n′- best derivations. We apply Algorithm 3 of Huang and Chiang (2005) for n-best list generation. Extended MERT runs on n-best translations plus n′- best derivations to optimize the feature weights. Note that feature weights of various models are tuned jointly in extended MERT. 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST </context>
<context position="23080" citStr="Chiang, 2005" startWordPosition="3785" endWordPosition="3786">test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The t1 t2 t3 f(x) (0, 0) x1 x2 x 581 Model Combination Max-derivation Max-translation Time BLEU Time BLEU hierarchical N/A 40.53 30.11 44.87 29.82 tree-to-string N/A 6.13 27.23 6.69 27.11 both translation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diag-final-and” (Koehn et al., 2003). About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set. Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007). Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses. For 15,157 sentences we failed to obtain 1-best parses. Therefore, only 93.7% of the training corpus were used by th</context>
<context position="29199" citStr="Chiang, 2005" startWordPosition="4724" endWordPosition="4725">echniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individ</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3152" citStr="Chiang, 2007" startWordPosition="472" endWordPosition="473"> derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than po</context>
<context position="4697" citStr="Chiang, 2007" startWordPosition="727" endWordPosition="728">yanjiang” into an English sentence “give a talk”. ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5). 2 Background Statistical machine translation is a decision problem where we need decide on the best of target sentence matching a source sentence. The process of searching for the best translation is conventionally called decoding, which usually involves sequences of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence f, the probabi</context>
<context position="23095" citStr="Chiang, 2007" startWordPosition="3787" endWordPosition="3788">valuated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The t1 t2 t3 f(x) (0, 0) x1 x2 x 581 Model Combination Max-derivation Max-translation Time BLEU Time BLEU hierarchical N/A 40.53 30.11 44.87 29.82 tree-to-string N/A 6.13 27.23 6.69 27.11 both translation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diag-final-and” (Koehn et al., 2003). About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set. Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007). Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses. For 15,157 sentences we failed to obtain 1-best parses. Therefore, only 93.7% of the training corpus were used by the tree-to-strin</context>
<context position="29261" citStr="Chiang, 2007" startWordPosition="4734" endWordPosition="4735">m, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proc. ofANLP94.</booktitle>
<contexts>
<context position="1154" citStr="Frederking and Nirenburg, 1994" startWordPosition="162" endWordPosition="165"> one decoder. Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually. Therefore, one model can share translations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in </context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proc. ofANLP94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. ofACL06.</booktitle>
<contexts>
<context position="8305" citStr="Galley et al., 2006" startWordPosition="1345" endWordPosition="1348">n both (a) and (b), indicating that the two models produce the same translations. give a talk 0-2 give talks 0-2 give a talk 0-2 make a speech 0-2 give 0-1 talk 1-2 give 0-1 speech 1-2 S S (a) packing (b) give a talk 0-2 give talks 0-2 make a speech 0-2 give 0-1 talk 1-2 speech 1-2 S (c) and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. Second, translation models differ in decoding algorithms. Depending on the generating order of a target sentence, we distinguish between two major categories: left-to-right and bottom-up. Decoders that use rules with flat structures (e.g., phrase pairs) usually generate target sentences from left to right while those using rules with hierarchical structures (e.g., SCFG rules) often run in a bott</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. ofACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMMbased hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP08.</booktitle>
<contexts>
<context position="1316" citStr="He et al., 2008" startWordPosition="187" endWordPosition="190">slations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly</context>
<context position="28232" citStr="He et al., 2008" startWordPosition="4583" endWordPosition="4586">We concatenate and normalize their feature weights for the joint decoder. By joint, we mean that they are trained together by the extended MERT algorithm. We found that joint training outperformed individual training significantly for both max-derivation decoding and max-translation decoding. 6 Related Work System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemblebased parsing (e.g., (Henderson and Brill, 1999)). In machine translation, confusion-network based combination techniques (e.g., (Rosti et al., 2007; He et al., 2008)) have achieved the state-of-theart performance in MT evaluations. From a difTraining Max-derivation Max-translation individual 30.70 29.95 joint 31.63 30.79 Table 4: Comparison of individual training and joint training. ferent perspective, we try to combine different approaches directly in decoding phase by using hypergraphs. While system combination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitl</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMMbased hypothesis alignment for combining outputs from machine translation systems. In Proc. of EMNLP08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proc. ofEMNLP99.</booktitle>
<contexts>
<context position="28114" citStr="Henderson and Brill, 1999" startWordPosition="4567" endWordPosition="4570">ws the effects of individual training and joint training. By individual, we mean that the two models are trained independently. We concatenate and normalize their feature weights for the joint decoder. By joint, we mean that they are trained together by the extended MERT algorithm. We found that joint training outperformed individual training significantly for both max-derivation decoding and max-translation decoding. 6 Related Work System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemblebased parsing (e.g., (Henderson and Brill, 1999)). In machine translation, confusion-network based combination techniques (e.g., (Rosti et al., 2007; He et al., 2008)) have achieved the state-of-theart performance in MT evaluations. From a difTraining Max-derivation Max-translation individual 30.70 29.95 joint 31.63 30.79 Table 4: Comparison of individual training and joint training. ferent perspective, we try to combine different approaches directly in decoding phase by using hypergraphs. While system combination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more infor</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proc. ofEMNLP99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. ofIWPT05.</booktitle>
<contexts>
<context position="10684" citStr="Huang and Chiang, 2005" startWordPosition="1723" endWordPosition="1726">”. The hyperedges between nodes denote the decision steps that produce head nodes from tail nodes. For example, the incoming hyperedge of the node (“give talks”, [0, 2]) could correspond to an SCFG rule: X —* (X1 yanjiang, X1 talks) Each hyperedge is associated with a number of weights, which are the feature values of the corresponding translation rules. A path of hyperedges constitutes a derivation. 578 Hypergraph Decoding node translation hyperedge rule path derivation Table 1: Correspondence between translation hypergraph and decoding. More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple (V, E, R), where V is a set of nodes, E is a set of hyperedges, and R is a set of weights. For a given source sentence f = f&apos;1 = f1 ... f,,,, each node v E V is in the form of (t, [i, j]), which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 ... fj). Each hyperedge e E E is a tuple e = (tails(e), head(e), w(e)), where head(e) E V is the consequent node in the deductive step, tails(e) E V ∗ is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translation </context>
<context position="21851" citStr="Huang and Chiang (2005)" startWordPosition="3586" endWordPosition="3589">st intersection of two curves, we divide the range from current critical intersection to the right bound into many bins (i.e., smaller ranges) and search the bins one by one from left to right. We assume that there is at most one intersection in each bin. As a result, we can use the Bisection method for finding the intersection in each bin. The search process ends immediately once an intersection is found. We divide max-translation decoding into three phases: (1) build the translation hypergraphs, (2) generate n-best translations, and (3) generate n′- best derivations. We apply Algorithm 3 of Huang and Chiang (2005) for n-best list generation. Extended MERT runs on n-best translations plus n′- best derivations to optimize the feature weights. Note that feature weights of various models are tuned jointly in extended MERT. 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST </context>
<context position="29199" citStr="Huang and Chiang, 2005" startWordPosition="4722" endWordPosition="4725">bination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individ</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. ofIWPT05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. ofACL07.</booktitle>
<contexts>
<context position="29261" citStr="Huang and Chiang, 2007" startWordPosition="4732" endWordPosition="4735">each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. ofACL07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ofACL08.</booktitle>
<contexts>
<context position="29213" citStr="Huang, 2008" startWordPosition="4726" endWordPosition="4727">pulate only the final translations of each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individual models are</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ofACL08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proc. ofACL08.</booktitle>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proc. ofACL08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL03.</booktitle>
<contexts>
<context position="3558" citStr="Koehn et al., 2003" startWordPosition="535" endWordPosition="538">ch, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP S (X1, X1) X (fabiao X1, give a X1) X (yanjiang, talk) Figure 1: A derivation composed of SCFG rules that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. ing with multiple models</context>
<context position="23266" citStr="Koehn et al., 2003" startWordPosition="3811" endWordPosition="3814">581 Model Combination Max-derivation Max-translation Time BLEU Time BLEU hierarchical N/A 40.53 30.11 44.87 29.82 tree-to-string N/A 6.13 27.23 6.69 27.11 both translation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diag-final-and” (Koehn et al., 2003). About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set. Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007). Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses. For 15,157 sentences we failed to obtain 1-best parses. Therefore, only 93.7% of the training corpus were used by the tree-to-string model. About 578K tree-to-string rules extracted from the training corpus were used on the test set. 5.2 Individual Decoding Vs. Joint Decoding Table 2 shows the results</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ofACL06.</booktitle>
<contexts>
<context position="3198" citStr="Liu et al., 2006" startWordPosition="478" endWordPosition="481">ce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings o</context>
<context position="8134" citStr="Liu et al., 2006" startWordPosition="1317" endWordPosition="1320">el; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. give a talk 0-2 give talks 0-2 give a talk 0-2 make a speech 0-2 give 0-1 talk 1-2 give 0-1 speech 1-2 S S (a) packing (b) give a talk 0-2 give talks 0-2 make a speech 0-2 give 0-1 talk 1-2 speech 1-2 S (c) and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. Second, translation models differ in decoding algorithms. Depending on the generating order of a target sentence, we distinguish between two major categories: left-to-right and bottom-up. Decoders that use rules with flat struc</context>
<context position="23427" citStr="Liu et al., 2006" startWordPosition="3837" endWordPosition="3840">slation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diag-final-and” (Koehn et al., 2003). About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set. Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007). Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses. For 15,157 sentences we failed to obtain 1-best parses. Therefore, only 93.7% of the training corpus were used by the tree-to-string model. About 578K tree-to-string rules extracted from the training corpus were used on the test set. 5.2 Individual Decoding Vs. Joint Decoding Table 2 shows the results of comparing individual decoding and joint decoding on the test set. With conventional max-derivation decoding, the hierarchical phrase-based model achieved a B</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. ofACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proc. ofACL07.</booktitle>
<contexts>
<context position="23446" citStr="Liu et al., 2007" startWordPosition="3841" endWordPosition="3844">.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diag-final-and” (Koehn et al., 2003). About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set. Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007). Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses. For 15,157 sentences we failed to obtain 1-best parses. Therefore, only 93.7% of the training corpus were used by the tree-to-string model. About 578K tree-to-string rules extracted from the training corpus were used on the test set. 5.2 Individual Decoding Vs. Joint Decoding Table 2 shows the results of comparing individual decoding and joint decoding on the test set. With conventional max-derivation decoding, the hierarchical phrase-based model achieved a BLEU score of 30.11 </context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proc. ofACL07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz J Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP08.</booktitle>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz J. Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proc. of EMNLP08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proc. ofEMNLP08.</booktitle>
<contexts>
<context position="29299" citStr="Mi and Huang, 2008" startWordPosition="4740" endWordPosition="4743">lity of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even hyperedges. As our decoder accounts f</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. ofEMNLP08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL08.</booktitle>
<contexts>
<context position="29278" citStr="Mi et al., 2008" startWordPosition="4736" endWordPosition="4739">opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We have presented a framework for including multiple translation models in one decoder. Representing search space as a translation hypergraph, individual models are accessible to others via sharing nodes and even hyperedges. As o</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. ofACL08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL02.</booktitle>
<contexts>
<context position="6244" citStr="Och and Ney, 2002" startWordPosition="990" endWordPosition="993">ponding translation e conditioned on a source sentence f: exp em Amhm(d, e, f) Pr(d, e|f) = (2) Z(f) where hm is a feature function, Am is the associated feature weight, and Z(f) is a constant for normalization: Z(f) = E E Eexp Amhm(d, e, f) (3) e d∈0(e,f) m A feature value is usually decomposed as the product of decision probabilities: 2 h(d, e, f) = � p(d) (4) d∈d where d is a decision in the derivation d. Although originally proposed for supporting large sets of non-independent and overlapping features, the latent variable model is actually a more general form of conventional linear model (Och and Ney, 2002). Accordingly, decoding for the latent variable model can be formalized as e� = argmax e {E exp E Amhm(d, e, f) } (5) d∈0(e,f) m 1 where Z(f) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency. They search for the 1- best derivation and take its target yield as the best translation: e� argmax e,d { E } m Amhm(d, e, f) (6) We refer to Eq. (5) as max-translation decoding and Eq. (6) as max-derivation decoding, which are first termed by Blunsom et al. (2008). By now, most curr</context>
<context position="18686" citStr="Och and Ney, 2002" startWordPosition="3043" endWordPosition="3046">ng because multiple derivations from various models are used. However, if all models involved belong to the same category, a joint decoder can also adopt the max-derivation fashion because all nodes and hyperedges are accessible now (Section 5.2). Allowing derivations for comprising rules from different models and integrating their strengths, derivation-level combination could hopefully produce new and better translations as compared with single models. 4 Extended Minimum Error Rate Training Minimum error rate training (Och, 2003) is widely used to optimize feature weights for a linear model (Och and Ney, 2002). The key idea of MERT is to tune one feature weight to minimize error rate each time while keep others fixed. Therefore, each 580 Figure 5: Calculation of critical intersections. candidate translation can be represented as a line: f(x) = a × x + b (8) where a is the feature value of current dimension, x is the feature weight being tuned, and b is the dotproduct of other dimensions. The intersection of two lines is where the candidate translation will change. Instead of computing all intersections, Och (2003) only computes critical intersections where highest-score translations will change. Th</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ofACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23185" citStr="Och and Ney, 2003" startWordPosition="3800" endWordPosition="3803"> 2002). Our joint decoder included two models. The t1 t2 t3 f(x) (0, 0) x1 x2 x 581 Model Combination Max-derivation Max-translation Time BLEU Time BLEU hierarchical N/A 40.53 30.11 44.87 29.82 tree-to-string N/A 6.13 27.23 6.69 27.11 both translation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diag-final-and” (Koehn et al., 2003). About 2.6M hierarchical phrase pairs extracted from the training corpus were used on the test set. Another model was the tree-to-string model (Liu et al., 2006; Liu et al., 2007). Based on the same word-aligned training corpus, we ran a Chinese parser on the source side to obtain 1-best parses. For 15,157 sentences we failed to obtain 1-best parses. Therefore, only 93.7% of the training corpus were used by the tree-to-string model. About 578K tree-to-string rules extracted from the training corpus were used on t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3538" citStr="Och and Ney, 2004" startWordPosition="531" endWordPosition="534">(MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP S (X1, X1) X (fabiao X1, give a X1) X (yanjiang, talk) Figure 1: A derivation composed of SCFG rules that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. ing </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL03.</booktitle>
<contexts>
<context position="2949" citStr="Och, 2003" startWordPosition="443" endWordPosition="444">rtial translations (Section 3.2). Although such translation-level combination will not produce new translations, it does change the way of selecting promising candidates. • Two models could even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). We evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et </context>
<context position="18604" citStr="Och, 2003" startWordPosition="3030" endWordPosition="3031"> derivation level. The joint decoder usually runs as maxtranslation decoding because multiple derivations from various models are used. However, if all models involved belong to the same category, a joint decoder can also adopt the max-derivation fashion because all nodes and hyperedges are accessible now (Section 5.2). Allowing derivations for comprising rules from different models and integrating their strengths, derivation-level combination could hopefully produce new and better translations as compared with single models. 4 Extended Minimum Error Rate Training Minimum error rate training (Och, 2003) is widely used to optimize feature weights for a linear model (Och and Ney, 2002). The key idea of MERT is to tune one feature weight to minimize error rate each time while keep others fixed. Therefore, each 580 Figure 5: Calculation of critical intersections. candidate translation can be represented as a line: f(x) = a × x + b (8) where a is the feature value of current dimension, x is the feature weight being tuned, and b is the dotproduct of other dimensions. The intersection of two lines is where the candidate translation will change. Instead of computing all intersections, Och (2003) onl</context>
<context position="29054" citStr="Och, 2003" startWordPosition="4702" endWordPosition="4703">raining. ferent perspective, we try to combine different approaches directly in decoding phase by using hypergraphs. While system combination techniques manipulate only the final translations of each system, our method opens the possibility of exploiting much more information. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. 7 Conclusion We ha</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL02.</booktitle>
<contexts>
<context position="22573" citStr="Papineni et al., 2002" startWordPosition="3707" endWordPosition="3710">optimize the feature weights. Note that feature weights of various models are tuned jointly in extended MERT. 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The t1 t2 t3 f(x) (0, 0) x1 x2 x 581 Model Combination Max-derivation Max-translation Time BLEU Time BLEU hierarchical N/A 40.53 30.11 44.87 29.82 tree-to-string N/A 6.13 27.23 6.69 27.11 both translation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och an</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ofACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation. In</title>
<date>2007</date>
<booktitle>Proc. ofACL07.</booktitle>
<contexts>
<context position="1298" citStr="Rosti et al., 2007" startWordPosition="183" endWordPosition="186">model can share translations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translati</context>
<context position="27186" citStr="Rosti et al., 2007" startWordPosition="4426" endWordPosition="4429">ierarchical phrase pairs and tree-to-string rules. To produce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from training data, 6,800 glue rules connecting partial translations monotonically, and 16,554 tree-to-string rules. While tree-to-string rules offer linguistically motivated non-local reordering during decoding, hierarchical phrase pairs ensure good rule coverage. Max-translation decoding still failed to surpass max-derivation decoding in this case. 5.3 Comparison with System Combination We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly lower than that of joint decoding. But this difference is not significant statistically. 5.4 Individual Training Vs. Joint Training Table 4 shows the effects of individual training and joint training. By individual, we mean that the two models are trained independently. We concatenate and normalize their feature weights for the joint decoder. By joint, we mean that they are trained together by the extended MERT algorithm. We found that j</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proc. ofACL07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proc. ofACL08.</booktitle>
<contexts>
<context position="8325" citStr="Shen et al., 2008" startWordPosition="1349" endWordPosition="1352">ndicating that the two models produce the same translations. give a talk 0-2 give talks 0-2 give a talk 0-2 make a speech 0-2 give 0-1 talk 1-2 give 0-1 speech 1-2 S S (a) packing (b) give a talk 0-2 give talks 0-2 make a speech 0-2 give 0-1 talk 1-2 speech 1-2 S (c) and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. Second, translation models differ in decoding algorithms. Depending on the generating order of a target sentence, we distinguish between two major categories: left-to-right and bottom-up. Decoders that use rules with flat structures (e.g., phrase pairs) usually generate target sentences from left to right while those using rules with hierarchical structures (e.g., SCFG rules) often run in a bottom-up style. In resp</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. ofACL08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data. In</title>
<date>2005</date>
<booktitle>Proc. ofACL05.</booktitle>
<contexts>
<context position="28052" citStr="Smith and Eisner, 2005" startWordPosition="4558" endWordPosition="4561">lly. 5.4 Individual Training Vs. Joint Training Table 4 shows the effects of individual training and joint training. By individual, we mean that the two models are trained independently. We concatenate and normalize their feature weights for the joint decoder. By joint, we mean that they are trained together by the extended MERT algorithm. We found that joint training outperformed individual training significantly for both max-derivation decoding and max-translation decoding. 6 Related Work System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemblebased parsing (e.g., (Henderson and Brill, 1999)). In machine translation, confusion-network based combination techniques (e.g., (Rosti et al., 2007; He et al., 2008)) have achieved the state-of-theart performance in MT evaluations. From a difTraining Max-derivation Max-translation individual 30.70 29.95 joint 31.63 30.79 Table 4: Comparison of individual training and joint training. ferent perspective, we try to combine different approaches directly in decoding phase by using hypergraphs. While system combination techniques manipulate only the final translations of each system, </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. ofACL05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extension language model modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ofICSLP02.</booktitle>
<contexts>
<context position="22297" citStr="Stolcke, 2002" startWordPosition="3659" endWordPosition="3660"> phases: (1) build the translation hypergraphs, (2) generate n-best translations, and (3) generate n′- best derivations. We apply Algorithm 3 of Huang and Chiang (2005) for n-best list generation. Extended MERT runs on n-best translations plus n′- best derivations to optimize the feature weights. Note that feature weights of various models are tuned jointly in extended MERT. 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The t1 t2 t3 f(x) (0, 0) x1 x2 x 581 Model Combination Max-derivation Max-translation Time BLEU Time BLEU hierarchical N/A 40.53 30.11 44.87 29.82 tree-to-string N/A 6.13 27.23 6.69 27.11 both translation N/A N/A 55.89 30.79 derivation 48.45 31.63 54.91 31.49 Table 2: Comparison of </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extension language model modeling toolkit. In Proc. ofICSLP02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proc. ofACL06.</booktitle>
<contexts>
<context position="14549" citStr="Watanabe et al. (2006)" startWordPosition="2380" endWordPosition="2383">ensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f&apos;1 , and a set of translation models M 579 1: procedure JOINTDECODING(fi , M) 2: G + -0 3: for l +- 1 ... n do 4: for all i, j s.t. j − i = l do 5: for all m E M do 6: ADD(G, i, j, m) 7: end for 8: PRUNE(G, i, j) 9: end </context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proc. ofACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<contexts>
<context position="14340" citStr="Wu, 1997" startWordPosition="2347" endWordPosition="2348"> model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f&apos;1 , and a </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ofACL06.</booktitle>
<contexts>
<context position="14293" citStr="Xiong et al. (2006)" startWordPosition="2336" endWordPosition="2339">rst model and the dashed lines denote those of the second model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The i</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proc. ofACL06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>