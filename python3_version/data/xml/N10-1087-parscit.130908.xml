<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009560">
<title confidence="0.958193">
Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds
</title>
<author confidence="0.894804">
Zornitsa Kozareva and Eduard Hovy
</author>
<affiliation confidence="0.869106">
USC Information Sciences Institute
</affiliation>
<address confidence="0.887495">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.999772">
{kozareva,hovy}@isi.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999522705882353">
Open-class semantic lexicon induction is of
great interest for current knowledge harvest-
ing algorithms. We propose a general frame-
work that uses patterns in bootstrapping fash-
ion to learn open-class semantic lexicons for
different kinds of relations. These patterns re-
quire seeds. To estimate the goodness (the po-
tential yield) of new seeds, we introduce a re-
gression model that considers the connectiv-
ity behavior of the seed during bootstrapping.
The generalized regression model is evaluated
on six different kinds of relations with over
10000 different seeds for English and Span-
ish patterns. Our approach reaches robust per-
formance of 90% correlation coefficient with
15% error rate for any of the patterns when
predicting the goodness of seeds.
</bodyText>
<sectionHeader confidence="0.944795" genericHeader="categories and subject descriptors">
1 Introduction: What is a Good Seed?
</sectionHeader>
<bodyText confidence="0.9997789375">
The automated construction of semantically typed
lexicons (terms classified into their appropriate se-
mantic class) from unstructured text is of great im-
portance for various kinds of information extraction
(Grishman and Sundheim, 1996), question answer-
ing (Moldovan et al., 1999), and ontology popu-
lation (Suchanek et al., 2007). Maintaining large
semantic lexicons is a time-consuming and tedious
task, because open classes (such as: all singers, all
types of insects) are hard to cover completely, and
even closed classes (such as: all countries, all large
software companies) change over time. Since it is
practically impossible for a human to collect such
knowledge adequately, many supervised, unsuper-
vised, and semi-supervised techniques have been de-
veloped.
All these techniques employ some sort of context
to specify the appearance in text of the desired in-
formation. This approach is based on the general
intuition, dating back at least to the distributional
similarity idea of (Harris, 1954), that certain con-
texts are specific enough to constrain terms or ex-
pressions within them to be specific classes or types.
Often, the context is a string of words with an empty
slot for the desired term(s); sometimes, it is a regu-
lar expression-like pattern that includes word classes
(syntactic or semantic); sometimes, it is a more ab-
stract set of features, including orthographic fea-
tures like capitalization, words, syntactic relations,
semantic types, and other characteristics, which is
the more complete version of the distributional sim-
ilarity approach.
In early information extraction work, these con-
texts were constructed manually, and resembled reg-
ular expressions (Appelt et al., 1995). More re-
cently, researchers have focused on learning them
automatically. Since unsupervised algorithms re-
quire large training data and may or may not produce
the types and granularities of the semantic class de-
sired by the user, and supervised algorithms may re-
quire a lot of manual oversight, semi-supervised al-
gorithms have become more popular. They require
only a couple of seeds (examples filling the desired
semantic context) to enable the learning mechanism
to learn patterns that extract from unlabeled texts
additional instances of the same class (Riloff and
Jones, 1999; Etzioni et al., 2005; Pasca, 2004).
Sometimes, the pattern(s) learned are satisfactory
</bodyText>
<page confidence="0.940281">
618
</page>
<note confidence="0.756282">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 618–626,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998774622222222">
enough to need no further elaboration. They are
applied to harvest as many additional terms of the
desired type as possible (for example, the instance-
learning pattern ‘&lt;type&gt; such as ?’ introduced in
(Hearst, 1992)). More often, the method is applied
recursively: once some pattern(s) have been learned,
they are used to find additional terms, which are then
used as new seeds in the patterns to search for addi-
tional new patterns, etc., until no further patterns are
found. At that point, the satisfactory patterns are se-
lected and large-scale harvesting proceeds as usual.
In an interesting variation of this method, (Kozareva
et al., 2008) describe the ‘doubly-anchored pat-
tern’ (DAP) that includes a seed term in conjunc-
tion with the open slot for the desired terms to be
learned, making the pattern itself recursive by al-
lowing learned terms to replace the initial seed terms
directly: ‘&lt;type&gt; such as &lt;seed&gt; and ?’.
Context-based information harvesting is well un-
derstood and has been the focus of extensive re-
search. The core unsolved problem is the selec-
tion of seeds. In current knowledge harvesting al-
gorithms, seeds are chosen either at random (Davi-
dov et al., 2007; Kozareva et al., 2008), by picking
the top N most frequent terms of the desired class
(Riloff and Jones, 1999; Igo and Riloff, 2009), or by
asking experts (Pantel et al., 2009). None of these
methods is quite satisfactory. (Etzioni et al., 2005)
report on the impact of seed set noise on the final
performance of semantic class learning, and Pan-
tel et al. observe a tremendous variation in the en-
tity set expansion depending on the initial seed set
composition. These studies show that the selection
of ‘good’ seeds is very important. Recently, (Vyas
et al., 2009) proposed an automatic system for im-
proving the seeds generated by editors (Pantel et al.,
2009). The results show 34% improvement in final
performance using the appropriate seed set. How-
ever, using editors to select seeds or to guide their
seed selection process is expensive and therefore not
always possible. Because of this, we address in this
paper two questions: “What is a good seed?” and
“How can the goodness of seeds be automatically
measured without human intervention?”.
The contributions of this paper are as follows:
</bodyText>
<listItem confidence="0.999721">
• First, we use recursive patterns to automatically
learn seeds for open-class semantic lexicons.
• Second, we define what the ‘goodness’ of a
</listItem>
<bodyText confidence="0.666438166666667">
seed term is. Then we introduce a regression
model of seed quality measurement that, after
a certain amount of training, automatically es-
timates the goodness of new seeds with above
90% accuracy for bootstrapping with the given
relation.
</bodyText>
<listItem confidence="0.924270333333333">
• Next, importantly, we discover that training a
regression model on certain relations enables
one to predict the goodness of a seed even for
other relations that have never been seen be-
fore, with an accuracy rate of over 80%.
• We conduct experiments with six kinds of
relations and more than 10000 automatically
harvested seed examples in both English and
Spanish.
</listItem>
<bodyText confidence="0.999917428571428">
The rest of the paper is organized as follows.
In the next section, we review related work. Sec-
tion 3 describes the recursive pattern bootstrap-
ping (Kozareva et al., 2008). Section 4 presents our
seed quality measurement regression model. Sec-
tion 5 discusses experiments and results. Finally, we
conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999685083333334">
Seeds are used in automatic pattern extraction from
text corpora (Riloff and Jones, 1999) and from the
Web (Banko, 2009). Seeds are used to harvest in-
stances (Pasca, 2004; Etzioni et al., 2005; Kozareva
et al., 2008) or attributes of a given class (Pas¸ca and
Van Durme, 2008), or to learn concept-specific re-
lations (Davidov et al., 2007), or to expand already
existing entity sets (Pantel et al., 2009). As men-
tioned above, (Etzioni et al., 2005) report that seed
set composition affects the correctness of the har-
vested instances, and (Pantel et al., 2009) observe an
increment of 42% precision and 39% recall between
the best and worst performing seed sets for the task
of entity set expansion.
Because of the large diversity of the usage of
seeds, there has been no general agreement regard-
ing exactly how many seeds are necessary for a
given task. According to (Pantel et al., 2009) 10 to
20 seeds are a sufficient starting set in a distribu-
tional similarity model to discover as many new cor-
rect instances as may ever be found. This observa-
tion differs from the claim of (Pas¸ca and Van Durme,
2008) that 1 or 2 instances are sufficient to dis-
cover thousands of instance attributes. For some
</bodyText>
<page confidence="0.998439">
619
</page>
<bodyText confidence="0.998226352941176">
pattern-based algorithms one to two seeds are suf-
ficient (Davidov et al., 2007; Kozareva et al., 2008),
some require ten seeds (Riloff and Jones, 1999; Igo
and Riloff, 2009), and others use a variation of 1, 5,
10 to 25 seeds (Talukdar et al., 2008).
As mentioned, seed selection is not yet well un-
derstood. Seeds may be chosen at random (Davi-
dov et al., 2007; Kozareva et al., 2008), by picking
the most frequent terms of the desired class (Riloff
and Jones, 1999; Igo and Riloff, 2009), or by ask-
ing humans (Pantel et al., 2009). The intuitions for
seed selection that experts develop over time seem
to prefer instances that are neither ambiguous nor
too frequent, but that at the same time are prolific
and quickly lead to the discovery of a diverse set of
instances. These criteria are vague and do not al-
ways lead to the discovery of good seeds. For some
approaches, infrequent and ambiguous seeds are ac-
ceptable while for others they lead to deterioration
in performance. For instance, the DAP (Kozareva et
al., 2008) performance is not affected by the ambi-
guity of the seed, because the class and the seed in
the pattern mutually disambiguate each other, while
for the distributional similarity model of (Pantel et
al., 2009), starting with an ambiguous seed leads
to ‘leakage’ and the harvesting of non-true class in-
stances. (Kozareva et al., 2008) show that for the
closed class country, both high-frequency seeds like
USA and low-frequency seeds like Burkina Faso
can equally well yield all remaining instances. An
open question to which no-one provides an answer
is whether and which high/low frequency seeds can
yield all instances of large, open classes like people
or singers.
</bodyText>
<sectionHeader confidence="0.807103" genericHeader="method">
3 Bootstrapping Recursive Patterns
</sectionHeader>
<bodyText confidence="0.99917565">
There are many algorithms for harvesting informa-
tion from the Web. The main objective of our work
is not the creation of a new algorithm, but rather de-
termining the effect of seed selection on the gen-
eral class of recursive bootstrapping harvesting al-
gorithms for the acquisition of semantic lexicons for
open class relations. For our experiments, since it
is time-consuming and difficult for humans to pro-
vide large sets of seeds to start the bootstrapping
process, we employ the recursive DAP mechanism
introduced by (Kozareva et al., 2008) that produces
seeds on its own.
The algorithm starts with a seed of type class
which is fed into the doubly-anchored pattern
‘&lt;class&gt; such as &lt;seed&gt; and *’ and learns in the
* position new instances of type class. The newly
learned instances are then systematically placed into
the position of the seed in the DAP pattern, and the
harvesting process is repeated until no new instances
are found. The general framework is as follows:
</bodyText>
<listItem confidence="0.903265">
1. Given:
</listItem>
<tableCaption confidence="0.774084333333333">
a language L={English, Spanish}
a pattern Pi={e.g., [verb prep, noun, verb]}
a seed seed for Pi
</tableCaption>
<listItem confidence="0.995818444444444">
2. Build a query in DAP-like fashion for Pi using
template Ti of the type ‘class such as seed and
*’, ‘* and seed verb prep’, ‘* and seed noun’,
‘* and seed verb’
3. submit Ti to Yahoo! or another search engine
4. extract instances occupying the * position
5. take instances from 4. and go to 2.
6. repeat steps 2–5 until no new instances are
found
</listItem>
<bodyText confidence="0.9974476">
At the end of bootstrapping, the harvested in-
stances can be considered to be seeds with which
the bootstrapping procedure could have been initi-
ated. We can now compare any of them to study
their relative ‘goodness’ as bootstrapping seeds.
</bodyText>
<sectionHeader confidence="0.997592" genericHeader="method">
4 Seed Quality Measurement
</sectionHeader>
<subsectionHeader confidence="0.997147">
4.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.988154909090909">
We define our task as:
Task Definition: Given a seed and a pattern in a
language (say English or Spanish), (1) use the boot-
strapping procedure to learn instances from the Web;
(2) build a predictive model to estimate the ‘good-
ness’ of seeds (whether generated by a human or
learned).
Given a desired semantic class, a recursive harvest-
ing pattern expressing its context, and a seed term
for use in this pattern, we define the ‘goodness’ of
the seed as consisting of two measures:
</bodyText>
<listItem confidence="0.9955966">
• the yield: the total number of instances learned,
not counting duplicates, until the bootstrapping
procedure has run to exhaustion;
• the distance: the number of iterations required
by the process to reach exhaustion.
</listItem>
<page confidence="0.989904">
620
</page>
<bodyText confidence="0.999994444444444">
Our approach is to build a model of the behavior of
many seeds for the given pattern. Any new seed can
then be compared against this model, once its basic
characteristics have been determined, and its yield
and distance estimates produced. In order to deter-
mine the characteristics of the new seed, it first has
to be employed in the pattern for a small number of
iterations. The next subsection describes the regres-
sion model we employ in our approach.
</bodyText>
<subsectionHeader confidence="0.894328">
4.2 Regression Model
</subsectionHeader>
<bodyText confidence="0.998799666666667">
Given a seed s, we seek to predict the yield g of s as
defined above. We do this via a parametrized func-
tion f:g = f(s; w), where w E Rd are the weights.
Our approach is to learn w from a collection of N
training examples {&lt; si, gi &gt;}Z_&apos;1, where each si is
a seed and each gi E R.
Support vector regression (Drucker et al., 1996)
is a well-known method for training a regression
model by solving the following optimization prob-
</bodyText>
<equation confidence="0.9429285">
max(0, |gi − f(si; w) |− E)
N&amp;quot;
</equation>
<bodyText confidence="0.974187761904762">
E-insensitive loss function
where C is a regularization constant and E con-
trols the training error. The training algorithm finds
weights w that define a function f minimizing the
empirical risk.
Let h be a function from seeds into some vector-
space representation C Rd, then the function f takes
the form: f(s; w) = h(s)&apos;w = �Z_&apos;1 αiK(s, si),
where f is re-parameterized in terms of a polyno-
mial kernel function K with dual weights αi. K
measures the similarity between two seeds. Full de-
tails of the regression model and its implementation
are beyond the scope of this paper; for more de-
tails see (Sch¨olkopf and Smola, 2001; Smola et al.,
2003). In our experimental study, we use the freely
available implementation of SVM in Weka (Witten
and Frank, 2005).
To evaluate the quality of our prediction model,
we compare the actual yield of a seed with the pre-
dicted value obtained, and compute the correlation
coefficient and the relative absolute error.
</bodyText>
<sectionHeader confidence="0.9965" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.962458">
5.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.999993090909091">
We conducted an exhaustive evaluation study with
the open semantic classes people and city, initiated
with the seeds John and London. For each class, we
submitted the DAP patterns as web queries to Ya-
hoo!Boss and retrieved the top 1000 web snippets
for each query, keeping only unique instances. In
total, we collected 1.5GB of snippets for people and
1.9GB of snippets for cities. The algorithm ran un-
til complete exhaustion, requiring 19 iterations for
people and 12 for cities. The total number of unique
harvested instances was 3798 for people and 5090
for cities. We used all instances as seeds and instan-
tiated for each seed the bootstrapping process from
the very beginning. This resulted in 3798 and 5090
separate bootstrapping runs for people and cities re-
spectively. For each seed, we recorded the total
number of instances learned at the end of bootstrap-
ping, the number of iterations, and the number of
unique instances extracted on each iteration. After
the harvesting part terminated, we analyzed the con-
nectivity / bootstrapping behavior of the seeds, and
produced the regression model.
</bodyText>
<subsectionHeader confidence="0.99924">
5.2 Seed Characteristics
</subsectionHeader>
<bodyText confidence="0.999524555555556">
For many knowledge harvesting algorithms, the se-
lection of a non-ambiguous seeds is of great impor-
tance. In the DAP bootstrapping framework, the am-
biguity of the seed is eliminated as the class and the
seed mutually disambiguate each other. Of great im-
portance to the bootstrapping algorithm is the selec-
tion of a seed that can yield a large number of in-
stances and can keep the bootstrapping process en-
ergized.
</bodyText>
<figureCaption confidence="0.999506">
Figure 1: Seed Connectivity
</figureCaption>
<bodyText confidence="0.99027825">
Figure 1 shows the different kinds of seeds we
found on analyzing the results of the bootstrapping
process. Based on the yield learned on each iter-
ation, we identify four major kinds of seeds: her-
mit, one-step, mid, and high connectors. In the
figure, seed (a) is a hermit because it does not dis-
cover other instances. Seed (b) is a one-step connec-
tor as it discovers instances on the first iteration but
</bodyText>
<equation confidence="0.981047714285714">
lem:
min
wERs
N
2||w||2 + 1
N
i=1
</equation>
<page confidence="0.971616">
621
</page>
<bodyText confidence="0.997574454545454">
then becomes inactive. Seeds (d) and (e) are high
connectors because they find a rich population of in-
stances. Seed (c) is a mid connector because it has
lower yield than (d) and (e), but higher than (a) and
(b).
Table 1 shows the results of classifying the 3798
people and 5090 city seeds into the four kinds of
seed. The majority of the seeds for both patterns are
hermits, from 23 to 41% are high connectors, and
the rest are one-step and mid connectors. For each
kind of seed, we also show three examples.
</bodyText>
<table confidence="0.8371586">
people such as X and * examples
#hermit 2271 (60%) Leila, Anne Boleyn, Sophocles
#one-step 329 (9%) Helene, Frida Kahlo, Cornelius
#mid 315 (8%) Brent, Ferdinand, Olivia
#high 883 (23%) Diana, Donald Trump, Christopher
cities such as X and * examples
#hermit 2393 (47%) Belfast, Najafabad, El Mirador
#one-step 406 (8%) Durnstein, Wexford, Al-Qaim
#mid 207 (4%) Bialystok, Gori, New Albany
#high 2084 (41%) Vienna, Chicago, Marrakesh
</table>
<tableCaption confidence="0.997606">
Table 1: Connectivity-based Seed Classification.
</tableCaption>
<bodyText confidence="0.999981384615385">
This study shows that humans are very likely to
choose non-productive seeds for bootstrapping: it is
difficult for a human to know a priori that a name
like Diana will be more productive than Leila, He-
lene, or Olivia.
Another interesting characteristic of a seed is the
speed of learning. Some seeds, such as (e), ex-
tract large quantity of instances from the very be-
ginning, resulting in fewer bootstrapping iterations,
while others, such as (d), spike much later, resulting
in more. In our analysis, we found that some high
connector seeds of the people pattern can learn the
whole population in 12 iterations, while others re-
quire from 15 to 20 iterations. Figure 2 shows the
speed of learning of ten high connector seeds for
the people pattern. The y axis shows the number
of unique instances harvested on each iteration. In-
tuitively, a good seed is the one that produces a large
yield of instances in short distance. Thus the ‘good-
ness’ of seed (e) is better than that of seed (d).
As shown in Figure 2, for each seed, we observe
a single hump that corresponds to the point in which
a seed generates the maximum number of instances.
The peak occurs on different iterations because it is
dependent both on the yield learned with each iter-
ation and the total distance, for each seed. The oc-
</bodyText>
<figure confidence="0.9973901">
1000
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
600
500
400
300
200
100
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
Iteration
</figure>
<figureCaption confidence="0.999851">
Figure 2: Seed Learning Speed
</figureCaption>
<bodyText confidence="0.99942">
currence of a single hump reveals regularity in the
connectivity behavior of seeds, and is discussed in
the Conclusion. We model this behavior as features
in our regression model and use it to measure the
quality of new seeds. The next subsection explains
the features of the regression model and the experi-
mental results obtained.
</bodyText>
<subsectionHeader confidence="0.997125">
5.3 Predicting the Goodness of Seeds
</subsectionHeader>
<bodyText confidence="0.999947607142857">
Building a pattern specific model: For each pat-
tern, we build N different regression models, where
N corresponds to the total number of bootstrapping
iterations of the pattern. For regression model RZ,
we use the yield of a seed from iterations 1 to i as
features. This information is used to model the ac-
tivity of the seed in the bootstrapping process and
later on to predict the extraction power of new seeds.
For example, in Figure 1 on the first iteration seeds
(b), (c), and (d) have the same low connectivity com-
pared to seed (e). As bootstrapping progresses, seed
(d) reaches productive neighbors that discover more
instances, while seeds (b) and (c) become inactive.
This example shows that the yield in the initial stage
of bootstrapping is not sufficient to accurately pre-
dict the quality of the seeds. Since we do not know
exactly how many iterations are necessary to accu-
rately determine the ‘goodness’ of seeds, we model
the yield learned on each iteration by each seed and
subsequently include this information in the regres-
sion models.
The yield of a seed sk at iteration i is computed as
yield(sk)Z = Em_1(sm), where n is the total num-
ber of unique instances sm harvested on iteration i.
Yield(sk)Z is high when sk discovers a large number
of instances (new seeds), and small otherwise. For
hermit seeds, yield=0 at any iteration, because the
seeds are totally isolated and do not discover other
</bodyText>
<figure confidence="0.926658666666667">
900
800
700
</figure>
<page confidence="0.985088">
622
</page>
<bodyText confidence="0.9996875625">
instances (seeds). For example, when building the
second regression model R2 using seeds (d) and (e)
from Figure 1, the feature values corresponding to
each seed in R2 are: yield(sd)1=1 and yield(sd)2=2
for seed (d), and yield(se)1=3 and yield(se)2=5 for
seed (e).
Results: Figure 3 shows the correlation coefficients
(cc) and the relative absolute errors of each regres-
sion model Ri for the people and city patterns. The
results are computed over ten-fold cross validation
of the 3798 people and 5090 city seeds. The x axis
shows the regression model Ri,. The y axis in the
two upper graphs shows the correlation coefficient
of the predicted and the actual total yield of the seeds
using Ri, and in the two lower graphs, the y axis
shows the error rate of each Ri.
</bodyText>
<figureCaption confidence="0.996877">
Figure 3: Regression for People and City.
</figureCaption>
<bodyText confidence="0.999968702702703">
We consider as a baseline model the regression
R1 which uses only the yield of the seeds on first
iteration. The prediction of R1 has cc=0.6 with
50% error for people and cc=0.4 with 70% error
for cities. These results confirm our previous obser-
vation that the quality of the seeds cannot be accu-
rately measured in the very beginning of bootstrap-
ping. However, by the ninth iteration, the regres-
sion models for people and cities reach cc=1.0 with
5% error rate. To make such an accurate prediction,
the model uses around one half of all bootstrapping
iterations—generally, just past the hump in Figure 2,
once the yield starts dropping.
Often in real applications or when under limited
resources (e.g., a fixed amount of Web queries per
day), running half the bootstrapping iterations is not
feasible. This problem can be resolved by employ-
ing different stopping criteria, at the cost of lower
cc and greater error. For example, one cut-off point
can be the (averaged) iteration number of the hump
for the given pattern. For people, the average hump
occurs at the seventh iteration, and for the city at
the fifth iteration. At this point, both patterns have a
cc=0.9 with 15% error rate. An alternative stopping
point can be the fourth iteration, where cc=0.7–0.8
with 35% error.
Overall, our study shows that it is possible to
model the behavior of seeds and use it to accurately
predict the ‘goodness’ of previously unseen seeds.
The results obtained for both people and city pat-
terns are very promising. However, a disadvantage
of this regression is that it requires training over the
whole extent of the given pattern. Also, each regres-
sion model is specific to the particular pattern it is
trained over. Next, we propose a generalized regres-
sion model which surmounts the problem of training
pattern-specific regression models.
</bodyText>
<subsectionHeader confidence="0.626115">
5.4 Generalized Model for Goodness of Seeds
</subsectionHeader>
<bodyText confidence="0.999955391304348">
We built a generalized regression model (RG) com-
bining evidence from the people and city patterns.
We generated the features of each model as previ-
ously described in Section 5.3. From each pattern,
we randomly picked 1000 examples which resulted
in 30% of the people and 20% of the city seeds. We
used these seed examples to train the RGi models.
In total, we built 15 RGi, which is the maximum
number of overlapping iterations between the two
patterns. We tested our RG model with the remain-
ing 2798 people and 4090 city seeds.
Figure 4 shows the results of the RGi models for
the people and city patterns. In the first two itera-
tions, the predictions of the RG model are poorer
compared to the pattern-specific regression. On the
fourth iteration, both models have cc=0.7 and 0.8 for
the people and city patterns respectively. The error
rates of the generalized model are 41% and 35% for
people and city, while for the pattern-specific model
the errors are 37% and 32%. The early iterations
show a difference of around 4% in the error rate of
the two models, but around the ninth iteration both
models have comparable results.
</bodyText>
<figure confidence="0.999317132352941">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Regression Model Ri
60
55
50
45
40
35
30
25
20
15
10
5
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Regression Model Ri
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
Cities
cut_off, t
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Regression Model Ri
70
65
60
55
Relative Absolute Error (%)
50
45
40
35
30
25
20
15
10
5
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Regression Model Ri
Correlation Coefficient
Relative Absolute Error (%)
Correlation Coefficient
People
cut_off, t
0.1
People
Cities
</figure>
<page confidence="0.998183">
623
</page>
<bodyText confidence="0.9999148">
This study shows that it is possible to combine
evidence from two patterns harvesting different se-
mantic information to predict accurately the behav-
ior of unseen seed examples for either of the two
patterns.
</bodyText>
<subsectionHeader confidence="0.988355">
5.5 Evaluating the Generalized Model on
Different Languages and Kinds of Patterns
</subsectionHeader>
<bodyText confidence="0.999960111111111">
So far, we have studied the performance of the gen-
eralized seed quality prediction method for specific
patterns in English. However, the connectivity be-
havior of the seeds might change for other languages
and kinds of patterns, making the generalized model
impractical to use in such cases. To verify this,
we evaluated the generalized model (RG) from Sec-
tion 5.4 with the people and city patterns in Spanish
(‘ gente como Xy *’ and ‘ ciudades como Xy *’), as
well as with two new kinds of patterns (‘ * and Xfly
to’ and ‘ * and X work for’1). For each pattern, we
ran the bootstrapping process from Section 3 until
exhaustion and collected all seeds.
First, for each pattern we studied the connectivity
behavior of the seeds. Table 2 shows the obtained
results. The distribution is similar to the seed distri-
bution for the English people and cities patterns. Al-
though the total number of harvested instances (i.e.,
seeds) is different for each pattern, the proportion of
hermits to other seeds remains larger. From 20%
to 37% of the seeds are high connectors, and the
rest are one-step and mid connectors. This analysis
shows that the connectivity behavior of seeds across
different languages and patterns is similar, at least
for the examples studied. In addition to the seed
analysis, we show in the table the total number of
bootstrapping iterations for each pattern. The ‘work
</bodyText>
<footnote confidence="0.981873">
1The X indicates the position of the seed and (*) corresponds
to the instances learned during bootstrapping.
</footnote>
<bodyText confidence="0.9991176">
for’ and ‘fly to’ patterns run for a longer distance
compared to the other patterns. While for the ma-
jority of the patterns the hump is observed on the
fifth or seventh iteration, for these two patterns the
average peak is observed on the fifteenth.
</bodyText>
<table confidence="0.999527916666667">
gente como X y ciudades como X y
#hermit 318 (56%) 1061 (51%)
#one-step 58 (10%) 150 (8%)
#mid 79 (14%) 79 (4%)
#high 117 (20%) 795 (38%)
tot#iter 20 16
and X fly to and X work for
#hermit 389 (45%) 1262 (48%)
#one-step 87 (9%) 238 (9%)
#mid 75 (8%) 214 (8%)
#high 322 (37%) 922 (35%)
tot#iter 26 33
</table>
<tableCaption confidence="0.9822855">
Table 2: Seed Classification for Spanish and Verb-Prep
Patterns.
</tableCaption>
<bodyText confidence="0.999959">
Second, we test the RGZ models from Section 5.4,
which were trained on people and cities, to predict
the total yield of the seeds in the new patterns. Fig-
ure 5 shows the correlation coefficient and the rela-
tive absolute error results of each pattern for RGZ.
</bodyText>
<figureCaption confidence="0.986835">
Figure 5: Generalized Regression for Different Lan-
guages and Patterns.
</figureCaption>
<bodyText confidence="0.999882642857143">
Interestingly, we found that our generalized
method has consistent performance across the dif-
ferent languages and patterns. On the twelfth iter-
ation, the model is able to predict the ‘goodness’
of seeds with cc=1.0 and from 0.4% to 8.0% error
rate. Around the fifth and sixth iterations, all pat-
terns reach cc=0.8 with error of 5% to 15%. The
higher error bound is for patterns like ‘work for’ and
‘fly to’ which run for a longer distance. This experi-
mental study confirms the robustness of our general-
ized model which is trained on the behavior of seeds
from one kind of pattern and tested with seeds in dif-
ferent languages and on completely different kinds
of patterns.
</bodyText>
<figure confidence="0.998928305555555">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Generalized Regression Model RGi
55
50
45
40
35
30
25
20
15
10
5
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Generalized Regression Model RGi
Correlation Coefficient
Relative Absolute Error (%)
Cities
70
65
60
Cities
People
People
</figure>
<figureCaption confidence="0.98142">
Figure 4: Generalized Regression for People and City.
</figureCaption>
<figure confidence="0.999556">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Generalized Regression Model RGi
Work For
Fly desTo
Ciuda
Gente
45
40
35
30
25
20
15
10
5
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Generalized Regression Model RGi
Correlation Coefficient
Relative Absolute Error (%)
60
55
50
Work For
Fly To
Ciudades
Gente
</figure>
<page confidence="0.993834">
624
</page>
<sectionHeader confidence="0.998189" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999372585106383">
It would, a fortiori, seem impossible to estimate the
goodness of a seed term used in a recursive boot-
strapping pattern for harvesting information from
the web. After all, its eventual total yield and dis-
tance depend on the cumulation of the terms pro-
duced in each iteration of the bootstrapping, and
there are no external constraints or known web lan-
guage structure to be exploited.
We have shown that it is possible to create, using
regression, a model of the grown behavior of seeds
for a given pattern, and fitting it with an indication of
a new seed’s growth (considering its grown behavior
in a limited number of bootstrapping iterations) in
order to obtain a quite reliable estimate of the new
seed’s eventual yield and distance.
Going further, we are delighted to observe that
the regularity of the single-hump harvesting behav-
ior makes it possible to learn a regression model that
enables one to predict, with some accuracy, both the
yield and the distance of a new seed, even when the
pattern being considered is not yet seen. All that is
required is the indication of the seed’s growth be-
havior, obtained through a number of iterations us-
ing the pattern of interest.
Our ongoing analysis takes the following ap-
proach. Let Ti be the set of all new terms (terms
not yet found) harvested during iteration i. Then
T0 = {t0,1}, just the initial seed term. Let NY (ti,j)
be the novel yield of term ti,j, that is, the number
of as yet undiscovered terms produced by a single
application of the pattern using the term ti,j. Notice
that bootstrapping ceases when for some i = d (the
distance), Ej NY (td,j) = 0. Since the total number
of terms that can be learned, Ed 0 Ej NY (ti,j) =
N, is finite and fixed, there are exactly three al-
ternatives for the growth of the NY curve when
it is shown summed over each iteration: (i) either
Ej NY (ti,j) = Ej NY (ti+1,j) and there is no
larger NY sum for any iteration; or (ii) Ej NY (ti,j)
grows to a maximal value for some iteration i =
m and then decreases again; or (iii) Ej NY (ti,j)
reaches more than one locally maximal value at dif-
ferent iterations. The first case, in which exactly
the same number of new terms is harvested every
iteration for several or all iterations, would require
that each new term once learned yields precisely and
only one subsequent new term, or that the number
of hermits is exactly balanced by the NY of one or
more of the other terms in that iteration. This situa-
tion is so unlikely as to be dismissed outright. Case
(ii), in which there is a single hump, appears to be
how text is written on the web, as shown in Fig-
ure 2. Case (iii), the multi-hump case, would re-
quire that the terms be linked in semi-disconnected
‘islands’, with a relatively much smaller inter-island
connectivity than intra-island one. Given our stud-
ies, it appears that language on the web is not orga-
nized this way, at least not for the patterns we stud-
ied. However, it is not impossible: this two-hump
case would have to have occurred in (Kozareva et
al., 2008) when the ambiguous seed term Georgia
was used in the DAP ‘states such as Georgia and *’,
where initially the US states were harvested but, at
some point, the learned term Georgia also initiated
harvesting of the ex-USSR states. Such ‘leakage’
into a new semantic domain requires not only ambi-
guity of the seed but also parallel ambiguity of the
class term, which is highly unlikely as well.
Accepting case (ii), therefore, we postulate that
for any (or all regular) patterns there is some iter-
ation m in which Ej NY (tm,j) is maximal. The
question is how rapidly the summed NY curve ap-
proaches it and then abates again. This depends on
the out-degree connectivity of terms overall. In the
population of N terms for a given semantic pattern,
is the distribution of out-degrees Poisson (or Zip-
fian), or is it normal (Gaussian)? In the former case,
there will be a few high-degree connector terms and
a large number (the long tail) of one-step and hermit
terms; in the latter, there will be a small but equal
number of low-end and high-end connector terms,
with the bulk of terms falling in the mid-connector
range. One direction of our ongoing work is to deter-
mine this distribution, and to empirically derive its
parameters. It might be possible to discover some in-
teresting regularities about the (preferential) uses of
terms within semantic domains, as reflected in term
network connectivity.
Although not all seeds are equal, it appears to
be possible to treat them with a single regression
model, regardless of pattern, to predict their ‘good-
ness’.
Acknowledgments: This research was supported by
NSF grant IIS-0705091.
</bodyText>
<page confidence="0.998788">
625
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999711097826087">
Douglas E. Appelt, Jerry R. Hobbs, John Bear, David Is-
rael, Megumi Kameyama, Andy Kehler, David Martin,
Karen Myers, and Mabry Tyson. 1995. SRI Interna-
tional FASTUS system MUC-6 test results and analy-
sis. In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 237–248.
Michele Banko. 2009. Open information extraction from
the web. In Ph.D. Dissertation from University of
Washington.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 232–239, June.
Harris Drucker, Chris J.C. Burges, Linda Kaufman, Alex
Smola, and Vladimir Vapnik. 1996. Support vector re-
gression machines. In Advances in NIPS, pages 155–
161.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91–134,
June.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: a brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics, pages 466–471.
Zellig S. Harris. 1954. Distributional structure. Word,
10:140–162.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th confer-
ence on Computational linguistics, pages 539–545.
Sean Igo and Ellen Riloff. 2009. Corpus-based seman-
tic lexicon induction with web-based corroboration.
In Proceedings of the Workshop on Unsupervised and
Minimally Supervised Learning ofLexical Semantics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048–1056.
Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. Lasso: A tool for surfing the answer
net. In TREC.
Marius Pas¸ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings ofACL-08: HLT.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938–
947, August.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the thirteenth ACM
international conference on Information and knowl-
edge management, pages 137–145.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In AAAI ’99/IAAI ’99: Proceedings of the
sixteenth national conference on Artificial intelligence
and the eleventh Innovative applications of artificial
intelligence conference innovative applications of ar-
tificial intelligence.
Bernhard Sch¨olkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond (Adaptive
Computation and Machine Learning). The MIT Press.
Alex J. Smola, Bernhard Schlkopf, and Bernhard Sch
Olkopf. 2003. A tutorial on support vector regression.
Technical report, Statistics and Computing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ’07: Proceedings of the 16th international
conference on World Wide Web, pages 697–706.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP 2008, pages
582–590.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Management,
CIKM, pages 225–234.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, second edition.
</reference>
<page confidence="0.998814">
626
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.746205">
<title confidence="0.998245">Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds</title>
<author confidence="0.964323">Zornitsa Kozareva</author>
<author confidence="0.964323">Eduard</author>
<affiliation confidence="0.993406">USC Information Sciences</affiliation>
<address confidence="0.987137">4676 Admiralty</address>
<author confidence="0.79595">Marina del Rey</author>
<author confidence="0.79595">CA</author>
<abstract confidence="0.999058388888889">Open-class semantic lexicon induction is of great interest for current knowledge harvesting algorithms. We propose a general framework that uses patterns in bootstrapping fashion to learn open-class semantic lexicons for different kinds of relations. These patterns reseeds. To estimate the potential yield) of new seeds, we introduce a regression model that considers the connectivity behavior of the seed during bootstrapping. The generalized regression model is evaluated on six different kinds of relations with over seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefficient with 15% error rate for any of the patterns when the seeds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
<author>Jerry R Hobbs</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Andy Kehler</author>
<author>David Martin</author>
<author>Karen Myers</author>
<author>Mabry Tyson</author>
</authors>
<title>results and analysis.</title>
<date>1995</date>
<booktitle>SRI International FASTUS system MUC-6 test</booktitle>
<pages>237--248</pages>
<contexts>
<context position="2704" citStr="Appelt et al., 1995" startWordPosition="412" endWordPosition="415"> within them to be specific classes or types. Often, the context is a string of words with an empty slot for the desired term(s); sometimes, it is a regular expression-like pattern that includes word classes (syntactic or semantic); sometimes, it is a more abstract set of features, including orthographic features like capitalization, words, syntactic relations, semantic types, and other characteristics, which is the more complete version of the distributional similarity approach. In early information extraction work, these contexts were constructed manually, and resembled regular expressions (Appelt et al., 1995). More recently, researchers have focused on learning them automatically. Since unsupervised algorithms require large training data and may or may not produce the types and granularities of the semantic class desired by the user, and supervised algorithms may require a lot of manual oversight, semi-supervised algorithms have become more popular. They require only a couple of seeds (examples filling the desired semantic context) to enable the learning mechanism to learn patterns that extract from unlabeled texts additional instances of the same class (Riloff and Jones, 1999; Etzioni et al., 200</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Kameyama, Kehler, Martin, Myers, Tyson, 1995</marker>
<rawString>Douglas E. Appelt, Jerry R. Hobbs, John Bear, David Israel, Megumi Kameyama, Andy Kehler, David Martin, Karen Myers, and Mabry Tyson. 1995. SRI International FASTUS system MUC-6 test results and analysis. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 237–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
</authors>
<title>Open information extraction from the web. In Ph.D. Dissertation from</title>
<date>2009</date>
<institution>University of Washington.</institution>
<contexts>
<context position="7049" citStr="Banko, 2009" startWordPosition="1123" endWordPosition="1124">rate of over 80%. • We conduct experiments with six kinds of relations and more than 10000 automatically harvested seed examples in both English and Spanish. The rest of the paper is organized as follows. In the next section, we review related work. Section 3 describes the recursive pattern bootstrapping (Kozareva et al., 2008). Section 4 presents our seed quality measurement regression model. Section 5 discusses experiments and results. Finally, we conclude in Section 6. 2 Related Work Seeds are used in automatic pattern extraction from text corpora (Riloff and Jones, 1999) and from the Web (Banko, 2009). Seeds are used to harvest instances (Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008) or attributes of a given class (Pas¸ca and Van Durme, 2008), or to learn concept-specific relations (Davidov et al., 2007), or to expand already existing entity sets (Pantel et al., 2009). As mentioned above, (Etzioni et al., 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al., 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. Because of the large </context>
</contexts>
<marker>Banko, 2009</marker>
<rawString>Michele Banko. 2009. Open information extraction from the web. In Ph.D. Dissertation from University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshel Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by web mining.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>232--239</pages>
<contexts>
<context position="4750" citStr="Davidov et al., 2007" startWordPosition="738" endWordPosition="742">proceeds as usual. In an interesting variation of this method, (Kozareva et al., 2008) describe the ‘doubly-anchored pattern’ (DAP) that includes a seed term in conjunction with the open slot for the desired terms to be learned, making the pattern itself recursive by allowing learned terms to replace the initial seed terms directly: ‘&lt;type&gt; such as &lt;seed&gt; and ?’. Context-based information harvesting is well understood and has been the focus of extensive research. The core unsolved problem is the selection of seeds. In current knowledge harvesting algorithms, seeds are chosen either at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the top N most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking experts (Pantel et al., 2009). None of these methods is quite satisfactory. (Etzioni et al., 2005) report on the impact of seed set noise on the final performance of semantic class learning, and Pantel et al. observe a tremendous variation in the entity set expansion depending on the initial seed set composition. These studies show that the selection of ‘good’ seeds is very important. Recently, (Vyas et al., 2009) proposed an automatic system for </context>
<context position="7268" citStr="Davidov et al., 2007" startWordPosition="1159" endWordPosition="1162">e next section, we review related work. Section 3 describes the recursive pattern bootstrapping (Kozareva et al., 2008). Section 4 presents our seed quality measurement regression model. Section 5 discusses experiments and results. Finally, we conclude in Section 6. 2 Related Work Seeds are used in automatic pattern extraction from text corpora (Riloff and Jones, 1999) and from the Web (Banko, 2009). Seeds are used to harvest instances (Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008) or attributes of a given class (Pas¸ca and Van Durme, 2008), or to learn concept-specific relations (Davidov et al., 2007), or to expand already existing entity sets (Pantel et al., 2009). As mentioned above, (Etzioni et al., 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al., 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. Because of the large diversity of the usage of seeds, there has been no general agreement regarding exactly how many seeds are necessary for a given task. According to (Pantel et al., 2009) 10 to 20 seeds are a sufficient starting set in a </context>
<context position="8491" citStr="Davidov et al., 2007" startWordPosition="1377" endWordPosition="1381">tributional similarity model to discover as many new correct instances as may ever be found. This observation differs from the claim of (Pas¸ca and Van Durme, 2008) that 1 or 2 instances are sufficient to discover thousands of instance attributes. For some 619 pattern-based algorithms one to two seeds are sufficient (Davidov et al., 2007; Kozareva et al., 2008), some require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), and others use a variation of 1, 5, 10 to 25 seeds (Talukdar et al., 2008). As mentioned, seed selection is not yet well understood. Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). The intuitions for seed selection that experts develop over time seem to prefer instances that are neither ambiguous nor too frequent, but that at the same time are prolific and quickly lead to the discovery of a diverse set of instances. These criteria are vague and do not always lead to the discovery of good seeds. For some approaches, infrequent and ambiguous seeds are acceptable while for others they lead to deteriorati</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, and Moshel Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by web mining. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucker</author>
<author>Chris J C Burges</author>
<author>Linda Kaufman</author>
<author>Alex Smola</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1996</date>
<booktitle>In Advances in NIPS,</booktitle>
<pages>155--161</pages>
<contexts>
<context position="13087" citStr="Drucker et al., 1996" startWordPosition="2177" endWordPosition="2180">termined, and its yield and distance estimates produced. In order to determine the characteristics of the new seed, it first has to be employed in the pattern for a small number of iterations. The next subsection describes the regression model we employ in our approach. 4.2 Regression Model Given a seed s, we seek to predict the yield g of s as defined above. We do this via a parametrized function f:g = f(s; w), where w E Rd are the weights. Our approach is to learn w from a collection of N training examples {&lt; si, gi &gt;}Z_&apos;1, where each si is a seed and each gi E R. Support vector regression (Drucker et al., 1996) is a well-known method for training a regression model by solving the following optimization probmax(0, |gi − f(si; w) |− E) N&amp;quot; E-insensitive loss function where C is a regularization constant and E controls the training error. The training algorithm finds weights w that define a function f minimizing the empirical risk. Let h be a function from seeds into some vectorspace representation C Rd, then the function f takes the form: f(s; w) = h(s)&apos;w = �Z_&apos;1 αiK(s, si), where f is re-parameterized in terms of a polynomial kernel function K with dual weights αi. K measures the similarity between tw</context>
</contexts>
<marker>Drucker, Burges, Kaufman, Smola, Vapnik, 1996</marker>
<rawString>Harris Drucker, Chris J.C. Burges, Linda Kaufman, Alex Smola, and Vladimir Vapnik. 1996. Support vector regression machines. In Advances in NIPS, pages 155– 161.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
</authors>
<location>Doug Downey, Ana-</location>
<marker>Etzioni, Cafarella, </marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Ana-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<marker>Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference-6: a brief history.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>466--471</pages>
<contexts>
<context position="1241" citStr="Grishman and Sundheim, 1996" startWordPosition="185" endWordPosition="188">onnectivity behavior of the seed during bootstrapping. The generalized regression model is evaluated on six different kinds of relations with over 10000 different seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefficient with 15% error rate for any of the patterns when predicting the goodness of seeds. 1 Introduction: What is a Good Seed? The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al., 1999), and ontology population (Suchanek et al., 2007). Maintaining large semantic lexicons is a time-consuming and tedious task, because open classes (such as: all singers, all types of insects) are hard to cover completely, and even closed classes (such as: all countries, all large software companies) change over time. Since it is practically impossible for a human to collect such knowledge adequately, many supervised, unsupervised, and semi-supervised techniques have been developed. All these techniques employ some sort of context to specify the appear</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference-6: a brief history. In Proceedings of the 16th conference on Computational linguistics, pages 466–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--140</pages>
<contexts>
<context position="2007" citStr="Harris, 1954" startWordPosition="306" endWordPosition="307">d tedious task, because open classes (such as: all singers, all types of insects) are hard to cover completely, and even closed classes (such as: all countries, all large software companies) change over time. Since it is practically impossible for a human to collect such knowledge adequately, many supervised, unsupervised, and semi-supervised techniques have been developed. All these techniques employ some sort of context to specify the appearance in text of the desired information. This approach is based on the general intuition, dating back at least to the distributional similarity idea of (Harris, 1954), that certain contexts are specific enough to constrain terms or expressions within them to be specific classes or types. Often, the context is a string of words with an empty slot for the desired term(s); sometimes, it is a regular expression-like pattern that includes word classes (syntactic or semantic); sometimes, it is a more abstract set of features, including orthographic features like capitalization, words, syntactic relations, semantic types, and other characteristics, which is the more complete version of the distributional similarity approach. In early information extraction work, </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10:140–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="3788" citStr="Hearst, 1992" startWordPosition="580" endWordPosition="581">rn patterns that extract from unlabeled texts additional instances of the same class (Riloff and Jones, 1999; Etzioni et al., 2005; Pasca, 2004). Sometimes, the pattern(s) learned are satisfactory 618 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 618–626, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics enough to need no further elaboration. They are applied to harvest as many additional terms of the desired type as possible (for example, the instancelearning pattern ‘&lt;type&gt; such as ?’ introduced in (Hearst, 1992)). More often, the method is applied recursively: once some pattern(s) have been learned, they are used to find additional terms, which are then used as new seeds in the patterns to search for additional new patterns, etc., until no further patterns are found. At that point, the satisfactory patterns are selected and large-scale harvesting proceeds as usual. In an interesting variation of this method, (Kozareva et al., 2008) describe the ‘doubly-anchored pattern’ (DAP) that includes a seed term in conjunction with the open slot for the desired terms to be learned, making the pattern itself rec</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th conference on Computational linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Igo</author>
<author>Ellen Riloff</author>
</authors>
<title>Corpus-based semantic lexicon induction with web-based corroboration.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Unsupervised and Minimally Supervised Learning ofLexical Semantics. Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.</booktitle>
<contexts>
<context position="4884" citStr="Igo and Riloff, 2009" startWordPosition="763" endWordPosition="766">hat includes a seed term in conjunction with the open slot for the desired terms to be learned, making the pattern itself recursive by allowing learned terms to replace the initial seed terms directly: ‘&lt;type&gt; such as &lt;seed&gt; and ?’. Context-based information harvesting is well understood and has been the focus of extensive research. The core unsolved problem is the selection of seeds. In current knowledge harvesting algorithms, seeds are chosen either at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the top N most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking experts (Pantel et al., 2009). None of these methods is quite satisfactory. (Etzioni et al., 2005) report on the impact of seed set noise on the final performance of semantic class learning, and Pantel et al. observe a tremendous variation in the entity set expansion depending on the initial seed set composition. These studies show that the selection of ‘good’ seeds is very important. Recently, (Vyas et al., 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al., 2009). The results show 34% improvement in final performance using the approp</context>
<context position="8305" citStr="Igo and Riloff, 2009" startWordPosition="1341" endWordPosition="1344">ere has been no general agreement regarding exactly how many seeds are necessary for a given task. According to (Pantel et al., 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. This observation differs from the claim of (Pas¸ca and Van Durme, 2008) that 1 or 2 instances are sufficient to discover thousands of instance attributes. For some 619 pattern-based algorithms one to two seeds are sufficient (Davidov et al., 2007; Kozareva et al., 2008), some require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), and others use a variation of 1, 5, 10 to 25 seeds (Talukdar et al., 2008). As mentioned, seed selection is not yet well understood. Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). The intuitions for seed selection that experts develop over time seem to prefer instances that are neither ambiguous nor too frequent, but that at the same time are prolific and quickly lead to the discovery of a diverse set of instances. Th</context>
</contexts>
<marker>Igo, Riloff, 2009</marker>
<rawString>Sean Igo and Ellen Riloff. 2009. Corpus-based semantic lexicon induction with web-based corroboration. In Proceedings of the Workshop on Unsupervised and Minimally Supervised Learning ofLexical Semantics. Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.</rawString>
</citation>
<citation valid="false">
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<marker></marker>
<rawString>Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL-08: HLT, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Moldovan</author>
<author>Sanda M Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Richard Goodrum</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
</authors>
<title>Lasso: A tool for surfing the answer net.</title>
<date>1999</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="1285" citStr="Moldovan et al., 1999" startWordPosition="192" endWordPosition="195">ing. The generalized regression model is evaluated on six different kinds of relations with over 10000 different seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefficient with 15% error rate for any of the patterns when predicting the goodness of seeds. 1 Introduction: What is a Good Seed? The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al., 1999), and ontology population (Suchanek et al., 2007). Maintaining large semantic lexicons is a time-consuming and tedious task, because open classes (such as: all singers, all types of insects) are hard to cover completely, and even closed classes (such as: all countries, all large software companies) change over time. Since it is practically impossible for a human to collect such knowledge adequately, many supervised, unsupervised, and semi-supervised techniques have been developed. All these techniques employ some sort of context to specify the appearance in text of the desired information. Thi</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Girju, Rus, 1999</marker>
<rawString>Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum, Roxana Girju, and Vasile Rus. 1999. Lasso: A tool for surfing the answer net. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Weaklysupervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:</booktitle>
<publisher>HLT.</publisher>
<marker>Pas¸ca, Van Durme, 2008</marker>
<rawString>Marius Pas¸ca and Benjamin Van Durme. 2008. Weaklysupervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings ofACL-08: HLT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan,</title>
<location>Arkady Borkovsky, Ana-</location>
<marker>Pantel, </marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Popescu</author>
<author>Vishnu Vyas</author>
</authors>
<title>Web-scale distributional similarity and entity set expansion.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>938--947</pages>
<marker>Popescu, Vyas, 2009</marker>
<rawString>Maria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938– 947, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proc. of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>137--145</pages>
<contexts>
<context position="3319" citStr="Pasca, 2004" startWordPosition="511" endWordPosition="512">ore recently, researchers have focused on learning them automatically. Since unsupervised algorithms require large training data and may or may not produce the types and granularities of the semantic class desired by the user, and supervised algorithms may require a lot of manual oversight, semi-supervised algorithms have become more popular. They require only a couple of seeds (examples filling the desired semantic context) to enable the learning mechanism to learn patterns that extract from unlabeled texts additional instances of the same class (Riloff and Jones, 1999; Etzioni et al., 2005; Pasca, 2004). Sometimes, the pattern(s) learned are satisfactory 618 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 618–626, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics enough to need no further elaboration. They are applied to harvest as many additional terms of the desired type as possible (for example, the instancelearning pattern ‘&lt;type&gt; such as ?’ introduced in (Hearst, 1992)). More often, the method is applied recursively: once some pattern(s) have been learned, they are used to find additional terms, w</context>
<context position="7099" citStr="Pasca, 2004" startWordPosition="1132" endWordPosition="1133"> kinds of relations and more than 10000 automatically harvested seed examples in both English and Spanish. The rest of the paper is organized as follows. In the next section, we review related work. Section 3 describes the recursive pattern bootstrapping (Kozareva et al., 2008). Section 4 presents our seed quality measurement regression model. Section 5 discusses experiments and results. Finally, we conclude in Section 6. 2 Related Work Seeds are used in automatic pattern extraction from text corpora (Riloff and Jones, 1999) and from the Web (Banko, 2009). Seeds are used to harvest instances (Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008) or attributes of a given class (Pas¸ca and Van Durme, 2008), or to learn concept-specific relations (Davidov et al., 2007), or to expand already existing entity sets (Pantel et al., 2009). As mentioned above, (Etzioni et al., 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al., 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. Because of the large diversity of the usage of seeds, there has been no</context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of categorized named entities for web search. In Proc. of the thirteenth ACM international conference on Information and knowledge management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI ’99/IAAI ’99: Proceedings of the sixteenth national conference on Artificial intelligence</booktitle>
<contexts>
<context position="3283" citStr="Riloff and Jones, 1999" startWordPosition="503" endWordPosition="506">d regular expressions (Appelt et al., 1995). More recently, researchers have focused on learning them automatically. Since unsupervised algorithms require large training data and may or may not produce the types and granularities of the semantic class desired by the user, and supervised algorithms may require a lot of manual oversight, semi-supervised algorithms have become more popular. They require only a couple of seeds (examples filling the desired semantic context) to enable the learning mechanism to learn patterns that extract from unlabeled texts additional instances of the same class (Riloff and Jones, 1999; Etzioni et al., 2005; Pasca, 2004). Sometimes, the pattern(s) learned are satisfactory 618 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 618–626, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics enough to need no further elaboration. They are applied to harvest as many additional terms of the desired type as possible (for example, the instancelearning pattern ‘&lt;type&gt; such as ?’ introduced in (Hearst, 1992)). More often, the method is applied recursively: once some pattern(s) have been learned, they </context>
<context position="4861" citStr="Riloff and Jones, 1999" startWordPosition="759" endWordPosition="762">nchored pattern’ (DAP) that includes a seed term in conjunction with the open slot for the desired terms to be learned, making the pattern itself recursive by allowing learned terms to replace the initial seed terms directly: ‘&lt;type&gt; such as &lt;seed&gt; and ?’. Context-based information harvesting is well understood and has been the focus of extensive research. The core unsolved problem is the selection of seeds. In current knowledge harvesting algorithms, seeds are chosen either at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the top N most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking experts (Pantel et al., 2009). None of these methods is quite satisfactory. (Etzioni et al., 2005) report on the impact of seed set noise on the final performance of semantic class learning, and Pantel et al. observe a tremendous variation in the entity set expansion depending on the initial seed set composition. These studies show that the selection of ‘good’ seeds is very important. Recently, (Vyas et al., 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al., 2009). The results show 34% improvement in final perfo</context>
<context position="7018" citStr="Riloff and Jones, 1999" startWordPosition="1115" endWordPosition="1118"> never been seen before, with an accuracy rate of over 80%. • We conduct experiments with six kinds of relations and more than 10000 automatically harvested seed examples in both English and Spanish. The rest of the paper is organized as follows. In the next section, we review related work. Section 3 describes the recursive pattern bootstrapping (Kozareva et al., 2008). Section 4 presents our seed quality measurement regression model. Section 5 discusses experiments and results. Finally, we conclude in Section 6. 2 Related Work Seeds are used in automatic pattern extraction from text corpora (Riloff and Jones, 1999) and from the Web (Banko, 2009). Seeds are used to harvest instances (Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008) or attributes of a given class (Pas¸ca and Van Durme, 2008), or to learn concept-specific relations (Davidov et al., 2007), or to expand already existing entity sets (Pantel et al., 2009). As mentioned above, (Etzioni et al., 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al., 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set e</context>
<context position="8282" citStr="Riloff and Jones, 1999" startWordPosition="1337" endWordPosition="1340">f the usage of seeds, there has been no general agreement regarding exactly how many seeds are necessary for a given task. According to (Pantel et al., 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. This observation differs from the claim of (Pas¸ca and Van Durme, 2008) that 1 or 2 instances are sufficient to discover thousands of instance attributes. For some 619 pattern-based algorithms one to two seeds are sufficient (Davidov et al., 2007; Kozareva et al., 2008), some require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), and others use a variation of 1, 5, 10 to 25 seeds (Talukdar et al., 2008). As mentioned, seed selection is not yet well understood. Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). The intuitions for seed selection that experts develop over time seem to prefer instances that are neither ambiguous nor too frequent, but that at the same time are prolific and quickly lead to the discovery of a diver</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI ’99/IAAI ’99: Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander J Smola</author>
</authors>
<date>2001</date>
<booktitle>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning).</booktitle>
<publisher>The MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 2001</marker>
<rawString>Bernhard Sch¨olkopf and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Schlkopf</author>
<author>Bernhard Sch Olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2003</date>
<tech>Technical report, Statistics and Computing.</tech>
<contexts>
<context position="13861" citStr="Smola et al., 2003" startWordPosition="2314" endWordPosition="2317">n where C is a regularization constant and E controls the training error. The training algorithm finds weights w that define a function f minimizing the empirical risk. Let h be a function from seeds into some vectorspace representation C Rd, then the function f takes the form: f(s; w) = h(s)&apos;w = �Z_&apos;1 αiK(s, si), where f is re-parameterized in terms of a polynomial kernel function K with dual weights αi. K measures the similarity between two seeds. Full details of the regression model and its implementation are beyond the scope of this paper; for more details see (Sch¨olkopf and Smola, 2001; Smola et al., 2003). In our experimental study, we use the freely available implementation of SVM in Weka (Witten and Frank, 2005). To evaluate the quality of our prediction model, we compare the actual yield of a seed with the predicted value obtained, and compute the correlation coefficient and the relative absolute error. 5 Experiments and Results 5.1 Data Collection We conducted an exhaustive evaluation study with the open semantic classes people and city, initiated with the seeds John and London. For each class, we submitted the DAP patterns as web queries to Yahoo!Boss and retrieved the top 1000 web snippe</context>
</contexts>
<marker>Smola, Schlkopf, Olkopf, 2003</marker>
<rawString>Alex J. Smola, Bernhard Schlkopf, and Bernhard Sch Olkopf. 2003. A tutorial on support vector regression. Technical report, Statistics and Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In WWW ’07: Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="1334" citStr="Suchanek et al., 2007" startWordPosition="200" endWordPosition="203">d on six different kinds of relations with over 10000 different seeds for English and Spanish patterns. Our approach reaches robust performance of 90% correlation coefficient with 15% error rate for any of the patterns when predicting the goodness of seeds. 1 Introduction: What is a Good Seed? The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al., 1999), and ontology population (Suchanek et al., 2007). Maintaining large semantic lexicons is a time-consuming and tedious task, because open classes (such as: all singers, all types of insects) are hard to cover completely, and even closed classes (such as: all countries, all large software companies) change over time. Since it is practically impossible for a human to collect such knowledge adequately, many supervised, unsupervised, and semi-supervised techniques have been developed. All these techniques employ some sort of context to specify the appearance in text of the desired information. This approach is based on the general intuition, dat</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>582--590</pages>
<contexts>
<context position="8381" citStr="Talukdar et al., 2008" startWordPosition="1357" endWordPosition="1360">ssary for a given task. According to (Pantel et al., 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. This observation differs from the claim of (Pas¸ca and Van Durme, 2008) that 1 or 2 instances are sufficient to discover thousands of instance attributes. For some 619 pattern-based algorithms one to two seeds are sufficient (Davidov et al., 2007; Kozareva et al., 2008), some require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), and others use a variation of 1, 5, 10 to 25 seeds (Talukdar et al., 2008). As mentioned, seed selection is not yet well understood. Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Riloff and Jones, 1999; Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). The intuitions for seed selection that experts develop over time seem to prefer instances that are neither ambiguous nor too frequent, but that at the same time are prolific and quickly lead to the discovery of a diverse set of instances. These criteria are vague and do not always lead to the discovery of good seeds</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishnu Vyas</author>
</authors>
<title>Patrick Pantel, and Eric Crestan.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM,</booktitle>
<pages>225--234</pages>
<marker>Vyas, 2009</marker>
<rawString>Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009. Helping editors choose better seed sets for entity set expansion. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM, pages 225–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<note>second edition.</note>
<contexts>
<context position="13972" citStr="Witten and Frank, 2005" startWordPosition="2332" endWordPosition="2335">hts w that define a function f minimizing the empirical risk. Let h be a function from seeds into some vectorspace representation C Rd, then the function f takes the form: f(s; w) = h(s)&apos;w = �Z_&apos;1 αiK(s, si), where f is re-parameterized in terms of a polynomial kernel function K with dual weights αi. K measures the similarity between two seeds. Full details of the regression model and its implementation are beyond the scope of this paper; for more details see (Sch¨olkopf and Smola, 2001; Smola et al., 2003). In our experimental study, we use the freely available implementation of SVM in Weka (Witten and Frank, 2005). To evaluate the quality of our prediction model, we compare the actual yield of a seed with the predicted value obtained, and compute the correlation coefficient and the relative absolute error. 5 Experiments and Results 5.1 Data Collection We conducted an exhaustive evaluation study with the open semantic classes people and city, initiated with the seeds John and London. For each class, we submitted the DAP patterns as web queries to Yahoo!Boss and retrieved the top 1000 web snippets for each query, keeping only unique instances. In total, we collected 1.5GB of snippets for people and 1.9GB</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, second edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>