<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000642">
<title confidence="0.996675">
Discriminative Reranking for Semantic Parsing
</title>
<author confidence="0.999785">
Ruifang Ge Raymond J. Mooney
</author>
<affiliation confidence="0.998718">
Department of Computer Sciences
University of Texas at Austin
</affiliation>
<address confidence="0.908703">
Austin, TX 78712
</address>
<email confidence="0.999632">
{grf,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979954545455">
Semantic parsing is the task of mapping
natural language sentences to complete
formal meaning representations. The per-
formance of semantic parsing can be po-
tentially improved by using discrimina-
tive reranking, which explores arbitrary
global features. In this paper, we investi-
gate discriminative reranking upon a base-
line semantic parser, SCISSOR, where the
composition of meaning representations is
guided by syntax. We examine if features
used for syntactic parsing can be adapted
for semantic parsing by creating similar
semantic features based on the mapping
between syntax and semantics. We re-
port experimental results on two real ap-
plications, an interpreter for coaching in-
structions in robotic soccer and a natural-
language database interface. The results
show that reranking can improve the per-
formance on the coaching interpreter, but
not on the database interface.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993916">
A long-standing challenge within natural language
processing has been to understand the meaning of
natural language sentences. In comparison with
shallow semantic analysis tasks, such as word-
sense disambiguation (Ide and Jean´eronis, 1998)
and semantic role labeling (Gildea and Jurafsky,
2002; Carreras and M`arquez, 2005), which only
partially tackle this problem by identifying the
meanings of target words or finding semantic roles
of predicates, semantic parsing (Kate et al., 2005;
Ge and Mooney, 2005; Zettlemoyer and Collins,
2005) pursues a more ambitious goal – mapping
natural language sentences to complete formal
meaning representations (MRs), where the mean-
ing of each part of a sentence is analyzed, includ-
ing noun phrases, verb phrases, negation, quanti-
fiers and so on. Semantic parsing enables logic
reasoning and is critical in many practical tasks,
such as speech understanding (Zue and Glass,
2000), question answering (Lev et al., 2004) and
advice taking (Kuhlmann et al., 2004).
Ge and Mooney (2005) introduced an approach,
SCISSOR, where the composition of meaning rep-
resentations is guided by syntax. First, a statis-
tical parser is used to generate a semantically-
augmented parse tree (SAPT), where each internal
node includes both a syntactic and semantic label.
Once a SAPT is generated, an additional meaning-
composition process guided by the tree structure is
used to translate it into a final formal meaning rep-
resentation.
The performance of semantic parsing can be po-
tentially improved by using discriminative rerank-
ing, which explores arbitrary global features.
While reranking has benefited many tagging and
parsing tasks (Collins, 2000; Collins, 2002c;
Charniak and Johnson, 2005) including semantic
role labeling (Toutanova et al., 2005), it has not
yet been applied to semantic parsing. In this paper,
we investigate the effect of discriminative rerank-
ing to semantic parsing.
We examine if the features used in reranking
syntactic parses can be adapted for semantic pars-
ing, more concretely, for reranking the top SAPTs
from the baseline model SCISSOR. The syntac-
tic features introduced by Collins (2000) for syn-
tactic parsing are extended with similar semantic
features, based on the coupling of syntax and se-
mantics. We present experimental results on two
corpora: an interpreter for coaching instructions
</bodyText>
<page confidence="0.980742">
263
</page>
<note confidence="0.7229765">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 263–270,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999839166666667">
in robotic soccer (CLANG) and a natural-language
database interface (GeoQuery). The best rerank-
ing model significantly improves F-measure on
CLANG from 82.3% to 85.1% (15.8% relative er-
ror reduction), however, it fails to show improve-
ments on GEOQUERY.
</bodyText>
<sectionHeader confidence="0.997897" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.9772385">
2.1 Application Domains
2.1.1 CLANG: the RoboCup Coach Language
</subsectionHeader>
<bodyText confidence="0.997672">
RoboCup (www.robocup.org) is an inter-
national AI research initiative using robotic soccer
as its primary domain. In the Coach Competition,
teams of agents compete on a simulated soccer
field and receive advice from a team coach in
a formal language called CLANG. In CLANG,
tactics and behaviors are expressed in terms of
if-then rules. As described in Chen et al. (2003),
its grammar consists of 37 non-terminal symbols
and 133 productions. Negation and quantifiers
like all are included in the language. Below is a
sample rule with its English gloss:
((bpos (penalty-area our))
(do (player-except our {4})
(pos (half our))))
“If the ball is in our penalty area, all our players
except player 4 should stay in our half.”
</bodyText>
<subsubsectionHeader confidence="0.317438">
2.1.2 GEOQUERY: a DB Query Language
</subsubsectionHeader>
<bodyText confidence="0.872392909090909">
GEOQUERY is a logical query language for
a small database of U.S. geography containing
about 800 facts. The GEOQUERY language
consists of Prolog queries augmented with several
meta-predicates (Zelle and Mooney, 1996). Nega-
tion and quantifiers like all and each are included
in the language. Below is a sample query with its
English gloss:
answer(A,count(B,(city(B),loc(B,C),
const(C,countryid(usa))),A))
“How many cities are there in the US?”
</bodyText>
<subsectionHeader confidence="0.996059">
2.2 SCISSOR: the Baseline Model
</subsectionHeader>
<bodyText confidence="0.8957292">
SCISSOR is based on a fairly standard approach
to compositional semantics (Jurafsky and Martin,
2000). First, a statistical parser is used to con-
struct a semantically-augmented parse tree that
captures the semantic interpretation of individual
</bodyText>
<figure confidence="0.867724666666667">
NP-PLAYER
PRP$-TEAM NN-PLAYER CD-UNUM
our player 2
</figure>
<figureCaption confidence="0.993624">
Figure 1: A SAPT for describing a simple CLANG
concept PLAYER.
</figureCaption>
<bodyText confidence="0.998559214285714">
words and the basic predicate-argument structure
of a sentence. Next, a recursive deterministic pro-
cedure is used to compose the MR of a parent
node from the MR of its children following the
tree structure.
Figure 1 shows the SAPT for a simple natural
language phrase describing the concept PLAYER
in CLANG. We can see that each internal node
in the parse tree is annotated with a semantic la-
bel (shown after dashes) representing concepts in
an application domain; when a node is semanti-
cally vacuous in the application domain, it is as-
signed with the semantic label NULL. The seman-
tic labels on words and non-terminal nodes repre-
sent the meanings of these words and constituents
respectively. For example, the word our repre-
sents a TEAM concept in CLANG with the value
our, whereas the constituent OUR PLAYER 2 rep-
resents a PLAYER concept. Some type concepts
do not take arguments, like team and unum (uni-
form number), while some concepts, which we
refer to as predicates, take an ordered list of ar-
guments, like player which requires both a TEAM
and a UNUM as its arguments.
SAPTs are given to a meaning composition
process to compose meaning, guided by both
tree structures and domain predicate-argument re-
quirements. In figure 1, the MR of our and 2
would fill the arguments of PLAYER to generate
the MR of the whole constituent PLAYER(OUR,2)
using this process.
SCISSOR is implemented by augmenting
Collins’ (1997) head-driven parsing model II to
incorporate the generation of semantic labels on
internal nodes. In a head-driven parsing model,
a tree can be seen as generated by expanding
non-terminals with grammar rules recursively.
To deal with the sparse data problem, the expan-
sion of a non-terminal (parent) is decomposed
into primitive steps: a child is chosen as the
head and is generated first, and then the other
children (modifiers) are generated independently
</bodyText>
<page confidence="0.996425">
264
</page>
<table confidence="0.664247833333333">
BACK-OFFLEVEL PL,(Li|...)
1 P,H,w,t,A,LC
2 P,H,t,A,LC
3 P,H,A,LC
4 P,H
5 P
</table>
<tableCaption confidence="0.996374">
Table 1: Extended back-off levels for the semantic
</tableCaption>
<bodyText confidence="0.998918266666667">
parameter PL,(Li|...), using the same notation as
in Ge and Mooney (2005). The symbols P, H and
Li are the semantic label of the parent, head, and
the ith left child, w is the head word of the parent,
t is the semantic label of the head word, S is the
distance between the head and the modifier, and
LC is the left semantic subcat.
constrained by the head. Here, we only describe
changes made to SCISSOR for reranking, for a
full description of SCISSOR see Ge and Mooney
(2005).
In SCISSOR, the generation of semantic labels
on modifiers are constrained by semantic subcat-
egorization frames, for which data can be very
sparse. An example of a semantic subcat in Fig-
ure 1 is that the head PLAYER associated with NN
requires a TEAM as its modifier. Although this
constraint improves SCISSOR’s precision, which
is important for semantic parsing, it also limits
its recall. To generate plenty of candidate SAPTs
for reranking, we extended the back-off levels for
the parameters generating semantic labels of mod-
ifiers. The new set is shown in Table 1 using the
parameters for the generation of the left-side mod-
ifiers as an example. The back-off levels 4 and 5
are newly added by removing the constraints from
the semantic subcat. Although the best SAPTs
found by the model may not be as precise as be-
fore, we expect that reranking can improve the re-
sults and rank correct SAPTs higher.
</bodyText>
<subsectionHeader confidence="0.986277">
2.3 The Averaged Perceptron Reranking
Model
</subsectionHeader>
<bodyText confidence="0.975578166666667">
Averaged perceptron (Collins, 2002a) has been
successfully applied to several tagging and parsing
reranking tasks (Collins, 2002c; Collins, 2002a),
and in this paper, we employed it in reranking
semantic parses generated by the base semantic
parser SCISSOR. The model is composed of three
parts (Collins, 2002a): a set of candidate SAPTs
GEN, which is the top n SAPTs of a sentence
from SCISSOR; a function 4b that maps a sentence
Inputs: A set of training examples (xi, yz ), i = 1...n, where xi
is a sentence, and yz is a candidate SAPT that has the highest
similarity score with the gold-standard SAPT
</bodyText>
<figure confidence="0.979315">
Initialization: Set W = 0
Algorithm:
Fort = 1...T, i = 1...n
Calculate yi = arg maxyEGEN(xi) 4)(xi, y) W
If (yi =� yi ) then W = W + 4)(xi, yi ) − `F(xi, yi)
Output: The parameter vector W�
</figure>
<figureCaption confidence="0.999976">
Figure 2: The perceptron training algorithm.
</figureCaption>
<bodyText confidence="0.999096571428571">
x and its SAPT y into a feature vector 4b(x, y) E
Rd; and a weight vector W associated with the set
of features. Each feature in a feature vector is a
function on a SAPT that maps the SAPT to a real
value. The SAPT with the highest score under a
parameter vector W is outputted, where the score
is calculated as:
</bodyText>
<equation confidence="0.980894">
score(x, y) = -b(x, y) · W (1)
</equation>
<bodyText confidence="0.999981">
The perceptron training algorithm for estimat-
ing the parameter vector W is shown in Fig-
ure 2. For a full description of the algorithm,
see (Collins, 2002a). The averaged perceptron, a
variant of the perceptron algorithm is often used in
testing to decrease generalization errors on unseen
test examples, where the parameter vectors used
in testing is the average of each parameter vector
generated during the training process.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="method">
3 Features for Reranking SAPTs
</sectionHeader>
<bodyText confidence="0.999943944444444">
In our setting, reranking models discriminate be-
tween SAPTs that can lead to correct MRs and
those that can not. Intuitively, both syntactic and
semantic features describing the syntactic and se-
mantic substructures of a SAPT would be good in-
dicators of the SAPT’s correctness.
The syntactic features introduced by Collins
(2000) for reranking syntactic parse trees have
been proven successfully in both English and
Spanish (Cowan and Collins, 2005). We exam-
ine if these syntactic features can be adapted for
semantic parsing by creating similar semantic fea-
tures. In the following section, we first briefly de-
scribe the syntactic features introduced by Collins
(2000), and then introduce two adapted semantic
feature sets. A SAPT in CLANG is shown in Fig-
ure 3 for illustrating the features throughout this
section.
</bodyText>
<page confidence="0.987799">
265
</page>
<figure confidence="0.9969948">
VP-ACTION.PASS
VB VP-ACTION.PASS
be
VBN-ACTION.PASS PP-POINT
passed
TO NP-POINT
to PRN-POINT
-LRB–POINT NP-NUM1 COMMA NP-NUM2 -RRB-
( CD-NUM , CD-NUM )
36 10
</figure>
<figureCaption confidence="0.6294894">
Figure 3: A SAPT for illustrating the reranking features, where the syntactic label “,” is replaced by
COMMA for a clearer description of features, and the NULL semantic labels are not shown. The head
of the rule “PRN-POINT—* -LRB–POINT NP-NUM1 COMMA NP-NUM2 -RRB-” is -LRB–POINT. The
semantic labels NUM1 and NUM2 are meta concepts in CLANG specifying the semantic role filled since
NUM can fill multiple semantic roles in the predicate POINT.
</figureCaption>
<subsectionHeader confidence="0.997191">
3.1 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999880166666667">
All syntactic features introduced by Collins (2000)
are included for reranking SAPTs. While the full
description of all the features is beyond the scope
of this paper, we still introduce several feature
types here for the convenience of introducing se-
mantic features later.
</bodyText>
<listItem confidence="0.955896842105263">
1. Rules. These are the counts of unique syntac-
tic context-free rules in a SAPT. The example
in Figure 3 has the feature f(PRN—* -LRB- NP
COMMA NP -RRB-)=1.
2. Bigrams. These are the counts of unique
bigrams of syntactic labels in a constituent.
They are also featured with the syntactic la-
bel of the constituent, and the bigram’s rel-
ative direction (left, right) to the head of the
constituent. The example in Figure 3 has the
feature f(NP COMMA, right, PRN)=1.
3. Grandparent Rules. These are the same as
Rules, but also include the syntactic label
above a rule. The example in Figure 3 has
the feature f([PRN—* -LRB- NP COMMA NP
-RRB-], NP)=1, where NP is the syntactic la-
bel above the rule “PRN—* -LRB- NP COMMA
NP -RRB-”.
4. Grandparent Bigrams. These are the same
</listItem>
<bodyText confidence="0.946394166666667">
as Bigrams, but also include the syntactic
label above the constituent containing a bi-
gram. The example in Figure 3 has the
feature f([NP COMMA, right, PRN], NP)=1,
where NP is the syntactic label above the con-
stituent PRN.
</bodyText>
<subsectionHeader confidence="0.9204745">
3.2 Semantic Features
3.2.1 Semantic Feature Set I
</subsectionHeader>
<bodyText confidence="0.999080666666667">
A similar semantic feature type is introduced for
each syntactic feature type used by Collins (2000)
by replacing syntactic labels with semantic ones
(with the semantic label NULL not included). The
corresponding semantic feature types for the fea-
tures in Section 3.1 are:
</bodyText>
<listItem confidence="0.909740666666667">
1. Rules. The example in Figure 3 has the fea-
ture f(POINT—* POINT NUM1 NUM2)=1.
2. Bigrams. The example in Figure 3 has the
feature f(NUM1 NUM2, right, POINT)=1,
where the bigram “NUM1 NUM2”appears to
the right of the head POINT.
3. Grandparent Rules. The example in Figure 3
has the feature f([POINT—* POINT NUM1
NUM2], POINT)=1, where the last POINT is
</listItem>
<page confidence="0.988438">
266
</page>
<figure confidence="0.794197">
ACTION.PASS
</figure>
<figureCaption confidence="0.7620784">
Figure 4: The tree generated by removing purely-
syntactic nodes from the SAPT in Figure 3 (with
syntactic labels omitted.)
the semantic label above the semantic rule
“POINT—* POINT NUM1 NUM2”.
</figureCaption>
<bodyText confidence="0.9602884">
4. Grandparent Bigrams. The example in Fig-
ure 3 has the feature f([NUM1 NUM2, right,
POINT], POINT)=1, where the last POINT is
the semantic label above the POINT associ-
ated with PRN.
</bodyText>
<subsectionHeader confidence="0.678105">
3.2.2 Semantic Feature Set II
</subsectionHeader>
<bodyText confidence="0.999931">
Purely-syntactic structures in SAPTs exist with
no meaning composition involved, such as the ex-
pansions from NP to PRN, and from PP to “TO NP”
in Figure 3. One possible drawback of the seman-
tic features derived directly from SAPTs as in Sec-
tion 3.2.1 is that they could include features with
no meaning composition involved, which are in-
tuitively not very useful. For example, the nodes
with purely-syntactic expansions mentioned above
would trigger a semantic rule feature with mean-
ing unchanged (from POINT to POINT). Another
possible drawback of these features is that the fea-
tures covering broader context could potentially
fail to capture the real high-level meaning compo-
sition information. For example, the Grandparent
Rule example in Section 3.2.1 has POINT as the
semantic grandparent of a POINT composition, but
not the real one ACTION.PASS.
To address these problems, another semantic
feature set is introduced by deriving semantic fea-
tures from trees where purely-syntactic nodes of
SAPTs are removed (the resulting tree for the
SAPT in Figure 3 is shown in Figure 4). In this
tree representation, the example in Figure 4 would
have the Grandparent Rule feature f([POINT—*
POINT NUM1 NUM2], ACTION.PASS)=1, with the
correct semantic grandparent ACTION.PASS in-
cluded.
</bodyText>
<sectionHeader confidence="0.996927" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.947437">
4.1 Experimental Methodology
</subsectionHeader>
<bodyText confidence="0.999896469387755">
Two corpora of natural language sentences paired
with MRs were used in the reranking experiments.
For CLANG, 300 pieces of coaching advice were
randomly selected from the log files of the 2003
RoboCup Coach Competition. Each formal in-
struction was translated into English by one of
four annotators (Kate et al., 2005). The average
length of an natural language sentence in this cor-
pus is 22.52 words. For GEOQUERY, 250 ques-
tions were collected by asking undergraduate stu-
dents to generate English queries for the given
database. Queries were then manually translated
into logical form (Zelle and Mooney, 1996). The
average length of a natural language sentence in
this corpus is 6.87 words.
We adopted standard 10-fold cross validation
for evaluation: 9/10 of the whole dataset was used
for training (training set), and 1/10 for testing (test
set). To train a reranking model on a training set,
a separate “internal” 10-fold cross validation over
the training set was employed to generate n-best
SAPTs for each training example using a base-
line learner, where each training set was again
separated into 10 folds with 9/10 for training the
baseline learner, and 1/10 for producing the n-
best SAPTs for training the reranker. Reranking
models trained in this way ensure that the n-best
SAPTs for each training example are not gener-
ated by a baseline model that has already seen that
example. To test a reranking model on a test set, a
baseline model trained on a whole training set was
used to generate n-best SAPTs for each test ex-
ample, and then the reranking model trained with
the above method was used to choose a best SAPT
from the candidate SAPTs.
The performance of semantic parsing was mea-
sured in terms ofprecision (the percentage of com-
pleted MRs that were correct), recall (the percent-
age of all sentences whose MRs were correctly
generated) and F-measure (the harmonic mean of
precision and recall). Since even a single mistake
in an MR could totally change the meaning of an
example (e.g. having OUR in an MR instead of OP-
PONENT in CLANG), no partial credit was given
for examples with partially-correct SAPTs.
Averaged perceptron (Collins, 2002a), which
has been successfully applied to several tag-
ging and parsing reranking tasks (Collins, 2002c;
Collins, 2002a), was employed for training rerank-
</bodyText>
<figure confidence="0.994483818181818">
ACTION.PASS
passed
POINT
NUM1
NUM
36
POINT
(
NUM2
NUM
10
</figure>
<page confidence="0.981978">
267
</page>
<table confidence="0.997762">
CLANG GEOQUERY
P R F P R F
SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8
SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4
</table>
<tableCaption confidence="0.9834645">
Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in
bold), where P = precision, R = recall, and F = F-measure.
</tableCaption>
<table confidence="0.998871666666667">
n 1 2 5 10 20 50
CLANG 78.0 81.3 83.0 84.0 85.0 85.3
GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6
</table>
<tableCaption confidence="0.999758">
Table 3: Oracle recalls on CLANG and GEOQUERY as a function of number n of n-best SAPTs.
</tableCaption>
<bodyText confidence="0.999923411764706">
ing models. To choose the correct SAPT of a
training example required for training the aver-
aged perceptron, we selected a SAPT that results
in the correct MR; if multiple such SAPTs exist,
the one with the highest baseline score was cho-
sen. Since no partial credit was awarded in evalua-
tion, a training example was discarded if it had no
correct SAPT. Rerankers were trained on the 50-
best SAPTs provided by SCISSOR, and the num-
ber of perceptron iterations over the training exam-
ples was limited to 10. Typically, in order to avoid
over-fitting, reranking features are filtered by re-
moving those occurring in less than some mini-
mal number of training examples. We only re-
moved features that never occurred in the training
data since experiments with higher cut-offs failed
to show any improvements.
</bodyText>
<sectionHeader confidence="0.71133" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<subsectionHeader confidence="0.892864">
4.2.1 Baseline Results
</subsectionHeader>
<bodyText confidence="0.999977470588235">
Table 2 shows the results comparing the base-
line learner SCISSOR using both the back-off pa-
rameters in Ge and Mooney (2005) (SCISSOR) and
the revised parameters in Section 2.2 (SCISSOR+).
As we expected, SCISSOR+ has better recall and
worse precision than SCISSOR on both corpora
due to the additional levels of back-off. SCISSOR+
is used as the baseline model for all reranking ex-
periments in the next section.
Table 3 gives oracle recalls for CLANG and
GEOQUERY where an oracle picks the correct
parse from the n-best SAPTs if any of them are
correct. Results are shown for increasing values
of n. The trends for CLANG and GEOQUERY are
different: small values of n show significant im-
provements for CLANG, while a larger n is needed
to improve results for GEOQUERY.
</bodyText>
<subsectionHeader confidence="0.791499">
4.2.2 Reranking Results
</subsectionHeader>
<bodyText confidence="0.999908055555555">
In this section, we describe the experiments
with reranking models utilizing different feature
sets. All models include the score assigned to a
SAPT by the baseline model as a special feature.
Table 4 shows results using different feature sets
derived directly from SAPTs. In general, rerank-
ing improves the performance of semantic parsing
on CLANG, but not on GEOQUERY. This could
be explained by the different oracle recall trends of
CLANG and GEOQUERY. We can see that in Ta-
ble 3, even a small n can increase the oracle score
on CLANG significantly, but not on GEOQUERY.
With the baseline score included as a feature, cor-
rect SAPTs closer to the top are more likely to
be reranked to the top than the ones in the back,
thus CLANG is more likely to have more sentences
reranked correct than GEOQUERY. On CLANG,
using the semantic feature set alone achieves the
best improvements over the baseline with 2.8%
absolute improvement in F-measure (15.8% rel-
ative error reduction), which is significant at the
95% confidence level using a paired Student’s t-
test. Nevertheless, the difference between SEM1
and SYN+SEM1 is very small (only one example).
Using syntactic features alone only slightly im-
proves the results because the syntactic features
do not directly discriminate between correct and
incorrect meaning representations. To put this
in perspective, Charniak and Johnson (2005) re-
ported that reranking improves the F-measure of
syntactic parsing from 89.7% to 91.0% with a 50-
best oracle F-measure score of 96.8%.
Table 5 compares results using semantic fea-
tures directly derived from SAPTs (SEM1), and
from trees with purely-syntactic nodes removed
(SEM2). It compares reranking models using these
</bodyText>
<page confidence="0.992685">
268
</page>
<table confidence="0.9993025">
CLANG GEOQUERY
P R F P R F
SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4
SYN 87.7 78.7 83.0 95.5 77.2 85.4
SEM1 90.0(23.1) 80.7(12.3) 85.1(15.8) 95.5 76.8 85.1
SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9
</table>
<tableCaption confidence="0.9901975">
Table 4: Reranking results on CLANG and GEOQUERY using different feature sets derived directly from
SAPTs (with the best results in bold and relative error reduction in parentheses). The reranking model
SYN uses the syntactic feature set in Section 3.1, SEM1 uses the semantic feature set in Section 3.2.1, and
SYN+SEM1 uses both.
</tableCaption>
<table confidence="0.9996155">
CLANG GEOQUERY
P R F P R F
SEM1 90.0 80.7 85.1 95.5 76.8 85.1
SEM2 88.1 79.0 83.3 96.0 77.2 85.6
SEM1+SEM2 88.5 79.3 83.7 95.5 76.4 84.9
SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9
SYN+SEM2 88.1 79.0 83.3 95.5 76.8 85.1
SYN+SEM1+SEM2 88.9 79.7 84.0 95.5 76.4 84.9
</table>
<tableCaption confidence="0.966286">
Table 5: Reranking results on CLANG and GEOQUERY comparing semantic features derived directly from
</tableCaption>
<bodyText confidence="0.998521791666667">
SAPTs, and semantic features from trees with purely-syntactic nodes removed. The symbol SEM1 and SEM2
refer to the semantic feature sets in Section 3.2.1 and 3.2.1 respectively, and SYN refers to the syntactic
feature set in Section 3.1.
feature sets alone and together, and using them
along with the syntactic feature set (SYN) alone
and together. Overall, SEM1 provides better results
than SEM2 on CLANG and slightly worse results
on GEOQUERY (only in one sentence), regard-
less of whether or not syntactic features are in-
cluded. Using both semantic feature sets does not
improve the results over just using SEM1. On one
hand, the better performance of SEM1 on CLANG
contradicts our expectation because of the reasons
discussed in Section 3.2.2; the reason behind this
needs to be investigated. On the other hand, how-
ever, it also suggests that the semantic features de-
rived directly from SAPTs can provide good evi-
dence for semantic correctness, even with redun-
dant purely syntactically motivated features.
We have also informally experimented with
smoothed semantic features utilizing domain on-
tology given by CLANG, which did not show im-
provements over reranking models not using these
features.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986227272727">
We have applied discriminative reranking to se-
mantic parsing, where reranking features are de-
veloped from features for reranking syntactic
parses based on the coupling of syntax and se-
mantics. The best reranking model significantly
improves F-measure on a Robocup coaching task
(CLANG) from 82.3% to 85.1%, while it fails to
improve the performance on a geography database
query task (GEOQUERY).
Future work includes further investigation of
the reasons behind the different utility of rerank-
ing for the CLANG and GEOQUERY tasks. We
also plan to explore other types of reranking
features, such as the features used in semantic
role labeling (SRL) (Gildea and Jurafsky, 2002;
Carreras and M`arquez, 2005), like the path be-
tween a target predicate and its argument, and
kernel methods (Collins, 2002b). Experimenting
with other effective reranking algorithms, such as
SVMs (Joachims, 2002) and MaxEnt (Charniak
and Johnson, 2005), is also a direction of our fu-
ture research.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.995185666666667">
We would like to thank Rohit J. Kate and anony-
mous reviewers for their insightful comments.
This research was supported by Defense Ad-
</bodyText>
<page confidence="0.993123">
269
</page>
<bodyText confidence="0.9264915">
vanced Research Projects Agency under grant
HR0011-04-1-0007.
</bodyText>
<sectionHeader confidence="0.992885" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999512538461539">
Xavier Carreras and Luis M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proc. of 9th Conf. on Computational
Natural Language Learning (CoNLL-2005), pages
152–164, Ann Arbor, MI, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 43nd Annual Meeting
of the Association for Computational Linguistics
(ACL-05), pages 173–180, Ann Arbor, MI, June.
Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros
Kapetanakis, Kostas Kostiadis, Johan Kummeneje,
Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-
fens, Yi Wang, and Xiang Yin. 2003. Users
manual: RoboCup soccer server manual for soccer
server version 7.07 and later. Available at http://
sourceforge.net/projects/sserver/.
Michael J. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of the 35th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-97), pages 16–23.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. of 17th Intl. Conf.
on Machine Learning (ICML-2000), pages 175–182,
Stanford, CA, June.
Michael Collins. 2002a. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conf. on Empirical Methods in Natural Lan-
guage Processing (EMNLP-02), Philadelphia, PA,
July.
Michael Collins. 2002b. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proc. of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 263–270,
Philadelphia, PA, July.
Michael Collins. 2002c. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2002), pages 489–496, Philadelphia, PA.
Brooke Cowan and Michael Collins. 2005. Mor-
phology and reranking for the statistical parsing of
Spanish. In Proc. of the Human Language Technol-
ogy Conf. and Conf. on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP-05), Van-
couver, B.C., Canada, October.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proc. of 9th Conf. on Computational
Natural Language Learning (CoNLL-2005), pages
9–16, Ann Arbor, MI, July.
Daniel Gildea and Daniel Jurafsky. 2002. Automated
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Nancy A. Ide and Jean´eronis. 1998. Introduction to
the special issue on word sense disambiguation: The
state of the art. Computational Linguistics, 24(1):1–
40.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proc. of 8th ACM
SIGKDD Intl. Conf. on Knowledge Discovery and
Data Mining (KDD-2002), Edmonton, Canada.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall, Upper
Saddle River, NJ.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Proc. of 20th Natl. Conf. on Artificial Intelli-
gence (AAAI-2005), pages 1062–1068, Pittsburgh,
PA, July.
Gregory Kuhlmann, Peter Stone, Raymond J. Mooney,
and Jude W. Shavlik. 2004. Guiding a reinforce-
ment learner with natural language advice: Initial
results in RoboCup soccer. In Proc. of the AAAI-04
Workshop on Supervisory Control of Learning and
Adaptive Systems, San Jose, CA, July.
Iddo Lev, Bill MacCartney, Christopher D. Manning,
and Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proc. of
2nd Workshop on Text Meaning and Interpretation,
ACL-04, Barcelona, Spain.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proc. of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), Ann Arbor, MI, June.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of 13th Natl. Conf. on Artifi-
cial Intelligence (AAAI-96), pages 1050–1055, Port-
land, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of 21th Conf. on Uncertainty in
Artificial Intelligence (UAI-2005), Edinburgh, Scot-
land, July.
Victor W. Zue and James R. Glass. 2000. Conversa-
tional interfaces: Advances and challenges. In Proc.
of the IEEE, volume 88(8), pages 1166–1180.
</reference>
<page confidence="0.996804">
270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989964">
<title confidence="0.99974">Discriminative Reranking for Semantic Parsing</title>
<author confidence="0.999953">Ruifang Ge Raymond J Mooney</author>
<affiliation confidence="0.999945">Department of Computer Sciences University of Texas at Austin</affiliation>
<address confidence="0.995178">Austin, TX 78712</address>
<abstract confidence="0.999785260869565">Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. In this paper, we investigate discriminative reranking upon a basesemantic parser, where the composition of meaning representations is guided by syntax. We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a naturallanguage database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, MI,</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Luis M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005), pages 152–164, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2793" citStr="Charniak and Johnson, 2005" startWordPosition="415" endWordPosition="418">ng representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental results on two corpora: an interpr</context>
<context position="21547" citStr="Charniak and Johnson (2005)" startWordPosition="3541" endWordPosition="3544">more sentences reranked correct than GEOQUERY. On CLANG, using the semantic feature set alone achieves the best improvements over the baseline with 2.8% absolute improvement in F-measure (15.8% relative error reduction), which is significant at the 95% confidence level using a paired Student’s ttest. Nevertheless, the difference between SEM1 and SYN+SEM1 is very small (only one example). Using syntactic features alone only slightly improves the results because the syntactic features do not directly discriminate between correct and incorrect meaning representations. To put this in perspective, Charniak and Johnson (2005) reported that reranking improves the F-measure of syntactic parsing from 89.7% to 91.0% with a 50- best oracle F-measure score of 96.8%. Table 5 compares results using semantic features directly derived from SAPTs (SEM1), and from trees with purely-syntactic nodes removed (SEM2). It compares reranking models using these 268 CLANG GEOQUERY P R F P R F SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4 SYN 87.7 78.7 83.0 95.5 77.2 85.4 SEM1 90.0(23.1) 80.7(12.3) 85.1(15.8) 95.5 76.8 85.1 SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9 Table 4: Reranking results on CLANG and GEOQUERY using different feature sets der</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proc. of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 173–180, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mao Chen</author>
</authors>
<title>Ehsan Foroughi, Fredrik Heintz, Spiros Kapetanakis, Kostas Kostiadis, Johan Kummeneje,</title>
<date>2003</date>
<location>Itsuki Noda, Oliver Obst, Patrick</location>
<marker>Chen, 2003</marker>
<rawString>Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros Kapetanakis, Kostas Kostiadis, Johan Kummeneje, Itsuki Noda, Oliver Obst, Patrick Riley, Timo Steffens, Yi Wang, and Xiang Yin. 2003. Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http:// sourceforge.net/projects/sserver/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics (ACL-97),</booktitle>
<pages>16--23</pages>
<marker>Collins, 1997</marker>
<rawString>Michael J. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics (ACL-97), pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. of 17th Intl. Conf. on Machine Learning (ICML-2000),</booktitle>
<pages>175--182</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="2748" citStr="Collins, 2000" startWordPosition="411" endWordPosition="412"> where the composition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present ex</context>
<context position="10903" citStr="Collins (2000)" startWordPosition="1765" endWordPosition="1766">rceptron, a variant of the perceptron algorithm is often used in testing to decrease generalization errors on unseen test examples, where the parameter vectors used in testing is the average of each parameter vector generated during the training process. 3 Features for Reranking SAPTs In our setting, reranking models discriminate between SAPTs that can lead to correct MRs and those that can not. Intuitively, both syntactic and semantic features describing the syntactic and semantic substructures of a SAPT would be good indicators of the SAPT’s correctness. The syntactic features introduced by Collins (2000) for reranking syntactic parse trees have been proven successfully in both English and Spanish (Cowan and Collins, 2005). We examine if these syntactic features can be adapted for semantic parsing by creating similar semantic features. In the following section, we first briefly describe the syntactic features introduced by Collins (2000), and then introduce two adapted semantic feature sets. A SAPT in CLANG is shown in Figure 3 for illustrating the features throughout this section. 265 VP-ACTION.PASS VB VP-ACTION.PASS be VBN-ACTION.PASS PP-POINT passed TO NP-POINT to PRN-POINT -LRB–POINT NP-NU</context>
<context position="13437" citStr="Collins (2000)" startWordPosition="2186" endWordPosition="2187">the syntactic label above a rule. The example in Figure 3 has the feature f([PRN—* -LRB- NP COMMA NP -RRB-], NP)=1, where NP is the syntactic label above the rule “PRN—* -LRB- NP COMMA NP -RRB-”. 4. Grandparent Bigrams. These are the same as Bigrams, but also include the syntactic label above the constituent containing a bigram. The example in Figure 3 has the feature f([NP COMMA, right, PRN], NP)=1, where NP is the syntactic label above the constituent PRN. 3.2 Semantic Features 3.2.1 Semantic Feature Set I A similar semantic feature type is introduced for each syntactic feature type used by Collins (2000) by replacing syntactic labels with semantic ones (with the semantic label NULL not included). The corresponding semantic feature types for the features in Section 3.1 are: 1. Rules. The example in Figure 3 has the feature f(POINT—* POINT NUM1 NUM2)=1. 2. Bigrams. The example in Figure 3 has the feature f(NUM1 NUM2, right, POINT)=1, where the bigram “NUM1 NUM2”appears to the right of the head POINT. 3. Grandparent Rules. The example in Figure 3 has the feature f([POINT—* POINT NUM1 NUM2], POINT)=1, where the last POINT is 266 ACTION.PASS Figure 4: The tree generated by removing purelysyntactic</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. of 17th Intl. Conf. on Machine Learning (ICML-2000), pages 175–182, Stanford, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 Conf. on Empirical Methods in Natural Language Processing (EMNLP-02),</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2763" citStr="Collins, 2002" startWordPosition="413" endWordPosition="414">osition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental resu</context>
<context position="8967" citStr="Collins, 2002" startWordPosition="1423" endWordPosition="1424">so limits its recall. To generate plenty of candidate SAPTs for reranking, we extended the back-off levels for the parameters generating semantic labels of modifiers. The new set is shown in Table 1 using the parameters for the generation of the left-side modifiers as an example. The back-off levels 4 and 5 are newly added by removing the constraints from the semantic subcat. Although the best SAPTs found by the model may not be as precise as before, we expect that reranking can improve the results and rank correct SAPTs higher. 2.3 The Averaged Perceptron Reranking Model Averaged perceptron (Collins, 2002a) has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), and in this paper, we employed it in reranking semantic parses generated by the base semantic parser SCISSOR. The model is composed of three parts (Collins, 2002a): a set of candidate SAPTs GEN, which is the top n SAPTs of a sentence from SCISSOR; a function 4b that maps a sentence Inputs: A set of training examples (xi, yz ), i = 1...n, where xi is a sentence, and yz is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization: Set W = 0 Alg</context>
<context position="10270" citStr="Collins, 2002" startWordPosition="1667" endWordPosition="1668"> then W = W + 4)(xi, yi ) − `F(xi, yi) Output: The parameter vector W� Figure 2: The perceptron training algorithm. x and its SAPT y into a feature vector 4b(x, y) E Rd; and a weight vector W associated with the set of features. Each feature in a feature vector is a function on a SAPT that maps the SAPT to a real value. The SAPT with the highest score under a parameter vector W is outputted, where the score is calculated as: score(x, y) = -b(x, y) · W (1) The perceptron training algorithm for estimating the parameter vector W is shown in Figure 2. For a full description of the algorithm, see (Collins, 2002a). The averaged perceptron, a variant of the perceptron algorithm is often used in testing to decrease generalization errors on unseen test examples, where the parameter vectors used in testing is the average of each parameter vector generated during the training process. 3 Features for Reranking SAPTs In our setting, reranking models discriminate between SAPTs that can lead to correct MRs and those that can not. Intuitively, both syntactic and semantic features describing the syntactic and semantic substructures of a SAPT would be good indicators of the SAPT’s correctness. The syntactic feat</context>
<context position="17876" citStr="Collins, 2002" startWordPosition="2917" endWordPosition="2918">n the reranking model trained with the above method was used to choose a best SAPT from the candidate SAPTs. The performance of semantic parsing was measured in terms ofprecision (the percentage of completed MRs that were correct), recall (the percentage of all sentences whose MRs were correctly generated) and F-measure (the harmonic mean of precision and recall). Since even a single mistake in an MR could totally change the meaning of an example (e.g. having OUR in an MR instead of OPPONENT in CLANG), no partial credit was given for examples with partially-correct SAPTs. Averaged perceptron (Collins, 2002a), which has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), was employed for training rerankACTION.PASS passed POINT NUM1 NUM 36 POINT ( NUM2 NUM 10 267 CLANG GEOQUERY P R F P R F SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8 SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4 Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in bold), where P = precision, R = recall, and F = F-measure. n 1 2 5 10 20 50 CLANG 78.0 81.3 83.0 84.0 85.0 85.3 GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6 Table 3: Oracle recalls on C</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002a. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of the 2002 Conf. on Empirical Methods in Natural Language Processing (EMNLP-02), Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>263--270</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2763" citStr="Collins, 2002" startWordPosition="413" endWordPosition="414">osition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental resu</context>
<context position="8967" citStr="Collins, 2002" startWordPosition="1423" endWordPosition="1424">so limits its recall. To generate plenty of candidate SAPTs for reranking, we extended the back-off levels for the parameters generating semantic labels of modifiers. The new set is shown in Table 1 using the parameters for the generation of the left-side modifiers as an example. The back-off levels 4 and 5 are newly added by removing the constraints from the semantic subcat. Although the best SAPTs found by the model may not be as precise as before, we expect that reranking can improve the results and rank correct SAPTs higher. 2.3 The Averaged Perceptron Reranking Model Averaged perceptron (Collins, 2002a) has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), and in this paper, we employed it in reranking semantic parses generated by the base semantic parser SCISSOR. The model is composed of three parts (Collins, 2002a): a set of candidate SAPTs GEN, which is the top n SAPTs of a sentence from SCISSOR; a function 4b that maps a sentence Inputs: A set of training examples (xi, yz ), i = 1...n, where xi is a sentence, and yz is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization: Set W = 0 Alg</context>
<context position="10270" citStr="Collins, 2002" startWordPosition="1667" endWordPosition="1668"> then W = W + 4)(xi, yi ) − `F(xi, yi) Output: The parameter vector W� Figure 2: The perceptron training algorithm. x and its SAPT y into a feature vector 4b(x, y) E Rd; and a weight vector W associated with the set of features. Each feature in a feature vector is a function on a SAPT that maps the SAPT to a real value. The SAPT with the highest score under a parameter vector W is outputted, where the score is calculated as: score(x, y) = -b(x, y) · W (1) The perceptron training algorithm for estimating the parameter vector W is shown in Figure 2. For a full description of the algorithm, see (Collins, 2002a). The averaged perceptron, a variant of the perceptron algorithm is often used in testing to decrease generalization errors on unseen test examples, where the parameter vectors used in testing is the average of each parameter vector generated during the training process. 3 Features for Reranking SAPTs In our setting, reranking models discriminate between SAPTs that can lead to correct MRs and those that can not. Intuitively, both syntactic and semantic features describing the syntactic and semantic substructures of a SAPT would be good indicators of the SAPT’s correctness. The syntactic feat</context>
<context position="17876" citStr="Collins, 2002" startWordPosition="2917" endWordPosition="2918">n the reranking model trained with the above method was used to choose a best SAPT from the candidate SAPTs. The performance of semantic parsing was measured in terms ofprecision (the percentage of completed MRs that were correct), recall (the percentage of all sentences whose MRs were correctly generated) and F-measure (the harmonic mean of precision and recall). Since even a single mistake in an MR could totally change the meaning of an example (e.g. having OUR in an MR instead of OPPONENT in CLANG), no partial credit was given for examples with partially-correct SAPTs. Averaged perceptron (Collins, 2002a), which has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), was employed for training rerankACTION.PASS passed POINT NUM1 NUM 36 POINT ( NUM2 NUM 10 267 CLANG GEOQUERY P R F P R F SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8 SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4 Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in bold), where P = precision, R = recall, and F = F-measure. n 1 2 5 10 20 50 CLANG 78.0 81.3 83.0 84.0 85.0 85.3 GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6 Table 3: Oracle recalls on C</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002b. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 263–270, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>489--496</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2763" citStr="Collins, 2002" startWordPosition="413" endWordPosition="414">osition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental resu</context>
<context position="8967" citStr="Collins, 2002" startWordPosition="1423" endWordPosition="1424">so limits its recall. To generate plenty of candidate SAPTs for reranking, we extended the back-off levels for the parameters generating semantic labels of modifiers. The new set is shown in Table 1 using the parameters for the generation of the left-side modifiers as an example. The back-off levels 4 and 5 are newly added by removing the constraints from the semantic subcat. Although the best SAPTs found by the model may not be as precise as before, we expect that reranking can improve the results and rank correct SAPTs higher. 2.3 The Averaged Perceptron Reranking Model Averaged perceptron (Collins, 2002a) has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), and in this paper, we employed it in reranking semantic parses generated by the base semantic parser SCISSOR. The model is composed of three parts (Collins, 2002a): a set of candidate SAPTs GEN, which is the top n SAPTs of a sentence from SCISSOR; a function 4b that maps a sentence Inputs: A set of training examples (xi, yz ), i = 1...n, where xi is a sentence, and yz is a candidate SAPT that has the highest similarity score with the gold-standard SAPT Initialization: Set W = 0 Alg</context>
<context position="10270" citStr="Collins, 2002" startWordPosition="1667" endWordPosition="1668"> then W = W + 4)(xi, yi ) − `F(xi, yi) Output: The parameter vector W� Figure 2: The perceptron training algorithm. x and its SAPT y into a feature vector 4b(x, y) E Rd; and a weight vector W associated with the set of features. Each feature in a feature vector is a function on a SAPT that maps the SAPT to a real value. The SAPT with the highest score under a parameter vector W is outputted, where the score is calculated as: score(x, y) = -b(x, y) · W (1) The perceptron training algorithm for estimating the parameter vector W is shown in Figure 2. For a full description of the algorithm, see (Collins, 2002a). The averaged perceptron, a variant of the perceptron algorithm is often used in testing to decrease generalization errors on unseen test examples, where the parameter vectors used in testing is the average of each parameter vector generated during the training process. 3 Features for Reranking SAPTs In our setting, reranking models discriminate between SAPTs that can lead to correct MRs and those that can not. Intuitively, both syntactic and semantic features describing the syntactic and semantic substructures of a SAPT would be good indicators of the SAPT’s correctness. The syntactic feat</context>
<context position="17876" citStr="Collins, 2002" startWordPosition="2917" endWordPosition="2918">n the reranking model trained with the above method was used to choose a best SAPT from the candidate SAPTs. The performance of semantic parsing was measured in terms ofprecision (the percentage of completed MRs that were correct), recall (the percentage of all sentences whose MRs were correctly generated) and F-measure (the harmonic mean of precision and recall). Since even a single mistake in an MR could totally change the meaning of an example (e.g. having OUR in an MR instead of OPPONENT in CLANG), no partial credit was given for examples with partially-correct SAPTs. Averaged perceptron (Collins, 2002a), which has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), was employed for training rerankACTION.PASS passed POINT NUM1 NUM 36 POINT ( NUM2 NUM 10 267 CLANG GEOQUERY P R F P R F SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8 SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4 Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in bold), where P = precision, R = recall, and F = F-measure. n 1 2 5 10 20 50 CLANG 78.0 81.3 83.0 84.0 85.0 85.3 GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6 Table 3: Oracle recalls on C</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002c. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 489–496, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Michael Collins</author>
</authors>
<title>Morphology and reranking for the statistical parsing of Spanish.</title>
<date>2005</date>
<booktitle>In Proc. of the Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP-05),</booktitle>
<location>Vancouver, B.C., Canada,</location>
<contexts>
<context position="11023" citStr="Cowan and Collins, 2005" startWordPosition="1781" endWordPosition="1784">unseen test examples, where the parameter vectors used in testing is the average of each parameter vector generated during the training process. 3 Features for Reranking SAPTs In our setting, reranking models discriminate between SAPTs that can lead to correct MRs and those that can not. Intuitively, both syntactic and semantic features describing the syntactic and semantic substructures of a SAPT would be good indicators of the SAPT’s correctness. The syntactic features introduced by Collins (2000) for reranking syntactic parse trees have been proven successfully in both English and Spanish (Cowan and Collins, 2005). We examine if these syntactic features can be adapted for semantic parsing by creating similar semantic features. In the following section, we first briefly describe the syntactic features introduced by Collins (2000), and then introduce two adapted semantic feature sets. A SAPT in CLANG is shown in Figure 3 for illustrating the features throughout this section. 265 VP-ACTION.PASS VB VP-ACTION.PASS be VBN-ACTION.PASS PP-POINT passed TO NP-POINT to PRN-POINT -LRB–POINT NP-NUM1 COMMA NP-NUM2 -RRB( CD-NUM , CD-NUM ) 36 10 Figure 3: A SAPT for illustrating the reranking features, where the synta</context>
</contexts>
<marker>Cowan, Collins, 2005</marker>
<rawString>Brooke Cowan and Michael Collins. 2005. Morphology and reranking for the statistical parsing of Spanish. In Proc. of the Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing (HLT/EMNLP-05), Vancouver, B.C., Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="1588" citStr="Ge and Mooney, 2005" startWordPosition="229" endWordPosition="232">king can improve the performance on the coaching interpreter, but not on the database interface. 1 Introduction A long-standing challenge within natural language processing has been to understand the meaning of natural language sentences. In comparison with shallow semantic analysis tasks, such as wordsense disambiguation (Ide and Jean´eronis, 1998) and semantic role labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is </context>
<context position="7579" citStr="Ge and Mooney (2005)" startWordPosition="1178" endWordPosition="1181">ate the generation of semantic labels on internal nodes. In a head-driven parsing model, a tree can be seen as generated by expanding non-terminals with grammar rules recursively. To deal with the sparse data problem, the expansion of a non-terminal (parent) is decomposed into primitive steps: a child is chosen as the head and is generated first, and then the other children (modifiers) are generated independently 264 BACK-OFFLEVEL PL,(Li|...) 1 P,H,w,t,A,LC 2 P,H,t,A,LC 3 P,H,A,LC 4 P,H 5 P Table 1: Extended back-off levels for the semantic parameter PL,(Li|...), using the same notation as in Ge and Mooney (2005). The symbols P, H and Li are the semantic label of the parent, head, and the ith left child, w is the head word of the parent, t is the semantic label of the head word, S is the distance between the head and the modifier, and LC is the left semantic subcat. constrained by the head. Here, we only describe changes made to SCISSOR for reranking, for a full description of SCISSOR see Ge and Mooney (2005). In SCISSOR, the generation of semantic labels on modifiers are constrained by semantic subcategorization frames, for which data can be very sparse. An example of a semantic subcat in Figure 1 is</context>
<context position="19495" citStr="Ge and Mooney (2005)" startWordPosition="3202" endWordPosition="3205">correct SAPT. Rerankers were trained on the 50- best SAPTs provided by SCISSOR, and the number of perceptron iterations over the training examples was limited to 10. Typically, in order to avoid over-fitting, reranking features are filtered by removing those occurring in less than some minimal number of training examples. We only removed features that never occurred in the training data since experiments with higher cut-offs failed to show any improvements. 4.2 Results 4.2.1 Baseline Results Table 2 shows the results comparing the baseline learner SCISSOR using both the back-off parameters in Ge and Mooney (2005) (SCISSOR) and the revised parameters in Section 2.2 (SCISSOR+). As we expected, SCISSOR+ has better recall and worse precision than SCISSOR on both corpora due to the additional levels of back-off. SCISSOR+ is used as the baseline model for all reranking experiments in the next section. Table 3 gives oracle recalls for CLANG and GEOQUERY where an oracle picks the correct parse from the n-best SAPTs if any of them are correct. Results are shown for increasing values of n. The trends for CLANG and GEOQUERY are different: small values of n show significant improvements for CLANG, while a larger </context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005), pages 9–16, Ann Arbor, MI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automated labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1374" citStr="Gildea and Jurafsky, 2002" startWordPosition="196" endWordPosition="199">ping between syntax and semantics. We report experimental results on two real applications, an interpreter for coaching instructions in robotic soccer and a naturallanguage database interface. The results show that reranking can improve the performance on the coaching interpreter, but not on the database interface. 1 Introduction A long-standing challenge within natural language processing has been to understand the meaning of natural language sentences. In comparison with shallow semantic analysis tasks, such as wordsense disambiguation (Ide and Jean´eronis, 1998) and semantic role labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understand</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automated labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy A Ide</author>
<author>Jean´eronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>40</pages>
<marker>Ide, Jean´eronis, 1998</marker>
<rawString>Nancy A. Ide and Jean´eronis. 1998. Introduction to the special issue on word sense disambiguation: The state of the art. Computational Linguistics, 24(1):1– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>Proc. of 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD-2002),</booktitle>
<location>Edmonton, Canada.</location>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. of 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD-2002), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, Upper Saddle River,</title>
<date>2000</date>
<location>NJ.</location>
<contexts>
<context position="5242" citStr="Jurafsky and Martin, 2000" startWordPosition="792" endWordPosition="795">in our half.” 2.1.2 GEOQUERY: a DB Query Language GEOQUERY is a logical query language for a small database of U.S. geography containing about 800 facts. The GEOQUERY language consists of Prolog queries augmented with several meta-predicates (Zelle and Mooney, 1996). Negation and quantifiers like all and each are included in the language. Below is a sample query with its English gloss: answer(A,count(B,(city(B),loc(B,C), const(C,countryid(usa))),A)) “How many cities are there in the US?” 2.2 SCISSOR: the Baseline Model SCISSOR is based on a fairly standard approach to compositional semantics (Jurafsky and Martin, 2000). First, a statistical parser is used to construct a semantically-augmented parse tree that captures the semantic interpretation of individual NP-PLAYER PRP$-TEAM NN-PLAYER CD-UNUM our player 2 Figure 1: A SAPT for describing a simple CLANG concept PLAYER. words and the basic predicate-argument structure of a sentence. Next, a recursive deterministic procedure is used to compose the MR of a parent node from the MR of its children following the tree structure. Figure 1 shows the SAPT for a simple natural language phrase describing the concept PLAYER in CLANG. We can see that each internal node </context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proc. of 20th Natl. Conf. on Artificial Intelligence (AAAI-2005),</booktitle>
<pages>1062--1068</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="1567" citStr="Kate et al., 2005" startWordPosition="225" endWordPosition="228">lts show that reranking can improve the performance on the coaching interpreter, but not on the database interface. 1 Introduction A long-standing challenge within natural language processing has been to understand the meaning of natural language sentences. In comparison with shallow semantic analysis tasks, such as wordsense disambiguation (Ide and Jean´eronis, 1998) and semantic role labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meanin</context>
<context position="16037" citStr="Kate et al., 2005" startWordPosition="2604" endWordPosition="2607">for the SAPT in Figure 3 is shown in Figure 4). In this tree representation, the example in Figure 4 would have the Grandparent Rule feature f([POINT—* POINT NUM1 NUM2], ACTION.PASS)=1, with the correct semantic grandparent ACTION.PASS included. 4 Experimental Evaluation 4.1 Experimental Methodology Two corpora of natural language sentences paired with MRs were used in the reranking experiments. For CLANG, 300 pieces of coaching advice were randomly selected from the log files of the 2003 RoboCup Coach Competition. Each formal instruction was translated into English by one of four annotators (Kate et al., 2005). The average length of an natural language sentence in this corpus is 22.52 words. For GEOQUERY, 250 questions were collected by asking undergraduate students to generate English queries for the given database. Queries were then manually translated into logical form (Zelle and Mooney, 1996). The average length of a natural language sentence in this corpus is 6.87 words. We adopted standard 10-fold cross validation for evaluation: 9/10 of the whole dataset was used for training (training set), and 1/10 for testing (test set). To train a reranking model on a training set, a separate “internal” </context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Proc. of 20th Natl. Conf. on Artificial Intelligence (AAAI-2005), pages 1062–1068, Pittsburgh, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Kuhlmann</author>
<author>Peter Stone</author>
<author>Raymond J Mooney</author>
<author>Jude W Shavlik</author>
</authors>
<title>Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer.</title>
<date>2004</date>
<booktitle>In Proc. of the AAAI-04 Workshop on Supervisory Control of Learning and Adaptive Systems,</booktitle>
<location>San Jose, CA,</location>
<contexts>
<context position="2080" citStr="Kuhlmann et al., 2004" startWordPosition="306" endWordPosition="309">ing the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While r</context>
</contexts>
<marker>Kuhlmann, Stone, Mooney, Shavlik, 2004</marker>
<rawString>Gregory Kuhlmann, Peter Stone, Raymond J. Mooney, and Jude W. Shavlik. 2004. Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer. In Proc. of the AAAI-04 Workshop on Supervisory Control of Learning and Adaptive Systems, San Jose, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iddo Lev</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
<author>Roger Levy</author>
</authors>
<title>Solving logic puzzles: From robust processing to precise semantics.</title>
<date>2004</date>
<booktitle>In Proc. of 2nd Workshop on Text Meaning and Interpretation, ACL-04,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2038" citStr="Lev et al., 2004" startWordPosition="299" endWordPosition="302">ially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which e</context>
</contexts>
<marker>Lev, MacCartney, Manning, Levy, 2004</marker>
<rawString>Iddo Lev, Bill MacCartney, Christopher D. Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Proc. of 2nd Workshop on Text Meaning and Interpretation, ACL-04, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2851" citStr="Toutanova et al., 2005" startWordPosition="423" endWordPosition="426">arser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model SCISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental results on two corpora: an interpreter for coaching instructions 263 Proceedings of the COLI</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proc. of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL-05), Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. of 13th Natl. Conf. on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR,</location>
<contexts>
<context position="4882" citStr="Zelle and Mooney, 1996" startWordPosition="740" endWordPosition="743">n Chen et al. (2003), its grammar consists of 37 non-terminal symbols and 133 productions. Negation and quantifiers like all are included in the language. Below is a sample rule with its English gloss: ((bpos (penalty-area our)) (do (player-except our {4}) (pos (half our)))) “If the ball is in our penalty area, all our players except player 4 should stay in our half.” 2.1.2 GEOQUERY: a DB Query Language GEOQUERY is a logical query language for a small database of U.S. geography containing about 800 facts. The GEOQUERY language consists of Prolog queries augmented with several meta-predicates (Zelle and Mooney, 1996). Negation and quantifiers like all and each are included in the language. Below is a sample query with its English gloss: answer(A,count(B,(city(B),loc(B,C), const(C,countryid(usa))),A)) “How many cities are there in the US?” 2.2 SCISSOR: the Baseline Model SCISSOR is based on a fairly standard approach to compositional semantics (Jurafsky and Martin, 2000). First, a statistical parser is used to construct a semantically-augmented parse tree that captures the semantic interpretation of individual NP-PLAYER PRP$-TEAM NN-PLAYER CD-UNUM our player 2 Figure 1: A SAPT for describing a simple CLANG</context>
<context position="16329" citStr="Zelle and Mooney, 1996" startWordPosition="2651" endWordPosition="2654">l Methodology Two corpora of natural language sentences paired with MRs were used in the reranking experiments. For CLANG, 300 pieces of coaching advice were randomly selected from the log files of the 2003 RoboCup Coach Competition. Each formal instruction was translated into English by one of four annotators (Kate et al., 2005). The average length of an natural language sentence in this corpus is 22.52 words. For GEOQUERY, 250 questions were collected by asking undergraduate students to generate English queries for the given database. Queries were then manually translated into logical form (Zelle and Mooney, 1996). The average length of a natural language sentence in this corpus is 6.87 words. We adopted standard 10-fold cross validation for evaluation: 9/10 of the whole dataset was used for training (training set), and 1/10 for testing (test set). To train a reranking model on a training set, a separate “internal” 10-fold cross validation over the training set was employed to generate n-best SAPTs for each training example using a baseline learner, where each training set was again separated into 10 folds with 9/10 for training the baseline learner, and 1/10 for producing the nbest SAPTs for training </context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. of 13th Natl. Conf. on Artificial Intelligence (AAAI-96), pages 1050–1055, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI-2005),</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1620" citStr="Zettlemoyer and Collins, 2005" startWordPosition="233" endWordPosition="236">performance on the coaching interpreter, but not on the database interface. 1 Introduction A long-standing challenge within natural language processing has been to understand the meaning of natural language sentences. In comparison with shallow semantic analysis tasks, such as wordsense disambiguation (Ide and Jean´eronis, 1998) and semantic role labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax. First, a stati</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI-2005), Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor W Zue</author>
<author>James R Glass</author>
</authors>
<title>Conversational interfaces: Advances and challenges.</title>
<date>2000</date>
<booktitle>In Proc. of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<pages>1166--1180</pages>
<contexts>
<context position="1999" citStr="Zue and Glass, 2000" startWordPosition="293" endWordPosition="296">reras and M`arquez, 2005), which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates, semantic parsing (Kate et al., 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005) pursues a more ambitious goal – mapping natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by </context>
</contexts>
<marker>Zue, Glass, 2000</marker>
<rawString>Victor W. Zue and James R. Glass. 2000. Conversational interfaces: Advances and challenges. In Proc. of the IEEE, volume 88(8), pages 1166–1180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>