<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.998273">
Perceptron Reranking for CCG Realization
</title>
<author confidence="0.99323">
Michael White and Rajakrishnan Rajkumar
</author>
<affiliation confidence="0.9930395">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.92418">
Columbus, OH, USA
</address>
<email confidence="0.999809">
{mwhite,raja}@ling.osu.edu
</email>
<sectionHeader confidence="0.994824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901722222222">
This paper shows that discriminative
reranking with an averaged perceptron
model yields substantial improvements in
realization quality with CCG. The paper
confirms the utility of including language
model log probabilities as features in the
model, which prior work on discrimina-
tive training with log linear models for
HPSG realization had called into question.
The perceptron model allows the combina-
tion of multiple n-gram models to be opti-
mized and then augmented with both syn-
tactic features and discriminative n-gram
features. The full model yields a state-
of-the-art BLEU score of 0.8506 on Sec-
tion 23 of the CCGbank, to our knowledge
the best score reported to date using a re-
versible, corpus-engineered grammar.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978">
In this paper, we show how discriminative train-
ing with averaged perceptron models (Collins,
2002) can be used to substantially improve surface
realization with Combinatory Categorial Gram-
mar (Steedman, 2000, CCG). Velldal and Oepen
(2005) and Nakanishi et al. (2005) have shown that
discriminative training with log-linear (maximum
entropy) models is effective in realization rank-
ing with Head-Driven Phrase Structure Grammar
(Pollard and Sag, 1994, HPSG). Here we show
that averaged perceptron models also perform well
for realization ranking with CCG. Averaged per-
ceptron models are very simple, just requiring a
decoder and a simple update function, yet despite
their simplicity they have been shown to achieve
state-of-the-art results in Treebank and CCG pars-
ing (Huang, 2008; Clark and Curran, 2007a) as
well as on other NLP tasks.
Along the way, we address the question of
whether it is beneficial to incorporate n-gram log
probabilities as baseline features in a discrimina-
tively trained realization ranking model. On a lim-
ited domain corpus, Velldal &amp; Oepen found that
including the n-gram log probability of each can-
didate realization as a feature in their log-linear
model yielded a substantial boost in ranking per-
formance; on the Penn Treebank (PTB), however,
Nakanishi et al. found that including an n-gram log
prob feature in their model was of no benefit (with
the use of bigrams instead of 4-grams suggested as
a possible explanation). With these mixed results,
the utility of n-gram baseline features for PTB-
scale discriminative realization ranking has been
unclear. In our particular setting, the question is:
Do n-gram log prob features improve performance
in broad coverage realization ranking with CCG,
where factored language models over words, part-
of-speech tags and supertags have previously been
employed (White et al., 2007; Espinosa et al.,
2008)?
We answer this question in the affirmative, con-
firming the results of Velldal &amp; Oepen, despite
the differences in corpus size and kind of lan-
guage model. We show that including n-gram log
prob features in the perceptron model is highly
beneficial, as the discriminative models we tested
without these features performed worse than the
generative baseline. These findings are in line
with Collins &amp; Roark’s (2004) results with incre-
mental parsing with perceptrons, where it is sug-
gested that a generative baseline feature provides
the perceptron algorithm with a much better start-
ing point for learning. We also show that discrim-
inative training allows the combination of multi-
ple n-gram models to be optimized, and that the
best model augments the n-gram log prob fea-
tures with both syntactic features and discrimina-
tive n-gram features. The full model yields a state-
of-the-art BLEU (Papineni et al., 2002) score of
0.8506 on Section 23 of the CCGbank, which is
to our knowledge the best score reported to date
</bodyText>
<page confidence="0.963016">
410
</page>
<note confidence="0.996665">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410–419,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.994672583333333">
using a reversible, corpus-engineered grammar.
The paper is organized as follows. Section 2 re-
views previous work on broad coverage realization
with OpenCCG. Section 3 describes our approach
to realization reranking with averaged perceptron
models. Section 4 presents our evaluation of the
perceptron models, comparing the results of dif-
ferent feature sets. Section 5 compares our results
to those obtained by related systems and discusses
the difficulties of cross-system comparisons. Fi-
nally, Section 6 concludes with a summary and
discussion of future directions for research.
</bodyText>
<sectionHeader confidence="0.997168" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.981826">
2.1 Surface Realization with CCG
</subsectionHeader>
<bodyText confidence="0.990793621621622">
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that encode
sub-categorization information as well as syntactic
feature information (e.g. number and agreement).
Complementing function application as the stan-
dard means of combining a head with its argu-
ment, type-raising and composition support trans-
parent analyses for a wide range of phenomena,
including right-node raising and long distance de-
pendencies. An example syntactic derivation ap-
pears in Figure 1, with a long-distance depen-
dency between point and make. Semantic com-
position happens in parallel with syntactic compo-
sition, which makes it attractive for generation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Surface realization is the process by which logical
forms are transduced to strings. OpenCCG uses
a hybrid symbolic-statistical chart realizer (White,
2006) which takes logical forms as input and pro-
duces sentences by using CCG combinators to
combine signs. Edges are grouped into equiva-
lence classes when they have the same syntactic
category and cover the same parts of the input log-
ical form. Alternative realizations are ranked us-
ing integrated n-gram or perceptron scoring, and
pruning takes place within equivalence classes of
edges. To more robustly support broad coverage
surface realization, OpenCCG greedily assembles
fragments in the event that the realizer fails to find
a complete realization.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 2. In
s[dcl]\np/np
</bodyText>
<figureCaption confidence="0.875437">
Figure 2: Semantic dependency graph from the
</figureCaption>
<bodyText confidence="0.9962365">
CCGbank for He has a point he wants to make
[... ], along with gold-standard supertags (cate-
gory labels)
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features
(e.g. (NUM)sg); nodes are connected via depen-
dency relations (e.g. (ARG0)). (Gold-standard su-
pertags, or category labels, are also shown; see
Section 2.4 for their role in hypertagging.) In-
ternally, such graphs are represented using Hy-
brid Logic Dependency Semantics (HLDS), a
dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). In
HLDS, each semantic head (corresponding to a
node in the graph) is associated with a nominal
that identifies its discourse referent, and relations
between heads and their dependents are modeled
as modal relations.
</bodyText>
<subsectionHeader confidence="0.999613">
2.2 Realization from an Enhanced CCGbank
</subsectionHeader>
<bodyText confidence="0.999532461538462">
Our starting point is an enhanced version of the
CCGbank (Hockenmaier and Steedman, 2007)—a
corpus of CCG derivations derived from the Penn
Treebank—with Propbank (Palmer et al., 2005)
roles projected onto it (Boxwell and White, 2008).
To engineer a grammar from this corpus suitable
for realization with OpenCCG, the derivations are
first revised to reflect the lexicalized treatment
of coordination and punctuation assumed by the
multi-modal version of CCG that is implemented
in OpenCCG (White and Rajkumar, 2008). Fur-
ther changes are necessary to support semantic de-
pendencies rather than surface syntactic ones; in
</bodyText>
<figure confidence="0.999164366666667">
np/n
&lt;Arg1&gt;
&lt;Arg0&gt;
np
&lt;Det&gt;
have.03
&lt;TENSE&gt;pres
&lt;Arg0&gt; &lt;Arg1&gt;
h1
n
&lt;Arg1&gt;
s[dcl]\np/(s[to]\np)
he
h2
&lt;NUM&gt;sg
point
P1
want.01
a1
a
W1
&lt;TENSE&gt;pres
M1
make.03
s[b]\np/np
np
h3
he
&lt;Arg0&gt;
&lt;GenRel&gt;
</figure>
<page confidence="0.619477">
411
</page>
<figure confidence="0.991922">
He has a point he wants to make
np sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np
&gt; &gt;T &gt;B
np s/(s\np) sto\np/np
sdcl\np/np
sdcl/np
np\np
&lt;
&gt;
&lt;
np
sdcl\np
sdcl
</figure>
<figureCaption confidence="0.998416">
Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [... ]
</figureCaption>
<figure confidence="0.700757">
&gt;B
&gt;B
</figure>
<bodyText confidence="0.9789942">
particular, the features and unification constraints
in the categories related to semantically empty
function words such complementizers, infinitival-
to, expletive subjects, and case-marking preposi-
tions are adjusted to reflect their purely syntactic
status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with log-
ical forms. Categories and unary type chang-
ing rules (corresponding to zero morphemes) are
sorted by frequency and extracted if they meet the
specified frequency thresholds. A separate trans-
formation then uses a few dozen generalized tem-
plates to add logical forms to the categories, in a
fashion reminiscent of (Bos, 2005). As shown in
Figure 2, numbered semantic roles are taken from
PropBank when available, and more specific rela-
tions are introduced in the categories for closed-
class items such as determiners.
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse
the sentences in the CCGbank according to the
gold-standard derivation. If the derivation can
be successfully followed, the parse yields a log-
ical form which is saved along with the corpus
sentence in order to later test the realizer. Cur-
rently, the algorithm succeeds in creating logical
forms for 98.85% of the sentences in the develop-
ment section (Sect. 00) of the converted CCGbank,
and 97.06% of the sentences in the test section
(Sect. 23). Of these, 95.99% of the development
LFs are semantic dependency graphs with a sin-
gle root, while 95.81% of the test LFs have a sin-
gle root. The remaining cases, with multiple roots,
are missing one or more dependencies required to
form a fully connected graph. Such missing de-
pendencies usually reflect remaining inadequacies
in the logical form templates.
An error analysis of OpenCCG output by Ra-
jkumar et al. (2009) recently revealed that out of
2331 named entities (NEs) annotated by the BBN
corpus (Weischedel and Brunstein, 2005), 238
were not realized correctly. For example, multi-
word NPs like Texas Instruments Japan Ltd. were
realized as Japan Texas Instruments Ltd. Accord-
ingly, inspired by Hogan et al.’s (2007)’s Experi-
ment 1, Rajkumar et al. used the BBN corpus NE
annotation to collapse certain classes of NEs. But
unlike Hogan et al.’s experiment where all the NEs
annotated by the BBN corpus were collapsed, Ra-
jkumar et al. chose to collapse into single tokens
only NEs whose exact form can be reasonably ex-
pected to be specified in the input to the realizer.
For example, while some quantificational or com-
paratives phrases like more than $ 10,000 are an-
notated as MONEY in the BBN corpus, Rajkumar
et al. only collapse $ 10,000 into an atomic unit,
with more than handled compositionally accord-
ing to the semantics assigned to it by the gram-
mar. Thus, after transferring the BBN annotations
to the CCGbank corpus, Rajkumar et al. (partially)
collapsed NEs which are CCGbank constituents
according to the following rules: (1) completely
collapse the PERSON, ORGANIZATION, GPE,
WORK OF ART major class type entitites; (2) ig-
nore phrases like three decades later, which are
annotated as DATE entities; and (3) collapse all
phrases with POS tags CD or NNP(S) or lexical
items % or $, ensuring that all prototypical named
entities are collapsed.
It is worth noting that improvements in our
corpus-based grammar engineering process—
including a more precise treatment of punctuation,
better named entity handling and the addition of
catch-all logical form templates—have resulted in
a 13.5 BLEU point improvement in our baseline
realization scores on Section 00 of the CCGbank,
from a score of 0.6567 in (Espinosa et al., 2008)
to 0.7917 in (Rajkumar et al., 2009), contribut-
ing greatly to the state-of-the-art results reported
</bodyText>
<page confidence="0.993009">
412
</page>
<bodyText confidence="0.9999714">
in Section 4. A further 4.5 point improvement is
obtained from the use of named entity classes in
language modeling and hypertagging (Rajkumar
et al., 2009), as described next, and from our per-
ceptron reranking model, described in Section 3.
</bodyText>
<subsectionHeader confidence="0.998072">
2.3 Factored Language Models
</subsectionHeader>
<bodyText confidence="0.999998083333333">
As in (White et al., 2007; Rajkumar et al., 2009),
we use factored language models (Bilmes and
Kirchhoff, 2003) over words, part-of-speech tags
and supertags1 to score partial and complete real-
izations. The trigram models were created using
the SRILM toolkit (Stolcke, 2002) on the standard
training sections (02–21) of the CCGbank, with
sentence-initial words (other than proper names)
uncapitalized. While these models are consider-
ably smaller than the ones used in (Langkilde-
Geary, 2002; Velldal and Oepen, 2005), the train-
ing data does have the advantage of being in the
same domain and genre. The models employ in-
terpolated Kneser-Ney smoothing with the default
frequency cutoffs. The best performing model
interpolates three component models using rank-
order centroid weights: (1) a word trigram model;
(2) a word model with semantic classes replac-
ing named entities; and (3) a trigram model that
chains a POS model with a supertag model, where
the POS model (P) conditions on the previous two
POS tags, and the supertag model (S) conditions
on the previous two POS tags as well as the current
one, as shown below:
</bodyText>
<equation confidence="0.997027333333333">
pPS(~Fi F~ i−1
i−2 ) = p(Pi Pi−1
i−2 )p(Si I Pii−2) (1)
</equation>
<bodyText confidence="0.971669041666666">
Training data for the semantic class–replaced
model was created by replacing (collapsed) words
with their NE classes, in order to address data spar-
sity issues caused by rare words in the same se-
mantic class. For example, the Section 00 sen-
tence Pierre Vinken , 61 years old, will join the
board as a nonexecutive director Nov. 29 . be-
comes PERSON, DATE:AGE DATE:AGE old,
will join the ORG DESC:OTHER as a nonexecu-
tive PER DESC DATE:DATE DATE:DATE . Dur-
ing realization, word forms are generated, but are
then replaced by their semantic classes for scoring
using the semantic class–replaced model, similar
to Oh and Rudnicky (2002).
Note that the use of supertags in the factored
language model to score possible realizations is
1With CCG, supertags (Bangalore and Joshi, 1999) are
lexical categories considered as fine-grained syntactic labels.
distinct from the prediction of supertags for lexical
category assignment: the former takes the words
in the local context into account (as in supertag-
ging for parsing), while the latter takes features of
the logical form into account. This latter process
we call hypertagging, to which we now turn.
</bodyText>
<subsectionHeader confidence="0.997524">
2.4 Hypertagging
</subsectionHeader>
<bodyText confidence="0.999990470588235">
A crucial component of the OpenCCG realizer is
the hypertagger (Espinosa et al., 2008), or su-
pertagger for surface realization, which uses a
maximum entropy model to assign the most likely
lexical categories to the predicates in the input log-
ical form, thereby greatly constraining the real-
izer’s search space.2 Figure 2 shows gold-standard
supertags for the lexical predicates in the graph;
such category labels are predicted by the hyper-
tagger at run-time. As in recent work on using
supertagging in parsing, the hypertagger operates
in a multitagging paradigm (Curran et al., 2006),
where a variable number of predictions are made
per input predicate. Instead of basing category as-
signment on linear word and POS context, how-
ever, the hypertagger predicts lexical categories
based on contexts within a directed graph structure
representing the logical form (LF) of the sentence
to be realized. The hypertagger generalizes Ban-
galore and Rambow’s (2000) method of using su-
pertags in generation by using maximum entropy
models with a larger local context.
During realization, the hypertagger returns a β-
best list of supertags in order of decreasing prob-
ability. Increasing the number of categories re-
turned clearly increases the likelihood that the
most-correct supertag is among them, but at a cor-
responding cost in chart size. Accordingly, the hy-
pertagger begins with a highly restrictive value for
β, and backs off to progressively less-restrictive
values if no complete realization can be found us-
ing the set of supertags returned. Clark and Curran
(2007b) have shown this iterative relaxation strat-
egy to be highly effective in CCG parsing.
</bodyText>
<sectionHeader confidence="0.985786" genericHeader="method">
3 Perceptron Reranking
</sectionHeader>
<bodyText confidence="0.9987195">
As Collins (2002) observes, perceptron training
involves a simple, on-line algorithm, with few it-
erations typically required to achieve good perfor-
mance. Moreover, averaged perceptrons—which
</bodyText>
<footnote confidence="0.99681">
2The approach has been dubbed hypertagging since it op-
erates at a level “above” the syntax, moving from semantic
representations to syntactic categories.
</footnote>
<page confidence="0.997084">
413
</page>
<bodyText confidence="0.67510525">
Input: training examples (xi, yi)
Initialization: set α = 0, or use optional input
model
Algorithm:
</bodyText>
<equation confidence="0.791155666666667">
fort = 1 ... T, i = 1 ... N
zi = argmaxy∈GEN(xi)Φ(xi, y) · α
if zi =� yi
α = α + Φ(xi, yi) − Φ(xi, zi)
Output: α =�T �N i=1 αti/TN
t=1
</equation>
<figureCaption confidence="0.999277">
Figure 3: Averaged perceptron training algorithm
</figureCaption>
<bodyText confidence="0.999970305555556">
approximate voted perceptrons, a maximum-
margin method with attractive theoretical
properties—seem to work remarkably well in
practice, while adding little further complexity.
Additionally, since features only take on non-
zero values when they appear in training items
requiring updates, perceptrons integrate feature
selection with, and often produce quite small
models, especially when starting with a good
baseline.
The generic averaged perceptron training algo-
rithm appears in Figure 3. In our case, the algo-
rithm trains a model for reranking the n-best real-
izations generated using our existing factored lan-
guage model for scoring, with the oracle-best re-
alization considered the correct answer. Accord-
ingly, the input to the algorithm is a list of pairs
(xi, yi), where xi is a logical form, GEN(xi) are
the n-best realizations for xi, and yi is the oracle-
best member of GEN(xi). The oracle-best realiza-
tion is determined using a 4-gram precision metric
(approximating BLEU) against the reference sen-
tence.
We have followed Huang (2008) in using
oracle-best targets for training, rather than gold
standard ones, in order to better approximate test
conditions during training. However, following
Clark &amp; Curran (2007a), during training we seed
the realizer with the gold-standard supertags, aug-
menting the hypertagger’s β-best list, in order to
ensure that the n-best realizations are generally of
high quality; consequently, the gold standard real-
ization (i.e., the corpus sentence) usually appears
in the n-best list.3 In addition, we use a hyper-
tagger trained on all the training data, to improve
hypertagger performance, while excluding the cur-
</bodyText>
<footnote confidence="0.7488735">
3As in Clark &amp; Curran’s approach, we use a single β value
during training, rather than iteratively loosening the β value;
the chosen β value determines the size of the discrimation
space.
</footnote>
<bodyText confidence="0.99989331372549">
rent training section (in jack-knifed fashion) from
the word-based parts of the language model, in or-
der to make the language model scores more re-
alistic. It remains for future work to determine
whether using a different compromise between en-
suring high-quality training data and remaining
faithful to the test conditions would yield better
results.
Since realization of the n-best lists for train-
ing is the most time-consuming part of the pro-
cess, in our current implementation we perform
this step once, generating event files along the way
containing feature vectors for each candidate real-
ization. The event files are used to calculate the
frequency distribution for the features, and mini-
mum cutoffs are chosen to trim the feature alpha-
bet to a reasonable size. Training then takes place
by iterating over the event files, ignoring features
that do not appear in the alphabet. As Figure 3
indicates, training consists of calculating the top-
ranked realization according to the current model
α, and performing an update when the top-ranked
realization does not match the oracle-best realiza-
tion. Updates to the model add the feature vec-
tor Φ(xi, yi) for the missed oracle-best realiza-
tion, and subtract the feature vector Φ(xi, zi) for
the mistakenly top-ranked realization. The final
model averages the models across the T iterations
over the training data, and N test cases within each
iteration.
Note that while training the perceptron model
involves n-best reranking, realization with the re-
sulting model can be viewed as forest rescoring,
since scoring of all partial realizations is integrated
into the realizer’s beam search. In future work, we
intend to investigate saving the realizer’s packed
charts, rather than event files, and integrating the
unpacking of the charts with the perceptron train-
ing algorithm.
The features we employ in our perceptron mod-
els are of three kinds. First, as in the log-linear
models of Velldal &amp; Oepen and Nakanishi et al.,
we incorporate the log probability of the candidate
realization’s word sequence according to our fac-
tored language model as a single feature in the per-
ceptron model. Since our language model linearly
interpolates three component models, we also in-
clude the log prob from each component language
model as a feature, so that the combination of
these components can be optimized.
Second, we include syntactic features in our
</bodyText>
<page confidence="0.997739">
414
</page>
<figure confidence="0.538800866666667">
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule sdcd — np sdcd\np
Rule + Word sdcd — np sdcd\np + bought
Rule + POS sdcd — np sdcd\np + VBD
Word-Word (company, sdcd — np sdcd\np, bought)
Word-POS (company, sdcd — np sdcd\np, VBD)
POS-Word (NN, sdcd — np sdcd\np, bought)
Word + Δw (bought, sdcd — np sdcd\np) + dw
POS + Δw (VBD, sdcd — np sdcd\np) + dw
Word + Δp (bought, sdcd — np sdcd\np) + dp
POS + Δp (VBD, sdcd — np sdcd\np) + dp
Word + Δv (bought, sdcd — np sdcd\np) + dv
POS + Δv (VBD, sdcd — np sdcd\np) + dv
</figure>
<tableCaption confidence="0.724227">
Table 1: Basic and dependency features from
</tableCaption>
<bodyText confidence="0.973526205882353">
Clark &amp; Curran’s (2007b) normal form model;
distances are in intervening words, punctuation
marks and verbs, and are capped at 3, 3 and 2,
respectively
model by implementing Clark &amp; Curran’s (2007b)
normal form model in OpenCCG.4 The features of
this model are listed in Table 1; they are integer-
valued, representing counts of occurrences in a
derivation. These syntactic features are quite com-
parable to the dominance-oriented features in the
union of the Velldal &amp; Oepen and Nakanishi et
al. models, except that our feature set does not
include grandparenting, which has been found to
have limited utility in CCG parsing. Our syntac-
tic features also include ones that measure the dis-
tance between headwords in terms of intervening
words, punctuation marks or verbs; these features
generalize the ones in Nakanishi et al.’s model.
Note that in contrast to parsing, in realization dis-
tance features are non-local, since different partial
realizations in the same equivalence class typically
differ in word order; as we are working in a rerank-
ing paradigm though, the non-local nature of these
features is unproblematic.
Third, we include discriminative n-gram fea-
tures in our model, following Roark et al.’s (2004)
approach to discriminative n-gram modeling for
speech recognition. By discriminative n-gram fea-
tures, we mean features counting the occurrences
of each n-gram that is scored by our factored lan-
guage model, rather than a feature whose value is
the log prob determined by the language model.
As Roark et al. note, discriminative training with
n-gram features has the potential to learn to nega-
</bodyText>
<footnote confidence="0.999502">
4We have omitted Clark &amp; Curran’s root features, since
the category we use for the full stop ensures that it must ap-
pear at the root of any complete derivation.
</footnote>
<table confidence="0.99905575">
Model #Alph-feats #Feats Acc Time
full-model 2402173 576176 96.40% 08:53
lp-ngram 1127437 342025 94.52% 05:19
lp-syn 1274740 291728 85.03% 05:57
</table>
<tableCaption confidence="0.989941">
Table 2: Perceptron Training Details—number of
</tableCaption>
<bodyText confidence="0.980803692307692">
features in the alphabet, number of features in the
model, training accuracy and training time (hours)
for 10 iterations on a single commodity server
tively weight n-grams that appear in some of the
GEN(xi) candidates, but which never appear in
the naturally occurring corpus used to train a stan-
dard, generative language model. Since our fac-
tored language model considers words, semantic
classes, part-of-speech tags and supertags, our n-
gram features represent a considerable generaliza-
tion of the sequence-oriented features in Velldal
&amp; Oepen’s model, which never contain more than
one word and do not include semantic classes.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.996993">
4.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.9999785">
For the experiments reported below, we used a
lexico-grammar extracted from Sections 02–21 of
our enhanced CCGbank, a hypertagging model in-
corporating named entity class features, and a tri-
gram factored language model over words, named
entity classes, part-of-speech tags and supertags,
as described in the preceding section. BLEU
scores were calculated after removing the under-
scores between collapsed NEs.
Events were generated for each training section
separately. As already noted, the hypertagger and
POS/supertag language model was trained on all
the training sections, while separate word-based
models were trained excluding each of the train-
ing sections in turn. Event files for 26530 training
sentences with complete realizations were gener-
ated in 7 hours and 16 minutes on a cluster us-
ing one commodity server per section, with an av-
erage n-best list size of 18.2. Perceptron models
were trained on single machines; details for three
of the models appear in Table 2. The complete set
of models is listed in Table 3.
</bodyText>
<sectionHeader confidence="0.542959" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9999314">
Realization results on the development section are
given in Table 4. As the first block of rows af-
ter the baseline shows, of the models incorporating
a single kind of feature, only the one with the n-
gram log prob features beats the baseline BLEU
</bodyText>
<page confidence="0.998416">
415
</page>
<table confidence="0.907898">
Model Description
baseline-w3 No perceptron (3g wd only)
baseline No perceptron
syn-only-nodist All syntactic features except distance
ngram-only Just ngram features
syn-only Just syntactic features
lp-only Just log prob features
lp-ngram Log prob + Ngram features
lp-syn Log prob + Syntactic features
full-model Log prob + Ngram +Syntactic features
</table>
<tableCaption confidence="0.996102">
Table 3: Legend for Experimental Conditions
</tableCaption>
<bodyText confidence="0.9999834">
score, with the other models falling well below
the baseline (though faring better than the trigram-
word LM baseline). This result confirms the im-
portance of including n-gram log prob features in
discriminative realization ranking models, in line
with Velldal &amp; Oepen’s findings, and contra those
of Nakanishi et al., even though it was Nakanishi
et al. who experimented with the Penn Treebank
corpus, while Velldal &amp; Oepen’s experiments were
on a much smaller, limited domain corpus. The
second block of rows shows that both the discrim-
inative n-gram features and the syntactic features
provide a substantial boost when used with the n-
gram log prob features, with the syntactic features
yielding a more than 3 BLEU point gain. The
final row shows that the full model works best,
though the boosts provided by the syntactic and
discriminative n-gram features are clearly not in-
dependent. The BLEU point trends are mirrored in
the percentage of exact match realizations, which
goes up by more than 10% from the baseline. The
percentage of complete (i.e., non-fragmentary) re-
alizations, however, goes down; we expect that
this is due to the time taken up by our current
naive method of feature extraction, which does not
cache the features calculated for partial realiza-
tions. Realization results on the standard test sec-
tion appear in Table 5, confirming the gains made
by the full model over the baseline.5
We calculated statistical significance for the
main results on the development section using
bootstrap random sampling.6 After re-sampling
1000 times, significance was calculated using a
paired t-test (999 d.f.). The results indicated that
lp-only exceeded the baseline, lp-ngram and lp-
</bodyText>
<footnote confidence="0.712708571428571">
5Note that the baseline for Section 23 uses 4-grams and a
filter for balanced punctuation (White and Rajkumar, 2008),
unlike the other reported configurations, which would explain
the somewhat smaller increase seen with this section.
6Scripts for running these tests are available at
http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm
</footnote>
<table confidence="0.9994957">
Model %Exact %Compl. BLEU Time
baseline-w3 26.00 83.15 0.7646 1.8
baseline 29.00 83.28 0.7963 2.0
syn-only-nodist 26.02 82.69 0.7754 3.2
ngram-only 27.67 82.95 0.7777 3.0
syn-only 28.34 82.74 0.7838 3.4
lp-only 32.01 83.02 0.8009 2.1
lp-ngram 36.31 80.47 0.8183 3.1
lp-syn 39.47 79.74 0.8323 3.5
full-model 40.11 79.63 0.8373 3.6
</table>
<tableCaption confidence="0.916058">
Table 4: Section 00 Results (98.9% coverage)—
percentage of exact match and grammatically
complete realizations, BLEU scores and average
times, in seconds
</tableCaption>
<table confidence="0.999367333333333">
Model %Exact %Complete BLEU
baseline 33.74 85.04 0.8173
full-model 40.45 83.88 0.8506
</table>
<tableCaption confidence="0.999509">
Table 5: Section 23 Results (97.06% coverage)
</tableCaption>
<bodyText confidence="0.9082255">
syn exceeded lp-only, and the full model exceeded
lp-syn, with p &lt; 0.0001 in each case.
</bodyText>
<subsectionHeader confidence="0.999057">
4.3 Examples
</subsectionHeader>
<bodyText confidence="0.9999918">
Table 6 presents four examples where the full
model improves upon the baseline. Example sen-
tence wsj 0020.10 in Table 6 is a case where the
perceptron successfully weights the component
ngram models, as the lp-ngram model and those
that build on it get it right. Note that here, the mod-
ifier ordering in small video-viewing is not speci-
fied in the logical form and either ordering is pos-
sible syntactically. In wsj 0024.2, number agree-
ment between the conjoined subject noun phrase
and verb is obtained only with the full model. This
suggests that the full model is more robust to cases
where the grammar is insufficiently precise (num-
ber agreement is enforced by the grammar in only
the simplest cases). Example wsj 0034.9 corrects
a VP ordering mismatch, where the corpus sen-
tence is clearly preferred to the one where into
oblivion is shifted to the end. Finally, wsj 0047.13
corrects an animacy mismatch on the wh-pronoun,
in large part due to the high negative weight as-
signed to the discriminative n-gram feature PER-
SON , which. Note that the full model still dif-
fers from the original sentence in its placement of
the adverb reportedly, choosing the arguably more
natural position following the auxiliary.
</bodyText>
<subsectionHeader confidence="0.944388">
4.4 Comparison to Other Systems
</subsectionHeader>
<tableCaption confidence="0.9101795">
Table 7 lists our results in the context of those re-
ported for other systems on PTB Section 23. The
</tableCaption>
<page confidence="0.991396">
416
</page>
<table confidence="0.822983916666667">
Ref-wsj 0020.10 that measure could compel Taipei ’s growing number of small video-viewing parlors to pay ...
baseline,syn-only,ngram-only that measure could compel Taipei ’s growing number of video-viewing small parlors to ...
lp-only, lp-ngram, full-model that measure could compel Taipei ’s growing number of small video-viewing parlors to ...
Ref-wsj 0024.2 Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operate the fields ...
all except full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operates the fields ...
full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operate the fields ...
Ref-wsj 0034.9 they fell into oblivion after the 1929 crash.
baseline, lp-ngram they fell after the 1929 crash into oblivion.
lp-only, ngram-only, syn-only, full-model they fell into oblivion after the 1929 crash.
Ref-wsj 0047.13 Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , reportedly has assured ...
baseline,baseline-w3, lp-syn, lp-only Antonio Novello , which Mr. Bush nominated to serve as surgeon general, has reportedly assured ...
full-model, lp-ngram, syn-only, ngram-syn Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , has reportedly assured ...
</table>
<tableCaption confidence="0.977662">
Table 6: Examples of realized output
</tableCaption>
<table confidence="0.999914888888889">
System Coverage BLEU %Exact
Callaway (05) 98.5% 0.9321 57.5
OpenCCG (09) 97.1% 0.8506 40.5
Ringger et al. (04) 100.0% 0.836 35.7
Langkilde-Geary (02) 83% 0.757 28.2
Guo et al. (08) 100.0% 0.7440 19.8
Hogan et al. (07) ≈100.0% 0.6882
OpenCCG (08) 96.0% 0.6701 16.0
Nakanishi et al. (05) 90.8% 0.7733
</table>
<tableCaption confidence="0.98019">
Table 7: PTB Section 23 BLEU scores and exact
</tableCaption>
<bodyText confidence="0.9813453">
match percentages in the NLG literature (Nakan-
ishi et al.’s results are for sentences of length 20 or
less)
most similar systems to ours are those of Nakan-
ishi et al. (2005) and Hogan et al. (2007), as they
both involve chart realizers for reversible gram-
mars engineered from the Penn Treebank. While
direct comparisons across systems cannot really
be made when inputs vary in their semantic depth
and specificity, we observe that our all-sentences
BLEU score of 0.8506 exceeds that of Hogan et
al., who report a top score of 0.6882 (though with
coverage near 100%), and also surpasses Nakan-
ishi et al.’s score of 0.7733, despite their results be-
ing limited to sentences of length 20 or less (with
91% coverage). Velldal &amp; Oepen’s (2005) system
is also closely related, as noted in the introduc-
tion, but as their experiments are on a limited do-
main corpus, their results cannot be compared at
all meaningfully.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
5 Related Work and Discussion
</sectionHeader>
<bodyText confidence="0.999978152173913">
As alluded to above, realization systems cannot be
easily compared, even on the same corpus, when
their inputs are not the same. This point is dra-
matically illustrated in Langkilde-Geary’s (2002)
system, where a BLEU score of 0.514 is reported
for minimally specified inputs on PTB Section 23,
while a score of 0.757 is reported for the ‘Per-
mute, no dir’ case (which perhaps most closely
resembles our inputs), and a score of 0.924 is re-
ported for the most fully specified inputs; note,
however, that in the latter case word order is deter-
mined by sibling order in the inputs, an assump-
tion not commonly made. As another example,
Guo et al. (2008) report a competitive result of
0.7440 (with 100% coverage) using a dependency-
based approach; however, their inputs, like those
of Hogan et al., include more surface syntactic in-
formation than ours, as they specify case-marking
prepositions, wh-pronouns and complementizers.
In a recent experiment to assess the impact of
input specificity, we found that including pred-
icates for all prepositions in our logical forms
boosted our baseline results by more than 3 BLEU
points, with complete realizations found in more
than 90% of the test cases, indicating that generat-
ing from a more surfacy input is indeed an easier
task than generating from a deeper representation.
Given the current lack of consensus on realizer in-
put specificity, we believe it is important to keep
in mind that within-system comparisons (such as
those in the preceding section) are the ones that
should be given the most credence.
Returning to our cross-system comparison, it is
perhaps surprising that Callaway (2005) reports
the best PTB BLEU score to date, 0.9321, with
98.5% coverage, using a purely symbolic, hand-
crafted grammar augmented to handle the most
frequent coverage issues for the PTB. While Call-
away’s inputs are unordered, word order is often
determined by positional features (e.g. front) or
by the type of modification (e.g. describer vs.
qualifier), and parts-of-speech are included
for lexical items. Additionally, in contrast to our
approach, Callaway makes use of a generation-
only grammar, rather than a reversible one, and his
approach is less well-suited to producing n-best
</bodyText>
<page confidence="0.994203">
417
</page>
<bodyText confidence="0.999969344827586">
outputs. Nevertheless, his high scores do suggest
the potential for precise grammar engineering to
improve realization quality.
While we have yet to perform a thorough er-
ror analysis, our impression is that although the
current set of syntactic features substantially im-
proves clausal constituent ordering, a variety of
disfluent cases remain. More thorough inves-
tigations of features for constituent ordering in
English have been performed by Ringger et al.
(2004), Filippova and Strube (2009) and Zhong
and Stent (2009), all of whom develop classifiers
for determining linear order. In future work, we
plan to investigate whether features inspired by
these approaches can be usefully integrated into
our perceptron reranker.
Also related to the present work is discrimina-
tive training in syntax-based MT (Turian et al.,
2007; Watanabe et al., 2007; Blunsom et al., 2008;
Chiang et al., 2009). Not surprisingly, since MT is
a harder problem than surface realization, syntax-
based MT systems have made use of less precise
grammars and more impoverished (target-side)
feature sets than those tackling realization rank-
ing. With progress on discriminative training with
large numbers of features in syntax-based MT, the
features found to be useful for high-quality sur-
face realization may become increasingly relevant
for MT as well.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99999032">
In this paper, we have shown how discriminative
reranking with an averaged perceptron model can
be used to achieve substantial improvements in re-
alization quality with CCG. Using a comprehen-
sive feature set, we have also confirmed the util-
ity of including language model log probabilities
as features in the model, which prior work on
discriminative training with log linear models for
HPSG realization had called into question. The
perceptron model allows the combination of mul-
tiple n-gram models to be optimized and then aug-
mented with both syntactic features and discrim-
inative n-gram features, inspired by related work
in discriminative parsing and language modeling
for speech recognition. The full model yields a
state-of-the-art BLEU score of 0.8506 on Section
23 of the CCGbank, to our knowledge the best
score reported to date using a reversible, corpus-
engineered grammar, despite our use of deeper,
less specific inputs. Finally, the perceptron model
paves the way for exploring the utility of richer
feature spaces in statistical realization, including
the use of linguistically-motivated and non-local
features, a topic which we plan to investigate in
future work.
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999944166666667">
This work was supported in part by NSF grant IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to the OSU Clippers group and the anony-
mous reviewers for helpful comments and discus-
sion.
</bodyText>
<sectionHeader confidence="0.998353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999636705882353">
Jason Baldridge and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237–265.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proc. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL-08: HLT.
Johan Bos. 2005. Towards wide-coverage semantic
interpretation. In Proc. IWCS-6.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-
08.
Charles Callaway. 2005. The types and distributions of
errors in a wide coverage surface realizer evaluation.
In Proceedings of the 10th European Workshop on
Natural Language Generation.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. NAACL HLT 2009.
Stephen Clark and James Curran. 2007a. Perceptron
training for a wide-coverage lexicalized-grammar
parser. In ACL 2007 Workshop on Deep Linguistic
Processing.
</reference>
<page confidence="0.981285">
418
</page>
<reference confidence="0.999427480769231">
Stephen Clark and James R. Curran. 2007b. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493–552.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL-04.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP-02.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proc. COLING/ACL-06.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08: HLT.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proc. NAACL HLT 2009 Short
Papers.
Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for
general purpose sentence realisation. In Proc.
COLING-08.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL-08:
HLT.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proc. INLG-02.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Alice H. Oh and Alexander I. Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer, Speech &amp; Language,
16(3/4):387–407.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, PA.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. University Of Chicago Press.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proc. NAACL HLT 2009
Short Papers.
Eric Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically informed statistical
models of constituent structure for ordering in sen-
tence realization. In Proc. COLING-04.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proc. ACL-04.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Andreas Stolcke. 2002. SRILM — An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learn-
ing for natural language parsing and translation. In
Proc. NIPS 19.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proc. EMNLP-
CoNLL-07.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report, BBN.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proc.
of the Workshop on Grammar Engineering Across
Frameworks (GEAF08).
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface re-
alization with CCG. In Proc. of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT).
Michael White. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39–75.
Huayan Zhong and Amanda Stent. 2009. Determining
the position of adverbial phrases in English. In Proc.
NAACL HLT 2009 Short Papers.
</reference>
<page confidence="0.998501">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.740378">
<title confidence="0.998884">Perceptron Reranking for CCG Realization</title>
<author confidence="0.955762">White</author>
<affiliation confidence="0.970157">Department of The Ohio State</affiliation>
<address confidence="0.965608">Columbus, OH,</address>
<abstract confidence="0.990261736842105">This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG. The paper confirms the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan Kruijff</author>
</authors>
<title>Coupling CCG and Hybrid Logic Dependency Semantics. In</title>
<date>2002</date>
<booktitle>Proc. ACL-02.</booktitle>
<contexts>
<context position="6913" citStr="Baldridge and Kruijff, 2002" startWordPosition="1059" endWordPosition="1062">. In s[dcl]\np/np Figure 2: Semantic dependency graph from the CCGbank for He has a point he wants to make [... ], along with gold-standard supertags (category labels) the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. (NUM)sg); nodes are connected via dependency relations (e.g. (ARG0)). (Gold-standard supertags, or category labels, are also shown; see Section 2.4 for their role in hypertagging.) Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first </context>
</contexts>
<marker>Baldridge, Kruijff, 2002</marker>
<rawString>Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling CCG and Hybrid Logic Dependency Semantics. In Proc. ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>Lexically Specified Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="5467" citStr="Baldridge, 2002" startWordPosition="839" endWordPosition="840"> function application as the standard means of combining a head with its argument, type-raising and composition support transparent analyses for a wide range of phenomena, including right-node raising and long distance dependencies. An example syntactic derivation appears in Figure 1, with a long-distance dependency between point and make. Semantic composition happens in parallel with syntactic composition, which makes it attractive for generation. OpenCCG is a parsing/generation library which works by combining lexical categories for words using CCG rules and multi-modal extensions on rules (Baldridge, 2002) to produce derivations. Surface realization is the process by which logical forms are transduced to strings. OpenCCG uses a hybrid symbolic-statistical chart realizer (White, 2006) which takes logical forms as input and produces sentences by using CCG combinators to combine signs. Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Alternative realizations are ranked using integrated n-gram or perceptron scoring, and pruning takes place within equivalence classes of edges. To more robustly support broad cove</context>
</contexts>
<marker>Baldridge, 2002</marker>
<rawString>Jason Baldridge. 2002. Lexically Specified Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An Approach to Almost Parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="14234" citStr="Bangalore and Joshi, 1999" startWordPosition="2255" endWordPosition="2258">ty issues caused by rare words in the same semantic class. For example, the Section 00 sentence Pierre Vinken , 61 years old, will join the board as a nonexecutive director Nov. 29 . becomes PERSON, DATE:AGE DATE:AGE old, will join the ORG DESC:OTHER as a nonexecutive PER DESC DATE:DATE DATE:DATE . During realization, word forms are generated, but are then replaced by their semantic classes for scoring using the semantic class–replaced model, similar to Oh and Rudnicky (2002). Note that the use of supertags in the factored language model to score possible realizations is 1With CCG, supertags (Bangalore and Joshi, 1999) are lexical categories considered as fine-grained syntactic labels. distinct from the prediction of supertags for lexical category assignment: the former takes the words in the local context into account (as in supertagging for parsing), while the latter takes features of the logical form into account. This latter process we call hypertagging, to which we now turn. 2.4 Hypertagging A crucial component of the OpenCCG realizer is the hypertagger (Espinosa et al., 2008), or supertagger for surface realization, which uses a maximum entropy model to assign the most likely lexical categories to the</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An Approach to Almost Parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proc. COLING-00.</booktitle>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proc. COLING-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and general parallelized backoff.</title>
<date>2003</date>
<booktitle>In Proc. HLT-03.</booktitle>
<contexts>
<context position="12394" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="1948" endWordPosition="1951">.5 BLEU point improvement in our baseline realization scores on Section 00 of the CCGbank, from a score of 0.6567 in (Espinosa et al., 2008) to 0.7917 in (Rajkumar et al., 2009), contributing greatly to the state-of-the-art results reported 412 in Section 4. A further 4.5 point improvement is obtained from the use of named entity classes in language modeling and hypertagging (Rajkumar et al., 2009), as described next, and from our perceptron reranking model, described in Section 3. 2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations. The trigram models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (02–21) of the CCGbank, with sentence-initial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (LangkildeGeary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre. The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language models and general parallelized backoff. In Proc. HLT-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT.</booktitle>
<contexts>
<context position="35914" citStr="Blunsom et al., 2008" startWordPosition="5756" endWordPosition="5759">es substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. With progress on discriminative training with large numbers of features in syntax-based MT, the features found to be useful for high-quality surface realization may become increasingly relevant for MT as well. 6 Conclusions In this paper, we have shown how discriminative reranking with an averaged perceptron model can be used to achieve substantial </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Towards wide-coverage semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proc. IWCS-6.</booktitle>
<contexts>
<context position="8934" citStr="Bos, 2005" startWordPosition="1379" endWordPosition="1380">egories related to semantically empty function words such complementizers, infinitivalto, expletive subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status. In the second step, a grammar is extracted from the converted CCGbank and augmented with logical forms. Categories and unary type changing rules (corresponding to zero morphemes) are sorted by frequency and extracted if they meet the specified frequency thresholds. A separate transformation then uses a few dozen generalized templates to add logical forms to the categories, in a fashion reminiscent of (Bos, 2005). As shown in Figure 2, numbered semantic roles are taken from PropBank when available, and more specific relations are introduced in the categories for closedclass items such as determiners. After logical form insertion, the extracted and augmented grammar is loaded and used to parse the sentences in the CCGbank according to the gold-standard derivation. If the derivation can be successfully followed, the parse yields a logical form which is saved along with the corpus sentence in order to later test the realizer. Currently, the algorithm succeeds in creating logical forms for 98.85% of the s</context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>Johan Bos. 2005. Towards wide-coverage semantic interpretation. In Proc. IWCS-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boxwell</author>
<author>Michael White</author>
</authors>
<title>Projecting Propbank roles onto the CCGbank. In</title>
<date>2008</date>
<booktitle>Proc. LREC08.</booktitle>
<contexts>
<context position="7407" citStr="Boxwell and White, 2008" startWordPosition="1135" endWordPosition="1138">id Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in np/n &lt;Arg1&gt; &lt;Arg0&gt; np &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h1 n &lt;Arg1&gt; s[dcl]\np/(s[to]\np) he h2 &lt;NUM&gt;sg point P1 want.01 a1 a W1 &lt;TENSE&gt;pres M1 make.03 s[b]\np/np np h3 he &lt;Arg0&gt; &lt;GenRel&gt; 411 He has a point he</context>
</contexts>
<marker>Boxwell, White, 2008</marker>
<rawString>Stephen Boxwell and Michael White. 2008. Projecting Propbank roles onto the CCGbank. In Proc. LREC08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Callaway</author>
</authors>
<title>The types and distributions of errors in a wide coverage surface realizer evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="34457" citStr="Callaway (2005)" startWordPosition="5532" endWordPosition="5533"> for all prepositions in our logical forms boosted our baseline results by more than 3 BLEU points, with complete realizations found in more than 90% of the test cases, indicating that generating from a more surfacy input is indeed an easier task than generating from a deeper representation. Given the current lack of consensus on realizer input specificity, we believe it is important to keep in mind that within-system comparisons (such as those in the preceding section) are the ones that should be given the most credence. Returning to our cross-system comparison, it is perhaps surprising that Callaway (2005) reports the best PTB BLEU score to date, 0.9321, with 98.5% coverage, using a purely symbolic, handcrafted grammar augmented to handle the most frequent coverage issues for the PTB. While Callaway’s inputs are unordered, word order is often determined by positional features (e.g. front) or by the type of modification (e.g. describer vs. qualifier), and parts-of-speech are included for lexical items. Additionally, in contrast to our approach, Callaway makes use of a generationonly grammar, rather than a reversible one, and his approach is less well-suited to producing n-best 417 outputs. Never</context>
</contexts>
<marker>Callaway, 2005</marker>
<rawString>Charles Callaway. 2005. The types and distributions of errors in a wide coverage surface realizer evaluation. In Proceedings of the 10th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<contexts>
<context position="35936" citStr="Chiang et al., 2009" startWordPosition="5760" endWordPosition="5763">ves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. With progress on discriminative training with large numbers of features in syntax-based MT, the features found to be useful for high-quality surface realization may become increasingly relevant for MT as well. 6 Conclusions In this paper, we have shown how discriminative reranking with an averaged perceptron model can be used to achieve substantial improvements in realiz</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proc. NAACL HLT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
</authors>
<title>Perceptron training for a wide-coverage lexicalized-grammar parser.</title>
<date>2007</date>
<booktitle>In ACL 2007 Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="1726" citStr="Clark and Curran, 2007" startWordPosition="254" endWordPosition="257">l Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log probabilities as baseline features in a discriminatively trained realization ranking model. On a limited domain corpus, Velldal &amp; Oepen found that including the n-gram log probability of each candidate realization as a feature in their log-linear model yielded a substantial boost in ranking performance; on the Penn Treebank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead </context>
<context position="16180" citStr="Clark and Curran (2007" startWordPosition="2562" endWordPosition="2565"> Rambow’s (2000) method of using supertags in generation by using maximum entropy models with a larger local context. During realization, the hypertagger returns a β- best list of supertags in order of decreasing probability. Increasing the number of categories returned clearly increases the likelihood that the most-correct supertag is among them, but at a corresponding cost in chart size. Accordingly, the hypertagger begins with a highly restrictive value for β, and backs off to progressively less-restrictive values if no complete realization can be found using the set of supertags returned. Clark and Curran (2007b) have shown this iterative relaxation strategy to be highly effective in CCG parsing. 3 Perceptron Reranking As Collins (2002) observes, perceptron training involves a simple, on-line algorithm, with few iterations typically required to achieve good performance. Moreover, averaged perceptrons—which 2The approach has been dubbed hypertagging since it operates at a level “above” the syntax, moving from semantic representations to syntactic categories. 413 Input: training examples (xi, yi) Initialization: set α = 0, or use optional input model Algorithm: fort = 1 ... T, i = 1 ... N zi = argmaxy</context>
<context position="18143" citStr="Clark &amp; Curran (2007" startWordPosition="2873" endWordPosition="2876">actored language model for scoring, with the oracle-best realization considered the correct answer. Accordingly, the input to the algorithm is a list of pairs (xi, yi), where xi is a logical form, GEN(xi) are the n-best realizations for xi, and yi is the oraclebest member of GEN(xi). The oracle-best realization is determined using a 4-gram precision metric (approximating BLEU) against the reference sentence. We have followed Huang (2008) in using oracle-best targets for training, rather than gold standard ones, in order to better approximate test conditions during training. However, following Clark &amp; Curran (2007a), during training we seed the realizer with the gold-standard supertags, augmenting the hypertagger’s β-best list, in order to ensure that the n-best realizations are generally of high quality; consequently, the gold standard realization (i.e., the corpus sentence) usually appears in the n-best list.3 In addition, we use a hypertagger trained on all the training data, to improve hypertagger performance, while excluding the cur3As in Clark &amp; Curran’s approach, we use a single β value during training, rather than iteratively loosening the β value; the chosen β value determines the size of the </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James Curran. 2007a. Perceptron training for a wide-coverage lexicalized-grammar parser. In ACL 2007 Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1726" citStr="Clark and Curran, 2007" startWordPosition="254" endWordPosition="257">l Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log probabilities as baseline features in a discriminatively trained realization ranking model. On a limited domain corpus, Velldal &amp; Oepen found that including the n-gram log probability of each candidate realization as a feature in their log-linear model yielded a substantial boost in ranking performance; on the Penn Treebank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead </context>
<context position="16180" citStr="Clark and Curran (2007" startWordPosition="2562" endWordPosition="2565"> Rambow’s (2000) method of using supertags in generation by using maximum entropy models with a larger local context. During realization, the hypertagger returns a β- best list of supertags in order of decreasing probability. Increasing the number of categories returned clearly increases the likelihood that the most-correct supertag is among them, but at a corresponding cost in chart size. Accordingly, the hypertagger begins with a highly restrictive value for β, and backs off to progressively less-restrictive values if no complete realization can be found using the set of supertags returned. Clark and Curran (2007b) have shown this iterative relaxation strategy to be highly effective in CCG parsing. 3 Perceptron Reranking As Collins (2002) observes, perceptron training involves a simple, on-line algorithm, with few iterations typically required to achieve good performance. Moreover, averaged perceptrons—which 2The approach has been dubbed hypertagging since it operates at a level “above” the syntax, moving from semantic representations to syntactic categories. 413 Input: training examples (xi, yi) Initialization: set α = 0, or use optional input model Algorithm: fort = 1 ... T, i = 1 ... N zi = argmaxy</context>
<context position="18143" citStr="Clark &amp; Curran (2007" startWordPosition="2873" endWordPosition="2876">actored language model for scoring, with the oracle-best realization considered the correct answer. Accordingly, the input to the algorithm is a list of pairs (xi, yi), where xi is a logical form, GEN(xi) are the n-best realizations for xi, and yi is the oraclebest member of GEN(xi). The oracle-best realization is determined using a 4-gram precision metric (approximating BLEU) against the reference sentence. We have followed Huang (2008) in using oracle-best targets for training, rather than gold standard ones, in order to better approximate test conditions during training. However, following Clark &amp; Curran (2007a), during training we seed the realizer with the gold-standard supertags, augmenting the hypertagger’s β-best list, in order to ensure that the n-best realizations are generally of high quality; consequently, the gold standard realization (i.e., the corpus sentence) usually appears in the n-best list.3 In addition, we use a hypertagger trained on all the training data, to improve hypertagger performance, while excluding the cur3As in Clark &amp; Curran’s approach, we use a single β value during training, rather than iteratively loosening the β value; the chosen β value determines the size of the </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. ACL-04.</booktitle>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP-02.</booktitle>
<contexts>
<context position="1020" citStr="Collins, 2002" startWordPosition="150" endWordPosition="151">ies as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar. 1 Introduction In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been </context>
<context position="16308" citStr="Collins (2002)" startWordPosition="2584" endWordPosition="2585">, the hypertagger returns a β- best list of supertags in order of decreasing probability. Increasing the number of categories returned clearly increases the likelihood that the most-correct supertag is among them, but at a corresponding cost in chart size. Accordingly, the hypertagger begins with a highly restrictive value for β, and backs off to progressively less-restrictive values if no complete realization can be found using the set of supertags returned. Clark and Curran (2007b) have shown this iterative relaxation strategy to be highly effective in CCG parsing. 3 Perceptron Reranking As Collins (2002) observes, perceptron training involves a simple, on-line algorithm, with few iterations typically required to achieve good performance. Moreover, averaged perceptrons—which 2The approach has been dubbed hypertagging since it operates at a level “above” the syntax, moving from semantic representations to syntactic categories. 413 Input: training examples (xi, yi) Initialization: set α = 0, or use optional input model Algorithm: fort = 1 ... T, i = 1 ... N zi = argmaxy∈GEN(xi)Φ(xi, y) · α if zi =� yi α = α + Φ(xi, yi) − Φ(xi, zi) Output: α =�T �N i=1 αti/TN t=1 Figure 3: Averaged perceptron tra</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proc. EMNLP-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing. In</title>
<date>2006</date>
<booktitle>Proc. COLING/ACL-06.</booktitle>
<contexts>
<context position="15204" citStr="Curran et al., 2006" startWordPosition="2407" endWordPosition="2410">which we now turn. 2.4 Hypertagging A crucial component of the OpenCCG realizer is the hypertagger (Espinosa et al., 2008), or supertagger for surface realization, which uses a maximum entropy model to assign the most likely lexical categories to the predicates in the input logical form, thereby greatly constraining the realizer’s search space.2 Figure 2 shows gold-standard supertags for the lexical predicates in the graph; such category labels are predicted by the hypertagger at run-time. As in recent work on using supertagging in parsing, the hypertagger operates in a multitagging paradigm (Curran et al., 2006), where a variable number of predictions are made per input predicate. Instead of basing category assignment on linear word and POS context, however, the hypertagger predicts lexical categories based on contexts within a directed graph structure representing the logical form (LF) of the sentence to be realized. The hypertagger generalizes Bangalore and Rambow’s (2000) method of using supertags in generation by using maximum entropy models with a larger local context. During realization, the hypertagger returns a β- best list of supertags in order of decreasing probability. Increasing the numbe</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R. Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proc. COLING/ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Espinosa</author>
<author>Michael White</author>
<author>Dennis Mehay</author>
</authors>
<title>Hypertagging: Supertagging for surface realization with CCG.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT.</booktitle>
<contexts>
<context position="2796" citStr="Espinosa et al., 2008" startWordPosition="425" endWordPosition="428">bank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead of 4-grams suggested as a possible explanation). With these mixed results, the utility of n-gram baseline features for PTBscale discriminative realization ranking has been unclear. In our particular setting, the question is: Do n-gram log prob features improve performance in broad coverage realization ranking with CCG, where factored language models over words, partof-speech tags and supertags have previously been employed (White et al., 2007; Espinosa et al., 2008)? We answer this question in the affirmative, confirming the results of Velldal &amp; Oepen, despite the differences in corpus size and kind of language model. We show that including n-gram log prob features in the perceptron model is highly beneficial, as the discriminative models we tested without these features performed worse than the generative baseline. These findings are in line with Collins &amp; Roark’s (2004) results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learn</context>
<context position="11907" citStr="Espinosa et al., 2008" startWordPosition="1869" endWordPosition="1872"> type entitites; (2) ignore phrases like three decades later, which are annotated as DATE entities; and (3) collapse all phrases with POS tags CD or NNP(S) or lexical items % or $, ensuring that all prototypical named entities are collapsed. It is worth noting that improvements in our corpus-based grammar engineering process— including a more precise treatment of punctuation, better named entity handling and the addition of catch-all logical form templates—have resulted in a 13.5 BLEU point improvement in our baseline realization scores on Section 00 of the CCGbank, from a score of 0.6567 in (Espinosa et al., 2008) to 0.7917 in (Rajkumar et al., 2009), contributing greatly to the state-of-the-art results reported 412 in Section 4. A further 4.5 point improvement is obtained from the use of named entity classes in language modeling and hypertagging (Rajkumar et al., 2009), as described next, and from our perceptron reranking model, described in Section 3. 2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations. The trigram models we</context>
<context position="14706" citStr="Espinosa et al., 2008" startWordPosition="2328" endWordPosition="2331">002). Note that the use of supertags in the factored language model to score possible realizations is 1With CCG, supertags (Bangalore and Joshi, 1999) are lexical categories considered as fine-grained syntactic labels. distinct from the prediction of supertags for lexical category assignment: the former takes the words in the local context into account (as in supertagging for parsing), while the latter takes features of the logical form into account. This latter process we call hypertagging, to which we now turn. 2.4 Hypertagging A crucial component of the OpenCCG realizer is the hypertagger (Espinosa et al., 2008), or supertagger for surface realization, which uses a maximum entropy model to assign the most likely lexical categories to the predicates in the input logical form, thereby greatly constraining the realizer’s search space.2 Figure 2 shows gold-standard supertags for the lexical predicates in the graph; such category labels are predicted by the hypertagger at run-time. As in recent work on using supertagging in parsing, the hypertagger operates in a multitagging paradigm (Curran et al., 2006), where a variable number of predictions are made per input predicate. Instead of basing category assi</context>
</contexts>
<marker>Espinosa, White, Mehay, 2008</marker>
<rawString>Dominic Espinosa, Michael White, and Dennis Mehay. 2008. Hypertagging: Supertagging for surface realization with CCG. In Proc. ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Tree linearization in English: Improving language model based approaches.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<note>Short Papers.</note>
<contexts>
<context position="35537" citStr="Filippova and Strube (2009)" startWordPosition="5696" endWordPosition="5699">ay makes use of a generationonly grammar, rather than a reversible one, and his approach is less well-suited to producing n-best 417 outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tack</context>
</contexts>
<marker>Filippova, Strube, 2009</marker>
<rawString>Katja Filippova and Michael Strube. 2009. Tree linearization in English: Improving language model based approaches. In Proc. NAACL HLT 2009 Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Josef van Genabith</author>
<author>Haifeng Wang</author>
</authors>
<title>Dependency-based n-gram models for general purpose sentence realisation.</title>
<date>2008</date>
<booktitle>In Proc. COLING-08.</booktitle>
<marker>Guo, van Genabith, Wang, 2008</marker>
<rawString>Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2008. Dependency-based n-gram models for general purpose sentence realisation. In Proc. COLING-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="7262" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1113" endWordPosition="1116">ndard supertags, or category labels, are also shown; see Section 2.4 for their role in hypertagging.) Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in np/n &lt;Arg1&gt; &lt;Arg0&gt; np &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h1 n &lt;A</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
<author>Conor Cafferkey</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Exploiting multi-word units in history-based probabilistic generation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<marker>Hogan, Cafferkey, Cahill, van Genabith, 2007</marker>
<rawString>Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef van Genabith. 2007. Exploiting multi-word units in history-based probabilistic generation. In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT.</booktitle>
<contexts>
<context position="1702" citStr="Huang, 2008" startWordPosition="252" endWordPosition="253">ory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log probabilities as baseline features in a discriminatively trained realization ranking model. On a limited domain corpus, Velldal &amp; Oepen found that including the n-gram log probability of each candidate realization as a feature in their log-linear model yielded a substantial boost in ranking performance; on the Penn Treebank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the</context>
<context position="17964" citStr="Huang (2008)" startWordPosition="2849" endWordPosition="2850"> averaged perceptron training algorithm appears in Figure 3. In our case, the algorithm trains a model for reranking the n-best realizations generated using our existing factored language model for scoring, with the oracle-best realization considered the correct answer. Accordingly, the input to the algorithm is a list of pairs (xi, yi), where xi is a logical form, GEN(xi) are the n-best realizations for xi, and yi is the oraclebest member of GEN(xi). The oracle-best realization is determined using a 4-gram precision metric (approximating BLEU) against the reference sentence. We have followed Huang (2008) in using oracle-best targets for training, rather than gold standard ones, in order to better approximate test conditions during training. However, following Clark &amp; Curran (2007a), during training we seed the realizer with the gold-standard supertags, augmenting the hypertagger’s β-best list, in order to ensure that the n-best realizations are generally of high quality; consequently, the gold standard realization (i.e., the corpus sentence) usually appears in the n-best list.3 In addition, we use a hypertagger trained on all the training data, to improve hypertagger performance, while exclud</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a generalpurpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proc. INLG-02.</booktitle>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a generalpurpose sentence generator. In Proc. INLG-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic methods for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proc. IWPT-05.</booktitle>
<contexts>
<context position="1189" citStr="Nakanishi et al. (2005)" startWordPosition="173" endWordPosition="176">odel allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar. 1 Introduction In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the</context>
<context position="32052" citStr="Nakanishi et al. (2005)" startWordPosition="5128" endWordPosition="5132"> nominated to serve as surgeon general , has reportedly assured ... Table 6: Examples of realized output System Coverage BLEU %Exact Callaway (05) 98.5% 0.9321 57.5 OpenCCG (09) 97.1% 0.8506 40.5 Ringger et al. (04) 100.0% 0.836 35.7 Langkilde-Geary (02) 83% 0.757 28.2 Guo et al. (08) 100.0% 0.7440 19.8 Hogan et al. (07) ≈100.0% 0.6882 OpenCCG (08) 96.0% 0.6701 16.0 Nakanishi et al. (05) 90.8% 0.7733 Table 7: PTB Section 23 BLEU scores and exact match percentages in the NLG literature (Nakanishi et al.’s results are for sentences of length 20 or less) most similar systems to ours are those of Nakanishi et al. (2005) and Hogan et al. (2007), as they both involve chart realizers for reversible grammars engineered from the Penn Treebank. While direct comparisons across systems cannot really be made when inputs vary in their semantic depth and specificity, we observe that our all-sentences BLEU score of 0.8506 exceeds that of Hogan et al., who report a top score of 0.6882 (though with coverage near 100%), and also surpasses Nakanishi et al.’s score of 0.7733, despite their results being limited to sentences of length 20 or less (with 91% coverage). Velldal &amp; Oepen’s (2005) system is also closely related, as </context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic methods for disambiguation of an HPSG-based chart generator. In Proc. IWPT-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice H Oh</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Stochastic natural language generation for spoken dialog systems.</title>
<date>2002</date>
<journal>Computer, Speech &amp; Language,</journal>
<pages>16--3</pages>
<contexts>
<context position="14088" citStr="Oh and Rudnicky (2002)" startWordPosition="2232" endWordPosition="2235">g data for the semantic class–replaced model was created by replacing (collapsed) words with their NE classes, in order to address data sparsity issues caused by rare words in the same semantic class. For example, the Section 00 sentence Pierre Vinken , 61 years old, will join the board as a nonexecutive director Nov. 29 . becomes PERSON, DATE:AGE DATE:AGE old, will join the ORG DESC:OTHER as a nonexecutive PER DESC DATE:DATE DATE:DATE . During realization, word forms are generated, but are then replaced by their semantic classes for scoring using the semantic class–replaced model, similar to Oh and Rudnicky (2002). Note that the use of supertags in the factored language model to score possible realizations is 1With CCG, supertags (Bangalore and Joshi, 1999) are lexical categories considered as fine-grained syntactic labels. distinct from the prediction of supertags for lexical category assignment: the former takes the words in the local context into account (as in supertagging for parsing), while the latter takes features of the logical form into account. This latter process we call hypertagging, to which we now turn. 2.4 Hypertagging A crucial component of the OpenCCG realizer is the hypertagger (Espi</context>
</contexts>
<marker>Oh, Rudnicky, 2002</marker>
<rawString>Alice H. Oh and Alexander I. Rudnicky. 2002. Stochastic natural language generation for spoken dialog systems. Computer, Speech &amp; Language, 16(3/4):387–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="7357" citStr="Palmer et al., 2005" startWordPosition="1127" endWordPosition="1130">rnally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in np/n &lt;Arg1&gt; &lt;Arg0&gt; np &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h1 n &lt;Arg1&gt; s[dcl]\np/(s[to]\np) he h2 &lt;NUM&gt;sg point P1 want.01 a1 a W1 &lt;TENSE&gt;pres M1 make.03 s[b]\np</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3704" citStr="Papineni et al., 2002" startWordPosition="573" endWordPosition="576">ed without these features performed worse than the generative baseline. These findings are in line with Collins &amp; Roark’s (2004) results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning. We also show that discriminative training allows the combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU (Papineni et al., 2002) score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date 410 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410–419, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP using a reversible, corpus-engineered grammar. The paper is organized as follows. Section 2 reviews previous work on broad coverage realization with OpenCCG. Section 3 describes our approach to realization reranking with averaged perceptron models. Section 4 presents our evaluation of the perceptron models, comparing the results of diffe</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University Of Chicago Press.</publisher>
<contexts>
<context position="1371" citStr="Pollard and Sag, 1994" startWordPosition="198" endWordPosition="201">f-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar. 1 Introduction In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log probabilities as baseline features in a discriminatively trained realization ranking model. On a limited domain corpus</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase Structure Grammar. University Of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
<author>Dominic Espinosa</author>
</authors>
<title>Exploiting named entity classes in CCG surface realization.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<note>Short Papers.</note>
<contexts>
<context position="10085" citStr="Rajkumar et al. (2009)" startWordPosition="1568" endWordPosition="1572">y, the algorithm succeeds in creating logical forms for 98.85% of the sentences in the development section (Sect. 00) of the converted CCGbank, and 97.06% of the sentences in the test section (Sect. 23). Of these, 95.99% of the development LFs are semantic dependency graphs with a single root, while 95.81% of the test LFs have a single root. The remaining cases, with multiple roots, are missing one or more dependencies required to form a fully connected graph. Such missing dependencies usually reflect remaining inadequacies in the logical form templates. An error analysis of OpenCCG output by Rajkumar et al. (2009) recently revealed that out of 2331 named entities (NEs) annotated by the BBN corpus (Weischedel and Brunstein, 2005), 238 were not realized correctly. For example, multiword NPs like Texas Instruments Japan Ltd. were realized as Japan Texas Instruments Ltd. Accordingly, inspired by Hogan et al.’s (2007)’s Experiment 1, Rajkumar et al. used the BBN corpus NE annotation to collapse certain classes of NEs. But unlike Hogan et al.’s experiment where all the NEs annotated by the BBN corpus were collapsed, Rajkumar et al. chose to collapse into single tokens only NEs whose exact form can be reasona</context>
<context position="11944" citStr="Rajkumar et al., 2009" startWordPosition="1876" endWordPosition="1879">ike three decades later, which are annotated as DATE entities; and (3) collapse all phrases with POS tags CD or NNP(S) or lexical items % or $, ensuring that all prototypical named entities are collapsed. It is worth noting that improvements in our corpus-based grammar engineering process— including a more precise treatment of punctuation, better named entity handling and the addition of catch-all logical form templates—have resulted in a 13.5 BLEU point improvement in our baseline realization scores on Section 00 of the CCGbank, from a score of 0.6567 in (Espinosa et al., 2008) to 0.7917 in (Rajkumar et al., 2009), contributing greatly to the state-of-the-art results reported 412 in Section 4. A further 4.5 point improvement is obtained from the use of named entity classes in language modeling and hypertagging (Rajkumar et al., 2009), as described next, and from our perceptron reranking model, described in Section 3. 2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations. The trigram models were created using the SRILM toolkit (S</context>
</contexts>
<marker>Rajkumar, White, Espinosa, 2009</marker>
<rawString>Rajakrishnan Rajkumar, Michael White, and Dominic Espinosa. 2009. Exploiting named entity classes in CCG surface realization. In Proc. NAACL HLT 2009 Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Ringger</author>
<author>Michael Gamon</author>
<author>Robert C Moore</author>
<author>David Rojas</author>
<author>Martine Smets</author>
<author>Simon CorstonOliver</author>
</authors>
<title>Linguistically informed statistical models of constituent structure for ordering in sentence realization.</title>
<date>2004</date>
<booktitle>In Proc. COLING-04.</booktitle>
<contexts>
<context position="35508" citStr="Ringger et al. (2004)" startWordPosition="5692" endWordPosition="5695">to our approach, Callaway makes use of a generationonly grammar, rather than a reversible one, and his approach is less well-suited to producing n-best 417 outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side)</context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, CorstonOliver, 2004</marker>
<rawString>Eric Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon CorstonOliver. 2004. Linguistically informed statistical models of constituent structure for ordering in sentence realization. In Proc. COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. ACL-04.</booktitle>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proc. ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1129" citStr="Steedman, 2000" startWordPosition="165" endWordPosition="166">lization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar. 1 Introduction In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) </context>
<context position="4611" citStr="Steedman, 2000" startWordPosition="713" endWordPosition="714">neered grammar. The paper is organized as follows. Section 2 reviews previous work on broad coverage realization with OpenCCG. Section 3 describes our approach to realization reranking with averaged perceptron models. Section 4 presents our evaluation of the perceptron models, comparing the results of different feature sets. Section 5 compares our results to those obtained by related systems and discusses the difficulties of cross-system comparisons. Finally, Section 6 concludes with a summary and discussion of future directions for research. 2 Background 2.1 Surface Realization with CCG CCG (Steedman, 2000) is a unification-based categorial grammar formalism which is defined almost entirely in terms of lexical entries that encode sub-categorization information as well as syntactic feature information (e.g. number and agreement). Complementing function application as the standard means of combining a head with its argument, type-raising and composition support transparent analyses for a wide range of phenomena, including right-node raising and long distance dependencies. An example syntactic derivation appears in Figure 1, with a long-distance dependency between point and make. Semantic compositi</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM — An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP-02.</booktitle>
<contexts>
<context position="12557" citStr="Stolcke, 2002" startWordPosition="1974" endWordPosition="1975">), contributing greatly to the state-of-the-art results reported 412 in Section 4. A further 4.5 point improvement is obtained from the use of named entity classes in language modeling and hypertagging (Rajkumar et al., 2009), as described next, and from our perceptron reranking model, described in Section 3. 2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations. The trigram models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (02–21) of the CCGbank, with sentence-initial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (LangkildeGeary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre. The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates three component models using rankorder centroid weights: (1) a word trigram model; (2) a word model with semantic classes replacing named entities; a</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM — An extensible language modeling toolkit. In Proc. ICSLP-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Benjamin Wellington</author>
<author>I Dan Melamed</author>
</authors>
<title>Scalable discriminative learning for natural language parsing and translation.</title>
<date>2007</date>
<booktitle>In Proc. NIPS 19.</booktitle>
<contexts>
<context position="35869" citStr="Turian et al., 2007" startWordPosition="5748" endWordPosition="5751">although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. With progress on discriminative training with large numbers of features in syntax-based MT, the features found to be useful for high-quality surface realization may become increasingly relevant for MT as well. 6 Conclusions In this paper, we have shown how discriminative reranking with an averaged percept</context>
</contexts>
<marker>Turian, Wellington, Melamed, 2007</marker>
<rawString>Joseph Turian, Benjamin Wellington, and I. Dan Melamed. 2007. Scalable discriminative learning for natural language parsing and translation. In Proc. NIPS 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum entropy models for realization ranking.</title>
<date>2005</date>
<booktitle>In Proc. MT Summit X.</booktitle>
<contexts>
<context position="1161" citStr="Velldal and Oepen (2005)" startWordPosition="168" endWordPosition="171">to question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar. 1 Introduction In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. A</context>
<context position="12798" citStr="Velldal and Oepen, 2005" startWordPosition="2008" endWordPosition="2011">escribed next, and from our perceptron reranking model, described in Section 3. 2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations. The trigram models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (02–21) of the CCGbank, with sentence-initial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (LangkildeGeary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre. The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates three component models using rankorder centroid weights: (1) a word trigram model; (2) a word model with semantic classes replacing named entities; and (3) a trigram model that chains a POS model with a supertag model, where the POS model (P) conditions on the previous two POS tags, and the supertag model (S) conditions on the previous two POS tags as well as the current one, as shown be</context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>Erik Velldal and Stephan Oepen. 2005. Maximum entropy models for realization ranking. In Proc. MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLPCoNLL-07.</booktitle>
<contexts>
<context position="35892" citStr="Watanabe et al., 2007" startWordPosition="5752" endWordPosition="5755">set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. With progress on discriminative training with large numbers of features in syntax-based MT, the features found to be useful for high-quality surface realization may become increasingly relevant for MT as well. 6 Conclusions In this paper, we have shown how discriminative reranking with an averaged perceptron model can be used t</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. EMNLPCoNLL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus.</title>
<date>2005</date>
<tech>Technical report, BBN.</tech>
<contexts>
<context position="10202" citStr="Weischedel and Brunstein, 2005" startWordPosition="1587" endWordPosition="1590">Sect. 00) of the converted CCGbank, and 97.06% of the sentences in the test section (Sect. 23). Of these, 95.99% of the development LFs are semantic dependency graphs with a single root, while 95.81% of the test LFs have a single root. The remaining cases, with multiple roots, are missing one or more dependencies required to form a fully connected graph. Such missing dependencies usually reflect remaining inadequacies in the logical form templates. An error analysis of OpenCCG output by Rajkumar et al. (2009) recently revealed that out of 2331 named entities (NEs) annotated by the BBN corpus (Weischedel and Brunstein, 2005), 238 were not realized correctly. For example, multiword NPs like Texas Instruments Japan Ltd. were realized as Japan Texas Instruments Ltd. Accordingly, inspired by Hogan et al.’s (2007)’s Experiment 1, Rajkumar et al. used the BBN corpus NE annotation to collapse certain classes of NEs. But unlike Hogan et al.’s experiment where all the NEs annotated by the BBN corpus were collapsed, Rajkumar et al. chose to collapse into single tokens only NEs whose exact form can be reasonably expected to be specified in the input to the realizer. For example, while some quantificational or comparatives p</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun coreference and entity type corpus. Technical report, BBN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>A more precise analysis of punctuation for broadcoverage surface realization with CCG.</title>
<date>2008</date>
<booktitle>In Proc. of the Workshop on Grammar Engineering Across Frameworks (GEAF08).</booktitle>
<contexts>
<context position="7689" citStr="White and Rajkumar, 2008" startWordPosition="1177" endWordPosition="1180"> between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in np/n &lt;Arg1&gt; &lt;Arg0&gt; np &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h1 n &lt;Arg1&gt; s[dcl]\np/(s[to]\np) he h2 &lt;NUM&gt;sg point P1 want.01 a1 a W1 &lt;TENSE&gt;pres M1 make.03 s[b]\np/np np h3 he &lt;Arg0&gt; &lt;GenRel&gt; 411 He has a point he wants to make np sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np &gt; &gt;T &gt;B np s/(s\np) sto\np/np sdcl\np/np sdcl/np np\np &lt; &gt; &lt; np sdcl\np sdcl Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [... ] &gt;B &gt;B particular, the features and</context>
<context position="27870" citStr="White and Rajkumar, 2008" startWordPosition="4458" endWordPosition="4461">feature extraction, which does not cache the features calculated for partial realizations. Realization results on the standard test section appear in Table 5, confirming the gains made by the full model over the baseline.5 We calculated statistical significance for the main results on the development section using bootstrap random sampling.6 After re-sampling 1000 times, significance was calculated using a paired t-test (999 d.f.). The results indicated that lp-only exceeded the baseline, lp-ngram and lp5Note that the baseline for Section 23 uses 4-grams and a filter for balanced punctuation (White and Rajkumar, 2008), unlike the other reported configurations, which would explain the somewhat smaller increase seen with this section. 6Scripts for running these tests are available at http://projectile.sv.cmu.edu/research/ public/tools/bootStrap/tutorial.htm Model %Exact %Compl. BLEU Time baseline-w3 26.00 83.15 0.7646 1.8 baseline 29.00 83.28 0.7963 2.0 syn-only-nodist 26.02 82.69 0.7754 3.2 ngram-only 27.67 82.95 0.7777 3.0 syn-only 28.34 82.74 0.7838 3.4 lp-only 32.01 83.02 0.8009 2.1 lp-ngram 36.31 80.47 0.8183 3.1 lp-syn 39.47 79.74 0.8323 3.5 full-model 40.11 79.63 0.8373 3.6 Table 4: Section 00 Results</context>
</contexts>
<marker>White, Rajkumar, 2008</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2008. A more precise analysis of punctuation for broadcoverage surface realization with CCG. In Proc. of the Workshop on Grammar Engineering Across Frameworks (GEAF08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards broad coverage surface realization with CCG.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</booktitle>
<contexts>
<context position="2772" citStr="White et al., 2007" startWordPosition="421" endWordPosition="424">ce; on the Penn Treebank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead of 4-grams suggested as a possible explanation). With these mixed results, the utility of n-gram baseline features for PTBscale discriminative realization ranking has been unclear. In our particular setting, the question is: Do n-gram log prob features improve performance in broad coverage realization ranking with CCG, where factored language models over words, partof-speech tags and supertags have previously been employed (White et al., 2007; Espinosa et al., 2008)? We answer this question in the affirmative, confirming the results of Velldal &amp; Oepen, despite the differences in corpus size and kind of language model. We show that including n-gram log prob features in the perceptron model is highly beneficial, as the discriminative models we tested without these features performed worse than the generative baseline. These findings are in line with Collins &amp; Roark’s (2004) results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better </context>
<context position="12308" citStr="White et al., 2007" startWordPosition="1935" endWordPosition="1938">ng and the addition of catch-all logical form templates—have resulted in a 13.5 BLEU point improvement in our baseline realization scores on Section 00 of the CCGbank, from a score of 0.6567 in (Espinosa et al., 2008) to 0.7917 in (Rajkumar et al., 2009), contributing greatly to the state-of-the-art results reported 412 in Section 4. A further 4.5 point improvement is obtained from the use of named entity classes in language modeling and hypertagging (Rajkumar et al., 2009), as described next, and from our perceptron reranking model, described in Section 3. 2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations. The trigram models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (02–21) of the CCGbank, with sentence-initial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (LangkildeGeary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre. The models employ interpola</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White, Rajakrishnan Rajkumar, and Scott Martin. 2007. Towards broad coverage surface realization with CCG. In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Efficient Realization of Coordinate Structures</title>
<date>2006</date>
<booktitle>in Combinatory Categorial Grammar. Research on Language and Computation,</booktitle>
<pages>4--1</pages>
<contexts>
<context position="5648" citStr="White, 2006" startWordPosition="864" endWordPosition="865">ht-node raising and long distance dependencies. An example syntactic derivation appears in Figure 1, with a long-distance dependency between point and make. Semantic composition happens in parallel with syntactic composition, which makes it attractive for generation. OpenCCG is a parsing/generation library which works by combining lexical categories for words using CCG rules and multi-modal extensions on rules (Baldridge, 2002) to produce derivations. Surface realization is the process by which logical forms are transduced to strings. OpenCCG uses a hybrid symbolic-statistical chart realizer (White, 2006) which takes logical forms as input and produces sentences by using CCG combinators to combine signs. Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Alternative realizations are ranked using integrated n-gram or perceptron scoring, and pruning takes place within equivalence classes of edges. To more robustly support broad coverage surface realization, OpenCCG greedily assembles fragments in the event that the realizer fails to find a complete realization. To illustrate the input to OpenCCG, consider the </context>
</contexts>
<marker>White, 2006</marker>
<rawString>Michael White. 2006. Efficient Realization of Coordinate Structures in Combinatory Categorial Grammar. Research on Language and Computation, 4(1):39–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huayan Zhong</author>
<author>Amanda Stent</author>
</authors>
<title>Determining the position of adverbial phrases in English.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<note>Short Papers.</note>
<contexts>
<context position="35564" citStr="Zhong and Stent (2009)" startWordPosition="5701" endWordPosition="5704"> grammar, rather than a reversible one, and his approach is less well-suited to producing n-best 417 outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. W</context>
</contexts>
<marker>Zhong, Stent, 2009</marker>
<rawString>Huayan Zhong and Amanda Stent. 2009. Determining the position of adverbial phrases in English. In Proc. NAACL HLT 2009 Short Papers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>