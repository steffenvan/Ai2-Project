<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.078605">
<title confidence="0.835006">
Alignment of Sound Track with Text in a TV Drama
</title>
<author confidence="0.825447">
Seigo Tanimura, Hiroshi Nakagawa
</author>
<affiliation confidence="0.936762">
Information Technology Center, The University of Tokyo.
</affiliation>
<address confidence="0.538469">
7-3-1 Hongo, Bunkyo, Tokyo, JAPAN 113-0033
</address>
<email confidence="0.841535">
ftanimura,nakagawalAr.dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.969762" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999930625">
We propose a system to align a sound track
and a part of TV drama video contents with
a script. We first use the number of moras
in each sentence of speech line, the sounding
time in a sound track and the shot change
time in the motion image to align them ap-
proximately. Then we perform DP matching
to align a sequence of words obtained from a
speech recognition system applied to a sound
track with each sentence of speech lines in a
script. Confident correspondences obtained
from the DP matching of the words act as
the pivots to improve alignment accuracy it-
eratively. The results show that around a
half of the sentences in a script were aligned
within the differences of up to two seconds.
</bodyText>
<sectionHeader confidence="0.995718" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976280701754">
Alignment of video to text is essential to
make video contents flexibly reusable. Al-
though it seems to be promising for this
purpose to apply a speech recognition tech-
nology to the sound track of a video con-
tent, speech recognition for a TV drama
or a feature film is actually difficult. The
major obstacle significant in a drama and
film is the poor accuracy of speech recog-
nition. Due to the background music, the
noise from the environment in a location and
audio compression like MPEG, the accuracy
of speech recognition is only around 10-30%.
Thus the result of speech recognition to the
sound track of a drama or film does not have
enough quality to directly reuse the video
contents.
Although the result of speech recognition
is of no use by itself, we still have the script of
a drama or film. Hence alternative method
is to align the script of a drama or film to the
video. Several proporsals have been made to
solve this problem. Yaginuma et al. (Yag-
inuma and Sakauchi, 1996) proposed time
alignment of a TV drama by the physical
shot changes in video, the volume in the
sound track and the number of characters
in the speech lines of the script. However,
the accuracy of alignment by their method
achieved only 70% for a sentence due to lack
of speech recognition in their method.
While speech recognition can improve the
accuracy of alignment, we need to solve a
couple of problems to apply speech recog-
nition to a sound track of a drama or film.
A state-of-the-art speech recognition system
performs speech recognition in a sentence-
to-sentence manner. In addition to that, ap-
plying speech recognition directly to a whole
length of sound track, say 30 minutes or
longer, involves unrealistic costs in both time
and memory. Hence a sound track needs to
be divided into sentences prior to applying a
speech recognition system. Due to the large
number of sentences in a TV drama, it may
result in inferior accuracy to divide a whole
sound track into sentences. We can avoid
this by dividing a sound track roughly first
into, say, logical scenes. Then we apply more
precise segmentation based on the sentences
to the logical scenes.
We developed a system to align each sen-
tence of speech lines in a script written
in Japanese to the corresponding part of a
sound track accompanying the script spoken
in Japanese. Our proposing system consists
of six modules. Figure 1 shows the architec-
</bodyText>
<figure confidence="0.987259025641025">
Pivots
Logical scene
alignment
Number of moras
in a logical scene
Roughly aligned
logical scenes
Adjustment of
logical scene boundaries
Time of
shot change
in motion image
Aligned
logical scenes
Feedback
Number of moras
in a sentence
Sentence
alignment
Roughly aligned
sentences
Speech
recognition
A language model
generated from
speech lines
Recognized
words
Results of
alignment
Sentences of
the speech lines
Word
alignment
The sound track
aligned to the
speech lines
A sound track
3 Sentence Alignment
</figure>
<bodyText confidence="0.998263">
A logical scene generally consists of several
sentences. However, unlike extracting the
logical scenes in a whole drama, it is diffi-
cult to extract the sentences directly from a
sound track or a sequence of image frames.
To segment out each sentence in the sound
track, we apply the method to use the num-
bers of moras and the duration of sounding
segments described in section 2, except that
we seek a part of the sound track correspond-
ing to not a logical scene but a sentence.
Since an utterance of a speech line may not
begin or end at a shot change, we modify the
alignment method proposed in section 2 by
omitting adjustment of starting and ending
time to the nearest shot change time.
</bodyText>
<sectionHeader confidence="0.989898" genericHeader="method">
4 Speech Recognition
</sectionHeader>
<subsectionHeader confidence="0.998411">
4.1 Language and Acoustic Models
</subsectionHeader>
<bodyText confidence="0.99997496969697">
In our system, the words to be uttered are
given as the speech lines of a script in ad-
vance. A word bigram language model used
during speech recognition is generated from
the whole speech lines to improve the ac-
curacy of a speech recognition system. Ev-
ery word in a language model is uttered at
least once, and no other words are uttered
with this tailored language model. This
avoids recognizing words not appearing in
the speech lines.
It also brings about a better accuracy
in speech recognition to choose the acous-
tic model adapted for a speaker. How-
ever, a general method of speaker adapta-
tion has some major problems in our sys-
tem, that is, the speakers in the segments
of the sound track are not given as the in-
put. Furthermore, not only one but several
speakers may utter in a single segment of
the sound track. Thus we cannot determine
a suitable acoustic model prior to speech
recognition. In order to solve this problem,
we perform speech recognition with multi-
ple acoustic models (Ming et al., 1999), say
female and male models, because the differ-
ence of these two models affects the accuracy
of a speech recognition system significantly.
To perform speech recognition simply, these
acoustic models are used in parallel. This
allows us to improve the accuracy of sen-
tence alignment without misselecting a suit-
able acoustic model, described in section 5.2.
</bodyText>
<subsectionHeader confidence="0.937645">
4.2 A Speech Recognition System
and Filtering out Noisy Words
</subsectionHeader>
<bodyText confidence="0.999958090909091">
We use JULIUS (Ito et al., 1998) as a speech
recognition system. A language model is
generated by applying a Japanese morpho-
logical analyser JUMAN (Kurohashi and
Nagao, 1997) and CMU-Cambridge SLM
Toolkit (Clarkson, 1997) to the speech lines.
We use HMMs of 16 mixed density for tri-
phones of 3000 states as the acoustic models.
HMMs for female and male speakers are used
in parallel.
We postprocess the recognized words in
order to improve the accuracy of sentence
alignment. A speech recognition system
treats unuttering duration in a sound track
as a comma or a full stop. However, these
do not usually match to the commas and full
stops in the speech lines. Thus commas and
full stops in the recognized words and speech
lines are apparently noise in word alignment.
We filter out these noisy words from the rec-
ognized words and the words in the speech
lines prior to word alignment.
</bodyText>
<sectionHeader confidence="0.919043" genericHeader="method">
5 Word Alignment with DP
Matching
</sectionHeader>
<bodyText confidence="0.9998798">
In this module, we align for each of logical
scenes the sequences of the recognized words
to the sentences of the speech lines based
on the similarity between a pair of words.
The similarity between words involved in
this alignment is computed by performing
mora-based DP matching. More precisely,
our word alignment system consists of two
level alignment modules; a word alignment
module for a pair of sentences described in
section 5.2 and a mora alignment for a pair
of words described in section 5.1. To proceed
the word alignment for sentences, the word
alignment module invokes the mora align-
ment module for words interactively.
</bodyText>
<figure confidence="0.996905070175439">
B’
p
(i-p+1,j)
(i,j)
1
1 1
b’mi
1 1 1 1
b’mi-1
(i-1,j-1)
(i-p,j-1)
1
p
1
1
(i-1,j-p)
A’
a’mi-1
a’mi
Bw
p
(i-p+1,j)
(i,j)
1 1
1
b wi
1
2
2 2 2
b wi-1
(i-1,j-1)
(i-p,j-1)
1
2
p
2
1
(i,j-p+1)
(i-1,j-p)
Aw
2
a wi
a wi-1
uttering duration
time
d1
wj’-1
d1
wj’
female
male
d2
wj
d2
wj-1
highest gu, (c,„ d„,2 3) is selected as 43_1.
6 Improvement by Feedback
</figure>
<bodyText confidence="0.998156476190476">
We improve the accuracy of alignment by
fixing confident correspondences in the re-
sult of the word alignment module discussed
in section 5 as fixed pivots in logical scene
alignment.
A confident correspondence should not
consist of short words. Short words in our
system refer to the words of only one or two
moras. Most of such the words are func-
tional words in Japanese. They appear quite
frequently in any speech lines. Moreover, a
speech recognition system may misrecognize
utterances or even noises to end up with spu-
rious functional words. On the contrary, a
correspondence of long words with an iden-
tical pronunciation can be confident. Such a
correspondence shows that the utterance is
recognized correctly with a matching word
in the speech lines. Counting these facts, we
define a confident corresponding word pair
as follows:
</bodyText>
<listItem confidence="0.9824955">
• The corresponding words have an iden-
tical pronunciation.
• The pronunciation should have a length
of at least three moras.
</listItem>
<bodyText confidence="0.999779">
We pick up the correspondences satisfying
both of these conditions shown above from
the result of word alignment as the confident
correspondences. Using these confident cor-
respondences as fixed pivots, we realign the
sounding segments of sound track with each
logical scenes and sentences of the speech
lines, and reperform the speech recognition
and word alignment described in section 2-5,
respectively.
</bodyText>
<sectionHeader confidence="0.991925" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.999967428571429">
We evaluated the alignment accuracy of our
system experimentally. Table 1 shows the
sample scene for our experiment.
We first counted for each cycle of iter-
ation the number of the recognized words
with three or more moras and the number of
the aligned sentences including at least one
</bodyText>
<tableCaption confidence="0.996756">
Table 1: Sample scene
</tableCaption>
<table confidence="0.999807833333333">
Number of logical scenes 24
Number of sentences 91
Number of words with 502
three or more moras
Duration of the sound 14:44
track [min:sec]
</table>
<tableCaption confidence="0.99658575">
pivot. The results are shown in figure 2 and
3, respectively.
Table 2: The numbers of the recognized
words with three or more moras
</tableCaption>
<table confidence="0.9812415">
Iteration No. of words
Cycle 1 59
Cycle 2 57
Cycle 3 59
</table>
<tableCaption confidence="0.9959365">
Table 3: The numbers of the aligned sen-
tences with at least one pivot
</tableCaption>
<table confidence="0.94385075">
Iteration No. of sentences
Cycle 1 31
Cycle 2 37
Cycle 3 36
</table>
<bodyText confidence="0.999871896551724">
Although the numbers of recognized words
shown in table 2 do not cover all of the words
in the script, we can still approximate the
accuracy of speech recognition by these re-
sults. The accuracy of speech recognition
stayed around 12%, indicating a poor qual-
ity of the sample sound track. Nevertheless a
third of the sentences were aligned with piv-
ots. In addition, we gained quite a few num-
ber of newly aligned sentences as iteration
proceeds, as shown in table 3. These results
imply that the pivots in a sentence obtained
in the first cycle diffuse to the neighbour sen-
tences.
In order to investigate the effect of the
pivot gain shown in table 3, we evaluated
the accuracy of alignment by the following
method. We measured the difference be-
tween the utterance beginning/ending time
of the recognized word aligned to the
first/last word of each sentence and the cor-
rect utterance beginning/ending time of the
sentence, which is expressed as cb/c, hence-
forth. We then counted the number of the
sentences satisfying 161 &lt; E where c is either
cb or c, and E is one of 1,3 or 5 seconds. The
average of 161 for the whole sentences, Av.
was also computed. The results are shown
in table 4.
</bodyText>
<tableCaption confidence="0.9073105">
Table 4: The numbers of the sentences sat-
isfying 1c1 &lt; E and the average of 1c1
</tableCaption>
<table confidence="0.996506625">
Iteration c E Av.[s]
l[s] 3[s] 5[s]
Cycle 1 cb 17 31 42 16.9
e, 7 21 35 18.3
Cycle 2 cb 32 47 60 8.4
c, 16 36 53 10.1
Cycle 3 cb 31 55 62 9.9
c, 19 43 53 12.0
</table>
<bodyText confidence="0.999448">
We can state from these results that our
alignment system can align not only a sen-
tence recognized correctly but also its neigh-
bor sentences. On the other hand, the aver-
age of e did not increase in cycle 3 because
the level of noise was extremely higher than
the level of utterance for 21 sentences ut-
tered in a running train. Due to the poor
accuracy of speech recognition of these 21
sentences, we obtained only 5 pivots at most
from these sentences, ending up with the
large Av.s shown in table 4.
</bodyText>
<sectionHeader confidence="0.995349" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999980166666667">
We proposed a system to align a sound track
and a sequence of image frames with sen-
tences of speech lines in a TV drama. Our
next target is to improve the accuracy of
speech recognition and to seek a promising
application area of our alignment method.
</bodyText>
<sectionHeader confidence="0.954404" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9705476">
This work is supported in part by the Grant-
in-Aid for Creative Basic Research: Multi
Media Mediation Mechanism(09NP1401), of
the Ministry of Education, Science, Sports
and Culture, Japan.
</bodyText>
<sectionHeader confidence="0.99097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9965947">
Philip Clarkson. 1997. The
CMU-Cambridge statistical lan-
guage modeling toolkit v2.
http://svr-www.eng.cam.ac.uk/
-prc14/toolkit .html.
Hitachi, Ltd. 1997. Mediachef/CUT for
Windows 95.
Katsunobu Ito, Tatsuya Kawahara, Kazuya
Takeda, and Kiyohiro Shikano. 1998.
Japanese dictation toolkit. Proc. of
the 12th Annual Conference of Japanese
Society for Artificial Intelligence. (In
Japanese).
Haruo Kubozono. 1999. Nihongo no On-
sei(Phonetics in Japanese). Iwanami
Shoten Publishers. (In Japanese).
Sadao Kurohashi and Makoto Nagao, 1997.
Japanese Morphological Analysis System
JUMAN Ver. 3.4. (In Japanese).
Ji Ming, Philip Hanna, Darryl Stewart,
Marie Ownes, and F. Jack Smith. 1999.
Improving speech recognition perfor-
mance by using multi-model approaches.
ICASSP 99, 1:161-164.
Y. Yaginuma and M. Sakauchi. 1996.
Content-based drama editing based on in-
termedia synchronization. Proc. of the
IEEE Computer Society, International
Conference on Multimedia Computing and
Systems &apos;96, pages 322-329, 6.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931022">
<title confidence="0.98362">Alignment of Sound Track with Text in a TV Drama</title>
<author confidence="0.965874">Seigo Tanimura</author>
<author confidence="0.965874">Hiroshi</author>
<affiliation confidence="0.997659">Information Technology Center, The University of</affiliation>
<address confidence="0.998123">7-3-1 Hongo, Bunkyo, Tokyo, JAPAN</address>
<email confidence="0.976977">ftanimura,nakagawalAr.dl.itc.u-tokyo.ac.jp</email>
<abstract confidence="0.999790705882353">We propose a system to align a sound track and a part of TV drama video contents with a script. We first use the number of moras in each sentence of speech line, the sounding time in a sound track and the shot change time in the motion image to align them approximately. Then we perform DP matching to align a sequence of words obtained from a speech recognition system applied to a sound track with each sentence of speech lines in a script. Confident correspondences obtained from the DP matching of the words act as the pivots to improve alignment accuracy iteratively. The results show that around a half of the sentences in a script were aligned within the differences of up to two seconds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
</authors>
<title>The CMU-Cambridge statistical language modeling toolkit v2. http://svr-www.eng.cam.ac.uk/ -prc14/toolkit .html.</title>
<date>1997</date>
<contexts>
<context position="6239" citStr="Clarkson, 1997" startWordPosition="1071" endWordPosition="1072">els, because the difference of these two models affects the accuracy of a speech recognition system significantly. To perform speech recognition simply, these acoustic models are used in parallel. This allows us to improve the accuracy of sentence alignment without misselecting a suitable acoustic model, described in section 5.2. 4.2 A Speech Recognition System and Filtering out Noisy Words We use JULIUS (Ito et al., 1998) as a speech recognition system. A language model is generated by applying a Japanese morphological analyser JUMAN (Kurohashi and Nagao, 1997) and CMU-Cambridge SLM Toolkit (Clarkson, 1997) to the speech lines. We use HMMs of 16 mixed density for triphones of 3000 states as the acoustic models. HMMs for female and male speakers are used in parallel. We postprocess the recognized words in order to improve the accuracy of sentence alignment. A speech recognition system treats unuttering duration in a sound track as a comma or a full stop. However, these do not usually match to the commas and full stops in the speech lines. Thus commas and full stops in the recognized words and speech lines are apparently noise in word alignment. We filter out these noisy words from the recognized </context>
</contexts>
<marker>Clarkson, 1997</marker>
<rawString>Philip Clarkson. 1997. The CMU-Cambridge statistical language modeling toolkit v2. http://svr-www.eng.cam.ac.uk/ -prc14/toolkit .html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ltd Hitachi</author>
</authors>
<date>1997</date>
<journal>Mediachef/CUT for Windows</journal>
<volume>95</volume>
<marker>Hitachi, 1997</marker>
<rawString>Hitachi, Ltd. 1997. Mediachef/CUT for Windows 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsunobu Ito</author>
<author>Tatsuya Kawahara</author>
<author>Kazuya Takeda</author>
<author>Kiyohiro Shikano</author>
</authors>
<title>Japanese dictation toolkit.</title>
<date>1998</date>
<booktitle>Proc. of the 12th Annual Conference of Japanese Society for Artificial Intelligence. (In Japanese).</booktitle>
<contexts>
<context position="6050" citStr="Ito et al., 1998" startWordPosition="1041" endWordPosition="1044">a suitable acoustic model prior to speech recognition. In order to solve this problem, we perform speech recognition with multiple acoustic models (Ming et al., 1999), say female and male models, because the difference of these two models affects the accuracy of a speech recognition system significantly. To perform speech recognition simply, these acoustic models are used in parallel. This allows us to improve the accuracy of sentence alignment without misselecting a suitable acoustic model, described in section 5.2. 4.2 A Speech Recognition System and Filtering out Noisy Words We use JULIUS (Ito et al., 1998) as a speech recognition system. A language model is generated by applying a Japanese morphological analyser JUMAN (Kurohashi and Nagao, 1997) and CMU-Cambridge SLM Toolkit (Clarkson, 1997) to the speech lines. We use HMMs of 16 mixed density for triphones of 3000 states as the acoustic models. HMMs for female and male speakers are used in parallel. We postprocess the recognized words in order to improve the accuracy of sentence alignment. A speech recognition system treats unuttering duration in a sound track as a comma or a full stop. However, these do not usually match to the commas and ful</context>
</contexts>
<marker>Ito, Kawahara, Takeda, Shikano, 1998</marker>
<rawString>Katsunobu Ito, Tatsuya Kawahara, Kazuya Takeda, and Kiyohiro Shikano. 1998. Japanese dictation toolkit. Proc. of the 12th Annual Conference of Japanese Society for Artificial Intelligence. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haruo Kubozono</author>
</authors>
<title>Nihongo no Onsei(Phonetics in Japanese). Iwanami Shoten Publishers.</title>
<date>1999</date>
<note>(In Japanese).</note>
<marker>Kubozono, 1999</marker>
<rawString>Haruo Kubozono. 1999. Nihongo no Onsei(Phonetics in Japanese). Iwanami Shoten Publishers. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<date>1997</date>
<journal>Japanese Morphological Analysis System JUMAN Ver.</journal>
<volume>3</volume>
<note>(In Japanese).</note>
<contexts>
<context position="6192" citStr="Kurohashi and Nagao, 1997" startWordPosition="1063" endWordPosition="1066">oustic models (Ming et al., 1999), say female and male models, because the difference of these two models affects the accuracy of a speech recognition system significantly. To perform speech recognition simply, these acoustic models are used in parallel. This allows us to improve the accuracy of sentence alignment without misselecting a suitable acoustic model, described in section 5.2. 4.2 A Speech Recognition System and Filtering out Noisy Words We use JULIUS (Ito et al., 1998) as a speech recognition system. A language model is generated by applying a Japanese morphological analyser JUMAN (Kurohashi and Nagao, 1997) and CMU-Cambridge SLM Toolkit (Clarkson, 1997) to the speech lines. We use HMMs of 16 mixed density for triphones of 3000 states as the acoustic models. HMMs for female and male speakers are used in parallel. We postprocess the recognized words in order to improve the accuracy of sentence alignment. A speech recognition system treats unuttering duration in a sound track as a comma or a full stop. However, these do not usually match to the commas and full stops in the speech lines. Thus commas and full stops in the recognized words and speech lines are apparently noise in word alignment. We fi</context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>Sadao Kurohashi and Makoto Nagao, 1997. Japanese Morphological Analysis System JUMAN Ver. 3.4. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ming</author>
<author>Philip Hanna</author>
<author>Darryl Stewart</author>
<author>Marie Ownes</author>
<author>F Jack Smith</author>
</authors>
<title>Improving speech recognition performance by using multi-model approaches.</title>
<date>1999</date>
<journal>ICASSP</journal>
<volume>99</volume>
<pages>1--161</pages>
<contexts>
<context position="5599" citStr="Ming et al., 1999" startWordPosition="968" endWordPosition="971">ing words not appearing in the speech lines. It also brings about a better accuracy in speech recognition to choose the acoustic model adapted for a speaker. However, a general method of speaker adaptation has some major problems in our system, that is, the speakers in the segments of the sound track are not given as the input. Furthermore, not only one but several speakers may utter in a single segment of the sound track. Thus we cannot determine a suitable acoustic model prior to speech recognition. In order to solve this problem, we perform speech recognition with multiple acoustic models (Ming et al., 1999), say female and male models, because the difference of these two models affects the accuracy of a speech recognition system significantly. To perform speech recognition simply, these acoustic models are used in parallel. This allows us to improve the accuracy of sentence alignment without misselecting a suitable acoustic model, described in section 5.2. 4.2 A Speech Recognition System and Filtering out Noisy Words We use JULIUS (Ito et al., 1998) as a speech recognition system. A language model is generated by applying a Japanese morphological analyser JUMAN (Kurohashi and Nagao, 1997) and CM</context>
</contexts>
<marker>Ming, Hanna, Stewart, Ownes, Smith, 1999</marker>
<rawString>Ji Ming, Philip Hanna, Darryl Stewart, Marie Ownes, and F. Jack Smith. 1999. Improving speech recognition performance by using multi-model approaches. ICASSP 99, 1:161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yaginuma</author>
<author>M Sakauchi</author>
</authors>
<title>Content-based drama editing based on intermedia synchronization.</title>
<date>1996</date>
<booktitle>Proc. of the IEEE Computer Society, International Conference on Multimedia Computing and Systems &apos;96,</booktitle>
<pages>322--329</pages>
<contexts>
<context position="1929" citStr="Yaginuma and Sakauchi, 1996" startWordPosition="331" endWordPosition="335">cy of speech recognition. Due to the background music, the noise from the environment in a location and audio compression like MPEG, the accuracy of speech recognition is only around 10-30%. Thus the result of speech recognition to the sound track of a drama or film does not have enough quality to directly reuse the video contents. Although the result of speech recognition is of no use by itself, we still have the script of a drama or film. Hence alternative method is to align the script of a drama or film to the video. Several proporsals have been made to solve this problem. Yaginuma et al. (Yaginuma and Sakauchi, 1996) proposed time alignment of a TV drama by the physical shot changes in video, the volume in the sound track and the number of characters in the speech lines of the script. However, the accuracy of alignment by their method achieved only 70% for a sentence due to lack of speech recognition in their method. While speech recognition can improve the accuracy of alignment, we need to solve a couple of problems to apply speech recognition to a sound track of a drama or film. A state-of-the-art speech recognition system performs speech recognition in a sentenceto-sentence manner. In addition to that,</context>
</contexts>
<marker>Yaginuma, Sakauchi, 1996</marker>
<rawString>Y. Yaginuma and M. Sakauchi. 1996. Content-based drama editing based on intermedia synchronization. Proc. of the IEEE Computer Society, International Conference on Multimedia Computing and Systems &apos;96, pages 322-329, 6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>