<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998722">
One-Class Clustering in the Text Domain
</title>
<author confidence="0.985646">
Ron Bekkerman Koby Crammer
</author>
<affiliation confidence="0.982748">
HP Laboratories University of Pennsylvania
</affiliation>
<address confidence="0.916499">
Palo Alto, CA 94304, USA Philadelphia, PA 19104, USA
</address>
<email confidence="0.997505">
ron.bekkerman@hp.com crammer@cis.upenn.edu
</email>
<sectionHeader confidence="0.984754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996013607142857">
Having seen a news title “Alba denies wedding
reports”, how do we infer that it is primar-
ily about Jessica Alba, rather than about wed-
dings or reports? We probably realize that, in a
randomly driven sentence, the word “Alba” is
less anticipated than “wedding” or “reports”,
which adds value to the word “Alba” if used.
Such anticipation can be modeled as a ratio
between an empirical probability of the word
(in a given corpus) and its estimated proba-
bility in general English. Aggregated over all
words in a document, this ratio may be used
as a measure of the document’s topicality. As-
suming that the corpus consists of on-topic
and off-topic documents (we call them the
core and the noise), our goal is to determine
which documents belong to the core. We pro-
pose two unsupervised methods for doing this.
First, we assume that words are sampled i.i.d.,
and propose an information-theoretic frame-
work for determining the core. Second, we
relax the independence assumption and use
a simple graphical model to rank documents
according to their likelihood of belonging to
the core. We discuss theoretical guarantees of
the proposed methods and show their useful-
ness for Web Mining and Topic Detection and
Tracking (TDT).
</bodyText>
<sectionHeader confidence="0.992018" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999865375">
Many intelligent applications in the text domain aim
at determining whether a document (a sentence, a
snippet etc.) is on-topic or off-topic. In some appli-
cations, topics are explicitly given. In binary text
classification, for example, the topic is described
in terms of positively and negatively labeled docu-
ments. In information retrieval, the topic is imposed
by a query. In many other applications, the topic
</bodyText>
<page confidence="0.668543">
41
</page>
<bodyText confidence="0.999970027027027">
is unspecified, however, its existence is assumed.
Examples of such applications are within text sum-
marization (extract the most topical sentences), text
clustering (group documents that are close topi-
cally), novelty detection (reason whether or not test
documents are on the same topic as training docu-
ments), spam filtering (reject incoming email mes-
sages that are too far topically from the content of a
personal email repository), etc.
Under the (standard) Bag-Of-Words (BOW) rep-
resentation of a document, words are the functional
units that bear the document’s topic. Since some
words are topical and some are not, the problem of
detecting on-topic documents has a dual formulation
of detecting topical words. This paper deals with the
following questions: (a) Which words can be con-
sidered topical? (b) How can topical words be de-
tected? (c) How can on-topic documents be detected
given a set of topical words?
The BOW formalism is usually translated into
the generative modeling terms by representing doc-
uments as multinomial word distributions. For the
on-topic/off-topic case, we assume that words in a
document are sampled from a mixture of two multi-
nomials: one over topical words and another one
over general English (i.e. the background). Obvi-
ously enough, the support of the “topic” multinomial
is significantly smaller than the support of the back-
ground. A document’s topicality is then determined
by aggregating the topicality of its words (see below
for details). Note that by introducing the background
distribution we refrain from explicitly modeling the
class of off-topic documents—a document is sup-
posed to be off-topic if it is “not topical enough”.
Such a formulation of topicality prescribes us-
ing the one-class modeling paradigm, as opposed
to sticking to the binary case. Besides being much
</bodyText>
<note confidence="0.9967695">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 41–50,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<figureCaption confidence="0.72027575">
Figure 1: The problem of hyperspherical decision bound-
aries in one-class models for text, as projected on 2D:
(left) a too small portion of the core is captured; (right)
too much space around the core is captured.
</figureCaption>
<bodyText confidence="0.989776032258065">
less widely studied and therefore much more attrac-
tive from the scientific point of view, one-class mod-
els appear to be more adequate for many real-world
tasks, where negative examples are not straightfor-
wardly observable. One-class models separate the
desired class of data instances (the core) from other
data instances (the noise). Structure of noise is either
unknown, or too complex to be explicitly modeled.
One-class problems are traditionally approached
using vector-space methods, where a convex deci-
sion boundary is built around the data instances of
the desired class, separating it from the rest of the
universe. In the text domain, however, those vector-
space models are questionably applicable—unlike
effective binary vector-space models. In binary
models, decision boundaries are linear1, whereas in
(vector-space) one-class models, the boundaries are
usually hyperspherical. Intuitively, since core docu-
ments tend to lie on a lower-dimensional manifold
(Lebanon, 2005), inducing hyperspherical bound-
aries may be sub-optimal as they tend to either cap-
ture just a portion of the core, or capture too much
space around it (see illustration in Figure 1). Here
we propose alternative ways for detecting the core,
which work well in text.
One-class learning problems have been studied as
either outlier detection or identifying a small coher-
ent subset. In one-class outlier detection (Tax and
Duin, 2001; Sch¨olkopf et al., 2001), the goal is to
identify a few outliers from the given set of exam-
ples, where the vast majority of the examples are
considered relevant. Alternatively, a complementary
goal is to distill a subset of relevant examples, in the
space with many outliers (Crammer and Chechik,
1As such, or after applying the kernel trick (Cristianini and
Shawe-Taylor, 2000)
2004; Gupta and Ghosh, 2005; Crammer et al.,
2008). Most of the one-class approaches employ ge-
ometrical concepts to capture the notion of relevancy
(or irrelevancy) using either hyperplanes (Sch¨olkopf
et al., 2001) or hyperspheres (Tax and Duin, 2001;
Crammer and Chechik, 2004; Gupta and Ghosh,
2005). In this paper we adopt the latter approach:
we formulate one-class clustering in text as an opti-
mization task of identifying the most coherent subset
(the core) of k documents drawn from a given pool
of n &gt; k documents.2
Given a collection D of on-topic and off-topic
documents, we assume that on-topic documents
share a portion of their vocabulary that consists of
“relatively rare” words, i.e. words that are used in D
more often than they are used in general English. We
call them topical words. For example, if some doc-
uments in D share words such as “Bayesian”, “clas-
sifier”, “reinforcement” and other machine learning
terms (infrequent in general English), whereas other
documents do not seem to share any subset of words
(besides stopwords), then we conclude that the ma-
chine learning documents compose the core of D,
while non-machine learning documents are noise.
We express the level of topicality of a word w
in terms of the ratio p(w) = 1&apos;(w), where p(w) is
</bodyText>
<equation confidence="0.860301">
Q(w)
</equation>
<bodyText confidence="0.93512125">
w’s empirical probability (in D), and q(w) is its es-
timated probability in general English. We discuss
an interesting characteristic of p(w): if D is large
enough, then, with high probability, p(w) values are
greater for topical words than for non-topical words.
Therefore, p(w) can be used as a mean to measure
the topicality of w.
Obviously, the quality of this measure depends on
the quality of estimating q(w), i.e. the general En-
glish word distribution, which is usually estimated
over a large text collection. The larger the collec-
tion is, the better would be the estimation. Recently,
Google has released the Web 1T dataset3 that pro-
vides q(w) estimated on a text collection of one tril-
lion tokens. We use it in our experimentation.
We propose two methods that use the p ratio to
2The parameter k is analogous to the number of clusters in
(multi-class) clustering, as well as to the number of outliers (Tax
and Duin, 2001) or the radius of Bregmanian ball (Crammer and
Chechik, 2004)—in other formulations of one-class clustering.
</bodyText>
<equation confidence="0.974239529411765">
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13
42
z
r
g
w
|d|
n
r
y z
w
|d|
n
Figure 3: (left) Words’ p(w) values when sorted by their
q(w) values; (right) words’ p(w) values.
g
</equation>
<figureCaption confidence="0.6622665">
Figure 2: (left) A simple generative model; (right) Latent
Topic/Background model (Section 4).
</figureCaption>
<bodyText confidence="0.989034987341772">
solve the one-class clustering problem. First, we ex-
press documents’ topicality in terms of aggregating
their words’ p ratios into an information-theoretic
“topicality measure”. The core is then composed
of k documents with the highest topicality measure.
We show that the proposed measure is optimal for
constructing the core cluster among documents of
equal length. However, our method is not useful
in a setup where some long documents have a top-
ical portion: such documents should be considered
on-topic, but their heavy tail of background words
overcomes the topical words’ influence. We gener-
alize our method to non-equally-long documents by
first extracting words that are supposed to be topi-
cal and then projecting documents over those words.
Such projection preserves the optimality characteris-
tic and results in constructing a more accurate core
cluster in practice. We call such a method of choos-
ing both topical words and core documents One-
Class Co-Clustering (OCCC).
It turns out that our OCCC method’s performance
depends heavily on choosing the number of topical
words. We propose a heuristic for setting this num-
ber. As another alternative, we propose a method
that does not require tuning this parameter: we
use words’ p ratios to initialize an EM algorithm
that computes the likelihood of documents to be-
long to the core—we then choose k documents of
maximal likelihood. We call this model the Latent
Topic/Background (LTB) model. LTB outperforms
OCCC in most of our test cases.
Our one-class clustering models have interesting
cross-links with models applied to other Informa-
tion Retrieval tasks. For example, a model that
resembles our OCCC, is proposed by Zhou and
Croft (2007) for query performance prediction. Tao
and Zhai (2004) describe a pseudo-relevance feed-
back model that is similar to our LTB. These types
of cross-links are common for the models that are
general enough and relatively simple. In this paper
we put particular emphasis on the simplicity of our
models, such that they are feasible for theoretical
analysis as well as for efficient implementation.
2 Motivation for using p ratios
Recall that we use the p(w) = ����
���� ratios to express
the level of our “surprise” of seeing the word w. A
high value of p(w) means that w is used in the cor-
pus more frequently than in general English, which,
we assume, implies that w is topical. The more top-
ical words a document contains, the more “topical”
it is—k most topical documents compose the core
Dk C D.
An important question is whether or not the p ra-
tios are sufficient to detecting the actually topical
words. To address this question, let us model the
corpus D using a simple graphical model (Figure 2
left). In this model, the word distribution p(w) is
represented as a mixture of two multinomial distri-
butions: pr over a set R of topical words, and pg
over all the words !9 D R in D. For each word wij
in a document di, we toss a coin Zij, such that, if
Zij = 1, then wij is sampled from pr, otherwise it
is sampled from pg. Define 7r °= p(Zij = 1).
If |9 |» |R |» 0, and if 7r » 0, then top-
ical words would tend to appear more often than
non-topical words. However, we cannot simply base
our conclusions on word counts, as some words are
naturally more frequent than others (in general En-
glish). Figure 3 (left) illustrates this observation: it
shows words’ p(w) values sorted by their q(w) val-
ues. It is hard to fit a curve that would separate be-
tween R and !g \ R. We notice however, that we can
“flatten” this graph by drawing p(w) values instead
(see Figure 3 right). Here, naturally frequent words
are penalized by the q factor, so we can assume that,
when re-normalized, p(w) behaves as a mixture of
two discrete uniform distributions. A simple thresh-
old can then separate between R and !9 \ R.
</bodyText>
<page confidence="0.933239">
43
</page>
<bodyText confidence="0.960758">
Proposition 1 Under the uniformity assumption, it
is sufficient to have a log-linear size sample (in |!9|)
in order to determine the set R with high probability.
See Bekkerman (2008) for the proof. The proposi-
tion states that in corpora of practical size4 the set of
topical words can be almost perfectly detected, sim-
ply by taking words with the highest ρ ratios. Con-
sequently, the core Dk will consist of k documents,
each of which contains more topical words than any
document from D \ Dk.
To illustrate this theoretical result, we followed
the generative process as described above, and con-
structed an artificial dataset with characteristics sim-
ilar to those of our WAD dataset (see Section 5.1).
In particular, we fixed the size of the artificial dataset
to be equal to the size of the WAD dataset (N =
330, 000). We set the ratio of topical words to 0.2
and assumed uniformity of the ρ values. In this
setup, we were able to detect the set of topical words
with a 98.5% accuracy.
</bodyText>
<subsectionHeader confidence="0.89292">
2.1 Max-KL Algorithm
</subsectionHeader>
<bodyText confidence="0.999958333333333">
In this section, we propose a simple information-
theoretic algorithm for identifying the core Dk, and
show that it is optimal under the uniformity assump-
tion. Given the ρ ratios of words, the aggregated
topicality of the corpus D can be expressed in terms
of the KL-divergence:
</bodyText>
<equation confidence="0.873132">
p(w)log p(w)
q(w)
p(d, w) log p(w)
q(w).
</equation>
<bodyText confidence="0.9967535">
A document d’s contribution to the aggregated topi-
cality measure will assess the topicality of d:
</bodyText>
<equation confidence="0.9610585">
p(d, w) log p(w)
q(w). (1)
</equation>
<bodyText confidence="0.999169666666667">
The core Dk will be composed of documents with
the highest topicality scores. A simple, greedy algo-
rithm for detecting Dk is then:
</bodyText>
<listItem confidence="0.9081968">
1. Sort documents according to their topicality
value (1), in decreasing order.
2. Select the first k documents.
4N = O(m log m), where N is the number of word tokens
in D, and m = |9 |is the size of the vocabulary.
</listItem>
<bodyText confidence="0.998136476190476">
Since the algorithm chooses documents with high
values of the KL divergence we call it the Max-KL
algorithm. We now argue that it is optimal under
the uniformity assumption. Indeed, if the corpus
D is large enough, then according to Proposition 1
(with high probability) any topical word w has a
lower ρ ratio than any non-topical word. Assume
that all documents are of the same length (|d |is con-
stant). The Max-KL algorithm chooses documents
that contain more topical words than any other doc-
ument in the corpus—which is exactly the definition
of the core, as presented in Section 1. We summarize
this observation in the following proposition:
Proposition 2 If the corpus D is large enough, and
all the documents are of the same length, then the
Max-KL algorithm is optimal for the one-class clus-
tering problem under the uniformity assumption.
In contrast to the (quite natural) uniformity assump-
tion, the all-the-same-length assumption is quite re-
strictive. Let us now propose an algorithm that over-
comes this issue.
</bodyText>
<sectionHeader confidence="0.981519" genericHeader="method">
3 One-Class Co-Clustering (OCCC)
</sectionHeader>
<bodyText confidence="0.999987333333333">
As accepted in Information Retrieval, we decide that
a document is on-topic if it has a topical portion, no
matter how long its non-topical portion is. There-
fore, we decide about documents’ topicality based
on topical words only—non-topical words can be
completely disregarded. This observation leads us to
proposing a one-class co-clustering (OCCC) algo-
rithm: we first detect the set R of topical words, rep-
resent documents over R, and then detect Dk based
on the new representation.5
We reexamine the document’s topicality score (1)
and omit non-topical words. The new score is then:
</bodyText>
<equation confidence="0.961373">
p�(d, w) log p(w)
q(w), (2)
</equation>
<bodyText confidence="0.995370125">
where p&apos;(d, w) = p(d, w)/(EwcR p(d, w)) is a
joint distribution of documents and (only) topical
words. The OCCC algorithm first uses ρ(w) to
5OCCC is the simplest, sequential co-clustering algorithm,
where words are clustered prior to clustering documents (see,
e.g., Slonim and Tishby (2000)). In OCCC, word clustering is
analogous to feature selection. More complex algorithms can
be considered, where this analogy is less obvious.
</bodyText>
<equation confidence="0.973278625">
KL(p||q) = �
wE9
= �dEE9
KLd(p||q) = �
wE9
KLd(p||q) = �
wER
44
</equation>
<bodyText confidence="0.997246666666667">
choose the most topical words, then it projects doc-
uments on these words and apply the Max-KL algo-
rithm, as summarized below:
</bodyText>
<listItem confidence="0.99922775">
1. Sort words according to their ρ ratios, in de-
creasing order.
2. Select a subset R of the first mr words.
3. Represent documents as bags-of-words over R
(delete counts of words from S \ R).
4. Sort documents according to their topicality
score (2), in decreasing order.
5. Select a subset Dk of the first k documents.
</listItem>
<bodyText confidence="0.952141222222222">
Considerations analogous to those presented in Sec-
tion 2.1, lead us to the following result:
Proposition 3 If the corpus D is large enough, the
OCCC algorithm is optimal for one-class clustering
of documents, under the uniformity assumption.
Despite its simplicity, the OCCC algorithm shows
excellent results on real-world data (see Section 5).
OCCC’s time complexity is particularly appealing:
O(N), where N is the number of word tokens in D.
</bodyText>
<subsectionHeader confidence="0.998379">
3.1 Choosing size mr of the word cluster
</subsectionHeader>
<bodyText confidence="0.999040857142857">
The choice of mr = |R |can be crucial. We propose
a useful heuristic for choosing it. We assume that
the distribution of ρ ratios for w E R is a Gaussian
with a mean µr » 1 and a variance σr, and that the
distribution of ρ ratios for w E S \ R is a Gaussian
with a mean µnr = 1 and a variance σnr. We also
assume that all the words with ρ(w) &lt; 1 are non-
topical. Since Gaussians are symmetric, we further
assume that the number of non-topical words with
ρ(w) &lt; 1 equals the number of non-topical words
with ρ(w) &gt; 1. Thus, our estimate of |S\R |is twice
the number of words with ρ(w) &lt; 1, and then the
number of topical words can be estimated as mr =
|S |− 2 · #{words with ρ(w) &lt; 11.
</bodyText>
<sectionHeader confidence="0.955928" genericHeader="method">
4 Latent Topic/Background (LTB) model
</sectionHeader>
<bodyText confidence="0.957554454545454">
Instead of sharply thresholding topical and non-
topical words, we can have them all, weighted with a
probability of being topical. Also, we notice that our
original generative model (Figure 2 left) assumes
that words are i.i.d. sampled, which can be relaxed
by deciding on the document topicality first. In our
new generative model (Figure 2 right), for each doc-
ument di, Yi is a Bernoulli random variable where
Algorithm 1 EM algorithm for one-class clustering
using the LTB model.
Input:
</bodyText>
<equation confidence="0.96539864516129">
D – the dataset
ρ(wl) = p(wL) – ρ scores for each word wl
q(wL) m l=1
T – number of EM iterations
Output: Posteriors p(Yi = 1|di, ΘT) for each doc di|ni=1
Initialization:
for each document di initialize π1i
for each word wl initialize p1r(wl) = Ωrρ(wl);
p1g(wl) = Ω9
ρ(wL), s.t. Ωr and Ωg are normalization factors
Main loop:
for all t = 1, ... ,Tdo
E-step:
for each document di compute αti = p(Yi = 1|di, Θt)
for each word token wij compute
βtij = p(Zij = 1|Yi = 1, wij, Θt)
M-step:
for each document di update πt+1 = |d: |Pj βij
for each word wl update
P P
i αt j δ(wij = wl) βt
i ij
(wl) = P P
i αt j βt
i ij
Nw − P P
i αt j δ(wij = wl) βt
i ij
N − P P
i αt j βt
i ij
</equation>
<bodyText confidence="0.9589159">
Yi = 1 corresponds to di being on-topic. As be-
fore, Zij decides on the topicality of a word token
wij, but now given Yi. Since not all words in a
core document are supposed to be topical, then for
each word of a core document we make a separate
decision (based on Zij) whether it is sampled from
pr(W) or pg(W). However, if a document does not
belong to the core (Yi = 0), each its word is sampled
from pg(W), i.e. p(Zij = 0|Yi = 0) = 1.
Inspired by Huang and Mitchell (2006), we use
the Expectation-Maximization (EM) algorithm to
exactly estimate parameters of our model from the
dataset. We now describe the model parameters O.
First, the probability of any document to belong to
the core is denoted by p(Yi = 1) = kn = pd (this
parameter is fixed and will not be learnt from data).
Second, for each document di, we maintain a proba-
bility of each its word to be topical given that the
document is on-topic, p(Zij = 1|Yi = 1) = πi
for i = 1, ... , n. Third, for each word wl (for
</bodyText>
<equation confidence="0.973385">
k = 1...m), we let p(wl|Zl = 1) = pr(wl) and
p(wl|Zl = 0) = pg(wl). The overall number of pa-
t+1
pr
t+1
pg
(wl) =
45
rameters is n + 2m + 1, one of which (pd) is preset.
The dataset likelihood is then:
[pd p(di|Yi = 1) + (1 − pd)p(di|Yi = 0)]
[πipr(wij) + (1 − πi)ps(wij)]
⎤ps(wij) ⎦ .
</equation>
<bodyText confidence="0.984239">
At each iteration t of the EM algorithm, we first
perform the E-step, where we compute the poste-
rior distribution of hidden variables {Yi} and {Zij}
given the current parameter values Θt and the data
D. Then, at the M-step, we compute the new pa-
rameter values Θt+1 that maximize the model log-
likelihood given Θt, D and the posterior distribution.
The initialization step is crucial for the EM al-
gorithm. Our pilot experimentation showed that if
distributions pr(W) and pg(W) are initialized as
uniform, the EM performance is close to random.
Therefore, we decided to initialize word probabili-
ties using normalized ρ scores. We do not propose
the optimal way to initialize πi parameters, however,
as we show later in Section 5, our LTB model ap-
pears to be quite robust to the choice of πi.
The EM procedure is presented in Algorithm 1.
For details, see Bekkerman (2008). After T itera-
tions, we sort the documents according to αi in de-
creasing order and choose the first k documents to
be the core. The complexity of Algorithm 1 is lin-
ear: O(TN). To avoid overfitting, we set T to be a
small number: in our experiments we fix T = 5.
</bodyText>
<sectionHeader confidence="0.996276" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.999494133333333">
We evaluate our OCCC and LTB models on two ap-
plications: a Web Mining task (Section 5.1), and a
Topic Detection and Tracking (TDT) (Allan, 2002)
task (Section 5.2).
To define our evaluation criteria, let C be the con-
structed cluster and let Cr be its portion consisting
of documents that actually belong to the core. We
define precision as Prec = |Cr|/|C|, recall as Rec =
|Cr|/k and F-measure as (2 Prec Rec)/(Prec+Rec).
Unless stated otherwise, in our experiments we fix
|C |= k, such that precision equals recall and is then
called one-class clustering accuracy, or just accu-
racy.
We applied our one-class clustering methods in
four setups:
</bodyText>
<listItem confidence="0.9958964">
• OCCC with the heuristic to choose mr (from
Section 3.1).
• OCCC with optimal mr. We unfairly choose
the number mr of topical words such that the
resulting accuracy is maximal. This setup
can be considered as the upper limit of the
OCCC’s performance, which can be hypotheti-
cally achieved if a better heuristic for choosing
mr is proposed.
• LTB initialized with πi = 0.5 (for each i).
</listItem>
<bodyText confidence="0.931489666666667">
As we show in Section 5.1 below, the LTB
model demonstrates good performance with
this straightforward initialization.
</bodyText>
<listItem confidence="0.627988428571428">
• LTB initialized with πi = pd. Quite naturally,
the number of topical words in a dataset de-
pends on the number of core documents. For
example, if the core is only 10% of a dataset, it
is unrealistic to assume that 50% of all words
are topical. In this setup, we condition the ratio
of topical words on the ratio of core documents.
</listItem>
<bodyText confidence="0.99456545">
We compare our methods with two existing al-
gorithms: (a) One-Class SVM clustering6 (Tax and
Duin, 2001); (b) One-Class Rate Distortion (OC-
RD) (Crammer et al., 2008). The later is considered
a state-of-the-art in one-class clustering. Also, to es-
tablish the lowest baseline, we show the result of a
random assignment of documents to the core Dk.
The OC-RD algorithm is based on rate-distortion
theory and expresses the one-class problem as a
lossy coding of each instance into a few possible
instance-dependent codewords. Each document is
represented as a distribution over words, and the KL-
divergence is used as a distortion function (gener-
ally, it can be any Bregman function). The algo-
rithm also uses an “inverse temperature” parameter
(denoted by β) that represents the tradeoff between
compression and distortion. An annealing process
is employed, in which the algorithm is applied with
a sequence of increasing values of β, when initial-
ized with the result obtained at the previous itera-
</bodyText>
<equation confidence="0.631095714285714">
6We used Chih-Jen Lin’s LibSVm with the -s 2 parame-
ter. We provided the core size using the -n parameter.
p(D) = Yn
i=1
= Yn
i=1
⎡
⎣pd
|d;|
Y
j=1
|d;|
+(1 − pd) Y
j=1
</equation>
<page confidence="0.714272">
46
</page>
<table confidence="0.999047125">
Method WAD TW
Random assignment 38.7% 34.9 f 3.1%
One-class SVM 46.3% 45.2 f 3.2%
One-class rate distortion 48.8% 63.6 f 3.5%
OCCC with the m,. heuristic 80.2% 61.4 f 4.5%
OCCC with optimal m 82.4% 68.3 f 3.6%
LTB initialized with iri = 0.5 79.8% 65.3 f 7.3%
LTB initialized with iri = pd 78.3% 68.0 f 5.9%
</table>
<tableCaption confidence="0.7214874">
Table 1: One-class clustering accuracy of our OCCC and
LTB models on the WAD and the TW detection tasks, as
compared to OC-SVM and OC-RD. For TW, the accura-
cies are macro-averaged over the 26 weekly chunks, with
the standard error of the mean presented after the f sign.
</tableCaption>
<bodyText confidence="0.999589333333333">
tion. The outcome is a sequence of cores with de-
creasing sizes. The annealing process is stopped
once the largest core size is equal to k.
</bodyText>
<subsectionHeader confidence="0.998595">
5.1 Web appearance disambiguation
</subsectionHeader>
<bodyText confidence="0.999931607142857">
Web appearance disambiguation (WAD) is proposed
by Bekkerman and McCallum (2005) as the problem
of reasoning whether a particular mention of a per-
son name in the Web refers to the person of interest
or to his or her unrelated namesake. The problem is
solved given a few names of people from one social
network, where the objective is to construct a cluster
of Web pages that mention names of related people,
while filtering out pages that mention their unrelated
namesakes.
WAD is a classic one-class clustering task, that
is tackled by Bekkerman and McCallum with simu-
lated one-class clustering: they use a sophisticated
agglomerative/conglomerative clustering method to
construct multiple clusters, out of which one cluster
is then selected. They also use a simple link struc-
ture (LS) analysis method that matches hyperlinks
of the Web pages in order to compose a cloud of
pages that are close to each other in the Web graph.
The authors suggest that the best performance can
be achieved by a hybrid of the two approaches.
We test our models on the WAD dataset,7 which
consists of 1085 Web pages that mention 12 people
names of AI researchers, such as Tom Mitchell and
Leslie Kaelbling. Out of the 1085 pages, 420 are
on-topic, so we apply our algorithms with k = 420.
At a preprocessing step, we binarize document vec-
tors and remove low frequent words (both in terms
</bodyText>
<table confidence="0.931277888888889">
7http://www.cs.umass.edu/˜ronb/name_
disambiguation.html
# OCCC LTB
1 cheyer artificial
2 kachites learning
3 quickreview cs
4 adddoc intelligence
5 aaai98 machine
6 kaelbling edu
7 mviews algorithms
8 mlittman proceedings
9 hardts computational
10 meuleau reinforcement
11 dipasquo papers
12 shakshuki cmu
13 xevil aaai
14 sangkyu workshop
15 gorfu kaelbling
</table>
<tableCaption confidence="0.990129">
Table 2: Most highly ranked words by OCCC and LTB,
on the WAD dataset.
</tableCaption>
<bodyText confidence="0.99988134375">
of p(w) and q(w)). The results are summarized in
the middle column of Table 1. We can see that both
OCCC and LTB dramatically outperform their com-
petitors, while showing practically indistinguishable
results compared to each other. Note that when the
size of the word cluster in OCCC is unfairly set to
its optimal value, mr = 2200, the OCCC method
is able to gain a 2% boost. However, for obvious
reasons, the optimal value of mr may not always be
obtained in practice.
Table 2 lists a few most topical words according
to the OCCC and LTB models. The OCCC algo-
rithm sorts words according to their p scores, such
that words that often occur in the dataset but rarely in
the Web, are on the top of the list. These are mostly
last names or login names of researchers, venues etc.
The EM algorithm of LTB is the given p scores as an
input to initialize p1r(w) and p1g(w), which are then
updated at each M-step. In the LTB columns, words
are sorted by pr(w). High quality of the LTB list
is due to conditional dependencies in our generative
model (via the Y nodes).
Solid lines in Figure 4 demonstrate the robustness
of our models to tuning their main parameters (mr
for OCCC, and the Tri initialization for LTB). As can
be seen from the left panel, OCCC shows robust
performance: the accuracy above 80% is obtained
when the word cluster is of any size in the 1000–
3000 range. The heuristic from Section 3.1 suggests
a cluster size of 1000. The LTB is even more robust:
practically any value of Tri (besides the very large
ones, Tri Pz� 1) can be chosen.
</bodyText>
<page confidence="0.503506">
47
</page>
<figure confidence="0.999441166666667">
OCCC method
OCCC
OCCC+link
0.82
0.78
0.74
0.7
0.660 2500 5000 7500 10000
size of word cluster
LTB method
0.86
0.82
0.78
0.74
0.7
0.660 0.2 0.4 0.6 0.8 1
πi parameter initialization
0.9
OCCC
OCC
LTB
0.8
0.7
0.6
0.5
200 400 600 800 1000
document cluster size
0.86
LTB
LTB+link
</figure>
<figureCaption confidence="0.6789825">
Figure 4: Web appearance disambiguation: (left)
OCCC accuracy as a function of the word cluster size;
(right) LTB accuracy over various initializations of Tri pa-
rameters. The red dotted lines show the accuracy of each
</figureCaption>
<bodyText confidence="0.999000588235294">
method’s results combined with the Link Structure model
results. On the absolute scale, OCCC outperforms LTB,
however LTB shows more robust behavior than OCCC.
To perform a fair comparison of our results
with those obtained by Bekkerman and McCal-
lum (2005), we construct hybrids of their link struc-
ture (LS) analysis model with our OCCC and LTB,
as follows. First, we take their LS core cluster,
which consists of 360 documents. Second, we pass
over all the WAD documents in the order as they
were ranked by either OCCC or LTB, and enlarge
the LS core with 60 most highly ranked documents
that did not occur in the LS core. In either case, we
end up with a hybrid core of 420 documents.
Dotted lines in Figure 4 show accuracies of the
resulting models. As the F-measure of the hy-
brid model proposed by Bekkerman and McCal-
lum (2005) is 80.3%, we can see that it is signifi-
cantly inferior to the results of either OCCC+LS or
LTB+LS, when their parameters are set to a small
value (mr &lt; 3000 for OCCC, Tri &lt; 0.06 for
LTB). Such a choice of parameter values can be
explained by the fact that we need only 60 docu-
ments to expand the LS core cluster to the required
size k = 420. When the values of mr and Tri are
small, both OCCC and LTB are able to build very
small and very precise core clusters, which is exactly
what we need here. The OCCC+LS hybrid is par-
ticularly successful, because it uses non-canonical
words (see Table 2) to compose a clean core that al-
most does not overlap with the LS core. Remark-
ably, the OCCC+LS model obtains 86.4% accuracy
with mr = 100, which is the state-of-the-art result
on the WAD dataset.
</bodyText>
<figureCaption confidence="0.659837">
Figure 5: Web appearance disambiguation: F-measure
</figureCaption>
<bodyText confidence="0.986470272727273">
as a function of document cluster size: a vertical line in-
dicates the point where precision equals recall (and there-
fore equals accuracy). “OCC” refers to the OCCC model
where all the words are taken as the word cluster (i.e. no
word filtering is done).
To answer the question how much our models are
sensitive to the choice of the core size k, we com-
puted the F-measure of both OCCC and LTB as a
function of k (Figure 5). It turns out that our meth-
ods are quite robust to tuning k: choosing any value
in the 300–500 range leads to good results.
</bodyText>
<subsectionHeader confidence="0.9968">
5.2 Detecting the topic of the week
</subsectionHeader>
<bodyText confidence="0.999929190476191">
Real-world data rarely consists of a clean core and
uniformly distributed noise. Usually, the noise has
some structure, namely, it may contain coherent
components. With this respect, one-class clustering
can be used to detect the largest coherent compo-
nent in a dataset, which is an integral part of many
applications. In this section, we solve the problem of
automatically detecting the Topic of the Week (TW)
in a newswire stream, i.e. detecting all articles in a
weekly news roundup that refer to the most broadly
discussed event.
We evaluate the TW detection task on the bench-
mark TDT-5 dataset8, which consists of 250 news
events spread over a time period of half a year, and
9,812 documents in English, Arabic and Chinese
(translated to English), annotated by their relation-
ship to those events.9 The largest event in TDT-5
dataset (#55106, titled “Bombing in Riyadh, Saudi
Arabia”) has 1,144 documents, while 66 out of the
250 events have only one document each. We split
the dataset to 26 weekly chunks (to have 26 full
</bodyText>
<figure confidence="0.8641686">
8http://projects.ldc.upenn.edu/TDT5/
9We take into account only labeled documents, while ignor-
ing unlabeled documents that can be found in the TDT-5 data.
48
Performance of OCCC and LTB on the &amp;quot;topic of the week&amp;quot; task
week
week
14 15 16 17 18 19 20 21 22 23 24 25 26
1
0.5
0
0.5
0
1
1 2 3 4 5 6 7 8 9 10 11 12 13
OCCC with the m h
r
OCCC with the opti
LTB initialized with n
LTB initialized with n
</figure>
<figureCaption confidence="0.999953">
Figure 6: “Topic of the week” detection task: Accuracies of two OCCC methods and two LTB methods.
</figureCaption>
<bodyText confidence="0.951574540540541">
weeks, we delete all the documents dated with the
last day in the dataset, which decreases the dataset’s
size to 9,781 documents). Each chunk contains from
138 to 1292 documents.
The one-class clustering accuracies, macro-
averaged over the 26 weekly chunks, are presented
in the right column of Table 1. As we can see, both
LTB models, as well as OCCC with the optimal mr,
outperform our baselines. Interestingly, even the op-
timal choice of mr does not lead OCCC to signif-
icantly superior results while compared with LTB.
The dataset-dependent initialization of LTB’s 7ri pa-
rameters (7ri = pd) appears to be preferable over the
dataset-independent one (7ri = 0.5).
Accuracies per week are shown in Figure 6. These
results reveal two interesting observations. First,
OCCC tends to outperform LTB only on data chunks
where the results are quite low in general (less than
60% accuracy). Specifically, on weeks 2, 4, 11,
and 16 the LTB models show extremely poor per-
formance. While investigating this phenomenon, we
discovered that in two of the four cases LTB was
able to construct very clean core clusters, however,
those clusters corresponded to the second largest
topic, while we evaluate our methods on the first
largest topic.10 Second, the (completely unsuper-
10For example, on the week-4 data, topic #55077 (“River
ferry sinks on Bangladeshi river”) was discovered by LTB as
the largest and most coherent one. However, in that dataset,
topic #55077 is represented by 20 documents, while topic
#55063 (“SARS Quarantined medics in Taiwan protest”) is
represented by 27 documents, such that topic #55077 is in fact
the second largest one.
vised) LTB model can obtain very good results on
some of the data chunks. For example, on weeks 5,
8, 19, 21, 23, 24, and 25 the LTB’s accuracy is above
90%, with a striking 100% on week-23.
</bodyText>
<sectionHeader confidence="0.998726" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999435">
We have developed the theory and proposed practi-
cal methods for one-class clustering in the text do-
main. The proposed algorithms are very simple,
very efficient and still surprisingly effective. More
sophisticated algorithms (e.g. an iterative11 version
of OCCC) are emerging.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999791727272727">
We thank Erik Learned-Miller for the inspiration
on this project. We also thank Gunjan Gupta,
James Allan, and Fernando Diaz for fruitful dis-
cussions. This work was supported in part by the
Center for Intelligent Information Retrieval and in
part by the Defense Advanced Research Projects
Agency (DARPA) under contract number HR0011-
06-C-0023. Any opinions, findings and conclusions
or recommendations expressed in this material are
the authors’ and do not necessarily reflect those of
the sponsor.
</bodyText>
<sectionHeader confidence="0.994294" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.973793220338983">
J. Allan, editor. 2002. Topic detection and tracking:
event-based information organization. Kluwer Aca-
demic Publishers.
11See, e.g., El-Yaniv and Souroujon (2001)
c
49
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proceedings of WWW-05, the 14th International World
Wide Web Conference.
R. Bekkerman. 2008. Combinatorial Markov Random
Fields and their Applications to Information Organi-
zation. Ph.D. thesis, University of Massachusetts at
Amherst.
K. Crammer and G. Chechik. 2004. A needle in a
haystack: local one-class optimization. In Proceed-
ings of the 21st International Conference on Machine
Learning.
K. Crammer, P. Talukdar, and F. Pereira. 2008. A rate-
distortion one-class model and its applications to clus-
tering. In Proceedings of the 25st International Con-
ference on Machine Learning.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
R. El-Yaniv and O. Souroujon. 2001. Iterative double
clustering for unsupervised and semi-supervised learn-
ing. In Advances in Neural Information Processing
Systems (NIPS-14).
G. Gupta and J. Ghosh. 2005. Robust one-class cluster-
ing using hybrid global and local search. In Proceed-
ings of the 22nd International Conference on Machine
Learning, pages 273–280.
Y. Huang and T. Mitchell. 2006. Text clustering with ex-
tended user feedback. In Proceedings of the 29th an-
nual international ACM SIGIR conference, pages 413–
420.
G. Lebanon. 2005. Riemannian Geometry and Statistical
Machine Learning. Ph.D. thesis, CMU.
B. Sch¨olkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Computa-
tion, 13(7):1443–1471.
N. Slonim and N. Tishby. 2000. Document cluster-
ing using word clusters via the information bottleneck
method. In Proceedings of the 23rd annual interna-
tional ACM SIGIR conference, pages 208–215.
T. Tao and C. Zhai. 2004. A two-stage mixture model for
pseudo feedback. In Proceedings of the 27th annual
international ACM SIGIR conference, pages 486–487.
D. M. J. Tax and R. P. W. Duin. 2001. Outliers and
data descriptions. In Proceedings of the 7th Annual
Conference of the Advanced Schoolfor Computing and
Imaging, pages 234–241.
Y. Zhou and W. B. Croft. 2007. Query performance pre-
diction in web search environments. In Proceedings
of the 30th Annual International ACM SIGIR Confer-
ence.
</reference>
<page confidence="0.867818">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495526">
<title confidence="0.999318">One-Class Clustering in the Text Domain</title>
<author confidence="0.998824">Ron Bekkerman Koby Crammer</author>
<affiliation confidence="0.99673">HP Laboratories University of Pennsylvania</affiliation>
<address confidence="0.999821">Palo Alto, CA 94304, USA Philadelphia, PA 19104, USA</address>
<email confidence="0.999708">ron.bekkerman@hp.comcrammer@cis.upenn.edu</email>
<abstract confidence="0.998776107142857">Having seen a news title “Alba denies wedding reports”, how do we infer that it is primarily about Jessica Alba, rather than about weddings or reports? We probably realize that, in a randomly driven sentence, the word “Alba” is less anticipated than “wedding” or “reports”, which adds value to the word “Alba” if used. Such anticipation can be modeled as a ratio between an empirical probability of the word (in a given corpus) and its estimated probability in general English. Aggregated over all words in a document, this ratio may be used as a measure of the document’s topicality. Assuming that the corpus consists of on-topic off-topic documents (we call them our goal is to determine which documents belong to the core. We propose two unsupervised methods for doing this. First, we assume that words are sampled i.i.d., and propose an information-theoretic framework for determining the core. Second, we relax the independence assumption and use a simple graphical model to rank documents according to their likelihood of belonging to the core. We discuss theoretical guarantees of the proposed methods and show their usefulness for Web Mining and Topic Detection and</abstract>
<intro confidence="0.511038">Tracking (TDT).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Topic detection and tracking: event-based information organization.</title>
<date>2002</date>
<editor>J. Allan, editor.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>2002</marker>
<rawString>J. Allan, editor. 2002. Topic detection and tracking: event-based information organization. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>e g 11See</author>
</authors>
<title>El-Yaniv and Souroujon</title>
<date>2001</date>
<marker>11See, 2001</marker>
<rawString>11See, e.g., El-Yaniv and Souroujon (2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>A McCallum</author>
</authors>
<title>Disambiguating web appearances of people in a social network. In</title>
<date>2005</date>
<booktitle>Proceedings of WWW-05, the 14th International World Wide Web Conference.</booktitle>
<contexts>
<context position="24946" citStr="Bekkerman and McCallum (2005)" startWordPosition="4313" endWordPosition="4316"> f 3.6% LTB initialized with iri = 0.5 79.8% 65.3 f 7.3% LTB initialized with iri = pd 78.3% 68.0 f 5.9% Table 1: One-class clustering accuracy of our OCCC and LTB models on the WAD and the TW detection tasks, as compared to OC-SVM and OC-RD. For TW, the accuracies are macro-averaged over the 26 weekly chunks, with the standard error of the mean presented after the f sign. tion. The outcome is a sequence of cores with decreasing sizes. The annealing process is stopped once the largest core size is equal to k. 5.1 Web appearance disambiguation Web appearance disambiguation (WAD) is proposed by Bekkerman and McCallum (2005) as the problem of reasoning whether a particular mention of a person name in the Web refers to the person of interest or to his or her unrelated namesake. The problem is solved given a few names of people from one social network, where the objective is to construct a cluster of Web pages that mention names of related people, while filtering out pages that mention their unrelated namesakes. WAD is a classic one-class clustering task, that is tackled by Bekkerman and McCallum with simulated one-class clustering: they use a sophisticated agglomerative/conglomerative clustering method to construc</context>
<context position="28971" citStr="Bekkerman and McCallum (2005)" startWordPosition="5014" endWordPosition="5018">0.74 0.7 0.660 0.2 0.4 0.6 0.8 1 πi parameter initialization 0.9 OCCC OCC LTB 0.8 0.7 0.6 0.5 200 400 600 800 1000 document cluster size 0.86 LTB LTB+link Figure 4: Web appearance disambiguation: (left) OCCC accuracy as a function of the word cluster size; (right) LTB accuracy over various initializations of Tri parameters. The red dotted lines show the accuracy of each method’s results combined with the Link Structure model results. On the absolute scale, OCCC outperforms LTB, however LTB shows more robust behavior than OCCC. To perform a fair comparison of our results with those obtained by Bekkerman and McCallum (2005), we construct hybrids of their link structure (LS) analysis model with our OCCC and LTB, as follows. First, we take their LS core cluster, which consists of 360 documents. Second, we pass over all the WAD documents in the order as they were ranked by either OCCC or LTB, and enlarge the LS core with 60 most highly ranked documents that did not occur in the LS core. In either case, we end up with a hybrid core of 420 documents. Dotted lines in Figure 4 show accuracies of the resulting models. As the F-measure of the hybrid model proposed by Bekkerman and McCallum (2005) is 80.3%, we can see tha</context>
</contexts>
<marker>Bekkerman, McCallum, 2005</marker>
<rawString>R. Bekkerman and A. McCallum. 2005. Disambiguating web appearances of people in a social network. In Proceedings of WWW-05, the 14th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
</authors>
<title>Combinatorial Markov Random Fields and their Applications to Information Organization.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts at Amherst.</institution>
<contexts>
<context position="12463" citStr="Bekkerman (2008)" startWordPosition="2049" endWordPosition="2050">sorted by their q(w) values. It is hard to fit a curve that would separate between R and !g \ R. We notice however, that we can “flatten” this graph by drawing p(w) values instead (see Figure 3 right). Here, naturally frequent words are penalized by the q factor, so we can assume that, when re-normalized, p(w) behaves as a mixture of two discrete uniform distributions. A simple threshold can then separate between R and !9 \ R. 43 Proposition 1 Under the uniformity assumption, it is sufficient to have a log-linear size sample (in |!9|) in order to determine the set R with high probability. See Bekkerman (2008) for the proof. The proposition states that in corpora of practical size4 the set of topical words can be almost perfectly detected, simply by taking words with the highest ρ ratios. Consequently, the core Dk will consist of k documents, each of which contains more topical words than any document from D \ Dk. To illustrate this theoretical result, we followed the generative process as described above, and constructed an artificial dataset with characteristics similar to those of our WAD dataset (see Section 5.1). In particular, we fixed the size of the artificial dataset to be equal to the siz</context>
<context position="21190" citStr="Bekkerman (2008)" startWordPosition="3651" endWordPosition="3652"> values Θt+1 that maximize the model loglikelihood given Θt, D and the posterior distribution. The initialization step is crucial for the EM algorithm. Our pilot experimentation showed that if distributions pr(W) and pg(W) are initialized as uniform, the EM performance is close to random. Therefore, we decided to initialize word probabilities using normalized ρ scores. We do not propose the optimal way to initialize πi parameters, however, as we show later in Section 5, our LTB model appears to be quite robust to the choice of πi. The EM procedure is presented in Algorithm 1. For details, see Bekkerman (2008). After T iterations, we sort the documents according to αi in decreasing order and choose the first k documents to be the core. The complexity of Algorithm 1 is linear: O(TN). To avoid overfitting, we set T to be a small number: in our experiments we fix T = 5. 5 Experimentation We evaluate our OCCC and LTB models on two applications: a Web Mining task (Section 5.1), and a Topic Detection and Tracking (TDT) (Allan, 2002) task (Section 5.2). To define our evaluation criteria, let C be the constructed cluster and let Cr be its portion consisting of documents that actually belong to the core. We</context>
</contexts>
<marker>Bekkerman, 2008</marker>
<rawString>R. Bekkerman. 2008. Combinatorial Markov Random Fields and their Applications to Information Organization. Ph.D. thesis, University of Massachusetts at Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>G Chechik</author>
</authors>
<title>A needle in a haystack: local one-class optimization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6143" citStr="Crammer and Chechik, 2004" startWordPosition="963" endWordPosition="966">al is to identify a few outliers from the given set of examples, where the vast majority of the examples are considered relevant. Alternatively, a complementary goal is to distill a subset of relevant examples, in the space with many outliers (Crammer and Chechik, 1As such, or after applying the kernel trick (Cristianini and Shawe-Taylor, 2000) 2004; Gupta and Ghosh, 2005; Crammer et al., 2008). Most of the one-class approaches employ geometrical concepts to capture the notion of relevancy (or irrelevancy) using either hyperplanes (Sch¨olkopf et al., 2001) or hyperspheres (Tax and Duin, 2001; Crammer and Chechik, 2004; Gupta and Ghosh, 2005). In this paper we adopt the latter approach: we formulate one-class clustering in text as an optimization task of identifying the most coherent subset (the core) of k documents drawn from a given pool of n &gt; k documents.2 Given a collection D of on-topic and off-topic documents, we assume that on-topic documents share a portion of their vocabulary that consists of “relatively rare” words, i.e. words that are used in D more often than they are used in general English. We call them topical words. For example, if some documents in D share words such as “Bayesian”, “classi</context>
<context position="8134" citStr="Crammer and Chechik, 2004" startWordPosition="1305" endWordPosition="1308">measure depends on the quality of estimating q(w), i.e. the general English word distribution, which is usually estimated over a large text collection. The larger the collection is, the better would be the estimation. Recently, Google has released the Web 1T dataset3 that provides q(w) estimated on a text collection of one trillion tokens. We use it in our experimentation. We propose two methods that use the p ratio to 2The parameter k is analogous to the number of clusters in (multi-class) clustering, as well as to the number of outliers (Tax and Duin, 2001) or the radius of Bregmanian ball (Crammer and Chechik, 2004)—in other formulations of one-class clustering. 3http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2006T13 42 z r g w |d| n r y z w |d| n Figure 3: (left) Words’ p(w) values when sorted by their q(w) values; (right) words’ p(w) values. g Figure 2: (left) A simple generative model; (right) Latent Topic/Background model (Section 4). solve the one-class clustering problem. First, we express documents’ topicality in terms of aggregating their words’ p ratios into an information-theoretic “topicality measure”. The core is then composed of k documents with the highest topicality measur</context>
</contexts>
<marker>Crammer, Chechik, 2004</marker>
<rawString>K. Crammer and G. Chechik. 2004. A needle in a haystack: local one-class optimization. In Proceedings of the 21st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>P Talukdar</author>
<author>F Pereira</author>
</authors>
<title>A ratedistortion one-class model and its applications to clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25st International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5915" citStr="Crammer et al., 2008" startWordPosition="929" endWordPosition="932"> which work well in text. One-class learning problems have been studied as either outlier detection or identifying a small coherent subset. In one-class outlier detection (Tax and Duin, 2001; Sch¨olkopf et al., 2001), the goal is to identify a few outliers from the given set of examples, where the vast majority of the examples are considered relevant. Alternatively, a complementary goal is to distill a subset of relevant examples, in the space with many outliers (Crammer and Chechik, 1As such, or after applying the kernel trick (Cristianini and Shawe-Taylor, 2000) 2004; Gupta and Ghosh, 2005; Crammer et al., 2008). Most of the one-class approaches employ geometrical concepts to capture the notion of relevancy (or irrelevancy) using either hyperplanes (Sch¨olkopf et al., 2001) or hyperspheres (Tax and Duin, 2001; Crammer and Chechik, 2004; Gupta and Ghosh, 2005). In this paper we adopt the latter approach: we formulate one-class clustering in text as an optimization task of identifying the most coherent subset (the core) of k documents drawn from a given pool of n &gt; k documents.2 Given a collection D of on-topic and off-topic documents, we assume that on-topic documents share a portion of their vocabula</context>
<context position="23117" citStr="Crammer et al., 2008" startWordPosition="3992" endWordPosition="3995">h i). As we show in Section 5.1 below, the LTB model demonstrates good performance with this straightforward initialization. • LTB initialized with πi = pd. Quite naturally, the number of topical words in a dataset depends on the number of core documents. For example, if the core is only 10% of a dataset, it is unrealistic to assume that 50% of all words are topical. In this setup, we condition the ratio of topical words on the ratio of core documents. We compare our methods with two existing algorithms: (a) One-Class SVM clustering6 (Tax and Duin, 2001); (b) One-Class Rate Distortion (OCRD) (Crammer et al., 2008). The later is considered a state-of-the-art in one-class clustering. Also, to establish the lowest baseline, we show the result of a random assignment of documents to the core Dk. The OC-RD algorithm is based on rate-distortion theory and expresses the one-class problem as a lossy coding of each instance into a few possible instance-dependent codewords. Each document is represented as a distribution over words, and the KLdivergence is used as a distortion function (generally, it can be any Bregman function). The algorithm also uses an “inverse temperature” parameter (denoted by β) that repres</context>
</contexts>
<marker>Crammer, Talukdar, Pereira, 2008</marker>
<rawString>K. Crammer, P. Talukdar, and F. Pereira. 2008. A ratedistortion one-class model and its applications to clustering. In Proceedings of the 25st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5864" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="920" endWordPosition="923">gure 1). Here we propose alternative ways for detecting the core, which work well in text. One-class learning problems have been studied as either outlier detection or identifying a small coherent subset. In one-class outlier detection (Tax and Duin, 2001; Sch¨olkopf et al., 2001), the goal is to identify a few outliers from the given set of examples, where the vast majority of the examples are considered relevant. Alternatively, a complementary goal is to distill a subset of relevant examples, in the space with many outliers (Crammer and Chechik, 1As such, or after applying the kernel trick (Cristianini and Shawe-Taylor, 2000) 2004; Gupta and Ghosh, 2005; Crammer et al., 2008). Most of the one-class approaches employ geometrical concepts to capture the notion of relevancy (or irrelevancy) using either hyperplanes (Sch¨olkopf et al., 2001) or hyperspheres (Tax and Duin, 2001; Crammer and Chechik, 2004; Gupta and Ghosh, 2005). In this paper we adopt the latter approach: we formulate one-class clustering in text as an optimization task of identifying the most coherent subset (the core) of k documents drawn from a given pool of n &gt; k documents.2 Given a collection D of on-topic and off-topic documents, we assume that o</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R El-Yaniv</author>
<author>O Souroujon</author>
</authors>
<title>Iterative double clustering for unsupervised and semi-supervised learning.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS-14).</booktitle>
<marker>El-Yaniv, Souroujon, 2001</marker>
<rawString>R. El-Yaniv and O. Souroujon. 2001. Iterative double clustering for unsupervised and semi-supervised learning. In Advances in Neural Information Processing Systems (NIPS-14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gupta</author>
<author>J Ghosh</author>
</authors>
<title>Robust one-class clustering using hybrid global and local search.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="5892" citStr="Gupta and Ghosh, 2005" startWordPosition="925" endWordPosition="928">for detecting the core, which work well in text. One-class learning problems have been studied as either outlier detection or identifying a small coherent subset. In one-class outlier detection (Tax and Duin, 2001; Sch¨olkopf et al., 2001), the goal is to identify a few outliers from the given set of examples, where the vast majority of the examples are considered relevant. Alternatively, a complementary goal is to distill a subset of relevant examples, in the space with many outliers (Crammer and Chechik, 1As such, or after applying the kernel trick (Cristianini and Shawe-Taylor, 2000) 2004; Gupta and Ghosh, 2005; Crammer et al., 2008). Most of the one-class approaches employ geometrical concepts to capture the notion of relevancy (or irrelevancy) using either hyperplanes (Sch¨olkopf et al., 2001) or hyperspheres (Tax and Duin, 2001; Crammer and Chechik, 2004; Gupta and Ghosh, 2005). In this paper we adopt the latter approach: we formulate one-class clustering in text as an optimization task of identifying the most coherent subset (the core) of k documents drawn from a given pool of n &gt; k documents.2 Given a collection D of on-topic and off-topic documents, we assume that on-topic documents share a po</context>
</contexts>
<marker>Gupta, Ghosh, 2005</marker>
<rawString>G. Gupta and J. Ghosh. 2005. Robust one-class clustering using hybrid global and local search. In Proceedings of the 22nd International Conference on Machine Learning, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Huang</author>
<author>T Mitchell</author>
</authors>
<title>Text clustering with extended user feedback.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference,</booktitle>
<pages>413--420</pages>
<contexts>
<context position="19531" citStr="Huang and Mitchell (2006)" startWordPosition="3337" endWordPosition="3340">: |Pj βij for each word wl update P P i αt j δ(wij = wl) βt i ij (wl) = P P i αt j βt i ij Nw − P P i αt j δ(wij = wl) βt i ij N − P P i αt j βt i ij Yi = 1 corresponds to di being on-topic. As before, Zij decides on the topicality of a word token wij, but now given Yi. Since not all words in a core document are supposed to be topical, then for each word of a core document we make a separate decision (based on Zij) whether it is sampled from pr(W) or pg(W). However, if a document does not belong to the core (Yi = 0), each its word is sampled from pg(W), i.e. p(Zij = 0|Yi = 0) = 1. Inspired by Huang and Mitchell (2006), we use the Expectation-Maximization (EM) algorithm to exactly estimate parameters of our model from the dataset. We now describe the model parameters O. First, the probability of any document to belong to the core is denoted by p(Yi = 1) = kn = pd (this parameter is fixed and will not be learnt from data). Second, for each document di, we maintain a probability of each its word to be topical given that the document is on-topic, p(Zij = 1|Yi = 1) = πi for i = 1, ... , n. Third, for each word wl (for k = 1...m), we let p(wl|Zl = 1) = pr(wl) and p(wl|Zl = 0) = pg(wl). The overall number of pat+</context>
</contexts>
<marker>Huang, Mitchell, 2006</marker>
<rawString>Y. Huang and T. Mitchell. 2006. Text clustering with extended user feedback. In Proceedings of the 29th annual international ACM SIGIR conference, pages 413– 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lebanon</author>
</authors>
<date>2005</date>
<booktitle>Riemannian Geometry and Statistical Machine Learning. Ph.D. thesis, CMU.</booktitle>
<contexts>
<context position="5055" citStr="Lebanon, 2005" startWordPosition="789" endWordPosition="790">wn, or too complex to be explicitly modeled. One-class problems are traditionally approached using vector-space methods, where a convex decision boundary is built around the data instances of the desired class, separating it from the rest of the universe. In the text domain, however, those vectorspace models are questionably applicable—unlike effective binary vector-space models. In binary models, decision boundaries are linear1, whereas in (vector-space) one-class models, the boundaries are usually hyperspherical. Intuitively, since core documents tend to lie on a lower-dimensional manifold (Lebanon, 2005), inducing hyperspherical boundaries may be sub-optimal as they tend to either capture just a portion of the core, or capture too much space around it (see illustration in Figure 1). Here we propose alternative ways for detecting the core, which work well in text. One-class learning problems have been studied as either outlier detection or identifying a small coherent subset. In one-class outlier detection (Tax and Duin, 2001; Sch¨olkopf et al., 2001), the goal is to identify a few outliers from the given set of examples, where the vast majority of the examples are considered relevant. Alterna</context>
</contexts>
<marker>Lebanon, 2005</marker>
<rawString>G. Lebanon. 2005. Riemannian Geometry and Statistical Machine Learning. Ph.D. thesis, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>J C Platt</author>
<author>J C Shawe-Taylor</author>
<author>A J Smola</author>
<author>R C Williamson</author>
</authors>
<title>Estimating the support of a high-dimensional distribution.</title>
<date>2001</date>
<journal>Neural Computation,</journal>
<volume>13</volume>
<issue>7</issue>
<marker>Sch¨olkopf, Platt, Shawe-Taylor, Smola, Williamson, 2001</marker>
<rawString>B. Sch¨olkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola, and R. C. Williamson. 2001. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443–1471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>Document clustering using word clusters via the information bottleneck method.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd annual international ACM SIGIR conference,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="16037" citStr="Slonim and Tishby (2000)" startWordPosition="2659" endWordPosition="2662"> observation leads us to proposing a one-class co-clustering (OCCC) algorithm: we first detect the set R of topical words, represent documents over R, and then detect Dk based on the new representation.5 We reexamine the document’s topicality score (1) and omit non-topical words. The new score is then: p�(d, w) log p(w) q(w), (2) where p&apos;(d, w) = p(d, w)/(EwcR p(d, w)) is a joint distribution of documents and (only) topical words. The OCCC algorithm first uses ρ(w) to 5OCCC is the simplest, sequential co-clustering algorithm, where words are clustered prior to clustering documents (see, e.g., Slonim and Tishby (2000)). In OCCC, word clustering is analogous to feature selection. More complex algorithms can be considered, where this analogy is less obvious. KL(p||q) = � wE9 = �dEE9 KLd(p||q) = � wE9 KLd(p||q) = � wER 44 choose the most topical words, then it projects documents on these words and apply the Max-KL algorithm, as summarized below: 1. Sort words according to their ρ ratios, in decreasing order. 2. Select a subset R of the first mr words. 3. Represent documents as bags-of-words over R (delete counts of words from S \ R). 4. Sort documents according to their topicality score (2), in decreasing ord</context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>N. Slonim and N. Tishby. 2000. Document clustering using word clusters via the information bottleneck method. In Proceedings of the 23rd annual international ACM SIGIR conference, pages 208–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tao</author>
<author>C Zhai</author>
</authors>
<title>A two-stage mixture model for pseudo feedback.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference,</booktitle>
<pages>486--487</pages>
<contexts>
<context position="10233" citStr="Tao and Zhai (2004)" startWordPosition="1634" endWordPosition="1637">As another alternative, we propose a method that does not require tuning this parameter: we use words’ p ratios to initialize an EM algorithm that computes the likelihood of documents to belong to the core—we then choose k documents of maximal likelihood. We call this model the Latent Topic/Background (LTB) model. LTB outperforms OCCC in most of our test cases. Our one-class clustering models have interesting cross-links with models applied to other Information Retrieval tasks. For example, a model that resembles our OCCC, is proposed by Zhou and Croft (2007) for query performance prediction. Tao and Zhai (2004) describe a pseudo-relevance feedback model that is similar to our LTB. These types of cross-links are common for the models that are general enough and relatively simple. In this paper we put particular emphasis on the simplicity of our models, such that they are feasible for theoretical analysis as well as for efficient implementation. 2 Motivation for using p ratios Recall that we use the p(w) = ���� ���� ratios to express the level of our “surprise” of seeing the word w. A high value of p(w) means that w is used in the corpus more frequently than in general English, which, we assume, impli</context>
</contexts>
<marker>Tao, Zhai, 2004</marker>
<rawString>T. Tao and C. Zhai. 2004. A two-stage mixture model for pseudo feedback. In Proceedings of the 27th annual international ACM SIGIR conference, pages 486–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M J Tax</author>
<author>R P W Duin</author>
</authors>
<title>Outliers and data descriptions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th Annual Conference of the Advanced Schoolfor Computing and Imaging,</booktitle>
<pages>234--241</pages>
<contexts>
<context position="5484" citStr="Tax and Duin, 2001" startWordPosition="858" endWordPosition="861">linear1, whereas in (vector-space) one-class models, the boundaries are usually hyperspherical. Intuitively, since core documents tend to lie on a lower-dimensional manifold (Lebanon, 2005), inducing hyperspherical boundaries may be sub-optimal as they tend to either capture just a portion of the core, or capture too much space around it (see illustration in Figure 1). Here we propose alternative ways for detecting the core, which work well in text. One-class learning problems have been studied as either outlier detection or identifying a small coherent subset. In one-class outlier detection (Tax and Duin, 2001; Sch¨olkopf et al., 2001), the goal is to identify a few outliers from the given set of examples, where the vast majority of the examples are considered relevant. Alternatively, a complementary goal is to distill a subset of relevant examples, in the space with many outliers (Crammer and Chechik, 1As such, or after applying the kernel trick (Cristianini and Shawe-Taylor, 2000) 2004; Gupta and Ghosh, 2005; Crammer et al., 2008). Most of the one-class approaches employ geometrical concepts to capture the notion of relevancy (or irrelevancy) using either hyperplanes (Sch¨olkopf et al., 2001) or </context>
<context position="8073" citStr="Tax and Duin, 2001" startWordPosition="1295" endWordPosition="1298">e the topicality of w. Obviously, the quality of this measure depends on the quality of estimating q(w), i.e. the general English word distribution, which is usually estimated over a large text collection. The larger the collection is, the better would be the estimation. Recently, Google has released the Web 1T dataset3 that provides q(w) estimated on a text collection of one trillion tokens. We use it in our experimentation. We propose two methods that use the p ratio to 2The parameter k is analogous to the number of clusters in (multi-class) clustering, as well as to the number of outliers (Tax and Duin, 2001) or the radius of Bregmanian ball (Crammer and Chechik, 2004)—in other formulations of one-class clustering. 3http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2006T13 42 z r g w |d| n r y z w |d| n Figure 3: (left) Words’ p(w) values when sorted by their q(w) values; (right) words’ p(w) values. g Figure 2: (left) A simple generative model; (right) Latent Topic/Background model (Section 4). solve the one-class clustering problem. First, we express documents’ topicality in terms of aggregating their words’ p ratios into an information-theoretic “topicality measure”. The core is th</context>
<context position="23056" citStr="Tax and Duin, 2001" startWordPosition="3982" endWordPosition="3985">ng mr is proposed. • LTB initialized with πi = 0.5 (for each i). As we show in Section 5.1 below, the LTB model demonstrates good performance with this straightforward initialization. • LTB initialized with πi = pd. Quite naturally, the number of topical words in a dataset depends on the number of core documents. For example, if the core is only 10% of a dataset, it is unrealistic to assume that 50% of all words are topical. In this setup, we condition the ratio of topical words on the ratio of core documents. We compare our methods with two existing algorithms: (a) One-Class SVM clustering6 (Tax and Duin, 2001); (b) One-Class Rate Distortion (OCRD) (Crammer et al., 2008). The later is considered a state-of-the-art in one-class clustering. Also, to establish the lowest baseline, we show the result of a random assignment of documents to the core Dk. The OC-RD algorithm is based on rate-distortion theory and expresses the one-class problem as a lossy coding of each instance into a few possible instance-dependent codewords. Each document is represented as a distribution over words, and the KLdivergence is used as a distortion function (generally, it can be any Bregman function). The algorithm also uses </context>
</contexts>
<marker>Tax, Duin, 2001</marker>
<rawString>D. M. J. Tax and R. P. W. Duin. 2001. Outliers and data descriptions. In Proceedings of the 7th Annual Conference of the Advanced Schoolfor Computing and Imaging, pages 234–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
<author>W B Croft</author>
</authors>
<title>Query performance prediction in web search environments.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th Annual International ACM SIGIR Conference.</booktitle>
<contexts>
<context position="10179" citStr="Zhou and Croft (2007)" startWordPosition="1626" endWordPosition="1629"> words. We propose a heuristic for setting this number. As another alternative, we propose a method that does not require tuning this parameter: we use words’ p ratios to initialize an EM algorithm that computes the likelihood of documents to belong to the core—we then choose k documents of maximal likelihood. We call this model the Latent Topic/Background (LTB) model. LTB outperforms OCCC in most of our test cases. Our one-class clustering models have interesting cross-links with models applied to other Information Retrieval tasks. For example, a model that resembles our OCCC, is proposed by Zhou and Croft (2007) for query performance prediction. Tao and Zhai (2004) describe a pseudo-relevance feedback model that is similar to our LTB. These types of cross-links are common for the models that are general enough and relatively simple. In this paper we put particular emphasis on the simplicity of our models, such that they are feasible for theoretical analysis as well as for efficient implementation. 2 Motivation for using p ratios Recall that we use the p(w) = ���� ���� ratios to express the level of our “surprise” of seeing the word w. A high value of p(w) means that w is used in the corpus more frequ</context>
</contexts>
<marker>Zhou, Croft, 2007</marker>
<rawString>Y. Zhou and W. B. Croft. 2007. Query performance prediction in web search environments. In Proceedings of the 30th Annual International ACM SIGIR Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>