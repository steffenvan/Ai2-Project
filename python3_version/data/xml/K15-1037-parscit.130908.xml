<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008720">
<title confidence="0.9950575">
One Million Sense-Tagged Instances
for Word Sense Disambiguation and Induction
</title>
<author confidence="0.990939">
Kaveh Taghipour
</author>
<affiliation confidence="0.999848">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9662305">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.997964">
kaveh@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99970135">
Supervised word sense disambiguation
(WSD) systems are usually the best per-
forming systems when evaluated on stan-
dard benchmarks. However, these systems
need annotated training data to function
properly. While there are some publicly
available open source WSD systems, very
few large annotated datasets are available
to the research community. The two main
goals of this paper are to extract and an-
notate a large number of samples and re-
lease them for public use, and also to eval-
uate this dataset against some word sense
disambiguation and induction tasks. We
show that the open source IMS WSD sys-
tem trained on our dataset achieves state-
of-the-art results in standard disambigua-
tion tasks and a recent word sense induc-
tion task, outperforming several task sub-
missions and strong baselines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999061764705883">
Identifying the meaning of a word automatically
has been an interesting research topic for a few
decades. The approaches used to solve this prob-
lem can be roughly categorized into two main
classes: Word Sense Disambiguation (WSD) and
Word Sense Induction (WSI) (Navigli, 2009). For
word sense disambiguation, some systems are
based on supervised machine learning algorithms
(Lee et al., 2004; Zhong and Ng, 2010), while oth-
ers use ontologies and other structured knowledge
sources (Ponzetto and Navigli, 2010; Agirre et al.,
2014; Moro et al., 2014).
There are several sense-annotated datasets for
WSD (Miller et al., 1993; Ng and Lee, 1996; Pas-
sonneau et al., 2012). However, these datasets
either include few samples per word sense or
only cover a small set of polysemous words.
</bodyText>
<author confidence="0.697382">
Hwee Tou Ng
</author>
<affiliation confidence="0.9984625">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9418555">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.982584">
nght@comp.nus.edu.sg
</email>
<bodyText confidence="0.999696658536585">
To overcome these limitations, automatic meth-
ods have been developed for annotating training
samples. For example, Ng et al. (2003), Chan
and Ng (2005), and Zhong and Ng (2009) used
Chinese-English parallel corpora to extract sam-
ples for training their supervised WSD system.
Diab (2004) proposed an unsupervised bootstrap-
ping method to automatically generate a sense-
annotated dataset. Another example of auto-
matically created datasets is the semi-supervised
method used in (K¨ubler and Zhekova, 2009),
which employed a supervised classifier to label in-
stances.
The two main contributions of this paper are
as follows. First, we employ the same method
used in (Ng et al., 2003; Chan and Ng, 2005) to
semi-automatically annotate one million training
samples based on the WordNet sense inventory
(Miller, 1995) and release the annotated corpus for
public use. To our knowledge, this annotated set of
sense-tagged samples is the largest publicly avail-
able dataset for word sense disambiguation. Sec-
ond, we train an open source supervised WSD sys-
tem, IMS (Zhong and Ng, 2010), using our data
and evaluate it against standard WSD and WSI
benchmarks. We show that our system outper-
forms other state-of-the-art systems in most cases.
As any WSD system is also a WSI system when
we treat the pre-defined sense inventory of the
WSD system as the induced word senses, a WSD
system can also be evaluated and used for WSI.
Some researchers believe that, in some cases, WSI
methods may perform better than WSD systems
(Jurgens and Klapaftis, 2013; Wang et al., 2015).
However, we argue that WSI systems have few
advantages compared to WSD methods and ac-
cording to our results, disambiguation systems
consistently outperform induction systems. Al-
though there are some cases where WSI systems
can be useful (e.g., for resource-poor languages),
in most cases WSD systems are preferable because
</bodyText>
<page confidence="0.97944">
338
</page>
<note confidence="0.614262">
Proceedings of the 19th Conference on Computational Language Learning, pages 338–344,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998043714285714">
of higher accuracy and better interpretability of
output.
The rest of this paper is composed of the follow-
ing sections. Section 2 explains our methodology
for creating the training data. We evaluate the ex-
tracted data in Section 3 and finally, we conclude
the paper in Section 4.
</bodyText>
<sectionHeader confidence="0.894972" genericHeader="method">
2 Training Data
</sectionHeader>
<bodyText confidence="0.999961260869565">
In order to train a supervised word sense disam-
biguation system, we extract and sense-tag data
from a freely available parallel corpus, in a semi-
automatic manner. To increase the coverage and
therefore the ultimate performance of our WSD
system, we also make use of existing sense-tagged
datasets. This section explains each step in detail.
Since the main purpose of this paper is to build
and release a publicly available training set for
word sense disambiguation systems, we selected
the MultiUN corpus (MUN) (Eisele and Chen,
2010) produced in the EuroMatrixPlus project1.
This corpus is freely available via the project
website and includes seven languages. An auto-
matically sentence-aligned version of this dataset
can be downloaded from the OPUS website2 and
therefore we decided to extract samples from this
sentence-aligned version.
To extract training data from the MultiUN par-
allel corpus, we follow the approach described
in (Chan and Ng, 2005) and select the Chinese-
English part of the MultiUN corpus. The extrac-
tion method has the following steps:
</bodyText>
<listItem confidence="0.984156214285714">
1. Tokenization and word segmentation: The
English side of the corpus is tokenized us-
ing the Penn TreeBank tokenizer3, while the
Chinese side of the corpus is segmented us-
ing the Chinese word segmenter of (Low et
al., 2005).
2. Word alignment: After tokenizing the texts,
GIZA++ (Och and Ney, 2000) is used to align
English and Chinese words.
3. Part-of-speech (POS) tagging and lemmati-
zation: After running GIZA++, we use the
OpenNLP POS tagger4 and then the Word-
Net lemmatizer to obtain POS tags and word
lemmas of the English sentence.
</listItem>
<footnote confidence="0.9999835">
1http://www.euromatrixplus.eu/multi-un
2http://opus.lingfil.uu.se/MultiUN.php
3http://www.cis.upenn.edu/∼treebank/tokenization.html
4http://opennlp.apache.org
</footnote>
<listItem confidence="0.677819272727273">
4. Annotation: In order to assign a WordNet
sense tag to an English word we in a sen-
tence, we make use of the aligned Chinese
translation w, of we, based on the automatic
word alignment formed by GIZA++. For
each sense i of we in the WordNet sense in-
ventory (WordNet 1.7.1), a list of Chinese
translations of sense i of we has been manu-
ally created. If w, matches one of these Chi-
nese translations of sense i, then we is tagged
with sense i.
</listItem>
<bodyText confidence="0.999750825">
The average time needed to manually assign
Chinese translations to the word senses of one
word type for noun, adjective, and verb is 20, 25,
and 40 minutes respectively (Chan, 2008). The
above procedure annotates the top 60% most fre-
quent word types (nouns, verbs, and adjectives) in
English, selected based on their frequency in the
Brown corpus. This set of selected word types in-
cludes 649 nouns, 190 verbs, and 319 adjectives.
Since automatic sentence and word alignment
can be noisy, and a Chinese word w, can occa-
sionally be a valid translation of more than one
sense of an English word we, the senses tagged
using the above procedure may be erroneous. To
get an idea of the accuracy of the senses tagged
with this procedure, we manually evaluated a sub-
set of 1,000 randomly selected sense-tagged in-
stances. Although the sense inventory is fine-
grained (WordNet 1.7.1), the sense-tag accuracy
achieved is 83.7%. We also performed an error
analysis to identify the sources of errors. We found
that only 4% of errors are caused by wrong sen-
tence or word alignment. However, 69% of er-
roneous sense-tagged instances are the result of a
Chinese word associated with multiple senses of
a target English word. In such cases, the Chinese
word is linked to multiple sense tags and therefore,
errors in sense-tagged data are introduced. Our re-
sults are similar to those reported in (Chan, 2008).
To speed up the training process, we perform
random sampling on the sense tags with more than
500 samples and limit the number of samples per
sense to 500. However, all samples of senses with
fewer than 500 samples are included in the train-
ing data. This sampling method ensures that rare
sense tags also have training samples during the
selection process.
In order to improve the coverage of the training
set, we augment it by adding samples from SEM-
COR (SC) (Miller et al., 1993) and the DSO cor-
</bodyText>
<page confidence="0.996685">
339
</page>
<table confidence="0.9989795">
Avg. # samples
per word type
MUN (before sampling) 19,837.6
MUN 852.5
MUN+SC 55.4
MUN+SC+DSO 63.7
</table>
<tableCaption confidence="0.97764">
Table 3: Average number of samples per word
type (WordNet 1.7.1)
</tableCaption>
<bodyText confidence="0.997897263157895">
pus (Ng and Lee, 1996). We only add the 28 most
frequent adverbs from SEMCOR because we ob-
serve almost no improvement when adding all ad-
verbs. We notice that the DSO corpus generally
improves the performance of our system. How-
ever, since the annotated DSO corpus is copy-
righted, we are unable to release a dataset in-
cluding the DSO corpus. Therefore, we experi-
ment with two different configurations, one with
the DSO corpus and one without, although the re-
leased dataset will not include the DSO corpus.
Since some shared tasks use newer WordNet
versions, we convert the training set sense labels
using the sense mapping files provided by Word-
Net5. As replicating our results requires WordNet
versions 1.7.1, 2.1, and 3.0, we release our sense-
tagged dataset in all three versions. Some statistics
about the sense-tagged training set can be found in
Table 1 to Table 3.
</bodyText>
<sectionHeader confidence="0.999502" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999717111111111">
For the WSD system, we use IMS (Zhong and
Ng, 2010) in our experiments. IMS is a super-
vised WSD system based on support vector ma-
chines (SVM). This WSD system comes with out-
of-the-box pre-trained models. However, since the
original training set is not released, we use our
own training set (see Section 2) to train IMS and
then evaluate it on standard WSD and WSI bench-
marks. This section presents the results obtained
on four WSD and one WSI shared tasks. The four
all-words WSD shared tasks are SensEval-2 (Ed-
monds and Cotton, 2001), SensEval-3 task 1 (Sny-
der and Palmer, 2004), and both the fine-grained
task 17 and coarse-grained task 7 of SemEval-
2007 (Pradhan et al., 2007; Navigli et al., 2007).
These all-words WSD shared tasks provide no
training data to the participants. The selected
word sense induction task in our experiments is
</bodyText>
<footnote confidence="0.90669">
5http://wordnet.princeton.edu/wordnet/download/current-
version/
</footnote>
<bodyText confidence="0.6553115">
SemEval-2013 task 13 (Jurgens and Klapaftis,
2013).
</bodyText>
<subsectionHeader confidence="0.97495">
3.1 WSD All-Words Tasks
</subsectionHeader>
<bodyText confidence="0.999969333333333">
The results of our experiments on WSD tasks are
presented in Table 4. For the SensEval-2 and
SensEval-3 test sets, we use the training set with
the WordNet 1.7.1 sense inventory and for the
SemEval-2007 test sets, we use training data with
the WordNet 2.1 sense inventory.
In Table 4, IMS (original) refers to the IMS sys-
tem trained with the original training instances as
reported in (Zhong and Ng, 2010). We also com-
pare our systems with two other configurations ob-
tained from training IMS on SEMCOR, and SEM-
COR plus DSO datasets. In Table 4, these two set-
tings are shown by IMS (SC) and IMS (SC+DSO),
respectively. Finally, Rank 1 and Rank 2 are the
top two participating systems in the respective all-
words tasks.
As shown in Table 4, our systems (both with
and without the DSO corpus as training instances)
perform competitively with and in some cases
even better than the original IMS and also the
best shared task submissions. This shows that
our training set is of high quality and training a
supervised WSD system using our training data
achieves state-of-the-art results on the all-words
tasks. Since the MUN dataset does not cover all
target word types in the all-words shared tasks,
the accuracy achieved with MUN alone is lower
than the SC and SC+DSO settings. However, the
evaluation results show that IMS trained on MUN
alone often performs better than or is competi-
tive with the WordNet Sense 1 baseline. Finally,
it can be seen that adding the training instances
from MUN (that is, IMS (MUN+SC) and IMS
(MUN+SC+DSO)) often achieves higher accuracy
than without MUN instances (IMS (SC) and IMS
(SC+DSO)).
</bodyText>
<subsectionHeader confidence="0.9826615">
3.2 SemEval-2013 Word Sense Induction
Task
</subsectionHeader>
<bodyText confidence="0.999883888888889">
In order to evaluate our system on a word sense
induction task, we selected SemEval-2013 task 13,
the latest WSI shared task. Unlike most other tasks
that assume a single sense is sufficient for repre-
senting word senses, this task allows each instance
to be associated with multiple sense labels with
their applicability weights. This WSI task con-
siders 50 lemmas, including 20 nouns, 20 verbs,
and 10 adjectives, annotated with the WordNet 3.1
</bodyText>
<page confidence="0.994293">
340
</page>
<table confidence="0.9973778">
noun verb adjective adverb total
MUN (before sampling) 649 190 319 0 1,158
MUN 649 190 319 0 1,158
MUN+SC 11,446 4,705 5,129 28 21,308
MUN+SC+DSO 11,446 4,705 5,129 28 21,308
</table>
<tableCaption confidence="0.993989">
Table 1: Number of word types in each part-of-speech (WordNet 1.7.1)
</tableCaption>
<table confidence="0.998884166666667">
number of training samples
noun verb adjective adverb total size
MUN (before sampling) 14,492,639 4,400,813 4,078,543 0 22,971,995 17.7 GB
MUN 503,408 265,785 218,046 0 987,239 745 MB
MUN+SC 582,028 341,141 251,362 6,207 1,180,738 872 MB
MUN+SC+DSO 687,871 412,482 251,362 6,207 1,357,922 939 MB
</table>
<tableCaption confidence="0.7503905">
Table 2: Number of training samples in each part-of-speech (WordNet 1.7.1). The size column shows
the total size of each dataset in megabytes or gigabytes.
</tableCaption>
<bodyText confidence="0.998130695652174">
sense inventory. We use WordNet 3.0 in our ex-
periments on this task.
We evaluated our system using all measures
used in the shared task. The results are presented
in Table 5. The columns in this table denote the
scores of the various systems according to the dif-
ferent evaluation metrics used in the WSI shared
task, which are Jaccard Index, Ksim
δ , WNDCG,
Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens
and Klapaftis, 2013) for details of the evaluation
metrics.
This table also includes the top two systems in
the shared task, AI-KU (Baskaya et al., 2013) and
Unimelb (Lau et al., 2013), as well as Wang-15
(Wang et al., 2015). AI-KU uses a language
model to find the most likely substitutes for a tar-
get word to represent the context. The clustering
method used in AI-KU is K-means and the sys-
tem gives good performance in the shared task.
Unimelb relies on Hierarchical Dirichlet Process
(Teh et al., 2006) to identify the sense of a tar-
get word using positional word features. Finally,
Wang-15 uses Latent Dirichlet Allocation (LDA)
(Blei et al., 2003) to model the word sense and
topic jointly. This system obtains high scores, ac-
cording to Fuzzy B-Cubed and Fuzzy NMI mea-
sures. The last three rows are some baseline
systems: grouping all instances into one cluster,
grouping each instance into a cluster of its own,
and assigning the most frequent sense in SEM-
COR to all instances. As shown in Table 5, train-
ing IMS on our training data outperforms all other
systems on three out of five evaluation metrics,
and performs competitively on the remaining two
metrics.
IMS trained on MUN alone (IMS (MUN))
outperforms IMS (SC) and IMS (SC+DSO) in
terms of the first three evaluation measures, and
achieves comparable Fuzzy NMI and Fuzzy B-
Cubed scores. Moreover, the evaluation results
show that IMS (MUN) often performs better than
the SEMCOR most frequent sense baseline. Fi-
nally, it can be observed that in most cases, adding
training instances from MUN significantly im-
proves IMS (SC) and IMS (SC+DSO).
</bodyText>
<sectionHeader confidence="0.99744" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.989068611111111">
One of the major problems in building supervised
word sense disambiguation systems is the train-
ing data acquisition bottleneck. In this paper,
we semi-automatically extracted and sense-tagged
an English corpus containing one million sense-
tagged instances. This large sense-tagged cor-
pus can be used for training any supervised WSD
systems. We then evaluated the performance of
IMS trained on our sense-tagged corpus in several
WSD and WSI shared tasks. Our sense-tagged
dataset has been released publicly6. We believe
our dataset is the largest publicly available anno-
tated dataset for WSD at present.
After training a supervised WSD system using
our training set, we evaluated the system using
standard benchmarks. The evaluation results show
that our sense-tagged corpus can be used to build a
WSD system that performs competitively with the
</bodyText>
<footnote confidence="0.976357">
6http://www.comp.nus.edu.sg/∼nlp/corpora.html
</footnote>
<page confidence="0.989488">
341
</page>
<table confidence="0.999523636363636">
SensEval-2 SensEval-3 SemEval -2007
Fine-grained Fine-grained Fine-grained Coarse-grained
IMS (MUN) 64.5 60.6 52.7 78.7
IMS (MUN+SC) 68.2 67.4 58.5 81.6
IMS (MUN+SC+DSO) 68.0 66.6 58.9 82.3
IMS (original) 68.2 67.6 58.3 82.6
IMS (SC) 66.1 67.0 58.7 81.9
IMS (SC+DSO) 66.5 67.0 57.8 81.6
Rank 1 69.0 65.2 59.1 82.5
Rank 2 63.6 64.6 58.7 81.6
WordNet Sense 1 61.9 62.4 51.4 78.9
</table>
<tableCaption confidence="0.997923">
Table 4: Accuracy (in %) on all-words word sense disambiguation tasks
</tableCaption>
<table confidence="0.999838722222222">
Jac. Ind. Ksim WNDCG Fuzzy NMI Fuzzy B-Cubed
δ
IMS (MUN) 24.6 64.9 33.0 6.9 57.1
IMS (MUN+SC) 25.0 65.4 34.2 9.1 55.9
IMS (MUN+SC+DSO) 25.5 65.4 35.1 9.7 55.4
IMS (original) 23.4 64.5 34.0 8.6 59.0
IMS (SC) 22.9 63.5 32.4 6.8 57.3
IMS (SC+DSO) 23.4 63.6 32.9 7.1 57.6
Wang-15 (ukWac) - - - 9.7 54.5
Wang-15 (actual) - - - 9.4 59.1
AI-KU (base) 19.7 62.0 38.7 6.5 39.0
AI-KU (add1000) 19.7 60.6 21.5 3.5 32.0
AI-KU (remove5-add1000) 24.4 64.2 33.2 3.9 45.1
Unimelb (5p) 21.8 61.4 36.5 5.6 45.9
Unimelb (50k) 21.3 62.0 37.1 6.0 48.3
all-instances-1cluster 19.2 60.9 28.8 0.0 62.3
each-instance-1cluster 0.0 0.0 0.0 7.1 0.0
SEMCOR most freq sense 19.2 60.9 28.8 0.0 62.3
</table>
<tableCaption confidence="0.884483">
Table 5: Supervised and unsupervised evaluation results (in %) on SemEval-2013 word sense induction
task
</tableCaption>
<bodyText confidence="0.987765">
top performing WSD systems in the SensEval-2,
SensEval-3, and SemEval-2007 fine-grained and
coarse-grained all-words tasks, as well as the top
systems in the SemEval-2013 WSI task.
</bodyText>
<sectionHeader confidence="0.98694" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995784125">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
We are also grateful to Christian Hadiwinoto and
Benjamin Yap for assistance with performing the
error analysis, and to the anonymous reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9885646">
Eneko Agirre, Oier L´opez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.
Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz
Yuret. 2013. AI-KU: using substitute vectors and
co-occurrence modeling for word sense induction
and disambiguation. In Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 300–306.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
</reference>
<page confidence="0.991143">
342
</page>
<reference confidence="0.994422757009346">
Proceedings of the 20th National Conference on Ar-
tificial Intelligence, pages 1037–1042.
Yee Seng Chan. 2008. Word Sense Disambiguation:
Scaling up, Domain Adaptation, and Application to
Machine Translation. Ph.D. thesis, National Univer-
sity of Singapore.
Mona Diab. 2004. Relieving the data acquisition bot-
tleneck in word sense disambiguation. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics, pages 303–310.
Philip Edmonds and Scott Cotton. 2001. SENSEVAL-
2: Overview. In Proceedings of the Second Interna-
tional Workshop on Evaluating Word Sense Disam-
biguation Systems, pages 1–5.
Andreas Eisele and Yu Chen. 2010. MultiUN: A
multilingual corpus from United Nation documents.
In Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation, pages
2868–2872.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded
and non-graded senses. In Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 290–299.
Sandra K¨ubler and Desislava Zhekova. 2009. Semi-
supervised learning for word sense disambiguation:
Quality vs. quantity. In Proceedings of the Inter-
national Conference on Recent Advances in Natural
Language Processing, pages 197–202.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013.
unimelb: topic modelling-based word sense induc-
tion. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 307–311.
Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Senseval-3: Third International Work-
shop on the Evaluation of Systems for the Semantic
Analysis of Text, pages 137–140.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161–164.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Workshop on Human Language
Technology, pages 303–308.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: a unified approach. Transactions of the
Association for Computational Linguistics, 2:231–
244.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. In Proceedings
of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 30–35.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41(2):10:1–
10:69.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 40–47.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: An empirical study. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 455–462.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440–447.
Rebecca Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012. The MASC word sense
sentence corpus. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation, pages 3025–3030.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1522–1531.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87–92.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the Third In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, pages 41–43.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Jing Wang, Mohit Bansal, Kevin Gimpel, Brian D.
Ziebart, and Clement T. Yu. 2015. A sense-topic
model for word sense induction with unsupervised
data enrichment. Transactions of the Association for
Computational Linguistics, 3:59–71.
</reference>
<page confidence="0.99097">
343
</page>
<reference confidence="0.9972961">
Zhi Zhong and Hwee Tou Ng. 2009. Word sense dis-
ambiguation for all words without hard labor. In
Proceedings of the Twenty-First International Joint
Conference on Artificial Intelligence, pages 1616–
1621.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics System Demonstrations, pages 78–83.
</reference>
<page confidence="0.998962">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.620534">
<title confidence="0.999296">One Million Sense-Tagged for Word Sense Disambiguation and Induction</title>
<author confidence="0.911826">Kaveh</author>
<affiliation confidence="0.911779333333333">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.943793">Singapore</address>
<email confidence="0.995486">kaveh@comp.nus.edu.sg</email>
<abstract confidence="0.999069047619048">Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source WSD systems, very few large annotated datasets are available to the research community. The two main goals of this paper are to extract and annotate a large number of samples and release them for public use, and also to evaluate this dataset against some word sense disambiguation and induction tasks. We show that the open source IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier L´opez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Eneko Agirre, Oier L´opez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osman Baskaya</author>
<author>Enis Sert</author>
<author>Volkan Cirik</author>
<author>Deniz Yuret</author>
</authors>
<title>AI-KU: using substitute vectors and co-occurrence modeling for word sense induction and disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>300--306</pages>
<contexts>
<context position="13776" citStr="Baskaya et al., 2013" startWordPosition="2261" endWordPosition="2264">mn shows the total size of each dataset in megabytes or gigabytes. sense inventory. We use WordNet 3.0 in our experiments on this task. We evaluated our system using all measures used in the shared task. The results are presented in Table 5. The columns in this table denote the scores of the various systems according to the different evaluation metrics used in the WSI shared task, which are Jaccard Index, Ksim δ , WNDCG, Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens and Klapaftis, 2013) for details of the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model the word sense and topic jointly. This system obtains high scores, according to Fuzzy B-Cu</context>
</contexts>
<marker>Baskaya, Sert, Cirik, Yuret, 2013</marker>
<rawString>Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz Yuret. 2013. AI-KU: using substitute vectors and co-occurrence modeling for word sense induction and disambiguation. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 300–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="14276" citStr="Blei et al., 2003" startWordPosition="2348" endWordPosition="2351"> the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model the word sense and topic jointly. This system obtains high scores, according to Fuzzy B-Cubed and Fuzzy NMI measures. The last three rows are some baseline systems: grouping all instances into one cluster, grouping each instance into a cluster of its own, and assigning the most frequent sense in SEMCOR to all instances. As shown in Table 5, training IMS on our training data outperforms all other systems on three out of five evaluation metrics, and performs competitively on the remaining two metrics. IMS trained on MUN alone (IMS (MUN)) outperforms IMS (SC) and IMS (SC+DSO) in terms o</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Scaling up word sense disambiguation via parallel texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence,</booktitle>
<pages>1037--1042</pages>
<contexts>
<context position="2098" citStr="Chan and Ng (2005)" startWordPosition="321" endWordPosition="324">ther structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training sa</context>
<context position="5250" citStr="Chan and Ng, 2005" startWordPosition="823" endWordPosition="826">l. Since the main purpose of this paper is to build and release a publicly available training set for word sense disambiguation systems, we selected the MultiUN corpus (MUN) (Eisele and Chen, 2010) produced in the EuroMatrixPlus project1. This corpus is freely available via the project website and includes seven languages. An automatically sentence-aligned version of this dataset can be downloaded from the OPUS website2 and therefore we decided to extract samples from this sentence-aligned version. To extract training data from the MultiUN parallel corpus, we follow the approach described in (Chan and Ng, 2005) and select the ChineseEnglish part of the MultiUN corpus. The extraction method has the following steps: 1. Tokenization and word segmentation: The English side of the corpus is tokenized using the Penn TreeBank tokenizer3, while the Chinese side of the corpus is segmented using the Chinese word segmenter of (Low et al., 2005). 2. Word alignment: After tokenizing the texts, GIZA++ (Och and Ney, 2000) is used to align English and Chinese words. 3. Part-of-speech (POS) tagging and lemmatization: After running GIZA++, we use the OpenNLP POS tagger4 and then the WordNet lemmatizer to obtain POS t</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings of the 20th National Conference on Artificial Intelligence, pages 1037–1042.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
</authors>
<title>Word Sense Disambiguation: Scaling up, Domain Adaptation, and Application to Machine Translation.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>National University of Singapore.</institution>
<contexts>
<context position="6677" citStr="Chan, 2008" startWordPosition="1057" endWordPosition="1058">n: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese translation w, of we, based on the automatic word alignment formed by GIZA++. For each sense i of we in the WordNet sense inventory (WordNet 1.7.1), a list of Chinese translations of sense i of we has been manually created. If w, matches one of these Chinese translations of sense i, then we is tagged with sense i. The average time needed to manually assign Chinese translations to the word senses of one word type for noun, adjective, and verb is 20, 25, and 40 minutes respectively (Chan, 2008). The above procedure annotates the top 60% most frequent word types (nouns, verbs, and adjectives) in English, selected based on their frequency in the Brown corpus. This set of selected word types includes 649 nouns, 190 verbs, and 319 adjectives. Since automatic sentence and word alignment can be noisy, and a Chinese word w, can occasionally be a valid translation of more than one sense of an English word we, the senses tagged using the above procedure may be erroneous. To get an idea of the accuracy of the senses tagged with this procedure, we manually evaluated a subset of 1,000 randomly </context>
</contexts>
<marker>Chan, 2008</marker>
<rawString>Yee Seng Chan. 2008. Word Sense Disambiguation: Scaling up, Domain Adaptation, and Application to Machine Translation. Ph.D. thesis, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>Relieving the data acquisition bottleneck in word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>303--310</pages>
<contexts>
<context position="2234" citStr="Diab (2004)" startWordPosition="344" endWordPosition="345">ets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNet sense inventory (Miller, 1995) and release the annotated corpus for public use. To our knowledge, this annota</context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Mona Diab. 2004. Relieving the data acquisition bottleneck in word sense disambiguation. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 303–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Scott Cotton</author>
</authors>
<title>SENSEVAL2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="9967" citStr="Edmonds and Cotton, 2001" startWordPosition="1627" endWordPosition="1631">stics about the sense-tagged training set can be found in Table 1 to Table 3. 3 Evaluation For the WSD system, we use IMS (Zhong and Ng, 2010) in our experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 5http://wordnet.princeton.edu/wordnet/download/currentversion/ SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 WSD All-Words Tasks The results of our experiments on WSD tasks are presented in Table 4. For the SensEval-2 and SensEval-3 test sets, we use the training set with the WordN</context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>Philip Edmonds and Scott Cotton. 2001. SENSEVAL2: Overview. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>MultiUN: A multilingual corpus from United Nation documents.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation,</booktitle>
<pages>2868--2872</pages>
<contexts>
<context position="4829" citStr="Eisele and Chen, 2010" startWordPosition="759" endWordPosition="762">ted data in Section 3 and finally, we conclude the paper in Section 4. 2 Training Data In order to train a supervised word sense disambiguation system, we extract and sense-tag data from a freely available parallel corpus, in a semiautomatic manner. To increase the coverage and therefore the ultimate performance of our WSD system, we also make use of existing sense-tagged datasets. This section explains each step in detail. Since the main purpose of this paper is to build and release a publicly available training set for word sense disambiguation systems, we selected the MultiUN corpus (MUN) (Eisele and Chen, 2010) produced in the EuroMatrixPlus project1. This corpus is freely available via the project website and includes seven languages. An automatically sentence-aligned version of this dataset can be downloaded from the OPUS website2 and therefore we decided to extract samples from this sentence-aligned version. To extract training data from the MultiUN parallel corpus, we follow the approach described in (Chan and Ng, 2005) and select the ChineseEnglish part of the MultiUN corpus. The extraction method has the following steps: 1. Tokenization and word segmentation: The English side of the corpus is </context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. MultiUN: A multilingual corpus from United Nation documents. In Proceedings of the Seventh International Conference on Language Resources and Evaluation, pages 2868–2872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Ioannis Klapaftis</author>
</authors>
<title>Semeval2013 task 13: Word sense induction for graded and non-graded senses.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>290--299</pages>
<contexts>
<context position="3480" citStr="Jurgens and Klapaftis, 2013" startWordPosition="546" endWordPosition="549">gged samples is the largest publicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WSI. Some researchers believe that, in some cases, WSI methods may perform better than WSD systems (Jurgens and Klapaftis, 2013; Wang et al., 2015). However, we argue that WSI systems have few advantages compared to WSD methods and according to our results, disambiguation systems consistently outperform induction systems. Although there are some cases where WSI systems can be useful (e.g., for resource-poor languages), in most cases WSD systems are preferable because 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of higher accuracy and better interpretability of output. The rest of this paper i</context>
<context position="10388" citStr="Jurgens and Klapaftis, 2013" startWordPosition="1688" endWordPosition="1691"> evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 5http://wordnet.princeton.edu/wordnet/download/currentversion/ SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 WSD All-Words Tasks The results of our experiments on WSD tasks are presented in Table 4. For the SensEval-2 and SensEval-3 test sets, we use the training set with the WordNet 1.7.1 sense inventory and for the SemEval-2007 test sets, we use training data with the WordNet 2.1 sense inventory. In Table 4, IMS (original) refers to the IMS system trained with the original training instances as reported in (Zhong and Ng, 2010). We also compare our systems with two other configurations obtained from training IMS on SEMCOR, and SEMCOR plus DSO datasets. In Table 4, these two settings are shown </context>
<context position="13643" citStr="Jurgens and Klapaftis, 2013" startWordPosition="2238" endWordPosition="2241">DSO 687,871 412,482 251,362 6,207 1,357,922 939 MB Table 2: Number of training samples in each part-of-speech (WordNet 1.7.1). The size column shows the total size of each dataset in megabytes or gigabytes. sense inventory. We use WordNet 3.0 in our experiments on this task. We evaluated our system using all measures used in the shared task. The results are presented in Table 5. The columns in this table denote the scores of the various systems according to the different evaluation metrics used in the WSI shared task, which are Jaccard Index, Ksim δ , WNDCG, Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens and Klapaftis, 2013) for details of the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet All</context>
</contexts>
<marker>Jurgens, Klapaftis, 2013</marker>
<rawString>David Jurgens and Ioannis Klapaftis. 2013. Semeval2013 task 13: Word sense induction for graded and non-graded senses. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 290–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Desislava Zhekova</author>
</authors>
<title>Semisupervised learning for word sense disambiguation: Quality vs. quantity.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>197--202</pages>
<marker>K¨ubler, Zhekova, 2009</marker>
<rawString>Sandra K¨ubler and Desislava Zhekova. 2009. Semisupervised learning for word sense disambiguation: Quality vs. quantity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 197–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>unimelb: topic modelling-based word sense induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>307--311</pages>
<contexts>
<context position="13807" citStr="Lau et al., 2013" startWordPosition="2267" endWordPosition="2270">aset in megabytes or gigabytes. sense inventory. We use WordNet 3.0 in our experiments on this task. We evaluated our system using all measures used in the shared task. The results are presented in Table 5. The columns in this table denote the scores of the various systems according to the different evaluation metrics used in the WSI shared task, which are Jaccard Index, Ksim δ , WNDCG, Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens and Klapaftis, 2013) for details of the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model the word sense and topic jointly. This system obtains high scores, according to Fuzzy B-Cubed and Fuzzy NMI measures. The</context>
</contexts>
<marker>Lau, Cook, Baldwin, 2013</marker>
<rawString>Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013. unimelb: topic modelling-based word sense induction. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 307–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
<author>Tee Kiah Chia</author>
</authors>
<title>Supervised word sense disambiguation with support vector machines and multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>137--140</pages>
<contexts>
<context position="1424" citStr="Lee et al., 2004" startWordPosition="215" endWordPosition="218">urce IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for anno</context>
</contexts>
<marker>Lee, Ng, Chia, 2004</marker>
<rawString>Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia. 2004. Supervised word sense disambiguation with support vector machines and multiple knowledge sources. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 137–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="5579" citStr="Low et al., 2005" startWordPosition="880" endWordPosition="883">tomatically sentence-aligned version of this dataset can be downloaded from the OPUS website2 and therefore we decided to extract samples from this sentence-aligned version. To extract training data from the MultiUN parallel corpus, we follow the approach described in (Chan and Ng, 2005) and select the ChineseEnglish part of the MultiUN corpus. The extraction method has the following steps: 1. Tokenization and word segmentation: The English side of the corpus is tokenized using the Penn TreeBank tokenizer3, while the Chinese side of the corpus is segmented using the Chinese word segmenter of (Low et al., 2005). 2. Word alignment: After tokenizing the texts, GIZA++ (Och and Ney, 2000) is used to align English and Chinese words. 3. Part-of-speech (POS) tagging and lemmatization: After running GIZA++, we use the OpenNLP POS tagger4 and then the WordNet lemmatizer to obtain POS tags and word lemmas of the English sentence. 1http://www.euromatrixplus.eu/multi-un 2http://opus.lingfil.uu.se/MultiUN.php 3http://www.cis.upenn.edu/∼treebank/tokenization.html 4http://opennlp.apache.org 4. Annotation: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="1655" citStr="Miller et al., 1993" startWordPosition="252" endWordPosition="255">ntifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsuperv</context>
<context position="8368" citStr="Miller et al., 1993" startWordPosition="1348" endWordPosition="1351"> multiple sense tags and therefore, errors in sense-tagged data are introduced. Our results are similar to those reported in (Chan, 2008). To speed up the training process, we perform random sampling on the sense tags with more than 500 samples and limit the number of samples per sense to 500. However, all samples of senses with fewer than 500 samples are included in the training data. This sampling method ensures that rare sense tags also have training samples during the selection process. In order to improve the coverage of the training set, we augment it by adding samples from SEMCOR (SC) (Miller et al., 1993) and the DSO cor339 Avg. # samples per word type MUN (before sampling) 19,837.6 MUN 852.5 MUN+SC 55.4 MUN+SC+DSO 63.7 Table 3: Average number of samples per word type (WordNet 1.7.1) pus (Ng and Lee, 1996). We only add the 28 most frequent adverbs from SEMCOR because we observe almost no improvement when adding all adverbs. We notice that the DSO corpus generally improves the performance of our system. However, since the annotated DSO corpus is copyrighted, we are unable to release a dataset including the DSO corpus. Therefore, we experiment with two different configurations, one with the DSO </context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proceedings of the Workshop on Human Language Technology, pages 303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2755" citStr="Miller, 1995" startWordPosition="424" endWordPosition="425">sh parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNet sense inventory (Miller, 1995) and release the annotated corpus for public use. To our knowledge, this annotated set of sense-tagged samples is the largest publicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WS</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity linking meets word sense disambiguation: a unified approach.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>244</pages>
<contexts>
<context position="1582" citStr="Moro et al., 2014" startWordPosition="241" endWordPosition="244">orming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples f</context>
</contexts>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity linking meets word sense disambiguation: a unified approach. Transactions of the Association for Computational Linguistics, 2:231– 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 07: Coarsegrained English all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>30--35</pages>
<contexts>
<context position="10133" citStr="Navigli et al., 2007" startWordPosition="1657" endWordPosition="1660">a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 5http://wordnet.princeton.edu/wordnet/download/currentversion/ SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 WSD All-Words Tasks The results of our experiments on WSD tasks are presented in Table 4. For the SensEval-2 and SensEval-3 test sets, we use the training set with the WordNet 1.7.1 sense inventory and for the SemEval-2007 test sets, we use training data with the WordNet 2.1 sense inventory. In Table 4, IMS (original) refers to the IMS s</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. Semeval-2007 task 07: Coarsegrained English all-words task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 30–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<pages>10--69</pages>
<contexts>
<context position="1309" citStr="Navigli, 2009" startWordPosition="200" endWordPosition="201">lso to evaluate this dataset against some word sense disambiguation and induction tasks. We show that the open source IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2):10:1– 10:69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1673" citStr="Ng and Lee, 1996" startWordPosition="256" endWordPosition="259">of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping</context>
<context position="8573" citStr="Ng and Lee, 1996" startWordPosition="1385" endWordPosition="1388">ense tags with more than 500 samples and limit the number of samples per sense to 500. However, all samples of senses with fewer than 500 samples are included in the training data. This sampling method ensures that rare sense tags also have training samples during the selection process. In order to improve the coverage of the training set, we augment it by adding samples from SEMCOR (SC) (Miller et al., 1993) and the DSO cor339 Avg. # samples per word type MUN (before sampling) 19,837.6 MUN 852.5 MUN+SC 55.4 MUN+SC+DSO 63.7 Table 3: Average number of samples per word type (WordNet 1.7.1) pus (Ng and Lee, 1996). We only add the 28 most frequent adverbs from SEMCOR because we observe almost no improvement when adding all adverbs. We notice that the DSO corpus generally improves the performance of our system. However, since the annotated DSO corpus is copyrighted, we are unable to release a dataset including the DSO corpus. Therefore, we experiment with two different configurations, one with the DSO corpus and one without, although the released dataset will not include the DSO corpus. Since some shared tasks use newer WordNet versions, we convert the training set sense labels using the sense mapping f</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="2078" citStr="Ng et al. (2003)" startWordPosition="317" endWordPosition="320">e ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="5654" citStr="Och and Ney, 2000" startWordPosition="892" endWordPosition="895">m the OPUS website2 and therefore we decided to extract samples from this sentence-aligned version. To extract training data from the MultiUN parallel corpus, we follow the approach described in (Chan and Ng, 2005) and select the ChineseEnglish part of the MultiUN corpus. The extraction method has the following steps: 1. Tokenization and word segmentation: The English side of the corpus is tokenized using the Penn TreeBank tokenizer3, while the Chinese side of the corpus is segmented using the Chinese word segmenter of (Low et al., 2005). 2. Word alignment: After tokenizing the texts, GIZA++ (Och and Ney, 2000) is used to align English and Chinese words. 3. Part-of-speech (POS) tagging and lemmatization: After running GIZA++, we use the OpenNLP POS tagger4 and then the WordNet lemmatizer to obtain POS tags and word lemmas of the English sentence. 1http://www.euromatrixplus.eu/multi-un 2http://opus.lingfil.uu.se/MultiUN.php 3http://www.cis.upenn.edu/∼treebank/tokenization.html 4http://opennlp.apache.org 4. Annotation: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese translation w, of we, based on the automatic word alignment formed by GIZA</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Collin Baker</author>
<author>Christiane Fellbaum</author>
<author>Nancy Ide</author>
</authors>
<title>The MASC word sense sentence corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation,</booktitle>
<pages>3025--3030</pages>
<contexts>
<context position="1699" citStr="Passonneau et al., 2012" startWordPosition="260" endWordPosition="264">cally has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically g</context>
</contexts>
<marker>Passonneau, Baker, Fellbaum, Ide, 2012</marker>
<rawString>Rebecca Passonneau, Collin Baker, Christiane Fellbaum, and Nancy Ide. 2012. The MASC word sense sentence corpus. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 3025–3030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="1541" citStr="Ponzetto and Navigli, 2010" startWordPosition="233" endWordPosition="236">s and a recent word sense induction task, outperforming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-Engl</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task-17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<contexts>
<context position="10110" citStr="Pradhan et al., 2007" startWordPosition="1653" endWordPosition="1656">r experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 5http://wordnet.princeton.edu/wordnet/download/currentversion/ SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 WSD All-Words Tasks The results of our experiments on WSD tasks are presented in Table 4. For the SensEval-2 and SensEval-3 test sets, we use the training set with the WordNet 1.7.1 sense inventory and for the SemEval-2007 test sets, we use training data with the WordNet 2.1 sense inventory. In Table 4, IMS (origin</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task-17: English lexical sample, SRL and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<contexts>
<context position="10012" citStr="Snyder and Palmer, 2004" startWordPosition="1635" endWordPosition="1639">be found in Table 1 to Table 3. 3 Evaluation For the WSD system, we use IMS (Zhong and Ng, 2010) in our experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 5http://wordnet.princeton.edu/wordnet/download/currentversion/ SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 WSD All-Words Tasks The results of our experiments on WSD tasks are presented in Table 4. For the SensEval-2 and SensEval-3 test sets, we use the training set with the WordNet 1.7.1 sense inventory and for the SemEval-</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English all-words task. In Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="14129" citStr="Teh et al., 2006" startWordPosition="2324" endWordPosition="2327">ed in the WSI shared task, which are Jaccard Index, Ksim δ , WNDCG, Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens and Klapaftis, 2013) for details of the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model the word sense and topic jointly. This system obtains high scores, according to Fuzzy B-Cubed and Fuzzy NMI measures. The last three rows are some baseline systems: grouping all instances into one cluster, grouping each instance into a cluster of its own, and assigning the most frequent sense in SEMCOR to all instances. As shown in Table 5, training IMS on our training data outperforms all other systems on three out of five evaluation metr</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Wang</author>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Brian D Ziebart</author>
<author>Clement T Yu</author>
</authors>
<title>A sense-topic model for word sense induction with unsupervised data enrichment.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--59</pages>
<contexts>
<context position="3500" citStr="Wang et al., 2015" startWordPosition="550" endWordPosition="553">ublicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WSI. Some researchers believe that, in some cases, WSI methods may perform better than WSD systems (Jurgens and Klapaftis, 2013; Wang et al., 2015). However, we argue that WSI systems have few advantages compared to WSD methods and according to our results, disambiguation systems consistently outperform induction systems. Although there are some cases where WSI systems can be useful (e.g., for resource-poor languages), in most cases WSD systems are preferable because 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of higher accuracy and better interpretability of output. The rest of this paper is composed of the fo</context>
<context position="13847" citStr="Wang et al., 2015" startWordPosition="2275" endWordPosition="2278">nventory. We use WordNet 3.0 in our experiments on this task. We evaluated our system using all measures used in the shared task. The results are presented in Table 5. The columns in this table denote the scores of the various systems according to the different evaluation metrics used in the WSI shared task, which are Jaccard Index, Ksim δ , WNDCG, Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens and Klapaftis, 2013) for details of the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model the word sense and topic jointly. This system obtains high scores, according to Fuzzy B-Cubed and Fuzzy NMI measures. The last three rows are some baseline syste</context>
</contexts>
<marker>Wang, Bansal, Gimpel, Ziebart, Yu, 2015</marker>
<rawString>Jing Wang, Mohit Bansal, Kevin Gimpel, Brian D. Ziebart, and Clement T. Yu. 2015. A sense-topic model for word sense induction with unsupervised data enrichment. Transactions of the Association for Computational Linguistics, 3:59–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Word sense disambiguation for all words without hard labor.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1616--1621</pages>
<contexts>
<context position="2123" citStr="Zhong and Ng (2009)" startWordPosition="326" endWordPosition="329">e sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNe</context>
</contexts>
<marker>Zhong, Ng, 2009</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2009. Word sense disambiguation for all words without hard labor. In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence, pages 1616– 1621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It Makes Sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics System Demonstrations,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="1445" citStr="Zhong and Ng, 2010" startWordPosition="219" endWordPosition="222">m trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg To overcome these limitations, automatic methods have been developed for annotating training sampl</context>
<context position="3018" citStr="Zhong and Ng, 2010" startWordPosition="466" endWordPosition="469">rvised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNet sense inventory (Miller, 1995) and release the annotated corpus for public use. To our knowledge, this annotated set of sense-tagged samples is the largest publicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WSI. Some researchers believe that, in some cases, WSI methods may perform better than WSD systems (Jurgens and Klapaftis, 2013; Wang et al., 2015). However, we argue that WSI systems have few advantages compared to WSD methods and according to our results, disambi</context>
<context position="9484" citStr="Zhong and Ng, 2010" startWordPosition="1543" endWordPosition="1546">including the DSO corpus. Therefore, we experiment with two different configurations, one with the DSO corpus and one without, although the released dataset will not include the DSO corpus. Since some shared tasks use newer WordNet versions, we convert the training set sense labels using the sense mapping files provided by WordNet5. As replicating our results requires WordNet versions 1.7.1, 2.1, and 3.0, we release our sensetagged dataset in all three versions. Some statistics about the sense-tagged training set can be found in Table 1 to Table 3. 3 Evaluation For the WSD system, we use IMS (Zhong and Ng, 2010) in our experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval</context>
<context position="10819" citStr="Zhong and Ng, 2010" startWordPosition="1763" endWordPosition="1766">participants. The selected word sense induction task in our experiments is 5http://wordnet.princeton.edu/wordnet/download/currentversion/ SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 WSD All-Words Tasks The results of our experiments on WSD tasks are presented in Table 4. For the SensEval-2 and SensEval-3 test sets, we use the training set with the WordNet 1.7.1 sense inventory and for the SemEval-2007 test sets, we use training data with the WordNet 2.1 sense inventory. In Table 4, IMS (original) refers to the IMS system trained with the original training instances as reported in (Zhong and Ng, 2010). We also compare our systems with two other configurations obtained from training IMS on SEMCOR, and SEMCOR plus DSO datasets. In Table 4, these two settings are shown by IMS (SC) and IMS (SC+DSO), respectively. Finally, Rank 1 and Rank 2 are the top two participating systems in the respective allwords tasks. As shown in Table 4, our systems (both with and without the DSO corpus as training instances) perform competitively with and in some cases even better than the original IMS and also the best shared task submissions. This shows that our training set is of high quality and training a super</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics System Demonstrations, pages 78–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>