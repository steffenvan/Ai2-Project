<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004281">
<title confidence="0.652575">
Movie Reviews and Revenues: An Experiment in Text Regression*
</title>
<author confidence="0.95563">
Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A. Smith
</author>
<affiliation confidence="0.890875333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997999">
{maheshj,dipanjan,kgimpel,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973166666667">
We consider the problem of predicting a
movie’s opening weekend revenue. Previous
work on this problem has used metadata about
a movie—e.g., its genre, MPAA rating, and
cast—with very limited work making use of
text about the movie. In this paper, we use
the text of film critics’ reviews from several
sources to predict opening weekend revenue.
We describe a new dataset pairing movie re-
views with metadata and revenue data, and
show that review text can substitute for meta-
data, and even improve over it, for prediction.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999783230769231">
Predicting gross revenue for movies is a problem
that has been studied in economics, marketing,
statistics, and forecasting. Apart from the economic
value of such predictions, we view the forecasting
problem as an application of NLP. In this paper, we
use the text of critics’ reviews to predict opening
weekend revenue. We also consider metadata for
each movie that has been shown to be successful for
similar prediction tasks in previous work.
There is a large body of prior work aimed at pre-
dicting gross revenue of movies (Simonoff and Spar-
row, 2000; Sharda and Delen, 2006; inter alia). Cer-
tain information is used in nearly all prior work on
these tasks, such as the movie’s genre, MPAA rating,
running time, release date, the number of screens on
which the movie debuted, and the presence of partic-
ular actors or actresses in the cast. Most prior text-
based work has used automatic text analysis tools,
deriving a small number of aggregate statistics. For
example, Mishne and Glance (2006) applied sen-
timent analysis techniques to pre-release and post-
release blog posts about movies and showed higher
*We appreciate reviewer feedback and technical advice
from Brendan O’Connor. This work was supported by NSF IIS-
0803482, NSF IIS-0844507, and DARPA NBCH-1080004.
correlation between actual revenue and sentiment-
based metrics, as compared to mention counts of the
movie. (They did not frame the task as a revenue
prediction problem.) Zhang and Skiena (2009) used
a news aggregation system to identify entities and
obtain domain-specific sentiment for each entity in
several domains. They used the aggregate sentiment
scores and mention counts of each movie in news
articles as predictors.
While there has been substantial prior work on
using critics’ reviews, to our knowledge all of this
work has used polarity of the review or the number
of stars given to it by a critic, rather than the review
text directly (Terry et al., 2005).
Our task is related to sentiment analysis (Pang et
al., 2002) on movie reviews. The key difference is
that our goal is to predict a future real-valued quan-
tity, restricting us from using any post-release text
data such as user reviews. Further, the most im-
portant clues about revenue may have little to do
with whether the reviewer liked the movie, but rather
what the reviewer found worth mentioning. This pa-
per is more in the tradition of Ghose et al. (2007) and
Kogan et al. (2009), who used text regression to di-
rectly quantify review “value” and make predictions
about future financial variables, respectively.
Our aim in using the full text is to identify partic-
ular words and phrases that predict the movie-going
tendencies of the public. We can also perform syn-
tactic and semantic analysis on the text to identify
richer constructions that are good predictors. Fur-
thermore, since we consider multiple reviews for
each movie, we can compare these features across
reviews to observe how they differ both in frequency
and predictive performance across different media
outlets and individual critics.
In this paper, we use linear regression from text
and non-text (meta) features to directly predict gross
revenue aggregated over the opening weekend, and
the same averaged per screen.
</bodyText>
<page confidence="0.968083">
293
</page>
<subsubsectionHeader confidence="0.457829">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 293–296,
</subsubsectionHeader>
<page confidence="0.209894">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<table confidence="0.999204666666667">
Domain train dev test total
Austin Chronicle 306 94 62 462
Boston Globe 461 154 116 731
LA Times 610 2 13 625
Entertainment Weekly 644 208 187 1039
New York Times 878 273 224 1375
Variety 927 297 230 1454
Village Voice 953 245 198 1396
# movies 1147 317 254 1718
</table>
<tableCaption confidence="0.910659">
Table 1: Total number of reviews from each domain for
the training, development and test sets.
</tableCaption>
<sectionHeader confidence="0.9832" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999995275862069">
We gathered data for movies released in 2005–2009.
For these movies, we obtained metadata and a list
of hyperlinks to movie reviews by crawling Meta-
Critic (www.metacritic.com). The metadata
include the name of the movie, its production house,
the set of genres it belongs to, the scriptwriter(s),
the director(s), the country of origin, the primary
actors and actresses starring in the movie, the re-
lease date, its MPAA rating, and its running time.
From The Numbers (www.the-numbers.com),
we retrieved each movie’s production budget, open-
ing weekend gross revenue, and the number of
screens on which it played during its opening week-
end. Only movies found on both MetaCritic and The
Numbers were included.
Next we chose seven review websites that most
frequently appeared in the review lists for movies at
Metacritic, and obtained the text of the reviews by
scraping the raw HTML. The sites chosen were the
Austin Chronicle, the Boston Globe, the LA Times,
Entertainment Weekly, the New York Times, Vari-
ety, and the Village Voice. We only chose those
reviews that appeared on or before the release date
of the movie (to ensure that revenue information is
not present in the review), arriving at a set of 1718
movies with at least one review. We partitioned this
set of movies temporally into training (2005–2007),
development (2008) and test (2009) sets. Not all
movies had reviews at all sites (see Table 1).
</bodyText>
<sectionHeader confidence="0.992952" genericHeader="method">
3 Predictive Task
</sectionHeader>
<bodyText confidence="0.999966176470588">
We consider two response variables, both in
U.S. dollars: the total revenue generated by a movie
during its release weekend, and the per screen rev-
enue during the release weekend. We evaluate these
predictions using (1) mean absolute error (MAE) in
U.S. dollars and (2) Pearson’s correlation between
the actual and predicted revenue.
We use linear regression to directly predict the
opening weekend gross earnings, denoted y, based
on features x extracted from the movie metadata
and/or the text of the reviews. That is, given an input
feature vector x E Rp, we predict an output y� E R
using a linear model: y� = 00 + xT,3. To learn val-
ues for the parameters 0 = (00,,O), the standard
approach is to minimize the sum of squared errors
for a training set containing n pairs (xi, yi) where
xi E Rp and yi E R for 1 G i G n:
</bodyText>
<equation confidence="0.984313">
( )2
yi − (00 + x� i �) +AP(�)
</equation>
<bodyText confidence="0.9999126">
A penalty term P(,3) is included in the objective for
regularization. Classical solutions use an E2 or E1
norm, known respectively as ridge and lasso regres-
sion. Introduced recently is a mixture of the two,
called the elastic net (Zou and Hastie, 2005):
</bodyText>
<equation confidence="0.983389">
P(3) = E� =1(2(1 − α)02 + α|0j|)
</equation>
<bodyText confidence="0.9999753">
where α E (0, 1) determines the trade-off be-
tween E1 and E2 regularization. For our experi-
ments we used the elastic net and specifically the
glmnet package which contains an implementa-
tion of an efficient coordinate ascent procedure for
training (Friedman et al., 2008).
We tune the α and A parameters on our develop-
ment set and select the model with the (α, A) com-
bination that yields minimum MAE on the develop-
ment set.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999805">
We compare predictors based on metadata, predic-
tors based on text, and predictors that use both kinds
of information. Results for two simple baselines of
predicting the training set mean and median are re-
ported in Table 2 (Pearson’s correlation is undefined
since the standard deviation is zero).
</bodyText>
<subsectionHeader confidence="0.996343">
4.1 Metadata Features
</subsectionHeader>
<bodyText confidence="0.99979">
We considered seven types of metadata features, and
evaluated their performance by adding them to our
pool of features in the following order: whether the
</bodyText>
<equation confidence="0.97716875">
0 = argmin
e=(β0,0)
1 n
2n i=1
</equation>
<page confidence="0.988352">
294
</page>
<bodyText confidence="0.9999521875">
film is of U.S. origin, running time (in minutes), the
logarithm of its budget, # opening screens, genre
(e.g., Action, Comedy) and MPAA rating (e.g., G,
PG, PG-13), whether the movie opened on a holiday
weekend or in summer months, total count as well as
of presence of individual Oscar-winning actors and
directors and high-grossing actors. For the first task
of predicting the total opening weekend revenue of
a movie, the best-performing feature set in terms of
MAE turned out to be all the features. However, for
the second task of predicting the per screen revenue,
addition of the last feature subset consisting of infor-
mation related to the actors and directors hurt perfor-
mance (MAE increased). Therefore, for the second
task, the best performing set contained only the first
six types of metadata features.
</bodyText>
<subsectionHeader confidence="0.989107">
4.2 Text Features
</subsectionHeader>
<bodyText confidence="0.996653">
We extract three types of text features (described be-
low). We only included feature instances that oc-
curred in at least five different movies’ reviews. We
stem and downcase individual word components in
all our features.
</bodyText>
<listItem confidence="0.7631965">
I. n-grams. We considered unigrams, bigrams, and
trigrams. A 25-word stoplist was used; bigrams
and trigrams were only filtered if all words were
stopwords.
II. Part-of-speech n-grams. As with words, we
added unigrams, bigrams, and trigrams. Tags
were obtained from the Stanford part-of-speech
tagger (Toutanova and Manning, 2000).
III. Dependency relations. We used the Stanford
parser (Klein and Manning, 2003) to parse the
</listItem>
<bodyText confidence="0.983178333333333">
critic reviews and extract syntactic dependen-
cies. The dependency relation features consist
of just the relation part of a dependency triple
(relation, head word, modifier word).
We consider three ways to combine the collec-
tion of reviews for a given movie. The first (“−”)
simply concatenates all of a movie’s reviews into
a single document before extracting features. The
second (“+”) conjoins each feature with the source
site (e.g., New York Times) from whose review it was
extracted. A third version (denoted “B”) combines
both the site-agnostic and site-specific features.
</bodyText>
<table confidence="0.999821208333334">
Features Site Total Per Screen
MAE r MAE r
($M) ($K)
Predict mean 11.672 – 6.862 –
Predict median 10.521 – 6.642 –
meta Best 5.983 0.722 6.540 0.272
text − 8.013 0.743 6.509 0.222
I + 7.722 0.781 6.071 0.466
see Tab. 3 B 7.627 0.793 6.060 0.411
− 8.060 0.743 6.542 0.233
I U II + 7.420 0.761 6.240 0.398
B 7.447 0.778 6.299 0.363
− 8.005 0.744 6.505 0.223
I U III + 7.721 0.785 6.013 0.473
B 7.595 0.796 †6.010 0.421
meta U text − 5.921 0.819 6.509 0.222
I + 5.757 0.810 6.063 0.470
B 5.750 0.819 6.052 0.414
− 5.952 0.818 6.542 0.233
I U II + 5.752 0.800 6.230 0.400
B 5.740 0.819 6.276 0.358
− 5.921 0.819 6.505 0.223
I U III + 5.738 0.812 6.003 0.477
B 5.750 0.819 †5.998 0.423
</table>
<tableCaption confidence="0.717082666666667">
Table 2: Test-set performance for various models, mea-
sured using mean absolute error (MAE) and Pearson’s
correlation (r), for two prediction tasks. Within a column,
boldface shows the best result among “text” and “meta U
text” settings. †Significantly better than the meta baseline
with p &lt; 0.01, using the Wilcoxon signed rank test.
</tableCaption>
<subsectionHeader confidence="0.963571">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9979845">
Table 2 shows our results for both prediction tasks.
For the total first-weekend revenue prediction task,
metadata features baseline result (r2 = 0.521) is
comparable to that reported by Simonoff and Spar-
row (2000) on a similar task of movie gross predic-
tion (r2 = 0.446). Features from critics’ reviews
by themselves improve correlation on both predic-
tion tasks, however improvement in MAE is only
observed for the per screen revenue prediction task.
A combination of the meta and text features
achieves the best performance both in terms of MAE
and r. While the text-only models have some high
negative weight features, the combined models do
not have any negatively weighted features and only
a very few metadata features. That is, the text is able
to substitute for the other metadata features.
Among the different types of text-based features
that we tried, lexical n-grams proved to be a strong
baseline to beat. None of the “I U *” feature sets are
significantly better than n-grams alone, but adding
</bodyText>
<page confidence="0.995539">
295
</page>
<bodyText confidence="0.999957692307692">
the dependency relation features (set III) to the n-
grams does improve the performance enough to
make it significantly better than the metadata-only
baseline for per screen revenue prediction.
Salient Text Features: Table 3 lists some of the
highly weighted features, which we have catego-
rized manually. The features are from the text-only
model annotated in Table 2 (total, not per screen).
The feature weights can be directly interpreted as
U.S. dollars contributed to the predicted value y by
each occurrence of the feature. Sentiment-related
features are not as prominent as might be expected,
and their overall proportion in the set of features
with non-zero weights is quite small (estimated in
preliminary trials at less than 15%). Phrases that
refer to metadata are the more highly weighted
and frequent ones. Consistent with previous re-
search, we found some positively-oriented sentiment
features to be predictive. Some other prominent
features not listed in the table correspond to spe-
cial effects (“Boston Globe: of the art”, “and cgi”),
particular movie franchises (“shrek movies”, “Vari-
ety: chronicle of”, “voldemort”), hype/expectations
(“blockbuster”, “anticipation”), film festival (“Vari-
ety: canne” with negative weight) and time of re-
lease (“summer movie”).
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999948">
We conclude that text features from pre-release re-
views can substitute for and improve over a strong
metadata-based first-weekend movie revenue pre-
diction. The dataset used in this paper has been
made available for research at http://www.
ark.cs.cmu.edu/movie$-data.
</bodyText>
<sectionHeader confidence="0.998955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997579083333333">
J. Friedman, T. Hastie, and R. Tibshirani. 2008. Regular-
ized paths for generalized linear models via coordinate
descent. Technical report, Stanford University.
A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007.
Opinion mining using econometrics: A case study on
reputation systems. In Proc. of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of NAACL, pages 272–280.
</reference>
<table confidence="0.999311130434783">
Feature Weight ($M)
pg +0.085
New York Times: adult -0.236
New York Times: rate r -0.364
this series +13.925
LA Times: the franchise +5.112
Variety: the sequel +4.224
Boston Globe: will smith +2.560
Variety: brittany +1.128
&amp;quot; producer brian +0.486
Variety: testosterone +1.945
Ent. Weekly: comedy for +1.143
Variety: a horror +0.595
documentary -0.037
independent -0.127
Boston Globe: best parts of +1.462
Boston Globe: smart enough +1.449
LA Times: a good thing +1.117
shame $ -0.098
bogeyman -0.689
Variety: torso +9.054
vehicle in +5.827
superhero $ +2.020
</table>
<tableCaption confidence="0.997327">
Table 3: Highly weighted features categorized manu-
</tableCaption>
<reference confidence="0.977596103448276">
ally. &amp;quot; and $ denote sentence boundaries. “brittany”
frequently refers to Brittany Snow and Brittany Murphy.
“&amp;quot; producer brian” refers to producer Brian Grazer (The
Da Vinci Code, among others).
G. Mishne and N. Glance. 2006. Predicting movie sales
from blogger sentiment. In AAAI Spring Symposium
on Computational Approaches to Analysing Weblogs.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79–86.
R. Sharda and D. Delen. 2006. Predicting box office suc-
cess of motion pictures with neural networks. Expert
Systems with Applications, 30(2):243–254.
J. S. Simonoff and I. R. Sparrow. 2000. Predicting movie
grosses: Winners and losers, blockbusters and sleep-
ers. Chance, 13(3):15–24.
N. Terry, M. Butler, and D. De’Armond. 2005. The de-
terminants of domestic box office performance in the
motion picture industry. Southwestern Economic Re-
view, 32:137–148.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63–70.
W. Zhang and S. Skiena. 2009. Improving movie gross
prediction through news analysis. In Web Intelligence,
pages 301–304.
H. Zou and T. Hastie. 2005. Regularization and variable
selection via the elastic net. Journal Of The Royal Sta-
tistical Society Series B, 67(5):768–768.
</reference>
<figure confidence="0.997002833333334">
rating
sequels
people
genre
sentiment
plot
</figure>
<page confidence="0.980162">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.507035">
<title confidence="0.993751">Reviews and Revenues: An Experiment in Text</title>
<author confidence="0.788065">Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A</author>
<affiliation confidence="0.8208145">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992414">Pittsburgh, PA 15213,</address>
<abstract confidence="0.985271230769231">We consider the problem of predicting a movie’s opening weekend revenue. Previous work on this problem has used metadata about a movie—e.g., its genre, MPAA rating, and cast—with very limited work making use of movie. In this paper, we use the text of film critics’ reviews from several sources to predict opening weekend revenue. We describe a new dataset pairing movie reviews with metadata and revenue data, and show that review text can substitute for metadata, and even improve over it, for prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Friedman</author>
<author>T Hastie</author>
<author>R Tibshirani</author>
</authors>
<title>Regularized paths for generalized linear models via coordinate descent.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="7425" citStr="Friedman et al., 2008" startWordPosition="1241" endWordPosition="1244"> xi E Rp and yi E R for 1 G i G n: ( )2 yi − (00 + x� i �) +AP(�) A penalty term P(,3) is included in the objective for regularization. Classical solutions use an E2 or E1 norm, known respectively as ridge and lasso regression. Introduced recently is a mixture of the two, called the elastic net (Zou and Hastie, 2005): P(3) = E� =1(2(1 − α)02 + α|0j|) where α E (0, 1) determines the trade-off between E1 and E2 regularization. For our experiments we used the elastic net and specifically the glmnet package which contains an implementation of an efficient coordinate ascent procedure for training (Friedman et al., 2008). We tune the α and A parameters on our development set and select the model with the (α, A) combination that yields minimum MAE on the development set. 4 Experiments We compare predictors based on metadata, predictors based on text, and predictors that use both kinds of information. Results for two simple baselines of predicting the training set mean and median are reported in Table 2 (Pearson’s correlation is undefined since the standard deviation is zero). 4.1 Metadata Features We considered seven types of metadata features, and evaluated their performance by adding them to our pool of feat</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2008</marker>
<rawString>J. Friedman, T. Hastie, and R. Tibshirani. 2008. Regularized paths for generalized linear models via coordinate descent. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghose</author>
<author>P G Ipeirotis</author>
<author>A Sundararajan</author>
</authors>
<title>Opinion mining using econometrics: A case study on reputation systems.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3190" citStr="Ghose et al. (2007)" startWordPosition="517" endWordPosition="520">ge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across dif</context>
</contexts>
<marker>Ghose, Ipeirotis, Sundararajan, 2007</marker>
<rawString>A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007. Opinion mining using econometrics: A case study on reputation systems. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in NIPS 15.</booktitle>
<contexts>
<context position="9569" citStr="Klein and Manning, 2003" startWordPosition="1593" endWordPosition="1596">t three types of text features (described below). We only included feature instances that occurred in at least five different movies’ reviews. We stem and downcase individual word components in all our features. I. n-grams. We considered unigrams, bigrams, and trigrams. A 25-word stoplist was used; bigrams and trigrams were only filtered if all words were stopwords. II. Part-of-speech n-grams. As with words, we added unigrams, bigrams, and trigrams. Tags were obtained from the Stanford part-of-speech tagger (Toutanova and Manning, 2000). III. Dependency relations. We used the Stanford parser (Klein and Manning, 2003) to parse the critic reviews and extract syntactic dependencies. The dependency relation features consist of just the relation part of a dependency triple (relation, head word, modifier word). We consider three ways to combine the collection of reviews for a given movie. The first (“−”) simply concatenates all of a movie’s reviews into a single document before extracting features. The second (“+”) conjoins each feature with the source site (e.g., New York Times) from whose review it was extracted. A third version (denoted “B”) combines both the site-agnostic and site-specific features. Feature</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kogan</author>
<author>D Levin</author>
<author>B R Routledge</author>
<author>J Sagi</author>
<author>N A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>272--280</pages>
<contexts>
<context position="3214" citStr="Kogan et al. (2009)" startWordPosition="522" endWordPosition="525">used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across different media outlets and</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A. Smith. 2009. Predicting risk from financial reports with regression. In Proc. of NAACL, pages 272–280.</rawString>
</citation>
<citation valid="false">
<title>denote sentence boundaries. “brittany” frequently refers to Brittany Snow and Brittany Murphy. “&amp;quot; producer brian” refers to producer Brian Grazer (The Da Vinci Code, among others).</title>
<marker></marker>
<rawString>ally. &amp;quot; and $ denote sentence boundaries. “brittany” frequently refers to Brittany Snow and Brittany Murphy. “&amp;quot; producer brian” refers to producer Brian Grazer (The Da Vinci Code, among others).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mishne</author>
<author>N Glance</author>
</authors>
<title>Predicting movie sales from blogger sentiment.</title>
<date>2006</date>
<booktitle>In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs.</booktitle>
<contexts>
<context position="1788" citStr="Mishne and Glance (2006)" startWordPosition="284" endWordPosition="287">been shown to be successful for similar prediction tasks in previous work. There is a large body of prior work aimed at predicting gross revenue of movies (Simonoff and Sparrow, 2000; Sharda and Delen, 2006; inter alia). Certain information is used in nearly all prior work on these tasks, such as the movie’s genre, MPAA rating, running time, release date, the number of screens on which the movie debuted, and the presence of particular actors or actresses in the cast. Most prior textbased work has used automatic text analysis tools, deriving a small number of aggregate statistics. For example, Mishne and Glance (2006) applied sentiment analysis techniques to pre-release and postrelease blog posts about movies and showed higher *We appreciate reviewer feedback and technical advice from Brendan O’Connor. This work was supported by NSF IIS0803482, NSF IIS-0844507, and DARPA NBCH-1080004. correlation between actual revenue and sentimentbased metrics, as compared to mention counts of the movie. (They did not frame the task as a revenue prediction problem.) Zhang and Skiena (2009) used a news aggregation system to identify entities and obtain domain-specific sentiment for each entity in several domains. They use</context>
</contexts>
<marker>Mishne, Glance, 2006</marker>
<rawString>G. Mishne and N. Glance. 2006. Predicting movie sales from blogger sentiment. In AAAI Spring Symposium on Computational Approaches to Analysing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="2791" citStr="Pang et al., 2002" startWordPosition="446" endWordPosition="449"> did not frame the task as a revenue prediction problem.) Zhang and Skiena (2009) used a news aggregation system to identify entities and obtain domain-specific sentiment for each entity in several domains. They used the aggregate sentiment scores and mention counts of each movie in news articles as predictors. While there has been substantial prior work on using critics’ reviews, to our knowledge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proc. of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sharda</author>
<author>D Delen</author>
</authors>
<title>Predicting box office success of motion pictures with neural networks.</title>
<date>2006</date>
<booktitle>Expert Systems with Applications,</booktitle>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="1370" citStr="Sharda and Delen, 2006" startWordPosition="213" endWordPosition="216">rove over it, for prediction. 1 Introduction Predicting gross revenue for movies is a problem that has been studied in economics, marketing, statistics, and forecasting. Apart from the economic value of such predictions, we view the forecasting problem as an application of NLP. In this paper, we use the text of critics’ reviews to predict opening weekend revenue. We also consider metadata for each movie that has been shown to be successful for similar prediction tasks in previous work. There is a large body of prior work aimed at predicting gross revenue of movies (Simonoff and Sparrow, 2000; Sharda and Delen, 2006; inter alia). Certain information is used in nearly all prior work on these tasks, such as the movie’s genre, MPAA rating, running time, release date, the number of screens on which the movie debuted, and the presence of particular actors or actresses in the cast. Most prior textbased work has used automatic text analysis tools, deriving a small number of aggregate statistics. For example, Mishne and Glance (2006) applied sentiment analysis techniques to pre-release and postrelease blog posts about movies and showed higher *We appreciate reviewer feedback and technical advice from Brendan O’C</context>
</contexts>
<marker>Sharda, Delen, 2006</marker>
<rawString>R. Sharda and D. Delen. 2006. Predicting box office success of motion pictures with neural networks. Expert Systems with Applications, 30(2):243–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Simonoff</author>
<author>I R Sparrow</author>
</authors>
<title>Predicting movie grosses: Winners and losers, blockbusters and sleepers.</title>
<date>2000</date>
<journal>Chance,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="1346" citStr="Simonoff and Sparrow, 2000" startWordPosition="208" endWordPosition="212">e for metadata, and even improve over it, for prediction. 1 Introduction Predicting gross revenue for movies is a problem that has been studied in economics, marketing, statistics, and forecasting. Apart from the economic value of such predictions, we view the forecasting problem as an application of NLP. In this paper, we use the text of critics’ reviews to predict opening weekend revenue. We also consider metadata for each movie that has been shown to be successful for similar prediction tasks in previous work. There is a large body of prior work aimed at predicting gross revenue of movies (Simonoff and Sparrow, 2000; Sharda and Delen, 2006; inter alia). Certain information is used in nearly all prior work on these tasks, such as the movie’s genre, MPAA rating, running time, release date, the number of screens on which the movie debuted, and the presence of particular actors or actresses in the cast. Most prior textbased work has used automatic text analysis tools, deriving a small number of aggregate statistics. For example, Mishne and Glance (2006) applied sentiment analysis techniques to pre-release and postrelease blog posts about movies and showed higher *We appreciate reviewer feedback and technical</context>
<context position="11403" citStr="Simonoff and Sparrow (2000)" startWordPosition="1914" endWordPosition="1918">0.223 I U III + 5.738 0.812 6.003 0.477 B 5.750 0.819 †5.998 0.423 Table 2: Test-set performance for various models, measured using mean absolute error (MAE) and Pearson’s correlation (r), for two prediction tasks. Within a column, boldface shows the best result among “text” and “meta U text” settings. †Significantly better than the meta baseline with p &lt; 0.01, using the Wilcoxon signed rank test. 4.3 Results Table 2 shows our results for both prediction tasks. For the total first-weekend revenue prediction task, metadata features baseline result (r2 = 0.521) is comparable to that reported by Simonoff and Sparrow (2000) on a similar task of movie gross prediction (r2 = 0.446). Features from critics’ reviews by themselves improve correlation on both prediction tasks, however improvement in MAE is only observed for the per screen revenue prediction task. A combination of the meta and text features achieves the best performance both in terms of MAE and r. While the text-only models have some high negative weight features, the combined models do not have any negatively weighted features and only a very few metadata features. That is, the text is able to substitute for the other metadata features. Among the diffe</context>
</contexts>
<marker>Simonoff, Sparrow, 2000</marker>
<rawString>J. S. Simonoff and I. R. Sparrow. 2000. Predicting movie grosses: Winners and losers, blockbusters and sleepers. Chance, 13(3):15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Terry</author>
<author>M Butler</author>
<author>D De’Armond</author>
</authors>
<title>The determinants of domestic box office performance in the motion picture industry.</title>
<date>2005</date>
<journal>Southwestern Economic Review,</journal>
<pages>32--137</pages>
<marker>Terry, Butler, De’Armond, 2005</marker>
<rawString>N. Terry, M. Butler, and D. De’Armond. 2005. The determinants of domestic box office performance in the motion picture industry. Southwestern Economic Review, 32:137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy partof-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="9487" citStr="Toutanova and Manning, 2000" startWordPosition="1581" endWordPosition="1584">t contained only the first six types of metadata features. 4.2 Text Features We extract three types of text features (described below). We only included feature instances that occurred in at least five different movies’ reviews. We stem and downcase individual word components in all our features. I. n-grams. We considered unigrams, bigrams, and trigrams. A 25-word stoplist was used; bigrams and trigrams were only filtered if all words were stopwords. II. Part-of-speech n-grams. As with words, we added unigrams, bigrams, and trigrams. Tags were obtained from the Stanford part-of-speech tagger (Toutanova and Manning, 2000). III. Dependency relations. We used the Stanford parser (Klein and Manning, 2003) to parse the critic reviews and extract syntactic dependencies. The dependency relation features consist of just the relation part of a dependency triple (relation, head word, modifier word). We consider three ways to combine the collection of reviews for a given movie. The first (“−”) simply concatenates all of a movie’s reviews into a single document before extracting features. The second (“+”) conjoins each feature with the source site (e.g., New York Times) from whose review it was extracted. A third version</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C. D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy partof-speech tagger. In Proc. of EMNLP, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zhang</author>
<author>S Skiena</author>
</authors>
<title>Improving movie gross prediction through news analysis.</title>
<date>2009</date>
<booktitle>In Web Intelligence,</booktitle>
<pages>301--304</pages>
<contexts>
<context position="2254" citStr="Zhang and Skiena (2009)" startWordPosition="355" endWordPosition="358">ast. Most prior textbased work has used automatic text analysis tools, deriving a small number of aggregate statistics. For example, Mishne and Glance (2006) applied sentiment analysis techniques to pre-release and postrelease blog posts about movies and showed higher *We appreciate reviewer feedback and technical advice from Brendan O’Connor. This work was supported by NSF IIS0803482, NSF IIS-0844507, and DARPA NBCH-1080004. correlation between actual revenue and sentimentbased metrics, as compared to mention counts of the movie. (They did not frame the task as a revenue prediction problem.) Zhang and Skiena (2009) used a news aggregation system to identify entities and obtain domain-specific sentiment for each entity in several domains. They used the aggregate sentiment scores and mention counts of each movie in news articles as predictors. While there has been substantial prior work on using critics’ reviews, to our knowledge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to pr</context>
</contexts>
<marker>Zhang, Skiena, 2009</marker>
<rawString>W. Zhang and S. Skiena. 2009. Improving movie gross prediction through news analysis. In Web Intelligence, pages 301–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zou</author>
<author>T Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal Of The Royal Statistical Society Series B,</journal>
<volume>67</volume>
<issue>5</issue>
<contexts>
<context position="7121" citStr="Zou and Hastie, 2005" startWordPosition="1188" endWordPosition="1191">r the text of the reviews. That is, given an input feature vector x E Rp, we predict an output y� E R using a linear model: y� = 00 + xT,3. To learn values for the parameters 0 = (00,,O), the standard approach is to minimize the sum of squared errors for a training set containing n pairs (xi, yi) where xi E Rp and yi E R for 1 G i G n: ( )2 yi − (00 + x� i �) +AP(�) A penalty term P(,3) is included in the objective for regularization. Classical solutions use an E2 or E1 norm, known respectively as ridge and lasso regression. Introduced recently is a mixture of the two, called the elastic net (Zou and Hastie, 2005): P(3) = E� =1(2(1 − α)02 + α|0j|) where α E (0, 1) determines the trade-off between E1 and E2 regularization. For our experiments we used the elastic net and specifically the glmnet package which contains an implementation of an efficient coordinate ascent procedure for training (Friedman et al., 2008). We tune the α and A parameters on our development set and select the model with the (α, A) combination that yields minimum MAE on the development set. 4 Experiments We compare predictors based on metadata, predictors based on text, and predictors that use both kinds of information. Results for</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>H. Zou and T. Hastie. 2005. Regularization and variable selection via the elastic net. Journal Of The Royal Statistical Society Series B, 67(5):768–768.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>