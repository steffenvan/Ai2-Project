<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.351469">
<title confidence="0.961985">
Semantic Back-Pointers from Gesture
</title>
<author confidence="0.853111">
Jacob Eisenstein
</author>
<note confidence="0.913532">
MIT Computer Science and Artificial Intelligence Laboratory
77 Massachusetts Ave, MA 02139
</note>
<email confidence="0.998852">
jacobe@csail.mit.edu
</email>
<sectionHeader confidence="0.999564" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997399928571429">
Although the natural-language processing commu-
nity has dedicated much of its focus to text, face-
to-face spoken language is ubiquitous, and offers
the potential for breakthrough applications in do-
mains such as meetings, lectures, and presentations.
Because spontaneous spoken language is typically
more disfluent and less structured than written text,
it may be critical to identify features from additional
modalities that can aid in language understanding.
However, due to the long-standing emphasis on text
datasets, there has been relatively little work on non-
textual features in unconstrained natural language
(prosody being the most studied non-textual modal-
ity, e.g. (Shriberg et al., 2000)).
There are many non-verbal modalities that may
contribute to face-to-face communication, includ-
ing body posture, hand gesture, facial expression,
prosody, and free-hand drawing. Hand gesture may
be more expressive than any non-verbal modality
besides drawing, since it serves as the foundation
for sign languages in hearing-disabled communi-
ties. While non-deaf speakers rarely use any such
systematized language as American Sign Language
(ASL) while gesturing, the existence of ASL speaks
to the potential of gesture for communicative expres-
sivity.
Hand gesture relates to spoken language in several
ways:
</bodyText>
<listItem confidence="0.929563260869565">
• Hand gesture communicates meaning. For ex-
ample, (Kopp et al., 2006) describe a model
of how hand gesture is used to convey spatial
properties of its referents when speakers give
navigational directions. This model both ex-
plains observed behavior of human speakers,
and serves as the basis for an implemented em-
bodied agent.
• Hand gesture communicates discourse struc-
ture. (Quek et al., 2002) and (McNeill, 1992)
describe how the structure of discourse is mir-
rored by the the structure of the gestures, when
speakers describe sequences of events in car-
toon narratives.
• Hand gesture segments in unison with speech,
suggesting possible applications to speech
recognition and syntactic processing. (Morrel-
Samuels and Krauss, 1992) show a strong cor-
relation between the onset and duration of ges-
tures, and their “lexical affiliates” – the phrase
that is thought to relate semantically to the ges-
ture. Also, (Chen et al., 2004) show that gesture
features may improve sentence segmentation.
</listItem>
<bodyText confidence="0.999190294117647">
These examples are a subset of a broad litera-
ture on gesture that suggests that this modality could
play an important role in improving the performance
of NLP systems on spontaneous spoken language.
However, the existence of significant relationships
between gesture and speech does not prove that
gesture will improve NLP; gesture features could
be redundant with existing textual features, or they
may be simply too noisy or speaker-dependant to be
useful. To test this, my thesis research will iden-
tify specific, objective NLP tasks, and attempt to
show that automatically-detected gestural features
improve performance beyond what is attainable us-
ing textual features.
The relationship between gesture and meaning is
particularly intriguing, since gesture seems to offer
a unique, spatial representation of meaning to sup-
</bodyText>
<page confidence="0.982419">
215
</page>
<note confidence="0.874662">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 215–218,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999951073170732">
plement verbal expression. However, the expression
of meaning through gesture is likely to be highly
variable and speaker dependent, as the set of pos-
sible mappings between meaning and gestural form
is large, if not infinite. For this reason, I take the
point of view that it is too difficult to attempt to de-
code individual gestures. A more feasible approach
is to identify similarities between pairs or groups
of gestures. If gestures do communicate semantics,
then similar gestures should predict semantic sim-
ilarity. Thus, gestures can help computers under-
stand speech by providing a set of “back pointers”
between moments that are semantically related. Us-
ing this model, my dissertation will explore mea-
sures of gesture similarity and applications of ges-
ture similarity to NLP.
A set of semantic “back pointers” decoded from
gestural features could be relevant to a number of
NLP benchmark problems. I will investigate two:
coreference resolution and disfluency detection. In
coreference resolution, we seek to identify whether
two noun phrases refer to the same semantic entity.
A similarity in the gestural features observed during
two different noun phrases might suggest a similar-
ity in meaning. This problem has the advantage of
permitting a quantitative evaluation of the relation-
ship between gesture and semantics, without requir-
ing the construction of a domain ontology.
Restarts are disfluencies that occur when a
speaker begins an utterance, and then stops and
starts over again. It is thought that the gesture
may return to its state at the beginning of the utter-
ance, providing a back-pointer to the restart inser-
tion point (Esposito et al., 2001). If so, then a similar
training procedure and set of gestural features can
be used for both coreference resolution and restart
correction. Both of these problems have objective,
quantifiable success measures, and both may play
an important role in bringing to spontaneous spoken
language useful NLP applications such as summa-
rization, segmentation, and question answering.
</bodyText>
<sectionHeader confidence="0.891648" genericHeader="keywords">
2 Current Status
</sectionHeader>
<bodyText confidence="0.999875076923077">
My initial work involved hand annotation of ges-
ture, using the system proposed in (McNeill, 1992).
It was thought that hand annotation would identify
relevant features to be detected by computer vision
systems. However, in (Eisenstein and Davis, 2004),
we found that the gesture phrase type (e.g., deic-
tic, iconic, beat) could be predicted accurately by
lexical information alone, without regard to hand
movement. This suggests that this level of annota-
tion inherently captures a synthesis of gesture and
speech, rather than gesture alone. This conclusion
was strengthened by (Eisenstein and Davis, 2005),
where we found that hand-annotated gesture fea-
tures correlate well with sentence boundaries, but
that the gesture features were almost completely re-
dundant with information in the lexical features, and
did not improve overall performance.
The corpus used in my initial research was not
suitable for automatic extraction of gesture features
by computer vision, so a new corpus was gath-
ered, using a better-defined experimental protocol
and higher quality video and audio recording (Adler
et al., 2004). An articulated upper body tracker,
largely based on the work of (Deutscher et al., 2000),
was used to identify hand and arm positions, using
color and motion cues. All future work will be based
on this new corpus, which contains six videos each
from nine pairs of speakers. Each video is roughly
two to three minutes in length.
Each speaker was presented with three different
experimental conditions regarding how information
in the corpus was to be presented: a) a pre-printed
diagram was provided, b) the speaker was allowed
to draw a diagram using a tracked marker, c) no pre-
sentational aids were allowed. The first condition
was designed to be relevant to presentations involv-
ing pre-created presentation materials, such as Pow-
erpoint slides. The second condition was intended to
be similar to classroom lectures or design presenta-
tions. The third condition was aimed more at direct
one-on-one interaction.
My preliminary work has involved data from the
first condition, in which speakers gestured at pre-
printed diagrams. An empirical study on this part
of the corpus has identified several gesture features
that are relevant to coreference resolution (Eisen-
stein and Davis, 2006a). In particular, gesture sim-
ilarity can be measured by hand position and the
choice of the hand which makes the gesture; these
similarities correlate with the likelihood of coref-
erence. In addition, the likelihood of a gestural
hold – where the hand rests in place for a period of
</bodyText>
<page confidence="0.994813">
216
</page>
<bodyText confidence="0.999890545454545">
time – acts as a meta-feature, indicating that gestural
cues are likely to be particularly important to disam-
biguate the meaning of the associated noun phrase.
In (Eisenstein and Davis, 2006b), these features are
combined with traditional textual features for coref-
erence resolution, with encouraging results. The
hand position gesture feature was found to be the
fifth most informative feature by Chi-squared anal-
ysis, and the inclusion of gesture features yielded a
statistically significant increase in performance over
the textual features.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
3 Future Directions
</sectionHeader>
<bodyText confidence="0.99998334375">
The work on coreference can be considered prelimi-
nary, because it is focused on a subset of our corpus
in which speakers use pre-printed diagrams as an ex-
planatory aide. This changes their gestures (Eisen-
stein and Davis, 2003), increasing the proportion of
deictic gestures, in which hand position is the most
important feature (McNeill, 1992). Hand position
is assumed to be less useful in characterizing the
similarity of iconic gestures, which express meaning
through motion or handshape. Using the subsection
of the corpus in which no explanatory aids were pro-
vided, I will investigate how to assess the similarity
of such dynamic gestures, in the hope that corefer-
ence resolution can still benefit from gestural cues in
this more general case.
Disfluency repair is another plausible domain in
which gesture might improve performance. There
are at least two ways in which gesture could be rel-
evant to disfluency repair. Using the semantic back-
pointer model, restart repairs could be identified if
there is a strong gestural similarity between the orig-
inal start point and the restart. Alternatively, gesture
could play a pragmatic function, if there are char-
acteristic gestures that indicate restarts or other re-
pairs. In one case, we are looking for a similarity
between the disfluency and the repair point; in the
other case, we are looking for similarities across all
disfluencies, or across all repair points. It is hoped
that this research will not only improve processing
of spoken natural language, but also enhance our un-
derstanding of how speakers use gesture to structure
their discourse.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9999625">
The bulk of research on multimodality in the NLP
community relates to multimodal dialogue systems
(e.g., (Johnston and Bangalore, 2000)). This re-
search differs fundamentally from mine in that it ad-
dresses human-computer interaction, whereas I am
studying human-human interaction. Multimodal di-
alogue systems tackle many interesting challenges,
but the grammar, vocabulary, and recognized ges-
tures are often pre-specified, and dialogue is con-
trolled at least in part by the computer. In my data,
all of these things are unconstrained.
Another important area of research is the gen-
eration of multimodal communication in animated
agents (e.g., (Cassell et al., 2001; Kopp et al., 2006;
Nakano et al., 2003)). While the models devel-
oped in these papers are interesting and often well-
motivated by the psychological literature, it remains
to be seen whether they are both broad and precise
enough to apply to gesture recognition.
There is a substantial body of empirical work de-
scribing relationships between non-verbal and lin-
guistic phenomena, much of which suggests that
gesture could be used to improve the detection of
such phenomena. (Quek et al., 2002) describe ex-
amples in which gesture correlates with topic shifts
in the discourse structure, raising the possibility
that topic segmentation and summarization could be
aided by gesture features; Cassell et al. (2001) make
a similar argument using body posture. (Nakano et
al., 2003) describes how head gestures and eye gaze
relate to turn taking and dialogue grounding. All
of the studies listed in this paragraph identify rel-
evant correlations between non-verbal communica-
tion and linguistic phenomena, but none construct a
predictive system that uses the non-verbal modali-
ties to improve performance beyond a text-only sys-
tem.
Prosody has been shown to improve performance
on several NLP problems, such as topic and sentence
segmentation (e.g., (Shriberg et al., 2000; Kim et
al., 2004)). The prosody literature demonstrates that
non-verbal features can improve performance on a
wide variety of NLP tasks. However, it also warns
that performance is often quite sensitive, both to the
representation of prosodic features, and how they are
integrated with other linguistic features.
</bodyText>
<page confidence="0.994121">
217
</page>
<bodyText confidence="0.999978272727273">
The literature on prosody would suggest paral-
lels for gesture features, but little such work has
been reported. (Chen et al., 2004) shows that ges-
ture may improve sentence segmentation; however,
in this study, the improvement afforded by gesture is
not statistically significant, and evaluation was per-
formed on a subset of their original corpus that was
chosen to include only the three speakers who ges-
tured most frequently. Still, this work provides a
valuable starting point for the integration of gesture
feature into NLP systems.
</bodyText>
<sectionHeader confidence="0.998566" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999922">
Spontaneous spoken language poses difficult prob-
lems for natural language processing, but these diffi-
culties may be offset by the availability of additional
communicative modalities. Using a model of hand
gesture as providing a set of semantic back-pointers
to previous utterances, I am exploring whether ges-
ture can improve performance on quantitative NLP
benchmark tasks. Preliminary results on coreference
resolution are encouraging.
</bodyText>
<sectionHeader confidence="0.998148" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99969084057971">
Aaron Adler, Jacob Eisenstein, Michael Oltmans, Lisa
Guttentag, and Randall Davis. 2004. Building the de-
sign studio of the future. In Making Pen-Based Inter-
action Intelligent and Natural, pages 1–7, Menlo Park,
California, October 21-24. AAAI Press.
Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-
more, Candace L. Sidner, and Charles Rich. 2001.
Non-verbal cues for discourse structure. In Proc. of
ACL, pages 106–115.
Lei Chen, Yang Liu, Mary P. Harper, and Eliza-
beth Shriberg. 2004. Multimodal model integra-
tion for sentence unit detection. In Proceedings of
International Conference on Multimodal Interfaces
(ICMI’04). ACM Press.
Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000.
Articulated body motion capture by annealed particle
filtering. In IEEE Conference on Computer Vision and
Pattern Recognition, volume 2, pages 126–133.
Jacob Eisenstein and Randall Davis. 2003. Natural ges-
ture in descriptive monologues. In UIST’03 Supple-
mental Proceedings, pages 69–70. ACM Press.
Jacob Eisenstein and Randall Davis. 2004. Visual and
linguistic information in gesture classification. In Pro-
ceedings of International Conference on Multimodal
Interfaces(ICMI’04). ACM Press.
Jacob Eisenstein and Randall Davis. 2005. Gestural cues
for sentence segmentation. Technical Report AIM-
2005-014, MIT AI Memo.
Jacob Eisenstein and Randall Davis. 2006a. Gesture fea-
tures for coreference resolution. In Workshop on Mul-
timodal Interaction and Related Machine Learning Al-
gorithms.
Jacob Eisenstein and Randall Davis. 2006b. Gesture
improves coreference resolution. In Proceedings of
NAACL.
Anna Esposito, Karl E. McCullough, and Francis Quek.
2001. Disfluencies in gesture: Gestural correlates to
filled and unfilled speech pauses. In Proceedings of
IEEE Workshop on Cues in Communication.
Michael Johnston and Srinivas Bangalore. 2000. Finite-
state multimodal parsing and understanding,. In Pro-
ceedings of COLING-2000, pages 369–375.
Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.
2004. Detecting structural metadata with decision
trees and transformation-based learning. In Proceed-
ings ofHLT-NAACL’04. ACL Press.
Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine
Cassell. 2006. Trading spaces: How humans and
humanoids use speech and gesture to give directions.
Spatial Cognition and Computation, In preparation.
David McNeill. 1992. Hand and Mind. The University
of Chicago Press.
P. Morrel-Samuels and R. M. Krauss. 1992. Word fa-
miliarity predicts temporal asynchrony of hand ges-
tures and speech. Journal of Experimental Psychol-
ogy: Learning, Memory and Cognition, 18:615–623.
Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-
tine Cassell. 2003. Towards a model of face-to-face
grounding. In Proceedings ofACL’03.
Francis Quek, David McNeill, Robert Bryll, Susan Dun-
can, Xin-Feng Ma, Cemil Kirbas, Karl E. McCul-
lough, and Rashid Ansari. 2002. Multimodal human
discourse: gesture and speech. ACM Transactions on
Computer-Human Interaction (TOCHI), pages 171–
193.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur,
and Gokhan Tur. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32.
</reference>
<page confidence="0.997166">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974694">
<title confidence="0.99896">Semantic Back-Pointers from Gesture</title>
<author confidence="0.99836">Jacob</author>
<affiliation confidence="0.999597">MIT Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.991952">77 Massachusetts Ave, MA</address>
<email confidence="0.984829">jacobe@csail.mit.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aaron Adler</author>
<author>Jacob Eisenstein</author>
<author>Michael Oltmans</author>
<author>Lisa Guttentag</author>
<author>Randall Davis</author>
</authors>
<title>Building the design studio of the future.</title>
<date>2004</date>
<booktitle>In Making Pen-Based Interaction Intelligent and Natural,</booktitle>
<pages>1--7</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, California,</location>
<contexts>
<context position="6646" citStr="Adler et al., 2004" startWordPosition="1012" endWordPosition="1015">is of gesture and speech, rather than gesture alone. This conclusion was strengthened by (Eisenstein and Davis, 2005), where we found that hand-annotated gesture features correlate well with sentence boundaries, but that the gesture features were almost completely redundant with information in the lexical features, and did not improve overall performance. The corpus used in my initial research was not suitable for automatic extraction of gesture features by computer vision, so a new corpus was gathered, using a better-defined experimental protocol and higher quality video and audio recording (Adler et al., 2004). An articulated upper body tracker, largely based on the work of (Deutscher et al., 2000), was used to identify hand and arm positions, using color and motion cues. All future work will be based on this new corpus, which contains six videos each from nine pairs of speakers. Each video is roughly two to three minutes in length. Each speaker was presented with three different experimental conditions regarding how information in the corpus was to be presented: a) a pre-printed diagram was provided, b) the speaker was allowed to draw a diagram using a tracked marker, c) no presentational aids wer</context>
</contexts>
<marker>Adler, Eisenstein, Oltmans, Guttentag, Davis, 2004</marker>
<rawString>Aaron Adler, Jacob Eisenstein, Michael Oltmans, Lisa Guttentag, and Randall Davis. 2004. Building the design studio of the future. In Making Pen-Based Interaction Intelligent and Natural, pages 1–7, Menlo Park, California, October 21-24. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
<author>Yukiko I Nakano</author>
<author>Timothy W Bickmore</author>
<author>Candace L Sidner</author>
<author>Charles Rich</author>
</authors>
<title>Non-verbal cues for discourse structure.</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>106--115</pages>
<contexts>
<context position="10963" citStr="Cassell et al., 2001" startWordPosition="1700" endWordPosition="1703">he NLP community relates to multimodal dialogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in these papers are interesting and often wellmotivated by the psychological literature, it remains to be seen whether they are both broad and precise enough to apply to gesture recognition. There is a substantial body of empirical work describing relationships between non-verbal and linguistic phenomena, much of which suggests that gesture could be used to improve the detection of such phenomena. (Quek et al., 2002) describe examples in which gesture correlates with topic shifts in the discourse structure, raising the poss</context>
</contexts>
<marker>Cassell, Nakano, Bickmore, Sidner, Rich, 2001</marker>
<rawString>Justine Cassell, Yukiko I. Nakano, Timothy W. Bickmore, Candace L. Sidner, and Charles Rich. 2001. Non-verbal cues for discourse structure. In Proc. of ACL, pages 106–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Yang Liu</author>
<author>Mary P Harper</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Multimodal model integration for sentence unit detection.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Multimodal Interfaces (ICMI’04).</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2410" citStr="Chen et al., 2004" startWordPosition="357" endWordPosition="360">d embodied agent. • Hand gesture communicates discourse structure. (Quek et al., 2002) and (McNeill, 1992) describe how the structure of discourse is mirrored by the the structure of the gestures, when speakers describe sequences of events in cartoon narratives. • Hand gesture segments in unison with speech, suggesting possible applications to speech recognition and syntactic processing. (MorrelSamuels and Krauss, 1992) show a strong correlation between the onset and duration of gestures, and their “lexical affiliates” – the phrase that is thought to relate semantically to the gesture. Also, (Chen et al., 2004) show that gesture features may improve sentence segmentation. These examples are a subset of a broad literature on gesture that suggests that this modality could play an important role in improving the performance of NLP systems on spontaneous spoken language. However, the existence of significant relationships between gesture and speech does not prove that gesture will improve NLP; gesture features could be redundant with existing textual features, or they may be simply too noisy or speaker-dependant to be useful. To test this, my thesis research will identify specific, objective NLP tasks, </context>
<context position="12672" citStr="Chen et al., 2004" startWordPosition="1969" endWordPosition="1972">e beyond a text-only system. Prosody has been shown to improve performance on several NLP problems, such as topic and sentence segmentation (e.g., (Shriberg et al., 2000; Kim et al., 2004)). The prosody literature demonstrates that non-verbal features can improve performance on a wide variety of NLP tasks. However, it also warns that performance is often quite sensitive, both to the representation of prosodic features, and how they are integrated with other linguistic features. 217 The literature on prosody would suggest parallels for gesture features, but little such work has been reported. (Chen et al., 2004) shows that gesture may improve sentence segmentation; however, in this study, the improvement afforded by gesture is not statistically significant, and evaluation was performed on a subset of their original corpus that was chosen to include only the three speakers who gestured most frequently. Still, this work provides a valuable starting point for the integration of gesture feature into NLP systems. 5 Summary Spontaneous spoken language poses difficult problems for natural language processing, but these difficulties may be offset by the availability of additional communicative modalities. Us</context>
</contexts>
<marker>Chen, Liu, Harper, Shriberg, 2004</marker>
<rawString>Lei Chen, Yang Liu, Mary P. Harper, and Elizabeth Shriberg. 2004. Multimodal model integration for sentence unit detection. In Proceedings of International Conference on Multimodal Interfaces (ICMI’04). ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Deutscher</author>
<author>Andrew Blake</author>
<author>Ian Reid</author>
</authors>
<title>Articulated body motion capture by annealed particle filtering.</title>
<date>2000</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>126--133</pages>
<contexts>
<context position="6736" citStr="Deutscher et al., 2000" startWordPosition="1027" endWordPosition="1030">by (Eisenstein and Davis, 2005), where we found that hand-annotated gesture features correlate well with sentence boundaries, but that the gesture features were almost completely redundant with information in the lexical features, and did not improve overall performance. The corpus used in my initial research was not suitable for automatic extraction of gesture features by computer vision, so a new corpus was gathered, using a better-defined experimental protocol and higher quality video and audio recording (Adler et al., 2004). An articulated upper body tracker, largely based on the work of (Deutscher et al., 2000), was used to identify hand and arm positions, using color and motion cues. All future work will be based on this new corpus, which contains six videos each from nine pairs of speakers. Each video is roughly two to three minutes in length. Each speaker was presented with three different experimental conditions regarding how information in the corpus was to be presented: a) a pre-printed diagram was provided, b) the speaker was allowed to draw a diagram using a tracked marker, c) no presentational aids were allowed. The first condition was designed to be relevant to presentations involving pre-</context>
</contexts>
<marker>Deutscher, Blake, Reid, 2000</marker>
<rawString>Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000. Articulated body motion capture by annealed particle filtering. In IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Natural gesture in descriptive monologues.</title>
<date>2003</date>
<booktitle>In UIST’03 Supplemental Proceedings,</booktitle>
<pages>69--70</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="8909" citStr="Eisenstein and Davis, 2003" startWordPosition="1375" endWordPosition="1379">Eisenstein and Davis, 2006b), these features are combined with traditional textual features for coreference resolution, with encouraging results. The hand position gesture feature was found to be the fifth most informative feature by Chi-squared analysis, and the inclusion of gesture features yielded a statistically significant increase in performance over the textual features. 3 Future Directions The work on coreference can be considered preliminary, because it is focused on a subset of our corpus in which speakers use pre-printed diagrams as an explanatory aide. This changes their gestures (Eisenstein and Davis, 2003), increasing the proportion of deictic gestures, in which hand position is the most important feature (McNeill, 1992). Hand position is assumed to be less useful in characterizing the similarity of iconic gestures, which express meaning through motion or handshape. Using the subsection of the corpus in which no explanatory aids were provided, I will investigate how to assess the similarity of such dynamic gestures, in the hope that coreference resolution can still benefit from gestural cues in this more general case. Disfluency repair is another plausible domain in which gesture might improve </context>
</contexts>
<marker>Eisenstein, Davis, 2003</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2003. Natural gesture in descriptive monologues. In UIST’03 Supplemental Proceedings, pages 69–70. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Visual and linguistic information in gesture classification.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Multimodal Interfaces(ICMI’04).</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="5791" citStr="Eisenstein and Davis, 2004" startWordPosition="881" endWordPosition="884"> similar training procedure and set of gestural features can be used for both coreference resolution and restart correction. Both of these problems have objective, quantifiable success measures, and both may play an important role in bringing to spontaneous spoken language useful NLP applications such as summarization, segmentation, and question answering. 2 Current Status My initial work involved hand annotation of gesture, using the system proposed in (McNeill, 1992). It was thought that hand annotation would identify relevant features to be detected by computer vision systems. However, in (Eisenstein and Davis, 2004), we found that the gesture phrase type (e.g., deictic, iconic, beat) could be predicted accurately by lexical information alone, without regard to hand movement. This suggests that this level of annotation inherently captures a synthesis of gesture and speech, rather than gesture alone. This conclusion was strengthened by (Eisenstein and Davis, 2005), where we found that hand-annotated gesture features correlate well with sentence boundaries, but that the gesture features were almost completely redundant with information in the lexical features, and did not improve overall performance. The co</context>
</contexts>
<marker>Eisenstein, Davis, 2004</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2004. Visual and linguistic information in gesture classification. In Proceedings of International Conference on Multimodal Interfaces(ICMI’04). ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Gestural cues for sentence segmentation.</title>
<date>2005</date>
<tech>Technical Report AIM2005-014,</tech>
<institution>MIT AI Memo.</institution>
<contexts>
<context position="6144" citStr="Eisenstein and Davis, 2005" startWordPosition="935" endWordPosition="938">ring. 2 Current Status My initial work involved hand annotation of gesture, using the system proposed in (McNeill, 1992). It was thought that hand annotation would identify relevant features to be detected by computer vision systems. However, in (Eisenstein and Davis, 2004), we found that the gesture phrase type (e.g., deictic, iconic, beat) could be predicted accurately by lexical information alone, without regard to hand movement. This suggests that this level of annotation inherently captures a synthesis of gesture and speech, rather than gesture alone. This conclusion was strengthened by (Eisenstein and Davis, 2005), where we found that hand-annotated gesture features correlate well with sentence boundaries, but that the gesture features were almost completely redundant with information in the lexical features, and did not improve overall performance. The corpus used in my initial research was not suitable for automatic extraction of gesture features by computer vision, so a new corpus was gathered, using a better-defined experimental protocol and higher quality video and audio recording (Adler et al., 2004). An articulated upper body tracker, largely based on the work of (Deutscher et al., 2000), was us</context>
</contexts>
<marker>Eisenstein, Davis, 2005</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2005. Gestural cues for sentence segmentation. Technical Report AIM2005-014, MIT AI Memo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Gesture features for coreference resolution.</title>
<date>2006</date>
<booktitle>In Workshop on Multimodal Interaction and Related Machine Learning Algorithms.</booktitle>
<contexts>
<context position="7831" citStr="Eisenstein and Davis, 2006" startWordPosition="1202" endWordPosition="1206">ked marker, c) no presentational aids were allowed. The first condition was designed to be relevant to presentations involving pre-created presentation materials, such as Powerpoint slides. The second condition was intended to be similar to classroom lectures or design presentations. The third condition was aimed more at direct one-on-one interaction. My preliminary work has involved data from the first condition, in which speakers gestured at preprinted diagrams. An empirical study on this part of the corpus has identified several gesture features that are relevant to coreference resolution (Eisenstein and Davis, 2006a). In particular, gesture similarity can be measured by hand position and the choice of the hand which makes the gesture; these similarities correlate with the likelihood of coreference. In addition, the likelihood of a gestural hold – where the hand rests in place for a period of 216 time – acts as a meta-feature, indicating that gestural cues are likely to be particularly important to disambiguate the meaning of the associated noun phrase. In (Eisenstein and Davis, 2006b), these features are combined with traditional textual features for coreference resolution, with encouraging results. The</context>
</contexts>
<marker>Eisenstein, Davis, 2006</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2006a. Gesture features for coreference resolution. In Workshop on Multimodal Interaction and Related Machine Learning Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Gesture improves coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="7831" citStr="Eisenstein and Davis, 2006" startWordPosition="1202" endWordPosition="1206">ked marker, c) no presentational aids were allowed. The first condition was designed to be relevant to presentations involving pre-created presentation materials, such as Powerpoint slides. The second condition was intended to be similar to classroom lectures or design presentations. The third condition was aimed more at direct one-on-one interaction. My preliminary work has involved data from the first condition, in which speakers gestured at preprinted diagrams. An empirical study on this part of the corpus has identified several gesture features that are relevant to coreference resolution (Eisenstein and Davis, 2006a). In particular, gesture similarity can be measured by hand position and the choice of the hand which makes the gesture; these similarities correlate with the likelihood of coreference. In addition, the likelihood of a gestural hold – where the hand rests in place for a period of 216 time – acts as a meta-feature, indicating that gestural cues are likely to be particularly important to disambiguate the meaning of the associated noun phrase. In (Eisenstein and Davis, 2006b), these features are combined with traditional textual features for coreference resolution, with encouraging results. The</context>
</contexts>
<marker>Eisenstein, Davis, 2006</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2006b. Gesture improves coreference resolution. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Esposito</author>
<author>Karl E McCullough</author>
<author>Francis Quek</author>
</authors>
<title>Disfluencies in gesture: Gestural correlates to filled and unfilled speech pauses.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE Workshop on Cues in Communication.</booktitle>
<contexts>
<context position="5149" citStr="Esposito et al., 2001" startWordPosition="784" endWordPosition="787">phrases refer to the same semantic entity. A similarity in the gestural features observed during two different noun phrases might suggest a similarity in meaning. This problem has the advantage of permitting a quantitative evaluation of the relationship between gesture and semantics, without requiring the construction of a domain ontology. Restarts are disfluencies that occur when a speaker begins an utterance, and then stops and starts over again. It is thought that the gesture may return to its state at the beginning of the utterance, providing a back-pointer to the restart insertion point (Esposito et al., 2001). If so, then a similar training procedure and set of gestural features can be used for both coreference resolution and restart correction. Both of these problems have objective, quantifiable success measures, and both may play an important role in bringing to spontaneous spoken language useful NLP applications such as summarization, segmentation, and question answering. 2 Current Status My initial work involved hand annotation of gesture, using the system proposed in (McNeill, 1992). It was thought that hand annotation would identify relevant features to be detected by computer vision systems</context>
</contexts>
<marker>Esposito, McCullough, Quek, 2001</marker>
<rawString>Anna Esposito, Karl E. McCullough, and Francis Quek. 2001. Disfluencies in gesture: Gestural correlates to filled and unfilled speech pauses. In Proceedings of IEEE Workshop on Cues in Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Finitestate multimodal parsing and understanding,.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000,</booktitle>
<pages>369--375</pages>
<contexts>
<context position="10436" citStr="Johnston and Bangalore, 2000" startWordPosition="1620" endWordPosition="1623"> pragmatic function, if there are characteristic gestures that indicate restarts or other repairs. In one case, we are looking for a similarity between the disfluency and the repair point; in the other case, we are looking for similarities across all disfluencies, or across all repair points. It is hoped that this research will not only improve processing of spoken natural language, but also enhance our understanding of how speakers use gesture to structure their discourse. 4 Related Work The bulk of research on multimodality in the NLP community relates to multimodal dialogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in</context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>Michael Johnston and Srinivas Bangalore. 2000. Finitestate multimodal parsing and understanding,. In Proceedings of COLING-2000, pages 369–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joungbum Kim</author>
<author>Sarah E Schwarm</author>
<author>Mari Osterdorf</author>
</authors>
<title>Detecting structural metadata with decision trees and transformation-based learning.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL’04.</booktitle>
<publisher>ACL Press.</publisher>
<contexts>
<context position="12242" citStr="Kim et al., 2004" startWordPosition="1903" endWordPosition="1906">ed by gesture features; Cassell et al. (2001) make a similar argument using body posture. (Nakano et al., 2003) describes how head gestures and eye gaze relate to turn taking and dialogue grounding. All of the studies listed in this paragraph identify relevant correlations between non-verbal communication and linguistic phenomena, but none construct a predictive system that uses the non-verbal modalities to improve performance beyond a text-only system. Prosody has been shown to improve performance on several NLP problems, such as topic and sentence segmentation (e.g., (Shriberg et al., 2000; Kim et al., 2004)). The prosody literature demonstrates that non-verbal features can improve performance on a wide variety of NLP tasks. However, it also warns that performance is often quite sensitive, both to the representation of prosodic features, and how they are integrated with other linguistic features. 217 The literature on prosody would suggest parallels for gesture features, but little such work has been reported. (Chen et al., 2004) shows that gesture may improve sentence segmentation; however, in this study, the improvement afforded by gesture is not statistically significant, and evaluation was pe</context>
</contexts>
<marker>Kim, Schwarm, Osterdorf, 2004</marker>
<rawString>Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf. 2004. Detecting structural metadata with decision trees and transformation-based learning. In Proceedings ofHLT-NAACL’04. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Kopp</author>
<author>Paul Tepper</author>
<author>Kim Ferriman</author>
<author>Justine Cassell</author>
</authors>
<title>Trading spaces: How humans and humanoids use speech and gesture to give directions. Spatial Cognition and Computation, In preparation.</title>
<date>2006</date>
<contexts>
<context position="1553" citStr="Kopp et al., 2006" startWordPosition="218" endWordPosition="221"> to face-to-face communication, including body posture, hand gesture, facial expression, prosody, and free-hand drawing. Hand gesture may be more expressive than any non-verbal modality besides drawing, since it serves as the foundation for sign languages in hearing-disabled communities. While non-deaf speakers rarely use any such systematized language as American Sign Language (ASL) while gesturing, the existence of ASL speaks to the potential of gesture for communicative expressivity. Hand gesture relates to spoken language in several ways: • Hand gesture communicates meaning. For example, (Kopp et al., 2006) describe a model of how hand gesture is used to convey spatial properties of its referents when speakers give navigational directions. This model both explains observed behavior of human speakers, and serves as the basis for an implemented embodied agent. • Hand gesture communicates discourse structure. (Quek et al., 2002) and (McNeill, 1992) describe how the structure of discourse is mirrored by the the structure of the gestures, when speakers describe sequences of events in cartoon narratives. • Hand gesture segments in unison with speech, suggesting possible applications to speech recognit</context>
<context position="10982" citStr="Kopp et al., 2006" startWordPosition="1704" endWordPosition="1707">es to multimodal dialogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in these papers are interesting and often wellmotivated by the psychological literature, it remains to be seen whether they are both broad and precise enough to apply to gesture recognition. There is a substantial body of empirical work describing relationships between non-verbal and linguistic phenomena, much of which suggests that gesture could be used to improve the detection of such phenomena. (Quek et al., 2002) describe examples in which gesture correlates with topic shifts in the discourse structure, raising the possibility that topic </context>
</contexts>
<marker>Kopp, Tepper, Ferriman, Cassell, 2006</marker>
<rawString>Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine Cassell. 2006. Trading spaces: How humans and humanoids use speech and gesture to give directions. Spatial Cognition and Computation, In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McNeill</author>
</authors>
<title>Hand and Mind.</title>
<date>1992</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1898" citStr="McNeill, 1992" startWordPosition="276" endWordPosition="277">guage as American Sign Language (ASL) while gesturing, the existence of ASL speaks to the potential of gesture for communicative expressivity. Hand gesture relates to spoken language in several ways: • Hand gesture communicates meaning. For example, (Kopp et al., 2006) describe a model of how hand gesture is used to convey spatial properties of its referents when speakers give navigational directions. This model both explains observed behavior of human speakers, and serves as the basis for an implemented embodied agent. • Hand gesture communicates discourse structure. (Quek et al., 2002) and (McNeill, 1992) describe how the structure of discourse is mirrored by the the structure of the gestures, when speakers describe sequences of events in cartoon narratives. • Hand gesture segments in unison with speech, suggesting possible applications to speech recognition and syntactic processing. (MorrelSamuels and Krauss, 1992) show a strong correlation between the onset and duration of gestures, and their “lexical affiliates” – the phrase that is thought to relate semantically to the gesture. Also, (Chen et al., 2004) show that gesture features may improve sentence segmentation. These examples are a subs</context>
<context position="5637" citStr="McNeill, 1992" startWordPosition="860" endWordPosition="861">o its state at the beginning of the utterance, providing a back-pointer to the restart insertion point (Esposito et al., 2001). If so, then a similar training procedure and set of gestural features can be used for both coreference resolution and restart correction. Both of these problems have objective, quantifiable success measures, and both may play an important role in bringing to spontaneous spoken language useful NLP applications such as summarization, segmentation, and question answering. 2 Current Status My initial work involved hand annotation of gesture, using the system proposed in (McNeill, 1992). It was thought that hand annotation would identify relevant features to be detected by computer vision systems. However, in (Eisenstein and Davis, 2004), we found that the gesture phrase type (e.g., deictic, iconic, beat) could be predicted accurately by lexical information alone, without regard to hand movement. This suggests that this level of annotation inherently captures a synthesis of gesture and speech, rather than gesture alone. This conclusion was strengthened by (Eisenstein and Davis, 2005), where we found that hand-annotated gesture features correlate well with sentence boundaries</context>
<context position="9026" citStr="McNeill, 1992" startWordPosition="1395" endWordPosition="1396">raging results. The hand position gesture feature was found to be the fifth most informative feature by Chi-squared analysis, and the inclusion of gesture features yielded a statistically significant increase in performance over the textual features. 3 Future Directions The work on coreference can be considered preliminary, because it is focused on a subset of our corpus in which speakers use pre-printed diagrams as an explanatory aide. This changes their gestures (Eisenstein and Davis, 2003), increasing the proportion of deictic gestures, in which hand position is the most important feature (McNeill, 1992). Hand position is assumed to be less useful in characterizing the similarity of iconic gestures, which express meaning through motion or handshape. Using the subsection of the corpus in which no explanatory aids were provided, I will investigate how to assess the similarity of such dynamic gestures, in the hope that coreference resolution can still benefit from gestural cues in this more general case. Disfluency repair is another plausible domain in which gesture might improve performance. There are at least two ways in which gesture could be relevant to disfluency repair. Using the semantic </context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>David McNeill. 1992. Hand and Mind. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Morrel-Samuels</author>
<author>R M Krauss</author>
</authors>
<title>Word familiarity predicts temporal asynchrony of hand gestures and speech.</title>
<date>1992</date>
<booktitle>Journal of Experimental Psychology: Learning, Memory and Cognition,</booktitle>
<pages>18--615</pages>
<marker>Morrel-Samuels, Krauss, 1992</marker>
<rawString>P. Morrel-Samuels and R. M. Krauss. 1992. Word familiarity predicts temporal asynchrony of hand gestures and speech. Journal of Experimental Psychology: Learning, Memory and Cognition, 18:615–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukiko Nakano</author>
<author>Gabe Reinstein</author>
<author>Tom Stocky</author>
<author>Justine Cassell</author>
</authors>
<title>Towards a model of face-to-face grounding.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL’03.</booktitle>
<contexts>
<context position="11004" citStr="Nakano et al., 2003" startWordPosition="1708" endWordPosition="1711">alogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in these papers are interesting and often wellmotivated by the psychological literature, it remains to be seen whether they are both broad and precise enough to apply to gesture recognition. There is a substantial body of empirical work describing relationships between non-verbal and linguistic phenomena, much of which suggests that gesture could be used to improve the detection of such phenomena. (Quek et al., 2002) describe examples in which gesture correlates with topic shifts in the discourse structure, raising the possibility that topic segmentation and summa</context>
</contexts>
<marker>Nakano, Reinstein, Stocky, Cassell, 2003</marker>
<rawString>Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Justine Cassell. 2003. Towards a model of face-to-face grounding. In Proceedings ofACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Quek</author>
<author>David McNeill</author>
<author>Robert Bryll</author>
<author>Susan Duncan</author>
<author>Xin-Feng Ma</author>
<author>Cemil Kirbas</author>
<author>Karl E McCullough</author>
<author>Rashid Ansari</author>
</authors>
<title>Multimodal human discourse: gesture and speech.</title>
<date>2002</date>
<journal>ACM Transactions on Computer-Human Interaction (TOCHI),</journal>
<pages>171--193</pages>
<contexts>
<context position="1878" citStr="Quek et al., 2002" startWordPosition="271" endWordPosition="274">ny such systematized language as American Sign Language (ASL) while gesturing, the existence of ASL speaks to the potential of gesture for communicative expressivity. Hand gesture relates to spoken language in several ways: • Hand gesture communicates meaning. For example, (Kopp et al., 2006) describe a model of how hand gesture is used to convey spatial properties of its referents when speakers give navigational directions. This model both explains observed behavior of human speakers, and serves as the basis for an implemented embodied agent. • Hand gesture communicates discourse structure. (Quek et al., 2002) and (McNeill, 1992) describe how the structure of discourse is mirrored by the the structure of the gestures, when speakers describe sequences of events in cartoon narratives. • Hand gesture segments in unison with speech, suggesting possible applications to speech recognition and syntactic processing. (MorrelSamuels and Krauss, 1992) show a strong correlation between the onset and duration of gestures, and their “lexical affiliates” – the phrase that is thought to relate semantically to the gesture. Also, (Chen et al., 2004) show that gesture features may improve sentence segmentation. These</context>
<context position="11454" citStr="Quek et al., 2002" startWordPosition="1781" endWordPosition="1784">nother important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in these papers are interesting and often wellmotivated by the psychological literature, it remains to be seen whether they are both broad and precise enough to apply to gesture recognition. There is a substantial body of empirical work describing relationships between non-verbal and linguistic phenomena, much of which suggests that gesture could be used to improve the detection of such phenomena. (Quek et al., 2002) describe examples in which gesture correlates with topic shifts in the discourse structure, raising the possibility that topic segmentation and summarization could be aided by gesture features; Cassell et al. (2001) make a similar argument using body posture. (Nakano et al., 2003) describes how head gestures and eye gaze relate to turn taking and dialogue grounding. All of the studies listed in this paragraph identify relevant correlations between non-verbal communication and linguistic phenomena, but none construct a predictive system that uses the non-verbal modalities to improve performanc</context>
</contexts>
<marker>Quek, McNeill, Bryll, Duncan, Ma, Kirbas, McCullough, Ansari, 2002</marker>
<rawString>Francis Quek, David McNeill, Robert Bryll, Susan Duncan, Xin-Feng Ma, Cemil Kirbas, Karl E. McCullough, and Rashid Ansari. 2002. Multimodal human discourse: gesture and speech. ACM Transactions on Computer-Human Interaction (TOCHI), pages 171– 193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokhan Tur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>32</volume>
<contexts>
<context position="876" citStr="Shriberg et al., 2000" startWordPosition="119" endWordPosition="122">ts focus to text, faceto-face spoken language is ubiquitous, and offers the potential for breakthrough applications in domains such as meetings, lectures, and presentations. Because spontaneous spoken language is typically more disfluent and less structured than written text, it may be critical to identify features from additional modalities that can aid in language understanding. However, due to the long-standing emphasis on text datasets, there has been relatively little work on nontextual features in unconstrained natural language (prosody being the most studied non-textual modality, e.g. (Shriberg et al., 2000)). There are many non-verbal modalities that may contribute to face-to-face communication, including body posture, hand gesture, facial expression, prosody, and free-hand drawing. Hand gesture may be more expressive than any non-verbal modality besides drawing, since it serves as the foundation for sign languages in hearing-disabled communities. While non-deaf speakers rarely use any such systematized language as American Sign Language (ASL) while gesturing, the existence of ASL speaks to the potential of gesture for communicative expressivity. Hand gesture relates to spoken language in severa</context>
<context position="12223" citStr="Shriberg et al., 2000" startWordPosition="1899" endWordPosition="1902">marization could be aided by gesture features; Cassell et al. (2001) make a similar argument using body posture. (Nakano et al., 2003) describes how head gestures and eye gaze relate to turn taking and dialogue grounding. All of the studies listed in this paragraph identify relevant correlations between non-verbal communication and linguistic phenomena, but none construct a predictive system that uses the non-verbal modalities to improve performance beyond a text-only system. Prosody has been shown to improve performance on several NLP problems, such as topic and sentence segmentation (e.g., (Shriberg et al., 2000; Kim et al., 2004)). The prosody literature demonstrates that non-verbal features can improve performance on a wide variety of NLP tasks. However, it also warns that performance is often quite sensitive, both to the representation of prosodic features, and how they are integrated with other linguistic features. 217 The literature on prosody would suggest parallels for gesture features, but little such work has been reported. (Chen et al., 2004) shows that gesture may improve sentence segmentation; however, in this study, the improvement afforded by gesture is not statistically significant, an</context>
</contexts>
<marker>Shriberg, Stolcke, Hakkani-Tur, Tur, 2000</marker>
<rawString>Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur, and Gokhan Tur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>