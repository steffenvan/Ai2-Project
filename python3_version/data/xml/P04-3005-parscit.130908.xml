<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032699">
<title confidence="0.986171">
Customizing Parallel Corpora at the Document Level
</title>
<author confidence="0.972327">
Monica ROGATI and Yiming YANG
</author>
<affiliation confidence="0.98504">
Computer Science Department, Carnegie Mellon University
</affiliation>
<address confidence="0.670961">
5000 Forbes Avenue
Pittsburgh, PA 15213
</address>
<email confidence="0.982162">
mrogati@cs.cmu.edu, yiming@cs.cmu.edu
</email>
<sectionHeader confidence="0.992154" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824045454545">
Recent research in cross-lingual
information retrieval (CLIR) established the
need for properly matching the parallel corpus
used for query translation to the target corpus.
We propose a document-level approach to
solving this problem: building a custom-made
parallel corpus by automatically assembling it
from documents taken from other parallel
corpora. Although the general idea can be
applied to any application that uses parallel
corpora, we present results for CLIR in the
medical domain. In order to extract the best-
matched documents from several parallel
corpora, we propose ranking individual
documents by using a length-normalized
Okapi-based similarity score between them and
the target corpus. This ranking allows us to
discard 50-90% of the training data, while
avoiding the performance drop caused by a
good but mismatched resource, and even
improving CLIR effectiveness by 4-7% when
compared to using all available training data.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884923076923">
Our recent research in cross-lingual information
retrieval (CLIR) established the need for properly
matching the parallel corpus used for query
translation to the target corpus (Rogati and Yang,
2004). In particular, we showed that using a
general purpose machine translation (MT) system
such as SYSTRAN, or a general purpose parallel
corpus - both of which perform very well for news
stories (Peters, 2003) - dramatically fails in the
medical domain. To explore solutions to this
problem, we used cosine similarity between
training and target corpora as respective weights
when building a translation model. This approach
treats a parallel corpus as a homogeneous entity, an
entity that is self-consistent in its domain and
document quality. In this paper, we propose that
instead of weighting entire resources, we can select
individual documents from these corpora in order
to build a parallel corpus that is tailor-made to fit a
specific target collection. To avoid confusion, it is
helpful to remember that in IR settings the true test
data are the queries, not the target documents. The
documents are available off-line and can be (and
usually are) used for training and system
development. In other words, by matching the
training corpora and the target documents we are
not using test data for training.
(Rogati and Yang, 2004) also discusses
indirectly related work, such as query translation
disambiguation and building domain-specific
language models for speech recognition. We are
not aware of any additional related work.
In addition to proposing individual documents
as the unit for building custom-made parallel
corpora, in this paper we start exploring the criteria
used for individual document selection by
examining the effect of ranking documents using
the length-normalized Okapi-based similarity score
between them and the target corpus.
</bodyText>
<sectionHeader confidence="0.993304" genericHeader="introduction">
2 Evaluation Data
</sectionHeader>
<subsectionHeader confidence="0.97409">
2.1 Medical Domain Corpus: Springer
</subsectionHeader>
<bodyText confidence="0.9998876">
The Springer corpus consists of 9640 documents
(titles plus abstracts of medical journal articles)
each in English and in German, with 25 queries in
both languages, and relevance judgments made by
native German speakers who are medical experts
and are fluent in English. We split this parallel
corpus into two subsets, and used the first subset
(4,688 documents) for training, and the remaining
subset (4,952 documents) as the test set in all our
experiments. This configuration allows us to
experiment with CLIR in both directions (EN-DE
and DE-EN). We applied an alignment algorithm
to the training documents, and obtained a sentence-
aligned parallel corpus with about 30K sentences
in each language.
</bodyText>
<subsectionHeader confidence="0.999426">
2.2 Training Corpora
</subsectionHeader>
<bodyText confidence="0.9995155">
In addition to Springer, we have used four other
English-German parallel corpora for training:
</bodyText>
<listItem confidence="0.93954405">
• NEWS is a collection of 59K sentence
aligned news stories, downloaded from the
web (1996-2000), and available at
http://www.isi.edu/~koehn/publications/de-
news/
• WAC is a small parallel corpus obtained by
mining the web (Nie et al., 2000), in no
particular domain
• EUROPARL is a parallel corpus provided
by (Koehn). Its documents are sentence
aligned European Parliament proceedings.
This is a large collection that has been
successfully used for CLEF, when the target
corpora were collections of news stories
(Rogati and Yang, 2003).
• MEDTITLE is an English-German parallel
corpus consisting of 549K paired titles of
medical journal articles. These titles were
gathered from the PubMed online database
(http://www.ncbi.nlm.nih.gov/PubMed/).
</listItem>
<tableCaption confidence="0.491542">
Table 1 presents a summary of the five training
corpora characteristics.
</tableCaption>
<table confidence="0.999832777777778">
Name Size (sent) Domain
NEWS 59K news
WAC 60K mixed
EUROPAR 665K politics
L
SPRINGE 30K medical
R
MEDTITL 550K medical
E
</table>
<tableCaption confidence="0.740723">
Table 1. Characteristics of Parallel Training
Corpora
</tableCaption>
<sectionHeader confidence="0.871389" genericHeader="method">
3 Selecting Documents from Parallel Corpora
</sectionHeader>
<bodyText confidence="0.999969838709678">
While selecting and weighing entire training
corpora is a problem already explored by (Rogati
and Yang, 2004), in this paper we focus on a lower
granularity level: individual documents in the
parallel corpora. We seek to construct a custom
parallel corpus, by choosing individual documents
which best match the testing collection. We
compute the similarity between the test collection
(in German or English) and each individual
document in the parallel corpora for that respective
language. We have a choice of similarity metrics,
but since this computation is simply retrieval with
a long query, we start with the Okapi model
(Robertson, 1993), as implemented by the Lemur
system (Olgivie and Callan, 2001). Although the
Okapi model takes into account average document
length, we compare it with its length-normalized
version, measuring per-word similarity. The two
measures are identified in the results section by
“Okapi” and “Normalized”.
Once the similarity is computed for each
document in the parallel corpora, only the top N
most similar documents are kept for training. They
are an approximation of the domain(s) of the test
collection. Selecting N has not been an issue for
this corpus (values between 10-75% were safe).
However, more generally, this parameter can be
tuned to a different test corpus as any other
parameter. Alternatively, the document score can
also be incorporated into the translation model,
eliminating the need for thresholding.
</bodyText>
<sectionHeader confidence="0.999038" genericHeader="method">
4 CLIR Method
</sectionHeader>
<bodyText confidence="0.99849125">
We used a corpus-based approach, similar to that
in (Rogati and Yang, 2003). Let L1 be the source
language and L2 be the target language. The cross-
lingual retrieval consists of the following steps:
</bodyText>
<listItem confidence="0.999032090909091">
1. Expanding a query in L1 using blind
feedback
2. Translating the query by taking the dot
product between the query vector (with
weights from step 1) and a translation
matrix obtained by calculating translation
probabilities or term-term similarity using
the parallel corpus.
3. Expanding the query in L2 using blind
feedback
4. Retrieving documents in L2
</listItem>
<bodyText confidence="0.9999141">
Here, blind feedback is the process of retrieving
documents and adding the terms of the top-ranking
documents to the query for expansion. We used
simplified Rocchio positive feedback as
implemented by Lemur (Olgivie and Callan, 2001).
For the results in this paper, we have used
Pointwise Mutual Information (PMI) instead of
IBM Model 1 (Brown et al., 1993), since (Rogati
and Yang, 2004) found it to be as effective on
Springer, but faster to compute.
</bodyText>
<sectionHeader confidence="0.802601" genericHeader="method">
5 Results and Discussion
5.1 Empirical Settings
</sectionHeader>
<bodyText confidence="0.999995375">
For the retrieval part of our system, we adapted
Lemur (Ogilvie and Callan, 2001) to allow the use
of weighted queries. Several parameters were
tuned, none of them on the test set. In our corpus-
based approach, the main parameters are those
used in query expansion based on pseudo-
relevance, i.e., the maximum number of documents
and the maximum number of words to be used, and
the relative weight of the expanded portion with
respect to the initial query. Since the Springer
training set is fairly small, setting aside a subset of
the data for parameter tuning was not desirable.
We instead chose parameter values that were stable
on the CLEF collection (Peters, 2003): 5 and 20 as
the maximum numbers of documents and words,
respectively. The relative weight of the expanded
portion with respect to the initial query was set to
0.5. The results were evaluated using mean
average precision (AvgP), a standard performance
measure for IR evaluations.
In the following sections, DE-EN refers to
retrieval where the query is in German and the
documents in English, while EN-DE refers to
retrieval in the opposite direction.
</bodyText>
<subsectionHeader confidence="0.999256">
5.2 Using the Parallel Corpora Separately
</subsectionHeader>
<bodyText confidence="0.995985833333333">
Can we simply choose a parallel corpus that
performed very well on news stories, hoping it is
robust across domains? Natural approaches also
include choosing the largest corpus available, or
using all corpora together. Figure 1 shows the
effect of these strategies.
</bodyText>
<figureCaption confidence="0.966459">
Figure 1. CLIR results on the Springer test set by
</figureCaption>
<sectionHeader confidence="0.931074" genericHeader="method">
AvgP. EN-DE DE-EN
</sectionHeader>
<bodyText confidence="0.984346">
using PMI with different training corpora.
We notice that choosing the largest collection
(EUROPARL), using all resources available
without weights (ALL), and even choosing a large
collection in the medical domain (MEDTITLE) are
all sub-optimal strategies.
Given these results, we believe that resource
selection and weighting is necessary. Thoroughly
exploring weighting strategies is beyond the scope
of this paper and it would involve collection size,
genre, and translation quality in addition to a
measure of domain match. Here, we start by
selecting individual documents that match the
domain of the test collection. We examine the
effect this choice has on domain-specific CLIR.
</bodyText>
<subsectionHeader confidence="0.9130105">
5.3 Using Okapi weights to build a custom
parallel corpus
</subsectionHeader>
<bodyText confidence="0.9998143">
Figures 2 and 3 compare the two document
selection strategies discussed in Section 3 to using
all available documents, and to the ideal (but not
truly optimal) situation where there exists a “best”
resource to choose and this collection is known. By
“best”, we mean one that can produce optimal
results on the test corpus, with respect to the given
metric In reality, the true “best” resource is
unknown: as seen above, many intuitive choices
for the best collection are not optimal.
</bodyText>
<figureCaption confidence="0.998749166666667">
Figure 2. CLIR DE-EN performance vs. Percent
of Parallel Documents Used. “Best Corpus” is
given by an oracle and is usually unknown.
Figure 3. CLIR EN-DE performance vs. Percent
of Parallel Documents Used. “Best Corpus” is
given by an oracle and is usually unknown
</figureCaption>
<figure confidence="0.996587366666667">
SPRINGER MEDTITLE WAC
NEWS EUROPARL ALL
Average Precision
45
60
55
50
40
1 10 100
Percent Used (log)
Okapi Normalized
All Corpora Best Corpus
Okapi Normalized
All Corpora Best Corpus
1 10 100
70
Average Precision
65
60
55
50
Percent Used (log)
70
60
50
40
30
20
10
0
</figure>
<bodyText confidence="0.999728142857143">
Notice that the normalized version performs better
and is more stable. Per-word similarity is, in this
case, important when the documents are used to
train translation scores: shorter parallel documents
are better when building the translation matrix. Our
strategy accounts for a 4-7% improvement over
using all resources with no weights, for both
retrieval directions. It is also very close to the
“oracle” condition, which chooses the best
collection in advance. More importantly, by using
this strategy we are avoiding the sharp
performance drop when using a mismatched,
although very good, resource (such as
EUROPARL).
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.99994075">
We are currently exploring weighting strategies
involving collection size, genre, and estimating
translation quality in addition to a measure of
domain match. Another question we are
examining is the granularity level used when
selecting resources, such as selection at the
document or cluster level.
Similarity and overlap between resources
themselves is also worth considering while
exploring tradeoffs between redundancy and noise.
We are also interested in how these approaches
would apply to other domains.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999958055555556">
We have examined the issue of selecting
appropriate training resources for cross-lingual
information retrieval. We have proposed and
evaluated a simple method for creating a
customized parallel corpus from other available
parallel corpora by matching the domain of the test
documents with that of individual parallel
documents. We noticed that choosing the largest
collection, using all resources available without
weights, and even choosing a large collection in
the medical domain are all sub-optimal strategies.
The techniques we have presented here are not
restricted to CLIR and can be applied to other
areas where parallel corpora are necessary, such as
statistical machine translation. The trained
translation matrix can also be reused and can be
converted to any of the formats required by such
applications.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998745">
We would like to thank Ralf Brown for collecting
the MEDTITLE and SPRINGER data.
This research is sponsored in part by the National
Science Foundation (NSF) under grant IIS-
9982226, and in part by the DOD under award
114008-N66001992891808. Any opinions and
conclusions in this paper are the authors’ and do
not necessarily reflect those of the sponsors.
</bodyText>
<sectionHeader confidence="0.998475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999500928571429">
Brown, P.F, Pietra, D., Pietra, D, Mercer, R.L. 1993.The
Mathematics of Statistical Machine Translation:
Parameter Estimation. In Computational Linguistics,
19:263-312
Koehn, P. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation. Draft,
Unpublished.
Nie, J. Y., Simard, M. and Foster, G.. 2000. Using
parallel web pages for multi-lingual IR. In C.
Peters(Ed.), Proceedings of the CLEF 2000 forum
Ogilvie, P. and Callan, J. 2001. Experiments using the
Lemur toolkit. In Proceedings of the Tenth Text
Retrieval Conference (TREC-10).
Peters, C. 2003. Results of the CLEF 2003 Cross-Language
System Evaluation Campaign. Working Notes for the
CLEF 2003 Workshop, 21-22 August, Trondheim,
Norway
Robertson, S.E. and all. 1993. Okapi at TREC. In The
First TREC Retrieval Conference, Gaithersburg, MD.
pp. 21-30
Rogati, M and Yang, Y. 2003. Multilingual Information
Retrieval using Open, Transparent Resources in
CLEF 2003 . In C. Peters (Ed.), Results of the
CLEF2003 cross-language evaluation forum
Rogati, M and Yang, Y. 2004. Resource Selection for
Domain Specific Cross-Lingual IR. In Proceedings of
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR&apos;04).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.999545">Customizing Parallel Corpora at the Document Level</title>
<author confidence="0.992226">Monica ROGATI</author>
<author confidence="0.992226">Yiming YANG</author>
<affiliation confidence="0.999958">Computer Science Department, Carnegie Mellon University</affiliation>
<address confidence="0.999841">5000 Forbes Avenue Pittsburgh, PA 15213</address>
<email confidence="0.999625">mrogati@cs.cmu.edu,yiming@cs.cmu.edu</email>
<abstract confidence="0.969437195020746">Recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus. We propose a document-level approach to solving this problem: building a custom-made parallel corpus by automatically assembling it from documents taken from other parallel corpora. Although the general idea can be applied to any application that uses parallel corpora, we present results for CLIR in the medical domain. In order to extract the bestmatched documents from several parallel corpora, we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus. This ranking allows us to discard 50-90% of the training data, while avoiding the performance drop caused by a good but mismatched resource, and even improving CLIR effectiveness by 4-7% when compared to using all available training data. Our recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus (Rogati and Yang, 2004). In particular, we showed that using a general purpose machine translation (MT) system such as SYSTRAN, or a general purpose parallel corpus both of which perform very well for news stories (Peters, 2003) dramatically fails in the medical domain. To explore solutions to this problem, we used cosine similarity between training and target corpora as respective weights when building a translation model. This approach treats a parallel corpus as a homogeneous entity, an entity that is self-consistent in its domain and document quality. In this paper, we propose that instead of weighting entire resources, we can select individual documents from these corpora in order to build a parallel corpus that is tailor-made to fit a specific target collection. To avoid confusion, it is to remember that in IR settings the true the queries, not the target documents. The documents are available off-line and can be (and usually are) used for training and system development. In other words, by matching the training corpora and the target documents we are not using test data for training. (Rogati and Yang, 2004) also discusses indirectly related work, such as query translation disambiguation and building domain-specific language models for speech recognition. We are not aware of any additional related work. In addition to proposing individual documents as the unit for building custom-made parallel corpora, in this paper we start exploring the criteria used for individual document selection by examining the effect of ranking documents using the length-normalized Okapi-based similarity score between them and the target corpus. Data Domain Corpus: Springer The Springer corpus consists of 9640 documents (titles plus abstracts of medical journal articles) each in English and in German, with 25 queries in both languages, and relevance judgments made by native German speakers who are medical experts and are fluent in English. We split this parallel corpus into two subsets, and used the first subset (4,688 documents) for training, and the remaining subset (4,952 documents) as the test set in all our experiments. This configuration allows us to experiment with CLIR in both directions (EN-DE and DE-EN). We applied an alignment algorithm to the training documents, and obtained a sentencealigned parallel corpus with about 30K sentences in each language. Corpora In addition to Springer, we have used four other English-German parallel corpora for training: • NEWS is a collection of 59K sentence aligned news stories, downloaded from the web (1996-2000), and available at http://www.isi.edu/~koehn/publications/denews/ • WAC is a small parallel corpus obtained by mining the web (Nie et al., 2000), in no particular domain • EUROPARL is a parallel corpus provided by (Koehn). Its documents are sentence aligned European Parliament proceedings. This is a large collection that has been successfully used for CLEF, when the target corpora were collections of news stories (Rogati and Yang, 2003). • MEDTITLE is an English-German parallel corpus consisting of 549K paired titles of medical journal articles. These titles were gathered from the PubMed online database (http://www.ncbi.nlm.nih.gov/PubMed/). Table 1 presents a summary of the five training corpora characteristics. Name Size (sent) Domain NEWS 59K news WAC 60K mixed EUROPAR 665K politics L SPRINGE 30K medical R MEDTITL 550K medical E Table 1. Characteristics of Parallel Training Corpora Documents from Parallel Corpora While selecting and weighing entire training corpora is a problem already explored by (Rogati and Yang, 2004), in this paper we focus on a lower granularity level: individual documents in the parallel corpora. We seek to construct a custom parallel corpus, by choosing individual documents which best match the testing collection. We compute the similarity between the test collection (in German or English) and each individual document in the parallel corpora for that respective language. We have a choice of similarity metrics, but since this computation is simply retrieval with a long query, we start with the Okapi model (Robertson, 1993), as implemented by the Lemur system (Olgivie and Callan, 2001). Although the Okapi model takes into account average document length, we compare it with its length-normalized version, measuring per-word similarity. The two measures are identified in the results section by “Okapi” and “Normalized”. Once the similarity is computed for each document in the parallel corpora, only the top N most similar documents are kept for training. They are an approximation of the domain(s) of the test collection. Selecting N has not been an issue for this corpus (values between 10-75% were safe). However, more generally, this parameter can be tuned to a different test corpus as any other parameter. Alternatively, the document score can also be incorporated into the translation model, eliminating the need for thresholding. Method We used a corpus-based approach, similar to that in (Rogati and Yang, 2003). Let L1 be the source language and L2 be the target language. The crosslingual retrieval consists of the following steps: 1. Expanding a query in L1 using blind feedback 2. Translating the query by taking the dot product between the query vector (with weights from step 1) and a translation matrix obtained by calculating translation probabilities or term-term similarity using the parallel corpus. 3. Expanding the query in L2 using blind feedback 4. Retrieving documents in L2 Here, blind feedback is the process of retrieving documents and adding the terms of the top-ranking documents to the query for expansion. We used simplified Rocchio positive feedback as implemented by Lemur (Olgivie and Callan, 2001). For the results in this paper, we have used Pointwise Mutual Information (PMI) instead of IBM Model 1 (Brown et al., 1993), since (Rogati and Yang, 2004) found it to be as effective on Springer, but faster to compute. and Discussion Settings For the retrieval part of our system, we adapted Lemur (Ogilvie and Callan, 2001) to allow the use of weighted queries. Several parameters were none of them on the test set. In our corpusbased approach, the main parameters are those used in query expansion based on pseudorelevance, i.e., the maximum number of documents and the maximum number of words to be used, and the relative weight of the expanded portion with respect to the initial query. Since the Springer training set is fairly small, setting aside a subset of the data for parameter tuning was not desirable. We instead chose parameter values that were stable on the CLEF collection (Peters, 2003): 5 and 20 as the maximum numbers of documents and words, respectively. The relative weight of the expanded portion with respect to the initial query was set to 0.5. The results were evaluated using mean average precision (AvgP), a standard performance measure for IR evaluations. In the following sections, DE-EN refers to retrieval where the query is in German and the documents in English, while EN-DE refers to retrieval in the opposite direction. the Parallel Corpora Separately Can we simply choose a parallel corpus that performed very well on news stories, hoping it is robust across domains? Natural approaches also include choosing the largest corpus available, or using all corpora together. Figure 1 shows the effect of these strategies. Figure 1. CLIR results on the Springer test set by DE-EN using PMI with different training corpora. We notice that choosing the largest collection (EUROPARL), using all resources available without weights (ALL), and even choosing a large collection in the medical domain (MEDTITLE) are all sub-optimal strategies. Given these results, we believe that resource selection and weighting is necessary. Thoroughly exploring weighting strategies is beyond the scope of this paper and it would involve collection size, genre, and translation quality in addition to a measure of domain match. Here, we start by selecting individual documents that match the domain of the test collection. We examine the effect this choice has on domain-specific CLIR. Okapi weights to build a custom parallel corpus Figures 2 and 3 compare the two document selection strategies discussed in Section 3 to using all available documents, and to the ideal (but not truly optimal) situation where there exists a “best” to choose this collection is By “best”, we mean one that can produce optimal results on the test corpus, with respect to the given metric In reality, the true “best” resource is unknown: as seen above, many intuitive choices for the best collection are not optimal. Figure 2. CLIR DE-EN performance vs. Percent of Parallel Documents Used. “Best Corpus” is given by an oracle and is usually unknown. Figure 3. CLIR EN-DE performance vs. Percent of Parallel Documents Used. “Best Corpus” is given by an oracle and is usually unknown</abstract>
<title confidence="0.241097333333333">SPRINGER MEDTITLE WAC NEWS EUROPARL ALL Average Precision</title>
<note confidence="0.796649222222222">45 60 55 50 40 1 10 100 Percent Used (log) Okapi Normalized All Corpora Best Corpus Okapi Normalized All Corpora Best Corpus 1 10 100 70 Average Precision 65 60 55 50 Percent Used (log) 70 60 50 40 30 20 10 0</note>
<abstract confidence="0.974192660377359">Notice that the normalized version performs better and is more stable. Per-word similarity is, in this case, important when the documents are used to train translation scores: shorter parallel documents are better when building the translation matrix. Our strategy accounts for a 4-7% improvement over using all resources with no weights, for both retrieval directions. It is also very close to the “oracle” condition, which chooses the best collection in advance. More importantly, by using this strategy we are avoiding the sharp performance drop when using a mismatched, although very good, resource (such as EUROPARL). Work We are currently exploring weighting strategies involving collection size, genre, and estimating translation quality in addition to a measure of domain match. Another question we are examining is the granularity level used when selecting resources, such as selection at the or cluster Similarity and overlap between resources themselves is also worth considering while exploring tradeoffs between redundancy and noise. We are also interested in how these approaches would apply to other domains. We have examined the issue of selecting appropriate training resources for cross-lingual information retrieval. We have proposed and evaluated a simple method for creating a customized parallel corpus from other available parallel corpora by matching the domain of the test documents with that of individual parallel documents. We noticed that choosing the largest collection, using all resources available without weights, and even choosing a large collection in the medical domain are all sub-optimal strategies. The techniques we have presented here are not restricted to CLIR and can be applied to other areas where parallel corpora are necessary, such as statistical machine translation. The trained translation matrix can also be reused and can be converted to any of the formats required by such applications. We would like to thank Ralf Brown for collecting the MEDTITLE and SPRINGER data. This research is sponsored in part by the National Science Foundation (NSF) under grant IIS- 9982226, and in part by the DOD under award 114008-N66001992891808. Any opinions and conclusions in this paper are the authors’ and do not necessarily reflect those of the sponsors.</abstract>
<note confidence="0.9789335">References Brown, P.F, Pietra, D., Pietra, D, Mercer, R.L. 1993.The</note>
<title confidence="0.894456">Mathematics of Statistical Machine Translation:</title>
<author confidence="0.869381">In</author>
<pubnum confidence="0.427883">19:263-312</pubnum>
<author confidence="0.887887">P Europarl A Multilingual Corpus for Koehn</author>
<affiliation confidence="0.8679015">Evaluation of Machine Translation. Draft, Unpublished.</affiliation>
<address confidence="0.789055">Nie, J. Y., Simard, M. and Foster, G.. 2000. Using</address>
<note confidence="0.84909">parallel web pages for multi-lingual IR. In C. of the CLEF 2000 forum Ogilvie, P. and Callan, J. 2001. Experiments using the toolkit. In of the Tenth Text Conference Peters, C. 2003. Results of the CLEF 2003 Cross-Language Evaluation Campaign. Notes for the 2003 21-22 August, Trondheim, Norway S.E. and all. 1993. Okapi at TREC. In TREC Retrieval Conference, MD. pp. 21-30 Rogati, M and Yang, Y. 2003. Multilingual Information Retrieval using Open, Transparent Resources in 2003 . In C. Peters (Ed.), of the CLEF2003 cross-language evaluation forum Rogati, M and Yang, Y. 2004. Resource Selection for Domain Specific Cross-Lingual IR. In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;04).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>P F Brown</author>
<author>D Pietra</author>
<author>D Pietra</author>
<author>R L Mercer</author>
</authors>
<booktitle>1993.The Mathematics of Statistical Machine Translation: Parameter Estimation. In Computational Linguistics,</booktitle>
<pages>19--263</pages>
<marker>Brown, Pietra, Pietra, Mercer, </marker>
<rawString>Brown, P.F, Pietra, D., Pietra, D, Mercer, R.L. 1993.The Mathematics of Statistical Machine Translation: Parameter Estimation. In Computational Linguistics, 19:263-312</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</title>
<tech>Draft, Unpublished.</tech>
<marker>Koehn, </marker>
<rawString>Koehn, P. Europarl: A Multilingual Corpus for Evaluation of Machine Translation. Draft, Unpublished.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Nie</author>
<author>M Simard</author>
<author>G Foster</author>
</authors>
<title>Using parallel web pages for multi-lingual IR. In</title>
<date>2000</date>
<booktitle>C. Peters(Ed.), Proceedings of the CLEF</booktitle>
<note>forum</note>
<contexts>
<context position="4154" citStr="Nie et al., 2000" startWordPosition="624" endWordPosition="627">l our experiments. This configuration allows us to experiment with CLIR in both directions (EN-DE and DE-EN). We applied an alignment algorithm to the training documents, and obtained a sentencealigned parallel corpus with about 30K sentences in each language. 2.2 Training Corpora In addition to Springer, we have used four other English-German parallel corpora for training: • NEWS is a collection of 59K sentence aligned news stories, downloaded from the web (1996-2000), and available at http://www.isi.edu/~koehn/publications/denews/ • WAC is a small parallel corpus obtained by mining the web (Nie et al., 2000), in no particular domain • EUROPARL is a parallel corpus provided by (Koehn). Its documents are sentence aligned European Parliament proceedings. This is a large collection that has been successfully used for CLEF, when the target corpora were collections of news stories (Rogati and Yang, 2003). • MEDTITLE is an English-German parallel corpus consisting of 549K paired titles of medical journal articles. These titles were gathered from the PubMed online database (http://www.ncbi.nlm.nih.gov/PubMed/). Table 1 presents a summary of the five training corpora characteristics. Name Size (sent) Doma</context>
</contexts>
<marker>Nie, Simard, Foster, 2000</marker>
<rawString>Nie, J. Y., Simard, M. and Foster, G.. 2000. Using parallel web pages for multi-lingual IR. In C. Peters(Ed.), Proceedings of the CLEF 2000 forum</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ogilvie</author>
<author>J Callan</author>
</authors>
<title>Experiments using the Lemur toolkit.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth Text Retrieval Conference (TREC-10).</booktitle>
<contexts>
<context position="7564" citStr="Ogilvie and Callan, 2001" startWordPosition="1159" endWordPosition="1162">ind feedback 4. Retrieving documents in L2 Here, blind feedback is the process of retrieving documents and adding the terms of the top-ranking documents to the query for expansion. We used simplified Rocchio positive feedback as implemented by Lemur (Olgivie and Callan, 2001). For the results in this paper, we have used Pointwise Mutual Information (PMI) instead of IBM Model 1 (Brown et al., 1993), since (Rogati and Yang, 2004) found it to be as effective on Springer, but faster to compute. 5 Results and Discussion 5.1 Empirical Settings For the retrieval part of our system, we adapted Lemur (Ogilvie and Callan, 2001) to allow the use of weighted queries. Several parameters were tuned, none of them on the test set. In our corpusbased approach, the main parameters are those used in query expansion based on pseudorelevance, i.e., the maximum number of documents and the maximum number of words to be used, and the relative weight of the expanded portion with respect to the initial query. Since the Springer training set is fairly small, setting aside a subset of the data for parameter tuning was not desirable. We instead chose parameter values that were stable on the CLEF collection (Peters, 2003): 5 and 20 as </context>
</contexts>
<marker>Ogilvie, Callan, 2001</marker>
<rawString>Ogilvie, P. and Callan, J. 2001. Experiments using the Lemur toolkit. In Proceedings of the Tenth Text Retrieval Conference (TREC-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Peters</author>
</authors>
<title>Results of the CLEF</title>
<date>2003</date>
<booktitle>Cross-Language System Evaluation Campaign. Working Notes for the CLEF 2003 Workshop,</booktitle>
<pages>21--22</pages>
<location>Trondheim, Norway</location>
<contexts>
<context position="1591" citStr="Peters, 2003" startWordPosition="230" endWordPosition="231">training data, while avoiding the performance drop caused by a good but mismatched resource, and even improving CLIR effectiveness by 4-7% when compared to using all available training data. 1 Introduction Our recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus (Rogati and Yang, 2004). In particular, we showed that using a general purpose machine translation (MT) system such as SYSTRAN, or a general purpose parallel corpus - both of which perform very well for news stories (Peters, 2003) - dramatically fails in the medical domain. To explore solutions to this problem, we used cosine similarity between training and target corpora as respective weights when building a translation model. This approach treats a parallel corpus as a homogeneous entity, an entity that is self-consistent in its domain and document quality. In this paper, we propose that instead of weighting entire resources, we can select individual documents from these corpora in order to build a parallel corpus that is tailor-made to fit a specific target collection. To avoid confusion, it is helpful to remember t</context>
<context position="8150" citStr="Peters, 2003" startWordPosition="1261" endWordPosition="1262"> (Ogilvie and Callan, 2001) to allow the use of weighted queries. Several parameters were tuned, none of them on the test set. In our corpusbased approach, the main parameters are those used in query expansion based on pseudorelevance, i.e., the maximum number of documents and the maximum number of words to be used, and the relative weight of the expanded portion with respect to the initial query. Since the Springer training set is fairly small, setting aside a subset of the data for parameter tuning was not desirable. We instead chose parameter values that were stable on the CLEF collection (Peters, 2003): 5 and 20 as the maximum numbers of documents and words, respectively. The relative weight of the expanded portion with respect to the initial query was set to 0.5. The results were evaluated using mean average precision (AvgP), a standard performance measure for IR evaluations. In the following sections, DE-EN refers to retrieval where the query is in German and the documents in English, while EN-DE refers to retrieval in the opposite direction. 5.2 Using the Parallel Corpora Separately Can we simply choose a parallel corpus that performed very well on news stories, hoping it is robust acros</context>
</contexts>
<marker>Peters, 2003</marker>
<rawString>Peters, C. 2003. Results of the CLEF 2003 Cross-Language System Evaluation Campaign. Working Notes for the CLEF 2003 Workshop, 21-22 August, Trondheim, Norway</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>all</author>
</authors>
<title>Okapi at TREC.</title>
<date>1993</date>
<booktitle>In The First TREC Retrieval Conference,</booktitle>
<pages>21--30</pages>
<location>Gaithersburg, MD.</location>
<marker>Robertson, all, 1993</marker>
<rawString>Robertson, S.E. and all. 1993. Okapi at TREC. In The First TREC Retrieval Conference, Gaithersburg, MD. pp. 21-30</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rogati</author>
<author>Y Yang</author>
</authors>
<title>Multilingual Information Retrieval using Open, Transparent Resources in CLEF</title>
<date>2003</date>
<contexts>
<context position="4450" citStr="Rogati and Yang, 2003" startWordPosition="670" endWordPosition="673">on to Springer, we have used four other English-German parallel corpora for training: • NEWS is a collection of 59K sentence aligned news stories, downloaded from the web (1996-2000), and available at http://www.isi.edu/~koehn/publications/denews/ • WAC is a small parallel corpus obtained by mining the web (Nie et al., 2000), in no particular domain • EUROPARL is a parallel corpus provided by (Koehn). Its documents are sentence aligned European Parliament proceedings. This is a large collection that has been successfully used for CLEF, when the target corpora were collections of news stories (Rogati and Yang, 2003). • MEDTITLE is an English-German parallel corpus consisting of 549K paired titles of medical journal articles. These titles were gathered from the PubMed online database (http://www.ncbi.nlm.nih.gov/PubMed/). Table 1 presents a summary of the five training corpora characteristics. Name Size (sent) Domain NEWS 59K news WAC 60K mixed EUROPAR 665K politics L SPRINGE 30K medical R MEDTITL 550K medical E Table 1. Characteristics of Parallel Training Corpora 3 Selecting Documents from Parallel Corpora While selecting and weighing entire training corpora is a problem already explored by (Rogati and </context>
<context position="6502" citStr="Rogati and Yang, 2003" startWordPosition="985" endWordPosition="988"> “Normalized”. Once the similarity is computed for each document in the parallel corpora, only the top N most similar documents are kept for training. They are an approximation of the domain(s) of the test collection. Selecting N has not been an issue for this corpus (values between 10-75% were safe). However, more generally, this parameter can be tuned to a different test corpus as any other parameter. Alternatively, the document score can also be incorporated into the translation model, eliminating the need for thresholding. 4 CLIR Method We used a corpus-based approach, similar to that in (Rogati and Yang, 2003). Let L1 be the source language and L2 be the target language. The crosslingual retrieval consists of the following steps: 1. Expanding a query in L1 using blind feedback 2. Translating the query by taking the dot product between the query vector (with weights from step 1) and a translation matrix obtained by calculating translation probabilities or term-term similarity using the parallel corpus. 3. Expanding the query in L2 using blind feedback 4. Retrieving documents in L2 Here, blind feedback is the process of retrieving documents and adding the terms of the top-ranking documents to the que</context>
</contexts>
<marker>Rogati, Yang, 2003</marker>
<rawString>Rogati, M and Yang, Y. 2003. Multilingual Information Retrieval using Open, Transparent Resources in CLEF 2003 . In C. Peters (Ed.), Results of the CLEF2003 cross-language evaluation forum</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rogati</author>
<author>Y Yang</author>
</authors>
<title>Resource Selection for Domain Specific Cross-Lingual IR.</title>
<date>2004</date>
<booktitle>In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;04).</booktitle>
<contexts>
<context position="1384" citStr="Rogati and Yang, 2004" startWordPosition="194" endWordPosition="197">ts from several parallel corpora, we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus. This ranking allows us to discard 50-90% of the training data, while avoiding the performance drop caused by a good but mismatched resource, and even improving CLIR effectiveness by 4-7% when compared to using all available training data. 1 Introduction Our recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus (Rogati and Yang, 2004). In particular, we showed that using a general purpose machine translation (MT) system such as SYSTRAN, or a general purpose parallel corpus - both of which perform very well for news stories (Peters, 2003) - dramatically fails in the medical domain. To explore solutions to this problem, we used cosine similarity between training and target corpora as respective weights when building a translation model. This approach treats a parallel corpus as a homogeneous entity, an entity that is self-consistent in its domain and document quality. In this paper, we propose that instead of weighting entir</context>
<context position="5061" citStr="Rogati and Yang, 2004" startWordPosition="759" endWordPosition="762">Yang, 2003). • MEDTITLE is an English-German parallel corpus consisting of 549K paired titles of medical journal articles. These titles were gathered from the PubMed online database (http://www.ncbi.nlm.nih.gov/PubMed/). Table 1 presents a summary of the five training corpora characteristics. Name Size (sent) Domain NEWS 59K news WAC 60K mixed EUROPAR 665K politics L SPRINGE 30K medical R MEDTITL 550K medical E Table 1. Characteristics of Parallel Training Corpora 3 Selecting Documents from Parallel Corpora While selecting and weighing entire training corpora is a problem already explored by (Rogati and Yang, 2004), in this paper we focus on a lower granularity level: individual documents in the parallel corpora. We seek to construct a custom parallel corpus, by choosing individual documents which best match the testing collection. We compute the similarity between the test collection (in German or English) and each individual document in the parallel corpora for that respective language. We have a choice of similarity metrics, but since this computation is simply retrieval with a long query, we start with the Okapi model (Robertson, 1993), as implemented by the Lemur system (Olgivie and Callan, 2001). </context>
<context position="7370" citStr="Rogati and Yang, 2004" startWordPosition="1126" endWordPosition="1129"> (with weights from step 1) and a translation matrix obtained by calculating translation probabilities or term-term similarity using the parallel corpus. 3. Expanding the query in L2 using blind feedback 4. Retrieving documents in L2 Here, blind feedback is the process of retrieving documents and adding the terms of the top-ranking documents to the query for expansion. We used simplified Rocchio positive feedback as implemented by Lemur (Olgivie and Callan, 2001). For the results in this paper, we have used Pointwise Mutual Information (PMI) instead of IBM Model 1 (Brown et al., 1993), since (Rogati and Yang, 2004) found it to be as effective on Springer, but faster to compute. 5 Results and Discussion 5.1 Empirical Settings For the retrieval part of our system, we adapted Lemur (Ogilvie and Callan, 2001) to allow the use of weighted queries. Several parameters were tuned, none of them on the test set. In our corpusbased approach, the main parameters are those used in query expansion based on pseudorelevance, i.e., the maximum number of documents and the maximum number of words to be used, and the relative weight of the expanded portion with respect to the initial query. Since the Springer training set </context>
</contexts>
<marker>Rogati, Yang, 2004</marker>
<rawString>Rogati, M and Yang, Y. 2004. Resource Selection for Domain Specific Cross-Lingual IR. In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;04).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>