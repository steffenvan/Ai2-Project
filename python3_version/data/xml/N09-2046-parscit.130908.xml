<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000501">
<title confidence="0.816867">
Improving SCL Model for Sentiment-Transfer Learning
</title>
<author confidence="0.970335">
Songbo Tan
</author>
<affiliation confidence="0.775046">
Institute of Computing Technology
Beijing, China
</affiliation>
<email confidence="0.979188">
tansongbo@software.ict.ac.cn
</email>
<sectionHeader confidence="0.988222" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999920333333333">
In recent years, Structural Correspondence
Learning (SCL) is becoming one of the most
promising techniques for sentiment-transfer
learning. However, SCL model treats each
feature as well as each instance by an
equivalent-weight strategy. To address the two
issues effectively, we proposed a weighted
SCL model (W-SCL), which weights the
features as well as the instances. More
specifically, W-SCL assigns a smaller weight
to high-frequency domain-specific (HFDS)
features and assigns a larger weight to
instances with the same label as the involved
pivot feature. The experimental results
indicate that proposed W-SCL model could
overcome the adverse influence of HFDS
features, and leverage knowledge from labels
of instances and pivot features.
</bodyText>
<sectionHeader confidence="0.998706" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998834909090909">
In the community of sentiment analysis (Turney
2002; Pang et al., 2002; Tang et al., 2009),
transferring a sentiment classifier from one source
domain to another target domain is still far from a
trivial work, because sentiment expression often
behaves with strong domain-specific nature.
Up to this time, many researchers have
proposed techniques to address this problem, such
as classifiers adaptation, generalizable features
detection and so on (DaumeIII et al., 2006; Jiang
et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan
et al., 2009). Among these techniques, SCL
(Structural Correspondence Learning) (Blitzer et
al., 2006) is regarded as a promising method to
tackle transfer-learning problem. The main idea
behind SCL model is to identify correspondences
among features from different domains by
modeling their correlations with pivot features (or
generalizable features). Pivot features behave
similarly in both domains. If non-pivot features
from different domains are correlated with many
of the same pivot features, then we assume them
</bodyText>
<page confidence="0.982687">
181
</page>
<author confidence="0.690852">
Xueqi Cheng
</author>
<affiliation confidence="0.6475325">
Institute of Computing Technology
Beijing, China
</affiliation>
<email confidence="0.698866">
cxq@ict.ac.cn
</email>
<bodyText confidence="0.999895166666666">
to be corresponded with each other, and treat them
similarly when training a sentiment classifier.
However, SCL model treats each feature as well
as each instance by an equivalent-weight strategy.
From the perspective of feature, this strategy fails
to overcome the adverse influence of high-
frequency domain-specific (HFDS) features. For
example, the words “stock” or “market” occurs
frequently in most of stock reviews, so these non-
sentiment features tend to have a strong
correspondence with pivot features. As a result,
the representative ability of the other sentiment
features will inevitably be weakened to some
degree.
To address this issue, we proposed Frequently
Exclusively-occurring Entropy (FEE) to pick out
HFDS features, and proposed a feature-weighted
SCL model (FW-SCL) to adjust the influence of
HFDS features in building correspondence. The
main idea of FW-SCL is to assign a smaller
weight to HFDS features so that the adverse
influence of HFDS features can be decreased.
From the other perspective, the equivalent-
weight strategy of SCL model ignores the labels
(“positive” or “negative”) of labeled instances.
Obviously, this is not a good idea. In fact, positive
pivot features tend to occur in positive instances,
so the correlations built on positive instances are
more reliable than that built on negative instances;
and vice versa. Consequently, utilization of labels
of instances and pivot features can decrease the
adverse influence of some co-occurrences, such as
co-occurrences involved with positive pivot
features and negative instances, or involved with
negative pivot features and positive instances.
In order to take into account the labels of
labeled instances, we proposed an instance-
weighted SCL model (IW-SCL), which assigns a
larger weight to instances with the same label as
the involved pivot feature. In this time, we obtain
a combined model: feature-weighted and instance-
weighted SCL model (FWIW-SCL). For the sake
</bodyText>
<subsubsectionHeader confidence="0.676427">
Proceedings of NAACL HLT 2009: Short Papers, pages 181–184,
</subsubsectionHeader>
<bodyText confidence="0.809274666666667">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
of convenience, we simplify “FWIW-SCL” as
“W-SCL” in the rest of this paper.
</bodyText>
<sectionHeader confidence="0.918274" genericHeader="method">
2 Structural Correspondence Learning
</sectionHeader>
<bodyText confidence="0.997950333333333">
In the section, we provide the detailed procedures
for SCL model.
First we need to pick out pivot features. Pivot
features occur frequently in both the source and
the target domain. In the community of sentiment
analysis, generalizable sentiment words are good
candidates for pivot features, such as “good” and
“excellent”. In the rest of this paper, we use K to
stand for the number of pivot features.
Second, we need to compute the pivot
predictors (or mapping vectors) using selected
pivot features. The pivot predictors are the key job,
because they directly decide the performance of
SCL. For each pivot feature k, we use a loss
function Lk,
</bodyText>
<equation confidence="0.9992385">
T
k =∑ k i
( ) 2
L i p (x )w xi 1
− +λ (1)
w
</equation>
<bodyText confidence="0.9998955">
where the function pk(xi) indicates whether the
pivot feature k occurs in the instance xi,
</bodyText>
<equation confidence="0.978979333333333">
⎧1 if x &gt; 0
ik
pk (xi)=⎨⎩−1 otherwise
</equation>
<bodyText confidence="0.999351857142857">
where the weight vector w encodes the
correspondence of the non-pivot features with the
pivot feature k (Blitzer et al., 2006).
Finally we use the augmented space [xT, xTW]T to
train the classifier on the source labeled data and
predict the examples on the target domain, where
W=[w1,w2, ..., wK].
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="method">
3 Feature-Weighted SCL Model
</sectionHeader>
<subsectionHeader confidence="0.999916">
3.1 Measure to pick out HFDS features
</subsectionHeader>
<bodyText confidence="0.999816857142857">
In order to pick out HFDS features, we proposed
Frequently Exclusively-occurring Entropy (FEE).
Our measure includes two criteria: occur in one
domain as frequently as possible, while occur on
another domain as rarely as possible. To satisfy
this requirement, we proposed the following
formula:
</bodyText>
<equation confidence="0.961567111111111">
⎛ max ( ), ( )
(P w P w ) ⎞
o n
= log max ( ), ( ) log
( (P w P w )) + ⎜
o n ) ⎟
⎝ min ( ), ( )
(P w P w
o n ⎠
</equation>
<bodyText confidence="0.980654333333333">
where Po(w) and Pn(w) indicate the probability of
word w in the source domain and the target
domain respectively:
</bodyText>
<equation confidence="0.955615">
( N w
( ) + α )
P w o
( ) = (3)
o ( α )
N + ⋅
2
o
( N w
( )+α)
P w n
( ) = (4)
n (Nn+2⋅α)
</equation>
<bodyText confidence="0.998643235294118">
where No(w) and Nn(w) is the number of examples
with word w in the source domain and the target
domain respectively; No and Nn is the number of
examples in the source domain and the target
domain respectively. In order to overcome
overflow, we set α=0.0001 in our experiment
reported in section 5.
To better understand this measure, let’s take a
simple example (see Table 1). Given a source
dataset with 1000 documents and a target dataset
with 1000 documents, 12 candidate features, and a
task to pick out 2 HFDS features. According to
our understanding, the best choice is to pick out
w4 and w8. According to formula (2), fortunately,
we successfully pick out w4, and w8. This simple
example validates the effectiveness of proposed
FEE formula.
</bodyText>
<tableCaption confidence="0.999682">
Table 1: A simple example for FEE
</tableCaption>
<table confidence="0.999789428571429">
Words No(w) Nn(w) FEE
Score Rank
w1 100 100 -2.3025 6
w2 100 90 -2.1971 4
w3 100 45 -1.5040 3
W4 100 4 0.9163 1
w5 50 50 -2.9956 8
w6 50 45 -2.8903 7
w7 50 23 -2.2192 5
W8 50 2 0.2231 2
w9 4 4 -5.5214 11
w10 4 3 -5.2337 10
w11 4 2 -4.8283 9
w12 1 1 -6.9077 12
</table>
<subsectionHeader confidence="0.971953">
3.2 Feature-Weighted SCL model
</subsectionHeader>
<bodyText confidence="0.997934">
To adjust the influence of HFDS features in
building correspondence, we proposed feature-
weighted SCL model (FW-SCL),
</bodyText>
<equation confidence="0.999116666666667">
L i p (x ) w x 1
( − + λ (5)
) 2
w
k = ∑ ∑ δ
k i l l l il
</equation>
<bodyText confidence="0.9999245">
where the function pk(xi) indicates whether the
pivot feature k occurs in the instance xi;
</bodyText>
<equation confidence="0.9973354">
1 if x &gt; 0
ik
⎨⎧
( )
x = ,
</equation>
<bodyText confidence="0.85343325">
i ⎩ −1 otherwise
an
d δl is the parameter to control the weight of the
HFDS feature l,
</bodyText>
<figure confidence="0.9714278">
fw
(2)
pk
182
⎧
=
1
δ
l
η
⎨
⎩
ifl ZHFDS
∈
otherwise
</figure>
<bodyText confidence="0.999892">
where ZHFDS indicates the HFDS feature set and r1
is located in the range [0,1]. When “r1=0”, it
indicates that no HFDS features are used to build
the correspondence vectors; while “r1=1” indicates
that all features are equally used to build the
correspondence vectors, that is to say, proposed
FW-SCL algorithm is simplified as traditional
SCL algorithm. Consequently, proposed FW-SCL
algorithm could be regarded as a generalized
version of traditional SCL algorithm.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="method">
4 Instance-Weighted SCL Model
</sectionHeader>
<bodyText confidence="0.999864181818182">
The traditional SCL model does not take into
account the labels (“positive” or “negative”) of
instances on the source domain and pivot features.
Although the labels of pivot features are not given
at first, it is very easy to obtain these labels
because the number of pivot features is typically
very small.
Obviously, positive pivot features tend to occur
in positive instances, so the correlations built on
positive instances are more reliable than the
correlations built on negative instances; and vice
versa. As a result, the ideal choice is to assign a
larger weight to the instances with the same label
as the involved pivot feature, while assign a
smaller weight to the instances with the different
label as the involved pivot feature. This strategy
can make correlations more reliable. This is the
key idea of instance-weighted SCL model (IW-
SCL). Combining the idea of feature-weighted
SCL model (FW-SCL), we obtain the feature-
weighted and instance-weighted SCL model
(FWIW-SCL),
</bodyText>
<equation confidence="0.999167222222222">
(1
)
(1
(
(k),
(x
))
pk(x
)
−γ
⋅∑
−ρ
ψ
ψ
j
)(
j
∑lδlwlxjl−1)
</equation>
<bodyText confidence="0.7885695">
indicates whether the pivot feature k occurs
in the instan
</bodyText>
<equation confidence="0.9860972">
pk(xi)
ce xi;
1 if x &gt; 0
ik
−1 otherwise
</equation>
<bodyText confidence="0.9724365">
where ZHFDS indicates the HFDS feature set and r1
is located in the range [0,1].
In equation (6), the function p(z,y) indicates
whether the two variables z and y have the same
non-zero value,
if z y and z 0
</bodyText>
<equation confidence="0.9360335">
= ≠
0 otherwise
</equation>
<bodyText confidence="0.7870922">
ables are either pivot features or instances,
For the sake of convenience, we simplify
“FWIW-SCL” as “W-SCL”.
(6)
where y is the instance weight and the function
an
d 8l is the parameter to control the weight of the
HFDS feature l,
and the function yr(z) is a hinge function, whose
vari
</bodyText>
<sectionHeader confidence="0.988416" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.880869">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9999840625">
We collected three Chinese domain-specific
datasets: Education Reviews (Edu, from
http://blog.sohu.com/learning/), Stock Reviews (Sto,
from http://blog.sohu.com/stock/) and Computer
Reviews (Comp, from http://detail.zol.com.cn/). All of
these datasets are annotated by three linguists. We
use ICTCLAS (a Chinese text POS tool,
http://ictclas.org/) to parse Chinese words.
The dataset Edu includes 1,012 negative
reviews and 254 positive reviews. The average
size of reviews is about 600 words. The dataset
Sto consists of 683 negative reviews and 364
positive reviews. The average length of reviews is
about 460 terms. The dataset Comp contains 390
negative reviews and 544 positive reviews. The
average length of reviews is about 120 words.
</bodyText>
<subsectionHeader confidence="0.983">
5.2 Comparison Methods
</subsectionHeader>
<bodyText confidence="0.9998895">
In our experiments, we run one supervised
baseline, i.e., Naïve Bayes (NB), which only uses
one source-domain labeled data as training data.
For transfer-learning baseline, we implement
traditional SCL model (T-SCL) (Blitzer et al.,
2006). Like TSVM, it makes use of the source-
domain labeled data as well as the target-domain
unlabeled data.
</bodyText>
<subsectionHeader confidence="0.996998">
5.3 Does proposed method work?
</subsectionHeader>
<bodyText confidence="0.999925833333333">
To conduct our experiments, we use source-
domain data as unlabeled set or labeled training
set, and use target-domain data as unlabeled set or
testing set. Note that we use 100 manual-
annotated pivot features for T-SCL, FW-SCL and
W-SCL in the following experiments. We select
</bodyText>
<figure confidence="0.961398815789473">
1 if z has a positive label
ψ = 0 unknown .
if z has a negative label
⎧
⎪⎨
( )
z
⎪⎩−
1
= ⋅
γ ρ ψ ψ
∑ ( ( ) ) ( ( )
( ), ∑ δ − 1 )
l l l il
2
L k x p x w x + λ w
k i k i
+
⎧
⎨
⎩
=
pk
(xi)
⎧
if l
η
ZHFDS
∈δ⎨ ,
l = 1
⎩ otherwise
⎧
⎨
⎩
ρ ( )
z,y =
1
;
</figure>
<page confidence="0.99709">
183
</page>
<bodyText confidence="0.999950806451613">
pivot features use three criteria: a) is a sentiment
word; b) occurs frequently in both domains; c) has
similar occurring probability. For T-SCL, FW-
SCL and W-SCL, we use prototype classifier
(Sebastiani, 2002) to train the final model.
Table 2 shows the results of experiments
comparing proposed method with supervised
learning, transductive learning and T-SCL. For
FW-SCL, the ZHFDS is set to 200 and η is set to 0.1;
For W-SCL, the ZHFDS is set to 200, η is set to 0.1,
and γ is set to 0.9.
As expected, proposed method FW-SCL does
indeed provide much better performance than
supervised baselines, TSVM and T-SCL model.
For example, the average accuracy of FW-SCL
beats supervised baselines by about 12 percents,
beats TSVM by about 11 percents and beats T-
SCL by about 10 percents. This result indicates
that proposed FW-SCL model could overcome the
shortcomings of HFDS features in building
correspondence vectors.
More surprisingly, instance-weighting strategy
can further boost the performance of FW-SCL by
about 4 percents. This result indicates that the
labels of instances and pivot features are very
useful in building the correlation vectors. This
result also verifies our analysis in section 4:
positive pivot features tend to occur in positive
instances, so the correlations built on positive
instances are more reliable than the correlations
built on negative instances, and vice versa.
</bodyText>
<tableCaption confidence="0.99779">
Table 2: Accuracy of different methods
</tableCaption>
<table confidence="0.838403">
NB T-SCL FW-SCL W-SCL
Edu-&gt;Sto 0.6704 0.7965 0.7917 0.8108
Edu-&gt;Comp 0.5085 0.8019 0.8993 0.9025
Sto-&gt;Edu 0.6824 0.7712 0.9072 0.9368
Sto-&gt;Comp 0.5053 0.8126 0.8126 0.8693
Comp-&gt;Sto 0.6580 0.6523 0.7010 0.7717
Comp-&gt;Edu 0.6114 0.5976 0.9112 0.9408
Average 0.6060 0.7387 0.8372 0.8720
</table>
<bodyText confidence="0.999106888888889">
Although SCL is a method designed for transfer
learning, but it cannot provide better performance
than TSVM. This result verifies the analysis in
section 3: a small amount of HFDS features
occupy a large amount of weight in classification
model, but hardly carry corresponding sentiment.
In another word, very few top-frequency words
degrade the representative ability of SCL model
for sentiment classification.
</bodyText>
<sectionHeader confidence="0.996338" genericHeader="conclusions">
6 Conclusion Remarks
</sectionHeader>
<bodyText confidence="0.999955818181818">
In this paper, we proposed a weighted SCL
model (W-SCL) for domain adaptation in the
context of sentiment analysis. On six domain-
transfer tasks, W-SCL consistently produces much
better performance than the supervised, semi-
supervised and transfer-learning baselines. As a
result, we can say that proposed W-SCL model
offers a better choice for sentiment-analysis
applications that require high-precision
classification but hardly have any labeled training
data.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999190333333333">
This work was mainly supported by two funds,
i.e., 0704021000 and 60803085, and one another
project, i.e., 2004CB318109.
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998423">
Blitzer, J. and McDonald, R. and Fernando Pereira.
Domain adaptation with structural correspondence
learning. EMNLP 2006.
DaumeIII, H. and Marcu, D. Domain adaptation for
statistical classifiers. Journal of Artificial
Intelligence Research, 2006, 26: 101-126.
Jiang, J., Zhai, C. A Two-Stage Approach to Domain
Adaptation for Statistical Classifiers. CIKM 2007.
Pang, B., Lee, L. and Vaithyanathan, S. Thumbs up?
Sentiment classification using machine learning
techniques. EMNLP 2002.
Sebastiani, F. Machine learning in automated text
categorization. ACM Computing Surveys. 2002,
34(1): 1-47.
S. Tan, G. Wu, H. Tang and X. Cheng. A novel
scheme for domain-transfer problem in the context
of sentiment analysis. CIKM 2007.
S. Tan, Y. Wang, G. Wu and X. Cheng. Using
unlabeled data to handle domain-transfer problem of
semantic detection. SAC 2008.
S. Tan, X. Cheng, Y. Wang and H. Xu. Adapting
Naive Bayes to Domain Adaptation for Sentiment
Analysis. ECIR 2009.
H. Tang, S. Tan, and X. Cheng. A Survey on
Sentiment Detection of Reviews. Expert Systems
with Applications. Elsevier. 2009,
doi:10.1016/j.eswa.2009.02.063.
Turney, P. D. Thumbs Up or Thumbs Down? Semantic
Orientation Applied to Unsupervised Classification
of Reviews. ACL 2002.
</reference>
<page confidence="0.998706">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943015">
<title confidence="0.99911">Improving SCL Model for Sentiment-Transfer Learning</title>
<author confidence="0.994555">Songbo Tan</author>
<affiliation confidence="0.999988">Institute of Computing Technology</affiliation>
<address confidence="0.993798">Beijing, China</address>
<email confidence="0.963209">tansongbo@software.ict.ac.cn</email>
<abstract confidence="0.999446421052631">In recent years, Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning. However, SCL model treats each feature as well as each instance by an equivalent-weight strategy. To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances. More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature. The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning. EMNLP</title>
<date>2006</date>
<contexts>
<context position="1546" citStr="Blitzer et al., 2006" startWordPosition="221" endWordPosition="224">ity of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem. The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features). Pivot features behave similarly in both domains. If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them 181 Xueqi Cheng Institute of Computing Technology Beijing, China cxq@ict.ac.cn to be corresponded with each other, and treat them similarly when training a sentiment classifier. How</context>
<context position="5202" citStr="Blitzer et al., 2006" startWordPosition="799" endWordPosition="802"> the rest of this paper, we use K to stand for the number of pivot features. Second, we need to compute the pivot predictors (or mapping vectors) using selected pivot features. The pivot predictors are the key job, because they directly decide the performance of SCL. For each pivot feature k, we use a loss function Lk, T k =∑ k i ( ) 2 L i p (x )w xi 1 − +λ (1) w where the function pk(xi) indicates whether the pivot feature k occurs in the instance xi, ⎧1 if x &gt; 0 ik pk (xi)=⎨⎩−1 otherwise where the weight vector w encodes the correspondence of the non-pivot features with the pivot feature k (Blitzer et al., 2006). Finally we use the augmented space [xT, xTW]T to train the classifier on the source labeled data and predict the examples on the target domain, where W=[w1,w2, ..., wK]. 3 Feature-Weighted SCL Model 3.1 Measure to pick out HFDS features In order to pick out HFDS features, we proposed Frequently Exclusively-occurring Entropy (FEE). Our measure includes two criteria: occur in one domain as frequently as possible, while occur on another domain as rarely as possible. To satisfy this requirement, we proposed the following formula: ⎛ max ( ), ( ) (P w P w ) ⎞ o n = log max ( ), ( ) log ( (P w P w </context>
<context position="10752" citStr="Blitzer et al., 2006" startWordPosition="1816" endWordPosition="1819">t Edu includes 1,012 negative reviews and 254 positive reviews. The average size of reviews is about 600 words. The dataset Sto consists of 683 negative reviews and 364 positive reviews. The average length of reviews is about 460 terms. The dataset Comp contains 390 negative reviews and 544 positive reviews. The average length of reviews is about 120 words. 5.2 Comparison Methods In our experiments, we run one supervised baseline, i.e., Naïve Bayes (NB), which only uses one source-domain labeled data as training data. For transfer-learning baseline, we implement traditional SCL model (T-SCL) (Blitzer et al., 2006). Like TSVM, it makes use of the sourcedomain labeled data as well as the target-domain unlabeled data. 5.3 Does proposed method work? To conduct our experiments, we use sourcedomain data as unlabeled set or labeled training set, and use target-domain data as unlabeled set or testing set. Note that we use 100 manualannotated pivot features for T-SCL, FW-SCL and W-SCL in the following experiments. We select 1 if z has a positive label ψ = 0 unknown . if z has a negative label ⎧ ⎪⎨ ( ) z ⎪⎩− 1 = ⋅ γ ρ ψ ψ ∑ ( ( ) ) ( ( ) ( ), ∑ δ − 1 ) l l l il 2 L k x p x w x + λ w k i k i + ⎧ ⎨ ⎩ = pk (xi) ⎧ i</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>Blitzer, J. and McDonald, R. and Fernando Pereira. Domain adaptation with structural correspondence learning. EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H DaumeIII</author>
<author>D Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<pages>101--126</pages>
<marker>DaumeIII, Marcu, 2006</marker>
<rawString>DaumeIII, H. and Marcu, D. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 2006, 26: 101-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>C Zhai</author>
</authors>
<title>A Two-Stage Approach to Domain Adaptation for Statistical Classifiers. CIKM</title>
<date>2007</date>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jiang, J., Zhai, C. A Two-Stage Approach to Domain Adaptation for Statistical Classifiers. CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<journal>EMNLP</journal>
<contexts>
<context position="982" citStr="Pang et al., 2002" startWordPosition="135" endWordPosition="138">alent-weight strategy. To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances. More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature. The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features. 1 Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method t</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, B., Lee, L. and Vaithyanathan, S. Thumbs up? Sentiment classification using machine learning techniques. EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys.</journal>
<volume>34</volume>
<issue>1</issue>
<pages>1--47</pages>
<contexts>
<context position="11623" citStr="Sebastiani, 2002" startWordPosition="2015" endWordPosition="2016">ta as unlabeled set or testing set. Note that we use 100 manualannotated pivot features for T-SCL, FW-SCL and W-SCL in the following experiments. We select 1 if z has a positive label ψ = 0 unknown . if z has a negative label ⎧ ⎪⎨ ( ) z ⎪⎩− 1 = ⋅ γ ρ ψ ψ ∑ ( ( ) ) ( ( ) ( ), ∑ δ − 1 ) l l l il 2 L k x p x w x + λ w k i k i + ⎧ ⎨ ⎩ = pk (xi) ⎧ if l η ZHFDS ∈δ⎨ , l = 1 ⎩ otherwise ⎧ ⎨ ⎩ ρ ( ) z,y = 1 ; 183 pivot features use three criteria: a) is a sentiment word; b) occurs frequently in both domains; c) has similar occurring probability. For T-SCL, FWSCL and W-SCL, we use prototype classifier (Sebastiani, 2002) to train the final model. Table 2 shows the results of experiments comparing proposed method with supervised learning, transductive learning and T-SCL. For FW-SCL, the ZHFDS is set to 200 and η is set to 0.1; For W-SCL, the ZHFDS is set to 200, η is set to 0.1, and γ is set to 0.9. As expected, proposed method FW-SCL does indeed provide much better performance than supervised baselines, TSVM and T-SCL model. For example, the average accuracy of FW-SCL beats supervised baselines by about 12 percents, beats TSVM by about 11 percents and beats TSCL by about 10 percents. This result indicates tha</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Sebastiani, F. Machine learning in automated text categorization. ACM Computing Surveys. 2002, 34(1): 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>G Wu</author>
<author>H Tang</author>
<author>X Cheng</author>
</authors>
<title>A novel scheme for domain-transfer problem in the context of sentiment analysis.</title>
<date>2007</date>
<journal>CIKM</journal>
<contexts>
<context position="1420" citStr="Tan et al., 2007" startWordPosition="202" endWordPosition="205">luence of HFDS features, and leverage knowledge from labels of instances and pivot features. 1 Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem. The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features). Pivot features behave similarly in both domains. If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them 181 Xueqi Cheng Institute of Computing Technology Beiji</context>
</contexts>
<marker>Tan, Wu, Tang, Cheng, 2007</marker>
<rawString>S. Tan, G. Wu, H. Tang and X. Cheng. A novel scheme for domain-transfer problem in the context of sentiment analysis. CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>Y Wang</author>
<author>G Wu</author>
<author>X Cheng</author>
</authors>
<title>Using unlabeled data to handle domain-transfer problem of semantic detection.</title>
<date>2008</date>
<publisher>SAC</publisher>
<contexts>
<context position="1438" citStr="Tan et al., 2008" startWordPosition="206" endWordPosition="209">tures, and leverage knowledge from labels of instances and pivot features. 1 Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem. The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features). Pivot features behave similarly in both domains. If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them 181 Xueqi Cheng Institute of Computing Technology Beijing, China cxq@ict.</context>
</contexts>
<marker>Tan, Wang, Wu, Cheng, 2008</marker>
<rawString>S. Tan, Y. Wang, G. Wu and X. Cheng. Using unlabeled data to handle domain-transfer problem of semantic detection. SAC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>X Cheng</author>
<author>Y Wang</author>
<author>H Xu</author>
</authors>
<title>Adapting Naive Bayes to Domain Adaptation for Sentiment Analysis. ECIR</title>
<date>2009</date>
<contexts>
<context position="1457" citStr="Tan et al., 2009" startWordPosition="210" endWordPosition="213">e knowledge from labels of instances and pivot features. 1 Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem. The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features). Pivot features behave similarly in both domains. If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them 181 Xueqi Cheng Institute of Computing Technology Beijing, China cxq@ict.ac.cn to be corresp</context>
</contexts>
<marker>Tan, Cheng, Wang, Xu, 2009</marker>
<rawString>S. Tan, X. Cheng, Y. Wang and H. Xu. Adapting Naive Bayes to Domain Adaptation for Sentiment Analysis. ECIR 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tang</author>
<author>S Tan</author>
<author>X Cheng</author>
</authors>
<title>A Survey on Sentiment Detection of Reviews. Expert Systems with Applications.</title>
<date>2009</date>
<pages>10--1016</pages>
<publisher>Elsevier.</publisher>
<contexts>
<context position="1002" citStr="Tang et al., 2009" startWordPosition="139" endWordPosition="142">gy. To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances. More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature. The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features. 1 Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-le</context>
</contexts>
<marker>Tang, Tan, Cheng, 2009</marker>
<rawString>H. Tang, S. Tan, and X. Cheng. A Survey on Sentiment Detection of Reviews. Expert Systems with Applications. Elsevier. 2009, doi:10.1016/j.eswa.2009.02.063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<publisher>ACL</publisher>
<contexts>
<context position="963" citStr="Turney 2002" startWordPosition="133" endWordPosition="134">e by an equivalent-weight strategy. To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances. More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature. The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features. 1 Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature. Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009). Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, P. D. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. ACL 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>