<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.998054">
A Probabilistic Earley Parser as a Psycholinguistic Model
</title>
<author confidence="0.996405">
John Hale
</author>
<affiliation confidence="0.776354333333333">
Department of Cognitive Science
The Johns Hopkins University
3400 North Charles Street; Baltimore MD 21218-2685
</affiliation>
<email confidence="0.998237">
hale@cogsci.jhu.edu
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998464375">
In human sentence processing, cognitive load can be
defined many ways. This report considers a defini-
tion of cognitive load in terms of the total probability
of structural options that have been disconfirmed at
some point in a sentence: the surprisal of word wi
given its prefix w0...i−1 on a phrase-structural lan-
guage model. These loads can be efficiently calcu-
lated using a probabilistic Earley parser (Stolcke,
1995) which is interpreted as generating predictions
about reading time on a word-by-word basis. Un-
der grammatical assumptions supported by corpus-
frequency data, the operation of Stolcke’s probabilis-
tic Earley parser correctly predicts processing phe-
nomena associated with garden path structural am-
biguity and with the subject/object relative asym-
metry.
</bodyText>
<sectionHeader confidence="0.979022" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.986571142857143">
What is the relation between a person’s knowledge of
grammar and that same person’s application of that
knowledge in perceiving syntactic structure? The
answer to be proposed here observes three principles.
Principle 1 The relation between the parser and
grammar is one of strong competence.
Strong competence holds that the human sentence
processing mechanism directly uses rules of gram-
mar in its operation, and that a bare minimum of
extragrammatical machinery is necessary. This hy-
pothesis, originally proposed by Chomsky (Chom-
sky, 1965, page 9) has been pursued by many re-
searchers (Bresnan, 1982) (Stabler, 1991) (Steed-
man, 1992) (Shieber and Johnson, 1993), and stands
in contrast with an approach directed towards the
discovery of autonomous principles unique to the
processing mechanism.
Principle 2 Frequency affects performance.
The explanatory success of neural network and
constraint-based lexicalist theories (McClelland and
St. John, 1989) (MacDonald et al., 1994) (Taboret
al., 1997) suggests a statistical theory of language
performance. The present work adopts a numerical
view of competition in grammar that is grounded in
probability.
Principle 3 Sentence processing is eager.
“Eager” in this sense means the experimental situa-
tions to be modeled are ones like self-paced reading
in which sentence comprehenders are unrushed and
no information is ignored at a point at which it could
be used.
The proposal is that a person’s difficulty per-
ceiving syntactic structure be modeled by word-to-
word surprisal (Attneave, 1959, page 6) which can
be directly computed from a probabilistic phrase-
structure grammar. The approach taken here uses
a parsing algorithm developed by Stolcke. In the
course of explaining the algorithm at a very high
level I will indicate how the algorithm, interpreted
as a psycholinguistic model, observes each principle.
After that will come some simulation results, and
then a conclusion.
</bodyText>
<sectionHeader confidence="0.97624" genericHeader="method">
1 Language models
</sectionHeader>
<bodyText confidence="0.999956357142857">
Stolcke’s parsing algorithm was initially applied as a
component of an automatic speech recognition sys-
tem. In speech recognition, one is often interested
in the probability that some word will follow, given
that a sequence of words has been seen. Given some
lexicon of all possible words, a language model as-
signs a probability to every string of words from
the lexicon. This defines a probabilistic language
(Grenander, 1967) (Booth and Thompson, 1973)
(Soule, 1974) (Wetherell, 1980).
A language model helps a speech recognizer focus
its attention on words that are likely continuations
of what it has recognized so far. This is typically
done using conditional probabilities of the form
</bodyText>
<equation confidence="0.699527">
P (Wn = wn|W1 = w1,... Wn−1 = wn−1)
</equation>
<bodyText confidence="0.99856716">
the probability that the nth word will actually be
wn given that the words leading up to the nth have
been w1, w2, ... wn−1. Given some finite lexicon, the
probability of each possible outcome for Wn can be
estimated using that outcome’s relative frequency in
a sample.
Traditional language models used for speech are n-
gram models, in which n − 1 words of history serve
as the basis for predicting the nth word. Such mod-
els do not have any notion of hierarchical syntactic
structure, except as might be visible through an n-
word window.
Aware that the n-gram obscures many
linguistically-significant distinctions (Chomsky,
1956, section 2.3), many speech researchers (Jelinek
and Lafferty, 1991) sought to incorporate hierar-
chical phrase structure into language modeling (see
(Stolcke, 1997)) although it was not until the late
1990s that such models were able to significantly
improve on 3-grams (Chelba and Jelinek, 1998).
Stolcke’s probabilistic Earley parser is one way
to use hierarchical phrase structure in a language
model. The grammar it parses is a probabilistic
context-free phrase structure grammar (PCFG),
e.g.
</bodyText>
<equation confidence="0.3354406">
1.0 S → NP VP
0.5 NP → Det N
0.5 NP → NP VP
... ...
see (Charniak, 1993, chapter 5)
</equation>
<bodyText confidence="0.999380419354839">
Such a grammar defines a probabilistic language in
terms of a stochastic process that rewrites strings of
grammar symbols according to the probabilities on
the rules. Then each sentence in the language of the
grammar has a probability equal to the product of
the probabilities of all the rules used to generate it.
This multiplication embodies the assumption that
rule choices are independent. Sentences with more
than one derivation accumulate the probability of all
derivations that generate them. Through recursion,
infinite languages can be specified; an important
mathematical question in this context is whether or
not such a grammar is consistent – whether it assigns
some probability to infinite derivations, or whether
all derivations are guaranteed to terminate.
Even if a PCFG is consistent, it would appear to
have another drawback: it only assigns probabili-
ties to complete sentences of its language. This is as
inconvenient for speech recognition as it is for mod-
eling reading times.
Stolcke’s algorithm solves this problem by com-
puting, at each word of an input string, the prefix
probability. This is the sum of the probabilities of all
derivations whose yield is compatible with the string
seen so far. If the grammar is consistent (the proba-
bilities of all derivations sum to 1.0) then subtracting
the prefix probability from 1.0 gives the total proba-
bility of all the analyses the parser has disconfirmed.
If the human parser is eager, then the “work” done
during sentence processing is exactly this disconfir-
mation.
</bodyText>
<sectionHeader confidence="0.884294" genericHeader="method">
2 Earley parsing
</sectionHeader>
<bodyText confidence="0.999403071428571">
The computation of prefix probabilities takes advan-
tage of the design of the Earley parser (Earley, 1970)
which by itself is not probabilistic. In this section I
provide a brief overview of Stolcke’s algorithm but
the original paper should be consulted for full details
(Stolcke, 1995).
Earley parsers work top-down, and propagate
predictions confirmed by the input string back up
through a set of states representing hypotheses the
parser is entertaining about the structure of the sen-
tence. The global state of the parser at any one time
is completely defined by this collection of states, a
chart, which defines a tree set. A state is a record
that specifies
</bodyText>
<listItem confidence="0.997598">
• the current input string position processed so
far
• a grammar rule
• a “dot-position” in the rule representing how
much of the rule has already been recognized
• the leftmost edge of the substring this rule gen-
erates
</listItem>
<bodyText confidence="0.997149670731708">
An Earley parser has three main functions, pre-
dict, scan and complete, each of which can enter
new states into the chart. Starting from a dummy
start state in which the dot is just to the left of the
grammar’s start symbol, predict adds new states for
rules which could expand the start symbol. In these
new predicted states, the dot is at the far left-hand
side of each rule. After prediction, scan checks the
input string: if the symbol immediately following
the dot matches the current word in the input, then
the dot is moved rightward, across the symbol. The
parser has “scanned” this word. Finally, complete
propagates this change throughout the chart. If, as
a result of scanning, any states are now present in
which the dot is at the end of a rule, then the left
hand side of that rule has been recognized, and any
other states having a dot immediately in front of
the newly-recognized left hand side symbol can now
have their dots moved as well. This happens over
and over until no new states are generated. Parsing
finishes when the dot in the dummy start state is
moved across the grammar’s start symbol.
Stolcke’s innovation, as regards prefix probabili-
ties is to add two additional pieces of information to
each state: α, the forward, or prefix probability, and
y the “inside” probability. He notes that
path An (unconstrained) Earley path,
or simply path, is a sequence of Earley
states linked by prediction, scanning,
or completion.
constrained A path is said to be con-
strained by, or generate a string x if
the terminals immediately to the left
of the dot in all scanned states, in se-
quence, form the string x.
. . .
The significance of Earley paths is that
they are in a one-to-one correspondence
with left-most derivations. This will al-
low us to talk about probabilities of deriva-
tions, strings and prefixes in terms of the
actions performed by Earley’s parser.
(Stolcke, 1995, page 8)
This correspondence between paths of parser op-
erations and derivations enables the computation of
the prefix probability – the sum of all derivations
compatible with the prefix seen so far. By the cor-
respondence between derivations and Earley paths,
one would need only to compute the sum of all paths
that are constrained by the observed prefix. But
this can be done in the course of parsing by storing
the current prefix probability in each state. Then,
when a new state is added by some parser opera-
tion, the contribution from each antecedent state –
each previous state linked by some parser operation
– is summed in the new state. Knowing the prefix
probability at each state and then summing for all
parser operations that result in the same new state
efficiently counts all possible derivations.
Predicting a rule corresponds to multiplying by
that rule’s probability. Scanning does not alter any
probabilities. Completion, though, requires knowing
y, the inside probability, which records how probable
was the inner structure of some recognized phrasal
node. When a state is completed, a bottom-up con-
firmation is united with a top-down prediction, so
the α value of the complete-ee is multiplied by the
y value of the complete-er.
Important technical problems involving left-
recursive and unit productions are examined and
overcome in (Stolcke, 1995). However, these com-
plications do not add any further machinery to the
parsing algorithm per se beyond the grammar rules
and the dot-moving conventions: in particular, there
are no heuristic parsing principles or intermediate
structures that are later destroyed. In this respect
the algorithm observes strong competence – princi-
ple 1. In virtue of being a probabilistic parser it
observes principle 2. Finally, in the sense that pre-
dict and complete each apply exhaustively at each
new input word, the algorithm is eager, satisfying
principle 3.
</bodyText>
<sectionHeader confidence="0.998006" genericHeader="method">
3 Parallelism
</sectionHeader>
<bodyText confidence="0.999990176470588">
Psycholinguistic theories vary regarding the amount
bandwidth they attribute to the human sentence
processing mechanism. Theories of initial parsing
preferences (Fodor and Ferreira, 1998) suggest that
the human parser is fundamentally serial: a func-
tion from a tree and new word to a new tree. These
theories explain processing difficulty by appealing
to “garden pathing” in which the current analysis
is faced with words that cannot be reconciled with
the structures built so far. A middle ground is held
by bounded-parallelism theories (Narayanan and Ju-
rafsky, 1998) (Roark and Johnson, 1999). In these
theories the human parser is modeled as a function
from some subset of consistent trees and the new
word, to a new tree subset. Garden paths arise in
these theories when analyses fall out of the set of
trees maintained from word to word, and have to
be reanalyzed, as on strictly serial theories. Finally,
there is the possibility of total parallelism, in which
the entire set of trees compatible with the input is
maintained somehow from word to word. On such
a theory, garden-pathing cannot be explained by re-
analysis.
The probabilistic Earley parser computes all
parses of its input, so as a psycholinguistic theory
it is a total parallelism theory. The explanation
for garden-pathing will turn on the reduction in the
probability of the new tree set compared with the
previous tree set – reanalysis plays no role. Before
illustrating this kind of explanation with a specific
example, it will be important to first clarify the na-
ture of the linking hypothesis between the operation
of the probabilistic Earley parser and the measured
effects of the human parser.
</bodyText>
<sectionHeader confidence="0.933007" genericHeader="method">
4 Linking hypothesis
</sectionHeader>
<bodyText confidence="0.999693175000001">
The measure of cognitive effort mentioned earlier is
defined over prefixes: for some observed prefix, the
cognitive effort expended to parse that prefix is pro-
portional to the total probability of all the struc-
tural analyses which cannot be compatible with the
observed prefix. This is consistent with eagerness
since, if the parser were to fail to infer the incom-
patibility of some incompatible analysis, it would
be delaying a computation, and hence not be eager.
This prefix-based linking hypothesis can be turned
into one that generates predictions about word-by-
word reading times by comparing the total effort
expended before some word to the total effort af-
ter: in particular, take the comparison to be a ratio.
Making the further assumption that the probabili-
ties on PCFG rules are statements about how diffi-
cult it is to disconfirm each rule&apos;, then the ratio of
&apos;This assumption is inevitable given principles 1 and 2. If
there were separate processing costs distinct from the opti-
mization costs postulated in the grammar, then strong com-
petence is violated. Defining all grammatical structures as
equally easy to disconfirm or perceive likewise voids the grad-
edness of grammaticality of any content.
the α value for the previous word to the α value for
the current word measures the combined difficulty
of disconfirming all disconfirmable structures at a
given word – the definition of cognitive load. Scal-
ing this number by taking its log gives the surprisal,
and defines a word-based measure of cognitive effort
in terms of the prefix-based one. Of course, if the
language model is sensitive to hierarchical structure,
then the measure of cognitive effort so defined will
be structure-sensitive as well.
could account for garden path structural ambiguity.
Grammar (1) generates the celebrated garden path
sentence “the horse raced past the barn fell” (Bever,
1970). English speakers hearing these words one by
one are inclined to take “the horse” as the subject of
“raced,” expecting the sentence to end at the word
“barn.” This is the main verb reading in figure 1.
</bodyText>
<figure confidence="0.461165">
S
NP VP
</figure>
<sectionHeader confidence="0.959711" genericHeader="method">
5 Plausibility of Probabilistic
Context-Free Grammar
</sectionHeader>
<bodyText confidence="0.999522129032258">
The debate over the form grammar takes in the mind
is clearly a fundamental one for cognitive science.
Much recent psycholinguistic work has generated a
wealth of evidence that frequency of exposure to lin-
guistic elements can affect our processing (Mitchell
et al., 1995) (MacDonald et al., 1994). However,
there is no clear consensus as to the size of the ele-
ments over which exposure has clearest effect. Gib-
son and Pearlmutter identify it as an “outstanding
question” whether or not phrase structure statistics
are necessary to explain performance effects in sen-
tence comprehension:
Are phrase-level contingent frequency con-
straints necessary to explain comprehen-
sion performance, or are the remaining
types of constraints sufficient. If phrase-
level contingent frequency constraints are
necessary, can they subsume the effects of
other constraints (e.g. locality) ?
(Gibson and Pearlmutter, 1998, page 13)
Equally, formal work in linguistics has demon-
strated the inadequacy of context-free grammars as
an appropriate model for natural language in the
general case (Shieber, 1985). To address this criti-
cism, the same prefix probabilities could be comput-
ing using tree-adjoining grammars (Nederhof et al.,
1998). With context-free grammars serving as the
implicit backdrop for much work in human sentence
processing, as well as linguistics2 simplicity seems as
good a guide as any in the selection of a grammar
formalism.
</bodyText>
<sectionHeader confidence="0.997177" genericHeader="method">
6 Garden-pathing
</sectionHeader>
<subsectionHeader confidence="0.999036">
6.1 A celebrated example
</subsectionHeader>
<bodyText confidence="0.976659">
Probabilistic context-free grammar (1) will help il-
lustrate the way a phrase-structured language model
</bodyText>
<footnote confidence="0.9632498">
2Some important work in computational psycholinguistics
(Ford, 1989) assumes a Lexical-Functional Grammar where
the c-structure rules are essentially context-free and have
attached to them “strengths” which one might interpret as
probabilities.
</footnote>
<figure confidence="0.85664725">
the horse VBD PP
raced IN NP
past
barn
</figure>
<figureCaption confidence="0.999951">
Figure 1: Main verb reading
</figureCaption>
<bodyText confidence="0.9999166">
The human sentence processing mechanism is
metaphorically led up the garden path by the main
verb reading, when, upon hearing “fell” it is forced
to accept the alternative reduced relative reading
shown in figure 2.
</bodyText>
<figure confidence="0.9001715">
past
barn
</figure>
<figureCaption confidence="0.999754">
Figure 2: Reduced relative reading
</figureCaption>
<bodyText confidence="0.9997842">
The confusion between the main verb and the re-
duced relative readings, which is resolved upon hear-
ing “fell” is the empirical phenomenon at issue.
As the parse trees indicate, grammar (1) analyzes
reduced relative clauses as a VP adjoined to an NP3.
In one sample of parsed text4 such adjunctions are
about 7 times less likely than simple NPs made up of
a determiner followed by a noun. The probabilities
of the other crucial rules are likewise estimated by
their relative frequencies in the sample.
</bodyText>
<footnote confidence="0.983706">
3See section 1.24 of the Treebank style guide
4The sample, starts at sentence 93 of section 16 of
the Treebank and goes for 500 sentences (12924 words)
For information about the Penn Treebank project see
http://www.cis.upenn.edu/~ treebank/
</footnote>
<table confidence="0.709945033333333">
S
the horse raced IN NP
NP
VP
VBD
NP
VP
VBN
NN
fell
DT
PP
DT NN
the
DT NN
the
1.0 S NP VP .
0.876404494831 NP DT NN
0.123595505169 NP NP VP
1.0 PP IN NP
0.171428571172 VP VBD PP
0.752380952552 VP VBN PP
0.0761904762759 VP VBD
(1) 1.0 DT the
0.5 NN horse
0.5 NN barn
0.5 VBD fell
0.5 VBD raced
1.0 VBN raced
1.0 IN past
</table>
<bodyText confidence="0.9962391">
This simple grammar exhibits the essential character
of the explanation: garden paths happen at points
where the parser can disconfirm alternatives that to-
gether comprise a great amount of probability. Note
the category ambiguity present with raced which can
show up as both a past-tense verb (VBD) and a past
participle (VBN).
Figure 3 shows the reading time predictions5 derived
via the linking hypothesis that reading time at word
n is proportional to the surprisal log (αα−1).
</bodyText>
<figure confidence="0.972377461538462">
n
previous prefix
Log[] current prefix garden-pathing
14
12
10
8
6
4
2
0.1906840.0641303
0
the horse raced past the barn fell
</figure>
<figureCaption confidence="0.9324595">
Figure 3: Predictions of probabilistic Earley parser
on simple grammar
</figureCaption>
<bodyText confidence="0.9998442">
At “fell,” the parser garden-paths: up until that
point, both the main-verb and reduced-relative
structures are consistent with the input. The prefix
probability before “fell” is scanned is more than 10
times greater than after, suggesting that the proba-
bility mass of the analyses disconfirmed at that point
was indeed great. In fact, all of the probability as-
signed to the main-verb structure is now lost, and
only parses that involve the low-probability NP rule
survive – a rule introduced 5 words back.
</bodyText>
<subsectionHeader confidence="0.999357">
6.2 A comparison
</subsectionHeader>
<bodyText confidence="0.9566705">
If this garden path effect is truly a result of both the
main verb and the reduced relative structures be-
ing simultaneously available up until the final verb,
5Whether the quantitative values of the predicted read-
ing times can be mapped onto a particular experiment in-
volves taking some position on the oft-observed (Gibson and
Sch¨utze, 1999) imperfect relationship between corpus fre-
quency and psychological norms.
then the effect should disappear when words inter-
vene that cancel the reduced relative interpretation
early on.
To examine this possibility, consider now a differ-
ent example sentence, this time from the language
of grammar (2).
</bodyText>
<table confidence="0.998412782608695">
0.574927953937 S NP VP
0.425072046063 S VP
1.0 SBAR WHNP S
0.80412371161 NP DT NN
0.082474226966 NP NP SBAR
0.113402061424 NP NP VP
0.11043 VP VBD PP
0.141104 VP VBD NP PP
0.214724 VP AUX VP
0.484663 VP VBN PP
0.0490798 VP VBD
(2) 1.0 PP IN NP
1.0 WHNP who
1.0 DT the
0.33 NN boss
0.33 NN banker
0.33 NN buy-back
0.5 IN about
0.5 IN by
1.0 AUX was
0.74309393 VBD told
0.25690607 VBD resigned
1.0 VBN told
</table>
<bodyText confidence="0.999596727272727">
The probabilities in grammar (2) are estimated from
the same sample as before. It generates a sentence
composed of words actually found in the sample,
“the banker told about the buy-back resigned.” This
sentence exhibits the same reduced relative clause
structure as does “the horse raced past the barn
fell.”
Grammar (2) also generates6 the subject relative
“the banker who was told about the buy-back re-
signed.” Now a comparison of two conditions is pos-
sible.
</bodyText>
<footnote confidence="0.9008985">
MV and RC the banker told about the buy-back re-
signed
6This grammar also generates active and simple passive
sentences, rating passive sentences as more probable than the
actives. This is presumably a fact about the writing style
favored by the Wall Street Journal.
</footnote>
<figure confidence="0.996410714285714">
S
NP
NP
VP
VP
VBD
banker
told
about the buy-back
the
PP
DT
NN
VBN
resigned
1.
5.90627
1.
0
previous prefix
Log[]
current prefix Subject Relative Clause
6
the banker who was told about the buy-backresigned
previous prefix
Log[]
current prefix grammar from Wall Street Journal sample
12.9214
the banker told about the buy-backresigned .
3.599913.45367
1.59946
1.59946
1.3212
0.798547
0.498082
0.
11.9496
11.9369
9.59068
8.59021
6.50046
2.92747
3.13979
5.87759
5
4
3
2
1
14
12
10
8
6
4
2
</figure>
<figureCaption confidence="0.93053">
Figure 4: Mean 10.5
</figureCaption>
<figure confidence="0.98536">
previous prefix
Log[]
current prefix Reduced Relative Clause
</figure>
<figureCaption confidence="0.8859215">
Figure 6: Predictions of Earley parser on richer
grammar
</figureCaption>
<figure confidence="0.9827085">
6.67629
6
5
4
3
1.59946 1.3212
0.798547 0.622262
the banker told about the buy-back resigned
</figure>
<figureCaption confidence="0.999708">
Figure 5: Mean: 16.44
</figureCaption>
<bodyText confidence="0.998635">
RC only the banker who was told about the buy-
back resigned
The words who was cancel the main verb reading,
and should make that condition easier to process.
This asymmetry is borne out in graphs 4 and 5. At
“resigned” the probabilistic Earley parser predicts
less reading time in the subject relative condition
than in the reduced relative condition.
This comparison verifies that the same sorts of
phenomena treated in reanalysis and bounded paral-
lelism parsing theories fall out as cases of the present,
total parallelism theory.
</bodyText>
<subsectionHeader confidence="0.999927">
6.3 An entirely empirical grammar
</subsectionHeader>
<bodyText confidence="0.999916125">
Although they used frequency estimates provided by
corpus data, the previous two grammars were par-
tially hand-built. They used a subset of the rules
found in the sample of parsed text. A grammar in-
cluding all rules observed in the entire sample sup-
ports the same sort of reasoning. In this grammar,
instead of just 2 NP rules there are 532, along with
120 S rules. Many of these generate analyses com-
patible with prefixes of the reduced relative clause at
various points during parsing, so the expectation is
that the parser will be disconfirming many more hy-
potheses at each word than in the simpler example.
Figure 6 shows the reading time predictions derived
from this much richer grammar.
Because the terminal vocabulary of this richer
grammar is so much larger, a comparatively large
amount of information is conveyed by the nouns
“banker” and “buy-back” leading to high surprisal
values at those words. However, the garden path
effect is still observable at “resigned” where the pre-
fix probability ratio is nearly 10 times greater than
at either of the nouns. Amid the lexical effects, the
probabilistic Earley parser is affected by the same
structural ambiguity that affects English speakers.
</bodyText>
<sectionHeader confidence="0.973258" genericHeader="method">
7 Subject/Object asymmetry
</sectionHeader>
<bodyText confidence="0.996417545454546">
The same kind of explanation supports an account
of the subject-object relative asymmetry (cf. refer-
ences in (Gibson, 1998)) in the processing of unre-
duced relative clauses. Since the Earley parser is
designed to work with context-free grammars, the
following example grammar adopts a GPSG-style
analysis of relative clauses (Gazdar et al., 1985, page
155). The estimates of the ratios for the two S[+R]
rules are obtained by counting the proportion of sub-
ject relatives among all relatives in the Treebank’s
parsed Brown corpus7.
</bodyText>
<table confidence="0.99899494117647">
0.33 NP → SPECNP NBAR
0.33 NP → you
0.33 NP → me
1.0 SPECNP → DT
0.5 NBAR → NBAR S[+R]
0.5 NBAR → N
1.0 S → NP VP
0.86864638 S[+R] → NP[+R] VP
(3) 0.13135362 S[+R] → NP[+R] S/NP
1.0 S/NP → NP VP/NP
1.0 VP/NP → V NP/NP
1.0 VP → V NP
1.0 V → saw
1.0 NP[+R] → who
1.0 DT → the
1.0 N → man
1.0 NP/NP → e
</table>
<footnote confidence="0.958777666666667">
7In particular, relative clauses in the Treebank are ana-
lyzed as NP → NP SBAR (rule 1) where the S con-
SBAR → WHNP S (rule 2)
tains a trace *T* coindexed with the WHNP. The total num-
ber of structures in which both rule 1 and rule 2 apply is
5489. The total number where the first child of S is null is
4768. This estimate puts the total number of object relatives
at 721 and the frequency of object relatives at 0.13135362 and
the frequency of subject relatives at 0.86864638.
</footnote>
<figure confidence="0.965534">
1.59946
0.
2
1
</figure>
<bodyText confidence="0.9925368">
Grammar (3) generates both subject and object rela-
tive clauses. S[+R] → NP[+R] VP is the rule that gen-
erates subject relatives and S[+R] → NP[+R] S/NP
generates object relatives. One might expect there
to be a greater processing load for object relatives as
soon as enough lexical material is present to deter-
mine that the sentence is in fact an object relatives.
The same probabilistic Earley parser (modified to
handle null-productions) explains this asymmetry in
the same way as it explains the garden path effect.
Its predictions, under the same linking hypothesis
as in the previous cases, are depicted in graphs 7
and 8. The mean surprisal for the object relative is
about 5.0 whereas the mean surprisal for the subject
relative is about 2.1.
</bodyText>
<figure confidence="0.981729">
previous prefix
Log[t
current prefix Subject Relative Clause
5
1.59946 1.59946
1. 1.
0.203159
the man who saw you saw me
</figure>
<figureCaption confidence="0.9996975">
Figure 7: Subject relative clause
Figure 8: Object relative clause
</figureCaption>
<sectionHeader confidence="0.929282" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999781125">
These examples suggest that a “total-parallelism”
parsing theory based on probabilistic grammar can
characterize some important processing phenomena.
In the domain of structural ambiguity in particular,
the explanation is of a different kind than in tradi-
tional reanalysis models: the order of processing is
not theoretically significant, but the estimate of its
magnitude at each point in a sentence is. Results
with empirically-derived grammars suggest an affir-
mative answer to Gibson and Pearlmutter’s ques-
sThe difference in probability between subject and object
rules could be due to the work necessary to set up storage
for the filler, effectively recapitulating the HOLD Hypothesis
(Wanner and Maratsos, 1978, page 119)
tion: phrase-level contingent frequencies can do the
work formerly done by other mechanisms.
Pursuit of methodological principles 1, 2 and 3
has identified a model capable of describing some of
the same phenomena that motivate psycholinguistic
interest in other theoretical frameworks. Moreover,
this recommends probabilistic grammars as an at-
tractive possibility for psycholinguistics by provid-
ing clear, testable predictions and the potential for
new mathematical insights.
</bodyText>
<sectionHeader confidence="0.984849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.816533659090909">
Fred Attneave. 1959. Applications of Information
Theory to Psychology: A summary of basic con-
cepts, methods and results. Holt, Rinehart and
Winston.
Thomas G. Bever. 1970. The cognitive basis for
linguistic structures. In J.R. Hayes, editor, Cog-
nition and the Development of Language, pages
279–362. Wiley, New York.
Taylor L. Booth and Richard A. Thompson. 1973.
Applying probability measures to abstract lan-
guages. IEEE Transactions on Computers, C-
22(5).
Joan Bresnan. 1982. Introduction: Grammars as
mental representations of language. In Joan Bres-
nan, editor, The Mental Representation of Gram-
matical Relations, pages xvii,lii. MIT Press, Cam-
bridge, MA.
Eugene Charniak. 1993. Statistical Language Learn-
ing. MIT Press.
Ciprian Chelba and Frederick Jelinek. 1998. Ex-
ploiting syntactic structure for language mod-
elling. In Proceedings of COLING-ACL ’98, pages
225–231, Montreal.
Noam Chomsky. 1956. Three models for the de-
scription of language. IRE Transactions on In-
formation Theory, 2(3):113–124.
Noam Chomsky. 1965. Aspects of the Theory of
Syntax. MIT Press, Cambridge MA.
Jay Earley. 1970. An efficient context-free pars-
ing algorithm. Communications of the Associa-
tion for Computing Machinery, 13(2), February.
Janet Dean Fodor and Fernanda Ferreira, editors.
1998. Reanalysis in sentence processing, vol-
ume 21 of Studies in Theoretical Psycholingustics.
Kluwer, Dordrecht.
Marilyn Ford. 1989. Parsing complexity and a the-
ory of parsing. In Greg N. Carlson and Michael K.
Tanenhaus, editors, Linguistic Structure in Lan-
guage Processing, pages 239–272. Kluwer.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and
Ivan Sag. 1985. Generalized Phrase Structure
Grammar. Harvard University Press, Cambridge,
MA.
Edward Gibson and Neal J. Pearlmutter. 1998.
</reference>
<figure confidence="0.99552915">
4.52793
1. 1.
0 0
man who you saw saw
previous prefix
Log[t
current prefix Object Relative Clause
5
4
3
2
1
1.59946
the
1.59946
me
4 1.59946
3
2
1 0
</figure>
<reference confidence="0.999035481012658">
Constraints on sentence processing. Trends in
Cognitive Sciences, 2:262–268.
Edward Gibson and Carson Sch¨utze. 1999. Disam-
biguation preferences in noun phrase conjunction
do not mirror corpus frequency. Journal of Mem-
ory and Language.
Edward Gibson. 1998. Linguistic complexity: local-
ity of syntactic dependencies. Cognition, 68:1–76.
Ulf Grenander. 1967. Syntax-controlled probabili-
ties. Technical report, Brown University Division
of Applied Mathematics, Providence, RI.
Frederick Jelinek and John D. Lafferty. 1991. Com-
putation of the probability of initial substring
generation by stochastic context-free grammars.
Computational Linguistics, 17(3).
Maryellen C. MacDonald, Neal J. Pearlmutter, and
Mark S. Seidenberg. 1994. Lexical nature of syn-
tactic ambiguity resolution. Psychological Review,
101(4):676–703.
James McClelland and Mark St. John. 1989. Sen-
tence comprehension: A PDP approach. Lan-
guage and Cognitive Processes, 4:287–336.
Don C. Mitchell, Fernando Cuetos, Martin M.B.
Corley, and Marc Brysbaert. 1995. Exposure-
based models of human parsing: Evidence for
the use of coarse-grained (nonlexical) statisti-
cal records. Journal of Psycholinguistic Research,
24(6):469–488.
Srini Narayanan and Daniel Jurafsky. 1998.
Bayesian models of human sentence processing.
In Proceedings of the 19th Annual Conference
of the Cognitive Science Society, University of
Wisconsin-Madson.
Mark-Jan Nederhof, Anoop Sarkar, and Giorgio
Satta. 1998. Prefix probabilities from stochas-
tic tree adjoining grammars. In Proceedings of
COLING-ACL ’98, pages 953–959, Montreal.
Brian Roark and Mark Johnson. 1999. Broad cover-
age predictive parsing. Presented at the 12th An-
nual CUNY Conference on Human Sentence Pro-
cessing, March.
Stuart Shieber and Mark Johnson. 1993. Variations
on incremental interpretation. Journal of Psy-
cholinguistic Research, 22(2):287–318.
Stuart Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Phi-
losophy, 8:333–343.
Stephen Soule. 1974. Entropies of probabilistic
grammars. Information and Control, 25(57–74).
Edward Stabler. 1991. Avoid the pedestrian’s para-
dox. In Robert C. Berwick, Steven P. Abney, and
Carol Tenny, editors, Principle-Based Parsing:
computation and psycholinguistics, Studies in Lin-
guistics and Philosophy, pages 199–237. Kluwer,
Dordrecht.
Mark Steedman. 1992. Grammars and processors.
Technical Report TR MS-CIS-92-52, University of
Pennsylvania CIS Department.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2).
Andreas Stolcke. 1997. Linguistic knowledge and
empirical methods in speech recognition. AI Mag-
azine, 18(4):25–31.
Whitney Tabor, Cornell Juliano, and Michael
Tanenhaus. 1997. Parsing in a dynamical sys-
tem: An attractor-based account of the interac-
tion of lexical and structural constraints in sen-
tence processing. Language and Cognitive Pro-
cesses, 12(2/3):211–271.
Eric Wanner and Michael Maratsos. 1978. An ATN
approach to comprehension. In Morris Halle, Joan
Bresnan, and George A. Miller, editors, Linguistic
Theory and Psychological Reality, chapter 3, pages
119–161. MIT Press, Cambridge, Massachusetts.
C.S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Sur-
veys, 12(4).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.546051">
<title confidence="0.998906">A Probabilistic Earley Parser as a Psycholinguistic Model</title>
<author confidence="0.977479">John</author>
<affiliation confidence="0.8509325">Department of Cognitive The Johns Hopkins</affiliation>
<address confidence="0.992829">3400 North Charles Street; Baltimore MD</address>
<email confidence="0.999375">hale@cogsci.jhu.edu</email>
<abstract confidence="0.988072705882353">In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at point in a sentence: the surprisal of word its prefix on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fred Attneave</author>
</authors>
<title>Applications of Information Theory to Psychology: A summary of basic concepts, methods and results.</title>
<date>1959</date>
<contexts>
<context position="2529" citStr="Attneave, 1959" startWordPosition="377" endWordPosition="378">McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense means the experimental situations to be modeled are ones like self-paced reading in which sentence comprehenders are unrushed and no information is ignored at a point at which it could be used. The proposal is that a person’s difficulty perceiving syntactic structure be modeled by word-toword surprisal (Attneave, 1959, page 6) which can be directly computed from a probabilistic phrasestructure grammar. The approach taken here uses a parsing algorithm developed by Stolcke. In the course of explaining the algorithm at a very high level I will indicate how the algorithm, interpreted as a psycholinguistic model, observes each principle. After that will come some simulation results, and then a conclusion. 1 Language models Stolcke’s parsing algorithm was initially applied as a component of an automatic speech recognition system. In speech recognition, one is often interested in the probability that some word wi</context>
</contexts>
<marker>Attneave, 1959</marker>
<rawString>Fred Attneave. 1959. Applications of Information Theory to Psychology: A summary of basic concepts, methods and results. Holt, Rinehart and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Bever</author>
</authors>
<title>The cognitive basis for linguistic structures.</title>
<date>1970</date>
<booktitle>Cognition and the Development of Language,</booktitle>
<pages>279--362</pages>
<editor>In J.R. Hayes, editor,</editor>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="14696" citStr="Bever, 1970" startWordPosition="2384" endWordPosition="2385">he current word measures the combined difficulty of disconfirming all disconfirmable structures at a given word – the definition of cognitive load. Scaling this number by taking its log gives the surprisal, and defines a word-based measure of cognitive effort in terms of the prefix-based one. Of course, if the language model is sensitive to hierarchical structure, then the measure of cognitive effort so defined will be structure-sensitive as well. could account for garden path structural ambiguity. Grammar (1) generates the celebrated garden path sentence “the horse raced past the barn fell” (Bever, 1970). English speakers hearing these words one by one are inclined to take “the horse” as the subject of “raced,” expecting the sentence to end at the word “barn.” This is the main verb reading in figure 1. S NP VP 5 Plausibility of Probabilistic Context-Free Grammar The debate over the form grammar takes in the mind is clearly a fundamental one for cognitive science. Much recent psycholinguistic work has generated a wealth of evidence that frequency of exposure to linguistic elements can affect our processing (Mitchell et al., 1995) (MacDonald et al., 1994). However, there is no clear consensus a</context>
</contexts>
<marker>Bever, 1970</marker>
<rawString>Thomas G. Bever. 1970. The cognitive basis for linguistic structures. In J.R. Hayes, editor, Cognition and the Development of Language, pages 279–362. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
<author>Richard A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="3392" citStr="Booth and Thompson, 1973" startWordPosition="512" endWordPosition="515">ow the algorithm, interpreted as a psycholinguistic model, observes each principle. After that will come some simulation results, and then a conclusion. 1 Language models Stolcke’s parsing algorithm was initially applied as a component of an automatic speech recognition system. In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen. Given some lexicon of all possible words, a language model assigns a probability to every string of words from the lexicon. This defines a probabilistic language (Grenander, 1967) (Booth and Thompson, 1973) (Soule, 1974) (Wetherell, 1980). A language model helps a speech recognizer focus its attention on words that are likely continuations of what it has recognized so far. This is typically done using conditional probabilities of the form P (Wn = wn|W1 = w1,... Wn−1 = wn−1) the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1. Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models used for speech are ngram mod</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Taylor L. Booth and Richard A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, C22(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Introduction: Grammars as mental representations of language.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>pages xvii,lii.</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1593" citStr="Bresnan, 1982" startWordPosition="237" endWordPosition="238">ntroduction What is the relation between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation between the parser and grammar is one of strong competence. Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eage</context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>Joan Bresnan. 1982. Introduction: Grammars as mental representations of language. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages xvii,lii. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4859" citStr="Charniak, 1993" startWordPosition="760" endWordPosition="761">ically-significant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Lafferty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to significantly improve on 3-grams (Chelba and Jelinek, 1998). Stolcke’s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model. The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g. 1.0 S → NP VP 0.5 NP → Det N 0.5 NP → NP VP ... ... see (Charniak, 1993, chapter 5) Such a grammar defines a probabilistic language in terms of a stochastic process that rewrites strings of grammar symbols according to the probabilities on the rules. Then each sentence in the language of the grammar has a probability equal to the product of the probabilities of all the rules used to generate it. This multiplication embodies the assumption that rule choices are independent. Sentences with more than one derivation accumulate the probability of all derivations that generate them. Through recursion, infinite languages can be specified; an important mathematical quest</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modelling.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL ’98,</booktitle>
<pages>225--231</pages>
<location>Montreal.</location>
<contexts>
<context position="4587" citStr="Chelba and Jelinek, 1998" startWordPosition="709" endWordPosition="712"> used for speech are ngram models, in which n − 1 words of history serve as the basis for predicting the nth word. Such models do not have any notion of hierarchical syntactic structure, except as might be visible through an nword window. Aware that the n-gram obscures many linguistically-significant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Lafferty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to significantly improve on 3-grams (Chelba and Jelinek, 1998). Stolcke’s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model. The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g. 1.0 S → NP VP 0.5 NP → Det N 0.5 NP → NP VP ... ... see (Charniak, 1993, chapter 5) Such a grammar defines a probabilistic language in terms of a stochastic process that rewrites strings of grammar symbols according to the probabilities on the rules. Then each sentence in the language of the grammar has a probability equal to the product of the probabilities of all the rules used to generate it. </context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntactic structure for language modelling. In Proceedings of COLING-ACL ’98, pages 225–231, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Three models for the description of language.</title>
<date>1956</date>
<journal>IRE Transactions on Information Theory,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="4291" citStr="Chomsky, 1956" startWordPosition="667" endWordPosition="668">ty that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1. Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models used for speech are ngram models, in which n − 1 words of history serve as the basis for predicting the nth word. Such models do not have any notion of hierarchical syntactic structure, except as might be visible through an nword window. Aware that the n-gram obscures many linguistically-significant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Lafferty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to significantly improve on 3-grams (Chelba and Jelinek, 1998). Stolcke’s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model. The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g. 1.0 S → NP VP 0.5 NP → Det N 0.5 NP → NP VP ... ... see (Charniak, 1993, chapter 5) Such a grammar defi</context>
</contexts>
<marker>Chomsky, 1956</marker>
<rawString>Noam Chomsky. 1956. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="1531" citStr="Chomsky, 1965" startWordPosition="225" endWordPosition="227">l ambiguity and with the subject/object relative asymmetry. Introduction What is the relation between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation between the parser and grammar is one of strong competence. Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded i</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="6530" citStr="Earley, 1970" startWordPosition="1029" endWordPosition="1030">uting, at each word of an input string, the prefix probability. This is the sum of the probabilities of all derivations whose yield is compatible with the string seen so far. If the grammar is consistent (the probabilities of all derivations sum to 1.0) then subtracting the prefix probability from 1.0 gives the total probability of all the analyses the parser has disconfirmed. If the human parser is eager, then the “work” done during sentence processing is exactly this disconfirmation. 2 Earley parsing The computation of prefix probabilities takes advantage of the design of the Earley parser (Earley, 1970) which by itself is not probabilistic. In this section I provide a brief overview of Stolcke’s algorithm but the original paper should be consulted for full details (Stolcke, 1995). Earley parsers work top-down, and propagate predictions confirmed by the input string back up through a set of states representing hypotheses the parser is entertaining about the structure of the sentence. The global state of the parser at any one time is completely defined by this collection of states, a chart, which defines a tree set. A state is a record that specifies • the current input string position process</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2), February.</rawString>
</citation>
<citation valid="true">
<title>Reanalysis in sentence processing,</title>
<date>1998</date>
<booktitle>of Studies in Theoretical Psycholingustics.</booktitle>
<volume>21</volume>
<editor>Janet Dean Fodor and Fernanda Ferreira, editors.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>1998</marker>
<rawString>Janet Dean Fodor and Fernanda Ferreira, editors. 1998. Reanalysis in sentence processing, volume 21 of Studies in Theoretical Psycholingustics. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Ford</author>
</authors>
<title>Parsing complexity and a theory of parsing.</title>
<date>1989</date>
<booktitle>Linguistic Structure in Language Processing,</booktitle>
<pages>239--272</pages>
<editor>In Greg N. Carlson and Michael K. Tanenhaus, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="16597" citStr="Ford, 1989" startWordPosition="2677" endWordPosition="2678">for natural language in the general case (Shieber, 1985). To address this criticism, the same prefix probabilities could be computing using tree-adjoining grammars (Nederhof et al., 1998). With context-free grammars serving as the implicit backdrop for much work in human sentence processing, as well as linguistics2 simplicity seems as good a guide as any in the selection of a grammar formalism. 6 Garden-pathing 6.1 A celebrated example Probabilistic context-free grammar (1) will help illustrate the way a phrase-structured language model 2Some important work in computational psycholinguistics (Ford, 1989) assumes a Lexical-Functional Grammar where the c-structure rules are essentially context-free and have attached to them “strengths” which one might interpret as probabilities. the horse VBD PP raced IN NP past barn Figure 1: Main verb reading The human sentence processing mechanism is metaphorically led up the garden path by the main verb reading, when, upon hearing “fell” it is forced to accept the alternative reduced relative reading shown in figure 2. past barn Figure 2: Reduced relative reading The confusion between the main verb and the reduced relative readings, which is resolved upon h</context>
</contexts>
<marker>Ford, 1989</marker>
<rawString>Marilyn Ford. 1989. Parsing complexity and a theory of parsing. In Greg N. Carlson and Michael K. Tanenhaus, editors, Linguistic Structure in Language Processing, pages 239–272. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="24018" citStr="Gazdar et al., 1985" startWordPosition="3922" endWordPosition="3925">ervable at “resigned” where the prefix probability ratio is nearly 10 times greater than at either of the nouns. Amid the lexical effects, the probabilistic Earley parser is affected by the same structural ambiguity that affects English speakers. 7 Subject/Object asymmetry The same kind of explanation supports an account of the subject-object relative asymmetry (cf. references in (Gibson, 1998)) in the processing of unreduced relative clauses. Since the Earley parser is designed to work with context-free grammars, the following example grammar adopts a GPSG-style analysis of relative clauses (Gazdar et al., 1985, page 155). The estimates of the ratios for the two S[+R] rules are obtained by counting the proportion of subject relatives among all relatives in the Treebank’s parsed Brown corpus7. 0.33 NP → SPECNP NBAR 0.33 NP → you 0.33 NP → me 1.0 SPECNP → DT 0.5 NBAR → NBAR S[+R] 0.5 NBAR → N 1.0 S → NP VP 0.86864638 S[+R] → NP[+R] VP (3) 0.13135362 S[+R] → NP[+R] S/NP 1.0 S/NP → NP VP/NP 1.0 VP/NP → V NP/NP 1.0 VP → V NP 1.0 V → saw 1.0 NP[+R] → who 1.0 DT → the 1.0 N → man 1.0 NP/NP → e 7In particular, relative clauses in the Treebank are analyzed as NP → NP SBAR (rule 1) where the S conSBAR → WHNP </context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
<author>Neal J Pearlmutter</author>
</authors>
<title>Constraints on sentence processing.</title>
<date>1998</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<pages>2--262</pages>
<contexts>
<context position="15858" citStr="Gibson and Pearlmutter, 1998" startWordPosition="2565" endWordPosition="2568">) (MacDonald et al., 1994). However, there is no clear consensus as to the size of the elements over which exposure has clearest effect. Gibson and Pearlmutter identify it as an “outstanding question” whether or not phrase structure statistics are necessary to explain performance effects in sentence comprehension: Are phrase-level contingent frequency constraints necessary to explain comprehension performance, or are the remaining types of constraints sufficient. If phraselevel contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ? (Gibson and Pearlmutter, 1998, page 13) Equally, formal work in linguistics has demonstrated the inadequacy of context-free grammars as an appropriate model for natural language in the general case (Shieber, 1985). To address this criticism, the same prefix probabilities could be computing using tree-adjoining grammars (Nederhof et al., 1998). With context-free grammars serving as the implicit backdrop for much work in human sentence processing, as well as linguistics2 simplicity seems as good a guide as any in the selection of a grammar formalism. 6 Garden-pathing 6.1 A celebrated example Probabilistic context-free gramm</context>
</contexts>
<marker>Gibson, Pearlmutter, 1998</marker>
<rawString>Edward Gibson and Neal J. Pearlmutter. 1998. Constraints on sentence processing. Trends in Cognitive Sciences, 2:262–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
<author>Carson Sch¨utze</author>
</authors>
<title>Disambiguation preferences in noun phrase conjunction do not mirror corpus frequency.</title>
<date>1999</date>
<journal>Journal of Memory and Language.</journal>
<marker>Gibson, Sch¨utze, 1999</marker>
<rawString>Edward Gibson and Carson Sch¨utze. 1999. Disambiguation preferences in noun phrase conjunction do not mirror corpus frequency. Journal of Memory and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>68--1</pages>
<contexts>
<context position="23796" citStr="Gibson, 1998" startWordPosition="3890" endWordPosition="3891">er grammar is so much larger, a comparatively large amount of information is conveyed by the nouns “banker” and “buy-back” leading to high surprisal values at those words. However, the garden path effect is still observable at “resigned” where the prefix probability ratio is nearly 10 times greater than at either of the nouns. Amid the lexical effects, the probabilistic Earley parser is affected by the same structural ambiguity that affects English speakers. 7 Subject/Object asymmetry The same kind of explanation supports an account of the subject-object relative asymmetry (cf. references in (Gibson, 1998)) in the processing of unreduced relative clauses. Since the Earley parser is designed to work with context-free grammars, the following example grammar adopts a GPSG-style analysis of relative clauses (Gazdar et al., 1985, page 155). The estimates of the ratios for the two S[+R] rules are obtained by counting the proportion of subject relatives among all relatives in the Treebank’s parsed Brown corpus7. 0.33 NP → SPECNP NBAR 0.33 NP → you 0.33 NP → me 1.0 SPECNP → DT 0.5 NBAR → NBAR S[+R] 0.5 NBAR → N 1.0 S → NP VP 0.86864638 S[+R] → NP[+R] VP (3) 0.13135362 S[+R] → NP[+R] S/NP 1.0 S/NP → NP </context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Grenander</author>
</authors>
<title>Syntax-controlled probabilities.</title>
<date>1967</date>
<tech>Technical report,</tech>
<institution>Brown University Division of Applied Mathematics,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="3365" citStr="Grenander, 1967" startWordPosition="510" endWordPosition="511"> I will indicate how the algorithm, interpreted as a psycholinguistic model, observes each principle. After that will come some simulation results, and then a conclusion. 1 Language models Stolcke’s parsing algorithm was initially applied as a component of an automatic speech recognition system. In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen. Given some lexicon of all possible words, a language model assigns a probability to every string of words from the lexicon. This defines a probabilistic language (Grenander, 1967) (Booth and Thompson, 1973) (Soule, 1974) (Wetherell, 1980). A language model helps a speech recognizer focus its attention on words that are likely continuations of what it has recognized so far. This is typically done using conditional probabilities of the form P (Wn = wn|W1 = w1,... Wn−1 = wn−1) the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1. Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models us</context>
</contexts>
<marker>Grenander, 1967</marker>
<rawString>Ulf Grenander. 1967. Syntax-controlled probabilities. Technical report, Brown University Division of Applied Mathematics, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="4359" citStr="Jelinek and Lafferty, 1991" startWordPosition="674" endWordPosition="677">e words leading up to the nth have been w1, w2, ... wn−1. Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models used for speech are ngram models, in which n − 1 words of history serve as the basis for predicting the nth word. Such models do not have any notion of hierarchical syntactic structure, except as might be visible through an nword window. Aware that the n-gram obscures many linguistically-significant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Lafferty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to significantly improve on 3-grams (Chelba and Jelinek, 1998). Stolcke’s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model. The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g. 1.0 S → NP VP 0.5 NP → Det N 0.5 NP → NP VP ... ... see (Charniak, 1993, chapter 5) Such a grammar defines a probabilistic language in terms of a stochastic process that r</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Frederick Jelinek and John D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryellen C MacDonald</author>
<author>Neal J Pearlmutter</author>
<author>Mark S Seidenberg</author>
</authors>
<title>Lexical nature of syntactic ambiguity resolution.</title>
<date>1994</date>
<journal>Psychological Review,</journal>
<volume>101</volume>
<issue>4</issue>
<contexts>
<context position="1970" citStr="MacDonald et al., 1994" startWordPosition="287" endWordPosition="290">nism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense means the experimental situations to be modeled are ones like self-paced reading in which sentence comprehenders are unrushed and no information is ignored at a point at which it could be used. The proposal is that a person’s difficulty perceiving syntactic structure be modeled by word-toword surprisal (Attneave, 1959, page 6) which can be directly computed </context>
<context position="15256" citStr="MacDonald et al., 1994" startWordPosition="2476" endWordPosition="2479">ath sentence “the horse raced past the barn fell” (Bever, 1970). English speakers hearing these words one by one are inclined to take “the horse” as the subject of “raced,” expecting the sentence to end at the word “barn.” This is the main verb reading in figure 1. S NP VP 5 Plausibility of Probabilistic Context-Free Grammar The debate over the form grammar takes in the mind is clearly a fundamental one for cognitive science. Much recent psycholinguistic work has generated a wealth of evidence that frequency of exposure to linguistic elements can affect our processing (Mitchell et al., 1995) (MacDonald et al., 1994). However, there is no clear consensus as to the size of the elements over which exposure has clearest effect. Gibson and Pearlmutter identify it as an “outstanding question” whether or not phrase structure statistics are necessary to explain performance effects in sentence comprehension: Are phrase-level contingent frequency constraints necessary to explain comprehension performance, or are the remaining types of constraints sufficient. If phraselevel contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ? (Gibson and Pearlmutter, 19</context>
</contexts>
<marker>MacDonald, Pearlmutter, Seidenberg, 1994</marker>
<rawString>Maryellen C. MacDonald, Neal J. Pearlmutter, and Mark S. Seidenberg. 1994. Lexical nature of syntactic ambiguity resolution. Psychological Review, 101(4):676–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John</author>
</authors>
<date>1989</date>
<booktitle>Sentence comprehension: A PDP approach. Language and Cognitive Processes,</booktitle>
<pages>4--287</pages>
<contexts>
<context position="1945" citStr="John, 1989" startWordPosition="285" endWordPosition="286">cessing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense means the experimental situations to be modeled are ones like self-paced reading in which sentence comprehenders are unrushed and no information is ignored at a point at which it could be used. The proposal is that a person’s difficulty perceiving syntactic structure be modeled by word-toword surprisal (Attneave, 1959, page 6) which </context>
</contexts>
<marker>John, 1989</marker>
<rawString>James McClelland and Mark St. John. 1989. Sentence comprehension: A PDP approach. Language and Cognitive Processes, 4:287–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don C Mitchell</author>
<author>Fernando Cuetos</author>
<author>Martin M B Corley</author>
<author>Marc Brysbaert</author>
</authors>
<title>Exposurebased models of human parsing: Evidence for the use of coarse-grained (nonlexical) statistical records.</title>
<date>1995</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>24</volume>
<issue>6</issue>
<contexts>
<context position="15231" citStr="Mitchell et al., 1995" startWordPosition="2472" endWordPosition="2475"> the celebrated garden path sentence “the horse raced past the barn fell” (Bever, 1970). English speakers hearing these words one by one are inclined to take “the horse” as the subject of “raced,” expecting the sentence to end at the word “barn.” This is the main verb reading in figure 1. S NP VP 5 Plausibility of Probabilistic Context-Free Grammar The debate over the form grammar takes in the mind is clearly a fundamental one for cognitive science. Much recent psycholinguistic work has generated a wealth of evidence that frequency of exposure to linguistic elements can affect our processing (Mitchell et al., 1995) (MacDonald et al., 1994). However, there is no clear consensus as to the size of the elements over which exposure has clearest effect. Gibson and Pearlmutter identify it as an “outstanding question” whether or not phrase structure statistics are necessary to explain performance effects in sentence comprehension: Are phrase-level contingent frequency constraints necessary to explain comprehension performance, or are the remaining types of constraints sufficient. If phraselevel contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ? (G</context>
</contexts>
<marker>Mitchell, Cuetos, Corley, Brysbaert, 1995</marker>
<rawString>Don C. Mitchell, Fernando Cuetos, Martin M.B. Corley, and Marc Brysbaert. 1995. Exposurebased models of human parsing: Evidence for the use of coarse-grained (nonlexical) statistical records. Journal of Psycholinguistic Research, 24(6):469–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Bayesian models of human sentence processing.</title>
<date>1998</date>
<booktitle>In Proceedings of the 19th Annual Conference of the Cognitive</booktitle>
<institution>Science Society, University of Wisconsin-Madson.</institution>
<contexts>
<context position="11697" citStr="Narayanan and Jurafsky, 1998" startWordPosition="1889" endWordPosition="1893">he algorithm is eager, satisfying principle 3. 3 Parallelism Psycholinguistic theories vary regarding the amount bandwidth they attribute to the human sentence processing mechanism. Theories of initial parsing preferences (Fodor and Ferreira, 1998) suggest that the human parser is fundamentally serial: a function from a tree and new word to a new tree. These theories explain processing difficulty by appealing to “garden pathing” in which the current analysis is faced with words that cannot be reconciled with the structures built so far. A middle ground is held by bounded-parallelism theories (Narayanan and Jurafsky, 1998) (Roark and Johnson, 1999). In these theories the human parser is modeled as a function from some subset of consistent trees and the new word, to a new tree subset. Garden paths arise in these theories when analyses fall out of the set of trees maintained from word to word, and have to be reanalyzed, as on strictly serial theories. Finally, there is the possibility of total parallelism, in which the entire set of trees compatible with the input is maintained somehow from word to word. On such a theory, garden-pathing cannot be explained by reanalysis. The probabilistic Earley parser computes a</context>
</contexts>
<marker>Narayanan, Jurafsky, 1998</marker>
<rawString>Srini Narayanan and Daniel Jurafsky. 1998. Bayesian models of human sentence processing. In Proceedings of the 19th Annual Conference of the Cognitive Science Society, University of Wisconsin-Madson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Anoop Sarkar</author>
<author>Giorgio Satta</author>
</authors>
<title>Prefix probabilities from stochastic tree adjoining grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL ’98,</booktitle>
<pages>953--959</pages>
<location>Montreal.</location>
<contexts>
<context position="16173" citStr="Nederhof et al., 1998" startWordPosition="2613" endWordPosition="2616">hrase-level contingent frequency constraints necessary to explain comprehension performance, or are the remaining types of constraints sufficient. If phraselevel contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ? (Gibson and Pearlmutter, 1998, page 13) Equally, formal work in linguistics has demonstrated the inadequacy of context-free grammars as an appropriate model for natural language in the general case (Shieber, 1985). To address this criticism, the same prefix probabilities could be computing using tree-adjoining grammars (Nederhof et al., 1998). With context-free grammars serving as the implicit backdrop for much work in human sentence processing, as well as linguistics2 simplicity seems as good a guide as any in the selection of a grammar formalism. 6 Garden-pathing 6.1 A celebrated example Probabilistic context-free grammar (1) will help illustrate the way a phrase-structured language model 2Some important work in computational psycholinguistics (Ford, 1989) assumes a Lexical-Functional Grammar where the c-structure rules are essentially context-free and have attached to them “strengths” which one might interpret as probabilities.</context>
</contexts>
<marker>Nederhof, Sarkar, Satta, 1998</marker>
<rawString>Mark-Jan Nederhof, Anoop Sarkar, and Giorgio Satta. 1998. Prefix probabilities from stochastic tree adjoining grammars. In Proceedings of COLING-ACL ’98, pages 953–959, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mark Johnson</author>
</authors>
<title>Broad coverage predictive parsing.</title>
<date>1999</date>
<booktitle>Presented at the 12th Annual CUNY Conference on Human Sentence Processing,</booktitle>
<contexts>
<context position="11723" citStr="Roark and Johnson, 1999" startWordPosition="1894" endWordPosition="1897">ng principle 3. 3 Parallelism Psycholinguistic theories vary regarding the amount bandwidth they attribute to the human sentence processing mechanism. Theories of initial parsing preferences (Fodor and Ferreira, 1998) suggest that the human parser is fundamentally serial: a function from a tree and new word to a new tree. These theories explain processing difficulty by appealing to “garden pathing” in which the current analysis is faced with words that cannot be reconciled with the structures built so far. A middle ground is held by bounded-parallelism theories (Narayanan and Jurafsky, 1998) (Roark and Johnson, 1999). In these theories the human parser is modeled as a function from some subset of consistent trees and the new word, to a new tree subset. Garden paths arise in these theories when analyses fall out of the set of trees maintained from word to word, and have to be reanalyzed, as on strictly serial theories. Finally, there is the possibility of total parallelism, in which the entire set of trees compatible with the input is maintained somehow from word to word. On such a theory, garden-pathing cannot be explained by reanalysis. The probabilistic Earley parser computes all parses of its input, so</context>
</contexts>
<marker>Roark, Johnson, 1999</marker>
<rawString>Brian Roark and Mark Johnson. 1999. Broad coverage predictive parsing. Presented at the 12th Annual CUNY Conference on Human Sentence Processing, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Mark Johnson</author>
</authors>
<title>Variations on incremental interpretation.</title>
<date>1993</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="1654" citStr="Shieber and Johnson, 1993" startWordPosition="244" endWordPosition="247">’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation between the parser and grammar is one of strong competence. Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense means the experimental situations to be mode</context>
</contexts>
<marker>Shieber, Johnson, 1993</marker>
<rawString>Stuart Shieber and Mark Johnson. 1993. Variations on incremental interpretation. Journal of Psycholinguistic Research, 22(2):287–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Evidence against the contextfreeness of natural language. Linguistics and Philosophy,</title>
<date>1985</date>
<pages>8--333</pages>
<contexts>
<context position="16042" citStr="Shieber, 1985" startWordPosition="2595" endWordPosition="2596">n” whether or not phrase structure statistics are necessary to explain performance effects in sentence comprehension: Are phrase-level contingent frequency constraints necessary to explain comprehension performance, or are the remaining types of constraints sufficient. If phraselevel contingent frequency constraints are necessary, can they subsume the effects of other constraints (e.g. locality) ? (Gibson and Pearlmutter, 1998, page 13) Equally, formal work in linguistics has demonstrated the inadequacy of context-free grammars as an appropriate model for natural language in the general case (Shieber, 1985). To address this criticism, the same prefix probabilities could be computing using tree-adjoining grammars (Nederhof et al., 1998). With context-free grammars serving as the implicit backdrop for much work in human sentence processing, as well as linguistics2 simplicity seems as good a guide as any in the selection of a grammar formalism. 6 Garden-pathing 6.1 A celebrated example Probabilistic context-free grammar (1) will help illustrate the way a phrase-structured language model 2Some important work in computational psycholinguistics (Ford, 1989) assumes a Lexical-Functional Grammar where t</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Stuart Shieber. 1985. Evidence against the contextfreeness of natural language. Linguistics and Philosophy, 8:333–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soule</author>
</authors>
<title>Entropies of probabilistic grammars.</title>
<date>1974</date>
<journal>Information and Control,</journal>
<pages>25--57</pages>
<contexts>
<context position="3406" citStr="Soule, 1974" startWordPosition="516" endWordPosition="517">ed as a psycholinguistic model, observes each principle. After that will come some simulation results, and then a conclusion. 1 Language models Stolcke’s parsing algorithm was initially applied as a component of an automatic speech recognition system. In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen. Given some lexicon of all possible words, a language model assigns a probability to every string of words from the lexicon. This defines a probabilistic language (Grenander, 1967) (Booth and Thompson, 1973) (Soule, 1974) (Wetherell, 1980). A language model helps a speech recognizer focus its attention on words that are likely continuations of what it has recognized so far. This is typically done using conditional probabilities of the form P (Wn = wn|W1 = w1,... Wn−1 = wn−1) the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1. Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models used for speech are ngram models, in which </context>
</contexts>
<marker>Soule, 1974</marker>
<rawString>Stephen Soule. 1974. Entropies of probabilistic grammars. Information and Control, 25(57–74).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
</authors>
<title>Avoid the pedestrian’s paradox. In</title>
<date>1991</date>
<booktitle>Principle-Based Parsing: computation and psycholinguistics, Studies in Linguistics and Philosophy,</booktitle>
<pages>199--237</pages>
<editor>Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1609" citStr="Stabler, 1991" startWordPosition="239" endWordPosition="240"> is the relation between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation between the parser and grammar is one of strong competence. Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense</context>
</contexts>
<marker>Stabler, 1991</marker>
<rawString>Edward Stabler. 1991. Avoid the pedestrian’s paradox. In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors, Principle-Based Parsing: computation and psycholinguistics, Studies in Linguistics and Philosophy, pages 199–237. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Grammars and processors.</title>
<date>1992</date>
<tech>Technical Report TR MS-CIS-92-52,</tech>
<institution>University of Pennsylvania CIS Department.</institution>
<contexts>
<context position="1626" citStr="Steedman, 1992" startWordPosition="241" endWordPosition="243"> between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation between the parser and grammar is one of strong competence. Strong competence holds that the human sentence processing mechanism directly uses rules of grammar in its operation, and that a bare minimum of extragrammatical machinery is necessary. This hypothesis, originally proposed by Chomsky (Chomsky, 1965, page 9) has been pursued by many researchers (Bresnan, 1982) (Stabler, 1991) (Steedman, 1992) (Shieber and Johnson, 1993), and stands in contrast with an approach directed towards the discovery of autonomous principles unique to the processing mechanism. Principle 2 Frequency affects performance. The explanatory success of neural network and constraint-based lexicalist theories (McClelland and St. John, 1989) (MacDonald et al., 1994) (Taboret al., 1997) suggests a statistical theory of language performance. The present work adopts a numerical view of competition in grammar that is grounded in probability. Principle 3 Sentence processing is eager. “Eager” in this sense means the experi</context>
</contexts>
<marker>Steedman, 1992</marker>
<rawString>Mark Steedman. 1992. Grammars and processors. Technical Report TR MS-CIS-92-52, University of Pennsylvania CIS Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="629" citStr="Stolcke, 1995" startWordPosition="93" endWordPosition="94"> Earley Parser as a Psycholinguistic Model John Hale Department of Cognitive Science The Johns Hopkins University 3400 North Charles Street; Baltimore MD 21218-2685 hale@cogsci.jhu.edu Abstract In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix w0...i−1 on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry. Introduction What is the relation between a person’s knowledge of grammar and that same person’s application of that knowledge in perceiving syntactic structure? The answer to be proposed here observes three principles. Principle 1 The relation betwee</context>
<context position="6710" citStr="Stolcke, 1995" startWordPosition="1058" endWordPosition="1059">he grammar is consistent (the probabilities of all derivations sum to 1.0) then subtracting the prefix probability from 1.0 gives the total probability of all the analyses the parser has disconfirmed. If the human parser is eager, then the “work” done during sentence processing is exactly this disconfirmation. 2 Earley parsing The computation of prefix probabilities takes advantage of the design of the Earley parser (Earley, 1970) which by itself is not probabilistic. In this section I provide a brief overview of Stolcke’s algorithm but the original paper should be consulted for full details (Stolcke, 1995). Earley parsers work top-down, and propagate predictions confirmed by the input string back up through a set of states representing hypotheses the parser is entertaining about the structure of the sentence. The global state of the parser at any one time is completely defined by this collection of states, a chart, which defines a tree set. A state is a record that specifies • the current input string position processed so far • a grammar rule • a “dot-position” in the rule representing how much of the rule has already been recognized • the leftmost edge of the substring this rule generates An </context>
<context position="9201" citStr="Stolcke, 1995" startWordPosition="1495" endWordPosition="1496">“inside” probability. He notes that path An (unconstrained) Earley path, or simply path, is a sequence of Earley states linked by prediction, scanning, or completion. constrained A path is said to be constrained by, or generate a string x if the terminals immediately to the left of the dot in all scanned states, in sequence, form the string x. . . . The significance of Earley paths is that they are in a one-to-one correspondence with left-most derivations. This will allow us to talk about probabilities of derivations, strings and prefixes in terms of the actions performed by Earley’s parser. (Stolcke, 1995, page 8) This correspondence between paths of parser operations and derivations enables the computation of the prefix probability – the sum of all derivations compatible with the prefix seen so far. By the correspondence between derivations and Earley paths, one would need only to compute the sum of all paths that are constrained by the observed prefix. But this can be done in the course of parsing by storing the current prefix probability in each state. Then, when a new state is added by some parser operation, the contribution from each antecedent state – each previous state linked by some p</context>
<context position="10569" citStr="Stolcke, 1995" startWordPosition="1717" endWordPosition="1718">ame new state efficiently counts all possible derivations. Predicting a rule corresponds to multiplying by that rule’s probability. Scanning does not alter any probabilities. Completion, though, requires knowing y, the inside probability, which records how probable was the inner structure of some recognized phrasal node. When a state is completed, a bottom-up confirmation is united with a top-down prediction, so the α value of the complete-ee is multiplied by the y value of the complete-er. Important technical problems involving leftrecursive and unit productions are examined and overcome in (Stolcke, 1995). However, these complications do not add any further machinery to the parsing algorithm per se beyond the grammar rules and the dot-moving conventions: in particular, there are no heuristic parsing principles or intermediate structures that are later destroyed. In this respect the algorithm observes strong competence – principle 1. In virtue of being a probabilistic parser it observes principle 2. Finally, in the sense that predict and complete each apply exhaustively at each new input word, the algorithm is eager, satisfying principle 3. 3 Parallelism Psycholinguistic theories vary regarding</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Linguistic knowledge and empirical methods in speech recognition.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4455" citStr="Stolcke, 1997" startWordPosition="689" endWordPosition="690">possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models used for speech are ngram models, in which n − 1 words of history serve as the basis for predicting the nth word. Such models do not have any notion of hierarchical syntactic structure, except as might be visible through an nword window. Aware that the n-gram obscures many linguistically-significant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Lafferty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to significantly improve on 3-grams (Chelba and Jelinek, 1998). Stolcke’s probabilistic Earley parser is one way to use hierarchical phrase structure in a language model. The grammar it parses is a probabilistic context-free phrase structure grammar (PCFG), e.g. 1.0 S → NP VP 0.5 NP → Det N 0.5 NP → NP VP ... ... see (Charniak, 1993, chapter 5) Such a grammar defines a probabilistic language in terms of a stochastic process that rewrites strings of grammar symbols according to the probabilities on the rules. Then each senten</context>
</contexts>
<marker>Stolcke, 1997</marker>
<rawString>Andreas Stolcke. 1997. Linguistic knowledge and empirical methods in speech recognition. AI Magazine, 18(4):25–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Whitney Tabor</author>
<author>Cornell Juliano</author>
<author>Michael Tanenhaus</author>
</authors>
<title>Parsing in a dynamical system: An attractor-based account of the interaction of lexical and structural constraints in sentence processing.</title>
<date>1997</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>12--2</pages>
<marker>Tabor, Juliano, Tanenhaus, 1997</marker>
<rawString>Whitney Tabor, Cornell Juliano, and Michael Tanenhaus. 1997. Parsing in a dynamical system: An attractor-based account of the interaction of lexical and structural constraints in sentence processing. Language and Cognitive Processes, 12(2/3):211–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Wanner</author>
<author>Michael Maratsos</author>
</authors>
<title>An ATN approach to comprehension. In</title>
<date>1978</date>
<booktitle>Linguistic Theory and Psychological Reality, chapter 3,</booktitle>
<pages>119--161</pages>
<editor>Morris Halle, Joan Bresnan, and George A. Miller, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Wanner, Maratsos, 1978</marker>
<rawString>Eric Wanner and Michael Maratsos. 1978. An ATN approach to comprehension. In Morris Halle, Joan Bresnan, and George A. Miller, editors, Linguistic Theory and Psychological Reality, chapter 3, pages 119–161. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions.</title>
<date>1980</date>
<journal>Computing Surveys,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="3424" citStr="Wetherell, 1980" startWordPosition="518" endWordPosition="519">linguistic model, observes each principle. After that will come some simulation results, and then a conclusion. 1 Language models Stolcke’s parsing algorithm was initially applied as a component of an automatic speech recognition system. In speech recognition, one is often interested in the probability that some word will follow, given that a sequence of words has been seen. Given some lexicon of all possible words, a language model assigns a probability to every string of words from the lexicon. This defines a probabilistic language (Grenander, 1967) (Booth and Thompson, 1973) (Soule, 1974) (Wetherell, 1980). A language model helps a speech recognizer focus its attention on words that are likely continuations of what it has recognized so far. This is typically done using conditional probabilities of the form P (Wn = wn|W1 = w1,... Wn−1 = wn−1) the probability that the nth word will actually be wn given that the words leading up to the nth have been w1, w2, ... wn−1. Given some finite lexicon, the probability of each possible outcome for Wn can be estimated using that outcome’s relative frequency in a sample. Traditional language models used for speech are ngram models, in which n − 1 words of his</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>C.S. Wetherell. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>