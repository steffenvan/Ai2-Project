<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005287">
<title confidence="0.875362">
MIMUS: A Multimodal and Multilingual Dialogue System for the Home
Domain
</title>
<author confidence="0.880012">
J. Gabriel Amores
</author>
<affiliation confidence="0.709778">
Julietta Research Group
Universidad de Sevilla
</affiliation>
<email confidence="0.942958">
jgabriel@us.es
</email>
<author confidence="0.890782">
Guillermo P´erez
</author>
<affiliation confidence="0.714142">
Julietta Research Group
Universidad de Sevilla
</affiliation>
<email confidence="0.934246">
gperez@us.es
</email>
<author confidence="0.914041">
Pilar Manch´on
</author>
<affiliation confidence="0.7387785">
Julietta Research Group
Universidad de Sevilla
</affiliation>
<email confidence="0.978392">
pmanchon@us.es
</email>
<sectionHeader confidence="0.993441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942384615385">
This paper describes MIMUS, a multimodal
and multilingual dialogue system for the in–
home scenario, which allows users to con-
trol some home devices by voice and/or
clicks. Its design relies on Wizard of Oz ex-
periments and is targeted at disabled users.
MIMUS follows the Information State Up-
date approach to dialogue management, and
supports English, German and Spanish, with
the possibility of changing language on–the–
fly. MIMUS includes a gestures–enabled
talking head which endows the system with
a human–like personality.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998010714285715">
This paper describes MIMUS, a multimodal and
multilingual dialogue system for the in–home sce-
nario, which allows users to control some home de-
vices by voice and/or clicks. The architecture of
MIMUS was first described in (P´erez et al., 2006c).
This work updates the description and includes a
life demo. MIMUS follows the Information State
Update approach to dialogue management, and has
been developed under the EU–funded TALK project
(Talk Project, 2004). Its architecture consists of a
set of OAA agents (Cheyer and Martin, 1972) linked
through a central Facilitator, as shown in figure 1:
The main agents in MIMUS are briefly described
hereafter:
</bodyText>
<listItem confidence="0.926550571428571">
• The system core is the Dialogue Manager,
which processes the information coming from
the different input modality agents by means of
a natural language understanding module and
provides output in the appropriate modality.
• The main input modality agent is the ASR
Manager, which is obtained through an OAA
</listItem>
<page confidence="0.776493">
1
</page>
<figureCaption confidence="0.993154">
Figure 1: MIMUS Architecture
</figureCaption>
<bodyText confidence="0.912497">
wrapper for Nuance. Currently, the system sup-
ports English, Spanish and German, with the
possibility of changing languages on–the–fly
without affecting the dialogue history.
</bodyText>
<listItem confidence="0.997476533333333">
• The HomeSetup agent displays the house lay-
out, with all the devices and their state. When-
ever a device changes its state, the HomeSetup
is notified and the graphical layout is updated.
• The Device Manager controls the physical de-
vices. When a command is sent, the Device
Manager notifies it to the HomeSetup and the
Knowledge Manager, guaranteeing coherence
in all the elements in MIMUS.
• The GUI Agents control each of the device–
specific GUIs. Thus, clicking on the telephone
icon, a telephone GUI will be displayed, and so
on for each type of service.
• The Knowledge Manager connects all the
agents to the common knowledge resource by
</listItem>
<subsubsectionHeader confidence="0.407956">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 1–4,
</subsubsectionHeader>
<bodyText confidence="0.970327918918919">
Prague, June 2007. c�2007 Association for Computational Linguistics
means of an OWL Ontology. Modality and Time features have been added in
• The Talking Head. MIMUS virtual charac- order to implement fusion strategies at dialogue
ter is synchronized with Loquendo’s TTS, and level.
has the ability to express emotions and play 3.2 Updating the Information State in MIMUS
some animations such as nodding or shaking This section provides an example of how the In-
the head. formation State Update approach is implemented
2 WoZ Experiments in MIMUS. Update rules are triggered by dialogue
MIMUS has been developed taking into account moves (any dialogue move whose DTAC structure
wheel–chair bound users. In order to collect first– unifies with the Attribute–Value pairs defined in the
hand information about the users’ natural behavior TriggeringCondition field) and may require addi-
in this scenario, several WoZ experiments were first tional information, defined as dialogue expectations
conducted. A rather sophisticated multilingual WoZ (again, those dialogue moves whose DTAC structure
experimental platform was built for this purpose. unify with the Attribute–Value pairs defined in the
The set of WoZ experiments conducted was de- DeclareExpectations field).
signed in order to collect data. In turn, these Consider the following DTAC, which represents
data helped determine the relevant factors to con- the information state returned by the NLU module
figure multimodal dialogue systems in general, and for the sentence switch on:
MIMUS in particular. �
A detailed description of the results obtained after DMOVE specifyCommand
the analysis of the experiments and their impact on TYPE SwitchOn �
the overall design of the system may be found in ARGS [Location, DeviceType] � �
(Manch´on et al., 2007). META INFO [CONFIDENCE
3 ISU–based Dialogue Management in ODALITY VOICE
MIMUS INIT 00:00:00
As pointed out above, MIMUS follows the ISU END 00:00:30
approach to dialogue management (Larsson and 700
Traum, 2000). The main element of the ISU ap- Consider now the (simplified) dialogue rule
proach in MIMUS is the dialogue history, repre- “ON”, defined as follows:
sented formally as a list of dialogue states. Dia- RuleID: ON;
logue rules update this information structure either TriggeringCondition:
by producing new dialogue states or by supplying (DMOVE:specifyCommand,
arguments to existing ones. TYPE:SwitchOn);
3.1 Multimodal DTAC structure DeclareExpectations: {
The information state in MIMUS is represented as a Location,
feature structure with four main attributes: Dialogue DeviceType }
Move, Type, Arguments and Contents. ActionsExpectations: {
</bodyText>
<listItem confidence="0.99996575">
• DMOVE: Identifies the kind of dialogue move. [DeviceType] =&gt;
• TYPE: This feature identifies the specific dia- {NLG(DeviceType);} }
logue move in the particular domain at hand. PostActions: {
• ARGS: The ARGS feature specifies the argu- ExecuteAction(@is-ON); }
</listItem>
<bodyText confidence="0.9574951">
ment structure of the DMOVE/TYPE pair. The DTAC obtained for switch on triggers the
2 dialogue rule ON. However, since two declared
expectations are still missing (Location and De-
viceType), the dialogue manager will activate the
ActionExpectations and prompt the user for the
kind of device she wants to switch on, by means
of a call to the natural language generation mod-
ule NLG(DeviceType). Once all expectations have
been fulfilled, the PostActions can be executed over
the desired device(s).
</bodyText>
<sectionHeader confidence="0.989868" genericHeader="method">
4 Integrating OWL in MIMUS
</sectionHeader>
<bodyText confidence="0.999536">
Initially, OWL Ontologies were integrated in
MIMUS in order to improve its knowledge manage-
ment module. This functionality implied the imple-
mentation of a new OAA wrapper capable of query-
ing OWL ontologies, see (P´erez et al., 2006b) for
details.
</bodyText>
<subsectionHeader confidence="0.984689">
4.1 From Ontologies to Grammars: OWL2Gra
</subsectionHeader>
<bodyText confidence="0.999986428571429">
OWL ontologies play a central role in MIMUS. This
role is limited, though, to the input side of the sys-
tem. The domain–dependent part of multimodal and
multilingual production rules for context–free gram-
mars is semi–automatically generated from an OWL
ontology.
This approach has achieved several goals: it lever-
ages the manual work of the linguist, and ensures
coherence and completeness between the Domain
Knowledge (Knowledge Manager Module) and the
Linguistic Knowledge (Natural Language Under-
standing Module) in the application. A detailed ex-
planation of the algorithm and the results obtained
can be found in (P´erez et al., 2006a)
</bodyText>
<subsectionHeader confidence="0.917429">
4.2 From OWL to the House Layout
</subsectionHeader>
<bodyText confidence="0.999687916666666">
MIMUS home layout does not consist of a pre–
defined static structure only usable for demonstra-
tion purposes. Instead, it is dynamically loaded at
execution time from the OWL ontology where all
the domain knowledge is stored, assuring the coher-
ence of the layout with the rest of the system.
This is achieved by means of an OWL–RDQL
wrapper. It is through this agent that the Home Setup
enquires for the location of the walls, the label of the
rooms, the location and type of devices per room and
so forth, building the 3D graphical image from these
data.
</bodyText>
<sectionHeader confidence="0.994489" genericHeader="method">
5 Multimodal Fusion Strategies
</sectionHeader>
<bodyText confidence="0.999963285714286">
MIMUS approach to multimodal fusion involves
combining inputs coming from different multimodal
channels at dialogue level (P´erez et al., 2005). The
idea is to check the multimodal input pool before
launching the actions expectations while waiting for
an “inter–modality” time. This strategy assumes
that each individual input can be considered as an
independent dialogue move. In this approach, the
multimodal input pool receives and stores all in-
puts including information such as time and modal-
ity. The Dialogue Manager checks the input pool
regularly to retrieve the corresponding input. If
more than one input is received during a certain time
frame, they are considered simultaneous or pseudo–
simultaneous. In this case, further analysis is needed
in order to determine whether those independent
multimodal inputs are truly related or not. Another,
improved strategy has been proposed at (Manch´on
et al., 2006), which combines the advantages of this
one, and those proposed for unification–based gram-
mars (Johnston et al., 1997; Johnston, 1998).
</bodyText>
<sectionHeader confidence="0.990217" genericHeader="method">
6 Multimodal Presentation in MIMUS
</sectionHeader>
<bodyText confidence="0.999888">
MIMUS offers graphical and voice output to the
users through an elaborate architecture composed of
a TTS Manager, a HomeSetup and GUI agents. The
multimodal presentation architecture in MIMUS
consists of three sequential modules. The current
version is a simple implementation that may be ex-
tended to allow for more complex theoretical issues
hereby proposed. The main three modules are:
</bodyText>
<listItem confidence="0.9888924375">
• Content Planner (CP): This module decides
on the information to be provided to the user.
As pointed out by (Wahlster et al., 1993), the
CP cannot determine the content independently
from the presentation planner (PP). In MIMUS,
the CP generates a set of possibilities, from
which the PP will select one, depending on
their feasibility.
• Presentation Planner (PP): The PP receives the
set of possible content representations and se-
lects the “best” one.
• Realization Module (RM): This module takes
the presentation generated and selected by
the CP–PP, divides the final DTAC structure
and sends each substructure to the appropriate
agent for rendering.
</listItem>
<sectionHeader confidence="0.893192" genericHeader="method">
7 The MIMUS Talking Head
</sectionHeader>
<bodyText confidence="0.9947845">
MIMUS virtual character is known as Ambrosio.
Endowing the character with a name results in per-
</bodyText>
<page confidence="0.997787">
3
</page>
<bodyText confidence="0.999969043478261">
sonalization, personification, and voice activation.
Ambrosio will remain inactive until called for duty
(voice activation); each user may name their per-
sonal assistant as they wish (Personalization); and
they will address the system at personal level, re-
inforcing the sense of human–like communication
(Personification). The virtual head has been imple-
mented in 3D to allow for more natural and realis-
tic gestures and movements. The graphical engine
used is OGRE (OGRE, 2006), a powerful, free and
easy to use tool. The current talking head is inte-
grated with Loquendo, a high quality commercial
synthesizer that launches the information about the
phonemes as asynchronous events, which allows for
lip synchronization. The dialogue manager controls
the talking head, and sends the appropriate com-
mands depending of the dialogue needs. Through-
out the dialogue, the dialogue manager may see it
fit to reinforce the communication channel with ges-
tures and expressions, which may or may not imply
synthesized utterances. For instance, the head may
just nod to acknowledge a command, without utter-
ing words.
</bodyText>
<sectionHeader confidence="0.995515" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99996205882353">
In this paper, an overall description of the MIMUS
system has been provided.
MIMUS is a fully multimodal and multilingual di-
alogue system within the Information State Update
approach. A number of theoretical and practical is-
sues have been addressed successfully, resulting in a
user–friendly, collaborative and humanized system.
We concluded from the experiments that a
human–like talking head would have a significant
positive impact on the subjects’ perception and will-
ingness to use the system.
Although no formal evaluation of the system has
taken place, MIMUS has already been presented
successfully in different forums, and as expected,
“Ambrosio” has always made quite an impression,
making the system more appealing to use and ap-
proachable.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973134">
Adam Cheyer and David Martin. 2001. The open
agent architecture. Journal ofAutonomous Agents and
Multi–Agent Systems, 4(12):143–148.
Michael Johnston, Philip R. Cohen, David McGee,
Sharon L. Oviatt, James A. Pitman and Ira A. Smith.
1997. Unification–based Multimodal Integration ACL
281–288.
Michael Johnston. 1998. Unification–based Multimodal
Parsing Coling–ACL 624–630.
Staffan Larsson and David Traum. 2000. Information
State and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language Engi-
neering, 6(34): 323-340.
Pilar Manch´on, Guillermo P´erez and Gabriel Amores.
2006. Multimodal Fusion: A New Hybrid Strategy
for Dialogue Systems. Proceedings of International
Congress of Multimodal Interfaces (ICMI06), 357–
363. ACM, New York, USA.
Pilar Manch´on, Carmen Del Solar, Gabriel Amores and
Guillermo P´erez. 2007. Multimodal Event Analysis
in the MIMUS Corpus. Multimodal Corpora: Special
Issue of the International Journal JLRE, submitted.
OGRE. 2006. Open Source Graphics Engine.
www.ogre3d.org
Guillermo P´erez, Gabriel Amores and Pilar Manch´on.
2005. Two Strategies for multimodal fusion. E.V.
Zudilova–Sainstra and T. Adriaansen (eds.) Proceed-
ings of Multimodal Interaction for the Visualization
and Exploration of Scientific Data, 26–32. Trento,
Italy.
Guillermo P´erez, Gabriel Amores, Pilar Manch´on and
David Gonz´alez Maline. 2006. Generating Multilin-
gual Grammars from OWL Ontologies. Research in
Computing Science, 18:3–14.
Guillermo P´erez, Gabriel Amores, Pilar Manch´on, Fer-
nando G´omez and Jes´us Gonz´alez. 2006. Integrating
OWL Ontologies with a Dialogue Manager. Proce-
samiento del Lenguaje Natural 37:153–160.
Guillermo P´erez, Gabriel Amores and Pilar Manch´on.
2006. A Multimodal Architecture For Home Con-
trol By Disabled Users. Proceedings of the IEEE/ACL
2006 Workshop on Spoken Language Technology,
134–137. IEEE, New York, USA.
Talk Project. Talk and Look: Linguistic Tools for Am-
bient Linguistic Knowledge. 2004. 6th Framework
Programme. www.talk-project.org
Wolfgang Wahlster, Elisabeth Andr´e, Wolfgang Finkler,
Hans–J¨urgen Profitlich and Thomas Rist. 1993. Plan–
Based integration of natural language and graphics
generation. Artificial intelligence, 63:287–247.
</reference>
<page confidence="0.996682">
4
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551980">
<title confidence="0.996061">MIMUS: A Multimodal and Multilingual Dialogue System for the Home Domain</title>
<author confidence="0.999274">J Gabriel Amores</author>
<affiliation confidence="0.992659">Julietta Research Group Universidad de Sevilla</affiliation>
<email confidence="0.909114">jgabriel@us.es</email>
<author confidence="0.997424">Guillermo P´erez</author>
<affiliation confidence="0.99462">Julietta Research Group Universidad de Sevilla</affiliation>
<email confidence="0.904964">gperez@us.es</email>
<author confidence="0.714889">Pilar Manch´on</author>
<affiliation confidence="0.9944725">Julietta Research Group Universidad de Sevilla</affiliation>
<email confidence="0.979639">pmanchon@us.es</email>
<abstract confidence="0.998257571428571">This paper describes MIMUS, a multimodal and multilingual dialogue system for the in– home scenario, which allows users to control some home devices by voice and/or clicks. Its design relies on Wizard of Oz experiments and is targeted at disabled users. MIMUS follows the Information State Update approach to dialogue management, and supports English, German and Spanish, with the possibility of changing language on–the– fly. MIMUS includes a gestures–enabled talking head which endows the system with a human–like personality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Cheyer</author>
<author>David Martin</author>
</authors>
<title>The open agent architecture.</title>
<date>2001</date>
<journal>Journal ofAutonomous Agents and Multi–Agent Systems,</journal>
<volume>4</volume>
<issue>12</issue>
<marker>Cheyer, Martin, 2001</marker>
<rawString>Adam Cheyer and David Martin. 2001. The open agent architecture. Journal ofAutonomous Agents and Multi–Agent Systems, 4(12):143–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
<author>Philip R Cohen</author>
<author>David McGee</author>
<author>Sharon L Oviatt</author>
<author>James A Pitman</author>
<author>Ira A Smith</author>
</authors>
<date>1997</date>
<journal>Unification–based Multimodal Integration ACL</journal>
<pages>281--288</pages>
<contexts>
<context position="8763" citStr="Johnston et al., 1997" startWordPosition="1361" endWordPosition="1364">ut pool receives and stores all inputs including information such as time and modality. The Dialogue Manager checks the input pool regularly to retrieve the corresponding input. If more than one input is received during a certain time frame, they are considered simultaneous or pseudo– simultaneous. In this case, further analysis is needed in order to determine whether those independent multimodal inputs are truly related or not. Another, improved strategy has been proposed at (Manch´on et al., 2006), which combines the advantages of this one, and those proposed for unification–based grammars (Johnston et al., 1997; Johnston, 1998). 6 Multimodal Presentation in MIMUS MIMUS offers graphical and voice output to the users through an elaborate architecture composed of a TTS Manager, a HomeSetup and GUI agents. The multimodal presentation architecture in MIMUS consists of three sequential modules. The current version is a simple implementation that may be extended to allow for more complex theoretical issues hereby proposed. The main three modules are: • Content Planner (CP): This module decides on the information to be provided to the user. As pointed out by (Wahlster et al., 1993), the CP cannot determine </context>
</contexts>
<marker>Johnston, Cohen, McGee, Oviatt, Pitman, Smith, 1997</marker>
<rawString>Michael Johnston, Philip R. Cohen, David McGee, Sharon L. Oviatt, James A. Pitman and Ira A. Smith. 1997. Unification–based Multimodal Integration ACL 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
</authors>
<title>Unification–based Multimodal Parsing Coling–ACL</title>
<date>1998</date>
<pages>624--630</pages>
<contexts>
<context position="8780" citStr="Johnston, 1998" startWordPosition="1365" endWordPosition="1366">ores all inputs including information such as time and modality. The Dialogue Manager checks the input pool regularly to retrieve the corresponding input. If more than one input is received during a certain time frame, they are considered simultaneous or pseudo– simultaneous. In this case, further analysis is needed in order to determine whether those independent multimodal inputs are truly related or not. Another, improved strategy has been proposed at (Manch´on et al., 2006), which combines the advantages of this one, and those proposed for unification–based grammars (Johnston et al., 1997; Johnston, 1998). 6 Multimodal Presentation in MIMUS MIMUS offers graphical and voice output to the users through an elaborate architecture composed of a TTS Manager, a HomeSetup and GUI agents. The multimodal presentation architecture in MIMUS consists of three sequential modules. The current version is a simple implementation that may be extended to allow for more complex theoretical issues hereby proposed. The main three modules are: • Content Planner (CP): This module decides on the information to be provided to the user. As pointed out by (Wahlster et al., 1993), the CP cannot determine the content indep</context>
</contexts>
<marker>Johnston, 1998</marker>
<rawString>Michael Johnston. 1998. Unification–based Multimodal Parsing Coling–ACL 624–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>David Traum</author>
</authors>
<title>Information State and dialogue management in the TRINDI Dialogue Move Engine Toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>34</issue>
<pages>323--340</pages>
<marker>Larsson, Traum, 2000</marker>
<rawString>Staffan Larsson and David Traum. 2000. Information State and dialogue management in the TRINDI Dialogue Move Engine Toolkit. Natural Language Engineering, 6(34): 323-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pilar Manch´on</author>
<author>Guillermo P´erez</author>
<author>Gabriel Amores</author>
</authors>
<title>Multimodal Fusion: A New Hybrid Strategy for Dialogue Systems.</title>
<date>2006</date>
<booktitle>Proceedings of International Congress of Multimodal Interfaces (ICMI06), 357– 363.</booktitle>
<publisher>ACM,</publisher>
<location>New York, USA.</location>
<marker>Manch´on, P´erez, Amores, 2006</marker>
<rawString>Pilar Manch´on, Guillermo P´erez and Gabriel Amores. 2006. Multimodal Fusion: A New Hybrid Strategy for Dialogue Systems. Proceedings of International Congress of Multimodal Interfaces (ICMI06), 357– 363. ACM, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pilar Manch´on</author>
<author>Carmen Del Solar</author>
<author>Gabriel Amores</author>
<author>Guillermo P´erez</author>
</authors>
<title>Multimodal Event Analysis in the MIMUS Corpus.</title>
<date>2007</date>
<journal>Multimodal Corpora: Special Issue of the International Journal JLRE, submitted. OGRE.</journal>
<note>www.ogre3d.org</note>
<marker>Manch´on, Solar, Amores, P´erez, 2007</marker>
<rawString>Pilar Manch´on, Carmen Del Solar, Gabriel Amores and Guillermo P´erez. 2007. Multimodal Event Analysis in the MIMUS Corpus. Multimodal Corpora: Special Issue of the International Journal JLRE, submitted. OGRE. 2006. Open Source Graphics Engine. www.ogre3d.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillermo P´erez</author>
<author>Gabriel Amores</author>
<author>Pilar Manch´on</author>
</authors>
<title>Two Strategies for multimodal fusion.</title>
<date>2005</date>
<booktitle>Proceedings of Multimodal Interaction for the Visualization and Exploration of Scientific Data,</booktitle>
<pages>26--32</pages>
<editor>E.V. Zudilova–Sainstra and T. Adriaansen (eds.)</editor>
<location>Trento, Italy.</location>
<marker>P´erez, Amores, Manch´on, 2005</marker>
<rawString>Guillermo P´erez, Gabriel Amores and Pilar Manch´on. 2005. Two Strategies for multimodal fusion. E.V. Zudilova–Sainstra and T. Adriaansen (eds.) Proceedings of Multimodal Interaction for the Visualization and Exploration of Scientific Data, 26–32. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillermo P´erez</author>
<author>Gabriel Amores</author>
<author>Pilar Manch´on</author>
<author>David Gonz´alez Maline</author>
</authors>
<date>2006</date>
<booktitle>Generating Multilingual Grammars from OWL Ontologies. Research in Computing Science,</booktitle>
<pages>18--3</pages>
<marker>P´erez, Amores, Manch´on, Maline, 2006</marker>
<rawString>Guillermo P´erez, Gabriel Amores, Pilar Manch´on and David Gonz´alez Maline. 2006. Generating Multilingual Grammars from OWL Ontologies. Research in Computing Science, 18:3–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillermo P´erez</author>
<author>Gabriel Amores</author>
<author>Pilar Manch´on</author>
<author>Fernando G´omez</author>
<author>Jes´us Gonz´alez</author>
</authors>
<date>2006</date>
<booktitle>Integrating OWL Ontologies with a Dialogue Manager. Procesamiento del Lenguaje Natural</booktitle>
<pages>37--153</pages>
<marker>P´erez, Amores, Manch´on, G´omez, Gonz´alez, 2006</marker>
<rawString>Guillermo P´erez, Gabriel Amores, Pilar Manch´on, Fernando G´omez and Jes´us Gonz´alez. 2006. Integrating OWL Ontologies with a Dialogue Manager. Procesamiento del Lenguaje Natural 37:153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillermo P´erez</author>
<author>Gabriel Amores</author>
<author>Pilar Manch´on</author>
</authors>
<title>A Multimodal Architecture For Home Control By Disabled Users.</title>
<date>2006</date>
<booktitle>Proceedings of the IEEE/ACL 2006 Workshop on Spoken Language Technology,</booktitle>
<pages>134--137</pages>
<publisher>IEEE,</publisher>
<location>New York, USA.</location>
<marker>P´erez, Amores, Manch´on, 2006</marker>
<rawString>Guillermo P´erez, Gabriel Amores and Pilar Manch´on. 2006. A Multimodal Architecture For Home Control By Disabled Users. Proceedings of the IEEE/ACL 2006 Workshop on Spoken Language Technology, 134–137. IEEE, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Talk Project</author>
</authors>
<title>Talk and Look: Linguistic Tools for Ambient Linguistic Knowledge.</title>
<date>2004</date>
<note>6th Framework Programme. www.talk-project.org</note>
<contexts>
<context position="1317" citStr="Project, 2004" startWordPosition="194" endWordPosition="195">ility of changing language on–the– fly. MIMUS includes a gestures–enabled talking head which endows the system with a human–like personality. 1 Introduction This paper describes MIMUS, a multimodal and multilingual dialogue system for the in–home scenario, which allows users to control some home devices by voice and/or clicks. The architecture of MIMUS was first described in (P´erez et al., 2006c). This work updates the description and includes a life demo. MIMUS follows the Information State Update approach to dialogue management, and has been developed under the EU–funded TALK project (Talk Project, 2004). Its architecture consists of a set of OAA agents (Cheyer and Martin, 1972) linked through a central Facilitator, as shown in figure 1: The main agents in MIMUS are briefly described hereafter: • The system core is the Dialogue Manager, which processes the information coming from the different input modality agents by means of a natural language understanding module and provides output in the appropriate modality. • The main input modality agent is the ASR Manager, which is obtained through an OAA 1 Figure 1: MIMUS Architecture wrapper for Nuance. Currently, the system supports English, Spani</context>
</contexts>
<marker>Project, 2004</marker>
<rawString>Talk Project. Talk and Look: Linguistic Tools for Ambient Linguistic Knowledge. 2004. 6th Framework Programme. www.talk-project.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Elisabeth Andr´e</author>
<author>Wolfgang Finkler</author>
<author>Hans–J¨urgen Profitlich</author>
<author>Thomas Rist</author>
</authors>
<title>Plan– Based integration of natural language and graphics generation.</title>
<date>1993</date>
<journal>Artificial intelligence,</journal>
<pages>63--287</pages>
<marker>Wahlster, Andr´e, Finkler, Profitlich, Rist, 1993</marker>
<rawString>Wolfgang Wahlster, Elisabeth Andr´e, Wolfgang Finkler, Hans–J¨urgen Profitlich and Thomas Rist. 1993. Plan– Based integration of natural language and graphics generation. Artificial intelligence, 63:287–247.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>