<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.138425">
<title confidence="0.9969925">
Embracing Ambiguity: A Comparison of Annotation Methodologies for
Crowdsourcing Word Sense Labels
</title>
<author confidence="0.99873">
David Jurgens
</author>
<affiliation confidence="0.998289">
Department of Computer Science
University of California, Los Angeles
</affiliation>
<email confidence="0.99852">
jurgens@cs.ucla.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999643733333333">
Word sense disambiguation aims to identify
which meaning of a word is present in a given
usage. Gathering word sense annotations is a
laborious and difficult task. Several methods
have been proposed to gather sense annota-
tions using large numbers of untrained anno-
tators, with mixed results. We propose three
new annotation methodologies for gathering
word senses where untrained annotators are
allowed to use multiple labels and weight the
senses. Our findings show that given the ap-
propriate annotation task, untrained workers
can obtain at least as high agreement as anno-
tators in a controlled setting, and in aggregate
generate equally as good of a sense labeling.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949137254902">
Word sense annotation is regarded as one of the most
difficult annotation tasks (Artstein and Poesio, 2008)
and building manually-annotated corpora with high-
quality sense labels is often a time- and resource-
consuming task. As a result, nearly all sense-tagged
corpora in wide-spread use are created using trained
annotators (Hovy et al., 2006; Passonneau et al.,
2010), which results in a knowledge acquisition bot-
tleneck for training systems that require sense labels
(Gale et al., 1992). In other NLP areas, this bot-
tleneck has been addressed through gathering anno-
tations using many untrained workers on platforms
such as Amazon Mechanical Turk (MTurk), a task
commonly referred to as crowdsourcing. Recently,
several works have proposed gathering sense anno-
tations using crowdsourcing (Snow et al., 2008; Bie-
mann and Nygaard, 2010; Passonneau et al., 2012b;
Rumshisky et al., 2012). However, these meth-
ods produce sense labels that are different from the
commonly used sense inventories such as WordNet
(Fellbaum, 1998) or OntoNotes (Hovy et al., 2006).
Furthermore, while Passonneau et al. (2012b) did
use WordNet sense labels, they found the quality
was well below that of trained experts.
We revisit the task of crowdsourcing word sense
annotations, focusing on two key aspects: (1) the
annotation methodology itself, and (2) the restric-
tion to single sense assignment. First, the choice in
sense inventory plays an important role in gathering
high-quality annotations; fine-grained inventories
such as WordNet often contain several related senses
for polysemous words, which untrained annotators
find difficult to correctly apply in a given context
(Chugur et al., 2002; McCarthy, 2006; Palmer et
al., 2007; Rumshisky and Batiukova, 2008; Brown
et al., 2010). However, many agreement studies
have restricted annotators to using a single sense,
which can significantly lower inter-annotator agree-
ment (IAA) in the presence of ambiguous or poly-
semous usages; indeed, multiple studies have shown
that when allowed, annotators readily assign multi-
ple senses to a single usage (V´eronis, 1998; Mur-
ray and Green, 2004; Erk et al., 2009; Passonneau
et al., 2012b). Therefore, we focus on annotation
methodologies that enable workers to use as many
labels as they feel appropriate, asking the question:
if allowed to make labeling ambiguity explicit, will
annotators agree? Furthermore, we adopt the goal
of Erk et al. (2009), which enabled annotators to
weight each sense by its applicability to the given
context, thereby quantifying the ambiguity.
</bodyText>
<page confidence="0.977757">
556
</page>
<subsectionHeader confidence="0.295411">
Proceedings of NAACL-HLT 2013, pages 556–562,
</subsectionHeader>
<bodyText confidence="0.960454777777778">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
This paper provides the following contributions.
First, we demonstrate that the choice in annotation
setup can significantly improve IAA and that the la-
bels of untrained workers follow consistent patterns
that enable creating high quality labeling from their
aggregate. Second, we find that the sense labeling
from crowdsourcing matches performance with an-
notators in a controlled setting.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.994246764705882">
Given the potential utility of a sense-labeled corpus,
multiple studies have examined how to efficiently
gather high quality sense annotations. Snow et al.
(2008) had MTurk workers, referred to as Turkers,
disambiguate uses of “president.” While they re-
ported extremely high IAA (0.952), their analysis
was only performed on a single word.
Biemann and Nygaard (2010) and Biemann
(2012) construct a sense-labeled corpus by concur-
rently constructing the sense inventory itself. Turk-
ers used a lexical substitution task to identify valid
substitutions of a target word. The contexts for the
resulting substitutions were clustered based on their
word overlap and the resulting clusters were labeled
as senses. Biemann and Nygaard (2010) showed that
the number of sense definitions for a word in their
inventory was correlated with the number in Word-
Net, often with their inventory having fewer senses
by combining related meanings and omitting rare
meanings.
Hong and Baker (2011) evaluated multiple anno-
tation strategies for gathering FrameNet sense anno-
tations, ultimately yielding high (&gt;90%) accuracy
for most terms after filtering. They highlight am-
biguous and polysemous usages as a notable source
of errors, which the present work directly addresses.
In the most related work, Passonneau et al.
(2012b) had Turkers annotate contexts using one or
more senses, with the requirement that a worker la-
bels all contexts. While they found that agreement
between all workers was low, their annotations could
be combined using the GLAD model (Whitehill et
al., 2000) to obtain good performance, though not
as good as trained annotators.
</bodyText>
<sectionHeader confidence="0.989062" genericHeader="method">
3 Annotation Methodologies
</sectionHeader>
<bodyText confidence="0.999901040816327">
We consider three methodologies for gathering
sense labels: (1) the methodology of Erk et al.
(2009) for gathering weighted labels, (2) a multi-
stage strategy that uses both binary and Likert rat-
ings, and (3) MaxDiff, a paired choice format.
Likert Ratings Likert rating scales provide the
most direct way of gathering weighted sense labels;
Turkers are presented with all senses of a word and
then asked to rate each on a numeric scale. We adopt
the annotation guidelines of Erk et al. (2009) which
used a five-point scale, ranging from 1 to 5, indicat-
ing the sense does not apply or that it matches the
contextual usage exactly, respectively.
Select and Rate Recent efforts in crowdsourc-
ing have proposed multi-stage processes for accom-
plishing complex tasks, where efforts by one group
of workers are used to create new subtasks for other
workers to complete (Bernstein et al., 2010; Kittur
et al., 2011; Kulkarni et al., 2012). We propose a
two-stage strategy that aims to reduce the complex-
ity of the annotation task, referred to as Select and
Rate (S+R). First, Turkers are presented with all the
senses and asked to make a binary choice of which
senses apply. Second, a Likert rating task is created
for only those senses whose selection frequency is
above a threshold, thereby concentrating worker fo-
cus on a potentially smaller set of senses.
Our motivation for S+R is two-fold. First, the
sense definitions of certain words may be unclear
or misinterpreted by a minority of the Turkers, who
then systematically rate inapplicable senses as appli-
cable. The Select task can potentially remove such
noise and therefore improve both IAA and rating
quality in the subsequent Rate task. Second, while
the present study analyzes words with 4–8 senses,
we are ultimately interested in annotating highly
polysemous words with tens of senses, which could
present a significant cognitive burden for an annota-
tor to rate concurrently. Here, the Select stage can
potentially reduce the number of senses presented,
leading to less cognitive burden in the Rate stage.
Furthermore, as a pragmatic benefit, removing in-
applicable senses reduces the visual space required
for displaying the questions on the MTurk platform,
which can improve annotation throughput.
MaxDiff MaxDiff is an alternative to scale-based
ratings in which Turkers are presented with a only
subset of all of a word’s senses and then asked to se-
lect (1) the sense option that best matches the mean-
</bodyText>
<page confidence="0.991272">
557
</page>
<table confidence="0.999803727272727">
add.v ask.v win.v argument.n interest.n paper.n different.a important.a
Erk et al. (2009) IAA 0.470 0.354 0.072 0.497 0.320 0.403 0.212 0.466
MTurk Likert IAA 0.336 0.212 0.129 0.250 0.209 0.522 0.030 0.240
MTurk Select 0.309 0.127 0.179 0.192 0.164 0.449 0.024 0.111
MTurk Rate 0.204 0.076 0.026 0.005 0.081 0.108 0.005 0.116
MTurk MaxDiff 0.493 0.353 0.295 - 0.349 0.391 0.220 0.511
Likert Mode 0.500 0.369 0.083 0.445 0.388 0.518 0.124 0.516
S+R Median 0.473 0.394 0.149 0.497 0.390 0.497 0.103 0.416
MTurk MaxDiff 0.508 0.412 0.184 - 0.408 0.496 0.115 0.501
Sampled Baseline 0.238 0.178 0.042 0.254 0.162 0.205 0.100 0.221
Random Baseline 0.239 0.186 0.045 0.249 0.269 0.200 0.110 0.269
</table>
<tableCaption confidence="0.999929">
Table 1: IAA per word (top) and IAA between aggregate labelings and the GWS annotators (bottom)
</tableCaption>
<bodyText confidence="0.999986428571429">
ing in the example context and (2) the sense option
that least matches (Louviere, 1991). In our setting,
we presented three options at a time for words with
fewer than seven senses, and four options for those
with seven senses. For a single context, multiple
subsets of the senses are presented and then their rel-
ative ranking is used to produce the numeric rating.
The final applicability ratings were produced using
a modification of the counting procedure of Orme
(2009). First, all sense ratings are computed as the
number of times the sense was rated best minus the
number of times rated least. Second, all negatively-
rated senses are assigned score of 1, and all posi-
tively ratings are normalized to be (1, 5].
</bodyText>
<sectionHeader confidence="0.999355" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999965541666667">
For measuring the difference in methodologies, we
propose three experiments based on different anal-
yses of comparing Turker and non-Turker annota-
tions on the same dataset, the latter of which we re-
fer to as the reference labeling. First, we measure
the ability of the Turkers individually by evaluat-
ing their IAA with the reference labeling. Second,
many studies using crowdsourcing combine the re-
sults into a single answer, thereby leveraging the
wisdom of the crowds (Surowiecki, 2005) to smooth
over inconsistencies in the data. Therefore, in the
second experiment, we evaluate different methods
of combining Turker responses into a single sense
labeling, referred to as an aggregate labeling, and
comparing that with the reference labeling. Third,
we measure the replicability of the Turker annota-
tions (Kilgarriff, 1999) using a sampling methodol-
ogy. Two equally-sized sets of Turker annotations
are created by randomly sampling without replace-
ment from the full set of annotations for each item.
IAA is calculated between the aggregate labelings
computed from each set. This sampling is repeated
50 times and we report the mean IAA as a measure
of the expected degree of replicability when anno-
tating using different groups of Turkers.
For the reference sense labeling, we use a subset
of the GWS dataset of Erk et al. (2009), where three
annotators rated 50 instances each for eight words.
For clarity, we refer to these individuals as the GWS
annotators. Given a word usage in a sentence, GWS
annotators rated the applicability of all WordNet 3.0
senses using the same Likert scale as described in
Section 3. Contexts were drawn evenly from the
SemCor (Miller et al., 1993) and SENSEVAL-3 lex-
ical substitution (Mihalcea et al., 2004) corpora.
GWS annotators were apt to use multiple senses,
with nearly all instances having multiple labels.
For each annotation task, Turkers were presented
with an identical set of annotation guidelines, fol-
lowed by methodology-specific instructions.1 To in-
crease the familiarity with the task, four instances
were shown per task, with all instances using the
same target word. Unlike Passonneau et al. (2012b),
we did not require a Turker to annotate all contexts
for a single word; however many Turkers did com-
plete the majority of instances. Both the Likert, Se-
lect, and Rate tasks used ten Turkers each. Senses
were passed from Select to Rate if they received at
</bodyText>
<footnote confidence="0.981537">
1Full guidelines are available at http://cs.ucla.
edu/˜jurgens/sense-annotation/
</footnote>
<page confidence="0.990232">
558
</page>
<bodyText confidence="0.999421176470588">
least three votes. For MaxDiff, we gathered at least
3n annotations per context where n is the number of
senses of the target word, ensuring that each sense
appeared at least once. Due to resource limitations,
we omitted the evaluation of argument.n for MaxD-
iff. Following the recommendation of Kosinski et al.
(2012), Turkers were paid $0.05USD for each Lik-
ert, Select, and Rate task. For MaxDiff, due to their
shorter nature and comparably high volume, Turkers
were paid $0.03USD per task.
To ensure fluency in English as well as reduce the
potential for low-quality results, we prefaced each
task with a simple test question that asked the Turker
to pick out a definition of the target word from a list
of four options. The incorrect options were selected
so that they would be nonsensical for anyone famil-
iar with the target word. Additionally, we rejected
all Turker responses where more than one option
was missing a rating. In the case of missing ratings,
we infer a rating of 1. Approximately 20-30% of the
submissions were rejected by these criteria, under-
scoring the importance of filtering.
For measuring IAA, we selected Krippendorff’s
α (Krippendorff, 1980; Artstein and Poesio, 2008),
which is an agreement coefficient that handles miss-
ing data, as well as different levels of measurement,
e.g., nominal data (Select and MaxDiff) and interval
data (Likert and Rate).2 Krippendorff’s α adjusts for
chance, ranging between [−1, 1] for nominal data
and (−1, 1] for interval data, where 1 indicates per-
fect agreement and -1 indicates systematic disagree-
ment; random labels would have an expected α of
zero. We treat each sense and instance combination
as a separate item to rate.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99659545">
The results of the first experiment appear in the top
of Table 1. Two important aspects emerge. First, the
word itself plays a significant role in IAA. Though
Erk et al. (2009) reported a pair-wise IAA of the
GWS annotators between 0.466 and 0.506 using
Spearman’s p, the IAA varies considerably between
words for both Turkers and GWS annotators when
measured using Krippendorff’s α.
Second, the choice of annotation methodology
2We note that although the ratings are technically given on
an ordinal scale (ranks), we use the interval scale to allow com-
parison with rational ratings from the aggregate solutions.
significantly impacts IAA. While both the Likert and
S+R tasks have lower IAA than the GWS annota-
tors do, the MaxDiff annotators achieve higher IAA
for almost all words. We hypothesize that compar-
ing senses for applicability is an easier task for the
untrained worker, rather than having to construct a
mental scale of what constitutes the applicability of
each sense. Surprisingly, the binary Select task has
a lower IAA than the more complex the Likert task.
An analysis of the duration of median task comple-
tion times for the Likert and Select tasks showed lit-
tle difference (with the exception of paper.n, which
was on average 50 second faster for Likert ratings),
suggesting that both tasks are equally as cognitively
demanding. In addition, the Rate task has the lowest
IAA, despite its similarity to the Likert task. An in-
spection of the annotations shows that the full rating
scale was used, so the low value is not due to Turk-
ers always using the same rating, which would yield
an IAA near chance.
In the second experiment, we created a aggregate
sense labeling and compared its IAA with the GWS
annotators, shown in Table 1 (bottom). For scale-
based ratings, we considered three arithmetic oper-
ations for selecting the final rating: mode, median,
and mean. We found that the mode yielded the high-
est average IAA for the Likert ratings and median for
S+R; however, the differences in IAA using each op-
eration were often small. We compare the IAA with
GWS annotators against two baselines: one gener-
ated by sampling from the GWS annotators’ rating
distribution, and a second generated by uniformly
sampling in [1, 5]. By comparison, the aggregate la-
belings have a much larger IAA than the baselines,
which is often at least as high as the IAA amongst
the GWS annotators themselves, indicating that the
Turkers in aggregate are capable of producing equiv-
alent ratings. Of the three annotation methodolo-
gies, MaxDiff provides the highest IAA both within
its annotators and with its aggregate key. Surpris-
ingly, neither the Likert or S+R aggregate labeling
appears better than the other.
Based on the second experiment, we measured
the average IAA across all words between the ag-
gregate Likert and MaxDiff solutions, which was
0.472. However, this IAA is significantly affected by
the annotations for win.v and different.a, which had
the lowest IAA among Turkers (Table 1) and there-
</bodyText>
<page confidence="0.998751">
559
</page>
<note confidence="0.998597125">
Corpus Sense Inventory IAA Measurement
SensEval-1 HECTOR 0.950 Replicability experiment
(Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999)
OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement
SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement
SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement
(Kilgarriff, 2002)
GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α
SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement
SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement
MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α
with MASI (Passonneau et al., 2006)
MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α
in Passonneau et al. (2010)
GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α
GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α
</note>
<table confidence="0.85520375">
† Not all words achieved this agreement.
‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagree-
ment, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case
of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A
re-annotation by Palmer et al. (2004) produced a similar pair-wise agreement of 71.0.
&apos;` Tou et al. (1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement
° Excludes agreement for argument.n, which was not annotated
° IAA ranges for 37 words; no corpus-wide IAA is provided.
</table>
<tableCaption confidence="0.955111">
Table 2: IAA for sense-annotated corpora
</tableCaption>
<bodyText confidence="0.999932">
fore produce noisy aggregate solutions. When win.v
and different.a are excluded, the agreement between
aggregate Likert and MaxDiff solutions is 0.649.
While this IAA is still moderate, it suggests that
Turkers can still produce similar annotations even
when using different annotation methodologies.
For the third experiment, replicability is reported
as the average IAA between the sampled aggregate
labelings for all annotated words. Table 2 shows this
IAA for Likert and MaxDiff methodologies in com-
parison to other sense annotation studies. Krippen-
dorff (2004) recommends that an α of 0.8 is nec-
essary to claim high-quality agreement, which is
achieved by the MaxDiff methodology. In contrast,
the average IAA between sampled Likert ratings is
significantly lower, though the methodology does
achieve an α of 0.812 for paper.n. However, when
the two words with the lowest IAA, win.v and differ-
ent.a, are excluded, the average α increases to 0.880
for MaxDiff and 0.649 for Likert. Overall, these re-
sults suggest that MaxDiff can generate highly repli-
cable annotations with agreement on par with that of
other high-quality sense-labeled corpora. Further-
more, the Likert methodology may in aggregate still
produce moderately replicable annotations in some
cases.
</bodyText>
<sectionHeader confidence="0.983868" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999343578947368">
Word sense disambiguation is a difficult task, both
for humans and algorithms, with an important bot-
tleneck in acquiring large sense annotated corpora.
As a potential solution, we proposed three annota-
tion methodologies for crowdsourcing sense labels.
Importantly, we relax the single sense assignment
restriction in order to let annotators explicitly note
ambiguity through weighted sense ratings. Our find-
ings reveal that moderate IAA can be obtained using
MaxDiff ratings, with IAA surpassing that of anno-
tators in a controlled setting. Furthermore, our find-
ings showed marked differences in rating difficulty
per word, even in the weighted rating setting. In
future work, we will investigate what factors influ-
ence annotation difficulty in order to improve IAA
to what is considered expert levels, drawing from
existing work analyzing difficulty in the single label
setting (Murray and Green, 2004; Passonneau et al.,
2009; Cinkov´a et al., 2012).
</bodyText>
<page confidence="0.995291">
560
</page>
<sectionHeader confidence="0.96197" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999522780952381">
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555–596.
Michael S. Bernstein, Ggreg Little, Robert C. Miller,
Bj¨on Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceedings
of UIST, pages 313–322. ACM.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing WordNet. In The 5th International Conference of
the Global WordNet Association (GWC-2010).
Chris Biemann. 2012. Turk Bootstrap Word Sense In-
ventory 2.0: A Large-Scale Resource for Lexical Sub-
stitution. In Proceedings of LREC.
Susan Windisch Brown, Travis Rood, and Martha Palmer.
2010. Number or nuance: Which factors restrict reli-
able word sense annotation? In Proceedings of LREC.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Recent
Successes and Future Directions, pages 32–39. ACL.
Silvie Cinkov´a, Martin Holub, and Vincent Kr´ı. 2012.
Managing uncertainty in semantic tagging. In Pro-
ceedings of EACL, pages 840–850. ACL.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of ACL, pages 10–18. ACL.
Christiane Fellbaum, Jaochim Grabowski, and Shari Lan-
des. 1998. Performance and confidence in a seman-
tic annotation task. WordNet: An electronic lexical
database, pages 217–237.
Christine Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. A method for disambiguating word
senses in a large corpus. Computers and the Humani-
ties, 26(5):415–439.
J. Hong and C.F. Baker. 2011. How Good is the Crowd
at ”real” WSD? In Proceedings of the Fifth Linguistic
Annotation Workshop (LAW V), pages 30–37. ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of NAACL, pages
57–60. ACL.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for english senseval. Computers and the Hu-
manities, 34(1):15–48.
Adam Kilgarriff. 1999. 95% replicability for manual
word sense tagging. In Proceedings of EACL, pages
277–278. ACL.
Adam Kilgarriff. 2002. English lexical sample task de-
scription. In Senseval-2: Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense Dis-
ambiguation Systems.
A. Kittur, B. Smus, S. Khamkar, and R.E. Kraut. 2011.
Crowdforge: Crowdsourcing complex work. In Pro-
ceedings of UIST, pages 43–52. ACM.
M. Kosinski, Y. Bachrach, G. Kasneci, J. Van-Gael, and
T. Graepel. 2012. Crowd IQ: Measuring the intelli-
gence of crowdsourcing platforms. In ACM Web Sci-
ences. ACM.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage, Beverly Hills, CA.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
A. Kulkarni, M. Can, and B. Hartmann. 2012. Collabo-
ratively crowdsourcing workflows with turkomatic. In
Proceedings of CSCW, pages 1003–1012. ACM.
J. J. Louviere. 1991. Best-Worst Scaling: A Model for
the Largest Difference Judgments. Technical report,
University of Alberta. Working Paper.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
ACL Workshop on Making Sense of Sense: Bringing
Psycholinguistics and Computational Linguistics To-
gether, pages 17–24.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 25–28. ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT, pages 303–308. ACL.
G. Craig Murray and Rebecca Green. 2004. Lexical
knowledge and human disagreement on a WSD task.
Computer Speech &amp; Language, 18(3):209–222.
Bryan Orme. 2009. MaxDiff Analysis: Simple Count-
ing, Individual-Level Logit, and HB. Sawtooth Soft-
ware.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the Second Work-
shop on Scalable Natural Language Understanding
Systems. ACL.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(02):137–163.
</reference>
<page confidence="0.972963">
561
</page>
<reference confidence="0.999794037735849">
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual
semantic annotation task. In Proceedings of LREC,
pages 1951–1956.
Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and Nancy
Ide. 2009. Making sense of word sense variation. In
Proceedings of the NAACL HLT Workshop on Seman-
tic Evaluations: Recent Achievements and Future Di-
rections.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators. In
Proceedings of LREC.
Rebecca J Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012a. The MASC word sense
sentence corpus. In Proceedings of LREC.
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multiply la-
beled word sense annotations. Language Resources
and Evaluation, 46(2):209–252.
Anna Rumshisky and Olga Batiukova. 2008. Polysemy
in verbs: systematic relations between senses and their
effect on annotation. In Proceedings of the Workshop
on Human Judgements in Computational Linguistics,
pages 33–41. ACL.
Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and
James Pustejovsky. 2012. Word Sense Inventories by
Non-experts. In Procoeedings of LREC.
Rion Snow, Brendan O’Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fastbut is it good?: Eval-
uating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254–263.
ACL.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 41–43.
James Surowiecki. 2005. The wisdom of crowds. An-
chor.
Ng Hwee Tou, Chung Yong Lim, and Shou King Foo.
1999. A Case Study on Inter-Annotator Agreement
for Word Sense Disambiguation. In Proceedings of
the ACL SIGLEX Workshop on Standardizing Lexical
Resources.
Jean V´eronis. 1998. A study of polysemy judgments and
inter-annotator agreement. In Program and advanced
papers of the Senseval workshop, pages 2–4.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2000. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In Proceedings of
NIPS.
</reference>
<page confidence="0.997332">
562
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980885">
<title confidence="0.999646">Embracing Ambiguity: A Comparison of Annotation Methodologies Crowdsourcing Word Sense Labels</title>
<author confidence="0.994413">David</author>
<affiliation confidence="0.9971325">Department of Computer University of California, Los</affiliation>
<email confidence="0.999428">jurgens@cs.ucla.edu</email>
<abstract confidence="0.9995754375">Word sense disambiguation aims to identify which meaning of a word is present in a given usage. Gathering word sense annotations is a laborious and difficult task. Several methods have been proposed to gather sense annotations using large numbers of untrained annotators, with mixed results. We propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1001" citStr="Artstein and Poesio, 2008" startWordPosition="147" endWordPosition="150">everal methods have been proposed to gather sense annotations using large numbers of untrained annotators, with mixed results. We propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Rece</context>
<context position="13312" citStr="Artstein and Poesio, 2008" startWordPosition="2114" endWordPosition="2117">sults, we prefaced each task with a simple test question that asked the Turker to pick out a definition of the target word from a list of four options. The incorrect options were selected so that they would be nonsensical for anyone familiar with the target word. Additionally, we rejected all Turker responses where more than one option was missing a rating. In the case of missing ratings, we infer a rating of 1. Approximately 20-30% of the submissions were rejected by these criteria, underscoring the importance of filtering. For measuring IAA, we selected Krippendorff’s α (Krippendorff, 1980; Artstein and Poesio, 2008), which is an agreement coefficient that handles missing data, as well as different levels of measurement, e.g., nominal data (Select and MaxDiff) and interval data (Likert and Rate).2 Krippendorff’s α adjusts for chance, ranging between [−1, 1] for nominal data and (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; random labels would have an expected α of zero. We treat each sense and instance combination as a separate item to rate. 5 Results The results of the first experiment appear in the top of Table 1. Two important aspects emerge. F</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>R. Artstein and M. Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S Bernstein</author>
<author>Ggreg Little</author>
<author>Robert C Miller</author>
<author>Bj¨on Hartmann</author>
<author>Mark S Ackerman</author>
<author>David R Karger</author>
<author>David Crowell</author>
<author>Katrina Panovich</author>
</authors>
<title>Soylent: a word processor with a crowd inside.</title>
<date>2010</date>
<booktitle>In Proceedings of UIST,</booktitle>
<pages>313--322</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6525" citStr="Bernstein et al., 2010" startWordPosition="1004" endWordPosition="1007"> rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of Erk et al. (2009) which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012). We propose a two-stage strategy that aims to reduce the complexity of the annotation task, referred to as Select and Rate (S+R). First, Turkers are presented with all the senses and asked to make a binary choice of which senses apply. Second, a Likert rating task is created for only those senses whose selection frequency is above a threshold, thereby concentrating worker focus on a potentially smaller set of senses. Our motivation for S+R is two-fold. First, the sense definitions of certain words may be unclear or misinterpreted by a minority of t</context>
</contexts>
<marker>Bernstein, Little, Miller, Hartmann, Ackerman, Karger, Crowell, Panovich, 2010</marker>
<rawString>Michael S. Bernstein, Ggreg Little, Robert C. Miller, Bj¨on Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2010. Soylent: a word processor with a crowd inside. In Proceedings of UIST, pages 313–322. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Valerie Nygaard</author>
</authors>
<title>Crowdsourcing WordNet.</title>
<date>2010</date>
<booktitle>In The 5th International Conference of the Global WordNet Association (GWC-2010).</booktitle>
<contexts>
<context position="1728" citStr="Biemann and Nygaard, 2010" startWordPosition="258" endWordPosition="262">nsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006). Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important r</context>
<context position="4349" citStr="Biemann and Nygaard (2010)" startWordPosition="655" endWordPosition="658"> the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate. Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting. 2 Related Work Given the potential utility of a sense-labeled corpus, multiple studies have examined how to efficiently gather high quality sense annotations. Snow et al. (2008) had MTurk workers, referred to as Turkers, disambiguate uses of “president.” While they reported extremely high IAA (0.952), their analysis was only performed on a single word. Biemann and Nygaard (2010) and Biemann (2012) construct a sense-labeled corpus by concurrently constructing the sense inventory itself. Turkers used a lexical substitution task to identify valid substitutions of a target word. The contexts for the resulting substitutions were clustered based on their word overlap and the resulting clusters were labeled as senses. Biemann and Nygaard (2010) showed that the number of sense definitions for a word in their inventory was correlated with the number in WordNet, often with their inventory having fewer senses by combining related meanings and omitting rare meanings. Hong and Ba</context>
</contexts>
<marker>Biemann, Nygaard, 2010</marker>
<rawString>Chris Biemann and Valerie Nygaard. 2010. Crowdsourcing WordNet. In The 5th International Conference of the Global WordNet Association (GWC-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Turk Bootstrap Word Sense Inventory 2.0: A Large-Scale Resource for Lexical Substitution.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="4368" citStr="Biemann (2012)" startWordPosition="660" endWordPosition="661">s follow consistent patterns that enable creating high quality labeling from their aggregate. Second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting. 2 Related Work Given the potential utility of a sense-labeled corpus, multiple studies have examined how to efficiently gather high quality sense annotations. Snow et al. (2008) had MTurk workers, referred to as Turkers, disambiguate uses of “president.” While they reported extremely high IAA (0.952), their analysis was only performed on a single word. Biemann and Nygaard (2010) and Biemann (2012) construct a sense-labeled corpus by concurrently constructing the sense inventory itself. Turkers used a lexical substitution task to identify valid substitutions of a target word. The contexts for the resulting substitutions were clustered based on their word overlap and the resulting clusters were labeled as senses. Biemann and Nygaard (2010) showed that the number of sense definitions for a word in their inventory was correlated with the number in WordNet, often with their inventory having fewer senses by combining related meanings and omitting rare meanings. Hong and Baker (2011) evaluate</context>
</contexts>
<marker>Biemann, 2012</marker>
<rawString>Chris Biemann. 2012. Turk Bootstrap Word Sense Inventory 2.0: A Large-Scale Resource for Lexical Substitution. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Windisch Brown</author>
<author>Travis Rood</author>
<author>Martha Palmer</author>
</authors>
<title>Number or nuance: Which factors restrict reliable word sense annotation?</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="2660" citStr="Brown et al., 2010" startWordPosition="401" endWordPosition="404"> quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators</context>
</contexts>
<marker>Brown, Rood, Palmer, 2010</marker>
<rawString>Susan Windisch Brown, Travis Rood, and Martha Palmer. 2010. Number or nuance: Which factors restrict reliable word sense annotation? In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Chugur</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>Polysemy and sense proximity in the senseval-2 test suite.</title>
<date>2002</date>
<booktitle>In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>32--39</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2571" citStr="Chugur et al., 2002" startWordPosition="387" endWordPosition="390">Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropri</context>
</contexts>
<marker>Chugur, Gonzalo, Verdejo, 2002</marker>
<rawString>Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002. Polysemy and sense proximity in the senseval-2 test suite. In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 32–39. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Martin Holub</author>
<author>Vincent Kr´ı</author>
</authors>
<title>Managing uncertainty in semantic tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>840--850</pages>
<publisher>ACL.</publisher>
<marker>Cinkov´a, Holub, Kr´ı, 2012</marker>
<rawString>Silvie Cinkov´a, Martin Holub, and Vincent Kr´ı. 2012. Managing uncertainty in semantic tagging. In Proceedings of EACL, pages 840–850. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>10--18</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3031" citStr="Erk et al., 2009" startWordPosition="459" endWordPosition="462">en contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of Erk et al. (2009), which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. 556 Proceedings of NAACL-HLT 2013, pages 556–562, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics This paper provides the following contributions. Firs</context>
<context position="5741" citStr="Erk et al. (2009)" startWordPosition="871" endWordPosition="874">hlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good performance, though not as good as trained annotators. 3 Annotation Methodologies We consider three methodologies for gathering sense labels: (1) the methodology of Erk et al. (2009) for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format. Likert Ratings Likert rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of Erk et al. (2009) which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have pro</context>
<context position="8193" citStr="Erk et al. (2009)" startWordPosition="1274" endWordPosition="1277">, the Select stage can potentially reduce the number of senses presented, leading to less cognitive burden in the Rate stage. Furthermore, as a pragmatic benefit, removing inapplicable senses reduces the visual space required for displaying the questions on the MTurk platform, which can improve annotation throughput. MaxDiff MaxDiff is an alternative to scale-based ratings in which Turkers are presented with a only subset of all of a word’s senses and then asked to select (1) the sense option that best matches the mean557 add.v ask.v win.v argument.n interest.n paper.n different.a important.a Erk et al. (2009) IAA 0.470 0.354 0.072 0.497 0.320 0.403 0.212 0.466 MTurk Likert IAA 0.336 0.212 0.129 0.250 0.209 0.522 0.030 0.240 MTurk Select 0.309 0.127 0.179 0.192 0.164 0.449 0.024 0.111 MTurk Rate 0.204 0.076 0.026 0.005 0.081 0.108 0.005 0.116 MTurk MaxDiff 0.493 0.353 0.295 - 0.349 0.391 0.220 0.511 Likert Mode 0.500 0.369 0.083 0.445 0.388 0.518 0.124 0.516 S+R Median 0.473 0.394 0.149 0.497 0.390 0.497 0.103 0.416 MTurk MaxDiff 0.508 0.412 0.184 - 0.408 0.496 0.115 0.501 Sampled Baseline 0.238 0.178 0.042 0.254 0.162 0.205 0.100 0.221 Random Baseline 0.239 0.186 0.045 0.249 0.269 0.200 0.110 0.26</context>
<context position="10952" citStr="Erk et al. (2009)" startWordPosition="1730" endWordPosition="1733">t with the reference labeling. Third, we measure the replicability of the Turker annotations (Kilgarriff, 1999) using a sampling methodology. Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item. IAA is calculated between the aggregate labelings computed from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotati</context>
<context position="13991" citStr="Erk et al. (2009)" startWordPosition="2229" endWordPosition="2232"> as well as different levels of measurement, e.g., nominal data (Select and MaxDiff) and interval data (Likert and Rate).2 Krippendorff’s α adjusts for chance, ranging between [−1, 1] for nominal data and (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; random labels would have an expected α of zero. We treat each sense and instance combination as a separate item to rate. 5 Results The results of the first experiment appear in the top of Table 1. Two important aspects emerge. First, the word itself plays a significant role in IAA. Though Erk et al. (2009) reported a pair-wise IAA of the GWS annotators between 0.466 and 0.506 using Spearman’s p, the IAA varies considerably between words for both Turkers and GWS annotators when measured using Krippendorff’s α. Second, the choice of annotation methodology 2We note that although the ratings are technically given on an ordinal scale (ranks), we use the interval scale to allow comparison with rational ratings from the aggregate solutions. significantly impacts IAA. While both the Likert and S+R tasks have lower IAA than the GWS annotators do, the MaxDiff annotators achieve higher IAA for almost all </context>
<context position="17708" citStr="Erk et al. (2009)" startWordPosition="2824" endWordPosition="2827">t SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A re-annotation by Palmer et al. (2004) produced a similar pair-wise agreement of 71.0. &apos;` Tou et al. (1999) perform a re-annotation test of the same </context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of ACL, pages 10–18. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>Jaochim Grabowski</author>
<author>Shari Landes</author>
</authors>
<title>Performance and confidence in a semantic annotation task. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>217--237</pages>
<contexts>
<context position="17290" citStr="Fellbaum et al., 1998" startWordPosition="2761" endWordPosition="2764">wever, this IAA is significantly affected by the annotations for win.v and different.a, which had the lowest IAA among Turkers (Table 1) and there559 Corpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item</context>
</contexts>
<marker>Fellbaum, Grabowski, Landes, 1998</marker>
<rawString>Christiane Fellbaum, Jaochim Grabowski, and Shari Landes. 1998. Performance and confidence in a semantic annotation task. WordNet: An electronic lexical database, pages 217–237.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christine Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christine Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<contexts>
<context position="1382" citStr="Gale et al., 1992" startWordPosition="206" endWordPosition="209">gh agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006). Furthermore, while Passonneau </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26(5):415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hong</author>
<author>C F Baker</author>
</authors>
<title>How Good is the Crowd at ”real” WSD?</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth Linguistic Annotation Workshop (LAW V),</booktitle>
<pages>30--37</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4959" citStr="Hong and Baker (2011)" startWordPosition="750" endWordPosition="753">aard (2010) and Biemann (2012) construct a sense-labeled corpus by concurrently constructing the sense inventory itself. Turkers used a lexical substitution task to identify valid substitutions of a target word. The contexts for the resulting substitutions were clustered based on their word overlap and the resulting clusters were labeled as senses. Biemann and Nygaard (2010) showed that the number of sense definitions for a word in their inventory was correlated with the number in WordNet, often with their inventory having fewer senses by combining related meanings and omitting rare meanings. Hong and Baker (2011) evaluated multiple annotation strategies for gathering FrameNet sense annotations, ultimately yielding high (&gt;90%) accuracy for most terms after filtering. They highlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good </context>
</contexts>
<marker>Hong, Baker, 2011</marker>
<rawString>J. Hong and C.F. Baker. 2011. How Good is the Crowd at ”real” WSD? In Proceedings of the Fifth Linguistic Annotation Workshop (LAW V), pages 30–37. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>57--60</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1236" citStr="Hovy et al., 2006" startWordPosition="183" endWordPosition="186"> multiple labels and weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are di</context>
<context position="16988" citStr="Hovy et al., 2006" startWordPosition="2721" endWordPosition="2724"> IAA both within its annotators and with its aggregate key. Surprisingly, neither the Likert or S+R aggregate labeling appears better than the other. Based on the second experiment, we measured the average IAA across all words between the aggregate Likert and MaxDiff solutions, which was 0.472. However, this IAA is significantly affected by the annotations for win.v and different.a, which had the lowest IAA among Turkers (Table 1) and there559 Corpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of NAACL, pages 57–60. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<title>Framework and results for english senseval.</title>
<date>2000</date>
<journal>Computers and the Humanities,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="16939" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="2714" endWordPosition="2717">he three annotation methodologies, MaxDiff provides the highest IAA both within its annotators and with its aggregate key. Surprisingly, neither the Likert or S+R aggregate labeling appears better than the other. Based on the second experiment, we measured the average IAA across all words between the aggregate Likert and MaxDiff solutions, which was 0.472. However, this IAA is significantly affected by the annotations for win.v and different.a, which had the lowest IAA among Turkers (Table 1) and there559 Corpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single </context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff and J. Rosenzweig. 2000. Framework and results for english senseval. Computers and the Humanities, 34(1):15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>95% replicability for manual word sense tagging.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>277--278</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10446" citStr="Kilgarriff, 1999" startWordPosition="1646" endWordPosition="1647">he reference labeling. First, we measure the ability of the Turkers individually by evaluating their IAA with the reference labeling. Second, many studies using crowdsourcing combine the results into a single answer, thereby leveraging the wisdom of the crowds (Surowiecki, 2005) to smooth over inconsistencies in the data. Therefore, in the second experiment, we evaluate different methods of combining Turker responses into a single sense labeling, referred to as an aggregate labeling, and comparing that with the reference labeling. Third, we measure the replicability of the Turker annotations (Kilgarriff, 1999) using a sampling methodology. Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item. IAA is calculated between the aggregate labelings computed from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to the</context>
<context position="16958" citStr="Kilgarriff, 1999" startWordPosition="2718" endWordPosition="2719"> MaxDiff provides the highest IAA both within its annotators and with its aggregate key. Surprisingly, neither the Likert or S+R aggregate labeling appears better than the other. Based on the second experiment, we measured the average IAA across all words between the aggregate Likert and MaxDiff solutions, which was 0.472. However, this IAA is significantly affected by the annotations for win.v and different.a, which had the lowest IAA among Turkers (Table 1) and there559 Corpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported Word</context>
</contexts>
<marker>Kilgarriff, 1999</marker>
<rawString>Adam Kilgarriff. 1999. 95% replicability for manual word sense tagging. In Proceedings of EACL, pages 277–278. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>English lexical sample task description.</title>
<date>2002</date>
<booktitle>In Senseval-2: Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems.</booktitle>
<contexts>
<context position="17192" citStr="Kilgarriff, 2002" startWordPosition="2749" endWordPosition="2750"> IAA across all words between the aggregate Likert and MaxDiff solutions, which was 0.472. However, this IAA is significantly affected by the annotations for win.v and different.a, which had the lowest IAA among Turkers (Table 1) and there559 Corpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreeme</context>
</contexts>
<marker>Kilgarriff, 2002</marker>
<rawString>Adam Kilgarriff. 2002. English lexical sample task description. In Senseval-2: Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kittur</author>
<author>B Smus</author>
<author>S Khamkar</author>
<author>R E Kraut</author>
</authors>
<title>Crowdforge: Crowdsourcing complex work.</title>
<date>2011</date>
<booktitle>In Proceedings of UIST,</booktitle>
<pages>43--52</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6546" citStr="Kittur et al., 2011" startWordPosition="1008" endWordPosition="1011">he most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of Erk et al. (2009) which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012). We propose a two-stage strategy that aims to reduce the complexity of the annotation task, referred to as Select and Rate (S+R). First, Turkers are presented with all the senses and asked to make a binary choice of which senses apply. Second, a Likert rating task is created for only those senses whose selection frequency is above a threshold, thereby concentrating worker focus on a potentially smaller set of senses. Our motivation for S+R is two-fold. First, the sense definitions of certain words may be unclear or misinterpreted by a minority of the Turkers, who then </context>
</contexts>
<marker>Kittur, Smus, Khamkar, Kraut, 2011</marker>
<rawString>A. Kittur, B. Smus, S. Khamkar, and R.E. Kraut. 2011. Crowdforge: Crowdsourcing complex work. In Proceedings of UIST, pages 43–52. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kosinski</author>
<author>Y Bachrach</author>
<author>G Kasneci</author>
<author>J Van-Gael</author>
<author>T Graepel</author>
</authors>
<title>Crowd IQ: Measuring the intelligence of crowdsourcing platforms.</title>
<date>2012</date>
<journal>In ACM Web Sciences. ACM.</journal>
<contexts>
<context position="12432" citStr="Kosinski et al. (2012)" startWordPosition="1968" endWordPosition="1971">tate all contexts for a single word; however many Turkers did complete the majority of instances. Both the Likert, Select, and Rate tasks used ten Turkers each. Senses were passed from Select to Rate if they received at 1Full guidelines are available at http://cs.ucla. edu/˜jurgens/sense-annotation/ 558 least three votes. For MaxDiff, we gathered at least 3n annotations per context where n is the number of senses of the target word, ensuring that each sense appeared at least once. Due to resource limitations, we omitted the evaluation of argument.n for MaxDiff. Following the recommendation of Kosinski et al. (2012), Turkers were paid $0.05USD for each Likert, Select, and Rate task. For MaxDiff, due to their shorter nature and comparably high volume, Turkers were paid $0.03USD per task. To ensure fluency in English as well as reduce the potential for low-quality results, we prefaced each task with a simple test question that asked the Turker to pick out a definition of the target word from a list of four options. The incorrect options were selected so that they would be nonsensical for anyone familiar with the target word. Additionally, we rejected all Turker responses where more than one option was miss</context>
</contexts>
<marker>Kosinski, Bachrach, Kasneci, Van-Gael, Graepel, 2012</marker>
<rawString>M. Kosinski, Y. Bachrach, G. Kasneci, J. Van-Gael, and T. Graepel. 2012. Crowd IQ: Measuring the intelligence of crowdsourcing platforms. In ACM Web Sciences. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="13284" citStr="Krippendorff, 1980" startWordPosition="2112" endWordPosition="2113">l for low-quality results, we prefaced each task with a simple test question that asked the Turker to pick out a definition of the target word from a list of four options. The incorrect options were selected so that they would be nonsensical for anyone familiar with the target word. Additionally, we rejected all Turker responses where more than one option was missing a rating. In the case of missing ratings, we infer a rating of 1. Approximately 20-30% of the submissions were rejected by these criteria, underscoring the importance of filtering. For measuring IAA, we selected Krippendorff’s α (Krippendorff, 1980; Artstein and Poesio, 2008), which is an agreement coefficient that handles missing data, as well as different levels of measurement, e.g., nominal data (Select and MaxDiff) and interval data (Likert and Rate).2 Krippendorff’s α adjusts for chance, ranging between [−1, 1] for nominal data and (−1, 1] for interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; random labels would have an expected α of zero. We treat each sense and instance combination as a separate item to rate. 5 Results The results of the first experiment appear in the top of Table 1. Two</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage,</title>
<date>2004</date>
<location>Thousand Oaks, CA,</location>
<note>second edition.</note>
<contexts>
<context position="19103" citStr="Krippendorff (2004)" startWordPosition="3040" endWordPosition="3042">is provided. Table 2: IAA for sense-annotated corpora fore produce noisy aggregate solutions. When win.v and different.a are excluded, the agreement between aggregate Likert and MaxDiff solutions is 0.649. While this IAA is still moderate, it suggests that Turkers can still produce similar annotations even when using different annotation methodologies. For the third experiment, replicability is reported as the average IAA between the sampled aggregate labelings for all annotated words. Table 2 shows this IAA for Likert and MaxDiff methodologies in comparison to other sense annotation studies. Krippendorff (2004) recommends that an α of 0.8 is necessary to claim high-quality agreement, which is achieved by the MaxDiff methodology. In contrast, the average IAA between sampled Likert ratings is significantly lower, though the methodology does achieve an α of 0.812 for paper.n. However, when the two words with the lowest IAA, win.v and different.a, are excluded, the average α increases to 0.880 for MaxDiff and 0.649 for Likert. Overall, these results suggest that MaxDiff can generate highly replicable annotations with agreement on par with that of other high-quality sense-labeled corpora. Furthermore, th</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Sage, Thousand Oaks, CA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulkarni</author>
<author>M Can</author>
<author>B Hartmann</author>
</authors>
<title>Collaboratively crowdsourcing workflows with turkomatic.</title>
<date>2012</date>
<booktitle>In Proceedings of CSCW,</booktitle>
<pages>1003--1012</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6570" citStr="Kulkarni et al., 2012" startWordPosition="1012" endWordPosition="1015"> gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of Erk et al. (2009) which used a five-point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively. Select and Rate Recent efforts in crowdsourcing have proposed multi-stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete (Bernstein et al., 2010; Kittur et al., 2011; Kulkarni et al., 2012). We propose a two-stage strategy that aims to reduce the complexity of the annotation task, referred to as Select and Rate (S+R). First, Turkers are presented with all the senses and asked to make a binary choice of which senses apply. Second, a Likert rating task is created for only those senses whose selection frequency is above a threshold, thereby concentrating worker focus on a potentially smaller set of senses. Our motivation for S+R is two-fold. First, the sense definitions of certain words may be unclear or misinterpreted by a minority of the Turkers, who then systematically rate inap</context>
</contexts>
<marker>Kulkarni, Can, Hartmann, 2012</marker>
<rawString>A. Kulkarni, M. Can, and B. Hartmann. 2012. Collaboratively crowdsourcing workflows with turkomatic. In Proceedings of CSCW, pages 1003–1012. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Louviere</author>
</authors>
<title>Best-Worst Scaling: A Model for the Largest Difference Judgments.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>University of Alberta. Working Paper.</institution>
<contexts>
<context position="8978" citStr="Louviere, 1991" startWordPosition="1407" endWordPosition="1408">24 0.111 MTurk Rate 0.204 0.076 0.026 0.005 0.081 0.108 0.005 0.116 MTurk MaxDiff 0.493 0.353 0.295 - 0.349 0.391 0.220 0.511 Likert Mode 0.500 0.369 0.083 0.445 0.388 0.518 0.124 0.516 S+R Median 0.473 0.394 0.149 0.497 0.390 0.497 0.103 0.416 MTurk MaxDiff 0.508 0.412 0.184 - 0.408 0.496 0.115 0.501 Sampled Baseline 0.238 0.178 0.042 0.254 0.162 0.205 0.100 0.221 Random Baseline 0.239 0.186 0.045 0.249 0.269 0.200 0.110 0.269 Table 1: IAA per word (top) and IAA between aggregate labelings and the GWS annotators (bottom) ing in the example context and (2) the sense option that least matches (Louviere, 1991). In our setting, we presented three options at a time for words with fewer than seven senses, and four options for those with seven senses. For a single context, multiple subsets of the senses are presented and then their relative ranking is used to produce the numeric rating. The final applicability ratings were produced using a modification of the counting procedure of Orme (2009). First, all sense ratings are computed as the number of times the sense was rated best minus the number of times rated least. Second, all negativelyrated senses are assigned score of 1, and all positively ratings </context>
</contexts>
<marker>Louviere, 1991</marker>
<rawString>J. J. Louviere. 1991. Best-Worst Scaling: A Model for the Largest Difference Judgments. Technical report, University of Alberta. Working Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Relating WordNet senses for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2587" citStr="McCarthy, 2006" startWordPosition="391" endWordPosition="392">ssonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the </context>
</contexts>
<marker>McCarthy, 2006</marker>
<rawString>Diana McCarthy. 2006. Relating WordNet senses for word sense disambiguation. In Proceedings of the ACL Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="11362" citStr="Mihalcea et al., 2004" startWordPosition="1798" endWordPosition="1801">eport the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word. Unlike Passonneau et al. (2012b), we did not require a Turker to annotate all contexts for a single word; however many Turkers did complete the majority of instances. Both the Likert, Select, and Rate tasks used ten Turke</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25–28. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>303--308</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="11302" citStr="Miller et al., 1993" startWordPosition="1789" endWordPosition="1792">from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word. Unlike Passonneau et al. (2012b), we did not require a Turker to annotate all contexts for a single word; however many Turkers did complete the majority of insta</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proceedings of HLT, pages 303–308. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Craig Murray</author>
<author>Rebecca Green</author>
</authors>
<title>Lexical knowledge and human disagreement on a WSD task.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="3013" citStr="Murray and Green, 2004" startWordPosition="454" endWordPosition="458">ries such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of Erk et al. (2009), which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. 556 Proceedings of NAACL-HLT 2013, pages 556–562, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics This paper provides the following c</context>
</contexts>
<marker>Murray, Green, 2004</marker>
<rawString>G. Craig Murray and Rebecca Green. 2004. Lexical knowledge and human disagreement on a WSD task. Computer Speech &amp; Language, 18(3):209–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Orme</author>
</authors>
<title>MaxDiff Analysis: Simple Counting, Individual-Level Logit, and HB.</title>
<date>2009</date>
<publisher>Sawtooth Software.</publisher>
<contexts>
<context position="9364" citStr="Orme (2009)" startWordPosition="1472" endWordPosition="1473">0.186 0.045 0.249 0.269 0.200 0.110 0.269 Table 1: IAA per word (top) and IAA between aggregate labelings and the GWS annotators (bottom) ing in the example context and (2) the sense option that least matches (Louviere, 1991). In our setting, we presented three options at a time for words with fewer than seven senses, and four options for those with seven senses. For a single context, multiple subsets of the senses are presented and then their relative ranking is used to produce the numeric rating. The final applicability ratings were produced using a modification of the counting procedure of Orme (2009). First, all sense ratings are computed as the number of times the sense was rated best minus the number of times rated least. Second, all negativelyrated senses are assigned score of 1, and all positively ratings are normalized to be (1, 5]. 4 Experiments For measuring the difference in methodologies, we propose three experiments based on different analyses of comparing Turker and non-Turker annotations on the same dataset, the latter of which we refer to as the reference labeling. First, we measure the ability of the Turkers individually by evaluating their IAA with the reference labeling. S</context>
</contexts>
<marker>Orme, 2009</marker>
<rawString>Bryan Orme. 2009. MaxDiff Analysis: Simple Counting, Individual-Level Logit, and HB. Sawtooth Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Olga Babko-Malaya</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Different sense granularities for different applications.</title>
<date>2004</date>
<booktitle>In Proceedings of the Second Workshop on Scalable Natural Language Understanding Systems. ACL.</booktitle>
<contexts>
<context position="18197" citStr="Palmer et al. (2004)" startWordPosition="2903" endWordPosition="2906">endorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A re-annotation by Palmer et al. (2004) produced a similar pair-wise agreement of 71.0. &apos;` Tou et al. (1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement ° Excludes agreement for argument.n, which was not annotated ° IAA ranges for 37 words; no corpus-wide IAA is provided. Table 2: IAA for sense-annotated corpora fore produce noisy aggregate solutions. When win.v and different.a are excluded, the agreement between aggregate Likert and MaxDiff solutions is 0.649. While this IAA is still moderate, it suggests that Turkers can still produce similar annotations even when </context>
</contexts>
<marker>Palmer, Babko-Malaya, Dang, 2004</marker>
<rawString>Martha Palmer, Olga Babko-Malaya, and Hoa Trang Dang. 2004. Different sense granularities for different applications. In Proceedings of the Second Workshop on Scalable Natural Language Understanding Systems. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="2608" citStr="Palmer et al., 2007" startWordPosition="393" endWordPosition="396">(2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed </context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(02):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Inter-annotator agreement on a multilingual semantic annotation task.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1951--1956</pages>
<contexts>
<context position="17525" citStr="Passonneau et al., 2006" startWordPosition="2795" endWordPosition="2798">ment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 mea</context>
</contexts>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Rebecca Passonneau, Nizar Habash, and Owen Rambow. 2006. Inter-annotator agreement on a multilingual semantic annotation task. In Proceedings of LREC, pages 1951–1956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Ansaf Salleb-Aouissi</author>
<author>Nancy Ide</author>
</authors>
<title>Making sense of word sense variation.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions.</booktitle>
<marker>Passonneau, Salleb-Aouissi, Ide, 2009</marker>
<rawString>Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and Nancy Ide. 2009. Making sense of word sense variation. In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Ansaf Salleb-Aoussi</author>
<author>Vikas Bhardwaj</author>
<author>Nancy Ide</author>
</authors>
<title>Word sense annotation of polysemous words by multiple annotators.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="1262" citStr="Passonneau et al., 2010" startWordPosition="187" endWordPosition="190">d weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are different from the commonly </context>
<context position="17616" citStr="Passonneau et al. (2010)" startWordPosition="2809" endWordPosition="2812">oNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A re-annotation by Palmer et al. (2004) produced a similar</context>
</contexts>
<marker>Passonneau, Salleb-Aoussi, Bhardwaj, Ide, 2010</marker>
<rawString>Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas Bhardwaj, and Nancy Ide. 2010. Word sense annotation of polysemous words by multiple annotators. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Collin Baker</author>
<author>Christiane Fellbaum</author>
<author>Nancy Ide</author>
</authors>
<title>The MASC word sense sentence corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="1753" citStr="Passonneau et al., 2012" startWordPosition="263" endWordPosition="266">nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006). Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-qua</context>
<context position="3056" citStr="Passonneau et al., 2012" startWordPosition="463" endWordPosition="466"> related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of Erk et al. (2009), which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. 556 Proceedings of NAACL-HLT 2013, pages 556–562, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics This paper provides the following contributions. First, we demonstrate that th</context>
<context position="5286" citStr="Passonneau et al. (2012" startWordPosition="799" endWordPosition="802">s were labeled as senses. Biemann and Nygaard (2010) showed that the number of sense definitions for a word in their inventory was correlated with the number in WordNet, often with their inventory having fewer senses by combining related meanings and omitting rare meanings. Hong and Baker (2011) evaluated multiple annotation strategies for gathering FrameNet sense annotations, ultimately yielding high (&gt;90%) accuracy for most terms after filtering. They highlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good performance, though not as good as trained annotators. 3 Annotation Methodologies We consider three methodologies for gathering sense labels: (1) the methodology of Erk et al. (2009) for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format. Like</context>
<context position="11771" citStr="Passonneau et al. (2012" startWordPosition="1860" endWordPosition="1863">cability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word. Unlike Passonneau et al. (2012b), we did not require a Turker to annotate all contexts for a single word; however many Turkers did complete the majority of instances. Both the Likert, Select, and Rate tasks used ten Turkers each. Senses were passed from Select to Rate if they received at 1Full guidelines are available at http://cs.ucla. edu/˜jurgens/sense-annotation/ 558 least three votes. For MaxDiff, we gathered at least 3n annotations per context where n is the number of senses of the target word, ensuring that each sense appeared at least once. Due to resource limitations, we omitted the evaluation of argument.n for Ma</context>
<context position="17443" citStr="Passonneau et al., 2012" startWordPosition="2782" endWordPosition="2785">rpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a ca</context>
</contexts>
<marker>Passonneau, Baker, Fellbaum, Ide, 2012</marker>
<rawString>Rebecca J Passonneau, Collin Baker, Christiane Fellbaum, and Nancy Ide. 2012a. The MASC word sense sentence corpus. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Vikas Bhardwaj</author>
<author>Ansaf SallebAouissi</author>
<author>Nancy Ide</author>
</authors>
<title>Multiplicity and word sense: evaluating and learning from multiply labeled word sense annotations. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>46--2</pages>
<contexts>
<context position="1753" citStr="Passonneau et al., 2012" startWordPosition="263" endWordPosition="266">nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006). Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-qua</context>
<context position="3056" citStr="Passonneau et al., 2012" startWordPosition="463" endWordPosition="466"> related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of Erk et al. (2009), which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. 556 Proceedings of NAACL-HLT 2013, pages 556–562, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics This paper provides the following contributions. First, we demonstrate that th</context>
<context position="5286" citStr="Passonneau et al. (2012" startWordPosition="799" endWordPosition="802">s were labeled as senses. Biemann and Nygaard (2010) showed that the number of sense definitions for a word in their inventory was correlated with the number in WordNet, often with their inventory having fewer senses by combining related meanings and omitting rare meanings. Hong and Baker (2011) evaluated multiple annotation strategies for gathering FrameNet sense annotations, ultimately yielding high (&gt;90%) accuracy for most terms after filtering. They highlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good performance, though not as good as trained annotators. 3 Annotation Methodologies We consider three methodologies for gathering sense labels: (1) the methodology of Erk et al. (2009) for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format. Like</context>
<context position="11771" citStr="Passonneau et al. (2012" startWordPosition="1860" endWordPosition="1863">cability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word. Unlike Passonneau et al. (2012b), we did not require a Turker to annotate all contexts for a single word; however many Turkers did complete the majority of instances. Both the Likert, Select, and Rate tasks used ten Turkers each. Senses were passed from Select to Rate if they received at 1Full guidelines are available at http://cs.ucla. edu/˜jurgens/sense-annotation/ 558 least three votes. For MaxDiff, we gathered at least 3n annotations per context where n is the number of senses of the target word, ensuring that each sense appeared at least once. Due to resource limitations, we omitted the evaluation of argument.n for Ma</context>
<context position="17443" citStr="Passonneau et al., 2012" startWordPosition="2782" endWordPosition="2785">rpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a ca</context>
</contexts>
<marker>Passonneau, Bhardwaj, SallebAouissi, Ide, 2012</marker>
<rawString>Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf SallebAouissi, and Nancy Ide. 2012b. Multiplicity and word sense: evaluating and learning from multiply labeled word sense annotations. Language Resources and Evaluation, 46(2):209–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
<author>Olga Batiukova</author>
</authors>
<title>Polysemy in verbs: systematic relations between senses and their effect on annotation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Human Judgements in Computational Linguistics,</booktitle>
<pages>33--41</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2639" citStr="Rumshisky and Batiukova, 2008" startWordPosition="397" endWordPosition="400">et sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity expl</context>
</contexts>
<marker>Rumshisky, Batiukova, 2008</marker>
<rawString>Anna Rumshisky and Olga Batiukova. 2008. Polysemy in verbs: systematic relations between senses and their effect on annotation. In Proceedings of the Workshop on Human Judgements in Computational Linguistics, pages 33–41. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
<author>Nick Botchan</author>
<author>Sophie Kushkuley</author>
<author>James Pustejovsky</author>
</authors>
<title>Word Sense Inventories by Non-experts.</title>
<date>2012</date>
<booktitle>In Procoeedings of LREC.</booktitle>
<contexts>
<context position="1779" citStr="Rumshisky et al., 2012" startWordPosition="267" endWordPosition="270">rpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; Rumshisky et al., 2012). However, these methods produce sense labels that are different from the commonly used sense inventories such as WordNet (Fellbaum, 1998) or OntoNotes (Hovy et al., 2006). Furthermore, while Passonneau et al. (2012b) did use WordNet sense labels, they found the quality was well below that of trained experts. We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-gra</context>
</contexts>
<marker>Rumshisky, Botchan, Kushkuley, Pustejovsky, 2012</marker>
<rawString>Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and James Pustejovsky. 2012. Word Sense Inventories by Non-experts. In Procoeedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Dan Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fastbut is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>254--263</pages>
<publisher>ACL.</publisher>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Dan Jurafsky, and Andrew Y. Ng. 2008. Cheap and fastbut is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP, pages 254–263. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<contexts>
<context position="17374" citStr="Snyder and Palmer, 2004" startWordPosition="2772" endWordPosition="2775">nt.a, which had the lowest IAA among Turkers (Table 1) and there559 Corpus Sense Inventory IAA Measurement SensEval-1 HECTOR 0.950 Replicability experiment (Kilgarriff and Rosenzweig, 2000) (Kilgarriff, 1999) OntoNotes (Hovy et al., 2006) OntoNotes &gt; 0.90† Pair-wise agreement SALSA (Burchardt et al., 2006) FrameNet 0.86 Percentage agreement SensEval-2 Lexical Sample WordNet 1.7 0.853, 0.710, 0.673‡ Adjudicated Agreement (Kilgarriff, 2002) GWS with MaxDiff Replicability* WordNet 3.0 0.815 Krippendorff’s α SemCor (Fellbaum et al., 1998) WordNet 1.6 0.786, 0.57&apos;` Percentage agreement SensEval-3 (Snyder and Palmer, 2004) WordNet 1.7 0.725 Percentage agreement MASC (Passonneau et al., 2012a) WordNet 3.1 -0.02 to 0.88° Krippendorff’s α with MASI (Passonneau et al., 2006) MASC, single phase reported WordNet 3.1 0.515 Krippendorff’s α in Passonneau et al. (2010) GWS with Likert Replicability WordNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotato</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Surowiecki</author>
</authors>
<title>The wisdom of crowds.</title>
<date>2005</date>
<journal>Anchor.</journal>
<contexts>
<context position="10108" citStr="Surowiecki, 2005" startWordPosition="1596" endWordPosition="1597">Second, all negativelyrated senses are assigned score of 1, and all positively ratings are normalized to be (1, 5]. 4 Experiments For measuring the difference in methodologies, we propose three experiments based on different analyses of comparing Turker and non-Turker annotations on the same dataset, the latter of which we refer to as the reference labeling. First, we measure the ability of the Turkers individually by evaluating their IAA with the reference labeling. Second, many studies using crowdsourcing combine the results into a single answer, thereby leveraging the wisdom of the crowds (Surowiecki, 2005) to smooth over inconsistencies in the data. Therefore, in the second experiment, we evaluate different methods of combining Turker responses into a single sense labeling, referred to as an aggregate labeling, and comparing that with the reference labeling. Third, we measure the replicability of the Turker annotations (Kilgarriff, 1999) using a sampling methodology. Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item. IAA is calculated between the aggregate labelings computed from each set. This sampli</context>
</contexts>
<marker>Surowiecki, 2005</marker>
<rawString>James Surowiecki. 2005. The wisdom of crowds. Anchor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ng Hwee Tou</author>
<author>Chung Yong Lim</author>
<author>Shou King Foo</author>
</authors>
<title>A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources.</booktitle>
<contexts>
<context position="18266" citStr="Tou et al. (1999)" startWordPosition="2915" endWordPosition="2918">rdNet 3.0 0.409 Krippendorff’s α GWS with Erk et al. (2009) annotators WordNet 3.0 0.349 Krippendorff’s α † Not all words achieved this agreement. ‡ Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A re-annotation by Palmer et al. (2004) produced a similar pair-wise agreement of 71.0. &apos;` Tou et al. (1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement ° Excludes agreement for argument.n, which was not annotated ° IAA ranges for 37 words; no corpus-wide IAA is provided. Table 2: IAA for sense-annotated corpora fore produce noisy aggregate solutions. When win.v and different.a are excluded, the agreement between aggregate Likert and MaxDiff solutions is 0.649. While this IAA is still moderate, it suggests that Turkers can still produce similar annotations even when using different annotation methodologies. For the third experiment, r</context>
</contexts>
<marker>Tou, Lim, Foo, 1999</marker>
<rawString>Ng Hwee Tou, Chung Yong Lim, and Shou King Foo. 1999. A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation. In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>A study of polysemy judgments and inter-annotator agreement.</title>
<date>1998</date>
<booktitle>In Program and advanced papers of the Senseval workshop,</booktitle>
<pages>2--4</pages>
<marker>V´eronis, 1998</marker>
<rawString>Jean V´eronis. 1998. A study of polysemy judgments and inter-annotator agreement. In Program and advanced papers of the Senseval workshop, pages 2–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2000</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="5543" citStr="Whitehill et al., 2000" startWordPosition="841" endWordPosition="844">ng rare meanings. Hong and Baker (2011) evaluated multiple annotation strategies for gathering FrameNet sense annotations, ultimately yielding high (&gt;90%) accuracy for most terms after filtering. They highlight ambiguous and polysemous usages as a notable source of errors, which the present work directly addresses. In the most related work, Passonneau et al. (2012b) had Turkers annotate contexts using one or more senses, with the requirement that a worker labels all contexts. While they found that agreement between all workers was low, their annotations could be combined using the GLAD model (Whitehill et al., 2000) to obtain good performance, though not as good as trained annotators. 3 Annotation Methodologies We consider three methodologies for gathering sense labels: (1) the methodology of Erk et al. (2009) for gathering weighted labels, (2) a multistage strategy that uses both binary and Likert ratings, and (3) MaxDiff, a paired choice format. Likert Ratings Likert rating scales provide the most direct way of gathering weighted sense labels; Turkers are presented with all senses of a word and then asked to rate each on a numeric scale. We adopt the annotation guidelines of Erk et al. (2009) which use</context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2000</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2000. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Proceedings of NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>