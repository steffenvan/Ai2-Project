<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.98958">
A Weakly-supervised Approach to Argumentative Zoning of Scientific
Documents
</title>
<author confidence="0.994577">
Yufan Guo Anna Korhonen Thierry Poibeau
</author>
<affiliation confidence="0.9606225">
Computer Laboratory Computer Laboratory LaTTiCe, UMR8094
University of Cambridge, UK University of Cambridge, UK CNRS &amp; ENS, France
</affiliation>
<email confidence="0.994295">
yg244@cam.ac.uk alk23@cam.ac.uk thierry.poibeau@ens.fr
</email>
<sectionHeader confidence="0.998549" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999482380952381">
Argumentative Zoning (AZ) – analysis of the
argumentative structure of a scientific paper –
has proved useful for a number of informa-
tion access tasks. Current approaches to AZ
rely on supervised machine learning (ML).
Requiring large amounts of annotated data,
these approaches are expensive to develop and
port to different domains and tasks. A poten-
tial solution to this problem is to use weakly-
supervised ML instead. We investigate the
performance of four weakly-supervised clas-
sifiers on scientific abstract data annotated for
multiple AZ classes. Our best classifier based
on the combination of active learning and self-
training outperforms our best supervised clas-
sifier, yielding a high accuracy of 81% when
using just 10% of the labeled data. This re-
sult suggests that weakly-supervised learning
could be employed to improve the practical
applicability and portability of AZ across dif-
ferent information access tasks.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999728133333333">
Many practical tasks require accessing specific types
of information in scientific literature. For example,
a reader of scientific literature may be looking for
information about the objective of the study in ques-
tion, the methods used in the study, the results ob-
tained, or the conclusions drawn by authors. Sim-
ilarly, many Natural Language Processing (NLP)
tasks focus on the extraction of specific types of in-
formation in documents only.
To date, a number of approaches have been pro-
posed for sentence-based classification of scien-
tific literature according to categories of information
structure (or discourse, rhetorical, argumentative or
conceptual structure, depending on the framework
in question). Some of these classify sentences ac-
cording to typical section names seen in scientific
documents (Lin et al., 2006; Hirohata et al., 2008),
while others are based e.g. on argumentative zones
(Teufel and Moens, 2002; Mizuta et al., 2006; Teufel
et al., 2009), qualitative dimensions (Shatkay et al.,
2008) or conceptual structure (Liakata et al., 2010)
of documents.
The best of current approaches have yielded
promising results and proved useful for information
retrieval, information extraction and summarization
tasks (Teufel and Moens, 2002; Mizuta et al., 2006;
Tbahriti et al., 2006; Ruch et al., 2007). How-
ever, relying on fully supervised machine learning
(ML) and a large body of annotated data, existing
approaches are expensive to develop and port to dif-
ferent scientific domains and tasks.
A potential solution to this bottleneck is to de-
velop techniques based on weakly-supervised ML.
Relying on a small amount of labeled data and
a large pool of unlabeled data, weakly-supervised
techniques (e.g. semi-supervision, active learning,
co/tri-training, self-training) aim to keep the advan-
tages of fully supervised approaches. They have
been applied to a wide range of NLP tasks, includ-
ing named-entity recognition, question answering,
information extraction, text classification and many
others (Abney, 2008), yielding performance levels
similar or equivalent to those of fully supervised
techniques.
To the best of our knowledge, such techniques
</bodyText>
<page confidence="0.981545">
273
</page>
<note confidence="0.9578535">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273–283,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966978723405">
have not yet been applied to the analysis of infor-
mation structure of scientific documents by afore-
mentioned approaches. Recent experiments have
demonstrated the usefulness of weakly-supervised
learning for classifying discourse relations in scien-
tific texts, e.g. (Hernault et al., 2011). However, fo-
cusing on local (rather than global) structure of doc-
uments and being much more fine-grained in nature,
this related task differs from ours considerably.
In this paper, we investigate the potential of
weakly-supervised learning for Argumentative Zon-
ing (AZ) of scientific abstracts. AZ is an approach to
information structure which provides an analysis of
the rhetorical progression of the scientific argument
in a document (Teufel and Moens, 2002). It has
been used to analyze scientific texts in various disci-
plines – including computational linguistics (Teufel
and Moens, 2002), law, (Hachey and Grover, 2006),
biology (Mizuta et al., 2006) and chemistry (Teufel
et al., 2009) – and has proved useful for NLP tasks
such as summarization (Teufel and Moens, 2002).
Although the basic scheme is said to be discipline-
independent (Teufel et al., 2009), its application to
different domains has resulted in various modifica-
tions and laborious annotation exercises. This sug-
gests that a weakly-supervised approach would be
more practical than a fully supervised one for the
real-world application of AZ.
Taking two supervised classifiers as a comparison
point – Support Vector Machines (SVM) and Con-
ditional Random Fields (CRF) – we investigate the
performance of four weakly-supervised classifiers
on the AZ task: two based on semi-supervised learn-
ing (transductive SVM and semi-supervised CRF)
and two on active learning (Active SVM alone and
in combination with self-training).
The results are promising. Our best weakly-
supervised classifier (Active SVM with self-
training) outperforms the best supervised classifier
(SVM), yielding high accuracy of 81% when using
just 10% of the labeled data. When using just one
third of the labeled data, it performs equally well as
a fully supervised SVM which uses 100% of the la-
beled data. Our investigation suggests that weakly-
supervised learning could be employed to improve
the practical applicability and portability of AZ to
different information access tasks.
</bodyText>
<sectionHeader confidence="0.977414" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999911152173913">
We used in our experiments the recent dataset of
(Guo et al., 2010). Guo et al. (2010) provide a cor-
pus of 1000 biomedical abstracts (consisting of 7985
sentences and 225785 words) annotated according
to three schemes of information structure – those
based on section names (Hirohata et al., 2008), AZ
(Mizuta et al., 2006) and Core Scientific Concepts
(CoreSC) (Liakata et al., 2010). We focus here on
AZ only, because it subsumes all the categories of
the simple section name -based scheme, and accord-
ing to the inter-annotator agreement and ML experi-
ments reported by Guo et al. (2010) it performs bet-
ter on this data than the fairly fine-grained CoreSC
scheme.
AZ is a scheme which provides an analysis of
the rhetorical progression of the scientific argument,
following the knowledge claims made by authors.
(Teufel and Moens, 2002) introduced AZ and ap-
plied it first to computational linguistics papers.
(Hachey and Grover, 2006) applied the scheme later
to legal texts and (Mizuta et al., 2006) modified it for
biology papers. More recently, (Teufel et al., 2009)
introduced a refined version of AZ and applied it to
chemistry papers.
The biomedical dataset of (Guo et al., 2010) has
been annotated according to the version of AZ de-
veloped for biology papers (Mizuta et al., 2006)
(with only minor modifications concerning zone
names). Seven categories of this scheme (out of the
10 possible) actually appear in abstracts and in the
resulting corpus. These are shown and explained
in Table 1. For example, the Method zone (METH)
is for sentences which describe a way of doing re-
search, esp. according to a defined and regular
plan; a special form of procedure or characteristic
set of procedures employed in a field of study as a
mode of investigation and inquiry.
An example of a biomedical abstract annotated
according to AZ is shown in Figure 1, with different
zones highlighted in different colors. For example,
the RES zone is highlighted in lemon green.
Table 2 shows the distribution of sentences per
scheme category in the corpus: Results (RES) is
by far the most frequent zone (accounting for 40%
of the corpus), while Background (BKG), Objective
(OBJ), Method (METH) and Conclusion (CON) cover
</bodyText>
<page confidence="0.999503">
274
</page>
<tableCaption confidence="0.999661">
Table 1: Categories of AZ appearing in the corpus of (Guo et al., 2010)
</tableCaption>
<figure confidence="0.972272266666667">
Category Abbr. Definition
Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form
of procedure or characteristic set of procedures employed in a field of study as a mode
of investigation and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula,
etc. obtained by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction,
induction; a proposition deduced by reasoning from other propositions; the result of
a discussion, or examination of a question, final determination, decision, resolution,
final arrangement or agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
Result
</figure>
<figureCaption confidence="0.539023625">
Figure 1: An example of an annotated abstract
Butadiene (BD) metabolism shows gender, species and conc
by cytochrome P450 2E1 to three epoxides, 1,2-epoxy-3-but
important to elucidate species differences in the internal form
an LC-MS/MS method for simultaneous detection of all three
method for the cyclic N,N-(2,3-dihydroxy-1,4-butadyil)-vali
peptide (HB-Val). The procedure utilizes trypsin hydrolysis o
Objective
</figureCaption>
<bodyText confidence="0.884472444444444">
monitoring of the transition from the singly charged molecula
production. As internal standard, the labeled rat-[(13)C(5)(15
characterized and quantified by LC-MS/MS and LC-UV. The
LC-MS/MS method to the N-terminal peptide from the alpha
HB-Val adducts during long term storage (about 10 years) b
Future w
potencies. Analysis of N-terminal globin adducts is a commo
These data are much lower compared to
coefficient of variation &lt;25%. The LOQ was set to 100 fmol
</bodyText>
<tableCaption confidence="0.9817575">
Table 2: Distribution of sentences in the AZ-annotated
corpus
</tableCaption>
<table confidence="0.913684428571429">
Method BD for 90 days were analyzed. The amounts of HB-Val pres METH RES CON REL FUT
detected in controls. OBJ
BKG
Word 36828 23493 41544 89538 30752 2456 1174
Sentence 1429 674 1473 3185 1082 95 47
evaluation of HB-Val in multiple species. internal standard for parallel analysis by GC-MS/MS and LC 18% 40% 14% 1% 1%
Sentence 18% 8%
</table>
<bodyText confidence="0.94344925">
8-18% of the corpus each. Two categories are very
low in frequency, only covering 1% of the corpus
each: Related work (REL) and Future work (FUT).
Guo et al. (2010) report the inter-annotator agree-
ment between their three annotators: one linguist,
one computational linguist and one domain expert.
According to Cohen’s kappa (Cohen, 1960) the
agreement is relatively high: κ = 0.85.
</bodyText>
<sectionHeader confidence="0.9894" genericHeader="method">
3 Automatic identification of AZ
</sectionHeader>
<subsectionHeader confidence="0.992276">
3.1 Features and feature extraction
</subsectionHeader>
<bodyText confidence="0.999949076923077">
Guo et al. (2010) used a variety of features in
their fully supervised ML experiments on different
schemes of information structure. Since their fea-
ture types cover the best performing feature types in
earlier works e.g. (Teufel and Moens, 2002; Lin et
al., 2006; Mullen et al., 2005; Hirohata et al., 2008;
Merity et al., 2009) we re-implemented and used
them in our experiment1. However, being aware
of the fact that some of these features may not be
optimal for weakly-supervised learning (i.e. when
learning from smaller data), we evaluate their per-
formance and suitability for the task later in sec-
tion 4.3.
</bodyText>
<listItem confidence="0.994823">
• Location. Zones tend to appear in typical po-
sitions in abstracts. Each abstract was there-
</listItem>
<footnote confidence="0.885664">
1The only exception is the history feature which was left out
because it cannot be applied to all of our methods
</footnote>
<page confidence="0.996685">
275
</page>
<bodyText confidence="0.9985085">
fore divided into ten parts (1-10, measured by
the number of words), and the location was de-
fined by the parts where the sentence begins
and ends.
</bodyText>
<listItem confidence="0.984764736842105">
• Word. All the words in the corpus.
• Bi-gram. Any combination of two adjacent
words in the corpus.
• Verb. All the verbs in the corpus.
• Verb Class. 60 verb classes appearing in
biomedical journal articles.
• Part-of-Speech – POS. The POS tag of each
verb in the corpus.
• Grammatical Relation – GR. Subject (nc-
subj), direct object (dobj), indirect object (iobj)
and second object (obj2) relations in the cor-
pus. e.g. (ncsubj observed 14 difference 5
obj). The value of this feature equals 1 if it
occurs in a particular sentence (and 0 if not).
• Subj and Obj. The subjects and objects ap-
pearing with any verbs in the corpus (extracted
from above GRs).
• Voice. The voice of verbs (active or passive) in
the corpus.
</listItem>
<bodyText confidence="0.99999425">
These features were extracted from the corpus us-
ing a number of tools. A tokenizer was used to detect
the boundaries of sentences and to separate punctu-
ation from adjacent words e.g. in complex biomed-
ical terms such as 2-amino-3,8-diethylimidazo[4,5-
f]quinoxaline. The C&amp;C tools (Curran et al., 2007)
trained on biomedical literature were employed for
POS tagging, lemmatization and parsing. The
lemma output was used for creating Word, Bi-gram
and Verb features. The GR output was used for cre-
ating the GR, Subj, Obj and Voice features. The
”obj” marker in a subject relation indicates passive
voice (e.g. (ncsubj observed 14 difference 5 obj)).
The verb classes were acquired automatically from
the corpus using the unsupervised spectral cluster-
ing method of (Sun and Korhonen, 2009). To con-
trol the number of features we lemmatized the lexi-
cal items for all the features, and removed the words
and GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences.
</bodyText>
<subsectionHeader confidence="0.998185">
3.2 Machine learning methods
</subsectionHeader>
<bodyText confidence="0.999862222222222">
Support Vector Machines (SVM) and Conditional
Random Fields (CRF) have proved the best perform-
ing fully supervised methods in most recent works
on information structure, e.g. (Teufel and Moens,
2002; Mullen et al., 2005; Hirohata et al., 2008; Guo
et al., 2010). We therefore implemented these meth-
ods as well as weakly supervised variations of them:
active SVM with and without self-training, transduc-
tive SVM and semi-supervised CRF.
</bodyText>
<sectionHeader confidence="0.500918" genericHeader="method">
3.2.1 Supervised methods
</sectionHeader>
<bodyText confidence="0.996132416666667">
SVM constructs hyperplanes in a multidimen-
sional space to separate data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w · x − b = 0, where w is its normal vec-
tor. We want to maximize the distance from the hy-
perplane to the data points, or the distance between
two parallel hyperplanes each of which separates the
data. The parallel hyperplanes can be written as:
w · x − b = 1 and w · x − b = −1, and the dis-
tance between them is 2
</bodyText>
<equation confidence="0.6645656">
w . The problem reduces to:
Minimize |w |(in w, b)
Subject to
w · x − b &gt; 1 for x of one class,
w · x − b &lt; −1 for x of the other,
</equation>
<bodyText confidence="0.998776555555556">
which can be solved by using the SMO algorithm
(Platt, 1999b). We used Weka software (Hall et al.,
2009) (employing its linear kernel) for SVM experi-
ments.
CRF is an undirected graphical model which de-
fines a probability distribution over the hidden states
(e.g. label sequences) given the observations. The
probability of a label sequence y given an observa-
tion sequence x can be written as:
</bodyText>
<equation confidence="0.822374">
p(y|x, 0) = 1
Z(x)exp(�j 0jFj(y, x)),
</equation>
<bodyText confidence="0.9992455">
where Fj(y, x) is a real-valued feature function of
the states and the observations; 0j is the weight of
Fj, and Z(x) is a normalization factor. The 0 pa-
rameters can be learned using the L-BFGS algorithm
(Nocedal, 1980). We used Mallet software (McCal-
lum, 2002) for CRF experiments.
</bodyText>
<sectionHeader confidence="0.797963" genericHeader="method">
3.2.2 Weakly-supervised methods
</sectionHeader>
<bodyText confidence="0.9017125">
Active SVM (ASVM) starts with a small amount of
labeled data, and iteratively chooses a proportion of
</bodyText>
<page confidence="0.988392">
276
</page>
<bodyText confidence="0.995071516129032">
unlabeled data for which SVM has less confidence
to be labeled (the labels can be restored from the
original corpus) and used in the next round of learn-
ing, i.e. active learning. Query strategies based on
the structure of SVM are frequently employed (Tong
and Koller, 2001; Novak et al., 2006). For exam-
ple, it is often assumed that the data points close to
the separating hyperplane are those that the SVM is
uncertain about. Unlike these methods, our learn-
ing algorithm compares the posterior probabilities
of the best estimate given each unlabeled instance,
and queries those with the lowest probabilities for
the next round of learning. The probabilities can be
obtained by fitting a Sigmoid after the standard SVM
(Platt, 1999a), and combined using a pairwise cou-
pling algorithm (Hastie and Tibshirani, 1998) in the
multi-class case. We used the SVM linear kernel in
Weka for classification, and the -M flag in Weka for
calculating the posterior probabilities.
Active SVM with self-training (ASSVM) is an ex-
tension of ASVM where each round of training has
two steps: (i) training on the labeled, and testing
on the unlabeled data, and querying; (ii) training on
both labeled and unlabeled/machine-labeled data by
using the estimates from step (i). The idea of ASSVM
is to make the best use of the labeled data, and to
make the most use of the unlabeled data.
Transductive SVM (TSVM) is an extension of
SVM which takes advantage of both labeled and un-
labeled data (Vapnik, 1998). Similar to SVM, the
problem is defined as:
</bodyText>
<equation confidence="0.9012196">
Minimize |w |(in w, b, y(u))
Subject to
y(l)(w x(l) − b) &gt; 1,
y(u)(w x(u) − b) &gt; 1 ,
y(u) E {−1,1},
</equation>
<bodyText confidence="0.997844357142857">
where x(u) is unlabeled data and y(u) the estimate
of its label. The problem can be solved by using
the CCCP algorithm (Collobert et al., 2006). We
used UniverSVM software (Sinz, 2011) for TSVM
experiments.
Semi-supervised CRF (SSCRF) can be imple-
mented with entropy regularization (ER). It ex-
tends the objective function on Labeled data
EL log p(y(l) |x(l), 0) with an additional term
EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize
the conditional entropy of the model’s predictions on
Unlabeled data (Jiao et al., 2006; Mann and Mccal-
lum, 2007). We used Mallet software (McCallum,
2002) for SSCRF experiments.
</bodyText>
<sectionHeader confidence="0.8647725" genericHeader="method">
4 Experimental evaluation
4.1 Evaluation methods
</sectionHeader>
<bodyText confidence="0.998981666666667">
We evaluated the ML results in terms of accuracy,
precision, recall, and F-measure against manual AZ
annotations in the corpus:
</bodyText>
<figure confidence="0.7624256">
no. of correctly classified sentences
aCC =
total no. ofsentences in the corpus
no. of sentences correctly identified as Classi
p = total no. of sentences identified as Classi
no. of sentences correctly identified as Classi
r =
total no. of sentences in Classi
f_ 2∗p∗r
- p+r
</figure>
<bodyText confidence="0.999773636363636">
We used 10-fold cross validation for all the meth-
ods to avoid the possible bias introduced by rely-
ing on any particular split of the data. More specif-
ically, the data was randomly assigned to ten folds
of roughly the same size. Each fold was used once
as test data and the remaining nine folds as training
data. The results were then averaged.
Following (Dietterich, 1998), we used McNe-
mar’s test (McNemar, 1947) to measure the statisti-
cal significance between the results of different ML
methods. The chosen significance level was .05.
</bodyText>
<sectionHeader confidence="0.779065" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9991184">
Table 3 shows the results for the four weakly-
supervised and two supervised methods when 10%
of the training data (i.e. ∼700 sentences) has been
labeled. We can see that ASSVM is the best perform-
ing method with an accuracy of 81% and the macro
</bodyText>
<tableCaption confidence="0.999585">
Table 3: Results when using 10% of the labeled data
</tableCaption>
<table confidence="0.959687875">
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
SVM .77 .74 .84 .68 .71 .82 .64 - -
CRF .70 .65 .75 .46 .48 .78 .76 - -
ASVM .80 .75 .88 .56 .68 .87 .78 .33
ASSVM .81 .76 .86 .56 .76 .88 .76 - -
TSVM .76 .73 .84 .61 .71 .79 .71 - -
SSCRF .73 .67 .76 .48 .52 .81 .78 - -
</table>
<footnote confidence="0.428594">
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
</footnote>
<page confidence="0.985586">
277
</page>
<figureCaption confidence="0.983827">
Figure 2: Learning curve for different methods when using 0-100% of the labeled data
</figureCaption>
<figure confidence="0.99557075">
SVM CRF ASVM ASSVM TSVM SSCRF
1
Accuracy
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Labeled
</figure>
<figureCaption confidence="0.971884">
Figure 3: Area under learning curves at different intervals
</figureCaption>
<figure confidence="0.785800333333333">
0.1
0
SVM CRF ASVM ASSVM TSVM SSCRF
</figure>
<bodyText confidence="0.959955368421053">
F-score of .76 (the macro F-score is calculated for
the 5 scheme categories which are found by all the
methods). ASVM performs nearly as well, with an
accuracy of 80% and F-score of .75. Both methods
outperform supervised SVM with a statistically sig-
nificant difference (p &lt; .001).
TSVM is the lowest performing SVM-based
method. Yielding an accuracy of 76% and F-score
of .73 its performance is lower than that of the super-
vised SVM. However, it does outperform both CRF-
based methods. SSCRF performs better than CRF
with 3% higher accuracy and .02 higher F-score.
The difference in accuracy is statistically significant
(p &lt; .001).
Only one method (ASVM) identifies six out of the
seven possible categories. Other methods identify
five categories. The 1-2 missing categories are very
low in frequency (accounting for 1% of the corpus
data each, see table 2). Looking at the results for
other categories, they seem to reflect the amount of
corpus data available for each category (Table 2),
with RES (Results) being the highest and OBJ (Ob-
jective) the lowest performing category with most
methods. Interestingly, the only method that per-
forms relatively well on OBJ is the supervised SVM.
The best method ASSVM outperforms other meth-
ods most clearly on METH (Method) category. Al-
though METH is a high frequency category (account-
ing for 18% of the corpus data) other methods tend
to confuse it with OBJ, presumably because a single
sentence may contain elements of both (e.g. scien-
tists may describe some of their method when de-
scribing the objective of the study).
Figure 2 shows the learning curve of different
methods (in terms of accuracy) when the percentage
of the labeled data (in the training set) ranges from 0
to 100%. ASSVM outperforms other methods, reach-
ing its best performance of 88% accuracy when us-
ing ∼40% of the labeled data. Indeed when using
33% of the labeled data, it performs already equally
well as fully-supervised SVM using 100% of the la-
beled data. The advantage of ASSVM over ASVM
(the second best method) is clear especially when
20-40% of the labeled data is used. SVM and TSVM
tend to perform quite similarly with each other when
more than 25% of the labeled data is used, but when
less data is available, SVM performs better. Look-
ing at the CRF-based methods, SSCRF outperforms
CRF in particular when 10-25% of the labeled data
is used. However, neither of them reaches the per-
formance level of SVM-based methods.
Figure 3 shows the area under the learning curves
(by the trapezoidal rule) at different intervals, which
gives a reasonable approximation to the overall per-
formance of different methods. The area under
ASSVM is the largest at each of the four intervals,
with a value of .08 at (0,10%], .07 at [10%,20%],
</bodyText>
<figure confidence="0.998522545454545">
1
0.9
0.8
0.7
Area
0.6
0.5
0.4
0.3
0.2
(0,10%] [10%,20%] [20%,40%] [40%,100%]
</figure>
<page confidence="0.993037">
278
</page>
<bodyText confidence="0.99828175">
.20 at [20%, 40%] and .50 at [40%,100%]. The dif-
ference between supervised and weakly-supervised
methods is more significant at (0, 20%] than at
[20%,100%].
</bodyText>
<subsectionHeader confidence="0.999283">
4.3 Further analysis of the features
</subsectionHeader>
<bodyText confidence="0.999703">
As explained in section 3.1, we employed in our
experiments a collection of features which had per-
formed well in previous supervised AZ experiments.
We conducted further analysis to investigate which
of these features are the most (and the least) useful
for weakly-supervised learning. We took our best
performing method ASSVM and conducted leave-
one-out analysis of the features with 10% of the la-
beled data. The results are shown in Table 4.
</bodyText>
<tableCaption confidence="0.9985115">
Table 4: Leaving one feature out results for ASSVM when
using 10% of the labeled data
</tableCaption>
<table confidence="0.8990655">
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
Location .73 .67 .67 .55 .62 .85 .65 - -
Word .80 .78 .87 .70 .74 .85 .72 - -
Bigram .81 .75 .83 .57 .71 .87 .78 .33 -
Verb .81 .79 .84 .77 .73 .87 .75 - -
VC .79 .75 .86 .62 .72 .84 .70 - -
POS .74 .70 .66 .65 .66 .82 .73 - -
GR .79 .75 .83 .67 .69 .84 .72 - -
Subj .80 .76 .87 .65 .73 .85 .72 - -
Obj .80 .78 .84 .75 .70 .85 .75 - -
Voice .78 .75 .88 .70 .71 .83 .62 - -
Φ .81 .76 .86 .56 .76 .88 .76 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
Φ: Employing all the features.
</table>
<bodyText confidence="0.999815103448276">
We can see that the Location feature is by far the
most useful feature for ASSVM. The performance
drops 8% in accuracy and .09 in F-score in the ab-
sence of this feature. Location is particularly im-
portant for BKG (which nearly always appears in the
same location: in the beginning of an abstract) and is
highly useful for METH and CON as well. Removing
POS has almost equally strong effect, in particular
on BKG and METH, suggesting that verb tense is par-
ticularly useful for distinguishing these categories.
Also Voice, Verb class and GR contribute to gen-
eral performance, especially to accuracy. Voice is
particularly important for CON, which differs from
other categories in the sense that it is marked by fre-
quent usage of active voice. Verb class is helpful for
METH, RES and CON while GR is helpful for all high
frequency categories.
Among the least helpful features are those which
suffer from sparse data problems, including e.g.
Word, Bi-gram, and Verb. They perform particularly
badly when applied to low frequency zones. How-
ever, this is not the case when using fully-supervised
methods (i.e. 100% of the labeled data), suggest-
ing that a good performance in fully supervised ex-
periments does not necessarily translate into a good
performance in weakly-supervised experiments, and
that careful feature analysis and selection is impor-
tant when aiming to optimize the performance when
learning from sparse data.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999446451612903">
In our experiments, the majority of weakly-
supervised methods outperformed their correspond-
ing supervised methods when using just 10% of
the labeled data. The SVM-based methods per-
formed better than the CRF-based ones (regardless of
whether they were weakly or fully supervised). Guo
et al. (2010) made a similar discovery when com-
paring fully supervised versions of SVM and CRF.
Our best performing weakly-supervised methods
were those based on active learning. Making a good
use of both labeled and unlabeled data, active learn-
ing combined with self-training (ASSVM) proved to
be the most useful method. Given 10% of the la-
beled data, ASSVM obtained an accuracy of 81% and
F-score of .76, outperforming the best supervised
method SVM with a statistically significant differ-
ence. It reached its top performance (88% accuracy)
when using 40% of the labeled data, and performed
equally well as fully supervised SVM (i.e. 100% of
the labeled data) when using just one third of the la-
beled data.
This result is in line with the results of many
other text classification works where active learn-
ing (alone or in combination with other techniques
such as self-training) has proved similarly useful,
e.g. (Lewis and Gale, 1994; Tong and Koller, 2002;
Brinker, 2006; Novak et al., 2006; Esuli and Sebas-
tiani, 2009; Yang et al., 2009).
While active learning iteratively explores the
unknown aspects of the unlabeled data, semi-
supervised learning attempts to make the best use
</bodyText>
<page confidence="0.99523">
279
</page>
<bodyText confidence="0.999992956521739">
of what it already knows about the data. In our ex-
periments, semi-supervised methods (TSVM and SS-
CRF) did not perform equally well as active learning
– TSVM even produced a lower accuracy than SVM
with the same amount of labeled data – although
these methods have gained success in related works.
We therefore looked into related works using
TSVM, e.g. (Chapelle and Zien, 2005), and discov-
ered that our dataset is much higher in dimensional-
ity than those employed in many other works. High
dimensional data is more sensitive, and therefore
fine-tuning with unlabeled data may cause a big de-
viation. We also looked into related works using
SSCRF, in particular the work of (Jiao et al., 2006)
who used the same SSCRF as the one we used in our
experiments. Jiao et al. (2006) employed a much
larger data set than we did – one including 5448 la-
beled instances (in 3 classes) and 5210-25145 unla-
beled instances. Given more labeled and unlabeled
data per class we might be able to obtain better per-
formance using SSCRF also on our task. However,
given the high cost of obtaining labeled data meth-
ods not needing it are preferable.
</bodyText>
<sectionHeader confidence="0.983312" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999974142857143">
Our experiments show that weakly-supervised
learning can be used to identify AZ in scientific
documents with good accuracy when only a limited
amount of labeled data is available. This is helpful
thinking of the real-world application and porting of
the approach to different tasks and domains. To the
best of our knowledge, no previous work has been
done on weakly-supervised learning of information
structure according to schemes of the type we have
focused on (Teufel and Moens, 2002; Mizuta et al.,
2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay
et al., 2008; Liakata et al., 2010).
Recently, some work has been done on the related
task of classification of discourse relations in sci-
entific texts: (Hernault et al., 2011) used structural
learning (Ando and Zhang, 2005) for this task. They
obtained 30-60% accuracy on the RST Discourse
Treebank (including 41 relation types) when using
100-10000 labeled and 100000 unlabeled instances.
The accuracy was 20-60% when using the labeled
data only. However, although related, the task of
discourse relation classification differs substantially
from our task in that it focuses on local discourse re-
lations while our task focuses on the global structure
of the scientific document.
In the future, we plan to improve and extend this
work in several directions. First, the approach to
active learning could be improved in various ways.
The query strategy we employed (uncertainty sam-
pling) is a relatively straightforward method which
only considers the best estimate for each unlabeled
instance, disregarding other estimates that may con-
tain useful information. In the future, we plan to
experiment with more sophisticated strategies, e.g.
the margin sampling algorithm by (Scheffer et al.,
2001) and the query-by-committee (QBC) algorithm
by (Seung et al., 1992). In addition, there are al-
gorithms designed for reducing the redundancy in
queries which may be worth investigating (Hoi et al.,
2006).
Also, (Hoi et al., 2006) shows that Logistic Re-
gression (LR) outperforms SVM when used with ac-
tive learning, yielding higher F-score on the Reuters-
21578 data set (binary classification, 10,788 docu-
ments in total, 100 of them labeled). It would be
interesting to explore whether supervised methods
other than SVM are optimal for active learning when
applied to our task.
Secondly, we plan to investigate other semi-
supervised methods, for example, the Expectation-
Maximization (EM) algorithm. (Lanquillon, 2000)
has shown that EM SVM performs better than super-
vised and transductive SVM on a text classification
task when applied to the dataset of 20 Newsgroups
(20 classes, 4000 documents for testing, 10000 un-
labeled ones), yielding up to ∼10% higher accu-
racy when 200-5000 labeled documents are used for
training.
In addition, other combinations of weakly-
supervised methods might be worth looking into,
such as EM+active learning (McCallum and Nigam,
1998) and co-training+EM+active learning (Muslea
et al., 2002), which have proved promising in related
text classification works.
Besides looking for optimal ML strategies, we
plan to look for optimal features for the task. Our
feature analysis showed that not all the features
which had proved promising in fully supervised ex-
periments were equally promising when applied to
weakly-supervised learning from smaller data. We
</bodyText>
<page confidence="0.977197">
280
</page>
<bodyText confidence="0.99994178125">
plan to look into ways of reducing the sparse data
problem in features, e.g. by classifying not only
verbs but also other word classes into semantically-
motivated categories.
One the key motivations for developing a weakly-
supervised approach is to facilitate easy porting of
schemes such as AZ to new tasks and domains. Re-
cent research shows that active learning in a target
domain can leverage information from a different
but related (source) domain (Rai et al., 2010). Mak-
ing use of existing annotated datasets in biology,
chemistry, computational linguistics and law (Teufel
and Moens, 2002; Mizuta et al., 2006; Hachey
and Grover, 2006; Teufel et al., 2009) we will ex-
plore optimal ways of combining weakly-supervised
learning with domain-adaptation.
The work presented in this paper has focused on
the abstracts annotated according to the AZ scheme.
In the future, we plan to investigate the usefulness
of weakly-supervised learning for identifying other
schemes of information structure, e.g. (Lin et al.,
2006; Hirohata et al., 2008; Shatkay et al., 2008;
Liakata et al., 2010), and not only in scientific ab-
stracts but also in full journal papers which typically
exemplify a larger set of scheme categories.
Finally, an important avenue of future research
is to evaluate the usefulness of weakly-supervised
identification of information structure for NLP tasks
such as summarization and information extraction
(Tbahriti et al., 2006; Ruch et al., 2007), and for
practical tasks such as manual review of scientific
papers for research purposes (Guo et al., 2010).
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.973943333333333">
The work reported in this paper was funded by the
Royal Society (UK). YG was funded by the Cam-
bridge International Scholarship.
</bodyText>
<sectionHeader confidence="0.997582" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99155519047619">
Steven Abney. 2008. Semi-supervised learning for com-
putational linguistics. Chapman &amp; Hall / CRC.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817–
1853.
Klaus Brinker. 2006. On active learning in multi-label
classification. In From Data and Information Analysis
to Knowledge Engineering, pages 206–213.
Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(1):37–46.
Ronan Collobert, Fabian Sinz, Jason Weston, and L´eon
Bottou. 2006. Trading convexity for scalability. In
Proceedings of the 23rd international conference on
Machine learning.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&amp;c and boxer. In Pro-
ceedings of the ACL 2007 Demonstrations Session.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Comput., 10:1895–1923.
Andrea Esuli and Fabrizio Sebastiani. 2009. Active
learning strategies for multi-label text classification.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305–
345.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10–18.
T. Hastie and R. Tibshirani. 1998. Classification by pair-
wise coupling. Advances in Neural Information Pro-
cessing Systems, 10.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structural learning. In CICLing (1).
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing.
Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-scale text categorization by batch mode active
learning. In Proceedings of the 15th international con-
ference on World Wide Web.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL.
Carsten Lanquillon. 2000. Learning from labeled and
unlabeled documents: A comparative study on semi-
supervised text classification. In Proceedings of the
</reference>
<page confidence="0.98594">
281
</page>
<reference confidence="0.997808075471698">
4th European Conference on Principles of Data Min-
ing and Knowledge Discovery.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batche-
lor. 2010. Corpora for the conceptualisation and zon-
ing of scientific papers. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC’10).
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06.
G. S. Mann and A. Mccallum. 2007. Efficient compu-
tation of entropy gradient for semi-supervised condi-
tional random fields. In HLT-NAACL.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing em and pool-based active learning for text classi-
fication. In Proceedings of the Fifteenth International
Conference on Machine Learning.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference Between Correlated Proportions or
Percentages. Psychometrika, 12(2):153–157.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468–487.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A base-
line feature set for learning rhetorical zones using full
articles in the biomedical domain. Natural language
processing and text mining, 7(1):52–58.
Ion Muslea, Steven Minton, and Craig A. Knoblock.
2002. Active + semi-supervised learning = robust
multi-view learning. In Proceedings of the Nineteenth
International Conference on Machine Learning.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773–782.
Bla Novak, Dunja Mladeni, and Marko Grobelnik. 2006.
Text classification with active learning. In From Data
and Information Analysis to Knowledge Engineering,
pages 398–405.
J. C. Platt. 1999a. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classiers,
pages 61–74.
John C. Platt. 1999b. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of the 1998 conference on Advances in neural
information processing systems II.
Piyush Rai, Avishek Saha, Hal Daum´e, III, and Suresh
Venkatasubramanian. 2010. Domain adaptation
meets active learning. In Proceedings of the NAACL
HLT 2010 Workshop on Active Learning for Natural
Language Processing.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195–200.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In Proceedings of the 4th International
Conference on Advances in Intelligent Data Analysis.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the fifth an-
nual workshop on Computational learning theory.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086–
2093.
F. Sinz, 2011. UniverSVM Support Vector Ma-
chine with Large Scale CCCP Functionality.
http://www.kyb.mpg.de/bs/people/fabee/universvm.html.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488–495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409–445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning: Ev-
idence from chemistry and computational linguistics.
In Proceedings of EMNLP.
S. Tong and D. Koller. 2001. Support vector machine
active learning with applications to text classification.
Journal of Machine Learning Research, 2:45–66.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45–66.
</reference>
<page confidence="0.960298">
282
</page>
<reference confidence="0.996062428571428">
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng
Chen. 2009. Effective multi-label active learning for
text classification. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
</reference>
<page confidence="0.99895">
283
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.561957">
<title confidence="0.997413">A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents</title>
<author confidence="0.988695">Yufan Guo Anna Korhonen Thierry Poibeau</author>
<affiliation confidence="0.797901">Computer Laboratory Computer Laboratory LaTTiCe, UMR8094 University of Cambridge, UK University of Cambridge, UK CNRS &amp; ENS, France</affiliation>
<email confidence="0.923665">yg244@cam.ac.ukalk23@cam.ac.ukthierry.poibeau@ens.fr</email>
<abstract confidence="0.9987725">Argumentative Zoning (AZ) – analysis of the argumentative structure of a scientific paper – has proved useful for a number of information access tasks. Current approaches to AZ rely on supervised machine learning (ML). Requiring large amounts of annotated data, these approaches are expensive to develop and port to different domains and tasks. A potential solution to this problem is to use weaklysupervised ML instead. We investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple AZ classes. Our best classifier based on the combination of active learning and selftraining outperforms our best supervised classifier, yielding a high accuracy of 81% when using just 10% of the labeled data. This result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of AZ across different information access tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Semi-supervised learning for computational linguistics.</title>
<date>2008</date>
<publisher>Chapman &amp; Hall / CRC.</publisher>
<contexts>
<context position="3279" citStr="Abney, 2008" startWordPosition="486" endWordPosition="487">isting approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML. Relying on a small amount of labeled data and a large pool of unlabeled data, weakly-supervised techniques (e.g. semi-supervision, active learning, co/tri-training, self-training) aim to keep the advantages of fully supervised approaches. They have been applied to a wide range of NLP tasks, including named-entity recognition, question answering, information extraction, text classification and many others (Abney, 2008), yielding performance levels similar or equivalent to those of fully supervised techniques. To the best of our knowledge, such techniques 273 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273–283, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics have not yet been applied to the analysis of information structure of scientific documents by aforementioned approaches. Recent experiments have demonstrated the usefulness of weakly-supervised learning for classifying discourse relations in scientific texts, e.</context>
</contexts>
<marker>Abney, 2008</marker>
<rawString>Steven Abney. 2008. Semi-supervised learning for computational linguistics. Chapman &amp; Hall / CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>6</volume>
<pages>1853</pages>
<contexts>
<context position="28780" citStr="Ando and Zhang, 2005" startWordPosition="4818" endWordPosition="4821">data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from our task in that it focuses on local discourse relations while our task focuses on the global structure of the scientific document. In the future, we plan to improve and extend this work in several directions. First, the approach to active learning could be improved in vario</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:1817– 1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Brinker</author>
</authors>
<title>On active learning in multi-label classification.</title>
<date>2006</date>
<booktitle>In From Data and Information Analysis to Knowledge Engineering,</booktitle>
<pages>206--213</pages>
<contexts>
<context position="26627" citStr="Brinker, 2006" startWordPosition="4453" endWordPosition="4454">tained an accuracy of 81% and F-score of .76, outperforming the best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised SVM (i.e. 100% of the labeled data) when using just one third of the labeled data. This result is in line with the results of many other text classification works where active learning (alone or in combination with other techniques such as self-training) has proved similarly useful, e.g. (Lewis and Gale, 1994; Tong and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien, 2005), and di</context>
</contexts>
<marker>Brinker, 2006</marker>
<rawString>Klaus Brinker. 2006. On active learning in multi-label classification. In From Data and Information Analysis to Knowledge Engineering, pages 206–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Alexander Zien</author>
</authors>
<title>Semisupervised classification by low density separation.</title>
<date>2005</date>
<contexts>
<context position="27219" citStr="Chapelle and Zien, 2005" startWordPosition="4551" endWordPosition="4554"> and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien, 2005), and discovered that our dataset is much higher in dimensionality than those employed in many other works. High dimensional data is more sensitive, and therefore fine-tuning with unlabeled data may cause a big deviation. We also looked into related works using SSCRF, in particular the work of (Jiao et al., 2006) who used the same SSCRF as the one we used in our experiments. Jiao et al. (2006) employed a much larger data set than we did – one including 5448 labeled instances (in 3 classes) and 5210-25145 unlabeled instances. Given more labeled and unlabeled data per class we might be able to o</context>
</contexts>
<marker>Chapelle, Zien, 2005</marker>
<rawString>Olivier Chapelle and Alexander Zien. 2005. Semisupervised classification by low density separation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="10805" citStr="Cohen, 1960" startWordPosition="1677" endWordPosition="1678">res METH RES CON REL FUT detected in controls. OBJ BKG Word 36828 23493 41544 89538 30752 2456 1174 Sentence 1429 674 1473 3185 1082 95 47 evaluation of HB-Val in multiple species. internal standard for parallel analysis by GC-MS/MS and LC 18% 40% 14% 1% 1% Sentence 18% 8% 8-18% of the corpus each. Two categories are very low in frequency, only covering 1% of the corpus each: Related work (REL) and Future work (FUT). Guo et al. (2010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Merity et al., 2009) we re-implemented and used them in our experiment1. However, being aware of the fact that some of these features may not be optimal for weakly-supervised lea</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Fabian Sinz</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
</authors>
<title>Trading convexity for scalability.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning.</booktitle>
<contexts>
<context position="17392" citStr="Collobert et al., 2006" startWordPosition="2827" endWordPosition="2830">g; (ii) training on both labeled and unlabeled/machine-labeled data by using the estimates from step (i). The idea of ASSVM is to make the best use of the labeled data, and to make the most use of the unlabeled data. Transductive SVM (TSVM) is an extension of SVM which takes advantage of both labeled and unlabeled data (Vapnik, 1998). Similar to SVM, the problem is defined as: Minimize |w |(in w, b, y(u)) Subject to y(l)(w x(l) − b) &gt; 1, y(u)(w x(u) − b) &gt; 1 , y(u) E {−1,1}, where x(u) is unlabeled data and y(u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data EL log p(y(l) |x(l), 0) with an additional term EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize the conditional entropy of the model’s predictions on Unlabeled data (Jiao et al., 2006; Mann and Mccallum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measur</context>
</contexts>
<marker>Collobert, Sinz, Weston, Bottou, 2006</marker>
<rawString>Ronan Collobert, Fabian Sinz, Jason Weston, and L´eon Bottou. 2006. Trading convexity for scalability. In Proceedings of the 23rd international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>S Clark</author>
<author>J Bos</author>
</authors>
<title>Linguistically motivated large-scale nlp with c&amp;c and boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="12908" citStr="Curran et al., 2007" startWordPosition="2040" endWordPosition="2043">in the corpus. e.g. (ncsubj observed 14 difference 5 obj). The value of this feature equals 1 if it occurs in a particular sentence (and 0 if not). • Subj and Obj. The subjects and objects appearing with any verbs in the corpus (extracted from above GRs). • Voice. The voice of verbs (active or passive) in the corpus. These features were extracted from the corpus using a number of tools. A tokenizer was used to detect the boundaries of sentences and to separate punctuation from adjacent words e.g. in complex biomedical terms such as 2-amino-3,8-diethylimidazo[4,5- f]quinoxaline. The C&amp;C tools (Curran et al., 2007) trained on biomedical literature were employed for POS tagging, lemmatization and parsing. The lemma output was used for creating Word, Bi-gram and Verb features. The GR output was used for creating the GR, Subj, Obj and Voice features. The ”obj” marker in a subject relation indicates passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words and</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically motivated large-scale nlp with c&amp;c and boxer. In Proceedings of the ACL 2007 Demonstrations Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Comput.,</journal>
<pages>10--1895</pages>
<contexts>
<context position="18687" citStr="Dietterich, 1998" startWordPosition="3044" endWordPosition="3045">tences aCC = total no. ofsentences in the corpus no. of sentences correctly identified as Classi p = total no. of sentences identified as Classi no. of sentences correctly identified as Classi r = total no. of sentences in Classi f_ 2∗p∗r - p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data. More specifically, the data was randomly assigned to ten folds of roughly the same size. Each fold was used once as test data and the remaining nine folds as training data. The results were then averaged. Following (Dietterich, 1998), we used McNemar’s test (McNemar, 1947) to measure the statistical significance between the results of different ML methods. The chosen significance level was .05. 4.2 Results Table 3 shows the results for the four weaklysupervised and two supervised methods when 10% of the training data (i.e. ∼700 sentences) has been labeled. We can see that ASSVM is the best performing method with an accuracy of 81% and the macro Table 3: Results when using 10% of the labeled data Acc. F-score MF BKG OBJ METH RES CON REL FUT SVM .77 .74 .84 .68 .71 .82 .64 - - CRF .70 .65 .75 .46 .48 .78 .76 - - ASVM .80 .7</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Comput., 10:1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Active learning strategies for multi-label text classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval.</booktitle>
<contexts>
<context position="26675" citStr="Esuli and Sebastiani, 2009" startWordPosition="4459" endWordPosition="4463">re of .76, outperforming the best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised SVM (i.e. 100% of the labeled data) when using just one third of the labeled data. This result is in line with the results of many other text classification works where active learning (alone or in combination with other techniques such as self-training) has proved similarly useful, e.g. (Lewis and Gale, 1994; Tong and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien, 2005), and discovered that our dataset is much higher in dime</context>
</contexts>
<marker>Esuli, Sebastiani, 2009</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2009. Active learning strategies for multi-label text classification. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Anna Korhonen</author>
<author>Maria Liakata</author>
<author>Ilona Silins Karolinska</author>
<author>Lin Sun</author>
<author>Ulla Stenius</author>
</authors>
<title>Identifying the information structure of scientific abstracts: an investigation of three different schemes.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing.</booktitle>
<contexts>
<context position="5996" citStr="Guo et al., 2010" startWordPosition="897" endWordPosition="900">self-training). The results are promising. Our best weaklysupervised classifier (Active SVM with selftraining) outperforms the best supervised classifier (SVM), yielding high accuracy of 81% when using just 10% of the labeled data. When using just one third of the labeled data, it performs equally well as a fully supervised SVM which uses 100% of the labeled data. Our investigation suggests that weaklysupervised learning could be employed to improve the practical applicability and portability of AZ to different information access tasks. 2 Data We used in our experiments the recent dataset of (Guo et al., 2010). Guo et al. (2010) provide a corpus of 1000 biomedical abstracts (consisting of 7985 sentences and 225785 words) annotated according to three schemes of information structure – those based on section names (Hirohata et al., 2008), AZ (Mizuta et al., 2006) and Core Scientific Concepts (CoreSC) (Liakata et al., 2010). We focus here on AZ only, because it subsumes all the categories of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ</context>
<context position="8215" citStr="Guo et al., 2010" startWordPosition="1268" endWordPosition="1271"> characteristic set of procedures employed in a field of study as a mode of investigation and inquiry. An example of a biomedical abstract annotated according to AZ is shown in Figure 1, with different zones highlighted in different colors. For example, the RES zone is highlighted in lemon green. Table 2 shows the distribution of sentences per scheme category in the corpus: Results (RES) is by far the most frequent zone (accounting for 40% of the corpus), while Background (BKG), Objective (OBJ), Method (METH) and Conclusion (CON) cover 274 Table 1: Categories of AZ appearing in the corpus of (Guo et al., 2010) Category Abbr. Definition Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc. Objective OBJ A thing aimed at or sought, a target or goal Method METH A way of doing research, esp. according to a defined and regular plan; a special form of procedure or characteristic set of procedures employed in a field of study as a mode of investigation and inquiry Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained by calculation Conclusion CON A judgment or statement arrived at by any reasoning proc</context>
<context position="10631" citStr="Guo et al. (2010)" startWordPosition="1651" endWordPosition="1654">efficient of variation &lt;25%. The LOQ was set to 100 fmol Table 2: Distribution of sentences in the AZ-annotated corpus Method BD for 90 days were analyzed. The amounts of HB-Val pres METH RES CON REL FUT detected in controls. OBJ BKG Word 36828 23493 41544 89538 30752 2456 1174 Sentence 1429 674 1473 3185 1082 95 47 evaluation of HB-Val in multiple species. internal standard for parallel analysis by GC-MS/MS and LC 18% 40% 14% 1% 1% Sentence 18% 8% 8-18% of the corpus each. Two categories are very low in frequency, only covering 1% of the corpus each: Related work (REL) and Future work (FUT). Guo et al. (2010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Meri</context>
<context position="13877" citStr="Guo et al., 2010" startWordPosition="2198" endWordPosition="2201">erb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and without self-training, transductive SVM and semi-supervised CRF. 3.2.1 Supervised methods SVM constructs hyperplanes in a multidimensional space to separate data points of different classes. Good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance between two parall</context>
<context position="25666" citStr="Guo et al. (2010)" startWordPosition="4292" endWordPosition="4295">the labeled data), suggesting that a good performance in fully supervised experiments does not necessarily translate into a good performance in weakly-supervised experiments, and that careful feature analysis and selection is important when aiming to optimize the performance when learning from sparse data. 5 Discussion In our experiments, the majority of weaklysupervised methods outperformed their corresponding supervised methods when using just 10% of the labeled data. The SVM-based methods performed better than the CRF-based ones (regardless of whether they were weakly or fully supervised). Guo et al. (2010) made a similar discovery when comparing fully supervised versions of SVM and CRF. Our best performing weakly-supervised methods were those based on active learning. Making a good use of both labeled and unlabeled data, active learning combined with self-training (ASSVM) proved to be the most useful method. Given 10% of the labeled data, ASSVM obtained an accuracy of 81% and F-score of .76, outperforming the best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as full</context>
</contexts>
<marker>Guo, Korhonen, Liakata, Karolinska, Sun, Stenius, 2010</marker>
<rawString>Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins Karolinska, Lin Sun, and Ulla Stenius. 2010. Identifying the information structure of scientific abstracts: an investigation of three different schemes. In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Claire Grover</author>
</authors>
<title>Extractive summarisation of legal texts.</title>
<date>2006</date>
<journal>Artif. Intell. Law,</journal>
<volume>14</volume>
<pages>345</pages>
<contexts>
<context position="4530" citStr="Hachey and Grover, 2006" startWordPosition="666" endWordPosition="669"> However, focusing on local (rather than global) structure of documents and being much more fine-grained in nature, this related task differs from ours considerably. In this paper, we investigate the potential of weakly-supervised learning for Argumentative Zoning (AZ) of scientific abstracts. AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (Teufel and Moens, 2002). It has been used to analyze scientific texts in various disciplines – including computational linguistics (Teufel and Moens, 2002), law, (Hachey and Grover, 2006), biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009) – and has proved useful for NLP tasks such as summarization (Teufel and Moens, 2002). Although the basic scheme is said to be disciplineindependent (Teufel et al., 2009), its application to different domains has resulted in various modifications and laborious annotation exercises. This suggests that a weakly-supervised approach would be more practical than a fully supervised one for the real-world application of AZ. Taking two supervised classifiers as a comparison point – Support Vector Machines (SVM) and Conditional Random F</context>
<context position="6864" citStr="Hachey and Grover, 2006" startWordPosition="1039" endWordPosition="1042">t al., 2006) and Core Scientific Concepts (CoreSC) (Liakata et al., 2010). We focus here on AZ only, because it subsumes all the categories of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ is a scheme which provides an analysis of the rhetorical progression of the scientific argument, following the knowledge claims made by authors. (Teufel and Moens, 2002) introduced AZ and applied it first to computational linguistics papers. (Hachey and Grover, 2006) applied the scheme later to legal texts and (Mizuta et al., 2006) modified it for biology papers. More recently, (Teufel et al., 2009) introduced a refined version of AZ and applied it to chemistry papers. The biomedical dataset of (Guo et al., 2010) has been annotated according to the version of AZ developed for biology papers (Mizuta et al., 2006) (with only minor modifications concerning zone names). Seven categories of this scheme (out of the 10 possible) actually appear in abstracts and in the resulting corpus. These are shown and explained in Table 1. For example, the Method zone (METH)</context>
<context position="31973" citStr="Hachey and Grover, 2006" startWordPosition="5316" endWordPosition="5319">o ways of reducing the sparse data problem in features, e.g. by classifying not only verbs but also other word classes into semanticallymotivated categories. One the key motivations for developing a weaklysupervised approach is to facilitate easy porting of schemes such as AZ to new tasks and domains. Recent research shows that active learning in a target domain can leverage information from a different but related (source) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories. Finally, an important a</context>
</contexts>
<marker>Hachey, Grover, 2006</marker>
<rawString>Ben Hachey and Claire Grover. 2006. Extractive summarisation of legal texts. Artif. Intell. Law, 14:305– 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<pages>11--10</pages>
<contexts>
<context position="14874" citStr="Hall et al., 2009" startWordPosition="2395" endWordPosition="2398">om the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance between two parallel hyperplanes each of which separates the data. The parallel hyperplanes can be written as: w · x − b = 1 and w · x − b = −1, and the distance between them is 2 w . The problem reduces to: Minimize |w |(in w, b) Subject to w · x − b &gt; 1 for x of one class, w · x − b &lt; −1 for x of the other, which can be solved by using the SMO algorithm (Platt, 1999b). We used Weka software (Hall et al., 2009) (employing its linear kernel) for SVM experiments. CRF is an undirected graphical model which defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as: p(y|x, 0) = 1 Z(x)exp(�j 0jFj(y, x)), where Fj(y, x) is a real-valued feature function of the states and the observations; 0j is the weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF exp</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
</authors>
<title>Classification by pairwise coupling.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>10</pages>
<contexts>
<context position="16435" citStr="Hastie and Tibshirani, 1998" startWordPosition="2653" endWordPosition="2656">strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that the SVM is uncertain about. Unlike these methods, our learning algorithm compares the posterior probabilities of the best estimate given each unlabeled instance, and queries those with the lowest probabilities for the next round of learning. The probabilities can be obtained by fitting a Sigmoid after the standard SVM (Platt, 1999a), and combined using a pairwise coupling algorithm (Hastie and Tibshirani, 1998) in the multi-class case. We used the SVM linear kernel in Weka for classification, and the -M flag in Weka for calculating the posterior probabilities. Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has two steps: (i) training on the labeled, and testing on the unlabeled data, and querying; (ii) training on both labeled and unlabeled/machine-labeled data by using the estimates from step (i). The idea of ASSVM is to make the best use of the labeled data, and to make the most use of the unlabeled data. Transductive SVM (TSVM) is an extension of SVM wh</context>
</contexts>
<marker>Hastie, Tibshirani, 1998</marker>
<rawString>T. Hastie and R. Tibshirani. 1998. Classification by pairwise coupling. Advances in Neural Information Processing Systems, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Semi-supervised discourse relation classification with structural learning.</title>
<date>2011</date>
<booktitle>In CICLing (1).</booktitle>
<contexts>
<context position="3905" citStr="Hernault et al., 2011" startWordPosition="571" endWordPosition="574">elding performance levels similar or equivalent to those of fully supervised techniques. To the best of our knowledge, such techniques 273 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273–283, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics have not yet been applied to the analysis of information structure of scientific documents by aforementioned approaches. Recent experiments have demonstrated the usefulness of weakly-supervised learning for classifying discourse relations in scientific texts, e.g. (Hernault et al., 2011). However, focusing on local (rather than global) structure of documents and being much more fine-grained in nature, this related task differs from ours considerably. In this paper, we investigate the potential of weakly-supervised learning for Argumentative Zoning (AZ) of scientific abstracts. AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (Teufel and Moens, 2002). It has been used to analyze scientific texts in various disciplines – including computational linguistics (Teufel and Moens, 2002), law, </context>
<context position="28732" citStr="Hernault et al., 2011" startWordPosition="4811" endWordPosition="4814">d accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from our task in that it focuses on local discourse relations while our task focuses on the global structure of the scientific document. In the future, we plan to improve and extend this work in several directions. First, the approa</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2011</marker>
<rawString>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2011. Semi-supervised discourse relation classification with structural learning. In CICLing (1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hirohata</author>
<author>N Okazaki</author>
<author>S Ananiadou</author>
<author>M Ishizuka</author>
</authors>
<title>Identifying sections in scientific abstracts using conditional random fields.</title>
<date>2008</date>
<booktitle>In Proceedings of 3rd International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="2099" citStr="Hirohata et al., 2008" startWordPosition="308" endWordPosition="311">methods used in the study, the results obtained, or the conclusions drawn by authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive </context>
<context position="6226" citStr="Hirohata et al., 2008" startWordPosition="934" endWordPosition="937"> data. When using just one third of the labeled data, it performs equally well as a fully supervised SVM which uses 100% of the labeled data. Our investigation suggests that weaklysupervised learning could be employed to improve the practical applicability and portability of AZ to different information access tasks. 2 Data We used in our experiments the recent dataset of (Guo et al., 2010). Guo et al. (2010) provide a corpus of 1000 biomedical abstracts (consisting of 7985 sentences and 225785 words) annotated according to three schemes of information structure – those based on section names (Hirohata et al., 2008), AZ (Mizuta et al., 2006) and Core Scientific Concepts (CoreSC) (Liakata et al., 2010). We focus here on AZ only, because it subsumes all the categories of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ is a scheme which provides an analysis of the rhetorical progression of the scientific argument, following the knowledge claims made by authors. (Teufel and Moens, 2002) introduced AZ and applied it first to computational linguis</context>
<context position="11225" citStr="Hirohata et al., 2008" startWordPosition="1745" endWordPosition="1748">rk (FUT). Guo et al. (2010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Merity et al., 2009) we re-implemented and used them in our experiment1. However, being aware of the fact that some of these features may not be optimal for weakly-supervised learning (i.e. when learning from smaller data), we evaluate their performance and suitability for the task later in section 4.3. • Location. Zones tend to appear in typical positions in abstracts. Each abstract was there1The only exception is the history feature which was left out because it cannot be applied to all of our methods 275 fore divided into ten parts (1-10, measured by the number of words), and the location</context>
<context position="13858" citStr="Hirohata et al., 2008" startWordPosition="2194" endWordPosition="2197">fference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and without self-training, transductive SVM and semi-supervised CRF. 3.2.1 Supervised methods SVM constructs hyperplanes in a multidimensional space to separate data points of different classes. Good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance</context>
<context position="28546" citStr="Hirohata et al., 2008" startWordPosition="4780" endWordPosition="4783">ata methods not needing it are preferable. 6 Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from our task in that it focuses on local disc</context>
<context position="32373" citStr="Hirohata et al., 2008" startWordPosition="5377" endWordPosition="5380">rent but related (source) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories. Finally, an important avenue of future research is to evaluate the usefulness of weakly-supervised identification of information structure for NLP tasks such as summarization and information extraction (Tbahriti et al., 2006; Ruch et al., 2007), and for practical tasks such as manual review of scientific papers for research purposes (Guo et al., 2010). Acknowledgments The work reported in this paper was funded by the Ro</context>
</contexts>
<marker>Hirohata, Okazaki, Ananiadou, Ishizuka, 2008</marker>
<rawString>K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka. 2008. Identifying sections in scientific abstracts using conditional random fields. In Proceedings of 3rd International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven C H Hoi</author>
<author>Rong Jin</author>
<author>Michael R Lyu</author>
</authors>
<title>Large-scale text categorization by batch mode active learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web.</booktitle>
<contexts>
<context position="29952" citStr="Hoi et al., 2006" startWordPosition="4998" endWordPosition="5001">to active learning could be improved in various ways. The query strategy we employed (uncertainty sampling) is a relatively straightforward method which only considers the best estimate for each unlabeled instance, disregarding other estimates that may contain useful information. In the future, we plan to experiment with more sophisticated strategies, e.g. the margin sampling algorithm by (Scheffer et al., 2001) and the query-by-committee (QBC) algorithm by (Seung et al., 1992). In addition, there are algorithms designed for reducing the redundancy in queries which may be worth investigating (Hoi et al., 2006). Also, (Hoi et al., 2006) shows that Logistic Regression (LR) outperforms SVM when used with active learning, yielding higher F-score on the Reuters21578 data set (binary classification, 10,788 documents in total, 100 of them labeled). It would be interesting to explore whether supervised methods other than SVM are optimal for active learning when applied to our task. Secondly, we plan to investigate other semisupervised methods, for example, the ExpectationMaximization (EM) algorithm. (Lanquillon, 2000) has shown that EM SVM performs better than supervised and transductive SVM on a text clas</context>
</contexts>
<marker>Hoi, Jin, Lyu, 2006</marker>
<rawString>Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006. Large-scale text categorization by batch mode active learning. In Proceedings of the 15th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jiao</author>
<author>S Wang</author>
<author>C Lee</author>
<author>R Greiner</author>
<author>D Schuurmans</author>
</authors>
<title>Semi-supervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In COLING/ACL.</booktitle>
<contexts>
<context position="17770" citStr="Jiao et al., 2006" startWordPosition="2888" endWordPosition="2891">mize |w |(in w, b, y(u)) Subject to y(l)(w x(l) − b) &gt; 1, y(u)(w x(u) − b) &gt; 1 , y(u) E {−1,1}, where x(u) is unlabeled data and y(u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data EL log p(y(l) |x(l), 0) with an additional term EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize the conditional entropy of the model’s predictions on Unlabeled data (Jiao et al., 2006; Mann and Mccallum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: no. of correctly classified sentences aCC = total no. ofsentences in the corpus no. of sentences correctly identified as Classi p = total no. of sentences identified as Classi no. of sentences correctly identified as Classi r = total no. of sentences in Classi f_ 2∗p∗r - p+r We used 10-fold cross validation for all the methods to</context>
<context position="27533" citStr="Jiao et al., 2006" startWordPosition="4605" endWordPosition="4608">ed methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien, 2005), and discovered that our dataset is much higher in dimensionality than those employed in many other works. High dimensional data is more sensitive, and therefore fine-tuning with unlabeled data may cause a big deviation. We also looked into related works using SSCRF, in particular the work of (Jiao et al., 2006) who used the same SSCRF as the one we used in our experiments. Jiao et al. (2006) employed a much larger data set than we did – one including 5448 labeled instances (in 3 classes) and 5210-25145 unlabeled instances. Given more labeled and unlabeled data per class we might be able to obtain better performance using SSCRF also on our task. However, given the high cost of obtaining labeled data methods not needing it are preferable. 6 Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a </context>
</contexts>
<marker>Jiao, Wang, Lee, Greiner, Schuurmans, 2006</marker>
<rawString>F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuurmans. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Lanquillon</author>
</authors>
<title>Learning from labeled and unlabeled documents: A comparative study on semisupervised text classification.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery.</booktitle>
<contexts>
<context position="30462" citStr="Lanquillon, 2000" startWordPosition="5079" endWordPosition="5080">orithms designed for reducing the redundancy in queries which may be worth investigating (Hoi et al., 2006). Also, (Hoi et al., 2006) shows that Logistic Regression (LR) outperforms SVM when used with active learning, yielding higher F-score on the Reuters21578 data set (binary classification, 10,788 documents in total, 100 of them labeled). It would be interesting to explore whether supervised methods other than SVM are optimal for active learning when applied to our task. Secondly, we plan to investigate other semisupervised methods, for example, the ExpectationMaximization (EM) algorithm. (Lanquillon, 2000) has shown that EM SVM performs better than supervised and transductive SVM on a text classification task when applied to the dataset of 20 Newsgroups (20 classes, 4000 documents for testing, 10000 unlabeled ones), yielding up to ∼10% higher accuracy when 200-5000 labeled documents are used for training. In addition, other combinations of weaklysupervised methods might be worth looking into, such as EM+active learning (McCallum and Nigam, 1998) and co-training+EM+active learning (Muslea et al., 2002), which have proved promising in related text classification works. Besides looking for optimal</context>
</contexts>
<marker>Lanquillon, 2000</marker>
<rawString>Carsten Lanquillon. 2000. Learning from labeled and unlabeled documents: A comparative study on semisupervised text classification. In Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="26589" citStr="Lewis and Gale, 1994" startWordPosition="4445" endWordPosition="4448">thod. Given 10% of the labeled data, ASSVM obtained an accuracy of 81% and F-score of .76, outperforming the best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised SVM (i.e. 100% of the labeled data) when using just one third of the labeled data. This result is in line with the results of many other text classification works where active learning (alone or in combination with other techniques such as self-training) has proved similarly useful, e.g. (Lewis and Gale, 1994; Tong and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, </context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liakata</author>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>C Batchelor</author>
</authors>
<title>Corpora for the conceptualisation and zoning of scientific papers.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="2312" citStr="Liakata et al., 2010" startWordPosition="341" endWordPosition="344">ly. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML. Relying on a small amount of labeled data and a large </context>
<context position="6313" citStr="Liakata et al., 2010" startWordPosition="948" endWordPosition="951">y supervised SVM which uses 100% of the labeled data. Our investigation suggests that weaklysupervised learning could be employed to improve the practical applicability and portability of AZ to different information access tasks. 2 Data We used in our experiments the recent dataset of (Guo et al., 2010). Guo et al. (2010) provide a corpus of 1000 biomedical abstracts (consisting of 7985 sentences and 225785 words) annotated according to three schemes of information structure – those based on section names (Hirohata et al., 2008), AZ (Mizuta et al., 2006) and Core Scientific Concepts (CoreSC) (Liakata et al., 2010). We focus here on AZ only, because it subsumes all the categories of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ is a scheme which provides an analysis of the rhetorical progression of the scientific argument, following the knowledge claims made by authors. (Teufel and Moens, 2002) introduced AZ and applied it first to computational linguistics papers. (Hachey and Grover, 2006) applied the scheme later to legal texts and (Miz</context>
<context position="28591" citStr="Liakata et al., 2010" startWordPosition="4788" endWordPosition="4791">Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from our task in that it focuses on local discourse relations while our task focuses on the</context>
<context position="32418" citStr="Liakata et al., 2010" startWordPosition="5385" endWordPosition="5388"> 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories. Finally, an important avenue of future research is to evaluate the usefulness of weakly-supervised identification of information structure for NLP tasks such as summarization and information extraction (Tbahriti et al., 2006; Ruch et al., 2007), and for practical tasks such as manual review of scientific papers for research purposes (Guo et al., 2010). Acknowledgments The work reported in this paper was funded by the Royal Society (UK). YG was funded by the Cambri</context>
</contexts>
<marker>Liakata, Teufel, Siddharthan, Batchelor, 2010</marker>
<rawString>M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor. 2010. Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Karakos</author>
<author>D Demner-Fushman</author>
<author>S Khudanpur</author>
</authors>
<title>Generative content models for structural analysis of medical abstracts.</title>
<date>2006</date>
<booktitle>In Proceedings of BioNLP-06.</booktitle>
<contexts>
<context position="2075" citStr="Lin et al., 2006" startWordPosition="304" endWordPosition="307"> in question, the methods used in the study, the results obtained, or the conclusions drawn by authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing a</context>
<context position="11181" citStr="Lin et al., 2006" startWordPosition="1737" endWordPosition="1740"> each: Related work (REL) and Future work (FUT). Guo et al. (2010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Merity et al., 2009) we re-implemented and used them in our experiment1. However, being aware of the fact that some of these features may not be optimal for weakly-supervised learning (i.e. when learning from smaller data), we evaluate their performance and suitability for the task later in section 4.3. • Location. Zones tend to appear in typical positions in abstracts. Each abstract was there1The only exception is the history feature which was left out because it cannot be applied to all of our methods 275 fore divided into ten parts (1-10, measur</context>
<context position="28523" citStr="Lin et al., 2006" startWordPosition="4776" endWordPosition="4779">btaining labeled data methods not needing it are preferable. 6 Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from our task in that i</context>
<context position="32350" citStr="Lin et al., 2006" startWordPosition="5373" endWordPosition="5376">ation from a different but related (source) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories. Finally, an important avenue of future research is to evaluate the usefulness of weakly-supervised identification of information structure for NLP tasks such as summarization and information extraction (Tbahriti et al., 2006; Ruch et al., 2007), and for practical tasks such as manual review of scientific papers for research purposes (Guo et al., 2010). Acknowledgments The work reported in this pap</context>
</contexts>
<marker>Lin, Karakos, Demner-Fushman, Khudanpur, 2006</marker>
<rawString>J. Lin, D. Karakos, D. Demner-Fushman, and S. Khudanpur. 2006. Generative content models for structural analysis of medical abstracts. In Proceedings of BioNLP-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>A Mccallum</author>
</authors>
<title>Efficient computation of entropy gradient for semi-supervised conditional random fields.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="17796" citStr="Mann and Mccallum, 2007" startWordPosition="2892" endWordPosition="2896">y(u)) Subject to y(l)(w x(l) − b) &gt; 1, y(u)(w x(u) − b) &gt; 1 , y(u) E {−1,1}, where x(u) is unlabeled data and y(u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data EL log p(y(l) |x(l), 0) with an additional term EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize the conditional entropy of the model’s predictions on Unlabeled data (Jiao et al., 2006; Mann and Mccallum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: no. of correctly classified sentences aCC = total no. ofsentences in the corpus no. of sentences correctly identified as Classi p = total no. of sentences identified as Classi no. of sentences correctly identified as Classi r = total no. of sentences in Classi f_ 2∗p∗r - p+r We used 10-fold cross validation for all the methods to avoid the possible bias i</context>
</contexts>
<marker>Mann, Mccallum, 2007</marker>
<rawString>G. S. Mann and A. Mccallum. 2007. Efficient computation of entropy gradient for semi-supervised conditional random fields. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing em and pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="30910" citStr="McCallum and Nigam, 1998" startWordPosition="5149" endWordPosition="5152">ive learning when applied to our task. Secondly, we plan to investigate other semisupervised methods, for example, the ExpectationMaximization (EM) algorithm. (Lanquillon, 2000) has shown that EM SVM performs better than supervised and transductive SVM on a text classification task when applied to the dataset of 20 Newsgroups (20 classes, 4000 documents for testing, 10000 unlabeled ones), yielding up to ∼10% higher accuracy when 200-5000 labeled documents are used for training. In addition, other combinations of weaklysupervised methods might be worth looking into, such as EM+active learning (McCallum and Nigam, 1998) and co-training+EM+active learning (Muslea et al., 2002), which have proved promising in related text classification works. Besides looking for optimal ML strategies, we plan to look for optimal features for the task. Our feature analysis showed that not all the features which had proved promising in fully supervised experiments were equally promising when applied to weakly-supervised learning from smaller data. We 280 plan to look into ways of reducing the sparse data problem in features, e.g. by classifying not only verbs but also other word classes into semanticallymotivated categories. On</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. Employing em and pool-based active learning for text classification. In Proceedings of the Fifteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="15462" citStr="McCallum, 2002" startWordPosition="2496" endWordPosition="2498">oftware (Hall et al., 2009) (employing its linear kernel) for SVM experiments. CRF is an undirected graphical model which defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as: p(y|x, 0) = 1 Z(x)exp(�j 0jFj(y, x)), where Fj(y, x) is a real-valued feature function of the states and the observations; 0j is the weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF experiments. 3.2.2 Weakly-supervised methods Active SVM (ASVM) starts with a small amount of labeled data, and iteratively chooses a proportion of 276 unlabeled data for which SVM has less confidence to be labeled (the labels can be restored from the original corpus) and used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that the SVM is uncertain about. Unlike these </context>
<context position="17838" citStr="McCallum, 2002" startWordPosition="2901" endWordPosition="2902">− b) &gt; 1 , y(u) E {−1,1}, where x(u) is unlabeled data and y(u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data EL log p(y(l) |x(l), 0) with an additional term EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize the conditional entropy of the model’s predictions on Unlabeled data (Jiao et al., 2006; Mann and Mccallum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: no. of correctly classified sentences aCC = total no. ofsentences in the corpus no. of sentences correctly identified as Classi p = total no. of sentences identified as Classi no. of sentences correctly identified as Classi r = total no. of sentences in Classi f_ 2∗p∗r - p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular spl</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. K. McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="18727" citStr="McNemar, 1947" startWordPosition="3051" endWordPosition="3052">orpus no. of sentences correctly identified as Classi p = total no. of sentences identified as Classi no. of sentences correctly identified as Classi r = total no. of sentences in Classi f_ 2∗p∗r - p+r We used 10-fold cross validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data. More specifically, the data was randomly assigned to ten folds of roughly the same size. Each fold was used once as test data and the remaining nine folds as training data. The results were then averaged. Following (Dietterich, 1998), we used McNemar’s test (McNemar, 1947) to measure the statistical significance between the results of different ML methods. The chosen significance level was .05. 4.2 Results Table 3 shows the results for the four weaklysupervised and two supervised methods when 10% of the training data (i.e. ∼700 sentences) has been labeled. We can see that ASSVM is the best performing method with an accuracy of 81% and the macro Table 3: Results when using 10% of the labeled data Acc. F-score MF BKG OBJ METH RES CON REL FUT SVM .77 .74 .84 .68 .71 .82 .64 - - CRF .70 .65 .75 .46 .48 .78 .76 - - ASVM .80 .75 .88 .56 .68 .87 .78 .33 ASSVM .81 .76 </context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages. Psychometrika, 12(2):153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Merity</author>
<author>T Murphy</author>
<author>J R Curran</author>
</authors>
<title>Accurate argumentative zoning with maximum entropy models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries.</booktitle>
<contexts>
<context position="11247" citStr="Merity et al., 2009" startWordPosition="1749" endWordPosition="1752">010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Merity et al., 2009) we re-implemented and used them in our experiment1. However, being aware of the fact that some of these features may not be optimal for weakly-supervised learning (i.e. when learning from smaller data), we evaluate their performance and suitability for the task later in section 4.3. • Location. Zones tend to appear in typical positions in abstracts. Each abstract was there1The only exception is the history feature which was left out because it cannot be applied to all of our methods 275 fore divided into ten parts (1-10, measured by the number of words), and the location was defined by the pa</context>
</contexts>
<marker>Merity, Murphy, Curran, 2009</marker>
<rawString>S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate argumentative zoning with maximum entropy models. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mizuta</author>
<author>A Korhonen</author>
<author>T Mullen</author>
<author>N Collier</author>
</authors>
<title>Zone analysis in biology articles as a basis for information extraction.</title>
<date>2006</date>
<journal>International Journal of Medical Informatics on Natural Language Processing in Biomedicine and Its Applications,</journal>
<volume>75</volume>
<issue>6</issue>
<contexts>
<context position="2196" citStr="Mizuta et al., 2006" startWordPosition="324" endWordPosition="327">y Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottl</context>
<context position="4561" citStr="Mizuta et al., 2006" startWordPosition="671" endWordPosition="674"> than global) structure of documents and being much more fine-grained in nature, this related task differs from ours considerably. In this paper, we investigate the potential of weakly-supervised learning for Argumentative Zoning (AZ) of scientific abstracts. AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (Teufel and Moens, 2002). It has been used to analyze scientific texts in various disciplines – including computational linguistics (Teufel and Moens, 2002), law, (Hachey and Grover, 2006), biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009) – and has proved useful for NLP tasks such as summarization (Teufel and Moens, 2002). Although the basic scheme is said to be disciplineindependent (Teufel et al., 2009), its application to different domains has resulted in various modifications and laborious annotation exercises. This suggests that a weakly-supervised approach would be more practical than a fully supervised one for the real-world application of AZ. Taking two supervised classifiers as a comparison point – Support Vector Machines (SVM) and Conditional Random Fields (CRF) – we investigate th</context>
<context position="6252" citStr="Mizuta et al., 2006" startWordPosition="939" endWordPosition="942">hird of the labeled data, it performs equally well as a fully supervised SVM which uses 100% of the labeled data. Our investigation suggests that weaklysupervised learning could be employed to improve the practical applicability and portability of AZ to different information access tasks. 2 Data We used in our experiments the recent dataset of (Guo et al., 2010). Guo et al. (2010) provide a corpus of 1000 biomedical abstracts (consisting of 7985 sentences and 225785 words) annotated according to three schemes of information structure – those based on section names (Hirohata et al., 2008), AZ (Mizuta et al., 2006) and Core Scientific Concepts (CoreSC) (Liakata et al., 2010). We focus here on AZ only, because it subsumes all the categories of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ is a scheme which provides an analysis of the rhetorical progression of the scientific argument, following the knowledge claims made by authors. (Teufel and Moens, 2002) introduced AZ and applied it first to computational linguistics papers. (Hachey and G</context>
<context position="28505" citStr="Mizuta et al., 2006" startWordPosition="4772" endWordPosition="4775">en the high cost of obtaining labeled data methods not needing it are preferable. 6 Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from </context>
<context position="31948" citStr="Mizuta et al., 2006" startWordPosition="5312" endWordPosition="5315"> 280 plan to look into ways of reducing the sparse data problem in features, e.g. by classifying not only verbs but also other word classes into semanticallymotivated categories. One the key motivations for developing a weaklysupervised approach is to facilitate easy porting of schemes such as AZ to new tasks and domains. Recent research shows that active learning in a target domain can leverage information from a different but related (source) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories</context>
</contexts>
<marker>Mizuta, Korhonen, Mullen, Collier, 2006</marker>
<rawString>Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006. Zone analysis in biology articles as a basis for information extraction. International Journal of Medical Informatics on Natural Language Processing in Biomedicine and Its Applications, 75(6):468–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mullen</author>
<author>Y Mizuta</author>
<author>N Collier</author>
</authors>
<title>A baseline feature set for learning rhetorical zones using full articles in the biomedical domain. Natural language processing and text mining,</title>
<date>2005</date>
<pages>7--1</pages>
<contexts>
<context position="11202" citStr="Mullen et al., 2005" startWordPosition="1741" endWordPosition="1744">k (REL) and Future work (FUT). Guo et al. (2010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Merity et al., 2009) we re-implemented and used them in our experiment1. However, being aware of the fact that some of these features may not be optimal for weakly-supervised learning (i.e. when learning from smaller data), we evaluate their performance and suitability for the task later in section 4.3. • Location. Zones tend to appear in typical positions in abstracts. Each abstract was there1The only exception is the history feature which was left out because it cannot be applied to all of our methods 275 fore divided into ten parts (1-10, measured by the number of w</context>
<context position="13835" citStr="Mullen et al., 2005" startWordPosition="2190" endWordPosition="2193">ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and without self-training, transductive SVM and semi-supervised CRF. 3.2.1 Supervised methods SVM constructs hyperplanes in a multidimensional space to separate data points of different classes. Good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data </context>
</contexts>
<marker>Mullen, Mizuta, Collier, 2005</marker>
<rawString>T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline feature set for learning rhetorical zones using full articles in the biomedical domain. Natural language processing and text mining, 7(1):52–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Muslea</author>
<author>Steven Minton</author>
<author>Craig A Knoblock</author>
</authors>
<title>Active + semi-supervised learning = robust multi-view learning.</title>
<date>2002</date>
<booktitle>In Proceedings of the Nineteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="30967" citStr="Muslea et al., 2002" startWordPosition="5156" endWordPosition="5159">vestigate other semisupervised methods, for example, the ExpectationMaximization (EM) algorithm. (Lanquillon, 2000) has shown that EM SVM performs better than supervised and transductive SVM on a text classification task when applied to the dataset of 20 Newsgroups (20 classes, 4000 documents for testing, 10000 unlabeled ones), yielding up to ∼10% higher accuracy when 200-5000 labeled documents are used for training. In addition, other combinations of weaklysupervised methods might be worth looking into, such as EM+active learning (McCallum and Nigam, 1998) and co-training+EM+active learning (Muslea et al., 2002), which have proved promising in related text classification works. Besides looking for optimal ML strategies, we plan to look for optimal features for the task. Our feature analysis showed that not all the features which had proved promising in fully supervised experiments were equally promising when applied to weakly-supervised learning from smaller data. We 280 plan to look into ways of reducing the sparse data problem in features, e.g. by classifying not only verbs but also other word classes into semanticallymotivated categories. One the key motivations for developing a weaklysupervised a</context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2002</marker>
<rawString>Ion Muslea, Steven Minton, and Craig A. Knoblock. 2002. Active + semi-supervised learning = robust multi-view learning. In Proceedings of the Nineteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating Quasi-Newton Matrices with Limited Storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<volume>35</volume>
<issue>151</issue>
<contexts>
<context position="15420" citStr="Nocedal, 1980" startWordPosition="2490" endWordPosition="2491"> algorithm (Platt, 1999b). We used Weka software (Hall et al., 2009) (employing its linear kernel) for SVM experiments. CRF is an undirected graphical model which defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as: p(y|x, 0) = 1 Z(x)exp(�j 0jFj(y, x)), where Fj(y, x) is a real-valued feature function of the states and the observations; 0j is the weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF experiments. 3.2.2 Weakly-supervised methods Active SVM (ASVM) starts with a small amount of labeled data, and iteratively chooses a proportion of 276 unlabeled data for which SVM has less confidence to be labeled (the labels can be restored from the original corpus) and used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating Quasi-Newton Matrices with Limited Storage. Mathematics of Computation, 35(151):773–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bla Novak</author>
<author>Dunja Mladeni</author>
<author>Marko Grobelnik</author>
</authors>
<title>Text classification with active learning.</title>
<date>2006</date>
<booktitle>In From Data and Information Analysis to Knowledge Engineering,</booktitle>
<pages>398--405</pages>
<contexts>
<context position="15915" citStr="Novak et al., 2006" startWordPosition="2570" endWordPosition="2573">weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF experiments. 3.2.2 Weakly-supervised methods Active SVM (ASVM) starts with a small amount of labeled data, and iteratively chooses a proportion of 276 unlabeled data for which SVM has less confidence to be labeled (the labels can be restored from the original corpus) and used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that the SVM is uncertain about. Unlike these methods, our learning algorithm compares the posterior probabilities of the best estimate given each unlabeled instance, and queries those with the lowest probabilities for the next round of learning. The probabilities can be obtained by fitting a Sigmoid after the standard SVM (Platt, 1999a), and combined using a pairwise coupling algorithm (Hastie and Tibshirani, 1998) in the multi-class case. We used the SVM linear kernel in Weka for classificati</context>
<context position="26647" citStr="Novak et al., 2006" startWordPosition="4455" endWordPosition="4458">acy of 81% and F-score of .76, outperforming the best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised SVM (i.e. 100% of the labeled data) when using just one third of the labeled data. This result is in line with the results of many other text classification works where active learning (alone or in combination with other techniques such as self-training) has proved similarly useful, e.g. (Lewis and Gale, 1994; Tong and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien, 2005), and discovered that our da</context>
</contexts>
<marker>Novak, Mladeni, Grobelnik, 2006</marker>
<rawString>Bla Novak, Dunja Mladeni, and Marko Grobelnik. 2006. Text classification with active learning. In From Data and Information Analysis to Knowledge Engineering, pages 398–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classiers,</booktitle>
<pages>61--74</pages>
<contexts>
<context position="14829" citStr="Platt, 1999" startWordPosition="2389" endWordPosition="2390">plane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance between two parallel hyperplanes each of which separates the data. The parallel hyperplanes can be written as: w · x − b = 1 and w · x − b = −1, and the distance between them is 2 w . The problem reduces to: Minimize |w |(in w, b) Subject to w · x − b &gt; 1 for x of one class, w · x − b &lt; −1 for x of the other, which can be solved by using the SMO algorithm (Platt, 1999b). We used Weka software (Hall et al., 2009) (employing its linear kernel) for SVM experiments. CRF is an undirected graphical model which defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as: p(y|x, 0) = 1 Z(x)exp(�j 0jFj(y, x)), where Fj(y, x) is a real-valued feature function of the states and the observations; 0j is the weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used</context>
<context position="16353" citStr="Platt, 1999" startWordPosition="2643" endWordPosition="2644"> used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that the SVM is uncertain about. Unlike these methods, our learning algorithm compares the posterior probabilities of the best estimate given each unlabeled instance, and queries those with the lowest probabilities for the next round of learning. The probabilities can be obtained by fitting a Sigmoid after the standard SVM (Platt, 1999a), and combined using a pairwise coupling algorithm (Hastie and Tibshirani, 1998) in the multi-class case. We used the SVM linear kernel in Weka for classification, and the -M flag in Weka for calculating the posterior probabilities. Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has two steps: (i) training on the labeled, and testing on the unlabeled data, and querying; (ii) training on both labeled and unlabeled/machine-labeled data by using the estimates from step (i). The idea of ASSVM is to make the best use of the labeled data, and to make the</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>J. C. Platt. 1999a. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classiers, pages 61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Using analytic qp and sparseness to speed training of support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1998 conference on Advances in neural information processing systems II.</booktitle>
<contexts>
<context position="14829" citStr="Platt, 1999" startWordPosition="2389" endWordPosition="2390">plane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance between two parallel hyperplanes each of which separates the data. The parallel hyperplanes can be written as: w · x − b = 1 and w · x − b = −1, and the distance between them is 2 w . The problem reduces to: Minimize |w |(in w, b) Subject to w · x − b &gt; 1 for x of one class, w · x − b &lt; −1 for x of the other, which can be solved by using the SMO algorithm (Platt, 1999b). We used Weka software (Hall et al., 2009) (employing its linear kernel) for SVM experiments. CRF is an undirected graphical model which defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as: p(y|x, 0) = 1 Z(x)exp(�j 0jFj(y, x)), where Fj(y, x) is a real-valued feature function of the states and the observations; 0j is the weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used</context>
<context position="16353" citStr="Platt, 1999" startWordPosition="2643" endWordPosition="2644"> used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that the SVM is uncertain about. Unlike these methods, our learning algorithm compares the posterior probabilities of the best estimate given each unlabeled instance, and queries those with the lowest probabilities for the next round of learning. The probabilities can be obtained by fitting a Sigmoid after the standard SVM (Platt, 1999a), and combined using a pairwise coupling algorithm (Hastie and Tibshirani, 1998) in the multi-class case. We used the SVM linear kernel in Weka for classification, and the -M flag in Weka for calculating the posterior probabilities. Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has two steps: (i) training on the labeled, and testing on the unlabeled data, and querying; (ii) training on both labeled and unlabeled/machine-labeled data by using the estimates from step (i). The idea of ASSVM is to make the best use of the labeled data, and to make the</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999b. Using analytic qp and sparseness to speed training of support vector machines. In Proceedings of the 1998 conference on Advances in neural information processing systems II.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piyush Rai</author>
<author>Avishek Saha</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Domain adaptation meets active learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing.</booktitle>
<marker>Rai, Saha, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Piyush Rai, Avishek Saha, Hal Daum´e, III, and Suresh Venkatasubramanian. 2010. Domain adaptation meets active learning. In Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Ruch</author>
<author>C Boyer</author>
<author>C Chichester</author>
<author>I Tbahriti</author>
<author>A Geissbuhler</author>
<author>P Fabry</author>
<author>J Gobeill</author>
<author>V Pillet</author>
<author>D Rebholz-</author>
</authors>
<marker>Ruch, Boyer, Chichester, Tbahriti, Geissbuhler, Fabry, Gobeill, Pillet, Rebholz-, </marker>
<rawString>P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geissbuhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lovis Schuhmann</author>
<author>A L Veuthey</author>
</authors>
<title>Using argumentation to extract key sentences from biomedical abstracts.</title>
<date>2007</date>
<journal>Int J Med Inform,</journal>
<pages>76--2</pages>
<marker>Schuhmann, Veuthey, 2007</marker>
<rawString>Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using argumentation to extract key sentences from biomedical abstracts. Int J Med Inform, 76(2-3):195–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Scheffer</author>
<author>Christian Decomain</author>
<author>Stefan Wrobel</author>
</authors>
<title>Active hidden markov models for information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis.</booktitle>
<contexts>
<context position="29750" citStr="Scheffer et al., 2001" startWordPosition="4966" endWordPosition="4969">uses on local discourse relations while our task focuses on the global structure of the scientific document. In the future, we plan to improve and extend this work in several directions. First, the approach to active learning could be improved in various ways. The query strategy we employed (uncertainty sampling) is a relatively straightforward method which only considers the best estimate for each unlabeled instance, disregarding other estimates that may contain useful information. In the future, we plan to experiment with more sophisticated strategies, e.g. the margin sampling algorithm by (Scheffer et al., 2001) and the query-by-committee (QBC) algorithm by (Seung et al., 1992). In addition, there are algorithms designed for reducing the redundancy in queries which may be worth investigating (Hoi et al., 2006). Also, (Hoi et al., 2006) shows that Logistic Regression (LR) outperforms SVM when used with active learning, yielding higher F-score on the Reuters21578 data set (binary classification, 10,788 documents in total, 100 of them labeled). It would be interesting to explore whether supervised methods other than SVM are optimal for active learning when applied to our task. Secondly, we plan to inves</context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001. Active hidden markov models for information extraction. In Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of the fifth annual workshop on Computational learning theory.</booktitle>
<contexts>
<context position="29817" citStr="Seung et al., 1992" startWordPosition="4976" endWordPosition="4979"> structure of the scientific document. In the future, we plan to improve and extend this work in several directions. First, the approach to active learning could be improved in various ways. The query strategy we employed (uncertainty sampling) is a relatively straightforward method which only considers the best estimate for each unlabeled instance, disregarding other estimates that may contain useful information. In the future, we plan to experiment with more sophisticated strategies, e.g. the margin sampling algorithm by (Scheffer et al., 2001) and the query-by-committee (QBC) algorithm by (Seung et al., 1992). In addition, there are algorithms designed for reducing the redundancy in queries which may be worth investigating (Hoi et al., 2006). Also, (Hoi et al., 2006) shows that Logistic Regression (LR) outperforms SVM when used with active learning, yielding higher F-score on the Reuters21578 data set (binary classification, 10,788 documents in total, 100 of them labeled). It would be interesting to explore whether supervised methods other than SVM are optimal for active learning when applied to our task. Secondly, we plan to investigate other semisupervised methods, for example, the ExpectationMa</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proceedings of the fifth annual workshop on Computational learning theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shatkay</author>
<author>F Pan</author>
<author>A Rzhetsky</author>
<author>W J Wilbur</author>
</authors>
<title>Multi-dimensional classification of biomedical text: Toward automated, practical provision of high-utility text to diverse users.</title>
<date>2008</date>
<journal>Bioinformatics,</journal>
<volume>24</volume>
<issue>18</issue>
<contexts>
<context position="2265" citStr="Shatkay et al., 2008" startWordPosition="334" endWordPosition="337">f specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML. Relying</context>
<context position="28568" citStr="Shatkay et al., 2008" startWordPosition="4784" endWordPosition="4787"> it are preferable. 6 Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differs substantially from our task in that it focuses on local discourse relations while </context>
<context position="32395" citStr="Shatkay et al., 2008" startWordPosition="5381" endWordPosition="5384">e) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories. Finally, an important avenue of future research is to evaluate the usefulness of weakly-supervised identification of information structure for NLP tasks such as summarization and information extraction (Tbahriti et al., 2006; Ruch et al., 2007), and for practical tasks such as manual review of scientific papers for research purposes (Guo et al., 2010). Acknowledgments The work reported in this paper was funded by the Royal Society (UK). YG w</context>
</contexts>
<marker>Shatkay, Pan, Rzhetsky, Wilbur, 2008</marker>
<rawString>H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008. Multi-dimensional classification of biomedical text: Toward automated, practical provision of high-utility text to diverse users. Bioinformatics, 24(18):2086– 2093.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sinz</author>
</authors>
<title>UniverSVM Support Vector Machine with Large Scale CCCP Functionality.</title>
<date>2011</date>
<note>http://www.kyb.mpg.de/bs/people/fabee/universvm.html.</note>
<contexts>
<context position="17433" citStr="Sinz, 2011" startWordPosition="2835" endWordPosition="2836">e-labeled data by using the estimates from step (i). The idea of ASSVM is to make the best use of the labeled data, and to make the most use of the unlabeled data. Transductive SVM (TSVM) is an extension of SVM which takes advantage of both labeled and unlabeled data (Vapnik, 1998). Similar to SVM, the problem is defined as: Minimize |w |(in w, b, y(u)) Subject to y(l)(w x(l) − b) &gt; 1, y(u)(w x(u) − b) &gt; 1 , y(u) E {−1,1}, where x(u) is unlabeled data and y(u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data EL log p(y(l) |x(l), 0) with an additional term EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize the conditional entropy of the model’s predictions on Unlabeled data (Jiao et al., 2006; Mann and Mccallum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the co</context>
</contexts>
<marker>Sinz, 2011</marker>
<rawString>F. Sinz, 2011. UniverSVM Support Vector Machine with Large Scale CCCP Functionality. http://www.kyb.mpg.de/bs/people/fabee/universvm.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preference.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13393" citStr="Sun and Korhonen, 2009" startWordPosition="2117" endWordPosition="2120">adjacent words e.g. in complex biomedical terms such as 2-amino-3,8-diethylimidazo[4,5- f]quinoxaline. The C&amp;C tools (Curran et al., 2007) trained on biomedical literature were employed for POS tagging, lemmatization and parsing. The lemma output was used for creating Word, Bi-gram and Verb features. The GR output was used for creating the GR, Subj, Obj and Voice features. The ”obj” marker in a subject relation indicates passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and withou</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>L. Sun and A. Korhonen. 2009. Improving verb clustering with automatically acquired selectional preference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tbahriti</author>
<author>C Chichester</author>
<author>Frederique Lisacek</author>
<author>P Ruch</author>
</authors>
<title>Using argumentation to retrieve articles with similar citations.</title>
<date>2006</date>
<journal>Int J Med Inform,</journal>
<volume>75</volume>
<issue>6</issue>
<contexts>
<context position="2548" citStr="Tbahriti et al., 2006" startWordPosition="375" endWordPosition="378">ng on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML. Relying on a small amount of labeled data and a large pool of unlabeled data, weakly-supervised techniques (e.g. semi-supervision, active learning, co/tri-training, self-training) aim to keep the advantages of fully supervised approaches. They have been applied to a wide range of NLP tasks</context>
</contexts>
<marker>Tbahriti, Chichester, Lisacek, Ruch, 2006</marker>
<rawString>I. Tbahriti, C. Chichester, Frederique Lisacek, and P. Ruch. 2006. Using argumentation to retrieve articles with similar citations. Int J Med Inform, 75(6):488–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--409</pages>
<contexts>
<context position="2175" citStr="Teufel and Moens, 2002" startWordPosition="320" endWordPosition="323"> authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential s</context>
<context position="4366" citStr="Teufel and Moens, 2002" startWordPosition="641" endWordPosition="644">t experiments have demonstrated the usefulness of weakly-supervised learning for classifying discourse relations in scientific texts, e.g. (Hernault et al., 2011). However, focusing on local (rather than global) structure of documents and being much more fine-grained in nature, this related task differs from ours considerably. In this paper, we investigate the potential of weakly-supervised learning for Argumentative Zoning (AZ) of scientific abstracts. AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (Teufel and Moens, 2002). It has been used to analyze scientific texts in various disciplines – including computational linguistics (Teufel and Moens, 2002), law, (Hachey and Grover, 2006), biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009) – and has proved useful for NLP tasks such as summarization (Teufel and Moens, 2002). Although the basic scheme is said to be disciplineindependent (Teufel et al., 2009), its application to different domains has resulted in various modifications and laborious annotation exercises. This suggests that a weakly-supervised approach would be more practical than a fully s</context>
<context position="6766" citStr="Teufel and Moens, 2002" startWordPosition="1024" endWordPosition="1027">mes of information structure – those based on section names (Hirohata et al., 2008), AZ (Mizuta et al., 2006) and Core Scientific Concepts (CoreSC) (Liakata et al., 2010). We focus here on AZ only, because it subsumes all the categories of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ is a scheme which provides an analysis of the rhetorical progression of the scientific argument, following the knowledge claims made by authors. (Teufel and Moens, 2002) introduced AZ and applied it first to computational linguistics papers. (Hachey and Grover, 2006) applied the scheme later to legal texts and (Mizuta et al., 2006) modified it for biology papers. More recently, (Teufel et al., 2009) introduced a refined version of AZ and applied it to chemistry papers. The biomedical dataset of (Guo et al., 2010) has been annotated according to the version of AZ developed for biology papers (Mizuta et al., 2006) (with only minor modifications concerning zone names). Seven categories of this scheme (out of the 10 possible) actually appear in abstracts and in t</context>
<context position="11163" citStr="Teufel and Moens, 2002" startWordPosition="1733" endWordPosition="1736">overing 1% of the corpus each: Related work (REL) and Future work (FUT). Guo et al. (2010) report the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. According to Cohen’s kappa (Cohen, 1960) the agreement is relatively high: κ = 0.85. 3 Automatic identification of AZ 3.1 Features and feature extraction Guo et al. (2010) used a variety of features in their fully supervised ML experiments on different schemes of information structure. Since their feature types cover the best performing feature types in earlier works e.g. (Teufel and Moens, 2002; Lin et al., 2006; Mullen et al., 2005; Hirohata et al., 2008; Merity et al., 2009) we re-implemented and used them in our experiment1. However, being aware of the fact that some of these features may not be optimal for weakly-supervised learning (i.e. when learning from smaller data), we evaluate their performance and suitability for the task later in section 4.3. • Location. Zones tend to appear in typical positions in abstracts. Each abstract was there1The only exception is the history feature which was left out because it cannot be applied to all of our methods 275 fore divided into ten p</context>
<context position="13814" citStr="Teufel and Moens, 2002" startWordPosition="2186" endWordPosition="2189">es passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and without self-training, transductive SVM and semi-supervised CRF. 3.2.1 Supervised methods SVM constructs hyperplanes in a multidimensional space to separate data points of different classes. Good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hy</context>
<context position="28484" citStr="Teufel and Moens, 2002" startWordPosition="4768" endWordPosition="4771">n our task. However, given the high cost of obtaining labeled data methods not needing it are preferable. 6 Conclusions and future work Our experiments show that weakly-supervised learning can be used to identify AZ in scientific documents with good accuracy when only a limited amount of labeled data is available. This is helpful thinking of the real-world application and porting of the approach to different tasks and domains. To the best of our knowledge, no previous work has been done on weakly-supervised learning of information structure according to schemes of the type we have focused on (Teufel and Moens, 2002; Mizuta et al., 2006; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010). Recently, some work has been done on the related task of classification of discourse relations in scientific texts: (Hernault et al., 2011) used structural learning (Ando and Zhang, 2005) for this task. They obtained 30-60% accuracy on the RST Discourse Treebank (including 41 relation types) when using 100-10000 labeled and 100000 unlabeled instances. The accuracy was 20-60% when using the labeled data only. However, although related, the task of discourse relation classification differ</context>
<context position="31927" citStr="Teufel and Moens, 2002" startWordPosition="5308" endWordPosition="5311">ng from smaller data. We 280 plan to look into ways of reducing the sparse data problem in features, e.g. by classifying not only verbs but also other word classes into semanticallymotivated categories. One the key motivations for developing a weaklysupervised approach is to facilitate easy porting of schemes such as AZ to new tasks and domains. Recent research shows that active learning in a target domain can leverage information from a different but related (source) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>S. Teufel and M. Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28:409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>C Batchelor</author>
</authors>
<title>Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2218" citStr="Teufel et al., 2009" startWordPosition="328" endWordPosition="331">ocessing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop te</context>
<context position="4597" citStr="Teufel et al., 2009" startWordPosition="677" endWordPosition="680"> and being much more fine-grained in nature, this related task differs from ours considerably. In this paper, we investigate the potential of weakly-supervised learning for Argumentative Zoning (AZ) of scientific abstracts. AZ is an approach to information structure which provides an analysis of the rhetorical progression of the scientific argument in a document (Teufel and Moens, 2002). It has been used to analyze scientific texts in various disciplines – including computational linguistics (Teufel and Moens, 2002), law, (Hachey and Grover, 2006), biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009) – and has proved useful for NLP tasks such as summarization (Teufel and Moens, 2002). Although the basic scheme is said to be disciplineindependent (Teufel et al., 2009), its application to different domains has resulted in various modifications and laborious annotation exercises. This suggests that a weakly-supervised approach would be more practical than a fully supervised one for the real-world application of AZ. Taking two supervised classifiers as a comparison point – Support Vector Machines (SVM) and Conditional Random Fields (CRF) – we investigate the performance of four weakly-supervi</context>
<context position="6999" citStr="Teufel et al., 2009" startWordPosition="1062" endWordPosition="1065"> of the simple section name -based scheme, and according to the inter-annotator agreement and ML experiments reported by Guo et al. (2010) it performs better on this data than the fairly fine-grained CoreSC scheme. AZ is a scheme which provides an analysis of the rhetorical progression of the scientific argument, following the knowledge claims made by authors. (Teufel and Moens, 2002) introduced AZ and applied it first to computational linguistics papers. (Hachey and Grover, 2006) applied the scheme later to legal texts and (Mizuta et al., 2006) modified it for biology papers. More recently, (Teufel et al., 2009) introduced a refined version of AZ and applied it to chemistry papers. The biomedical dataset of (Guo et al., 2010) has been annotated according to the version of AZ developed for biology papers (Mizuta et al., 2006) (with only minor modifications concerning zone names). Seven categories of this scheme (out of the 10 possible) actually appear in abstracts and in the resulting corpus. These are shown and explained in Table 1. For example, the Method zone (METH) is for sentences which describe a way of doing research, esp. according to a defined and regular plan; a special form of procedure or </context>
<context position="31995" citStr="Teufel et al., 2009" startWordPosition="5320" endWordPosition="5323">arse data problem in features, e.g. by classifying not only verbs but also other word classes into semanticallymotivated categories. One the key motivations for developing a weaklysupervised approach is to facilitate easy porting of schemes such as AZ to new tasks and domains. Recent research shows that active learning in a target domain can leverage information from a different but related (source) domain (Rai et al., 2010). Making use of existing annotated datasets in biology, chemistry, computational linguistics and law (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) we will explore optimal ways of combining weakly-supervised learning with domain-adaptation. The work presented in this paper has focused on the abstracts annotated according to the AZ scheme. In the future, we plan to investigate the usefulness of weakly-supervised learning for identifying other schemes of information structure, e.g. (Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Liakata et al., 2010), and not only in scientific abstracts but also in full journal papers which typically exemplify a larger set of scheme categories. Finally, an important avenue of future resear</context>
</contexts>
<marker>Teufel, Siddharthan, Batchelor, 2009</marker>
<rawString>S. Teufel, A. Siddharthan, and C. Batchelor. 2009. Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tong</author>
<author>D Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--45</pages>
<contexts>
<context position="15894" citStr="Tong and Koller, 2001" startWordPosition="2566" endWordPosition="2569">bservations; 0j is the weight of Fj, and Z(x) is a normalization factor. The 0 parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF experiments. 3.2.2 Weakly-supervised methods Active SVM (ASVM) starts with a small amount of labeled data, and iteratively chooses a proportion of 276 unlabeled data for which SVM has less confidence to be labeled (the labels can be restored from the original corpus) and used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller, 2001; Novak et al., 2006). For example, it is often assumed that the data points close to the separating hyperplane are those that the SVM is uncertain about. Unlike these methods, our learning algorithm compares the posterior probabilities of the best estimate given each unlabeled instance, and queries those with the lowest probabilities for the next round of learning. The probabilities can be obtained by fitting a Sigmoid after the standard SVM (Platt, 1999a), and combined using a pairwise coupling algorithm (Hastie and Tibshirani, 1998) in the multi-class case. We used the SVM linear kernel in </context>
</contexts>
<marker>Tong, Koller, 2001</marker>
<rawString>S. Tong and D. Koller. 2001. Support vector machine active learning with applications to text classification. Journal of Machine Learning Research, 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--45</pages>
<contexts>
<context position="26612" citStr="Tong and Koller, 2002" startWordPosition="4449" endWordPosition="4452"> labeled data, ASSVM obtained an accuracy of 81% and F-score of .76, outperforming the best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised SVM (i.e. 100% of the labeled data) when using just one third of the labeled data. This result is in line with the results of many other text classification works where active learning (alone or in combination with other techniques such as self-training) has proved similarly useful, e.g. (Lewis and Gale, 1994; Tong and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien</context>
</contexts>
<marker>Tong, Koller, 2002</marker>
<rawString>Simon Tong and Daphne Koller. 2002. Support vector machine active learning with applications to text classification. J. Mach. Learn. Res., 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="17104" citStr="Vapnik, 1998" startWordPosition="2771" endWordPosition="2772">in Weka for classification, and the -M flag in Weka for calculating the posterior probabilities. Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has two steps: (i) training on the labeled, and testing on the unlabeled data, and querying; (ii) training on both labeled and unlabeled/machine-labeled data by using the estimates from step (i). The idea of ASSVM is to make the best use of the labeled data, and to make the most use of the unlabeled data. Transductive SVM (TSVM) is an extension of SVM which takes advantage of both labeled and unlabeled data (Vapnik, 1998). Similar to SVM, the problem is defined as: Minimize |w |(in w, b, y(u)) Subject to y(l)(w x(l) − b) &gt; 1, y(u)(w x(u) − b) &gt; 1 , y(u) E {−1,1}, where x(u) is unlabeled data and y(u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data EL log p(y(l) |x(l), 0) with an additional term EU EY p(y|x(u), 0) log p(y|x(u), 0) to minimize the conditional entro</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. N. Vapnik. 1998. Statistical learning theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Jian-Tao Sun</author>
<author>Tengjiao Wang</author>
<author>Zheng Chen</author>
</authors>
<title>Effective multi-label active learning for text classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="26695" citStr="Yang et al., 2009" startWordPosition="4464" endWordPosition="4467"> best supervised method SVM with a statistically significant difference. It reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised SVM (i.e. 100% of the labeled data) when using just one third of the labeled data. This result is in line with the results of many other text classification works where active learning (alone or in combination with other techniques such as self-training) has proved similarly useful, e.g. (Lewis and Gale, 1994; Tong and Koller, 2002; Brinker, 2006; Novak et al., 2006; Esuli and Sebastiani, 2009; Yang et al., 2009). While active learning iteratively explores the unknown aspects of the unlabeled data, semisupervised learning attempts to make the best use 279 of what it already knows about the data. In our experiments, semi-supervised methods (TSVM and SSCRF) did not perform equally well as active learning – TSVM even produced a lower accuracy than SVM with the same amount of labeled data – although these methods have gained success in related works. We therefore looked into related works using TSVM, e.g. (Chapelle and Zien, 2005), and discovered that our dataset is much higher in dimensionality than thos</context>
</contexts>
<marker>Yang, Sun, Wang, Chen, 2009</marker>
<rawString>Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng Chen. 2009. Effective multi-label active learning for text classification. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>