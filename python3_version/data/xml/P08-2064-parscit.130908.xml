<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028981">
<title confidence="0.979707">
Decompounding query keywords from compounding languages
</title>
<author confidence="0.928167">
Enrique Alfonseca Slaven Bilac Stefan Pharies
</author>
<affiliation confidence="0.906161">
Google Inc. Google Inc. Google Inc.
</affiliation>
<email confidence="0.996541">
ealfonseca@google.com slaven@google.com stefanp@google.com
</email>
<sectionHeader confidence="0.99561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999181">
Splitting compound words has proved to be
useful in areas such as Machine Translation,
Speech Recognition or Information Retrieval
(IR). Furthermore, real-time IR systems (such
as search engines) need to cope with noisy
data, as user queries are sometimes written
quickly and submitted without review. In this
paper we apply a state-of-the-art procedure for
German decompounding to other compound-
ing languages, and we show that it is possible
to have a single decompounding model that is
applicable across languages.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998682024390244">
Compounding languages (Krott, 1999), such as Ger-
man, Dutch, Danish, Norwegian, Swedish, Greek
or Finnish, allow the generation of complex words
by merging together simpler ones. So, for instance,
the flower bouquet can be expressed in German as
Blumenstr¨auf3e, made up of Blumen (flower) and
str¨auf3e (bouquet), and in Finnish as kukkakimppu,
from kukka (flower) and kimppu (bunch, collection).
For many language processing tools that rely on lex-
icons or language models it is very useful to be able
to decompose compounds to increase their cover-
age and reduce out-of-vocabulary terms. Decom-
pounders have been used successfully in Informa-
tion Retrieval (Braschler and Ripplinger, 2004), Ma-
chine Translation (Brown, 2002; Koehn and Knight,
2003) and Speech Recognition (Adda-Decker et al.,
2000). The Cross Language Evaluation Forum
(CLEF) competitions have shown that very simple
approaches can produce big gains in Cross Lan-
guage Information Retrieval (CLIR) for German and
Dutch (Monz and de Rijke, 2001) and for Finnish
(Adafre et al., 2004).
When working with web data, which has not nec-
essarily been reviewed for correctness, many of the
words are more difficult to analyze than when work-
ing with standard texts. There are more words with
spelling mistakes, and many texts mix words from
different languages. This problem exists to a larger
degree when handling user queries: they are writ-
ten quickly, not paying attention to mistakes. How-
ever, being able to identify that achzigerjahre should
be decompounded as achzig+jahre (where achzig is
a misspelled variation of achtzig) is still useful in
obtaining some meaning from the user query and
in helping the spelling correction system. This pa-
per evaluates a state-of-the-art procedure for Ger-
man splitting (Alfonseca et al., 2008), robust enough
to handle query data, on different languages, and
shows that it is possible to have a single decom-
pounding model that can be applied to all the lan-
guages under study.
</bodyText>
<sectionHeader confidence="0.841476" genericHeader="method">
2 Problem definition and evaluation
settings
</sectionHeader>
<bodyText confidence="0.99909375">
Any set of query keywords contains a large amount
of noisy data, such as words in foreign languages
or misspelled words. In order to be robust enough
to handle this kind of corpus, we require the fol-
lowing for a decompounder: first, obviously, com-
pounds should be split, and non-compounds should
be left untouched. This also applies if they are mis-
spelled. Unknown words or words involving a part
</bodyText>
<page confidence="0.987286">
253
</page>
<reference confidence="0.2219045">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 253–256,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999987911111111">
in a foreign language are split if there is a plausi-
ble interpretation of them being a compound word.
An example is Turingmaschine (Turing machine) in
German, where Turing is an English word. Finally,
words that are not really grammatical compounds,
but due to the user forgetting to input the blankspace
between the words (like desktopcomputer) are split.
For the evaluation, we have built and manually
annotated gold standard sets for German, Dutch,
Danish, Norwegian, Swedish and Finnish from fully
anonymized search query logs. Because people
do not use capitalization consistently when writing
queries, all the query logs are lowercased. By ran-
domly sampling keywords we would get few com-
pounds (as their frequency is small compared to that
of non-compounds), so we have proceeded in the
following way to ensure that the gold-standards con-
tain a substantial amount of compounds: we started
by building a very naive decompounder that splits a
word in several parts using a frequency-based com-
pound splitting method (Koehn and Knight, 2003).
Using this procedure, we obtain two random sam-
ples with possibly repeated words: one with words
that are considered non-compounds, and the other
with words that are considered compounds by this
naive approach. Next, we removed all the dupli-
cates from the previous list, and we had them an-
notated manually as compounds or non-compounds,
including the correct splittings. The sizes of the final
training sets vary between 2,000 and 3,600 words
depending on the language. Each compound was
annotated by two human judges who had received
the previous instructions on when to consider that
a keyword is a compound. For all the languages
considered, exactly one of the two judges was a na-
tive speaker living in a country where it is the of-
ficial language1. Table 1 shows the percentage of
agreement in classifying words as compounds or
non-compounds (Compound Classification Agree-
ment, CCA) for each language and the Kappa score
(Carletta, 1996) obtained from it, and the percent-
age of words for which also the decomposition pro-
vided was identical (Decompounding Agreement,
DA). The most common source of disagreement
were long words that could be split into two or more
</bodyText>
<footnote confidence="0.926638">
1This requisite is important because many queries contain
novel or fashionable words.
</footnote>
<table confidence="0.997126857142857">
Language CCA Kappa DA
German 93% 0.86 88%
Dutch 96% 0.92 96%
Danish 89% 0.78 89%
Norwegian 93% 0.86 81%
Swedish 96% 0.92 95%
Finnish 92% 0.84 89%
</table>
<tableCaption confidence="0.999753">
Table 1: Inter-judge agreement metrics.
</tableCaption>
<figure confidence="0.838450428571429">
Language Morphemes
German 0,-e,+s,+e,+en,+nen,+ens,+es,+ns,+er
Dutch 0,-e,+s,+e,+en
Danish 0,+e,+s
Norwegian 0,+e,+s
Swedish 0,+o,+u,+e,+s
Finnish 0
</figure>
<tableCaption confidence="0.980684">
Table 2: Linking morphemes used in this work.
</tableCaption>
<bodyText confidence="0.97139625">
parts.
The evaluation is done using the metrics preci-
sion, recall and accuracy, defined in the following
way (Koehn and Knight, 2003):
</bodyText>
<listItem confidence="0.999804333333333">
• Correct splits: no. of compounds that are split correctly.
• Correct non-splits: no. non-compounds that are not split.
• Wrong non-splits: no. of compounds and are not split.
• Wrong faulty splits: no. of compounds that are incor-
rectly split.
• Wrong splits: no. of non-compounds that are split.
</listItem>
<figure confidence="0.967503444444444">
Precision =
correct splits
correct splits + wrong faulty splits + wrong splits
correct splits
Recall =
correct splits + wrong faulty splits + wrong non-splits
correct splits
Accuracy =
correct splits + wrong splits
</figure>
<sectionHeader confidence="0.567757" genericHeader="method">
3 Combining corpus-based features
</sectionHeader>
<bodyText confidence="0.999601833333333">
Most approaches for decompounding can be consid-
ered as having this general structure: given a word
w, calculate every possible way of splitting w in
one or more parts, and score those parts according
to some weighting function. If the highest scoring
splitting contains just one part, it means that w is
not a compound.
For the first step (calculating every possible split-
ting), it is common to take into account that modi-
fiers inside compound words sometimes need link-
ing morphemes. Table 2 lists the ones used in our
system (Langer, 1998; Marek, 2006; Krott, 1999).
</bodyText>
<page confidence="0.987665">
254
</page>
<table confidence="0.999909333333333">
Method Precision Recall Accuracy
Never split - 0.00% 64.09%
Geometric mean of frequencies 39.77% 54.06% 65.58%
Compound probability 60.41% 80.68% 76.23%
Mutual Information 82.00% 48.29% 80.52%
Support-Vector Machine 83.56% 79.48% 87.21%
</table>
<tableCaption confidence="0.999921">
Table 3: Results of the several configurations.
</tableCaption>
<bodyText confidence="0.999177487804878">
Concerning the second step, there is some work
that uses, for scoring, additional information such
as rules for cognate recognition (Brown, 2002) or
sentence-aligned parallel corpora and a translation
model, as in the full system described by Koehn
and Knight (2003). When those resources are not
available, the most common methods used for com-
pound splitting are using features such as the geo-
metric mean of the frequencies of compound parts in
a corpus, as in Koehn and Knight (2003)’s back-off
method, or learning a language model from a cor-
pus and estimating the probability of each sequence
of possible compound parts (Schiller, 2005; Marek,
2006). While these methods are useful for sev-
eral applications, such as CLIR and MT, they have
known weaknesses, such as preferring a decompo-
sition if a compound part happens to be very fre-
quent by chance, in the case of the frequency-based
method, or the preference of decompositions with
the least possible number of parts, in the case of the
probability-based method.
Alfonseca et al. (2008) describe an integration of
the previous methods, together with the Mutual In-
formation and additional features obtained from web
anchor texts to train a supervised German decom-
pounder that outperforms the previous methods used
as standalone. The geometric mean of the frequen-
cies of compound parts and the probability estimated
from the language model usually attain a high recall,
given they are based on unigram features which are
easy to collect, but they have some weaknesses, as
mentioned above. On the other hand, while Mutual
Information is a much more precise metric, it is less
likely to have evidence about every single possible
pair of compound parts from a corpus, so it suffers
from low recall. A combination of all these metrics
into a learning model is able to attain a high recall.
An ablation study, reported in that paper, indicated
that the contribution of the web anchor texts is mini-
mal, so in this study we have just kept the other three
metrics. Table 3 shows the results reported for Ger-
</bodyText>
<table confidence="0.999423428571428">
Language P R A
German 83.56% 79.48% 87.21%
Dutch 78.99% 76.18% 83.45%
Danish 81.97% 87.12% 85.36%
Norwegian 88.13% 93.05% 90.40%
Swedish 83.34% 92.98% 87.79%
Finnish 90.79% 91.21% 91.62%
</table>
<tableCaption confidence="0.999969">
Table 4: Results in all the different languages.
</tableCaption>
<bodyText confidence="0.999685714285714">
man, training (i.e. counting frequencies and learn-
ing the language model) on the query keywords, and
running a 10-fold cross validation of a SVM with a
polynomial kernel using the German gold-standard.
The supervised system improves over the single un-
supervised metrics, attaining simultaneously good
recall and precision metrics.
</bodyText>
<sectionHeader confidence="0.986591" genericHeader="evaluation">
4 Experiments and evaluation
</sectionHeader>
<bodyText confidence="0.999967206896552">
The first motivation of this work is to test whether
the results reported for German are easy to repro-
duce in other languages. The results, shown in
Table 4, are very similar across languages, having
precision and recall values over 80% for most lan-
guages. A notable exception is Dutch, for which
the inter-judge agreement was the highest, so we ex-
pected the set of words to be easier to classify. An
analysis of the errors reported in the 10-fold cross-
validation indicates that most errors in Dutch were
wrong non-splits (in 147 cases) and wrong splits (in
139 cases), with wrong faulty splits happening only
in 20 occasions. Many of the wrong splits are loca-
tion names and trademarks, like youtube, piratebay
or smallville.
While the supervised model gives much better
results than the unsupervised ones, it still requires
the construction of a goldstandard from which to
train, which is usually costly. Therefore, we ran
another experiment to check whether the models
trained from some languages are applicable to other
languages. Table 5 shows the results obtained in this
case, the last column indicating the results when the
model is trained from the training instances from
all the other languages together. For each row, the
highest value and those which are inside its 95%
confidence interval are highlighted. Interestingly,
apart from a few exceptions, the results are rather
good for all the pairs of training and test language.
</bodyText>
<page confidence="0.994785">
255
</page>
<table confidence="0.9999216">
de nl Language for training fi others
da no sv
de P:83.56 P:78.69 P:74.96 P:88.93 P:82.72 P:89.69 P:80.89
R:79.48 R:75.48 R:92.77 R:89.26 R:90.79 R:89.96 R:76.07
A:87.21 A:82.76 A:83.53 A:90.31 A:86.53 A:90.82 A:88.15
nl P:79.52 P:78.99 P:76.93 P:92.81 P:85.67 P:90.98 P:77.53
R:75.74 R:76.18 R:89.02 R:55.08 R:87.15 R:86.73 R:76.54
A:87.77 A:83.45 A:83.21 A:91.00 A:86.47 A:88.95 A:82.32
da P:82.21 P:90.86 P:81.97 P:90.61 P:85.52 P:92.65 P:76.28
R:45.01 R:42.94 R:87.12 R:80.25 R:81.41 R:82.46 R:94.84
A:78.95 A:74.78 A:85.36 A:89.30 A:83.70 A:87.55 A:84.60
no P:68.23 P:70.18 P:74.85 P:88.13 P:82.25 P:90.08 P:88.78
R:83.33 R:87.18 R:96.67 R:93.05 R:94.21 R:91.84 R:90.88
A:83.77 A:80.67 A:84.18 A:90.40 A:87.24 A:91.41 A:89.85
sv P:76.57 P:77.33 P:76.31 P:89.00 P:83.34 P:90.81 P:83.89
R:79.76 R:81.79 R:94.66 R:90.41 R:92.98 R:90.86 R:92.05
A:87.18 A:83.38 A:84.57 A:89.67 A:87.79 A:91.38 A:87.69
fi P:74.12 P:74.50 P:75.93 P:88.71 P:83.54 P:90.79 P:90.70
R:80.12 R:81.67 R:95.39 R:91.46 R:92.70 R:91.21 R:90.62
A:85.93 A:81.98 A:84.51 A:90.07 A:87.52 A:91.62 A:91.18
</table>
<tableCaption confidence="0.98809">
Table 5: Result training and testing in different lan-
guages.
</tableCaption>
<bodyText confidence="0.999990576923077">
Thus, the use of features like frequencies, proba-
bilities or mutual information of compound parts is
truly language-independent and the models learned
from one language can safely be applied for decom-
pounding a different language without the need of
annotating a gold-standard for it.
Still, some trends in the results can be observed:
training with the Danish corpus produced the best
results in terms of recall for all the languages, but
recall for Danish still improved when we trained on
data from all languages. We believe that this in-
dicates that the Danish dataset contains items with
a more varied sets of feature combinations, so that
the models trained from it have a good coverage on
different kinds of compounds, but models trained
in other languages are not able to identify many of
the compounds in the Danish dataset. Concerning
precision, training with either the Norwegian or the
Finnish data produced very good results for most
languages. This is consistent with the monolingual
experiments (see Table 4) in which these languages
had the best results. We believe these trends are
probably due to the quality of the training data. In-
terestingly, the size of the training data is not so rel-
evant, as most of the best results are not located at
the last column in the table.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999977421052632">
This paper shows that a combination of several
corpus-based metrics for decompounding, previ-
ously applied to German, with big improvements
with respect to other state-of-the-art systems, is also
useful for other compounding languages. More in-
terestingly, models learned from a goldstandard cre-
ated for some language can be applied to other
languages, sometimes producing better results than
when a model is trained and tested in the same lan-
guage. This should alleviate the fact that the pro-
posed system is supervised, as there should just be
the need of creating a goldstandard in one language
in order to train a generic decompounder, thus facil-
itating the availability of decompounders for smaller
languages like Faroese. For future work, we plan to
investigate more deeply how the quality of the data
affects the results, with a more detailed error analy-
sis. Other open lines include exploring the addition
of new features to the trained models.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994203125">
S.F. Adafre, W.R. van Hage, J. Kamps, G.L. de Melo, and
M. de Rijke. 2004. The University of Amsterdam at
CLEF 2004. CLEF 2004 Workshop, pages 91–98.
M. Adda-Decker, G. Adda, and L. Lamel. 2000. Inves-
tigating text normalization and pronunciation variants
for German broadcast transcription. In ICSLP-2000.
E. Alfonseca, S. Bilac, and S. Pharies. 2008. German
decompounding in a difficult corpus. In CICLING.
M. Braschler and B. Ripplinger. 2004. How effective
is stemming and decompounding for german text re-
trieval? Information Retrieval, 7:291–316.
R.D. Brown. 2002. Corpus-driven splitting of compound
words. In TMI-2002.
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistics. Computational Linguistics,
22(2):249–254.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In ACL-2003.
A. Krott. 1999. Linking elements in compounds. LIN-
GUIST, 7 Oct 1999. http://listserv.linguistlist.org/cgi-
bin/wa?A2=ind9910a&amp;L=linguist&amp;P=6009.
S. Langer. 1998. Zur Morphologie und Semantik von
Nominalkomposita. Tagungsband der 4. Konferenz
zur Verarbeitung naturlicher Sprache (KONVENS).
T. Marek. 2006. Analysis of german compounds using
weighted finite state transducers. Technical report, BA
Thesis, Universit¨at Tbingen.
C. Monz and M. de Rijke. 2001. Shallow morpholog-
ical analysis in monolingual information retrieval for
Dutch, German and Italian. In CLEF-2001.
A. Schiller. 2005. German compound analysis with
wfsc. In Finite State Methods and NLP 2005.
</reference>
<page confidence="0.998448">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933713">
<title confidence="0.999937">Decompounding query keywords from compounding languages</title>
<author confidence="0.998169">Enrique Alfonseca Slaven Bilac Stefan Pharies</author>
<affiliation confidence="0.998053">Google Inc. Google Inc. Google Inc.</affiliation>
<email confidence="0.996675">ealfonseca@google.comslaven@google.comstefanp@google.com</email>
<abstract confidence="0.994238769230769">Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). Furthermore, real-time IR systems (such as search engines) need to cope with noisy data, as user queries are sometimes written quickly and submitted without review. In this paper we apply a state-of-the-art procedure for German decompounding to other compounding languages, and we show that it is possible to have a single decompounding model that is applicable across languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>253--256</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 253–256,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<title>c�2008 Association for Computational Linguistics</title>
<date>2008</date>
<booktitle>The University of Amsterdam at CLEF 2004. CLEF 2004 Workshop,</booktitle>
<pages>91--98</pages>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics S.F. Adafre, W.R. van Hage, J. Kamps, G.L. de Melo, and M. de Rijke. 2004. The University of Amsterdam at CLEF 2004. CLEF 2004 Workshop, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Adda-Decker</author>
<author>G Adda</author>
<author>L Lamel</author>
</authors>
<title>Investigating text normalization and pronunciation variants for German broadcast transcription.</title>
<date>2000</date>
<booktitle>In ICSLP-2000.</booktitle>
<contexts>
<context position="1534" citStr="Adda-Decker et al., 2000" startWordPosition="220" endWordPosition="223"> ones. So, for instance, the flower bouquet can be expressed in German as Blumenstr¨auf3e, made up of Blumen (flower) and str¨auf3e (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection). For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms. Decompounders have been used successfully in Information Retrieval (Braschler and Ripplinger, 2004), Machine Translation (Brown, 2002; Koehn and Knight, 2003) and Speech Recognition (Adda-Decker et al., 2000). The Cross Language Evaluation Forum (CLEF) competitions have shown that very simple approaches can produce big gains in Cross Language Information Retrieval (CLIR) for German and Dutch (Monz and de Rijke, 2001) and for Finnish (Adafre et al., 2004). When working with web data, which has not necessarily been reviewed for correctness, many of the words are more difficult to analyze than when working with standard texts. There are more words with spelling mistakes, and many texts mix words from different languages. This problem exists to a larger degree when handling user queries: they are writ</context>
</contexts>
<marker>Adda-Decker, Adda, Lamel, 2000</marker>
<rawString>M. Adda-Decker, G. Adda, and L. Lamel. 2000. Investigating text normalization and pronunciation variants for German broadcast transcription. In ICSLP-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Alfonseca</author>
<author>S Bilac</author>
<author>S Pharies</author>
</authors>
<title>German decompounding in a difficult corpus.</title>
<date>2008</date>
<booktitle>In CICLING.</booktitle>
<contexts>
<context position="2527" citStr="Alfonseca et al., 2008" startWordPosition="380" endWordPosition="383">ult to analyze than when working with standard texts. There are more words with spelling mistakes, and many texts mix words from different languages. This problem exists to a larger degree when handling user queries: they are written quickly, not paying attention to mistakes. However, being able to identify that achzigerjahre should be decompounded as achzig+jahre (where achzig is a misspelled variation of achtzig) is still useful in obtaining some meaning from the user query and in helping the spelling correction system. This paper evaluates a state-of-the-art procedure for German splitting (Alfonseca et al., 2008), robust enough to handle query data, on different languages, and shows that it is possible to have a single decompounding model that can be applied to all the languages under study. 2 Problem definition and evaluation settings Any set of query keywords contains a large amount of noisy data, such as words in foreign languages or misspelled words. In order to be robust enough to handle this kind of corpus, we require the following for a decompounder: first, obviously, compounds should be split, and non-compounds should be left untouched. This also applies if they are misspelled. Unknown words o</context>
</contexts>
<marker>Alfonseca, Bilac, Pharies, 2008</marker>
<rawString>E. Alfonseca, S. Bilac, and S. Pharies. 2008. German decompounding in a difficult corpus. In CICLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Braschler</author>
<author>B Ripplinger</author>
</authors>
<title>How effective is stemming and decompounding for german text retrieval? Information Retrieval,</title>
<date>2004</date>
<pages>7--291</pages>
<contexts>
<context position="1425" citStr="Braschler and Ripplinger, 2004" startWordPosition="204" endWordPosition="207">ch, Danish, Norwegian, Swedish, Greek or Finnish, allow the generation of complex words by merging together simpler ones. So, for instance, the flower bouquet can be expressed in German as Blumenstr¨auf3e, made up of Blumen (flower) and str¨auf3e (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection). For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms. Decompounders have been used successfully in Information Retrieval (Braschler and Ripplinger, 2004), Machine Translation (Brown, 2002; Koehn and Knight, 2003) and Speech Recognition (Adda-Decker et al., 2000). The Cross Language Evaluation Forum (CLEF) competitions have shown that very simple approaches can produce big gains in Cross Language Information Retrieval (CLIR) for German and Dutch (Monz and de Rijke, 2001) and for Finnish (Adafre et al., 2004). When working with web data, which has not necessarily been reviewed for correctness, many of the words are more difficult to analyze than when working with standard texts. There are more words with spelling mistakes, and many texts mix wor</context>
</contexts>
<marker>Braschler, Ripplinger, 2004</marker>
<rawString>M. Braschler and B. Ripplinger. 2004. How effective is stemming and decompounding for german text retrieval? Information Retrieval, 7:291–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Brown</author>
</authors>
<title>Corpus-driven splitting of compound words.</title>
<date>2002</date>
<booktitle>In TMI-2002.</booktitle>
<contexts>
<context position="1459" citStr="Brown, 2002" startWordPosition="211" endWordPosition="212">w the generation of complex words by merging together simpler ones. So, for instance, the flower bouquet can be expressed in German as Blumenstr¨auf3e, made up of Blumen (flower) and str¨auf3e (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection). For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms. Decompounders have been used successfully in Information Retrieval (Braschler and Ripplinger, 2004), Machine Translation (Brown, 2002; Koehn and Knight, 2003) and Speech Recognition (Adda-Decker et al., 2000). The Cross Language Evaluation Forum (CLEF) competitions have shown that very simple approaches can produce big gains in Cross Language Information Retrieval (CLIR) for German and Dutch (Monz and de Rijke, 2001) and for Finnish (Adafre et al., 2004). When working with web data, which has not necessarily been reviewed for correctness, many of the words are more difficult to analyze than when working with standard texts. There are more words with spelling mistakes, and many texts mix words from different languages. This </context>
</contexts>
<marker>Brown, 2002</marker>
<rawString>R.D. Brown. 2002. Corpus-driven splitting of compound words. In TMI-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the Kappa statistics.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: the Kappa statistics. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In ACL-2003.</booktitle>
<contexts>
<context position="1484" citStr="Koehn and Knight, 2003" startWordPosition="213" endWordPosition="216">ion of complex words by merging together simpler ones. So, for instance, the flower bouquet can be expressed in German as Blumenstr¨auf3e, made up of Blumen (flower) and str¨auf3e (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection). For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms. Decompounders have been used successfully in Information Retrieval (Braschler and Ripplinger, 2004), Machine Translation (Brown, 2002; Koehn and Knight, 2003) and Speech Recognition (Adda-Decker et al., 2000). The Cross Language Evaluation Forum (CLEF) competitions have shown that very simple approaches can produce big gains in Cross Language Information Retrieval (CLIR) for German and Dutch (Monz and de Rijke, 2001) and for Finnish (Adafre et al., 2004). When working with web data, which has not necessarily been reviewed for correctness, many of the words are more difficult to analyze than when working with standard texts. There are more words with spelling mistakes, and many texts mix words from different languages. This problem exists to a large</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>P. Koehn and K. Knight. 2003. Empirical methods for compound splitting. In ACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krott</author>
</authors>
<title>Linking elements in compounds.</title>
<date>1999</date>
<journal>LINGUIST,</journal>
<volume>7</volume>
<note>http://listserv.linguistlist.org/cgibin/wa?A2=ind9910a&amp;L=linguist&amp;P=6009.</note>
<contexts>
<context position="773" citStr="Krott, 1999" startWordPosition="105" endWordPosition="106">laven@google.com stefanp@google.com Abstract Splitting compound words has proved to be useful in areas such as Machine Translation, Speech Recognition or Information Retrieval (IR). Furthermore, real-time IR systems (such as search engines) need to cope with noisy data, as user queries are sometimes written quickly and submitted without review. In this paper we apply a state-of-the-art procedure for German decompounding to other compounding languages, and we show that it is possible to have a single decompounding model that is applicable across languages. 1 Introduction Compounding languages (Krott, 1999), such as German, Dutch, Danish, Norwegian, Swedish, Greek or Finnish, allow the generation of complex words by merging together simpler ones. So, for instance, the flower bouquet can be expressed in German as Blumenstr¨auf3e, made up of Blumen (flower) and str¨auf3e (bouquet), and in Finnish as kukkakimppu, from kukka (flower) and kimppu (bunch, collection). For many language processing tools that rely on lexicons or language models it is very useful to be able to decompose compounds to increase their coverage and reduce out-of-vocabulary terms. Decompounders have been used successfully in In</context>
</contexts>
<marker>Krott, 1999</marker>
<rawString>A. Krott. 1999. Linking elements in compounds. LINGUIST, 7 Oct 1999. http://listserv.linguistlist.org/cgibin/wa?A2=ind9910a&amp;L=linguist&amp;P=6009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Langer</author>
</authors>
<title>Zur Morphologie und Semantik von Nominalkomposita. Tagungsband der 4. Konferenz zur Verarbeitung naturlicher Sprache (KONVENS).</title>
<date>1998</date>
<marker>Langer, 1998</marker>
<rawString>S. Langer. 1998. Zur Morphologie und Semantik von Nominalkomposita. Tagungsband der 4. Konferenz zur Verarbeitung naturlicher Sprache (KONVENS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Marek</author>
</authors>
<title>Analysis of german compounds using weighted finite state transducers.</title>
<date>2006</date>
<tech>Technical report, BA Thesis,</tech>
<institution>Universit¨at Tbingen.</institution>
<marker>Marek, 2006</marker>
<rawString>T. Marek. 2006. Analysis of german compounds using weighted finite state transducers. Technical report, BA Thesis, Universit¨at Tbingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
<author>M de Rijke</author>
</authors>
<title>Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian.</title>
<date>2001</date>
<booktitle>In CLEF-2001.</booktitle>
<marker>Monz, de Rijke, 2001</marker>
<rawString>C. Monz and M. de Rijke. 2001. Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian. In CLEF-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schiller</author>
</authors>
<title>German compound analysis with wfsc.</title>
<date>2005</date>
<booktitle>In Finite State Methods and NLP</booktitle>
<marker>Schiller, 2005</marker>
<rawString>A. Schiller. 2005. German compound analysis with wfsc. In Finite State Methods and NLP 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>