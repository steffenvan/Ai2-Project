<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001286">
<title confidence="0.996704">
Selective Phrase Pair Extraction for
Improved Statistical Machine Translation
</title>
<author confidence="0.579295">
Luke S. Zettlemoyer Robert C. Moore
</author>
<affiliation confidence="0.425423">
MIT CSAIL Microsoft Research
</affiliation>
<address confidence="0.8330205">
Cambridge, MA 02139 One Microsoft Way
lsz@csail.mit.edu Redmond, WA 98052
</address>
<email confidence="0.995541">
bobmoore@microsoft.com
</email>
<sectionHeader confidence="0.995578" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998998">
Phrase-based statistical machine transla-
tion systems depend heavily on the knowl-
edge represented in their phrase transla-
tion tables. However, the phrase pairs
included in these tables are typically se-
lected using simple heuristics that poten-
tially leave much room for improvement.
In this paper, we present a technique for
selecting the phrase pairs to include in
phrase translation tables based on their es-
timated quality according to a translation
model. This method not only reduces the
size of the phrase translation table, but
also improves translation quality as mea-
sured by the BLEU metric.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895916666667">
Phrase translation tables are the heart of phrase-
based statistical machine translation (SMT) systems.
They provide pairs of phrases that are used to con-
struct a large set of potential translations for each
input sentence, along with feature values associated
with each phrase pair that are used to select the best
translation from this set.1
The most widely used method for building phrase
translation tables (Koehn et al., 2003) selects, from
a word alignment of a parallel bilingual training cor-
pus, all pairs of phrases (up to a given length) that
are consistent with the alignment. This procedure
</bodyText>
<footnote confidence="0.625613">
1A “phrase” in this sense can be any contiguous sequence of
words, and need not be a complete linguistic constituent.
</footnote>
<bodyText confidence="0.999910952380952">
typically generates many phrase pairs that are not re-
motely reasonable translation candidates.2 To avoid
creating translations that use these pairs, a set of fea-
tures is computed for each pair. These features are
used to train a translation model, and phrase pairs
that produce low scoring translations are avoided. In
practice, it is often assumed that current translation
models are good enough to avoid building transla-
tions with these unreasonable phrase pairs.
In this paper, we question this assumption by in-
vestigating methods for pruning low quality phrase
pairs. We present a simple procedure that reduces
the overall phrase translation table size while in-
creasing translation quality. The basic idea is to
initially gather the phrase pairs and train an trans-
lation model as usual, but to then select a subset of
the overall phrases that performs the best, prune the
others, and retrain the translation model. In experi-
ments, this approach reduced the size of the phrase
tranlsation table by half, and improved the BLEU
score of the resulting translations by up to 1.5 points.
</bodyText>
<sectionHeader confidence="0.976054" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99848675">
As a baseline, we present a relatively standard SMT
approach, following Koehn et al. (2003). Potential
translations are scored using a linear model where
the best translation is computed as
</bodyText>
<construct confidence="0.270528">
Aifi(s, a, t)
</construct>
<bodyText confidence="0.9985155">
where s is the input sentence, t is the output sen-
tence, and a is a phrasal alignment that specifies how
</bodyText>
<footnote confidence="0.9980435">
2In one experiment, we managed to generate more than
117,000 English phrases for the the French word “de”.
</footnote>
<equation confidence="0.4527201">
n
i=1
arg max
t,a
209
Proceedings of NAACL HLT 2007, Companion Volume, pages 209–212,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
Monsieur le Orateur , je invoque le R`egement
� �// � �
Mr. Speaker , I rise on a point of order
</equation>
<figureCaption confidence="0.996932">
Figure 1: A word aligned sentence pair.
</figureCaption>
<bodyText confidence="0.999640166666667">
t is constructed from s. The weights Ai associated
with each feature fi are tuned to maximize the qual-
ity of the translations.
The training procedure starts by computing a
word alignment for each sentence pair in the train-
ing corpus. A word alignment is a relation between
the words in two sentences where, intuitively, words
are aligned to their translation in the other language.
In this work, we use a discriminatively trained word
aligner (Moore et al., 2006) that has state of the art
performance. Figure 1 presents a high quality align-
ment produced by this aligner.
Given a word aligned corpus, the second step is to
extract a phrase translation table. Each entry in this
table contains a source language phrase s, a target
language phrase t, and a list of feature values 0(s, t).
It is usual to extract every phrase pair, up to a cer-
tain phrase length, that is consistent with the word
alignment that is annotated in the corpus. Each con-
sistent pair must have at least one word alignment
between words within the phrases and no words in
either phrase can be aligned any words outside of the
phrases. For example, Figure 2 shows some of the
phrase pairs that would be extracted from the word-
aligned sentence pair in Figure 1. A full list using
phrases of up to three words would include 28 pairs.
For each extracted phrase pair (s, t), feature val-
ues 0(s, t) = (log p(s|t), log p(t|s), log l(s, t)) are
computed. The first two features, the log translation
and inverse translation probabilities, are estimated
by counting phrase cooccurrences, following Koehn
et al. (2003). The third feature is the logarithm of
a lexical score l(s, t) that provides a simple form of
smoothing by weighting a phrase pair based on how
likely individual words within the phrases are to be
translations of each other. We use a version from
Foster et al. (2006), modified from (Koehn et al.,
2003), which is an average of pairwise word transla-
tion probabilities.
In phrase-based SMT, the decoder produces trans-
lations by dividing the source sentence into a se-
quence of phrases, choosing a target language phrase
</bodyText>
<table confidence="0.994599153846154">
# Source Lang. Phrase Target Lang. Phrase
1 Monsieur Mr.
2 Monsieur le Mr.
3 Monsieur le Orateur Mr. Speaker
4 le Orateur Speaker
5 Orateur Speaker
� � � � � � � � �
23 le R`eglement point of order
24 le R`eglement of order
25 le R`eglement order
26 R`eglement point of order
27 R`eglement of order
28 R`eglement order
</table>
<figureCaption confidence="0.982433">
Figure 2: Phrase pairs consistent with the word
alignment in Figure 1.
</figureCaption>
<bodyText confidence="0.999937722222222">
as a translation for each source language phrase, and
ordering the target language phrases to build the fi-
nal translated sentence. Each potential translation is
scored according to a weighted linear model. We
use the three features from the phrase translation ta-
ble, summing their values for each phrase pair used
in the translation. We also use four additional fea-
tures: a target language model, a distortion penalty,
the target sentence word count, and the phrase pair
count, all computed as described in (Koehn, 2004).
For all of the experiments in this paper, we used the
Pharaoh beam-search decoder (Koehn, 2004) with
the features described above.
Finally, to estimate the parameters Ai of the
weighted linear model, we adopt the popular min-
imum error rate training procedure (Och, 2003)
which directly optimizes translation quality as mea-
sured by the BLEU metric.
</bodyText>
<sectionHeader confidence="0.987207" genericHeader="method">
3 Selective Phrase Pair Extraction
</sectionHeader>
<bodyText confidence="0.999475333333333">
In order to improve performance, it is important to
select high quality phrase pairs for the phrase trans-
lation table. We use two key ideas to guide selection:
</bodyText>
<listItem confidence="0.9723275">
• Preferential Scoring: Phrase pairs are selected
using a function q(s, t) that returns a high score
for source, target phrase pairs (s, t) that lead to
high quality translations.
• Redundancy Constraints: Our intuition is
that each occurrence of a source or target lan-
</listItem>
<bodyText confidence="0.8994355">
guage phrase really has at most one translation
for that sentence pair. Redundancy constraints
minimize the number of possible translations
that are extracted for each phrase occurrence.
</bodyText>
<page confidence="0.991285">
210
</page>
<bodyText confidence="0.999784638888889">
Selecting phrases that a translation model prefers
and eliminating at least some of the ambiguity that
comes with extracting multiple translations for a sin-
gle phrase occurrence creates a smaller phrase trans-
lation table with higher quality entries.
The ideal scoring metric would give high scores
to phrase pairs that lead to high-quality translations
and low scores to those that would decrease transla-
tion quality. The best such metric we have available
is provided by the overall translation model. Our
scoring metric q(s, t) is therefore computed by first
extracting a full phrase translation table, then train-
ing a full translation model, and finally using a sub-
part of the model to score individual phrase pairs in
isolation. Because the scoring is tied to a model that
is optimized to maximize translation quality, more
desirable phrase pairs should be given higher scores.
More specifically, q(s, t) _ 0(s, t) · A where
0(s, t) is the length three vector that contains the
feature values stored with the phrase pair (s, t) in the
phrase translation table, and A is a vector of the three
parameter values that were learned for these features
by the full translation model. The rest of the features
are ignored because they are either constant or de-
pend on the target language sentence which is fixed
during phrase extraction. In essence, we are using
the subpart of a full translation model that looks at
phrase pair identity and scoring the pair based on
how the full model would like it.
This scoring metric is used in a phrase pair se-
lection algorithm inspired by competitive linking
for word alignment (Melamed, 2000). Local com-
petitive linking extracts high scoring phrase pairs
while enforcing a redundancy constraint that mini-
mizes the number of phrase pairs that share a com-
mon phrase. For each sentence pair in the training
set, this algorithm marks the highest scoring phrase
pair, according to q(s, t), containing each source
language phrase and the highest scoring phrase pair
containing each target language phrase. Each of
these marked phrase pairs is selected and the phrase
translation table is rebuilt. This is a soft redundancy
constraint because a phrase pair will only be ex-
cluded if there is a higher scoring pair that shares
its source language phrase and a higher scoring pair
that shares its target language phrase. For example,
consider again the phrase pairs in Figure 2 and as-
sume they are sorted by their scores. Local compet-
itive linking will select every phrase pair except for
27 and 28. All other pairs are the highest scoring
options for at least one of their phrases.
Selective phrase extraction with competitive link-
ing can be seen as a Viterbi reestimation algorithm.
Because we are extracting fewer phrase pairs, the
features associated with each phrase pair will differ.
If the removed phrases were not real translations of
each other in the first place, the translation features
p(s|t) and p(t|s) should be better estimates because
the high quality phrases that remain will be given
the probability mass that was assigned to the pruned
phrase pairs. Although we are running it in a purely
discriminative setting, it has a similar feel to an EM
algorithm. First, a full phrase translation table and
parameter estimate is computed. Then, based on that
estimate, a subset of the phrases is selected which,
in turn, supplies a new estimate for the parameters.
One question is how many times to run this reestima-
tion procedure. We found, on the development set,
that it never helped to run more than one iteration.
Perhaps because of the hard nature of the algorithm,
repeated iterations caused slight decreases in phrase
translation table size and overall performance.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999751">
In this section, we report experiments conducted
with Canadian Hansards data from the 2003 HLT-
NAACL word-alignment workshop (Mihalcea and
Pedersen, 2003). Phrase pairs are extracted
from 500,000 word-aligned French-English sen-
tence pairs. Translation quality is evaluated accord-
ing to the BLEU metric (with one reference trans-
lation). Three additional disjoint data sets (from the
same source) were used, one with 500 sentence pairs
for minimum error rate training, another with 1000
pairs for development testing, and a final set of 2000
sentence pairs for the final test. For each experiment,
we trained the full translation model as described in
Section 2. Each trial varied only in the phrase trans-
lation table that was used.3
One important question is what the maximum
phrase length should be for extraction. To inves-
tigate this issue, we ran experiments on the devel-
</bodyText>
<footnote confidence="0.99916125">
3These experiments also used the default pruning from the
Pharaoh decoder, allowing only the 10 best output phrases to be
considered for each input phrase. This simple global pruning
cannot be substituted for the competitive linking described here.
</footnote>
<page confidence="0.997305">
211
</page>
<figure confidence="0.9850143">
Full Phrase Trans. Ta
Local Comp. Lin
80
70
60
50
40
30
20
10
</figure>
<bodyText confidence="0.999201923076923">
allow for a significantly smaller phrase translation
table, but never improve translation performance.
In this paper, we presented an algorithm that
improves translation quality by selecting a smaller
phrase translation table. We hope that this work
highlights the need to think carefully about the qual-
ity of the phrase translation table, which is the cen-
tral knowledge source for most modern statistical
machine translation systems. The methods used in
the experiments are so simple that we believe that
there is significant potential for improvement by us-
ing better methods for scoring phrase pairs and se-
lecting phrase pairs based those scores.
</bodyText>
<figure confidence="0.9697425">
3 4 5
Maximum Phrase
</figure>
<figureCaption confidence="0.999878">
Figure 3: Scaling the maximum phrase length.
</figureCaption>
<bodyText confidence="0.999961444444444">
opment set. Figure 3 shows a comparison of the
full phrase table to local competitive linking as the
maximum phrase length is varied. Local competi-
tive linking consistently outperforms the full table
and the difference in BLEU score seems to increase
with the length. The growth in the size of the phrase
translation table seems to be linear with maximum
phrase length in both cases, with the table size grow-
ing at a slower rate under local competitive linking.
To verify these results, we tested the model
trained with the full phrase translation table against
the model trained with the table selected by local
competitive linking on the heldout test data. Both ta-
bles included phrases up to length 7 and the models
were tested on a set of 2000 unseen sentence pairs.
The results matched the development experiments.
The full system scored 26.78 while the local linking
achieved 28.30, a difference of 1.52 BLEU points.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999987777777778">
The most closely related work attempts to create
higher quality phrase translation tables by learning
a generative model that directly incorporates phrase
pair selection. The original approach (Marcu and
Wong, 2002) was limited due to computational con-
straints but recent work (DeNero et al., 2006; Birch
et al., 2006) has improved the efficiency by using
word alignments as constraints on the set of possible
phrase pairs. The best results from this line of work
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99945954054054">
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and
Philipp Koehn. 2006. Constraining the phrase-based, join
probability statistical translation model. In Proceedings of
the Workshop on Stastical Machine Translation.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Proceedings of the Workshop on Stastical Machine
Translation.
George Foster, Roland Kuhn, and Howard Johnson. 2006.
Phrasetable smoothing for stastical machine translation. In
Proceedings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Stastical phrase-based translation. In Proceedings of the
North American Chapter of the Association for Computa-
tional Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of The Sixth Conference of the Association for Ma-
chine Translation in the Americas.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing.
I. Dan Melamed. 2000. Models of translation equivalence
amoung words. Computational Linguistics, 26(2):221–249.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise
for word alignment. In Proceedings of the HLT-NAACL 2003
Workshop, Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Im-
proved discriminative bilingual word alignment. In Proceed-
ings of the 44th Annual Meeting of the Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.
</reference>
<page confidence="0.998455">
212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.714438">
<title confidence="0.998186">Selective Phrase Pair Extraction Improved Statistical Machine Translation</title>
<author confidence="0.999949">Luke S Zettlemoyer Robert C Moore</author>
<affiliation confidence="0.999076">MIT CSAIL Microsoft Research</affiliation>
<address confidence="0.997816">Cambridge, MA 02139 One Microsoft Way lsz@csail.mit.edu WA 98052</address>
<email confidence="0.99974">bobmoore@microsoft.com</email>
<abstract confidence="0.98246025">Phrase-based statistical machine translation systems depend heavily on the knowledge represented in their phrase translation tables. However, the phrase pairs included in these tables are typically selected using simple heuristics that potentially leave much room for improvement. In this paper, we present a technique for selecting the phrase pairs to include in phrase translation tables based on their estimated quality according to a translation model. This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Constraining the phrase-based, join probability statistical translation model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Stastical Machine Translation.</booktitle>
<marker>Birch, Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Alexandra Birch, Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Constraining the phrase-based, join probability statistical translation model. In Proceedings of the Workshop on Stastical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Stastical Machine Translation.</booktitle>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Proceedings of the Workshop on Stastical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for stastical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5242" citStr="Foster et al. (2006)" startWordPosition="872" endWordPosition="875">e 1. A full list using phrases of up to three words would include 28 pairs. For each extracted phrase pair (s, t), feature values 0(s, t) = (log p(s|t), log p(t|s), log l(s, t)) are computed. The first two features, the log translation and inverse translation probabilities, are estimated by counting phrase cooccurrences, following Koehn et al. (2003). The third feature is the logarithm of a lexical score l(s, t) that provides a simple form of smoothing by weighting a phrase pair based on how likely individual words within the phrases are to be translations of each other. We use a version from Foster et al. (2006), modified from (Koehn et al., 2003), which is an average of pairwise word translation probabilities. In phrase-based SMT, the decoder produces translations by dividing the source sentence into a sequence of phrases, choosing a target language phrase # Source Lang. Phrase Target Lang. Phrase 1 Monsieur Mr. 2 Monsieur le Mr. 3 Monsieur le Orateur Mr. Speaker 4 le Orateur Speaker 5 Orateur Speaker � � � � � � � � � 23 le R`eglement point of order 24 le R`eglement of order 25 le R`eglement order 26 R`eglement point of order 27 R`eglement of order 28 R`eglement order Figure 2: Phrase pairs consist</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for stastical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Stastical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1291" citStr="Koehn et al., 2003" startWordPosition="193" endWordPosition="196">according to a translation model. This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric. 1 Introduction Phrase translation tables are the heart of phrasebased statistical machine translation (SMT) systems. They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence, along with feature values associated with each phrase pair that are used to select the best translation from this set.1 The most widely used method for building phrase translation tables (Koehn et al., 2003) selects, from a word alignment of a parallel bilingual training corpus, all pairs of phrases (up to a given length) that are consistent with the alignment. This procedure 1A “phrase” in this sense can be any contiguous sequence of words, and need not be a complete linguistic constituent. typically generates many phrase pairs that are not remotely reasonable translation candidates.2 To avoid creating translations that use these pairs, a set of features is computed for each pair. These features are used to train a translation model, and phrase pairs that produce low scoring translations are avo</context>
<context position="2771" citStr="Koehn et al. (2003)" startWordPosition="436" endWordPosition="439"> We present a simple procedure that reduces the overall phrase translation table size while increasing translation quality. The basic idea is to initially gather the phrase pairs and train an translation model as usual, but to then select a subset of the overall phrases that performs the best, prune the others, and retrain the translation model. In experiments, this approach reduced the size of the phrase tranlsation table by half, and improved the BLEU score of the resulting translations by up to 1.5 points. 2 Background As a baseline, we present a relatively standard SMT approach, following Koehn et al. (2003). Potential translations are scored using a linear model where the best translation is computed as Aifi(s, a, t) where s is the input sentence, t is the output sentence, and a is a phrasal alignment that specifies how 2In one experiment, we managed to generate more than 117,000 English phrases for the the French word “de”. n i=1 arg max t,a 209 Proceedings of NAACL HLT 2007, Companion Volume, pages 209–212, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Monsieur le Orateur , je invoque le R`egement � �// � � Mr. Speaker , I rise on a point of order Figure 1: A word</context>
<context position="4974" citStr="Koehn et al. (2003)" startWordPosition="823" endWordPosition="826">t have at least one word alignment between words within the phrases and no words in either phrase can be aligned any words outside of the phrases. For example, Figure 2 shows some of the phrase pairs that would be extracted from the wordaligned sentence pair in Figure 1. A full list using phrases of up to three words would include 28 pairs. For each extracted phrase pair (s, t), feature values 0(s, t) = (log p(s|t), log p(t|s), log l(s, t)) are computed. The first two features, the log translation and inverse translation probabilities, are estimated by counting phrase cooccurrences, following Koehn et al. (2003). The third feature is the logarithm of a lexical score l(s, t) that provides a simple form of smoothing by weighting a phrase pair based on how likely individual words within the phrases are to be translations of each other. We use a version from Foster et al. (2006), modified from (Koehn et al., 2003), which is an average of pairwise word translation probabilities. In phrase-based SMT, the decoder produces translations by dividing the source sentence into a sequence of phrases, choosing a target language phrase # Source Lang. Phrase Target Lang. Phrase 1 Monsieur Mr. 2 Monsieur le Mr. 3 Mons</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Stastical phrase-based translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of The Sixth Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="6403" citStr="Koehn, 2004" startWordPosition="1074" endWordPosition="1075"> 28 R`eglement order Figure 2: Phrase pairs consistent with the word alignment in Figure 1. as a translation for each source language phrase, and ordering the target language phrases to build the final translated sentence. Each potential translation is scored according to a weighted linear model. We use the three features from the phrase translation table, summing their values for each phrase pair used in the translation. We also use four additional features: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above. Finally, to estimate the parameters Ai of the weighted linear model, we adopt the popular minimum error rate training procedure (Och, 2003) which directly optimizes translation quality as measured by the BLEU metric. 3 Selective Phrase Pair Extraction In order to improve performance, it is important to select high quality phrase pairs for the phrase translation table. We use two key ideas to guide selection: • Preferential Scoring: Phrase pairs are selected using</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings of The Sixth Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translation equivalence amoung words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="9037" citStr="Melamed, 2000" startWordPosition="1511" endWordPosition="1512">rase pair (s, t) in the phrase translation table, and A is a vector of the three parameter values that were learned for these features by the full translation model. The rest of the features are ignored because they are either constant or depend on the target language sentence which is fixed during phrase extraction. In essence, we are using the subpart of a full translation model that looks at phrase pair identity and scoring the pair based on how the full model would like it. This scoring metric is used in a phrase pair selection algorithm inspired by competitive linking for word alignment (Melamed, 2000). Local competitive linking extracts high scoring phrase pairs while enforcing a redundancy constraint that minimizes the number of phrase pairs that share a common phrase. For each sentence pair in the training set, this algorithm marks the highest scoring phrase pair, according to q(s, t), containing each source language phrase and the highest scoring phrase pair containing each target language phrase. Each of these marked phrase pairs is selected and the phrase translation table is rebuilt. This is a soft redundancy constraint because a phrase pair will only be excluded if there is a higher</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translation equivalence amoung words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop, Building and Using Parallel Texts: Data Driven Machine Translation and Beyond.</booktitle>
<contexts>
<context position="11287" citStr="Mihalcea and Pedersen, 2003" startWordPosition="1878" endWordPosition="1881">ter estimate is computed. Then, based on that estimate, a subset of the phrases is selected which, in turn, supplies a new estimate for the parameters. One question is how many times to run this reestimation procedure. We found, on the development set, that it never helped to run more than one iteration. Perhaps because of the hard nature of the algorithm, repeated iterations caused slight decreases in phrase translation table size and overall performance. 4 Experiments In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLTNAACL word-alignment workshop (Mihalcea and Pedersen, 2003). Phrase pairs are extracted from 500,000 word-aligned French-English sentence pairs. Translation quality is evaluated according to the BLEU metric (with one reference translation). Three additional disjoint data sets (from the same source) were used, one with 500 sentence pairs for minimum error rate training, another with 1000 pairs for development testing, and a final set of 2000 sentence pairs for the final test. For each experiment, we trained the full translation model as described in Section 2. Each trial varied only in the phrase translation table that was used.3 One important question</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of the HLT-NAACL 2003 Workshop, Building and Using Parallel Texts: Data Driven Machine Translation and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3858" citStr="Moore et al., 2006" startWordPosition="625" endWordPosition="628">tional Linguistics Monsieur le Orateur , je invoque le R`egement � �// � � Mr. Speaker , I rise on a point of order Figure 1: A word aligned sentence pair. t is constructed from s. The weights Ai associated with each feature fi are tuned to maximize the quality of the translations. The training procedure starts by computing a word alignment for each sentence pair in the training corpus. A word alignment is a relation between the words in two sentences where, intuitively, words are aligned to their translation in the other language. In this work, we use a discriminatively trained word aligner (Moore et al., 2006) that has state of the art performance. Figure 1 presents a high quality alignment produced by this aligner. Given a word aligned corpus, the second step is to extract a phrase translation table. Each entry in this table contains a source language phrase s, a target language phrase t, and a list of feature values 0(s, t). It is usual to extract every phrase pair, up to a certain phrase length, that is consistent with the word alignment that is annotated in the corpus. Each consistent pair must have at least one word alignment between words within the phrases and no words in either phrase can b</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6675" citStr="Och, 2003" startWordPosition="1118" endWordPosition="1119"> weighted linear model. We use the three features from the phrase translation table, summing their values for each phrase pair used in the translation. We also use four additional features: a target language model, a distortion penalty, the target sentence word count, and the phrase pair count, all computed as described in (Koehn, 2004). For all of the experiments in this paper, we used the Pharaoh beam-search decoder (Koehn, 2004) with the features described above. Finally, to estimate the parameters Ai of the weighted linear model, we adopt the popular minimum error rate training procedure (Och, 2003) which directly optimizes translation quality as measured by the BLEU metric. 3 Selective Phrase Pair Extraction In order to improve performance, it is important to select high quality phrase pairs for the phrase translation table. We use two key ideas to guide selection: • Preferential Scoring: Phrase pairs are selected using a function q(s, t) that returns a high score for source, target phrase pairs (s, t) that lead to high quality translations. • Redundancy Constraints: Our intuition is that each occurrence of a source or target language phrase really has at most one translation for that s</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>