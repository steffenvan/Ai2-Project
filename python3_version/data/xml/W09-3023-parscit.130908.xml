<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.104413">
<note confidence="0.588058">
Developing Novel Multimodal and Linguistic Annotation Software
Podlasov A., O’Halloran K., Tan S., Smith B., Nagarajan A.
Multimodal Analysis Lab, IDMI, National University of Singapore
9 Prince George’s Park, Singapore
</note>
<email confidence="0.994319">
{idmpa,k.ohalloran,sabinetan,idmbas,idmarun}@nus.edu.sg
</email>
<sectionHeader confidence="0.994077" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992130777777778">
In this paper we present a collaborative work
between computer and social scientists result-
ing in the development of annotation software
for conducting research and analysis in social
semiotics in both multimodal and linguistic
aspects. The paper describes the proposed
software and discusses how this tool can con-
tribute for development of social semiotic the-
ory and practice.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886384615385">
Despite the advances that have been achieved in
the conceptualization of multimodal theories for
analysing language, images and other semiotic
resources, many frameworks and models for the
transcription and analysis of multimodal data
continue to rely on ‘low-tech’, manual and com-
puter-assisted, technologies. A construct pre-
ferred by many researchers for analyzing and
representing the interplay of semiotic resources
in multimodal data is the Table, as exemplified
by a range of transcription templates (Baldry &amp;
Thibault, 2006). Although tables can be effective
for recording which specific semiotic modes and
resources are co-deployed at a given moment in
the text, they essentially remain bounded by the
confines of the printable page.
Besides being laborious to create, page-based
analysis severely limits the researchers’ ability to
effectively capture and portray the complex in-
terplay of semiotic modes and resources as they
unfold on a larger scale, especially in the case of
dynamic video texts and interactive digital media
sites (O’Halloran, 2009). In addition, the analysis
will necessarily be restricted to manual transcrip-
tion and annotation techniques, which often in-
volve a complexity of symbolic notations and
</bodyText>
<figureCaption confidence="0.991313">
Figure 1. Systemics 1.0 interface.
</figureCaption>
<bodyText confidence="0.99747984">
abbreviations which may be difficult to learn and
comprehend (e.g. Thibault, 2000).
We propose that the study of meaning-bearing
phenomena requires a collaborative effort be-
tween social and computer scientists. Social se-
miotic study clearly can benefit from theoretical
concepts and analytical approaches beyond those
traditionally developed for typical social science
data, and which draw upon the rich variety of
computational techniques developed within
computational science. This collaborative ap-
proach proved to be productive in development
of the Systemics 1.0 (Judd &amp; O’Halloran, 2002)
software for research and teaching Systemic
Functional Linguistics (SFL) (O’Halloran,
2003). Systemics 1.0 provides a default systemic
functional grammar which can be used to manu-
ally code the linguistic analysis from pull-down
menus, see Figure 1. The results of the analysis
are stored in a data-base format which makes it
relatively simple to extract linguistic patterns
through relational data base searches. The default
grammar can be changed to suit user require-
ments, so that the software is used for teaching
and research purposes. The success of the re-
</bodyText>
<page confidence="0.9798">
134
</page>
<note confidence="0.957429">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 134–137,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99994132">
search collaboration which resulted in Systemics
1.0 gives strong motivation to consider com-
puter-based approaches for the more general case
of multimodal studies which involves, along with
linguistic analysis, analyses of various meaning-
bearing phenomena such as gaze, gesture, intona-
tion etc (e.g. Bateman, 2008; O’Halloran 2009)
In this paper we report on an ongoing interdis-
ciplinary collaboration between computer scien-
tists and social scientists taking place in Multi-
modal Analysis Lab, National University of Sin-
gapore in the frame of the Events in the World
project. The project aims at developing an inter-
active platform for manual, computer-assisted
and automatic annotation, analysis and represen-
tation of multimedia data for social semiotic and
SFL research. The project involves a range of
researchers working together to develop the
software interface and operational functionali-
ties. The respective roles of the interdisciplinary
research team are evolving out of a close col-
laboration, and the researchers are learning new
approaches, perspectives and knowledge which
are resulting in new ways of thinking about mul-
timodal phenomena, annotation and analysis.
</bodyText>
<sectionHeader confidence="0.749959" genericHeader="introduction">
2 Proposed system
</sectionHeader>
<bodyText confidence="0.999859333333333">
The purpose of the proposed software is to pro-
vide social semioticians with the technical means
to improve their efficiency and capabilities for
studying and annotating multimedia texts, as
well as accessing, visualizing and sharing analy-
ses.
</bodyText>
<subsectionHeader confidence="0.98944">
2.1 Multimedia resources
</subsectionHeader>
<bodyText confidence="0.999710857142857">
Along with supporting standard multimedia re-
sources like video, audio and imagery files, the
proposed system supports real-value resources
to facilitate the use of real-time numerical data in
semiotic analysis. Any parameter can be meas-
ured during the main media acquisition process
or calculated off-line.
</bodyText>
<subsectionHeader confidence="0.995637">
2.2 The interface
</subsectionHeader>
<bodyText confidence="0.9999225625">
The interface of the software follows a graphical
point-and-click paradigm and is designed to pro-
vide easy and straightforward interaction for the
user. At the moment of publication, the proposed
software is oriented more to the analysis of dy-
namic time-stamped data, therefore, the main
interface window is organized in a partiture-like
layout with the horizontal axis corresponding to
time. The application’s GUI during a sample an-
notation of a news video clip (© Reuters 2008) is
illustrated in Figure 2.
The annotation is organized into strips – con-
tainers for semantically grouped annotation ele-
ments, which are rendered with respect to the
time scale, see Figure 2, D and E. Several strips
can be embedded within another strip, which al-
lows hierarchical organization of the annotation
data (see Figure 2, ‘Annotations’ strip with em-
bedded (D) ‘Type-in’, (E) ‘Systemic’ and (F)
‘Images’ strips). A strip can be ‘collapsed’
(wrapped) in case its display is not needed at any
particular time, which is a useful feature when
the analysis becomes overloaded with details
(see Figure 2, G).
Resources are rendered in the main annotation
window in a similar strip-like way. A movie re-
source control (Figure 2, A) is drawn as a strip of
frames enabling interaction with the Movie
Viewer tool window. Sound resource control can
be drawn as waveform (Figure 2, B). At the mo-
ment of publication this control is not interactive
and used for display only. Real-value resource
control is rendered as a graph (Figure 2, C).
Time stamped nodes with associated text or
categorical information remain the main tool for
annotation and analysis. The node control is ren-
dered as a rectangle and provides the interface
for manipulating its position (modifying time-
stamps) and displaying the associated data,
which is text (Figure 2, D) or categorical associa-
tion (Figure 2, E) using the Systems Browser
tool (Figure 3). The software allows rendering of
static image resources. The annotation control
representing the image also belongs to a strip in
order to follow the general partiture-like organi-
zation. The software also allows extraction of
frames from the video for annotation in a manner
similar to images (see Figure 2, F).
</bodyText>
<subsectionHeader confidence="0.999799">
2.3 Categorical analysis
</subsectionHeader>
<bodyText confidence="0.992867333333333">
By categorical (systemic) analysis we understand
a process of associating time stamped annotation
controls (nodes) with choices from a system of
</bodyText>
<figureCaption confidence="0.999369">
Figure 2. Interface of the proposed software.
</figureCaption>
<page confidence="0.858913">
135
</page>
<figureCaption confidence="0.991406">
Figure 4. Overlay Editor tool window.
Figure 3. Systems Browser tool window and sam-
ple camera movement and tone analysis (super-
imposed).
</figureCaption>
<bodyText confidence="0.9999895">
categories describing the phenomena semioti-
cally or by other theoretical means. The Systems
Browser tool window provides the interface to
create and manipulate those systems, as well as
use them for annotation. Figure 3 illustrates two
simple categorical systems: describing camera
motion – eg. stationary or mobile camera; the
type of tracking (pan, tilt or zoom); and tone
choices in speech (cf Halliday 1967). The appli-
cation allows not only to define the semiotic sys-
tem itself, but also to associate textual informa-
tion with every choice, which makes it a helpful
interactive glossary for the analyst. Besides that,
it allows associating different style attributes of
the particular choice: fill color, outline color and
dash style, etc. As the user makes systemic
choices, the system uses text and style attributes
to make the selections visible.
This becomes an important feature of the pro-
posed software. The analysis becomes visible not
only textually, but also graphically. In Figure 3
we superimposed the annotation strips with ana-
lyzed camera movement and tone for a sample
video clip. The filmstrip itself does not visualize
the dynamics of the camera in the clip, nor the
soundtrack identify tone or other language
choices. On the other hand, systemic analysis
using style attributes gives a graphical perspec-
tive on such choices, so that it is easier to ob-
serve and pick up outliers, patterns or points of
interest, especially when compared to the tradi-
tional page-based annotation. This is especially
important when one has to deal with dozen of
strips each having hundreds of nodes.
</bodyText>
<subsectionHeader confidence="0.958952">
2.4 Overlays
</subsectionHeader>
<bodyText confidence="0.9883122">
Images and video frames are not dynamic re-
sources and, therefore, are annotated and ana-
lyzed by graphical and textual means. Still, tex-
tual annotation often needs to be associated with
a particular location in the image. Therefore, the
proposed software provides a basic graphical
annotation tool window – Overlay Editor, see
Figure 4 - shown also in the thumbnail image in
the main interface, giving the analyst the ability
to glance over the whole annotation.
</bodyText>
<subsectionHeader confidence="0.932666">
2.5 Visualization
</subsectionHeader>
<bodyText confidence="0.9999700625">
Visualization of the data is especially important
in cases when there are multiple analyses of the
same text presented or there is a large corpus of
texts being analyzed. In terms of the former,
visualizations may be useful in picturing correla-
tions, interactions and collaborations between
different systems operating concurrently in the
multimodal text: e.g. linguistic aspects, gaze,
gesture, intonation, etc. In terms of the latter,
visualizations may reveal patterns and departures
from patterns within a text. At the moment of
publication, the annotation data is visualized by
the means of style attributes of systemic choices.
However, the architecture allows for more so-
phisticated visualization techniques to be imple-
mented in future.
</bodyText>
<subsectionHeader confidence="0.995744">
2.6 Automation
</subsectionHeader>
<bodyText confidence="0.998982933333333">
Generally, the proposed system is designed for
the human analyst and, therefore, assumes man-
ual annotation as a default way of producing the
analysis data. Nevertheless, the current state-of-
art in computer science allows many annotation
tasks to be semi- or fully automated. When con-
sidering techniques to automate semiotic re-
search, one must keep in mind that the proposed
software is designed for use by social semioti-
cians, who study media in many different ways.
Therefore, when considering automation tech-
niques we assume that the technique must be
general enough to be useful for broad class of
applications. For example, one can consider
automatic shot boundary-detection, automatic
</bodyText>
<page confidence="0.997486">
136
</page>
<bodyText confidence="0.99996075">
face recognition and more general object track-
ing as technologies, which may be employed for
automation. Current state-of-the-art algorithms in
automatic speech recognition may also be con-
sidered since many important multimedia genres,
like news or studio TV broadcasts, public presen-
tations, and similar, contain clearly spoken
speech which is realistic to recognize.
</bodyText>
<subsectionHeader confidence="0.997694">
2.7 Templates and collaboration
</subsectionHeader>
<bodyText confidence="0.999964181818182">
Different analytical domains – e.g. speech and
gesture - and theoretical perspectives require dif-
ferent organization of the annotation document
and the use of different semiotic systems. The
proposed system allows organizing of pre-
defined templates for different analytical pur-
poses. The template is realized as a standalone
file where the required structures are defined.
The user is provided with a wizard dialog to se-
lect the template, which is then imported into the
current annotation document.
</bodyText>
<subsectionHeader confidence="0.927669">
2.8 Limitations
</subsectionHeader>
<bodyText confidence="0.999973916666667">
The scope of proposed functionalities is broad
and, therefore, complete coverage is challenging.
A main limitation for the proposed software is
the ability to automate the analysis. It is very un-
realistic to consider that human analyst can be
removed from the process of annotation, since
the domain itself, which is social semiotics, is
fundamentally human-oriented. Therefore, we
consider that semiotic analysis will remain man-
ual to a large extent, and the automation will be
employed mostly for routine and technically im-
plementable tasks.
</bodyText>
<sectionHeader confidence="0.999653" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.99957593939394">
In this paper we present a collaborative work
between computer and social scientists develop-
ing a software platform facilitating research in
social semiotics. The proposed software is used
for annotation and analysis of multimodal data
supporting different media types simultaneously
(video, sound, images, text and real-value data)
and it provides intuitive GUI for entering the
analysis. It supports innovative categorized (sys-
temic) analysis and provides the library of prede-
fined analytical systems developed in the litera-
ture together with the ability to create and cus-
tomize new systems. In addition, the software
provides the interface for graphical analysis of
imagery data and provides functions for data
visualization.
The project contributes to social semiotics by
providing a specialized domain-oriented software
tool which significantly increases the productiv-
ity of the analyst resulting in more extensive
studies and better-grounded theories. The pro-
posed visualization interface makes it easier to
see data patterns, outliers and recognize points of
interest. The ability to customize descriptive sys-
tems facilitates experimentation and promotes
the development of theoretical frameworks in
social semiotics. Unified frameworks for annota-
tion of different media types encourage cross-
domain analysis and integration of research from
different fields. Though the project is still at the
development stage, the obtained results and
feedback from practicing social semioticians are
promising.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.917078">
This work was supported by the Singapore Na-
tional Research Foundation (NRF) Interactive
Digital Media R&amp;D Program, under research
Grant NRF2007IDM-IDM002-066 awarded by
the Media Development Authority (MDA) of
Singapore.
</bodyText>
<sectionHeader confidence="0.99228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998534928571429">
A. Baldry, P. J. Thibault, Multimodal Transcription
and Text Analysis, Equinox, London, 2006.
Tan, S. 2005. A Systemic Functional Approach to the
Analysis of Corporate Television Advertisements.
MA Thesis, Department of English Language and
Literature, National University of Singapore.
P. J. Thibault, “The Multimodal Transcription of a
Television Advertisement: Theory and Practice”, in
Anthony Baldry (ed.) Multimodality and Multime-
diality in the Distance Learning Age, pp. 311-385,
Palladino Editore, Campobasso, Italy, 2000.
K. L. O&apos;Halloran, “Multimodal Analysis and Digital
Technology”, In A. Baldry and E. Montagna (eds.),
Interdisciplinary Perspectives on Multimodality:
Theory and Practice, Proceedings of the Third In-
ternational Conference on Multimodality, Pallad-
ino, Campobasso, 2009.
Halliday, M. A. K. (1967). Intonation and Grammar
in British English. The Hague; Paris: Mouton.
K. L. O&apos;Halloran, “Systemics 1.0: Software for Re-
search and Teaching Systemic Functional Linguis-
tics”, RELC Journal, vol. 34(2), pp. 157-178, 2003.
Bateman, J. (2008). Multimodality and Genre: A
Foundation for the Systematic Analysis of Multi-
modal Documents. Hampshire: Palgrave Macmil-
lan.
Judd, K. &amp; O&apos;Halloran, K. L. (2002) Systemics 1.0.
Singapore: Singapore University Press.
</reference>
<page confidence="0.997862">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.873795">
<title confidence="0.998262">Developing Novel Multimodal and Linguistic Annotation Software</title>
<author confidence="0.988702">A Podlasov</author>
<author confidence="0.988702">K O’Halloran</author>
<author confidence="0.988702">S Tan</author>
<author confidence="0.988702">B Smith</author>
<author confidence="0.988702">Nagarajan</author>
<affiliation confidence="0.916387">Multimodal Analysis Lab, IDMI, National University of</affiliation>
<address confidence="0.963368">9 Prince George’s Park, Singapore</address>
<email confidence="0.99493">idmpa@nus.edu.sg</email>
<email confidence="0.99493">k.ohalloran@nus.edu.sg</email>
<email confidence="0.99493">sabinetan@nus.edu.sg</email>
<email confidence="0.99493">idmbas@nus.edu.sg</email>
<email confidence="0.99493">idmarun@nus.edu.sg</email>
<abstract confidence="0.9962853">In this paper we present a collaborative work between computer and social scientists resulting in the development of annotation software for conducting research and analysis in social semiotics in both multimodal and linguistic aspects. The paper describes the proposed software and discusses how this tool can contribute for development of social semiotic theory and practice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Baldry</author>
<author>P J Thibault</author>
</authors>
<title>Multimodal Transcription and Text Analysis,</title>
<date>2006</date>
<location>Equinox, London,</location>
<contexts>
<context position="1209" citStr="Baldry &amp; Thibault, 2006" startWordPosition="165" endWordPosition="168">s tool can contribute for development of social semiotic theory and practice. 1 Introduction Despite the advances that have been achieved in the conceptualization of multimodal theories for analysing language, images and other semiotic resources, many frameworks and models for the transcription and analysis of multimodal data continue to rely on ‘low-tech’, manual and computer-assisted, technologies. A construct preferred by many researchers for analyzing and representing the interplay of semiotic resources in multimodal data is the Table, as exemplified by a range of transcription templates (Baldry &amp; Thibault, 2006). Although tables can be effective for recording which specific semiotic modes and resources are co-deployed at a given moment in the text, they essentially remain bounded by the confines of the printable page. Besides being laborious to create, page-based analysis severely limits the researchers’ ability to effectively capture and portray the complex interplay of semiotic modes and resources as they unfold on a larger scale, especially in the case of dynamic video texts and interactive digital media sites (O’Halloran, 2009). In addition, the analysis will necessarily be restricted to manual t</context>
</contexts>
<marker>Baldry, Thibault, 2006</marker>
<rawString>A. Baldry, P. J. Thibault, Multimodal Transcription and Text Analysis, Equinox, London, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
</authors>
<title>A Systemic Functional Approach to the Analysis of Corporate Television Advertisements.</title>
<date>2005</date>
<tech>MA Thesis,</tech>
<institution>Department of English Language and Literature, National University of Singapore.</institution>
<marker>Tan, 2005</marker>
<rawString>Tan, S. 2005. A Systemic Functional Approach to the Analysis of Corporate Television Advertisements. MA Thesis, Department of English Language and Literature, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Thibault</author>
</authors>
<title>The Multimodal Transcription of a Television Advertisement: Theory and Practice”,</title>
<date>2000</date>
<booktitle>in Anthony Baldry (ed.) Multimodality and Multimediality in the Distance Learning Age,</booktitle>
<pages>311--385</pages>
<location>Palladino Editore, Campobasso, Italy,</location>
<contexts>
<context position="2025" citStr="Thibault, 2000" startWordPosition="290" endWordPosition="291">ble page. Besides being laborious to create, page-based analysis severely limits the researchers’ ability to effectively capture and portray the complex interplay of semiotic modes and resources as they unfold on a larger scale, especially in the case of dynamic video texts and interactive digital media sites (O’Halloran, 2009). In addition, the analysis will necessarily be restricted to manual transcription and annotation techniques, which often involve a complexity of symbolic notations and Figure 1. Systemics 1.0 interface. abbreviations which may be difficult to learn and comprehend (e.g. Thibault, 2000). We propose that the study of meaning-bearing phenomena requires a collaborative effort between social and computer scientists. Social semiotic study clearly can benefit from theoretical concepts and analytical approaches beyond those traditionally developed for typical social science data, and which draw upon the rich variety of computational techniques developed within computational science. This collaborative approach proved to be productive in development of the Systemics 1.0 (Judd &amp; O’Halloran, 2002) software for research and teaching Systemic Functional Linguistics (SFL) (O’Halloran, 20</context>
</contexts>
<marker>Thibault, 2000</marker>
<rawString>P. J. Thibault, “The Multimodal Transcription of a Television Advertisement: Theory and Practice”, in Anthony Baldry (ed.) Multimodality and Multimediality in the Distance Learning Age, pp. 311-385, Palladino Editore, Campobasso, Italy, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L O&apos;Halloran</author>
</authors>
<title>Multimodal Analysis and Digital Technology”,</title>
<date>2009</date>
<booktitle>Interdisciplinary Perspectives on Multimodality: Theory and Practice, Proceedings of the Third International Conference on Multimodality,</booktitle>
<editor>In A. Baldry and E. Montagna (eds.),</editor>
<location>Palladino, Campobasso,</location>
<marker>O&apos;Halloran, 2009</marker>
<rawString>K. L. O&apos;Halloran, “Multimodal Analysis and Digital Technology”, In A. Baldry and E. Montagna (eds.), Interdisciplinary Perspectives on Multimodality: Theory and Practice, Proceedings of the Third International Conference on Multimodality, Palladino, Campobasso, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Intonation and Grammar in British English. The Hague;</title>
<date>1967</date>
<location>Paris: Mouton.</location>
<contexts>
<context position="8018" citStr="Halliday 1967" startWordPosition="1217" endWordPosition="1218">rom a system of Figure 2. Interface of the proposed software. 135 Figure 4. Overlay Editor tool window. Figure 3. Systems Browser tool window and sample camera movement and tone analysis (superimposed). categories describing the phenomena semiotically or by other theoretical means. The Systems Browser tool window provides the interface to create and manipulate those systems, as well as use them for annotation. Figure 3 illustrates two simple categorical systems: describing camera motion – eg. stationary or mobile camera; the type of tracking (pan, tilt or zoom); and tone choices in speech (cf Halliday 1967). The application allows not only to define the semiotic system itself, but also to associate textual information with every choice, which makes it a helpful interactive glossary for the analyst. Besides that, it allows associating different style attributes of the particular choice: fill color, outline color and dash style, etc. As the user makes systemic choices, the system uses text and style attributes to make the selections visible. This becomes an important feature of the proposed software. The analysis becomes visible not only textually, but also graphically. In Figure 3 we superimposed</context>
</contexts>
<marker>Halliday, 1967</marker>
<rawString>Halliday, M. A. K. (1967). Intonation and Grammar in British English. The Hague; Paris: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L O&apos;Halloran</author>
</authors>
<title>Systemics 1.0: Software for Research and Teaching Systemic Functional Linguistics”,</title>
<date>2003</date>
<journal>RELC Journal,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>157--178</pages>
<marker>O&apos;Halloran, 2003</marker>
<rawString>K. L. O&apos;Halloran, “Systemics 1.0: Software for Research and Teaching Systemic Functional Linguistics”, RELC Journal, vol. 34(2), pp. 157-178, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bateman</author>
</authors>
<title>Multimodality and Genre: A Foundation for the Systematic Analysis of Multimodal Documents.</title>
<date>2008</date>
<publisher>Hampshire: Palgrave Macmillan.</publisher>
<contexts>
<context position="3561" citStr="Bateman, 2008" startWordPosition="516" endWordPosition="517">. The default grammar can be changed to suit user requirements, so that the software is used for teaching and research purposes. The success of the re134 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 134–137, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP search collaboration which resulted in Systemics 1.0 gives strong motivation to consider computer-based approaches for the more general case of multimodal studies which involves, along with linguistic analysis, analyses of various meaningbearing phenomena such as gaze, gesture, intonation etc (e.g. Bateman, 2008; O’Halloran 2009) In this paper we report on an ongoing interdisciplinary collaboration between computer scientists and social scientists taking place in Multimodal Analysis Lab, National University of Singapore in the frame of the Events in the World project. The project aims at developing an interactive platform for manual, computer-assisted and automatic annotation, analysis and representation of multimedia data for social semiotic and SFL research. The project involves a range of researchers working together to develop the software interface and operational functionalities. The respective</context>
</contexts>
<marker>Bateman, 2008</marker>
<rawString>Bateman, J. (2008). Multimodality and Genre: A Foundation for the Systematic Analysis of Multimodal Documents. Hampshire: Palgrave Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Judd</author>
<author>K L O&apos;Halloran</author>
</authors>
<date>2002</date>
<journal>Systemics</journal>
<volume>1</volume>
<publisher>Singapore University Press.</publisher>
<marker>Judd, O&apos;Halloran, 2002</marker>
<rawString>Judd, K. &amp; O&apos;Halloran, K. L. (2002) Systemics 1.0. Singapore: Singapore University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>