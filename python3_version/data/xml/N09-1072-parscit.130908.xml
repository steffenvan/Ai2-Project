<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9844665">
Extracting Social Meaning: Identifying Interactional Style in Spoken
Conversation
</title>
<author confidence="0.990938">
Dan Jurafsky Rajesh Ranganath Dan McFarland
</author>
<affiliation confidence="0.9970995">
Linguistics Department Computer Science Department School of Education
Stanford University Stanford University Stanford University
</affiliation>
<email confidence="0.998892">
jurafsky@stanford.edu rajeshr@cs.stanford.edu dmcfarla@stanford.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582666666667">
Automatically extracting social meaning and
intention from spoken dialogue is an impor-
tant task for dialogue systems and social com-
puting. We describe a system for detecting
elements of interactional style: whether a
speaker is awkward, friendly, or flirtatious.
We create and use a new spoken corpus of 991
4-minute speed-dates. Participants rated their
interlocutors for these elements of style. Us-
ing rich dialogue, lexical, and prosodic fea-
tures, we are able to detect flirtatious, awk-
ward, and friendly styles in noisy natural con-
versational data with up to 75% accuracy,
compared to a 50% baseline. We describe sim-
ple ways to extract relatively rich dialogue fea-
tures, and analyze which features performed
similarly for men and women and which were
gender-specific.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892512195122">
How can we extract social meaning from speech, de-
ciding if a speaker is particularly engaged in the con-
versation, is uncomfortable or awkward, or is partic-
ularly friendly and flirtatious? Understanding these
meanings and how they are signaled in language is
an important sociolinguistic task in itself. Extracting
them automatically from dialogue speech and text
is crucial for developing socially aware computing
systems for tasks such as detection of interactional
problems or matching conversational style, and will
play an important role in creating more natural dia-
logue agents (Pentland, 2005; Nass and Brave, 2005;
Brave et al., 2005).
Cues for social meaning permeate speech at every
level of linguistic structure. Acoustic cues such as
low and high F0 or energy and spectral tilt are impor-
tant in detecting emotions such as annoyance, anger,
sadness, or boredom (Ang et al., 2002; Lee and
Narayanan, 2002; Liscombe et al., 2003), speaker
characteristics such as charisma (Rosenberg and
Hirschberg, 2005), or personality features like extro-
version (Mairesse et al., 2007; Mairesse and Walker,
2008). Lexical cues to social meaning abound.
Speakers with links to depression or speakers who
are under stress use more first person singular pro-
nouns (Rude et al., 2004; Pennebaker and Lay, 2002;
Cohn et al., 2004), positive emotion words are cues
to agreeableness (Mairesse et al., 2007), and neg-
ative emotion words are useful cues to deceptive
speech (Newman et al., 2003). The number of words
in a sentence can be a useful feature for extroverted
personality (Mairesse et al., 2007). Finally, dia-
log features such as the presence of disfluencies
can inform listeners about speakers’ problems in ut-
terance planning or about confidence (Brennan and
Williams, 1995; Brennan and Schober, 2001).
Our goal is to see whether cues of this sort are
useful in detecting particular elements of conversa-
tional style and social intention; whether a speaker
in a speed-dating conversation is judged by the in-
terlocutor as friendly, awkward, or flirtatious.
</bodyText>
<sectionHeader confidence="0.997471" genericHeader="introduction">
2 The Corpus
</sectionHeader>
<bodyText confidence="0.995922666666667">
Our experiments make use of a new corpus we have
collected, the SpeedDate Corpus. The corpus is
based on three speed-dating sessions run at an elite
</bodyText>
<page confidence="0.96961">
638
</page>
<note confidence="0.891469">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638–646,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999713862745098">
private American university in 2005 and inspired
by prior speed-dating research (Madan et al., 2005;
Pentland, 2005). The graduate student participants
volunteered to be in the study and were promised
emails of persons with whom they reported mutual
liking. Each date was conducted in an open setting
where there was substantial background noise. All
participants wore audio recorders on a shoulder sash,
thus resulting in two audio recordings of the approx-
imately 1100 4-minute dates. In addition to the au-
dio, we collected pre-test surveys, event scorecards,
and post-test surveys. This is the largest sample we
know of where audio data and detailed survey infor-
mation were combined in a natural experiment.
The rich survey information included date per-
ceptions and follow-up interest, as well as gen-
eral attitudes, preferences, and demographic infor-
mation. Participants were also asked about the
conversational style and intention of the interlocu-
tor. Each speaker was asked to report how of-
ten their date’s speech reflected different conversa-
tional styles (awkward, friendly, flirtatious, funny,
assertive) on a scale of 1-10 (1=never, 10=con-
stantly): “How often did the other person behave in
the following ways on this ‘date’?”. We chose three
of these five to focus on in this paper.
We acquired acoustic information by taking the
acoustic wave file from each recorder and manually
segmenting it into a sequence of wavefiles, each cor-
responding to one 4-minute date. Since both speak-
ers wore microphones, most dates had two record-
ings, one from the male recorder and one from the
female recorder. Because of mechanical, opera-
tor, and experimenter errors, some recordings were
lost, and thus some dates had only one recording.
Transcribers at a professional transcription service
used the two recordings to create a transcript for
each date, and time-stamped the start and end time
of each speaker turn. Transcribers were instructed
to mark various disfluencies as well as some non-
verbal elements of the conversation such as laughter.
Because of noise, participants who accidentally
turned off their mikes, and some segmentation and
transcription errors, a number of dates were not pos-
sible to analyze. 19 dates were lost completely, and
for an additional 130 we lost one of the two audio
tracks and had to use the remaining track to extract
features for both interlocutors. The current study fo-
cuses on the 991 remaining clean dates for which
we had usable audio, transcripts, and survey infor-
mation.
</bodyText>
<sectionHeader confidence="0.986412" genericHeader="method">
3 The Experiments
</sectionHeader>
<bodyText confidence="0.999974857142857">
Our goal is to detect three of the style variables, in
particular awkward, friendly, or flirtatious speakers,
via a machine learning classifier. Recall that each
speaker in a date (each conversation side) was la-
beled by his or her interlocutor with a rating from
1-10 for awkward, friendly, or flirtatious behavior.
For the experiments, the 1-10 Likert scale ratings
were first mean-centered within each respondent so
that the average was 0. Then the top ten percent of
the respondent-centered meaned Likert ratings were
marked as positive for the trait, and the bottom ten
percent were marked as negative for a trait. Thus
each respondent labels the other speaker as either
positive, negative, or NA for each of the three traits.
We run our binary classification experiments to
predict this output variable.
For each speaker side of each 4-minute conversa-
tion, we extracted features from the wavefiles and
the transcript, as described in the next section. We
then trained six separate binary classifiers (for each
gender for the 3 tasks), as described in Section 5.
</bodyText>
<sectionHeader confidence="0.992959" genericHeader="method">
4 Feature Extraction
</sectionHeader>
<bodyText confidence="0.999904736842105">
In selecting features we drew on previous research
on the use of relatively simple surface features that
cue social meaning, described in the next sections.
Each date was represented by the two 4-minute
wavefiles, one from the recorder worn by each
speaker, and a single transcription. Because of the
very high level of noise, the speaker wearing the
recorder was much clearer on his/her own recording,
and so we extracted the acoustic features for each
speaker from their own microphone (except for the
130 dates for which we only had one audio file). All
lexical and discourse features were extracted from
the transcripts.
All features describe the speaker of the conversa-
tion side being labeled for style. The features for
a conversation side thus indicate whether a speaker
who talks a lot, laughs, is more disfluent, has higher
F0, etc., is more or less likely to be considered flir-
tatious, friendly, or awkward by the interlocutor. We
</bodyText>
<page confidence="0.99857">
639
</page>
<bodyText confidence="0.9996048">
also computed the same features for the alter inter-
locutor. Alter features thus indicate the conversa-
tional behavior of the speaker talking with an inter-
locutor they considered to be flirtatious, friendly, or
awkward.
</bodyText>
<subsectionHeader confidence="0.997573">
4.1 Prosodic Features
</subsectionHeader>
<bodyText confidence="0.999939923076923">
F0 and RMS amplitude features were extracted us-
ing Praat scripts (Boersma and Weenink, 2005).
Since the start and end of each turn were time-
marked by hand, each feature was easily extracted
over a turn, and then averages and standard devia-
tions were taken over the turns in an entire conversa-
tion side. Thus the feature F0 MIN for a conversation
side was computed by taking the F0 min of each turn
in that conversation side (not counting zero values of
F0), and then averaging these values over all turns in
the side. F0 MIN SD is the standard deviation across
turns of this same measure.
Note that we coded four measures of f0 varia-
tion, not knowing in advance which one was likely
to be the most useful: F0 MEAN SD is the deviation
across turns from the global F0 mean for the con-
versation side, measuring how variable the speakers
mean f0 is across turns. F0 SD is the standard devia-
tion within a turn for the f0 mean, and then averaged
over turns, hence measures how variable the speak-
ers f0 is within a turn. F0 SD SD measures how much
the within-turn f0 variance varies from turn to turn,
and hence is another measure of cross-turn f0 vari-
ation. PITCH RANGE SD measures how much the
speakers pitch range varies from turn to turn, and
hence is another measure of cross-turn f0 variation.
</bodyText>
<subsectionHeader confidence="0.997526">
4.2 Lexical Features
</subsectionHeader>
<bodyText confidence="0.999962230769231">
Lexical features have been widely explored in the
psychological and computational literature. For
these features we drew mainly on the LIWC lexicons
of Pennebaker et al. (2007), the standard for social
psychological analysis of lexical features. From the
large variety of lexical categories in LIWC we se-
lected ten that the previous work of Mairesse et al.
(2007) had found to be very significant in detect-
ing personality-related features. The 10 LIWC fea-
tures we used were Anger, Assent, Ingest, Insight,
Negemotion, Sexual, Swear, I, We, and You. We also
added two new lexical features, “past tense auxil-
iary”, a heuristic for automatically detecting narra-
</bodyText>
<figure confidence="0.875127481481482">
F0 MIN minimum (non-zero) F0 per turn, av-
eraged over turns
F0 MIN SD standard deviation from F0 min
F0 MAX maximum F0 per turn, averaged over
turns
F0 MAX SD standard deviation from F0 max
F0 MEAN mean F0 per turn, averaged over turns
F0 MEAN SD standard deviation (across turns) from
F0 mean
F0 SD standard deviation (within a turn)
from F0 mean, averaged over turns
F0 SD SD standard deviation from the f0 sd
PITCH RANGE f0 max - f0 min per turn, averaged
over turns
PITCH RANGE standard deviation from mean pitch
SD range
RMS MIN minimum amplitude per turn, aver-
aged over turns
RMS MIN SD standard deviation from RMS min
RMS MAX maximum amplitude per turn, aver-
aged over turns
RMS MAX SD standard deviation from RMS max
RMS MEAN mean amplitude per turn, averaged
over turns
RMS MEAN SD standard deviation from RMS mean
TURN DUR duration of turn in seconds, averaged
over turns
</figure>
<table confidence="0.805765">
TIME total time for a speaker for a conversa-
tion side, in seconds
RATE OF number of words in turn divided by
SPEECH duration of turn in seconds, averaged
over turns
</table>
<tableCaption confidence="0.987366666666667">
Table 1: Prosodic features for each conversation side,
extracted using Praat from the hand-segmented turns of
each side.
</tableCaption>
<bodyText confidence="0.999691">
tive or story-telling behavior, and Metadate, for dis-
cussion about the speed-date itself. The features are
summarized in Table 2.
</bodyText>
<subsectionHeader confidence="0.999723">
4.3 Dialogue Act and Adjacency Pair Features
</subsectionHeader>
<bodyText confidence="0.999992222222222">
A number of discourse features were extracted,
drawing from the conversation analysis, disfluency
and dialog act literature (Sacks et al., 1974; Juraf-
sky et al., 1998; Jurafsky, 2001). While discourse
features are clearly important for extracting social
meaning, previous work on social meaning has met
with less success in use of such features (with the
exception of the ‘critical segments’ work of (Enos
et al., 2007)), presumably because discourse fea-
</bodyText>
<page confidence="0.990735">
640
</page>
<table confidence="0.4000745">
TOTAL WORDS total number of words
PAST TENSE uses of past tense auxiliaries was, were, had
</table>
<construct confidence="0.969015636363636">
METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research
YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know)
WE lets, let’s, our, ours, ourselves, us, we, we’d, we’ll, we’re, we’ve
I I’d, I’ll, I’m, I’ve, me, mine, my, myself (not counting I mean)
ASSENT yeah, okay, cool, yes, awesome, absolutely, agree
SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*
INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder
ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit
NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,
SEXUAL love*, passion*, loves, virgin, sex, screw
INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish
</construct>
<tableCaption confidence="0.97158725">
Table 2: Lexical features. Each feature value is a total count of the words in that class for each conversation side;
asterisks indicate that suffixed forms were included (e.g., love, loves, loving). All except the first three are from LIWC
(Pennebaker et al., 2007) (modified slightly, for example by removing you know and I mean). The last five classes
include more words in addition to those shown.
</tableCaption>
<bodyText confidence="0.97669235">
tures are expensive to hand-label and hard to auto-
matically extract. We chose a suggestive discourse
features that we felt might still be automatically ex-
tracted.
Four particular dialog acts were chosen as shown
in Table 3. Backchannels (or continuers) and ap-
preciations (a continuer expressing positive affect)
were coded by hand-built regular expressions. The
regular expressions were based on analysis of the
backchannels and appreciations in the hand-labeled
Switchboard corpus of dialog acts (Jurafsky et al.,
1997). Questions were coded simply by the pres-
ence of question marks.
Finally, repair questions (also called NTRIs; next
turn repair indicators) are turns in which a speaker
signals lack of hearing or understanding (Schegloff
et al., 1977). To detect these, we used a simple
heuristic: the presence of ‘Excuse me’ or ‘Wait’, as
in the following example:
FEMALE: Okay. Are you excited about that?
</bodyText>
<sectionHeader confidence="0.468515" genericHeader="method">
MALE: Excuse me?
</sectionHeader>
<bodyText confidence="0.999944571428571">
A collaborative completion is a turn where a
speaker completes the utterance begun by the alter
(Lerner, 1991; Lerner, 1996). Our heuristic for iden-
tifying collaborative completions was to select sen-
tences for which the first word of the speaker was
extremely predictable from the last two words of the
previous speaker. We trained a word trigram model1
</bodyText>
<footnote confidence="0.5766455">
1interpolated, with Good Turing smoothing, trained on the
Treebank 3 Switchboard transcripts after stripping punctuation.
</footnote>
<bodyText confidence="0.715865636363636">
and used it to compute the probability p of the first
word of a speaker’s turn given the last two words
of the interlocutor’s turn. We arbitrarily chose the
threshold .01, labeling all turns for which p &gt; .01 as
collaborative completions and used the total number
of collaborative completions in a conversation side
as our variable. This simple heuristic was errorful,
but did tend to find completions beginning with and
or or (1 below) and wh-questions followed by an NP
or PP phrase that is grammatically coherent with the
end of the question (2 and 3):
</bodyText>
<listItem confidence="0.9999135">
(1) FEMALE: The driving range.
(1) MALE: And the tennis court, too.
(2) MALE: What year did you graduate?
(2) FEMALE: From high school?
(3) FEMALE: What department are you in?
(3) MALE: The business school.
</listItem>
<bodyText confidence="0.9996447">
We also marked aspects of the preference struc-
ture of language. A dispreferred action is one in
which a speaker avoids the face-threat to the inter-
locutor that would be caused by, e.g., refusing a
request or not answering a question, by using spe-
cific strategies such as the use of well, hesitations, or
restarts (Schegloff et al., 1977; Pomerantz, 1984).
Finally, we included the number of instances of
laughter for the side, as well as the total number of
turns a speaker took.
</bodyText>
<subsectionHeader confidence="0.97375">
4.4 Disfluency Features
</subsectionHeader>
<bodyText confidence="0.9995105">
A second group of discourse features relating to re-
pair, disfluency, and speaker overlap are summarized
</bodyText>
<page confidence="0.992463">
641
</page>
<table confidence="0.818455375">
BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.)
APPRECIATIONS number of appreciations in side (Wow, That’s true, Oh, great)
QUESTIONS number of questions in side
NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)
COMPLETION (an approximation to) utterances that were ‘collaborative completions’
DISPREFERRED (an approximation to) dispreferred responses, beginning with discourse marker well
LAUGH number of instances of laughter in side
TURNS total number of turns in side
</table>
<tableCaption confidence="0.967173">
Table 3: Dialog act/adjacency pair features.
</tableCaption>
<table confidence="0.591596">
in Table 4. Filled pauses (um, uh) were coded by
UH/UM total number of filled pauses (uh or
um) in conversation side
RESTART total number of disfluent restarts in
conversation side
OVERLAP number of turns in side which the two
speakers overlapped
</table>
<tableCaption confidence="0.965585">
Table 4: Disfluency features
</tableCaption>
<bodyText confidence="0.837085">
regular expressions (the transcribers had been in-
structed to transcribe all filled pauses). Restarts are
a type of repair in which speakers begin a phrase,
break off, and then restart the syntactic phrase. The
following example shows a restart; the speaker starts
a sentence Uh, I and then restarts, There’s a group...:
Uh, I–there’s a group of us that came in–
Overlaps are cases in which both speakers were
talking at the same time, and were marked by the
transcribers in the transcripts:
MALE: But-and also obviously–
FEMALE: It sounds bigger.
MALE: –people in the CS school are not
quite as social in general as other–
</bodyText>
<sectionHeader confidence="0.991518" genericHeader="method">
5 Classifier Training
</sectionHeader>
<bodyText confidence="0.999789">
Before performing the classification task, we prepro-
cessed the data in two ways. First, we standardized
all the variables to have zero mean and unit variance.
We did this to avoid imposing a prior on any of the
features based on their numerical values.2 Second,
</bodyText>
<footnote confidence="0.7937155">
2Consider a feature A with mean 100 and a feature B with
mean .1 where A and B are correlated with the output. Since
regularization favors small weights there is a bias to put weight
on feature A because intuitively the weight on feature B would
</footnote>
<bodyText confidence="0.9996086">
we removed features correlated greater than .7. One
goal of removing correlated features was to remove
as much colinearity as possible from the regression
so that the regression weights could be ranked for
their importance in the classification. In addition,
we hoped to improve classification because a large
number of features require more training examples
(Ng, 2004). For example for male flirt we removed
f0 range (highly correlated with f0 max), f0 min sd
(highly correlated with f0 min), and Swear (highly
correlated with Anger).
For each classification task due to the small
amounts of data we performed k-fold cross vali-
dation to learn and evaluate our models. We used
a variant of k-fold cross validation with five folds
where three folds are used for training, one fold is
used for validation, and one fold is used as a test set.
This test fold is not used in any training step. This
yields a datasplit of 60% for training, 20% for val-
idation, and 20% for testing, or 120 training exam-
ples, 40 validation examples, and 40 test examples.
To ensure that we were not learning something spe-
cific to our data split, we randomized our data order-
ing and repeated the k-fold cross validation variant
25 times.
We used regularized logistic regression for clas-
sification. Recall that in logistic regression we train
a vector of feature weights θ ∈ ][Rn so as to make
the following classification of some output variable
y for an input observation x:3
</bodyText>
<equation confidence="0.997747666666667">
1
p(y|x; θ) = (1)
1 + exp(−θTx)
</equation>
<bodyText confidence="0.951514">
In regularized logistic regression we find the
</bodyText>
<footnote confidence="0.982808666666667">
need to be 1000 times larger to carry the same effect. This ar-
gument holds similarly for the reduction to unit variance.
3Where n is the number of features plus 1 for the intercept.
</footnote>
<page confidence="0.99026">
642
</page>
<bodyText confidence="0.649854">
-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1
weights B which maximize the following optimiza-
tion problem:
</bodyText>
<equation confidence="0.94595">
j:
argmax log p(yi|xi; B) − α * R(B) (2)
0
</equation>
<bodyText confidence="0.983762285714286">
R(B) is a regularization term used to penalize
large weights. We chose R(B), the regularization
function, to be the L1 norm of B. That is, R(B) =
||B||1 = Eni=1 |Bi|.
In our case, given the training set Strain, test set
Stest, and validation set Sval, we trained the weights
B as follows:
</bodyText>
<equation confidence="0.572459">
accuracy(Bα, Sval) (3)
</equation>
<bodyText confidence="0.992702">
where for a given sparsity parameter α
</bodyText>
<equation confidence="0.978502666666667">
j:
Bα = argmax log p(yi|xi; B) − α * R(B) (4)
0 i
</equation>
<bodyText confidence="0.935344703703704">
We chose L1-regularization because the number of
training examples to learn well grows logarithmi-
cally with the number of input variables (Ng, 2004),
and to achieve a sparse activation of our features
to find only the most salient explanatory variables.
This choice of regularization was made to avoid the
problems that often plague supervised learning in
situations with large number of features but only a
small number of examples. The search space over
the sparsity parameter α is bounded around an ex-
pected sparsity to prevent overfitting.
Finally, to evaluate our model on the learned α
and Bα we used the features X of the test set Stest to
compute the predicted outputs Y using the logistic
regression model. Accuracy is simply computed as
the percent of correct predictions.
To avoid any data ordering bias, we calculated
a Bα for each randomized run. The output of the
runs was a vector of weights for each feature. We
kept any feature if the median of its weight vector
was nonzero.4 A sample boxplot for the highest
weighted ego features for predicting male flirt can
be found in Figure 1.
4We also performed a t-test to find salient feature values
significantly different than zero; the non-zero median method
turned out to be a more conservative measure in practice (intu-
itively, because Ll normed regression pushes weights to 0).
</bodyText>
<figure confidence="0.970348818181818">
question
f0 mean std
you
rate
intensity min
backchannel
appreciation
repair quest
intensity max
laugh
I
</figure>
<figureCaption confidence="0.9964438">
Figure 1: An illustrative boxplot for flirtation in men
showing the 10 most significant features and one not
significant (‘I’). Shown are median values (central red
line), first quartile, third quartile, outliers (red X’s) and
interquartile range (filled box).
</figureCaption>
<sectionHeader confidence="0.995856" genericHeader="evaluation">
6 Results
</sectionHeader>
<tableCaption confidence="0.9850125">
Results for the 6 binary classifiers are presented in
Table 5.
</tableCaption>
<table confidence="0.9901655">
Awk Flirt Friendly
M F M F M F
Speaker 63% 51% 67% 60% 72% 68%
+other 64% 64% 71% 60% 73% 75%
</table>
<tableCaption confidence="0.995622">
Table 5: Accuracy of binary classification of each con-
</tableCaption>
<bodyText confidence="0.924701380952381">
versation side, where chance is 50%. The first row uses
features only from the single speaker; the second adds all
the features from the interlocutor as well. These accu-
racy results were aggregated from 25 randomized runs of
5-fold cross validation.
The first row shows results using features ex-
tracted from the speaker being labeled. Here, all
conversational styles are easiest to detect in men.
The second row of table 5 shows the accuracy
when using features from both speakers. Not sur-
prisingly, adding information about the interlocutor
tends to improve classification, and especially for
women, suggesting that male speaking has greater
sway over perceptions of conversational style. We
discuss below the role of these features.
We first considered the features that helped clas-
sification when considering only the ego (i.e., the re-
sults in the first row of Table 5). Table 6 shows fea-
ture weights for the features (features were normed
so weights are comparable), and is summarized in
the following paragraphs:
</bodyText>
<listItem confidence="0.956658">
• Men who are labeled as friendly use you, col-
</listItem>
<equation confidence="0.487817666666667">
i
argmax
α
</equation>
<page confidence="0.995332">
643
</page>
<table confidence="0.973830847826087">
MALE FRIENDLY MALE FLIRT
backchannel -0.737 question 0.376
you 0.631 f0 mean sd 0.288
intensity min sd 0.552 you 0.214
f0 sd sd -0.446 rate 0.190
intensity min -0.445 intensity min -0.163
completion 0.337 backchannel -0.142
time -0.270 appreciation -0.136
Insight -0.249 repair question 0.128
f0 min -0.226 intensity max -0.121
intensity max -0.221 laugh 0.107
overlap 0.213 time -0.092
laugh 0.192 overlap -0.090
turn dur -0.059 f0 min 0.089
Sexual 0.059 Sexual 0.082
appreciation -0.054 Negemo 0.075
Anger -0.051 metadate -0.041
FEMALE FRIENDLY FEMALE FLIRT
intensity min sd 0.420 f0 max 0.475
intensity max sd -0.367 rate 0.346
completion 0.276 intensity min sd 0.269
repair question 0.255 f0 mean sd 0.21
appreciation 0.253 Swear 0.156
f0 max 0.233 question -0.153
Swear -0.194 Assent -0.127
wordcount 0.165 f0 min -0.111
restart 0.172 intensity max 0.092
uh 0.241 I 0.073
I 0.111 metadate -0.071
past -0.060 wordcount 0.065
laugh 0.048 laugh 0.054
Negemotion -0.021 restart 0.046
intensity min -0.02 overlap -0.036
Ingest -0.017 f0 sd sd -0.025
Assent 0.0087 Ingest -0.024
f0 max sd 0.0089
MALE AWK
restart 0.502 completion -0.141
f0 sd sd 0.371 intensity max -0.135
appreciation -0.354 f0 mean sd -0.091
turns -0.292 Ingest -0.079
uh 0.270 Anger 0.075
you -0.210 repair question -0.067
overlap -0.190 Insight -0.056
past -0.175 rate 0.049
intensity min sd -0.173
</table>
<tableCaption confidence="0.776575">
Table 6: Feature weights (median weights of the random-
</tableCaption>
<bodyText confidence="0.713370555555555">
ized runs) for the non-zero predictors for each classifier.
Since our accuracy for detecting awkwardness in women
based solely on ego features is so close to chance, we
didn’t analyze the awkwardness features for women here.
laborative completions, laugh, overlap, but don’t
backchannel or use appreciations. Their utterances
are shorter (in seconds and words) and they are qui-
eter and their (minimum) pitch is lower and some-
what less variable.
</bodyText>
<listItem confidence="0.974273041666667">
• Women labeled as friendly have more collab-
orative completions, repair questions, laughter, and
appreciations. They use more words overall, and use
I more often. They are more disfluent (both restarts
and uh) but less likely to swear. Prosodically their f0
is higher, and there seems to be some pattern involv-
ing quiet speech; more variation in intensity mini-
mum than intensity max.
• Men who are labeled as flirting ask more ques-
tions, including repair questions, and use you. They
don’t use backchannels or appreciations, or overlap
as much. They laugh more, and use more sexual and
negative emotional words. Prosodically they speak
faster, with higher and more variable pitch, but qui-
eter (lower intensity max).
• The strongest features for women who are la-
beled as flirting are prosodic; they speak faster and
louder with higher and more variable pitch. They
also use more words in general, swear more, don’t
ask questions or use Assent, use more I, laugh more,
and are somewhat more disfluent (restarts).
• Men who are labeled as awkward are more dis-
fluent, with increased restarts and filled pauses (uh
and um). They are also not ‘collaborative’ conversa-
</listItem>
<bodyText confidence="0.988007947368421">
tionalists; they don’t use appreciations, repair ques-
tions, collaborative completions, past-tense, or you,
take fewer turns overall, and don’t overlap. Prosod-
ically the awkward labels are hard to characterize;
there is both an increase in pitch variation (f0 sd sd)
and a decrease (f0 mean sd). They don’t seem to get
quite as loud (intensity max).
The previous analysis showed what features of the
ego help in classification. We next asked about fea-
tures of the alter, based on the results using both
ego and alter features in the second row of Table 5.
Here we are asking about the linguistic behaviors of
a speaker who describes the interlocutor as flirting,
friendly, or awkward.
While we don’t show these values in a table, we
offer here an overview of their tendencies. For
example for women who labeled their male in-
terlocutors as friendly, the women got much qui-
eter, used ‘well’ much more, laughed, asked more
</bodyText>
<page confidence="0.996794">
644
</page>
<bodyText confidence="0.999964684210526">
repair questions, used collaborative completions,
and backchanneled more. When a man labeled a
woman as friendly, he used an expanded intensity
range (quieter intensity min, louder intensity max).
laughed more, used more sexual terms, used less
negative emotional terms, and overlapped more.
When women labeled their male interlocutor as
flirting, the women used many more repair ques-
tions, laughed more, and got quieter (lower intensity
min). By contrast, when a man said his female inter-
locutor was flirting, he used more Insight and Anger
words, and raised his pitch.
When women labeled their male interlocutor as
awkward, the women asked a lot of questions, used
well, were disfluent (restarts), had a diminished
pitch range, and didn’t use I. In listening to some
of these conversations, it was clear that the conver-
sation lagged repeatedly, and the women used ques-
tions at these points to restart the conversations.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999993092307693">
The results presented here should be regarded with
some caution. The sample is not a random sample of
English speakers or American adults, and speed dat-
ing is not a natural context for expressing every con-
versational style. Therefore, a wider array of studies
across populations and genres would be required be-
fore a more general theory of conversational styles is
established.
On the other hand, the presented results may
under-reflect the relations being captured. The qual-
ity of recordings and coarse granularity (1 second)
of the time-stamps likely cloud the relations, and as
the data is cleaned and improved, we expect the as-
sociations to only grow stronger.
Caveats aside, we believe the evidence indicates
that the perception of several types of conversational
style have relatively clear signals across genders, but
with some additional gender contextualization.
Both genders convey flirtation by laughing more,
speaking faster, and using higher and more variable
pitch. Both genders convey friendliness by laughing
more, and using collaborative completions.
However, we do find gender differences; men asl
more questions when (labeled as) flirting, women
ask fewer. Men labeled as flirting are softer, but
women labeled as flirting are louder. Women flirt-
ing swear more, while men are more likely to use
sexual vocabulary. Gender differences exist as well
for the other variables. Men labeled as friendly use
you while women labeled as friendly use I. Friendly
women are very disfluent; friendly men are not.
While the features for friendly and flirtatious
speech overlap, there are clear differences. Men
speaker faster and with higher f0 (min) in flirtatious
speech, but not faster and with lower f0 (min) in
friendly speech. For men, flirtatious speech involves
more questions and repair questions, while friendly
speech does not. For women, friendly speech is
more disfluent than flirtatious speech, and has more
collaborative style (completions, repair questions,
appreciations).
We also seem to see a model of collaborative con-
versational style (probably related to the collabo-
rative floor of Edelsky (1981) and Coates (1996)),
cued by the use of more collaborative completions,
repair questions and other questions, you, and laugh-
ter. These collaborative techniques were used by
both women and men who were labeled as friendly,
and occurred less with men labeled as awkward.
Women themselves displayed more of this collab-
orative conversational style when they labeled the
men as friendly. For women only, collaborative style
included appreciations; while for men only, collabo-
rative style included overlaps.
In addition to these implications for social sci-
ence, our work has implications for the extraction of
meaning in general. A key focus of our work was on
ways to extract useful dialog act and disfluency fea-
tures (repair questions, backchannels, appreciations,
restarts, dispreferreds) with very shallow methods.
These features were indeed extractable and proved
to be useful features in classification.
We are currently extending these results to predict
date outcomes including ‘liking’, extending work
such as Madan and Pentland (2006).
</bodyText>
<sectionHeader confidence="0.998867" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.92471525">
Thanks to three anonymous reviewers, Sonal Nalkur and
Tanzeem Choudhury for assistance and advice on data
collection, Sandy Pentland for a helpful discussion about
feature extraction, and to Google for gift funding.
</bodyText>
<page confidence="0.998687">
645
</page>
<sectionHeader confidence="0.996286" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883666666667">
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-Based Automatic Detection of
Annoyance and Frustration in Human-Computer Dia-
log. In INTERSPEECH-02.
P. Boersma and D. Weenink. 2005. Praat: doing pho-
netics by computer (version 4.3.14). [Computer pro-
gram]. Retrieved May 26, 2005, from http://www.
praat.org/.
S. Brave, C. Nass, and K. Hutchinson. 2005. Comput-
ers that care: Investigating the effects of orientation
of emotion exhibited by an embodied conversational
agent. International Journal of Human-Computer
Studies, 62(2):161–178.
S. E. Brennan and M. F. Schober. 2001. How listen-
ers compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44:274–296.
S. E. Brennan and M. Williams. 1995. The feeling of
another’s knowing: Prosody and filled pauses as cues
to listeners about the metacognitive states of speakers.
Journal of Memory and Language, 34:383–398.
J. Coates. 1996. Women Talk. Blackwell.
M. A. Cohn, M. R. Mehl, and J. W. Pennebaker. 2004.
Linguistic markers of psychological change surround-
ing September 11, 2001. Psychological Science,
15:687–693.
C. Edelsky. 1981. Who’s got the floor? Language in
Society, 10:383–421.
F. Enos, E. Shriberg, M. Graciarena, J. Hirschberg, and
A. Stolcke. 2007. Detecting Deception Using Critical
Segments. In INTERSPEECH-07.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Labeling Project Coder’s Man-
ual, Draft 13. Technical Report 97-02, University of
Colorado Institute of Cognitive Science.
D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. 1998. Lex-
ical, prosodic, and syntactic cues for dialog acts. In
Proceedings, COLING-ACL Workshop on Discourse
Relations and Discourse Markers, pages 114–120.
D. Jurafsky. 2001. Pragmatics and computational lin-
guistics. In L. R. Horn and G. Ward, editors, Hand-
book of Pragmatics. Blackwell.
C. M. Lee and S. S. Narayanan. 2002. Combining acous-
tic and language information for emotion recognition.
In ICSLP-02, pages 873–876, Denver, CO.
G. H. Lerner. 1991. On the syntax of sentences-in-
progress. Language in Society, 20(3):441–458.
G. H. Lerner. 1996. On the “semi-permeable” character
of grammatical units in conversation: Conditional en-
try into the turn space of another speaker. In E. Ochs,
E. A. Schegloff, and S. A. Thompson, editors, Interac-
tion and Grammar, pages 238–276. Cambridge Uni-
versity Press.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003. Clas-
sifying Subject Ratings of Emotional Speech Using
Acoustic Features. In INTERSPEECH-03.
A. Madan and A. Pentland. 2006. Vibefones: Socially
aware mobile phones. In Tenth IEEE International
Symposium on Wearable Computers.
A. Madan, R. Caneel, and A. Pentland. 2005. Voices
of attraction. Presented at Augmented Cognition, HCI
2005, Las Vegas.
F. Mairesse and M. Walker. 2008. Trainable generation
of big-five personality styles through data-driven pa-
rameter estimation. In ACL-08, Columbus.
F. Mairesse, M. Walker, M. Mehl, and R. Moore. 2007.
Using linguistic cues for the automatic recognition of
personality in conversation and text. Journal of Artifi-
cial Intelligence Research (JAIR), 30:457–500.
C. Nass and S. Brave. 2005. Wired for speech: How
voice activates and advances the human-computer re-
lationship. MIT Press, Cambridge, MA.
M. L. Newman, J. W. Pennebaker, D. S. Berry, and J. M.
Richards. 2003. Lying words: Predicting deception
from linguistic style. Personality and Social Psychol-
ogy Bulletin, 29:665–675.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regulariza-
tion, and rotational invariance. In ICML 2004.
J. W. Pennebaker and T. C. Lay. 2002. Language use and
personality during crises: Analyses of Mayor Rudolph
Giuliani’s press conferences. Journal of Research in
Personality, 36:271–282.
J. W. Pennebaker, R. Booth, and M. Francis. 2007. Lin-
guistic inquiry and word count: LIWC2007 operator’s
manual. Technical report, University of Texas.
A. Pentland. 2005. Socially aware computation and
communication. Computer, pages 63–70.
A. M. Pomerantz. 1984. Agreeing and disagreeing with
assessment: Some features of preferred/dispreferred
turn shapes. In J. M. Atkinson and J. Heritage, edi-
tors, Structure of Social Action: Studies in Conversa-
tion Analysis. Cambridge University Press.
A. Rosenberg and J. Hirschberg. 2005. Acous-
tic/prosodic and lexical correlates of charismatic
speech. In EUROSPEECH-05, pages 513–516, Lis-
bon, Portugal.
S. S. Rude, E. M. Gortner, and J. W. Pennebaker. 2004.
Language use of depressed and depression-vulnerable
college students. Cognition and Emotion, 18:1121–
1133.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696–735.
E. A. Schegloff, G. Jefferson, and H. Sacks. 1977. The
preference for self-correction in the organization of re-
pair in conversation. Language, 53:361–382.
</reference>
<page confidence="0.998867">
646
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883587">
<title confidence="0.9997545">Extracting Social Meaning: Identifying Interactional Style in Spoken Conversation</title>
<author confidence="0.999935">Dan Jurafsky Rajesh Ranganath Dan McFarland</author>
<affiliation confidence="0.9994135">Linguistics Department Computer Science Department School of Education Stanford University Stanford University Stanford University</affiliation>
<email confidence="0.999214">jurafsky@stanford.edurajeshr@cs.stanford.edudmcfarla@stanford.edu</email>
<abstract confidence="0.993916157894737">Automatically extracting social meaning and intention from spoken dialogue is an important task for dialogue systems and social computing. We describe a system for detecting of whether a is or We create and use a new spoken corpus of 991 4-minute speed-dates. Participants rated their interlocutors for these elements of style. Using rich dialogue, lexical, and prosodic features, we are able to detect flirtatious, awkward, and friendly styles in noisy natural conversational data with up to 75% accuracy, compared to a 50% baseline. We describe simple ways to extract relatively rich dialogue features, and analyze which features performed similarly for men and women and which were gender-specific.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-Based Automatic Detection of Annoyance and Frustration in Human-Computer Dialog.</title>
<date>2002</date>
<booktitle>In INTERSPEECH-02.</booktitle>
<contexts>
<context position="2009" citStr="Ang et al., 2002" startWordPosition="294" endWordPosition="297">ociolinguistic task in itself. Extracting them automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The numb</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stolcke. 2002. Prosody-Based Automatic Detection of Annoyance and Frustration in Human-Computer Dialog. In INTERSPEECH-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
<author>D Weenink</author>
</authors>
<title>Praat: doing phonetics by computer (version 4.3.14). [Computer program]. Retrieved</title>
<date>2005</date>
<note>from http://www. praat.org/.</note>
<contexts>
<context position="8420" citStr="Boersma and Weenink, 2005" startWordPosition="1327" endWordPosition="1330">speaker of the conversation side being labeled for style. The features for a conversation side thus indicate whether a speaker who talks a lot, laughs, is more disfluent, has higher F0, etc., is more or less likely to be considered flirtatious, friendly, or awkward by the interlocutor. We 639 also computed the same features for the alter interlocutor. Alter features thus indicate the conversational behavior of the speaker talking with an interlocutor they considered to be flirtatious, friendly, or awkward. 4.1 Prosodic Features F0 and RMS amplitude features were extracted using Praat scripts (Boersma and Weenink, 2005). Since the start and end of each turn were timemarked by hand, each feature was easily extracted over a turn, and then averages and standard deviations were taken over the turns in an entire conversation side. Thus the feature F0 MIN for a conversation side was computed by taking the F0 min of each turn in that conversation side (not counting zero values of F0), and then averaging these values over all turns in the side. F0 MIN SD is the standard deviation across turns of this same measure. Note that we coded four measures of f0 variation, not knowing in advance which one was likely to be the</context>
</contexts>
<marker>Boersma, Weenink, 2005</marker>
<rawString>P. Boersma and D. Weenink. 2005. Praat: doing phonetics by computer (version 4.3.14). [Computer program]. Retrieved May 26, 2005, from http://www. praat.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brave</author>
<author>C Nass</author>
<author>K Hutchinson</author>
</authors>
<title>Computers that care: Investigating the effects of orientation of emotion exhibited by an embodied conversational agent.</title>
<date>2005</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>62</volume>
<issue>2</issue>
<contexts>
<context position="1762" citStr="Brave et al., 2005" startWordPosition="252" endWordPosition="255">ial meaning from speech, deciding if a speaker is particularly engaged in the conversation, is uncomfortable or awkward, or is particularly friendly and flirtatious? Understanding these meanings and how they are signaled in language is an important sociolinguistic task in itself. Extracting them automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singu</context>
</contexts>
<marker>Brave, Nass, Hutchinson, 2005</marker>
<rawString>S. Brave, C. Nass, and K. Hutchinson. 2005. Computers that care: Investigating the effects of orientation of emotion exhibited by an embodied conversational agent. International Journal of Human-Computer Studies, 62(2):161–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>M F Schober</author>
</authors>
<title>How listeners compensate for disfluencies in spontaneous speech.</title>
<date>2001</date>
<journal>Journal of Memory and Language,</journal>
<pages>44--274</pages>
<contexts>
<context position="2917" citStr="Brennan and Schober, 2001" startWordPosition="437" endWordPosition="440">depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utterance planning or about confidence (Brennan and Williams, 1995; Brennan and Schober, 2001). Our goal is to see whether cues of this sort are useful in detecting particular elements of conversational style and social intention; whether a speaker in a speed-dating conversation is judged by the interlocutor as friendly, awkward, or flirtatious. 2 The Corpus Our experiments make use of a new corpus we have collected, the SpeedDate Corpus. The corpus is based on three speed-dating sessions run at an elite 638 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638–646, Boulder, Colorado, June 2009. c�2009 Association for Computational </context>
</contexts>
<marker>Brennan, Schober, 2001</marker>
<rawString>S. E. Brennan and M. F. Schober. 2001. How listeners compensate for disfluencies in spontaneous speech. Journal of Memory and Language, 44:274–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>M Williams</author>
</authors>
<title>The feeling of another’s knowing: Prosody and filled pauses as cues to listeners about the metacognitive states of speakers.</title>
<date>1995</date>
<journal>Journal of Memory</journal>
<publisher>Women Talk. Blackwell.</publisher>
<contexts>
<context position="2889" citStr="Brennan and Williams, 1995" startWordPosition="433" endWordPosition="436">und. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utterance planning or about confidence (Brennan and Williams, 1995; Brennan and Schober, 2001). Our goal is to see whether cues of this sort are useful in detecting particular elements of conversational style and social intention; whether a speaker in a speed-dating conversation is judged by the interlocutor as friendly, awkward, or flirtatious. 2 The Corpus Our experiments make use of a new corpus we have collected, the SpeedDate Corpus. The corpus is based on three speed-dating sessions run at an elite 638 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638–646, Boulder, Colorado, June 2009. c�2009 As</context>
</contexts>
<marker>Brennan, Williams, 1995</marker>
<rawString>S. E. Brennan and M. Williams. 1995. The feeling of another’s knowing: Prosody and filled pauses as cues to listeners about the metacognitive states of speakers. Journal of Memory and Language, 34:383–398. J. Coates. 1996. Women Talk. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Cohn</author>
<author>M R Mehl</author>
<author>J W Pennebaker</author>
</authors>
<date>2004</date>
<booktitle>Linguistic markers of psychological change surrounding September 11,</booktitle>
<pages>15--687</pages>
<publisher>Psychological Science,</publisher>
<contexts>
<context position="2439" citStr="Cohn et al., 2004" startWordPosition="361" endWordPosition="364"> linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utterance planning or about confidence (Brennan and Williams, 1995; Brennan and Schober, 2001). Our goal is to see whether cues of this sort are useful in detecting particular elements of conversational style and soc</context>
</contexts>
<marker>Cohn, Mehl, Pennebaker, 2004</marker>
<rawString>M. A. Cohn, M. R. Mehl, and J. W. Pennebaker. 2004. Linguistic markers of psychological change surrounding September 11, 2001. Psychological Science, 15:687–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Edelsky</author>
</authors>
<title>Who’s got the floor? Language in Society,</title>
<date>1981</date>
<pages>10--383</pages>
<contexts>
<context position="30515" citStr="Edelsky (1981)" startWordPosition="4989" endWordPosition="4990"> are not. While the features for friendly and flirtatious speech overlap, there are clear differences. Men speaker faster and with higher f0 (min) in flirtatious speech, but not faster and with lower f0 (min) in friendly speech. For men, flirtatious speech involves more questions and repair questions, while friendly speech does not. For women, friendly speech is more disfluent than flirtatious speech, and has more collaborative style (completions, repair questions, appreciations). We also seem to see a model of collaborative conversational style (probably related to the collaborative floor of Edelsky (1981) and Coates (1996)), cued by the use of more collaborative completions, repair questions and other questions, you, and laughter. These collaborative techniques were used by both women and men who were labeled as friendly, and occurred less with men labeled as awkward. Women themselves displayed more of this collaborative conversational style when they labeled the men as friendly. For women only, collaborative style included appreciations; while for men only, collaborative style included overlaps. In addition to these implications for social science, our work has implications for the extraction</context>
</contexts>
<marker>Edelsky, 1981</marker>
<rawString>C. Edelsky. 1981. Who’s got the floor? Language in Society, 10:383–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Enos</author>
<author>E Shriberg</author>
<author>M Graciarena</author>
<author>J Hirschberg</author>
<author>A Stolcke</author>
</authors>
<title>Detecting Deception Using Critical Segments.</title>
<date>2007</date>
<booktitle>In INTERSPEECH-07.</booktitle>
<contexts>
<context position="12060" citStr="Enos et al., 2007" startWordPosition="1961" endWordPosition="1964">ted turns of each side. tive or story-telling behavior, and Metadate, for discussion about the speed-date itself. The features are summarized in Table 2. 4.3 Dialogue Act and Adjacency Pair Features A number of discourse features were extracted, drawing from the conversation analysis, disfluency and dialog act literature (Sacks et al., 1974; Jurafsky et al., 1998; Jurafsky, 2001). While discourse features are clearly important for extracting social meaning, previous work on social meaning has met with less success in use of such features (with the exception of the ‘critical segments’ work of (Enos et al., 2007)), presumably because discourse fea640 TOTAL WORDS total number of words PAST TENSE uses of past tense auxiliaries was, were, had METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know) WE lets, let’s, our, ours, ourselves, us, we, we’d, we’ll, we’re, we’ve I I’d, I’ll, I’m, I’ve, me, mine, my, myself (not counting I mean) ASSENT yeah, okay, cool, yes, awesome, absolutely, agree SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck* INSIGHT think*/thought, feel*/felt, find/found, underst</context>
</contexts>
<marker>Enos, Shriberg, Graciarena, Hirschberg, Stolcke, 2007</marker>
<rawString>F. Enos, E. Shriberg, M. Graciarena, J. Hirschberg, and A. Stolcke. 2007. Detecting Deception Using Critical Segments. In INTERSPEECH-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>D Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL Labeling Project Coder’s Manual, Draft 13.</title>
<date>1997</date>
<tech>Technical Report 97-02,</tech>
<institution>University of Colorado Institute of Cognitive Science.</institution>
<contexts>
<context position="13966" citStr="Jurafsky et al., 1997" startWordPosition="2247" endWordPosition="2250">y removing you know and I mean). The last five classes include more words in addition to those shown. tures are expensive to hand-label and hard to automatically extract. We chose a suggestive discourse features that we felt might still be automatically extracted. Four particular dialog acts were chosen as shown in Table 3. Backchannels (or continuers) and appreciations (a continuer expressing positive affect) were coded by hand-built regular expressions. The regular expressions were based on analysis of the backchannels and appreciations in the hand-labeled Switchboard corpus of dialog acts (Jurafsky et al., 1997). Questions were coded simply by the presence of question marks. Finally, repair questions (also called NTRIs; next turn repair indicators) are turns in which a speaker signals lack of hearing or understanding (Schegloff et al., 1977). To detect these, we used a simple heuristic: the presence of ‘Excuse me’ or ‘Wait’, as in the following example: FEMALE: Okay. Are you excited about that? MALE: Excuse me? A collaborative completion is a turn where a speaker completes the utterance begun by the alter (Lerner, 1991; Lerner, 1996). Our heuristic for identifying collaborative completions was to sel</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switchboard SWBD-DAMSL Labeling Project Coder’s Manual, Draft 13. Technical Report 97-02, University of Colorado Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>B Fox</author>
<author>T Curl</author>
</authors>
<title>Lexical, prosodic, and syntactic cues for dialog acts.</title>
<date>1998</date>
<booktitle>In Proceedings, COLING-ACL Workshop on Discourse Relations and Discourse Markers,</booktitle>
<pages>114--120</pages>
<contexts>
<context position="11807" citStr="Jurafsky et al., 1998" startWordPosition="1920" endWordPosition="1924">otal time for a speaker for a conversation side, in seconds RATE OF number of words in turn divided by SPEECH duration of turn in seconds, averaged over turns Table 1: Prosodic features for each conversation side, extracted using Praat from the hand-segmented turns of each side. tive or story-telling behavior, and Metadate, for discussion about the speed-date itself. The features are summarized in Table 2. 4.3 Dialogue Act and Adjacency Pair Features A number of discourse features were extracted, drawing from the conversation analysis, disfluency and dialog act literature (Sacks et al., 1974; Jurafsky et al., 1998; Jurafsky, 2001). While discourse features are clearly important for extracting social meaning, previous work on social meaning has met with less success in use of such features (with the exception of the ‘critical segments’ work of (Enos et al., 2007)), presumably because discourse fea640 TOTAL WORDS total number of words PAST TENSE uses of past tense auxiliaries was, were, had METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know) WE lets, let’s, our, ours, ourselves, us, we, we’d, we</context>
</contexts>
<marker>Jurafsky, Shriberg, Fox, Curl, 1998</marker>
<rawString>D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. 1998. Lexical, prosodic, and syntactic cues for dialog acts. In Proceedings, COLING-ACL Workshop on Discourse Relations and Discourse Markers, pages 114–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>Pragmatics and computational linguistics.</title>
<date>2001</date>
<booktitle>Handbook of Pragmatics.</booktitle>
<editor>In L. R. Horn and G. Ward, editors,</editor>
<publisher>Blackwell.</publisher>
<contexts>
<context position="11824" citStr="Jurafsky, 2001" startWordPosition="1925" endWordPosition="1926"> for a conversation side, in seconds RATE OF number of words in turn divided by SPEECH duration of turn in seconds, averaged over turns Table 1: Prosodic features for each conversation side, extracted using Praat from the hand-segmented turns of each side. tive or story-telling behavior, and Metadate, for discussion about the speed-date itself. The features are summarized in Table 2. 4.3 Dialogue Act and Adjacency Pair Features A number of discourse features were extracted, drawing from the conversation analysis, disfluency and dialog act literature (Sacks et al., 1974; Jurafsky et al., 1998; Jurafsky, 2001). While discourse features are clearly important for extracting social meaning, previous work on social meaning has met with less success in use of such features (with the exception of the ‘critical segments’ work of (Enos et al., 2007)), presumably because discourse fea640 TOTAL WORDS total number of words PAST TENSE uses of past tense auxiliaries was, were, had METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know) WE lets, let’s, our, ours, ourselves, us, we, we’d, we’ll, we’re, we’ve</context>
</contexts>
<marker>Jurafsky, 2001</marker>
<rawString>D. Jurafsky. 2001. Pragmatics and computational linguistics. In L. R. Horn and G. Ward, editors, Handbook of Pragmatics. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S S Narayanan</author>
</authors>
<title>Combining acoustic and language information for emotion recognition. In</title>
<date>2002</date>
<booktitle>ICSLP-02,</booktitle>
<pages>873--876</pages>
<location>Denver, CO.</location>
<contexts>
<context position="2034" citStr="Lee and Narayanan, 2002" startWordPosition="298" endWordPosition="301">k in itself. Extracting them automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence</context>
</contexts>
<marker>Lee, Narayanan, 2002</marker>
<rawString>C. M. Lee and S. S. Narayanan. 2002. Combining acoustic and language information for emotion recognition. In ICSLP-02, pages 873–876, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Lerner</author>
</authors>
<title>On the syntax of sentences-inprogress.</title>
<date>1991</date>
<journal>Language in Society,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="14483" citStr="Lerner, 1991" startWordPosition="2334" endWordPosition="2335">and appreciations in the hand-labeled Switchboard corpus of dialog acts (Jurafsky et al., 1997). Questions were coded simply by the presence of question marks. Finally, repair questions (also called NTRIs; next turn repair indicators) are turns in which a speaker signals lack of hearing or understanding (Schegloff et al., 1977). To detect these, we used a simple heuristic: the presence of ‘Excuse me’ or ‘Wait’, as in the following example: FEMALE: Okay. Are you excited about that? MALE: Excuse me? A collaborative completion is a turn where a speaker completes the utterance begun by the alter (Lerner, 1991; Lerner, 1996). Our heuristic for identifying collaborative completions was to select sentences for which the first word of the speaker was extremely predictable from the last two words of the previous speaker. We trained a word trigram model1 1interpolated, with Good Turing smoothing, trained on the Treebank 3 Switchboard transcripts after stripping punctuation. and used it to compute the probability p of the first word of a speaker’s turn given the last two words of the interlocutor’s turn. We arbitrarily chose the threshold .01, labeling all turns for which p &gt; .01 as collaborative complet</context>
</contexts>
<marker>Lerner, 1991</marker>
<rawString>G. H. Lerner. 1991. On the syntax of sentences-inprogress. Language in Society, 20(3):441–458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Lerner</author>
</authors>
<title>On the “semi-permeable” character of grammatical units in conversation: Conditional entry into the turn space of another speaker.</title>
<date>1996</date>
<booktitle>Interaction and Grammar,</booktitle>
<pages>238--276</pages>
<editor>In E. Ochs, E. A. Schegloff, and S. A. Thompson, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14498" citStr="Lerner, 1996" startWordPosition="2336" endWordPosition="2337">ons in the hand-labeled Switchboard corpus of dialog acts (Jurafsky et al., 1997). Questions were coded simply by the presence of question marks. Finally, repair questions (also called NTRIs; next turn repair indicators) are turns in which a speaker signals lack of hearing or understanding (Schegloff et al., 1977). To detect these, we used a simple heuristic: the presence of ‘Excuse me’ or ‘Wait’, as in the following example: FEMALE: Okay. Are you excited about that? MALE: Excuse me? A collaborative completion is a turn where a speaker completes the utterance begun by the alter (Lerner, 1991; Lerner, 1996). Our heuristic for identifying collaborative completions was to select sentences for which the first word of the speaker was extremely predictable from the last two words of the previous speaker. We trained a word trigram model1 1interpolated, with Good Turing smoothing, trained on the Treebank 3 Switchboard transcripts after stripping punctuation. and used it to compute the probability p of the first word of a speaker’s turn given the last two words of the interlocutor’s turn. We arbitrarily chose the threshold .01, labeling all turns for which p &gt; .01 as collaborative completions and used t</context>
</contexts>
<marker>Lerner, 1996</marker>
<rawString>G. H. Lerner. 1996. On the “semi-permeable” character of grammatical units in conversation: Conditional entry into the turn space of another speaker. In E. Ochs, E. A. Schegloff, and S. A. Thompson, editors, Interaction and Grammar, pages 238–276. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liscombe</author>
<author>J Venditti</author>
<author>J Hirschberg</author>
</authors>
<title>Classifying Subject Ratings of Emotional Speech Using Acoustic Features.</title>
<date>2003</date>
<booktitle>In INTERSPEECH-03.</booktitle>
<contexts>
<context position="2058" citStr="Liscombe et al., 2003" startWordPosition="302" endWordPosition="305">hem automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature</context>
</contexts>
<marker>Liscombe, Venditti, Hirschberg, 2003</marker>
<rawString>J. Liscombe, J. Venditti, and J. Hirschberg. 2003. Classifying Subject Ratings of Emotional Speech Using Acoustic Features. In INTERSPEECH-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Madan</author>
<author>A Pentland</author>
</authors>
<title>Vibefones: Socially aware mobile phones.</title>
<date>2006</date>
<booktitle>In Tenth IEEE International Symposium on Wearable Computers.</booktitle>
<marker>Madan, Pentland, 2006</marker>
<rawString>A. Madan and A. Pentland. 2006. Vibefones: Socially aware mobile phones. In Tenth IEEE International Symposium on Wearable Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Madan</author>
<author>R Caneel</author>
<author>A Pentland</author>
</authors>
<title>Voices of attraction. Presented at Augmented Cognition,</title>
<date>2005</date>
<location>HCI</location>
<contexts>
<context position="3628" citStr="Madan et al., 2005" startWordPosition="549" endWordPosition="552"> conversational style and social intention; whether a speaker in a speed-dating conversation is judged by the interlocutor as friendly, awkward, or flirtatious. 2 The Corpus Our experiments make use of a new corpus we have collected, the SpeedDate Corpus. The corpus is based on three speed-dating sessions run at an elite 638 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638–646, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics private American university in 2005 and inspired by prior speed-dating research (Madan et al., 2005; Pentland, 2005). The graduate student participants volunteered to be in the study and were promised emails of persons with whom they reported mutual liking. Each date was conducted in an open setting where there was substantial background noise. All participants wore audio recorders on a shoulder sash, thus resulting in two audio recordings of the approximately 1100 4-minute dates. In addition to the audio, we collected pre-test surveys, event scorecards, and post-test surveys. This is the largest sample we know of where audio data and detailed survey information were combined in a natural e</context>
</contexts>
<marker>Madan, Caneel, Pentland, 2005</marker>
<rawString>A. Madan, R. Caneel, and A. Pentland. 2005. Voices of attraction. Presented at Augmented Cognition, HCI 2005, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mairesse</author>
<author>M Walker</author>
</authors>
<title>Trainable generation of big-five personality styles through data-driven parameter estimation.</title>
<date>2008</date>
<booktitle>In ACL-08,</booktitle>
<location>Columbus.</location>
<contexts>
<context position="2227" citStr="Mairesse and Walker, 2008" startWordPosition="325" endWordPosition="328">atching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utte</context>
</contexts>
<marker>Mairesse, Walker, 2008</marker>
<rawString>F. Mairesse and M. Walker. 2008. Trainable generation of big-five personality styles through data-driven parameter estimation. In ACL-08, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mairesse</author>
<author>M Walker</author>
<author>M Mehl</author>
<author>R Moore</author>
</authors>
<title>Using linguistic cues for the automatic recognition of personality in conversation and text.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>30--457</pages>
<contexts>
<context position="2199" citStr="Mairesse et al., 2007" startWordPosition="321" endWordPosition="324">ractional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners abou</context>
<context position="10005" citStr="Mairesse et al. (2007)" startWordPosition="1611" endWordPosition="1614">n f0 variance varies from turn to turn, and hence is another measure of cross-turn f0 variation. PITCH RANGE SD measures how much the speakers pitch range varies from turn to turn, and hence is another measure of cross-turn f0 variation. 4.2 Lexical Features Lexical features have been widely explored in the psychological and computational literature. For these features we drew mainly on the LIWC lexicons of Pennebaker et al. (2007), the standard for social psychological analysis of lexical features. From the large variety of lexical categories in LIWC we selected ten that the previous work of Mairesse et al. (2007) had found to be very significant in detecting personality-related features. The 10 LIWC features we used were Anger, Assent, Ingest, Insight, Negemotion, Sexual, Swear, I, We, and You. We also added two new lexical features, “past tense auxiliary”, a heuristic for automatically detecting narraF0 MIN minimum (non-zero) F0 per turn, averaged over turns F0 MIN SD standard deviation from F0 min F0 MAX maximum F0 per turn, averaged over turns F0 MAX SD standard deviation from F0 max F0 MEAN mean F0 per turn, averaged over turns F0 MEAN SD standard deviation (across turns) from F0 mean F0 SD standa</context>
</contexts>
<marker>Mairesse, Walker, Mehl, Moore, 2007</marker>
<rawString>F. Mairesse, M. Walker, M. Mehl, and R. Moore. 2007. Using linguistic cues for the automatic recognition of personality in conversation and text. Journal of Artificial Intelligence Research (JAIR), 30:457–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nass</author>
<author>S Brave</author>
</authors>
<title>Wired for speech: How voice activates and advances the human-computer relationship.</title>
<date>2005</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1741" citStr="Nass and Brave, 2005" startWordPosition="248" endWordPosition="251">How can we extract social meaning from speech, deciding if a speaker is particularly engaged in the conversation, is uncomfortable or awkward, or is particularly friendly and flirtatious? Understanding these meanings and how they are signaled in language is an important sociolinguistic task in itself. Extracting them automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use mo</context>
</contexts>
<marker>Nass, Brave, 2005</marker>
<rawString>C. Nass and S. Brave. 2005. Wired for speech: How voice activates and advances the human-computer relationship. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Newman</author>
<author>J W Pennebaker</author>
<author>D S Berry</author>
<author>J M Richards</author>
</authors>
<title>Lying words: Predicting deception from linguistic style. Personality and Social Psychology Bulletin,</title>
<date>2003</date>
<pages>29--665</pages>
<contexts>
<context position="2599" citStr="Newman et al., 2003" startWordPosition="387" endWordPosition="390">, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utterance planning or about confidence (Brennan and Williams, 1995; Brennan and Schober, 2001). Our goal is to see whether cues of this sort are useful in detecting particular elements of conversational style and social intention; whether a speaker in a speed-dating conversation is judged by the interlocutor as friendly, awkward, or flirtatious. 2 The Corpus Our experiments</context>
</contexts>
<marker>Newman, Pennebaker, Berry, Richards, 2003</marker>
<rawString>M. L. Newman, J. W. Pennebaker, D. S. Berry, and J. M. Richards. 2003. Lying words: Predicting deception from linguistic style. Personality and Social Psychology Bulletin, 29:665–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Y Ng</author>
</authors>
<title>Feature selection, L1 vs. L2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In ICML</booktitle>
<contexts>
<context position="18601" citStr="Ng, 2004" startWordPosition="3009" endWordPosition="3010">er a feature A with mean 100 and a feature B with mean .1 where A and B are correlated with the output. Since regularization favors small weights there is a bias to put weight on feature A because intuitively the weight on feature B would we removed features correlated greater than .7. One goal of removing correlated features was to remove as much colinearity as possible from the regression so that the regression weights could be ranked for their importance in the classification. In addition, we hoped to improve classification because a large number of features require more training examples (Ng, 2004). For example for male flirt we removed f0 range (highly correlated with f0 max), f0 min sd (highly correlated with f0 min), and Swear (highly correlated with Anger). For each classification task due to the small amounts of data we performed k-fold cross validation to learn and evaluate our models. We used a variant of k-fold cross validation with five folds where three folds are used for training, one fold is used for validation, and one fold is used as a test set. This test fold is not used in any training step. This yields a datasplit of 60% for training, 20% for validation, and 20% for tes</context>
<context position="20644" citStr="Ng, 2004" startWordPosition="3381" endWordPosition="3382">ization problem: j: argmax log p(yi|xi; B) − α * R(B) (2) 0 R(B) is a regularization term used to penalize large weights. We chose R(B), the regularization function, to be the L1 norm of B. That is, R(B) = ||B||1 = Eni=1 |Bi|. In our case, given the training set Strain, test set Stest, and validation set Sval, we trained the weights B as follows: accuracy(Bα, Sval) (3) where for a given sparsity parameter α j: Bα = argmax log p(yi|xi; B) − α * R(B) (4) 0 i We chose L1-regularization because the number of training examples to learn well grows logarithmically with the number of input variables (Ng, 2004), and to achieve a sparse activation of our features to find only the most salient explanatory variables. This choice of regularization was made to avoid the problems that often plague supervised learning in situations with large number of features but only a small number of examples. The search space over the sparsity parameter α is bounded around an expected sparsity to prevent overfitting. Finally, to evaluate our model on the learned α and Bα we used the features X of the test set Stest to compute the predicted outputs Y using the logistic regression model. Accuracy is simply computed as t</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>A. Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>T C Lay</author>
</authors>
<title>Language use and personality during crises: Analyses of Mayor Rudolph Giuliani’s press conferences.</title>
<date>2002</date>
<journal>Journal of Research in Personality,</journal>
<pages>36--271</pages>
<contexts>
<context position="2419" citStr="Pennebaker and Lay, 2002" startWordPosition="357" endWordPosition="360">e speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utterance planning or about confidence (Brennan and Williams, 1995; Brennan and Schober, 2001). Our goal is to see whether cues of this sort are useful in detecting particular elements of conversa</context>
</contexts>
<marker>Pennebaker, Lay, 2002</marker>
<rawString>J. W. Pennebaker and T. C. Lay. 2002. Language use and personality during crises: Analyses of Mayor Rudolph Giuliani’s press conferences. Journal of Research in Personality, 36:271–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>R Booth</author>
<author>M Francis</author>
</authors>
<title>Linguistic inquiry and word count: LIWC2007 operator’s manual.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>University of Texas.</institution>
<contexts>
<context position="9818" citStr="Pennebaker et al. (2007)" startWordPosition="1580" endWordPosition="1583">s the standard deviation within a turn for the f0 mean, and then averaged over turns, hence measures how variable the speakers f0 is within a turn. F0 SD SD measures how much the within-turn f0 variance varies from turn to turn, and hence is another measure of cross-turn f0 variation. PITCH RANGE SD measures how much the speakers pitch range varies from turn to turn, and hence is another measure of cross-turn f0 variation. 4.2 Lexical Features Lexical features have been widely explored in the psychological and computational literature. For these features we drew mainly on the LIWC lexicons of Pennebaker et al. (2007), the standard for social psychological analysis of lexical features. From the large variety of lexical categories in LIWC we selected ten that the previous work of Mairesse et al. (2007) had found to be very significant in detecting personality-related features. The 10 LIWC features we used were Anger, Assent, Ingest, Insight, Negemotion, Sexual, Swear, I, We, and You. We also added two new lexical features, “past tense auxiliary”, a heuristic for automatically detecting narraF0 MIN minimum (non-zero) F0 per turn, averaged over turns F0 MIN SD standard deviation from F0 min F0 MAX maximum F0 </context>
<context position="13310" citStr="Pennebaker et al., 2007" startWordPosition="2146" endWordPosition="2149">ine, wonder ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry, SEXUAL love*, passion*, loves, virgin, sex, screw INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish Table 2: Lexical features. Each feature value is a total count of the words in that class for each conversation side; asterisks indicate that suffixed forms were included (e.g., love, loves, loving). All except the first three are from LIWC (Pennebaker et al., 2007) (modified slightly, for example by removing you know and I mean). The last five classes include more words in addition to those shown. tures are expensive to hand-label and hard to automatically extract. We chose a suggestive discourse features that we felt might still be automatically extracted. Four particular dialog acts were chosen as shown in Table 3. Backchannels (or continuers) and appreciations (a continuer expressing positive affect) were coded by hand-built regular expressions. The regular expressions were based on analysis of the backchannels and appreciations in the hand-labeled S</context>
</contexts>
<marker>Pennebaker, Booth, Francis, 2007</marker>
<rawString>J. W. Pennebaker, R. Booth, and M. Francis. 2007. Linguistic inquiry and word count: LIWC2007 operator’s manual. Technical report, University of Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pentland</author>
</authors>
<title>Socially aware computation and communication.</title>
<date>2005</date>
<journal>Computer,</journal>
<pages>63--70</pages>
<contexts>
<context position="1719" citStr="Pentland, 2005" startWordPosition="246" endWordPosition="247"> 1 Introduction How can we extract social meaning from speech, deciding if a speaker is particularly engaged in the conversation, is uncomfortable or awkward, or is particularly friendly and flirtatious? Understanding these meanings and how they are signaled in language is an important sociolinguistic task in itself. Extracting them automatically from dialogue speech and text is crucial for developing socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who a</context>
<context position="3645" citStr="Pentland, 2005" startWordPosition="553" endWordPosition="554">e and social intention; whether a speaker in a speed-dating conversation is judged by the interlocutor as friendly, awkward, or flirtatious. 2 The Corpus Our experiments make use of a new corpus we have collected, the SpeedDate Corpus. The corpus is based on three speed-dating sessions run at an elite 638 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638–646, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics private American university in 2005 and inspired by prior speed-dating research (Madan et al., 2005; Pentland, 2005). The graduate student participants volunteered to be in the study and were promised emails of persons with whom they reported mutual liking. Each date was conducted in an open setting where there was substantial background noise. All participants wore audio recorders on a shoulder sash, thus resulting in two audio recordings of the approximately 1100 4-minute dates. In addition to the audio, we collected pre-test surveys, event scorecards, and post-test surveys. This is the largest sample we know of where audio data and detailed survey information were combined in a natural experiment. The ri</context>
</contexts>
<marker>Pentland, 2005</marker>
<rawString>A. Pentland. 2005. Socially aware computation and communication. Computer, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Pomerantz</author>
</authors>
<title>Agreeing and disagreeing with assessment: Some features of preferred/dispreferred turn shapes.</title>
<date>1984</date>
<booktitle>Structure of Social Action: Studies in Conversation Analysis.</booktitle>
<editor>In J. M. Atkinson and J. Heritage, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="15967" citStr="Pomerantz, 1984" startWordPosition="2583" endWordPosition="2584">cally coherent with the end of the question (2 and 3): (1) FEMALE: The driving range. (1) MALE: And the tennis court, too. (2) MALE: What year did you graduate? (2) FEMALE: From high school? (3) FEMALE: What department are you in? (3) MALE: The business school. We also marked aspects of the preference structure of language. A dispreferred action is one in which a speaker avoids the face-threat to the interlocutor that would be caused by, e.g., refusing a request or not answering a question, by using specific strategies such as the use of well, hesitations, or restarts (Schegloff et al., 1977; Pomerantz, 1984). Finally, we included the number of instances of laughter for the side, as well as the total number of turns a speaker took. 4.4 Disfluency Features A second group of discourse features relating to repair, disfluency, and speaker overlap are summarized 641 BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.) APPRECIATIONS number of appreciations in side (Wow, That’s true, Oh, great) QUESTIONS number of questions in side NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me) COMPLETION (an approximation to) utterances that were ‘collaborative c</context>
</contexts>
<marker>Pomerantz, 1984</marker>
<rawString>A. M. Pomerantz. 1984. Agreeing and disagreeing with assessment: Some features of preferred/dispreferred turn shapes. In J. M. Atkinson and J. Heritage, editors, Structure of Social Action: Studies in Conversation Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>Acoustic/prosodic and lexical correlates of charismatic speech.</title>
<date>2005</date>
<booktitle>In EUROSPEECH-05,</booktitle>
<pages>513--516</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2133" citStr="Rosenberg and Hirschberg, 2005" startWordPosition="311" endWordPosition="314">eloping socially aware computing systems for tasks such as detection of interactional problems or matching conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog featu</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2005</marker>
<rawString>A. Rosenberg and J. Hirschberg. 2005. Acoustic/prosodic and lexical correlates of charismatic speech. In EUROSPEECH-05, pages 513–516, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Rude</author>
<author>E M Gortner</author>
<author>J W Pennebaker</author>
</authors>
<title>Language use of depressed and depression-vulnerable college students. Cognition and Emotion,</title>
<date>2004</date>
<volume>18</volume>
<pages>1133</pages>
<contexts>
<context position="2393" citStr="Rude et al., 2004" startWordPosition="353" endWordPosition="356">ial meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utterance planning or about confidence (Brennan and Williams, 1995; Brennan and Schober, 2001). Our goal is to see whether cues of this sort are useful in detecting parti</context>
</contexts>
<marker>Rude, Gortner, Pennebaker, 2004</marker>
<rawString>S. S. Rude, E. M. Gortner, and J. W. Pennebaker. 2004. Language use of depressed and depression-vulnerable college students. Cognition and Emotion, 18:1121– 1133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
<author>E A Schegloff</author>
<author>G Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turntaking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="11784" citStr="Sacks et al., 1974" startWordPosition="1916" endWordPosition="1919">ed over turns TIME total time for a speaker for a conversation side, in seconds RATE OF number of words in turn divided by SPEECH duration of turn in seconds, averaged over turns Table 1: Prosodic features for each conversation side, extracted using Praat from the hand-segmented turns of each side. tive or story-telling behavior, and Metadate, for discussion about the speed-date itself. The features are summarized in Table 2. 4.3 Dialogue Act and Adjacency Pair Features A number of discourse features were extracted, drawing from the conversation analysis, disfluency and dialog act literature (Sacks et al., 1974; Jurafsky et al., 1998; Jurafsky, 2001). While discourse features are clearly important for extracting social meaning, previous work on social meaning has met with less success in use of such features (with the exception of the ‘critical segments’ work of (Enos et al., 2007)), presumably because discourse fea640 TOTAL WORDS total number of words PAST TENSE uses of past tense auxiliaries was, were, had METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know) WE lets, let’s, our, ours, ours</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>H. Sacks, E. A. Schegloff, and G. Jefferson. 1974. A simplest systematics for the organization of turntaking for conversation. Language, 50(4):696–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Schegloff</author>
<author>G Jefferson</author>
<author>H Sacks</author>
</authors>
<title>The preference for self-correction in the organization of repair in conversation.</title>
<date>1977</date>
<journal>Language,</journal>
<pages>53--361</pages>
<contexts>
<context position="14200" citStr="Schegloff et al., 1977" startWordPosition="2284" endWordPosition="2287">ill be automatically extracted. Four particular dialog acts were chosen as shown in Table 3. Backchannels (or continuers) and appreciations (a continuer expressing positive affect) were coded by hand-built regular expressions. The regular expressions were based on analysis of the backchannels and appreciations in the hand-labeled Switchboard corpus of dialog acts (Jurafsky et al., 1997). Questions were coded simply by the presence of question marks. Finally, repair questions (also called NTRIs; next turn repair indicators) are turns in which a speaker signals lack of hearing or understanding (Schegloff et al., 1977). To detect these, we used a simple heuristic: the presence of ‘Excuse me’ or ‘Wait’, as in the following example: FEMALE: Okay. Are you excited about that? MALE: Excuse me? A collaborative completion is a turn where a speaker completes the utterance begun by the alter (Lerner, 1991; Lerner, 1996). Our heuristic for identifying collaborative completions was to select sentences for which the first word of the speaker was extremely predictable from the last two words of the previous speaker. We trained a word trigram model1 1interpolated, with Good Turing smoothing, trained on the Treebank 3 Swi</context>
<context position="15949" citStr="Schegloff et al., 1977" startWordPosition="2579" endWordPosition="2582"> phrase that is grammatically coherent with the end of the question (2 and 3): (1) FEMALE: The driving range. (1) MALE: And the tennis court, too. (2) MALE: What year did you graduate? (2) FEMALE: From high school? (3) FEMALE: What department are you in? (3) MALE: The business school. We also marked aspects of the preference structure of language. A dispreferred action is one in which a speaker avoids the face-threat to the interlocutor that would be caused by, e.g., refusing a request or not answering a question, by using specific strategies such as the use of well, hesitations, or restarts (Schegloff et al., 1977; Pomerantz, 1984). Finally, we included the number of instances of laughter for the side, as well as the total number of turns a speaker took. 4.4 Disfluency Features A second group of discourse features relating to repair, disfluency, and speaker overlap are summarized 641 BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.) APPRECIATIONS number of appreciations in side (Wow, That’s true, Oh, great) QUESTIONS number of questions in side NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me) COMPLETION (an approximation to) utterances that wer</context>
</contexts>
<marker>Schegloff, Jefferson, Sacks, 1977</marker>
<rawString>E. A. Schegloff, G. Jefferson, and H. Sacks. 1977. The preference for self-correction in the organization of repair in conversation. Language, 53:361–382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>