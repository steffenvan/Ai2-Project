<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993549">
Experiments with Spectral Learning of Latent-Variable PCFGs
</title>
<author confidence="0.999075">
Shay B. Cohen&apos;, Karl Stratos&apos;, Michael Collins&apos;, Dean P. Foster2, and Lyle Ungar&apos;
</author>
<affiliation confidence="0.9926325">
&apos;Dept. of Computer Science, Columbia University
2Dept. of Statistics/&apos;Dept. of Computer and Information Science, University of Pennsylvania
</affiliation>
<email confidence="0.982846">
{scohen,stratos,mcollinsl@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
</email>
<sectionHeader confidence="0.997316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997640055555556">
Latent-variable PCFGs (L-PCFGs) are a
highly successful model for natural language
parsing. Recent work (Cohen et al., 2012)
has introduced a spectral algorithm for param-
eter estimation of L-PCFGs, which—unlike
the EM algorithm—is guaranteed to give con-
sistent parameter estimates (it has PAC-style
guarantees of sample complexity). This paper
describes experiments using the spectral algo-
rithm. We show that the algorithm provides
models with the same accuracy as EM, but is
an order of magnitude more efficient. We de-
scribe a number of key steps used to obtain
this level of performance; these should be rel-
evant to other work on the application of spec-
tral learning algorithms. We view our results
as strong empirical evidence for the viability
of spectral methods as an alternative to EM.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999772789473684">
Latent-variable PCFGS (L-PCFGs) are a highly suc-
cessful model for natural language parsing (Mat-
suzaki et al., 2005; Petrov et al., 2006). Recent
work (Cohen et al., 2012) has introduced a spectral
learning algorithm for L-PCFGs. A crucial prop-
erty of the algorithm is that it is guaranteed to pro-
vide consistent parameter estimates—in fact it has
PAC-style guarantees of sample complexity.1 This
is in contrast to the EM algorithm, the usual method
for parameter estimation in L-PCFGs, which has the
weaker guarantee of reaching a local maximum of
the likelihood function. The spectral algorithm is
relatively simple and efficient, relying on a singular
value decomposition of the training examples, fol-
lowed by a single pass over the data where parame-
ter values are calculated.
Cohen et al. (2012) describe the algorithm, and
the theory behind it, but as yet no experimental re-
sults have been reported for the method. This paper
</bodyText>
<footnote confidence="0.850214">
1under assumptions on certain singular values in the model;
see section 2.3.1.
</footnote>
<bodyText confidence="0.999647114285714">
describes experiments on natural language parsing
using the spectral algorithm for parameter estima-
tion. The algorithm provides models with slightly
higher accuracy than EM (88.05% F-measure on test
data for the spectral algorithm, vs 87.76% for EM),
but is an order of magnitude more efficient (9h52m
for training, compared to 187h12m, a speed-up of
19 times).
We describe a number of key steps in obtain-
ing this level of performance. A simple backed-off
smoothing method is used to estimate the large num-
ber of parameters in the model. The spectral algo-
rithm requires functions mapping inside and outside
trees to feature vectors—we make use of features
corresponding to single level rules, and larger tree
fragments composed of two or three levels of rules.
We show that it is important to scale features by their
inverse variance, in a manner that is closely related
to methods used in canonical correlation analysis.
Negative values can cause issues in spectral algo-
rithms, but we describe a solution to these problems.
In recent work there has been a series of results in
spectral learning algorithms for latent-variable mod-
els (Vempala and Wang, 2004; Hsu et al., 2009;
Bailly et al., 2010; Siddiqi et al., 2010; Parikh et
al., 2011; Balle et al., 2011; Arora et al., 2012;
Dhillon et al., 2012; Anandkumar et al., 2012). Most
of these results are theoretical (although see Luque
et al. (2012) for empirical results of spectral learn-
ing for dependency parsing). While the focus of
our experiments is on parsing, our findings should
be relevant to the application of spectral methods to
other latent-variable models. We view our results as
strong empirical evidence for the viability of spec-
tral methods as an alternative to EM.
</bodyText>
<sectionHeader confidence="0.996997" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999063333333333">
In this section we first give basic definitions for L-
PCFGs, and then describe the spectral learning algo-
rithm of Cohen et al. (2012).
</bodyText>
<page confidence="0.968973">
148
</page>
<note confidence="0.5207395">
Proceedings of NAACL-HLT 2013, pages 148–157,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.874034">
2.1 L-PCFGs: Basic Definitions
</subsectionHeader>
<bodyText confidence="0.943359">
We follow the definition in Cohen et al. (2012)
of L-PCFGs. An L-PCFG is an 8-tuple
(N, I, P, m, n, π, t, q) where:
</bodyText>
<listItem confidence="0.997528333333333">
• N is the set of non-terminal symbols in the
grammar. I ⊂ N is a finite set of in-terminals.
P ⊂ N is a finite set of pre-terminals. We as-
sume that N = I∪P, and I∩P = ∅. Hence we
have partitioned the set of non-terminals into
two subsets.
• [m] is the set of possible hidden states.2
• [n] is the set of possible words.
• For all a ∈ I, b, c ∈ N, h1, h2, h3 ∈
[m], we have a context-free rule a(h1) →
b(h2) c(h3). The rule has an associated pa-
rameter t(a → b c, h2, h3|a, h1).
• For all a ∈ P, h ∈ [m], x ∈ [n], we have a
context-free rule a(h) → x. The rule has an
associated parameter q(a → x|a, h).
• For all a ∈ I, h ∈ [m], π(a, h) is a parameter
specifying the probability of a(h) being at the
root of a tree.
</listItem>
<bodyText confidence="0.998138714285714">
A skeletal tree (s-tree) is a sequence of rules
r1 ... rN where each ri is either of the form a → b c
or a → x. The rule sequence forms a top-down, left-
most derivation under a CFG with skeletal rules.
A full tree consists of an s-tree r1 ... rN, together
with values h1 ... hN. Each hi is the value for
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
For a given skeletal tree r1 ... rN, define ai to be
the non-terminal on the left-hand-side of rule ri. For
any i ∈ [N] such that ri is of the form a → b c, de-
fine h(2) iand h(3) ias the hidden state value of the left
and right child respectively. The model then defines
a probability mass function (PMF) as
</bodyText>
<equation confidence="0.892518166666667">
p(r1 ... rN, h1 ... hN) =
fjπ(a1, h1) t(ri, h �2)i, h �3)  |ai, hi) fj q(ri|ai, hi)
i:aiEZ i:aiEP
The PMF over skeletal trees is p(r1 ... rN) =
Eh1...hN p(r1 ... rN, h1 ... hN).
2For any integer n, we use [n] to denote the set {1, 2,... n}.
</equation>
<bodyText confidence="0.999242333333333">
The parsing problem is to take a sentence as in-
put, and produce a skeletal tree as output. A stan-
dard method for parsing with L-PCFGs is as follows.
First, for a given input sentence x1 ... xn, for any
triple (a, i, j) such that a ∈ N and 1 ≤ i ≤ j ≤ n,
the marginal µ(a, i, j) is defined as
</bodyText>
<equation confidence="0.985249">
µ(a,i,j) = � p(t) (1)
t:(a,i,j)Et
</equation>
<bodyText confidence="0.925000875">
where the sum is over all skeletal trees t for
x1 ... xn that include non-terminal a spanning
words xi ... xj. A variant of the inside-outside
algorithm can be used to calculate marginals.
Once marginals have been computed, Good-
man’s algorithm (Goodman, 1996) is used to find
�
arg maxt (a,i,j)�t µ(a, i, j).3
</bodyText>
<subsectionHeader confidence="0.999771">
2.2 The Spectral Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999911625">
We now give a sketch of the spectral learning algo-
rithm. The training data for the algorithm is a set
of skeletal trees. The output from the algorithm is a
set of parameter estimates for t, q and π (more pre-
cisely, the estimates are estimates of linearly trans-
formed parameters; see Cohen et al. (2012) and sec-
tion 2.3.1 for more details).
The algorithm takes two inputs in addition to the
set of skeletal trees. The first is an integer m, speci-
fying the number of latent state values in the model.
Typically m is a relatively small number; in our ex-
periments we test values such as m = 8,16 or 32.
The second is a pair of functions φ and ψ, that re-
spectively map inside and outside trees to feature
vectors in Rd and Rd&amp;quot; where d and d&apos; are integers.
Each non-terminal in a skeletal tree has an associ-
ated inside and outside tree. The inside tree for a
node contains the entire subtree below that node; the
outside tree contains everything in the tree excluding
the inside tree. We will refer to the node above the
inside tree that has been removed as the “foot” of the
outside tree. See figure 1 for an example.
Section 3.1 gives definitions of φ(t) and ψ(o)
used in our experiments. The definitions of φ(t) and
</bodyText>
<footnote confidence="0.933192">
3In fact, in our implementation we calculate marginals
µ(a → b c, i, k, j) for a, b, c E N and 1 &lt; i &lt; k &lt; j, and
µ(a, i, i) for a E N, 1 &lt; i &lt; n, then apply the CKY algorithm
to find the parse tree that maximizes the sum of the marginals.
For simplicity of presentation we will refer to marginals of the
form µ(a, i, j) in the remainder of this paper.
</footnote>
<page confidence="0.997924">
149
</page>
<figure confidence="0.996538">
For each outside tree with a foot node labeled
a, define
S
V
VP
VP
NP
NP
N
D
the
cat
saw
</figure>
<figureCaption confidence="0.9967125">
Figure 1: The inside tree (shown left) and out-
side tree (shown right) for the non-terminal VP
in the parse tree [S [NP [D the ] [N cat]]
[VP [V saw] [NP [D the] [N dog]]]]
</figureCaption>
<bodyText confidence="0.999811">
ψ(o) are typically high-dimensional, sparse feature
vectors, similar to those in log-linear models. For
example φ might track the rule immediately below
the root of the inside tree, or larger tree fragments;
ψ might include similar features tracking rules or
larger rule fragments above the relevant node.
The spectral learning algorithm proceeds in two
steps. In step 1, we learn an m-dimensional rep-
resentation of inside and outside trees, using the
functions φ and ψ in combination with a projection
step defined through singular value decomposition
(SVD). In step 2, we derive parameter estimates di-
rectly from training examples.
</bodyText>
<subsectionHeader confidence="0.954454">
2.2.1 Step 1: An SVD-Based Projection
</subsectionHeader>
<bodyText confidence="0.999663428571429">
For a given non-terminal a ∈ N, each instance of
a in the training data has an associated outside tree,
and an associated inside tree. We define Oa to be
the set of pairs of inside/outside trees seen with a in
the training data: each member of Oa is a pair (o, t)
where o is an outside tree, and t is an inside tree.
Step 1 of the algorithm is then as follows:
</bodyText>
<equation confidence="0.90550175">
1. For each a ∈ N calculate f2a ∈ Rdxd&apos; as
1
Pli,j = |Oa |11 φi(t)ψ j(o)
(o,t)EOa
</equation>
<listItem confidence="0.998497625">
2. Perform an SVD on SZa. Define Ua ∈ Rdxm
(V a ∈ Rd&apos;xm) to be a matrix containing the
m left (right) singular vectors corresponding
to the m largest singular values; define Ea ∈
Rmxm to be the diagonal matrix with the m
largest singular values on its diagonal.
3. For each inside tree in the corpus with root la-
bel a, define
</listItem>
<equation confidence="0.999543">
Y (t) = (Ua)Tφ(t)
Z(o) = (Ea)−1(V a)Tψ(o)
</equation>
<bodyText confidence="0.996102333333333">
Note that Y (t) and Z(o) are both m-dimensional
vectors; thus we have used SVD to project inside
and outside trees to m-dimensional vectors.
</bodyText>
<subsectionHeader confidence="0.998853">
2.3 Step 2: Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9999598">
We now describe how the functions Y (t) and Z(o)
are used in estimating parameters of the model.
First, consider the t(a → b c, h2, h3|a, h1) parame-
ters. Each instance of a given rule a → b c in the
training corpus has an outside tree o associated with
the parent labeled a, and inside trees t2 and t3 as-
sociated with the children labeled b and c. For any
rule a → b c we define Qa,b c to be the set of triples
(o, t(2), t(3)) occurring with that rule in the corpus.
The parameter estimate is then
</bodyText>
<figure confidence="0.554722125">
a → b c k a i = count(a → b c) × Ea—.b c
c
( J, l , ) count(a) i,j,k
(2)
where
�(o,t(2),t(3)) Zi(o) × Yj(t(2)) × Yk(t(3))
EQa be
|Qa—.b c|
</figure>
<bodyText confidence="0.999947">
Here we use count(a → b c) and count(a) to refer
to the count of the rule a → b c and the non-terminal
a in the corpus. Note that once the SVD step has
been used to compute representations Y (t) and Z(o)
for each inside and outside tree in the corpus, calcu-
lating the parameter value c(a → b c, j, k|a, i) is a
very simple operation.
Similarly, for any rule a → x, define Qa,x to
be the set of outside trees seen with that rule in the
training corpus. The parameter estimate is then
</bodyText>
<equation confidence="0.998103333333333">
count(a → x) a—�
c(a → x|a,i) = × Ei (3)
count(a)
</equation>
<bodyText confidence="0.844628">
where Ea,x
</bodyText>
<equation confidence="0.798531">
i = EoEQa—x Zi(o)/|Qa,x|.
</equation>
<bodyText confidence="0.988411333333333">
A similar method is used for estimating parame-
ters c(a, i) that play the role of the π parameters (de-
tails omitted for brevity; see Cohen et al. (2012)).
</bodyText>
<subsubsectionHeader confidence="0.438102">
2.3.1 Guarantees for the Algorithm
</subsubsectionHeader>
<bodyText confidence="0.971929">
Once the c(a → b c, j, k|a, i), c(a → x|a, i) and
c(a, i) parameters have been estimated from the
</bodyText>
<figure confidence="0.9910985">
D N
the dog
a—.b c
Ei,j,k =
</figure>
<page confidence="0.981243">
150
</page>
<bodyText confidence="0.9998248">
training corpus, they can be used in place of the t,
q and π parameters in the inside-outside algorithm
for computing marginals (see Eq. 1). Call the re-
sulting marginals ˆµ(a, i, j). The guarantees for the
parameter estimation method are as follows:
</bodyText>
<listItem confidence="0.939132">
• Define Ωa = E[φ(T)(ψ(O))T|A = a] where
</listItem>
<bodyText confidence="0.867006444444444">
A, O, T are random variables corresponding to
the non-terminal label at a node, the outside
tree, and the inside tree (see Cohen et al. (2012)
for a precise definition). Note that ˆΩa, as de-
fined above, is an estimate of Ωa. Then if Ωa
has rank m, the marginals µˆ will converge to
the true values µ as the number of training ex-
amples goes to infinity, assuming that the train-
ing samples are i.i.d. samples from an L-PCFG.
</bodyText>
<listItem confidence="0.95855325">
• Define σ to be the m’th largest singular value
of Ωa. Then the number of samples required
for µˆ to be c-close to µ with probability at least
1 − δ is polynomial in 1/c, 1/δ, and 1/σ.
</listItem>
<bodyText confidence="0.987838111111111">
Under the first assumption, (Cohen et al.,
2012) show that the cˆ parameters converge to
values that are linear transforms of the orig-
inal parameters in the L-PCFG. For example,
define c(a —4 b c, j, k|a, i) to be the value that
ˆc(a —4 b c, j, k|a, i) converges to in the limit of infi-
nite data. Then there exist invertible matrices Ga C
Rm�m for all a C N such that for any a —4 b c, for
any h1, h2, h3 C Rm,
</bodyText>
<equation confidence="0.780782333333333">
t(a —4 b c, h2, h3|a, h1) =
E [Ga]i,h1[(Gb)−1]j,h,[(G&apos;)−1]k,h,,c(a —4 b c,j, k|a, i)
i,j,k
</equation>
<bodyText confidence="0.99981125">
The transforms defined by the Ga matrices are be-
nign, in that they cancel in the inside-outside algo-
rithm when marginals µ(a, i, j) are calculated. Sim-
ilar relationships hold for the π and q parameters.
</bodyText>
<sectionHeader confidence="0.890627" genericHeader="method">
3 Implementation of the Algorithm
</sectionHeader>
<bodyText confidence="0.999560166666667">
Cohen et al. (2012) introduced the spectral learning
algorithm, but did not perform experiments, leaving
several choices open in how the algorithm is imple-
mented in practice. This section describes a number
of key choices made in our implementation of the
algorithm. In brief, they are as follows:
The choice of functions φ and ψ. We will de-
scribe basic features used in φ and ψ (single-level
rules, larger tree fragments, etc.). We will also de-
scribe a method for scaling different features in φ
and ψ by their variance, which turns out to be im-
portant for empirical results.
</bodyText>
<subsectionHeader confidence="0.47992">
Estimation of Ea�b c
</subsectionHeader>
<bodyText confidence="0.9639471875">
i,j,k and Ea�x
i . There area very
large number of parameters in the model, lead-
ing to challenges in estimation. The estimates in
Eqs. 2 and 3 are unsmoothed. We describe a simple
backed-off smoothing method that leads to signifi-
cant improvements in performance of the method.
Handling positive and negative values. As de-
fined, the cˆ parameters may be positive or negative;
as a result, the µˆ values may also be positive or neg-
ative. We find that negative values can be a signif-
icant problem if not handled correctly; but with a
very simple fix to the algorithm, it performs well.
We now turn to these three issues in more detail.
Section 4 will describe experiments measuring the
impact of the different choices.
</bodyText>
<subsectionHeader confidence="0.998508">
3.1 The Choice of Functions φ and ψ
</subsectionHeader>
<bodyText confidence="0.999786954545455">
Cohen et al. (2012) show that the choice of feature
definitions φ and ψ is crucial in two respects. First,
for all non-terminals a C N, the matrix Ωa must
be of rank m: otherwise the parameter-estimation
algorithm will not be consistent. Second, the num-
ber of samples required for learning is polynomial
in 1/σ, where σ = minaEN σm(Ωa), and σm(Ωa)
is the m’th smallest singular value of Ωa. (Note that
the second condition is stronger than the first; σ &gt; 0
implies that Ωa is of rank m for all a.) The choice
of φ and ψ has a direct impact on the value for σ:
roughly speaking, the value for σ can be thought of
as a measure of how informative the functions φ and
ψ are about the hidden state values.
With this in mind, our goal is to define a rel-
atively simple set of features, which nevertheless
provide significant information about hidden-state
values, and hence provide high accuracy under the
model. The inside-tree feature function φ(t) makes
use of the following indicator features (throughout
these definitions assume that a —4 b c is at the root
of the inside tree t):
</bodyText>
<listItem confidence="0.977376">
• The pair of nonterminals (a, b). E.g., for the in-
side tree in figure 1 this would be the pair (VP, V).
</listItem>
<page confidence="0.86431">
151
</page>
<listItem confidence="0.965895">
• The pair (a, c). E.g., (VP, NP).
• The rule a —4 b c. E.g., VP —4 V NP.
• The rule a —4 b c paired with the rule at the
root of t(i,2). E.g., for the inside tree in fig-
ure 1 this would correspond to the tree fragment
(VP (V saw) NP).
• The rule a —4 b c paired with the rule at
the root of t(i,3). E.g., the tree fragment
(VP V (NP D N)).
• The head part-of-speech of t(i,1) paired with a.4
E.g., the pair (VP, V).
• The number of words dominated by t(i,1) paired
with a (this is an integer valued feature).
</listItem>
<bodyText confidence="0.9988568">
In the case of an inside tree consisting of a single
rule a —4 x the feature vector simply indicates the
identity of that rule.
To illustrate the function ψ, it will be useful to
make use of the following example outside tree:
</bodyText>
<equation confidence="0.7595095">
S
dog
</equation>
<bodyText confidence="0.950529">
Note that in this example the foot node of the out-
side tree is labeled D. The features are as follows:
</bodyText>
<listItem confidence="0.969770142857143">
• The rule above the foot node. We take care
to mark which non-terminal is the foot, using a
* symbol. In the above example this feature is
NP —4 D* N.
• The two-level and three-level rule fragments
above the foot node. In the above example these fea-
tures would be
</listItem>
<equation confidence="0.458496">
S
NP VP
V NP
D* N
</equation>
<listItem confidence="0.8895929375">
• The label of the foot node, together with the
label of its parent. In the above example this is
(D, NP).
• The label of the foot node, together with the la-
bel of its parent and grandparent. In the above ex-
ample this is (D, NP, VP).
• The part of speech of the first head word along
the path from the foot of the outside tree to the root
of the tree which is different from the head node of
4We use the English head rules from the Stanford parser
(Klein and Manning, 2003).
the foot node. In the above example this is N.
• The width of the span to the left of the foot node,
paired with the label of the foot node.
• The width of the span to the right of the foot
node, paired with the label of the foot node.
</listItem>
<bodyText confidence="0.9718856">
Scaling of features. The features defined above
are almost all binary valued features. We scale the
features in the following way. For each feature φi(t),
define count(i) to be the number of times the feature
is equal to 1, and M to be the number of training
examples. The feature is then redefined to be
�φi(t) x
where κ is a smoothing term (the method is rela-
tively insensitive to the choice of κ; we set κ = 5 in
our experiments). A similar process is applied to the
ψ features. The method has the effect of decreasing
the importance of more frequent features in the SVD
step of the algorithm.
The SVD-based step of the algorithm is very
closely related to previous work on CCA (Hotelling,
1936; Hardoon et al., 2004; Kakade and Foster,
2009); and the scaling step is derived from previ-
ous work on CCA (Dhillon et al., 2011). In CCA
the φ and ψ vectors are “whitened” in a preprocess-
ing step, before an SVD is applied. This whiten-
ing process involves calculating covariance matrices
Cx = E[φφT] and Cy = E[ψψT], and replacing φ
by (Cx)−1/2φ and ψ by (Cy)−1/2ψ. The exact cal-
culation of (Cx)−1/2 and (Cy)−1/2 is challenging in
high dimensions, however, as these matrices will not
be sparse; the transformation described above can
be considered an approximation where off-diagonal
members of Cx and Cy are set to zero. We will see
that empirically this scaling gives much improved
accuracy.
</bodyText>
<subsectionHeader confidence="0.99943">
3.2 Estimation of Ea—&apos;b c
</subsectionHeader>
<bodyText confidence="0.969478909090909">
i,j,k and Ea—&apos;x
i
The number of Ea—&apos;b c
i,j,k parameters is very large,
and the estimation method described in Eqs. 2–3 is
unsmoothed. We have found significant improve-
ments in performance using a relatively simple back-
off smoothing method. The intuition behind this
method is as follows: given two random variables X
and Y , under the assumption that the random vari-
ables are independent, E[XY ] = E[X] x E[Y ]. It
</bodyText>
<figure confidence="0.978167">
NP
VP
NP
D N
D
the
cat
N
saw
V
VP
V NP
D* N
M
count(i) + κ
</figure>
<page confidence="0.990307">
152
</page>
<bodyText confidence="0.976928285714286">
makes sense to define “backed off” estimates which
make increasingly strong independence assumptions
of this form.
Smoothing of binary rules For any rule a → b c
and indices i, j ∈ [m] we can define a second-order
moment as follows:
The definitions of Ea→b c
</bodyText>
<equation confidence="0.754315833333333">
i,·,k and Ea→b c
·,j,k are analogous.
We can define a first-order estimate as follows:
E(o,t(2),t(3)) Yk(t(3))
∈Qa→b c
|Qa→b c|
</equation>
<bodyText confidence="0.5089665">
Again, we have analogous definitions of Ea→b c i,·,·and
Ea→b c. Different levels of smoothed estimate can
,j,·
be derived from these different terms. The first is
</bodyText>
<equation confidence="0.9768878">
2,a→b c
Ez,j,k =
Ea-.,b c X Ea→b c+ Eza b c X Ea→b c+ Ea→ b c X Ea→b c
z,,··,·,k z,•,k·,j,· ·,j,k z,·,·
3
</equation>
<bodyText confidence="0.999899666666667">
Note that we give an equal weight of 1/3 to each of
the three backed-off estimates seen in the numerator.
A second smoothed estimate is
</bodyText>
<equation confidence="0.993406333333333">
E3,a→b c
i,j,k = Ea→b c
i,·,· × Ea→b c
</equation>
<bodyText confidence="0.827524">
·,j,· × Ea→b c ·,·,k
Using the definition of Oa given in section 2.2.1, we
also define
</bodyText>
<equation confidence="0.9856865">
Fa — E(o,t)∈Oa Yi(t) Ha — a — E(o,t)∈Oa Zi(o)
i — |Oa ||Oa|
</equation>
<bodyText confidence="0.516212">
and our next smoothed estimate as E4,a→b c
</bodyText>
<equation confidence="0.853461125">
i,j,k = Hai ×
Fjb × Fc k.
Our final estimate is
λEiaj,k c + (1 − λ) (λEi �,k b c + (1 − λ)Kaj b c)
where Ka→b c
i,j,k = λE3,a→b c
i,j,k + (1 − λ)E4,a→b c
i,j,k .
</equation>
<bodyText confidence="0.998009166666667">
Here λ ∈ [0, 1] is a smoothing parameter, set to
�,I|Qa→ b c |/ (C + �/ |Qa→b c|) in our experiments,
where C is a parameter that is chosen by optimiza-
tion of accuracy on a held-out set of data.
Smoothing lexical rules We define a similar
method for the Ea→x
</bodyText>
<equation confidence="0.88694575">
i parameters. Define
Ea Ex0 Eo∈Qa→.0 Zi(o)
=
i Ex0 |Qa→x0|
</equation>
<bodyText confidence="0.95266875">
hence Eai ignores the identity of x in making its es-
timate. The smoothed estimate is then defined as
νEa→x
i +(1−ν)Ea i . Here, ν is a value in [0, 1] which
is tuned on a development set. We only smooth lex-
ical rules which appear in the data less than a fixed
number of times. Unlike binary rules, for which the
estimation depends on a high order moment (third
moment), the lexical rules use first-order moments,
and therefore it is not required to smooth rules with
a relatively high count. The maximal count for this
kind of smoothing is set using a development set.
</bodyText>
<subsectionHeader confidence="0.999951">
3.3 Handling Positive and Negative Values
</subsectionHeader>
<bodyText confidence="0.976786125">
As described before, the parameter estimates may
be positive or negative, and as a result the
marginals computed by the algorithm may in some
cases themselves be negative. In early exper-
iments we found this to be a signficant prob-
lem, with some parses having a very large num-
ber of negatives, and being extremely poor in qual-
ity. Our fix is to define the output of the parser
</bodyText>
<equation confidence="0.970853666666667">
E
to be arg maxt (a,i,j)∈t |µ(a, i, j) |rather than
E
</equation>
<bodyText confidence="0.999806307692308">
arg maxt (a,i,j)∈t µ(a, i, j) as defined in Good-
man’s algorithm. Thus if a marginal value µ(a, i, j)
is negative, we simply replace it with its absolute
value. This step was derived after inspection of the
parsing charts for bad parses, where we saw evi-
dence that in these cases the entire set of marginal
values had been negated (and hence decoding under
Eq. 1 actually leads to the lowest probability parse
being output under the model). We suspect that this
is because in some cases a dominant parameter has
had its sign flipped due to sampling error; more the-
oretical and empirical work is required in fully un-
derstanding this issue.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999585">
In this section we describe parsing experiments us-
ing the L-PCFG estimation method. We give com-
parisons to the EM algorithm, considering both
speed of training, and accuracy of the resulting
model; we also give experiments investigating the
various choices described in the previous section.
</bodyText>
<equation confidence="0.985816428571429">
E(o,t(2),t(3))
Ea→b c ∈Qa→b c
i,j,· =
Zi(o) × Yj(t(2))
|Qa→b c|
Ea→b c =
·,·,k
</equation>
<page confidence="0.996128">
153
</page>
<bodyText confidence="0.9997977">
We use the Penn WSJ treebank (Marcus et al.,
1993) for our experiments. Sections 2–21 were
used as training data, and sections 0 and 22 were
used as development data. Section 23 is used as
the final test set. We binarize the trees in train-
ing data using the same method as that described in
Petrov et al. (2006). For example, the non-binary
rule VP → V NP PP SBAR would be converted
to the structure [VP [@VP [@VP V NP] PP]
SBAR] where @VP is a new symbol in the grammar.
Unary rules are removed by collapsing non-terminal
chains: for example the unary rule S → VP would
be replaced by a single non-terminal S|VP.
For the EM algorithm we use the initialization
method described in Matsuzaki et al. (2005). For ef-
ficiency, we use a coarse-to-fine algorithm for pars-
ing with either the EM or spectral derived gram-
mar: a PCFG without latent states is used to calcu-
late marginals, and dynamic programming items are
removed if their marginal probability is lower than
some threshold (0.00005 in our experiments).
For simplicity the parser takes part-of-speech
tagged sentences as input. We use automatically
tagged data from Turbo Tagger (Martins et al.,
2010). The tagger is used to tag both the devel-
opment data and the test data. The tagger was re-
trained on sections 2–21. We use the Fl measure
according to the Parseval metric (Black et al., 1991).
For the spectral algorithm, we tuned the smoothing
parameters using section 0 of the treebank.
</bodyText>
<subsectionHeader confidence="0.999515">
4.1 Comparison to EM: Accuracy
</subsectionHeader>
<bodyText confidence="0.999457538461539">
We compare models trained using EM and the spec-
tral algorithm using values for m in {8,16, 24, 32}.5
For EM, we found that it was important to use de-
velopment data to choose the number of iterations
of training. We train the models for 100 iterations,
then test accuracy of the model on section 22 (devel-
opment data) at different iteration numbers. Table 1
shows that a peak level of accuracy is reached for all
values of m, other than m = 8, at iteration 20–30,
with sometimes substantial overtraining beyond that
point.
The performance of a regular PCFG model, esti-
mated using maximum likelihood and with no latent
</bodyText>
<footnote confidence="0.931078">
5Lower values of m, such as 2 or 4, lead to substantially
lower performance for both models.
</footnote>
<table confidence="0.9989775">
section 22 section 23
EM spectral EM spectral
m = 8 86.87 85.60 — —
m = 16 88.32 87.77 — —
m = 24 88.35 88.53 — —
m = 32 88.56 88.82 87.76 88.05
</table>
<tableCaption confidence="0.982675666666667">
Table 2: Results on the development data (section 22,
with machine-generated POS tags) and test data (section
23, with machine-generated POS tags).
</tableCaption>
<bodyText confidence="0.911015875">
states, is 68.62%.
Table 2 gives results for the EM-trained models
and spectral-trained models. The spectral models
give very similar accuracy to the EM-trained model
on the test set. Results on the development set with
varying m show that the EM-based models perform
better for m = 8, but that the spectral algorithm
quickly catches up as m increases.
</bodyText>
<subsectionHeader confidence="0.997544">
4.2 Comparison to EM: Training Speed
</subsectionHeader>
<bodyText confidence="0.99980936">
Table 3 gives training times for the EM algorithm
and the spectral algorithm for m ∈ {8,16, 24, 32}.
All timing experiments were done on a single Intel
Xeon 2.67GHz CPU. The implementations for the
EM algorithm and the spectral algorithm were writ-
ten in Java. The spectral algorithm also made use
of Matlab for several matrix calculations such as the
SVD calculation.
For EM we show the time to train a single iter-
ation, and also the time to train the optimal model
(time for 30 iterations of training for m = 8, 16, 24,
and time for 20 iterations for m = 32). Note that
this latter time is optimistic, as it assumes an oracle
specifying exactly when it is possible to terminate
EM training with no loss in performance. The spec-
tral method is considerably faster than EM: for ex-
ample, for m = 32 the time for training the spectral
model is just under 10 hours, compared to 187 hours
for EM, a factor of almost 19 times faster.6
The reason for these speed ups is as follows.
Step 1 of the spectral algorithm (feature calculation,
transfer + scaling, and SVD) is not required by EM,
but takes a relatively small amount of time (about
1.2 hours for all values of m). Once step 1 has been
completed, step 2 of the spectral algorithm takes a
</bodyText>
<footnote confidence="0.968296">
6In practice, in order to overcome the speed issue with EM
training, we parallelized the E-step on multiple cores. The spec-
tral algorithm can be similarly parallelized, computing statistics
and parameters for each nonterminal separately.
</footnote>
<page confidence="0.997861">
154
</page>
<table confidence="0.9969726">
10 20 30 40 50 60 70 80 90 100
m = 8 83.51 86.45 86.68 86.69 86.63 86.67 86.70 86.82 86.87 86.83
m = 16 85.18 87.94 88.32 88.21 88.10 87.86 87.70 87.46 87.34 87.24
m = 24 83.62 88.19 88.35 88.25 87.73 87.41 87.35 87.26 87.02 86.80
m = 32 83.23 88.56 88.52 87.82 87.06 86.47 86.38 85.85 85.75 85.57
</table>
<tableCaption confidence="0.97405">
Table 1: Results on section 22 for the EM algorithm, varying the number of iterations used. Best results in each row
are in boldface.
</tableCaption>
<table confidence="0.999487571428571">
single EM total spectral algorithm a --* b c a --* x
EM iter. best model feature transfer + scaling SVD
m = 8 6m 3h 3h32m 7h15m 49m 1h34m 10m
m = 16 52m 26h6m 5h19m 22m 36m 3h13m 19m
m = 24 3h7m 93h36m 9h52m 34m 4h54m 28m
m = 32 9h21m 187h12m 36m 7h16m 41m
35m
</table>
<tableCaption confidence="0.99967">
Table 3: Running time for the EM algorithm and the various stages in the spectral algorithm. For EM we show the
</tableCaption>
<bodyText confidence="0.961255277777778">
time for a single iteration, and the time to train the optimal model (time for 30 iterations of training for m = 8, 16, 24,
time for 20 iterations of training for m = 32). For the spectral method we show the following: “total” is the total
training time; “feature” is the time to compute the 0 and 0 vectors for all data points; “transfer + scaling” is time
to transfer the data from Java to Matlab, combined with the time for scaling of the features; “SVD” is the time for
the SVD computation; a --* b c is the time to compute the c(a --* b c, h2, h3|a, h1) parameters; a --* x is the time to
compute the c(a --* x, h|a, h) parameters. Note that “feature” and “transfer + scaling” are the same step for all values
of m, so we quote a single runtime for these steps.
single pass over the data: in contrast, EM requires
a few tens of passes (certainly more than 10 passes,
from the results in table 1). The computations per-
formed by the spectral algorithm in its single pass
are relatively cheap. In contrast to EM, the inside-
outside algorithm is not required; however various
operations such as calculating smoothing terms in
the spectral method add some overhead. The net re-
sult is that for m = 32 the time for training the spec-
tral method takes a very similar amount of time to a
single pass of the EM algorithm.
</bodyText>
<subsectionHeader confidence="0.999436">
4.3 Smoothing, Features, and Negatives
</subsectionHeader>
<bodyText confidence="0.989378444444444">
We now describe experiments demonstrating the im-
pact of various components described in section 3.
The effect of smoothing (section 3.2) Without
smoothing, results on section 22 are 85.05% (m =
8, −1.82), 86.84% (m = 16, −1.48), 86.47%
(m = 24, −1.88), 86.47% (m = 32, −2.09) (in
each case we show the decrease in performance from
the results in table 2). Smoothing is clearly impor-
tant.
Scaling of features (section 3.1) Without scaling
of features, the accuracy on section 22 with m = 32
is 84.40%, a very significant drop from the 88.82%
accuracy achieved with scaling.
Handling negative values (section 3.3) Replac-
ing marginal values µ(a, i, j) with their absolute
values is also important: without this step, accu-
racy on section 22 decreases to 80.61% (m = 32).
319 sentences out of 1700 examples have different
parses when this step is implemented, implying that
the problem with negative values described in sec-
tion 3.3 occurs on around 18% of all sentences.
The effect of feature functions To test the effect
of features on accuracy, we experimented with a
simpler set of features than those described in sec-
tion 3.1. This simple set just includes an indicator
for the rule below a nonterminal (for inside trees)
and the rule above a nonterminal (for outside trees).
Even this simpler set of features achieves relatively
high accuracy (m = 8: 86.44 , m = 16: 86.86,
m = 24: 87.24 , m = 32: 88.07 ).
This set of features is reminiscent of a PCFG
model where the nonterminals are augmented their
parents (vertical Markovization of order 2) and bina-
rization is done while retaining sibling information
(horizontal Markovization of order 1). See Klein
and Manning (2003) for more information. The per-
</bodyText>
<page confidence="0.997079">
155
</page>
<bodyText confidence="0.999973904761905">
formance of this Markovized PCFG model lags be-
hind the spectral model: it is 82.59%. This is prob-
ably due to the complexity of the grammar which
causes ovefitting. Condensing the sibling and parent
information using latent states as done in the spectral
model leads to better generalization.
It is important to note that the results for both
EM and the spectral algorithm are comparable to
state of the art, but there are other results previ-
ously reported in the literature which are higher.
For example, Hiroyuki et al. (2012) report an ac-
curacy of 92.4 Fl on section 23 of the Penn WSJ
treebank using a Bayesian tree substitution gram-
mar; Charniak and Johnson (2005) report accuracy
of 91.4 using a discriminative reranking model; Car-
reras et al. (2008) report 91.1 Fl accuracy for a dis-
criminative, perceptron-trained model; Petrov and
Klein (2007) report an accuracy of 90.1 Fl, using
L-PCFGs, but with a split-merge training procedure.
Collins (2003) reports an accuracy of 88.2 Fl, which
is comparable to the results in this paper.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999932375">
The spectral learning algorithm gives the same level
of accuracy as EM in our experiments, but has sig-
nificantly faster training times. There are several ar-
eas for future work. There are a large number of pa-
rameters in the model, and we suspect that more so-
phisticated regularization methods than the smooth-
ing method we have described may improve perfor-
mance. Future work should also investigate other
choices for the functions 0 and 0. There are natu-
ral ways to extend the approach to semi-supervised
learning; for example the SVD step, where repre-
sentations of outside and inside trees are learned,
could be applied to unlabeled data parsed by a first-
pass parser. Finally, the methods we have described
should be applicable to spectral learning for other
latent variable models.
</bodyText>
<sectionHeader confidence="0.998713" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999552928571429">
Columbia University gratefully acknowledges the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA,
AFRL, or the US government. Shay Cohen was
supported by the National Science Foundation un-
der Grant #1136996 to the Computing Research As-
sociation for the CIFellows Project. Dean Foster
was supported by National Science Foundation grant
1106743.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858275">
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for learn-
ing latent-variable models. arXiv:1210.7559.
S. Arora, R. Se, and A. Moitra. 2012. Learning topic
models - going beyond SVD. In Proceedings of
FOCS.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings ofALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In Proceedings of CoNLL, pages
9–16.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings ofACL.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589–637.
P. Dhillon, D. P. Foster, and L. H. Ungar. 2011. Multi-
view learning of word embeddings via CCA. In Pro-
ceedings of NIPS.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with latent
variables. In Proceedings of EMNLP.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings ofACL.
</reference>
<page confidence="0.985592">
156
</page>
<reference confidence="0.999566333333333">
D. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004.
Canonical correlation analysis: An overview with ap-
plication to learning methods. Neural Computation,
16(12):2639–2664.
S. Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki.
2012. Bayesian symbol-refined tree substitution gram-
mars for syntactic parsing. In Proceedings of ACL,
pages 440–448.
H. Hotelling. 1936. Relations between two sets of vari-
ants. Biometrika, 28:321–377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
S. M. Kakade and D. P. Foster. 2009. Multi-view regres-
sion via canonical correlation analysis. In COLT.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. ofACL, pages 423–430.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313–330.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. T.
Figueiredo, and M. Q. Aguiar. 2010. TurboParsers:
Dependency parsing by approximate variational infer-
ence. In Proceedings of EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. JMLR, 9:741–748.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841–860.
</reference>
<page confidence="0.997709">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.860028">
<title confidence="0.998839">Experiments with Spectral Learning of Latent-Variable PCFGs</title>
<author confidence="0.971511">B Karl Michael Dean P</author>
<author confidence="0.971511">Lyle</author>
<affiliation confidence="0.9271365">of Computer Science, Columbia of of Computer and Information Science, University of</affiliation>
<email confidence="0.999752">foster@wharton.upenn.edu,ungar@cis.upenn.edu</email>
<abstract confidence="0.997612421052632">Latent-variable PCFGs (L-PCFGs) are a highly successful model for natural language parsing. Recent work (Cohen et al., 2012) has introduced a spectral algorithm for parameter estimation of L-PCFGs, which—unlike the EM algorithm—is guaranteed to give consistent parameter estimates (it has PAC-style guarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Anandkumar</author>
<author>R Ge</author>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>M Telgarsky</author>
</authors>
<title>Tensor decompositions for learning latent-variable models.</title>
<date>2012</date>
<pages>1210--7559</pages>
<contexts>
<context position="3524" citStr="Anandkumar et al., 2012" startWordPosition="551" endWordPosition="554"> tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral learning algorithm of Cohen et al. (2012). 148 Proceedings of NAACL-HLT 2013, pages 148–</context>
</contexts>
<marker>Anandkumar, Ge, Hsu, Kakade, Telgarsky, 2012</marker>
<rawString>A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. 2012. Tensor decompositions for learning latent-variable models. arXiv:1210.7559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Arora</author>
<author>R Se</author>
<author>A Moitra</author>
</authors>
<title>Learning topic models - going beyond SVD.</title>
<date>2012</date>
<booktitle>In Proceedings of FOCS.</booktitle>
<contexts>
<context position="3476" citStr="Arora et al., 2012" startWordPosition="543" endWordPosition="546">sponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral learning algorithm of Cohen et al. (2012</context>
</contexts>
<marker>Arora, Se, Moitra, 2012</marker>
<rawString>S. Arora, R. Se, and A. Moitra. 2012. Learning topic models - going beyond SVD. In Proceedings of FOCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bailly</author>
<author>A Habrar</author>
<author>F Denis</author>
</authors>
<title>A spectral approach for probabilistic grammatical inference on trees.</title>
<date>2010</date>
<booktitle>In Proceedings ofALT.</booktitle>
<contexts>
<context position="3393" citStr="Bailly et al., 2010" startWordPosition="527" endWordPosition="530">ns mapping inside and outside trees to feature vectors—we make use of features corresponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions </context>
</contexts>
<marker>Bailly, Habrar, Denis, 2010</marker>
<rawString>R. Bailly, A. Habrar, and F. Denis. 2010. A spectral approach for probabilistic grammatical inference on trees. In Proceedings ofALT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Balle</author>
<author>A Quattoni</author>
<author>X Carreras</author>
</authors>
<title>A spectral learning algorithm for finite state transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of ECML.</booktitle>
<contexts>
<context position="3456" citStr="Balle et al., 2011" startWordPosition="539" endWordPosition="542">se of features corresponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral learning algorithm o</context>
</contexts>
<marker>Balle, Quattoni, Carreras, 2011</marker>
<rawString>B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral learning algorithm for finite state transducers. In Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of DARPA Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="24531" citStr="Black et al., 1991" startWordPosition="4582" endWordPosition="4585">e a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the Fl measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8,16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the model on section 22 (development data) at different iteration numbers. Table 1 shows that a peak level of accuracy is reached for all values of m, other than m = 8, at iteration 20–30, with some</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of DARPA Workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="32153" citStr="Carreras et al. (2008)" startWordPosition="5963" endWordPosition="5967">ammar which causes ovefitting. Condensing the sibling and parent information using latent states as done in the spectral model leads to better generalization. It is important to note that the results for both EM and the spectral algorithm are comparable to state of the art, but there are other results previously reported in the literature which are higher. For example, Hiroyuki et al. (2012) report an accuracy of 92.4 Fl on section 23 of the Penn WSJ treebank using a Bayesian tree substitution grammar; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 Fl accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 Fl, using L-PCFGs, but with a split-merge training procedure. Collins (2003) reports an accuracy of 88.2 Fl, which is comparable to the results in this paper. 5 Conclusion The spectral learning algorithm gives the same level of accuracy as EM in our experiments, but has significantly faster training times. There are several areas for future work. There are a large number of parameters in the model, and we suspect that more sophisticated regularization methods than the smo</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing. In Proceedings of CoNLL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="32066" citStr="Charniak and Johnson (2005)" startWordPosition="5950" endWordPosition="5953">gs behind the spectral model: it is 82.59%. This is probably due to the complexity of the grammar which causes ovefitting. Condensing the sibling and parent information using latent states as done in the spectral model leads to better generalization. It is important to note that the results for both EM and the spectral algorithm are comparable to state of the art, but there are other results previously reported in the literature which are higher. For example, Hiroyuki et al. (2012) report an accuracy of 92.4 Fl on section 23 of the Penn WSJ treebank using a Bayesian tree substitution grammar; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 Fl accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 Fl, using L-PCFGs, but with a split-merge training procedure. Collins (2003) reports an accuracy of 88.2 Fl, which is comparable to the results in this paper. 5 Conclusion The spectral learning algorithm gives the same level of accuracy as EM in our experiments, but has significantly faster training times. There are several areas for future work. There are a large number of parameters i</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Stratos</author>
<author>M Collins</author>
<author>D F Foster</author>
<author>L Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1358" citStr="Cohen et al., 2012" startWordPosition="194" endWordPosition="197">iments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the data where parameter values are cal</context>
<context position="4077" citStr="Cohen et al. (2012)" startWordPosition="643" endWordPosition="646">Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral learning algorithm of Cohen et al. (2012). 148 Proceedings of NAACL-HLT 2013, pages 148–157, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2.1 L-PCFGs: Basic Definitions We follow the definition in Cohen et al. (2012) of L-PCFGs. An L-PCFG is an 8-tuple (N, I, P, m, n, π, t, q) where: • N is the set of non-terminal symbols in the grammar. I ⊂ N is a finite set of in-terminals. P ⊂ N is a finite set of pre-terminals. We assume that N = I∪P, and I∩P = ∅. Hence we have partitioned the set of non-terminals into two subsets. • [m] is the set of possible hidden states.2 • [n] is the set of possible word</context>
<context position="6990" citStr="Cohen et al. (2012)" startWordPosition="1235" endWordPosition="1238">letal trees t for x1 ... xn that include non-terminal a spanning words xi ... xj. A variant of the inside-outside algorithm can be used to calculate marginals. Once marginals have been computed, Goodman’s algorithm (Goodman, 1996) is used to find � arg maxt (a,i,j)�t µ(a, i, j).3 2.2 The Spectral Learning Algorithm We now give a sketch of the spectral learning algorithm. The training data for the algorithm is a set of skeletal trees. The output from the algorithm is a set of parameter estimates for t, q and π (more precisely, the estimates are estimates of linearly transformed parameters; see Cohen et al. (2012) and section 2.3.1 for more details). The algorithm takes two inputs in addition to the set of skeletal trees. The first is an integer m, specifying the number of latent state values in the model. Typically m is a relatively small number; in our experiments we test values such as m = 8,16 or 32. The second is a pair of functions φ and ψ, that respectively map inside and outside trees to feature vectors in Rd and Rd&amp;quot; where d and d&apos; are integers. Each non-terminal in a skeletal tree has an associated inside and outside tree. The inside tree for a node contains the entire subtree below that node;</context>
<context position="11534" citStr="Cohen et al. (2012)" startWordPosition="2128" endWordPosition="2131"> a in the corpus. Note that once the SVD step has been used to compute representations Y (t) and Z(o) for each inside and outside tree in the corpus, calculating the parameter value c(a → b c, j, k|a, i) is a very simple operation. Similarly, for any rule a → x, define Qa,x to be the set of outside trees seen with that rule in the training corpus. The parameter estimate is then count(a → x) a—� c(a → x|a,i) = × Ei (3) count(a) where Ea,x i = EoEQa—x Zi(o)/|Qa,x|. A similar method is used for estimating parameters c(a, i) that play the role of the π parameters (details omitted for brevity; see Cohen et al. (2012)). 2.3.1 Guarantees for the Algorithm Once the c(a → b c, j, k|a, i), c(a → x|a, i) and c(a, i) parameters have been estimated from the D N the dog a—.b c Ei,j,k = 150 training corpus, they can be used in place of the t, q and π parameters in the inside-outside algorithm for computing marginals (see Eq. 1). Call the resulting marginals ˆµ(a, i, j). The guarantees for the parameter estimation method are as follows: • Define Ωa = E[φ(T)(ψ(O))T|A = a] where A, O, T are random variables corresponding to the non-terminal label at a node, the outside tree, and the inside tree (see Cohen et al. (2012</context>
<context position="13360" citStr="Cohen et al. (2012)" startWordPosition="2482" endWordPosition="2485">parameters in the L-PCFG. For example, define c(a —4 b c, j, k|a, i) to be the value that ˆc(a —4 b c, j, k|a, i) converges to in the limit of infinite data. Then there exist invertible matrices Ga C Rm�m for all a C N such that for any a —4 b c, for any h1, h2, h3 C Rm, t(a —4 b c, h2, h3|a, h1) = E [Ga]i,h1[(Gb)−1]j,h,[(G&apos;)−1]k,h,,c(a —4 b c,j, k|a, i) i,j,k The transforms defined by the Ga matrices are benign, in that they cancel in the inside-outside algorithm when marginals µ(a, i, j) are calculated. Similar relationships hold for the π and q parameters. 3 Implementation of the Algorithm Cohen et al. (2012) introduced the spectral learning algorithm, but did not perform experiments, leaving several choices open in how the algorithm is implemented in practice. This section describes a number of key choices made in our implementation of the algorithm. In brief, they are as follows: The choice of functions φ and ψ. We will describe basic features used in φ and ψ (single-level rules, larger tree fragments, etc.). We will also describe a method for scaling different features in φ and ψ by their variance, which turns out to be important for empirical results. Estimation of Ea�b c i,j,k and Ea�x i . Th</context>
<context position="14710" citStr="Cohen et al. (2012)" startWordPosition="2720" endWordPosition="2723">othed. We describe a simple backed-off smoothing method that leads to significant improvements in performance of the method. Handling positive and negative values. As defined, the cˆ parameters may be positive or negative; as a result, the µˆ values may also be positive or negative. We find that negative values can be a significant problem if not handled correctly; but with a very simple fix to the algorithm, it performs well. We now turn to these three issues in more detail. Section 4 will describe experiments measuring the impact of the different choices. 3.1 The Choice of Functions φ and ψ Cohen et al. (2012) show that the choice of feature definitions φ and ψ is crucial in two respects. First, for all non-terminals a C N, the matrix Ωa must be of rank m: otherwise the parameter-estimation algorithm will not be consistent. Second, the number of samples required for learning is polynomial in 1/σ, where σ = minaEN σm(Ωa), and σm(Ωa) is the m’th smallest singular value of Ωa. (Note that the second condition is stronger than the first; σ &gt; 0 implies that Ωa is of rank m for all a.) The choice of φ and ψ has a direct impact on the value for σ: roughly speaking, the value for σ can be thought of as a me</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and L. Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--589</pages>
<contexts>
<context position="32353" citStr="Collins (2003)" startWordPosition="5996" endWordPosition="5997"> EM and the spectral algorithm are comparable to state of the art, but there are other results previously reported in the literature which are higher. For example, Hiroyuki et al. (2012) report an accuracy of 92.4 Fl on section 23 of the Penn WSJ treebank using a Bayesian tree substitution grammar; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 Fl accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 Fl, using L-PCFGs, but with a split-merge training procedure. Collins (2003) reports an accuracy of 88.2 Fl, which is comparable to the results in this paper. 5 Conclusion The spectral learning algorithm gives the same level of accuracy as EM in our experiments, but has significantly faster training times. There are several areas for future work. There are a large number of parameters in the model, and we suspect that more sophisticated regularization methods than the smoothing method we have described may improve performance. Future work should also investigate other choices for the functions 0 and 0. There are natural ways to extend the approach to semi-supervised l</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29:589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dhillon</author>
<author>D P Foster</author>
<author>L H Ungar</author>
</authors>
<title>Multiview learning of word embeddings via CCA. In</title>
<date>2011</date>
<booktitle>Proceedings of NIPS.</booktitle>
<contexts>
<context position="18546" citStr="Dhillon et al., 2011" startWordPosition="3475" endWordPosition="3478">re is equal to 1, and M to be the number of training examples. The feature is then redefined to be �φi(t) x where κ is a smoothing term (the method is relatively insensitive to the choice of κ; we set κ = 5 in our experiments). A similar process is applied to the ψ features. The method has the effect of decreasing the importance of more frequent features in the SVD step of the algorithm. The SVD-based step of the algorithm is very closely related to previous work on CCA (Hotelling, 1936; Hardoon et al., 2004; Kakade and Foster, 2009); and the scaling step is derived from previous work on CCA (Dhillon et al., 2011). In CCA the φ and ψ vectors are “whitened” in a preprocessing step, before an SVD is applied. This whitening process involves calculating covariance matrices Cx = E[φφT] and Cy = E[ψψT], and replacing φ by (Cx)−1/2φ and ψ by (Cy)−1/2ψ. The exact calculation of (Cx)−1/2 and (Cy)−1/2 is challenging in high dimensions, however, as these matrices will not be sparse; the transformation described above can be considered an approximation where off-diagonal members of Cx and Cy are set to zero. We will see that empirically this scaling gives much improved accuracy. 3.2 Estimation of Ea—&apos;b c i,j,k and</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>P. Dhillon, D. P. Foster, and L. H. Ungar. 2011. Multiview learning of word embeddings via CCA. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dhillon</author>
<author>J Rodu</author>
<author>M Collins</author>
<author>D P Foster</author>
<author>L H Ungar</author>
</authors>
<title>Spectral dependency parsing with latent variables.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3498" citStr="Dhillon et al., 2012" startWordPosition="547" endWordPosition="550">evel rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral learning algorithm of Cohen et al. (2012). 148 Proceedings of </context>
</contexts>
<marker>Dhillon, Rodu, Collins, Foster, Ungar, 2012</marker>
<rawString>P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H. Ungar. 2012. Spectral dependency parsing with latent variables. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6601" citStr="Goodman, 1996" startWordPosition="1165" endWordPosition="1166">denote the set {1, 2,... n}. The parsing problem is to take a sentence as input, and produce a skeletal tree as output. A standard method for parsing with L-PCFGs is as follows. First, for a given input sentence x1 ... xn, for any triple (a, i, j) such that a ∈ N and 1 ≤ i ≤ j ≤ n, the marginal µ(a, i, j) is defined as µ(a,i,j) = � p(t) (1) t:(a,i,j)Et where the sum is over all skeletal trees t for x1 ... xn that include non-terminal a spanning words xi ... xj. A variant of the inside-outside algorithm can be used to calculate marginals. Once marginals have been computed, Goodman’s algorithm (Goodman, 1996) is used to find � arg maxt (a,i,j)�t µ(a, i, j).3 2.2 The Spectral Learning Algorithm We now give a sketch of the spectral learning algorithm. The training data for the algorithm is a set of skeletal trees. The output from the algorithm is a set of parameter estimates for t, q and π (more precisely, the estimates are estimates of linearly transformed parameters; see Cohen et al. (2012) and section 2.3.1 for more details). The algorithm takes two inputs in addition to the set of skeletal trees. The first is an integer m, specifying the number of latent state values in the model. Typically m is</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hardoon</author>
<author>S Szedmak</author>
<author>J Shawe-Taylor</author>
</authors>
<title>Canonical correlation analysis: An overview with application to learning methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="18438" citStr="Hardoon et al., 2004" startWordPosition="3455" endWordPosition="3458"> features in the following way. For each feature φi(t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be �φi(t) x where κ is a smoothing term (the method is relatively insensitive to the choice of κ; we set κ = 5 in our experiments). A similar process is applied to the ψ features. The method has the effect of decreasing the importance of more frequent features in the SVD step of the algorithm. The SVD-based step of the algorithm is very closely related to previous work on CCA (Hotelling, 1936; Hardoon et al., 2004; Kakade and Foster, 2009); and the scaling step is derived from previous work on CCA (Dhillon et al., 2011). In CCA the φ and ψ vectors are “whitened” in a preprocessing step, before an SVD is applied. This whitening process involves calculating covariance matrices Cx = E[φφT] and Cy = E[ψψT], and replacing φ by (Cx)−1/2φ and ψ by (Cy)−1/2ψ. The exact calculation of (Cx)−1/2 and (Cy)−1/2 is challenging in high dimensions, however, as these matrices will not be sparse; the transformation described above can be considered an approximation where off-diagonal members of Cx and Cy are set to zero.</context>
</contexts>
<marker>Hardoon, Szedmak, Shawe-Taylor, 2004</marker>
<rawString>D. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hiroyuki</author>
<author>M Yusuke</author>
<author>F Akinori</author>
<author>N Masaaki</author>
</authors>
<title>Bayesian symbol-refined tree substitution grammars for syntactic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--448</pages>
<contexts>
<context position="31925" citStr="Hiroyuki et al. (2012)" startWordPosition="5924" endWordPosition="5927">ontal Markovization of order 1). See Klein and Manning (2003) for more information. The per155 formance of this Markovized PCFG model lags behind the spectral model: it is 82.59%. This is probably due to the complexity of the grammar which causes ovefitting. Condensing the sibling and parent information using latent states as done in the spectral model leads to better generalization. It is important to note that the results for both EM and the spectral algorithm are comparable to state of the art, but there are other results previously reported in the literature which are higher. For example, Hiroyuki et al. (2012) report an accuracy of 92.4 Fl on section 23 of the Penn WSJ treebank using a Bayesian tree substitution grammar; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 Fl accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 Fl, using L-PCFGs, but with a split-merge training procedure. Collins (2003) reports an accuracy of 88.2 Fl, which is comparable to the results in this paper. 5 Conclusion The spectral learning algorithm gives the same level of accuracy as EM in ou</context>
</contexts>
<marker>Hiroyuki, Yusuke, Akinori, Masaaki, 2012</marker>
<rawString>S. Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of ACL, pages 440–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hotelling</author>
</authors>
<title>Relations between two sets of variants.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--321</pages>
<contexts>
<context position="18416" citStr="Hotelling, 1936" startWordPosition="3453" endWordPosition="3454">res. We scale the features in the following way. For each feature φi(t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be �φi(t) x where κ is a smoothing term (the method is relatively insensitive to the choice of κ; we set κ = 5 in our experiments). A similar process is applied to the ψ features. The method has the effect of decreasing the importance of more frequent features in the SVD step of the algorithm. The SVD-based step of the algorithm is very closely related to previous work on CCA (Hotelling, 1936; Hardoon et al., 2004; Kakade and Foster, 2009); and the scaling step is derived from previous work on CCA (Dhillon et al., 2011). In CCA the φ and ψ vectors are “whitened” in a preprocessing step, before an SVD is applied. This whitening process involves calculating covariance matrices Cx = E[φφT] and Cy = E[ψψT], and replacing φ by (Cx)−1/2φ and ψ by (Cy)−1/2ψ. The exact calculation of (Cx)−1/2 and (Cy)−1/2 is challenging in high dimensions, however, as these matrices will not be sparse; the transformation described above can be considered an approximation where off-diagonal members of Cx a</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>H. Hotelling. 1936. Relations between two sets of variants. Biometrika, 28:321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>T Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="3372" citStr="Hsu et al., 2009" startWordPosition="523" endWordPosition="526">m requires functions mapping inside and outside trees to feature vectors—we make use of features corresponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first gi</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kakade</author>
<author>D P Foster</author>
</authors>
<title>Multi-view regression via canonical correlation analysis.</title>
<date>2009</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="18464" citStr="Kakade and Foster, 2009" startWordPosition="3459" endWordPosition="3462">wing way. For each feature φi(t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be �φi(t) x where κ is a smoothing term (the method is relatively insensitive to the choice of κ; we set κ = 5 in our experiments). A similar process is applied to the ψ features. The method has the effect of decreasing the importance of more frequent features in the SVD step of the algorithm. The SVD-based step of the algorithm is very closely related to previous work on CCA (Hotelling, 1936; Hardoon et al., 2004; Kakade and Foster, 2009); and the scaling step is derived from previous work on CCA (Dhillon et al., 2011). In CCA the φ and ψ vectors are “whitened” in a preprocessing step, before an SVD is applied. This whitening process involves calculating covariance matrices Cx = E[φφT] and Cy = E[ψψT], and replacing φ by (Cx)−1/2φ and ψ by (Cy)−1/2ψ. The exact calculation of (Cx)−1/2 and (Cy)−1/2 is challenging in high dimensions, however, as these matrices will not be sparse; the transformation described above can be considered an approximation where off-diagonal members of Cx and Cy are set to zero. We will see that empirica</context>
</contexts>
<marker>Kakade, Foster, 2009</marker>
<rawString>S. M. Kakade and D. P. Foster. 2009. Multi-view regression via canonical correlation analysis. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="17481" citStr="Klein and Manning, 2003" startWordPosition="3271" endWordPosition="3274">ture is NP —4 D* N. • The two-level and three-level rule fragments above the foot node. In the above example these features would be S NP VP V NP D* N • The label of the foot node, together with the label of its parent. In the above example this is (D, NP). • The label of the foot node, together with the label of its parent and grandparent. In the above example this is (D, NP, VP). • The part of speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of 4We use the English head rules from the Stanford parser (Klein and Manning, 2003). the foot node. In the above example this is N. • The width of the span to the left of the foot node, paired with the label of the foot node. • The width of the span to the right of the foot node, paired with the label of the foot node. Scaling of features. The features defined above are almost all binary valued features. We scale the features in the following way. For each feature φi(t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be �φi(t) x where κ is a smoothing term (the method is relat</context>
<context position="31364" citStr="Klein and Manning (2003)" startWordPosition="5828" endWordPosition="5831">imented with a simpler set of features than those described in section 3.1. This simple set just includes an indicator for the rule below a nonterminal (for inside trees) and the rule above a nonterminal (for outside trees). Even this simpler set of features achieves relatively high accuracy (m = 8: 86.44 , m = 16: 86.86, m = 24: 87.24 , m = 32: 88.07 ). This set of features is reminiscent of a PCFG model where the nonterminals are augmented their parents (vertical Markovization of order 2) and binarization is done while retaining sibling information (horizontal Markovization of order 1). See Klein and Manning (2003) for more information. The per155 formance of this Markovized PCFG model lags behind the spectral model: it is 82.59%. This is probably due to the complexity of the grammar which causes ovefitting. Condensing the sibling and parent information using latent states as done in the spectral model leads to better generalization. It is important to note that the results for both EM and the spectral algorithm are comparable to state of the art, but there are other results previously reported in the literature which are higher. For example, Hiroyuki et al. (2012) report an accuracy of 92.4 Fl on secti</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ofACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Luque</author>
<author>A Quattoni</author>
<author>B Balle</author>
<author>X Carreras</author>
</authors>
<title>Spectral learning for non-deterministic dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="3597" citStr="Luque et al. (2012)" startWordPosition="563" endWordPosition="566">mportant to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral learning algorithm of Cohen et al. (2012). 148 Proceedings of NAACL-HLT 2013, pages 148–157, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computation</context>
</contexts>
<marker>Luque, Quattoni, Balle, Carreras, 2012</marker>
<rawString>F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012. Spectral learning for non-deterministic dependency parsing. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="23235" citStr="Marcus et al., 1993" startWordPosition="4352" endWordPosition="4355">his is because in some cases a dominant parameter has had its sign flipped due to sampling error; more theoretical and empirical work is required in fully understanding this issue. 4 Experiments In this section we describe parsing experiments using the L-PCFG estimation method. We give comparisons to the EM algorithm, considering both speed of training, and accuracy of the resulting model; we also give experiments investigating the various choices described in the previous section. E(o,t(2),t(3)) Ea→b c ∈Qa→b c i,j,· = Zi(o) × Yj(t(2)) |Qa→b c| Ea→b c = ·,·,k 153 We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the init</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>M T Figueiredo</author>
<author>M Q Aguiar</author>
</authors>
<title>TurboParsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="24340" citStr="Martins et al., 2010" startWordPosition="4545" endWordPosition="4548"> example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the Fl measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8,16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the mod</context>
</contexts>
<marker>Martins, Smith, Xing, Figueiredo, Aguiar, 2010</marker>
<rawString>A. F. T. Martins, N. A. Smith, E. P. Xing, M. T. Figueiredo, and M. Q. Aguiar. 2010. TurboParsers: Dependency parsing by approximate variational inference. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1302" citStr="Matsuzaki et al., 2005" startWordPosition="183" endWordPosition="187">uarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a </context>
<context position="23889" citStr="Matsuzaki et al. (2005)" startWordPosition="4473" endWordPosition="4476">2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the Fl measure according t</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Parikh</author>
<author>L Song</author>
<author>E P Xing</author>
</authors>
<title>A spectral algorithm for latent tree graphical models.</title>
<date>2011</date>
<booktitle>In Proceedings of The 28th International Conference on Machine Learningy (ICML</booktitle>
<contexts>
<context position="3436" citStr="Parikh et al., 2011" startWordPosition="535" endWordPosition="538">ure vectors—we make use of features corresponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then describe the spectral </context>
</contexts>
<marker>Parikh, Song, Xing, 2011</marker>
<rawString>A. Parikh, L. Song, and E. P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of The 28th International Conference on Machine Learningy (ICML 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="32249" citStr="Petrov and Klein (2007)" startWordPosition="5978" endWordPosition="5981">s as done in the spectral model leads to better generalization. It is important to note that the results for both EM and the spectral algorithm are comparable to state of the art, but there are other results previously reported in the literature which are higher. For example, Hiroyuki et al. (2012) report an accuracy of 92.4 Fl on section 23 of the Penn WSJ treebank using a Bayesian tree substitution grammar; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 Fl accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 Fl, using L-PCFGs, but with a split-merge training procedure. Collins (2003) reports an accuracy of 88.2 Fl, which is comparable to the results in this paper. 5 Conclusion The spectral learning algorithm gives the same level of accuracy as EM in our experiments, but has significantly faster training times. There are several areas for future work. There are a large number of parameters in the model, and we suspect that more sophisticated regularization methods than the smoothing method we have described may improve performance. Future work should also investigate oth</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1324" citStr="Petrov et al., 2006" startWordPosition="188" endWordPosition="191">lexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the d</context>
<context position="23496" citStr="Petrov et al. (2006)" startWordPosition="4401" endWordPosition="4404">stimation method. We give comparisons to the EM algorithm, considering both speed of training, and accuracy of the resulting model; we also give experiments investigating the various choices described in the previous section. E(o,t(2),t(3)) Ea→b c ∈Qa→b c i,j,· = Zi(o) × Yj(t(2)) |Qa→b c| Ea→b c = ·,·,k 153 We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siddiqi</author>
<author>B Boots</author>
<author>G Gordon</author>
</authors>
<title>Reducedrank hidden markov models.</title>
<date>2010</date>
<journal>JMLR,</journal>
<pages>9--741</pages>
<contexts>
<context position="3415" citStr="Siddiqi et al., 2010" startWordPosition="531" endWordPosition="534"> outside trees to feature vectors—we make use of features corresponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this section we first give basic definitions for LPCFGs, and then d</context>
</contexts>
<marker>Siddiqi, Boots, Gordon, 2010</marker>
<rawString>S. Siddiqi, B. Boots, and G. Gordon. 2010. Reducedrank hidden markov models. JMLR, 9:741–748.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vempala</author>
<author>G Wang</author>
</authors>
<title>A spectral algorithm for learning mixtures of distributions.</title>
<date>2004</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>68</volume>
<issue>4</issue>
<contexts>
<context position="3354" citStr="Vempala and Wang, 2004" startWordPosition="519" endWordPosition="522">l. The spectral algorithm requires functions mapping inside and outside trees to feature vectors—we make use of features corresponding to single level rules, and larger tree fragments composed of two or three levels of rules. We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in canonical correlation analysis. Negative values can cause issues in spectral algorithms, but we describe a solution to these problems. In recent work there has been a series of results in spectral learning algorithms for latent-variable models (Vempala and Wang, 2004; Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2012; Dhillon et al., 2012; Anandkumar et al., 2012). Most of these results are theoretical (although see Luque et al. (2012) for empirical results of spectral learning for dependency parsing). While the focus of our experiments is on parsing, our findings should be relevant to the application of spectral methods to other latent-variable models. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 2 Background In this s</context>
</contexts>
<marker>Vempala, Wang, 2004</marker>
<rawString>S. Vempala and G. Wang. 2004. A spectral algorithm for learning mixtures of distributions. Journal of Computer and System Sciences, 68(4):841–860.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>