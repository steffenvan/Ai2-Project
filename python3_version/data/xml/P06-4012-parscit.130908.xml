<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010415">
<title confidence="0.7179515">
LexNet: A Graphical Environment for Graph-Based NLP
Dragomir R. Radev, G¨unes¸ Erkan, Anthony Fader,
</title>
<author confidence="0.761625">
Patrick Jordan, Siwei Shen, and James P. Sweeney
</author>
<affiliation confidence="0.99877">
Department of Electrical Engineering and Computer Science
School of Information
Department of Mathematics
University of Michigan
</affiliation>
<address confidence="0.98426">
Ann Arbor, MI 48109
</address>
<email confidence="0.981089">
radev, gerkan, afader, prjordan, shens, jpsweeney@umich.edu
</email>
<sectionHeader confidence="0.993491" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999225588235294">
This interactive presentation describes
LexNet, a graphical environment for
graph-based NLP developed at the Uni-
versity of Michigan. LexNet includes
LexRank (for text summarization), bi-
ased LexRank (for passage retrieval), and
TUMBL (for binary classification). All
tools in the collection are based on random
walks on lexical graphs, that is graphs
where different NLP objects (e.g., sen-
tences or phrases) are represented as nodes
linked by edges proportional to the lexi-
cal similarity between the two nodes. We
will demonstrate these tools on a variety of
NLP tasks including summarization, ques-
tion answering, and prepositional phrase
attachment.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995752">
We will present a series of graph-based tools for a
variety of NLP tasks such as text summarization,
passage retrieval, prepositional phrase attachment,
and binary classification in general.
Recently proposed graph-based methods
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002b; Zhu and Ghahramani, 2002a;
Toutanova et al., 2004) are particularly well
suited for transductive learning (Vapnik, 1998;
Joachims, 1999). Transductive learning is based
on the idea (Vapnik, 1998) that instead of splitting
a learning problem into two possibly harder
problems, namely induction and deduction, one
can build a model that covers both labeled and
unlabeled data. Unlabeled data are abundant as
well as significantly cheaper than labeled data in
a variety of natural language applications. Parsing
and machine translation both offer examples of
this relationship, with unparsed text from the Web
and untranslated texts being computationally less
costly. These can then be used to supplement
manually translated and aligned corpora. Hence
transductive methods are of great potential for
NLP problems and, as a result, LexNet includes a
number of transductive methods.
</bodyText>
<sectionHeader confidence="0.936319" genericHeader="method">
2 LexRank: text summarization
</sectionHeader>
<bodyText confidence="0.999859470588235">
LexRank (Erkan and Radev, 2004) embodies the
idea of representing a text (e.g., a document or a
collection of related documents) as a graph. Each
node corresponds to a sentence in the input and the
edge between two nodes is related to the lexical
similarity (either cosine similarity or n-gram gen-
eration probability) between the two sentences.
LexRank computes the steady-state distribution of
the random walk probabilities on this similarity
graph. The LexRank score of each node gives
the probability of a random walk ending up in
that node in the long run. An extractive summary
is generated by retrieving the sentences with the
highest score in the graph. Such sentences typ-
ically correspond to the nodes that have strong
connections to other nodes with high scores in the
graph. Figure 1 demonstrates LexRank.
</bodyText>
<sectionHeader confidence="0.963017" genericHeader="method">
3 Biased LexRank: passage retrieval
</sectionHeader>
<bodyText confidence="0.999967785714286">
The basic idea behind Biased LexRank is to label
a small number of sentences (or passages) that are
relevant to a particular query and then propagate
relevance from these sentences to other (unanno-
tated) sentences. Relevance propagation is per-
formed on a bipartite graph. In that graph, one
of the modes corresponds to the sentences and
the other – to certain words from these sentences.
Each sentence is connected to the words that ap-
pear in it. Thus indirectly, each sentence is two
hops away from any other sentence that shares
words in it. Intuitively, the sentences that are
close to the labeled sentences tend to get higher
scores. However, the relevance propagation en-
</bodyText>
<page confidence="0.975928">
45
</page>
<note confidence="0.316338">
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 45–48,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.7998215">
Figure 1: A sample snapshot of LexRank. A 3-
sentence summary is produced from a set of 11
related input sentences. The summary sentences
are shown as larger squares.
</figureCaption>
<bodyText confidence="0.951704333333333">
ables us to mark certain sentences that are not im-
mediate neighbors of the labeled sentences via in-
direct connections. The effect of the propagation
is discounted by a parameter at each step so that
the relationships between closer nodes are favored
more. Biased LexRank also allows for negative
relevance to be propagated through the network as
the example shows. See Figures 2– 3 for a demon-
stration of Biased LexRank.
</bodyText>
<figureCaption confidence="0.9337925">
Figure 2: Display of Biased LexRank. One sen-
tence at the top is annotated as positive while an-
other at the bottom is marked negative. Sentences
are displayed as circles and the word features are
shown as squares.
Figure 3: After convergence of Biased LexRank.
</figureCaption>
<sectionHeader confidence="0.9058175" genericHeader="method">
4 TUMBL: prepositional phrase
attachment
</sectionHeader>
<bodyText confidence="0.999959041666667">
A number of NLP problems such as word sense
disambiguation, text categorization, and extractive
summarization can be cast as classification prob-
lems. This fact is used to great effect in the de-
sign and application of many machine learning
methods used in modern NLP, including TUMBL,
through the utilization of vector representations.
Each object is represented as a vector of fea-
tures. The main assumption made is that a pair of
objects and will be classified the same way
if the distance between them in some space is
small (Zhu and Ghahramani, 2002a).
This algorithm propagates polarity information
first from the labeled data to the features, capturing
whether each feature is more indicative of posi-
tive class or more negative learned. Such informa-
tion is further transferred to the unlabeled set. The
backward steps update feature polarity with infor-
mation learned from the structure of the unlabeled
data. This process is repeated with a damping fac-
tor to discount later rounds. This process is illus-
tracted in Figure 4. TUMBL was first described
in (Radev, 2004). A series of snapshots showing
TUMBL in Figures 5– 7.
</bodyText>
<sectionHeader confidence="0.996179" genericHeader="method">
5 Technical information
</sectionHeader>
<subsectionHeader confidence="0.975546">
5.1 Code implementation
</subsectionHeader>
<bodyText confidence="0.999923833333333">
The LexRank and TUMBL demonstrations are
provided as both an applet and an application.
The user is presented with a graphical visualiza-
tion of the algorithm that was conveniently de-
veloped using the JUNG API (http://jung.
sourceforge.net/faq.html).
</bodyText>
<page confidence="0.999579">
46
</page>
<figureCaption confidence="0.758559916666667">
Figure 5: A 10-pp prepositional phrase attachment
problem is displayed. The head of each preposi-
tional phrase is ine middle column. Four types of
features are represented in four columns. The first
column is Noun1 in the 4-tuple. The second col-
umn is Noun2. The first column from the right is
verb of the 4-tuple while the second column from
the right is the actual head of the prepositional
phrase. At this time one positive and one negative
example (high and low attachment) are annotated.
The rest of the circles correspond to the unlabeled
examples.
</figureCaption>
<figure confidence="0.989892666666667">
(a) Initial graph (b) Forward pass
(c) Backward (d) Convergence
pass
</figure>
<figureCaption confidence="0.999734">
Figure 4: TUMBL snapshots: the circular vertices
are objects while the square vertices are features.
(a) The initial graph with features showing no bias.
(b) The forward pass where objects propagate la-
bels forward. (c) The backward pass where fea-
tures propagate labels backward. (d) Convergence
of the TUMBL algorithm after successive itera-
tions.
Figure 6: The final configuration.
</figureCaption>
<page confidence="0.994355">
47
</page>
<figureCaption confidence="0.6936245">
Figure 7: XML file corresponding to the PP at-
tachment problem. The XML DTD allows layout
information to be encoded along with algorithmic
information such as label and polarity.
</figureCaption>
<bodyText confidence="0.999865947368421">
In TUMBL, each object is represented by a cir-
cular vertex in the graph and each feature as a
square. Vertices are assigned a color according to
their label. The colors are assignable by the user
and designate the probability of membership of a
class.
To allow for a range of uses, data can be
entered either though the GUI or read in from
an XML file. The schema for TUMBL files is
shown athttp://tangra.si.umich.edu/
clair/tumbl.
In the LexRank demo, each sentence becomes a
node. Selected nodes for the summary are shown
in larger size and in blue while the rest are smaller
and drawn in red. The link between two nodes has
a weight proportional to the lexical similarity be-
tween the two corresponding sentences. The demo
also reports the metrics precision, recall, and F-
measure.
</bodyText>
<subsectionHeader confidence="0.99278">
5.2 Availability
</subsectionHeader>
<bodyText confidence="0.9990028">
The demos are available both as locally based and
remotely accessible from http://tangra.
si.umich.edu/clair/lexrank and
http://tangra.si.umich.edu/clair/
tumbl.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9989515">
This work was partially supported by the U.S.
National Science Foundation under the follow-
ing two grants: 0329043 “Probabilistic and link-
based Methods for Exploiting Very Large Textual
Repositories” administered through the IDM pro-
gram and 0308024 “Collaborative Research: Se-
mantic Entity and Relation Extraction from Web-
Scale Text Document Collections” run by the HLC
program. All opinions, findings, conclusions, and
recommendations in this paper are made by the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99947932">
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Arti�cial Intelligence Research
(JAIR).
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML ’99.
Dragomir Radev. 2004. Weakly supervised graph-
based methods for classification. Technical Report
CSE-TR-500-04, University of Michigan.
Martin Szummer and Tommi Jaakkola. 2001. Partially
labeled classification with Markov random walks. In
NIPS ’01, volume 14. MIT Pres.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk mod-
els for inducing word dependency distributions. In
ICML ’04, New York, New York, USA. ACM Press.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Xiaojin Zhu and Zoubin Ghahramani. 2002a. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, CMU-CALD-02-107.
Xiaojin Zhu and Zoubin Ghahramani. 2002b. Towards
semi-supervised classification with Markov random
fields. Technical report, CMU-CALD-02-106.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913552">
<title confidence="0.999921">LexNet: A Graphical Environment for Graph-Based NLP</title>
<author confidence="0.9892445">R Radev</author>
<author confidence="0.9892445">Anthony Fader Erkan</author>
<author confidence="0.9892445">Patrick Jordan</author>
<author confidence="0.9892445">Siwei Shen</author>
<author confidence="0.9892445">James P Sweeney</author>
<affiliation confidence="0.99989675">Department of Electrical Engineering and Computer Science School of Information Department of Mathematics University of Michigan</affiliation>
<address confidence="0.999865">Ann Arbor, MI 48109</address>
<email confidence="0.99961">radev,gerkan,afader,prjordan,shens,jpsweeney@umich.edu</email>
<abstract confidence="0.996006166666667">This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan. LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification). All tools in the collection are based on random on that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Arti�cial Intelligence Research (JAIR).</journal>
<contexts>
<context position="2257" citStr="Erkan and Radev, 2004" startWordPosition="325" endWordPosition="328">uild a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text from the Web and untranslated texts being computationally less costly. These can then be used to supplement manually translated and aligned corpora. Hence transductive methods are of great potential for NLP problems and, as a result, LexNet includes a number of transductive methods. 2 LexRank: text summarization LexRank (Erkan and Radev, 2004) embodies the idea of representing a text (e.g., a document or a collection of related documents) as a graph. Each node corresponds to a sentence in the input and the edge between two nodes is related to the lexical similarity (either cosine similarity or n-gram generation probability) between the two sentences. LexRank computes the steady-state distribution of the random walk probabilities on this similarity graph. The LexRank score of each node gives the probability of a random walk ending up in that node in the long run. An extractive summary is generated by retrieving the sentences with th</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Arti�cial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In ICML ’99.</booktitle>
<contexts>
<context position="1454" citStr="Joachims, 1999" startWordPosition="206" endWordPosition="207">ical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 1 Introduction We will present a series of graph-based tools for a variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general. Recently proposed graph-based methods (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002b; Zhu and Ghahramani, 2002a; Toutanova et al., 2004) are particularly well suited for transductive learning (Vapnik, 1998; Joachims, 1999). Transductive learning is based on the idea (Vapnik, 1998) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text from the Web and untranslated texts being computationally less costly. These can then be used to supplement manually translated and aligned co</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In ICML ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
</authors>
<title>Weakly supervised graphbased methods for classification.</title>
<date>2004</date>
<tech>Technical Report CSE-TR-500-04,</tech>
<institution>University of Michigan.</institution>
<contexts>
<context position="2257" citStr="Radev, 2004" startWordPosition="327" endWordPosition="328">el that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text from the Web and untranslated texts being computationally less costly. These can then be used to supplement manually translated and aligned corpora. Hence transductive methods are of great potential for NLP problems and, as a result, LexNet includes a number of transductive methods. 2 LexRank: text summarization LexRank (Erkan and Radev, 2004) embodies the idea of representing a text (e.g., a document or a collection of related documents) as a graph. Each node corresponds to a sentence in the input and the edge between two nodes is related to the lexical similarity (either cosine similarity or n-gram generation probability) between the two sentences. LexRank computes the steady-state distribution of the random walk probabilities on this similarity graph. The LexRank score of each node gives the probability of a random walk ending up in that node in the long run. An extractive summary is generated by retrieving the sentences with th</context>
<context position="5866" citStr="Radev, 2004" startWordPosition="920" endWordPosition="921">ed the same way if the distance between them in some space is small (Zhu and Ghahramani, 2002a). This algorithm propagates polarity information first from the labeled data to the features, capturing whether each feature is more indicative of positive class or more negative learned. Such information is further transferred to the unlabeled set. The backward steps update feature polarity with information learned from the structure of the unlabeled data. This process is repeated with a damping factor to discount later rounds. This process is illustracted in Figure 4. TUMBL was first described in (Radev, 2004). A series of snapshots showing TUMBL in Figures 5– 7. 5 Technical information 5.1 Code implementation The LexRank and TUMBL demonstrations are provided as both an applet and an application. The user is presented with a graphical visualization of the algorithm that was conveniently developed using the JUNG API (http://jung. sourceforge.net/faq.html). 46 Figure 5: A 10-pp prepositional phrase attachment problem is displayed. The head of each prepositional phrase is ine middle column. Four types of features are represented in four columns. The first column is Noun1 in the 4-tuple. The second col</context>
</contexts>
<marker>Radev, 2004</marker>
<rawString>Dragomir Radev. 2004. Weakly supervised graphbased methods for classification. Technical Report CSE-TR-500-04, University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Szummer</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Partially labeled classification with Markov random walks.</title>
<date>2001</date>
<booktitle>In NIPS ’01,</booktitle>
<volume>14</volume>
<publisher>MIT Pres.</publisher>
<contexts>
<context position="1289" citStr="Szummer and Jaakkola, 2001" startWordPosition="180" endWordPosition="183">ased on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 1 Introduction We will present a series of graph-based tools for a variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general. Recently proposed graph-based methods (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002b; Zhu and Ghahramani, 2002a; Toutanova et al., 2004) are particularly well suited for transductive learning (Vapnik, 1998; Joachims, 1999). Transductive learning is based on the idea (Vapnik, 1998) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationsh</context>
</contexts>
<marker>Szummer, Jaakkola, 2001</marker>
<rawString>Martin Szummer and Tommi Jaakkola. 2001. Partially labeled classification with Markov random walks. In NIPS ’01, volume 14. MIT Pres.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning random walk models for inducing word dependency distributions.</title>
<date>2004</date>
<booktitle>In ICML ’04,</booktitle>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="1368" citStr="Toutanova et al., 2004" startWordPosition="193" endWordPosition="196"> (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 1 Introduction We will present a series of graph-based tools for a variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general. Recently proposed graph-based methods (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002b; Zhu and Ghahramani, 2002a; Toutanova et al., 2004) are particularly well suited for transductive learning (Vapnik, 1998; Joachims, 1999). Transductive learning is based on the idea (Vapnik, 1998) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text from the Web and untranslated texts being computationall</context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, and Andrew Y. Ng. 2004. Learning random walk models for inducing word dependency distributions. In ICML ’04, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="1437" citStr="Vapnik, 1998" startWordPosition="204" endWordPosition="205">nal to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 1 Introduction We will present a series of graph-based tools for a variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general. Recently proposed graph-based methods (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002b; Zhu and Ghahramani, 2002a; Toutanova et al., 2004) are particularly well suited for transductive learning (Vapnik, 1998; Joachims, 1999). Transductive learning is based on the idea (Vapnik, 1998) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text from the Web and untranslated texts being computationally less costly. These can then be used to supplement manually translat</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report,</tech>
<pages>02--107</pages>
<contexts>
<context position="1315" citStr="Zhu and Ghahramani, 2002" startWordPosition="184" endWordPosition="188">cal graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 1 Introduction We will present a series of graph-based tools for a variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general. Recently proposed graph-based methods (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002b; Zhu and Ghahramani, 2002a; Toutanova et al., 2004) are particularly well suited for transductive learning (Vapnik, 1998; Joachims, 1999). Transductive learning is based on the idea (Vapnik, 1998) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text fro</context>
<context position="5347" citStr="Zhu and Ghahramani, 2002" startWordPosition="835" endWordPosition="838"> convergence of Biased LexRank. 4 TUMBL: prepositional phrase attachment A number of NLP problems such as word sense disambiguation, text categorization, and extractive summarization can be cast as classification problems. This fact is used to great effect in the design and application of many machine learning methods used in modern NLP, including TUMBL, through the utilization of vector representations. Each object is represented as a vector of features. The main assumption made is that a pair of objects and will be classified the same way if the distance between them in some space is small (Zhu and Ghahramani, 2002a). This algorithm propagates polarity information first from the labeled data to the features, capturing whether each feature is more indicative of positive class or more negative learned. Such information is further transferred to the unlabeled set. The backward steps update feature polarity with information learned from the structure of the unlabeled data. This process is repeated with a damping factor to discount later rounds. This process is illustracted in Figure 4. TUMBL was first described in (Radev, 2004). A series of snapshots showing TUMBL in Figures 5– 7. 5 Technical information 5.</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002a. Learning from labeled and unlabeled data with label propagation. Technical report, CMU-CALD-02-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Towards semi-supervised classification with Markov random fields.</title>
<date>2002</date>
<tech>Technical report,</tech>
<pages>02--106</pages>
<contexts>
<context position="1315" citStr="Zhu and Ghahramani, 2002" startWordPosition="184" endWordPosition="188">cal graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment. 1 Introduction We will present a series of graph-based tools for a variety of NLP tasks such as text summarization, passage retrieval, prepositional phrase attachment, and binary classification in general. Recently proposed graph-based methods (Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002b; Zhu and Ghahramani, 2002a; Toutanova et al., 2004) are particularly well suited for transductive learning (Vapnik, 1998; Joachims, 1999). Transductive learning is based on the idea (Vapnik, 1998) that instead of splitting a learning problem into two possibly harder problems, namely induction and deduction, one can build a model that covers both labeled and unlabeled data. Unlabeled data are abundant as well as significantly cheaper than labeled data in a variety of natural language applications. Parsing and machine translation both offer examples of this relationship, with unparsed text fro</context>
<context position="5347" citStr="Zhu and Ghahramani, 2002" startWordPosition="835" endWordPosition="838"> convergence of Biased LexRank. 4 TUMBL: prepositional phrase attachment A number of NLP problems such as word sense disambiguation, text categorization, and extractive summarization can be cast as classification problems. This fact is used to great effect in the design and application of many machine learning methods used in modern NLP, including TUMBL, through the utilization of vector representations. Each object is represented as a vector of features. The main assumption made is that a pair of objects and will be classified the same way if the distance between them in some space is small (Zhu and Ghahramani, 2002a). This algorithm propagates polarity information first from the labeled data to the features, capturing whether each feature is more indicative of positive class or more negative learned. Such information is further transferred to the unlabeled set. The backward steps update feature polarity with information learned from the structure of the unlabeled data. This process is repeated with a damping factor to discount later rounds. This process is illustracted in Figure 4. TUMBL was first described in (Radev, 2004). A series of snapshots showing TUMBL in Figures 5– 7. 5 Technical information 5.</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002b. Towards semi-supervised classification with Markov random fields. Technical report, CMU-CALD-02-106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>