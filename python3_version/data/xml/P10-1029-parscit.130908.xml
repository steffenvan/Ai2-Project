<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000279">
<title confidence="0.986839">
Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing
</title>
<author confidence="0.9987">
Ruihong Huang and Ellen Riloff
</author>
<affiliation confidence="0.876378666666667">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.999517">
{huangrh,riloff}@cs.utah.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954476190476">
This research explores the idea of inducing
domain-specific semantic class taggers us-
ing only a domain-specific text collection
and seed words. The learning process be-
gins by inducing a classifier that only has
access to contextual features, forcing it to
generalize beyond the seeds. The contex-
tual classifier then labels new instances,
to expand and diversify the training set.
Next, a cross-category bootstrapping pro-
cess simultaneously trains a suite of clas-
sifiers for multiple semantic classes. The
positive instances for one class are used as
negative instances for the others in an it-
erative bootstrapping cycle. We also ex-
plore a one-semantic-class-per-discourse
heuristic, and use the classifiers to dynam-
ically create semantic features. We eval-
uate our approach by inducing six seman-
tic taggers from a collection of veterinary
medicine message board posts.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942413793103">
The goal of our research is to create semantic class
taggers that can assign a semantic class label to ev-
ery noun phrase in a sentence. For example, con-
sider the sentence: “The lab mix was diagnosed
with parvo and given abx”. A semantic tagger
should identify the “the lab mix” as an ANIMAL,
“parvo” as a DISEASE, and “abx” (antibiotics)
as a DRUG. Accurate semantic tagging could be
beneficial for many NLP tasks, including coref-
erence resolution and word sense disambiguation,
and many NLP applications, such as event extrac-
tion systems and question answering technology.
Semantic class tagging has been the subject of
previous research, primarily under the guises of
named entity recognition (NER) and mention de-
tection. Named entity recognizers perform se-
mantic tagging on proper name noun phrases, and
sometimes temporal and numeric expressions as
well. The mention detection task was introduced
in recent ACE evaluations (e.g., (ACE, 2007;
ACE, 2008)) and requires systems to identify all
noun phrases (proper names, nominals, and pro-
nouns) that correspond to 5-7 semantic classes.
Despite widespread interest in semantic tag-
ging, nearly all semantic taggers for comprehen-
sive NP tagging still rely on supervised learn-
ing, which requires annotated data for training.
A few annotated corpora exist, but they are rela-
tively small and most were developed for broad-
coverage NLP. Many domains, however, are re-
plete with specialized terminology and jargon that
cannot be adequately handled by general-purpose
systems. Domains such as biology, medicine, and
law are teeming with specialized vocabulary that
necessitates training on domain-specific corpora.
Our research explores the idea of inducing
domain-specific semantic taggers using a small
set of seed words as the only form of human su-
pervision. Given an (unannotated) collection of
domain-specific text, we automatically generate
training instances by labelling every instance of a
seed word with its designated semantic class. We
then train a classifier to do semantic tagging using
these seed-based annotations, using bootstrapping
to iteratively improve performance.
On the surface, this approach appears to be a
contradiction. The classifier must learn how to as-
sign different semantic tags to different instances
of the same word based on context (e.g., “lab”
may refer to an animal in one context but a labora-
tory in another). And yet, we plan to train the clas-
sifier using stand-alone seed words, making the as-
sumption that every instance of the seed belongs to
the same semantic class. We resolve this apparent
contradiction by using semantically unambiguous
seeds and by introducing an initial context-only
training phase before bootstrapping begins. First,
we train a strictly contextual classifier that only
</bodyText>
<page confidence="0.975361">
275
</page>
<note confidence="0.943197">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 275–285,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999968575757576">
has access to contextual features and cannot see
the seed. Then we apply the classifier to the corpus
to automatically label new instances, and combine
these new instances with the seed-based instances.
This process expands and diversifies the training
set to fuel subsequent bootstrapping.
Another challenge is that we want to use a small
set of seeds to minimize the amount of human ef-
fort, and then use bootstrapping to fully exploit
the domain-specific corpus. Iterative self-training,
however, often has difficulty sustaining momen-
tum or it succumbs to semantic drift (Komachi
et al., 2008; McIntosh and Curran, 2009). To
address these issues, we simultaneously induce
a suite of classifiers for multiple semantic cat-
egories, using the positive instances of one se-
mantic category as negative instances for the oth-
ers. As bootstrapping progresses, the classifiers
gradually improve themselves, and each other,
over many iterations. We also explore a one-
semantic-class-per-discourse (OSCPD) heuristic
that infuses the learning process with fresh train-
ing instances, which may be substantially differ-
ent from the ones seen previously, and we use the
labels produced by the classifiers to dynamically
create semantic features.
We evaluate our approach by creating six se-
mantic taggers using a collection of message board
posts in the domain of veterinary medicine. Our
results show this approach produces high-quality
semantic taggers after a sustained bootstrapping
cycle that maintains good precision while steadily
increasing recall over many iterations.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999526714285714">
Semantic class tagging is most closely related to
named entity recognition (NER), mention detec-
tion, and semantic lexicon induction. NER sys-
tems (e.g., (Bikel et al., 1997; Collins and Singer,
1999; Cucerzan and Yarowsky, 1999; Fleischman
and Hovy, 2002) identify proper named entities,
such as people, organizations, and locations. Sev-
eral bootstrapping methods for NER have been
previously developed (e.g., (Collins and Singer,
1999; Niu et al., 2003)). NER systems, how-
ever, do not identify nominal NP instances (e.g.,
“a software manufacturer” or “the beach”), or han-
dle semantic classes that are not associated with
proper named entities (e.g., symptoms).1 ACE
</bodyText>
<footnote confidence="0.53984">
1Some NER systems also handle specialized constructs
such as dates and monetary amounts.
</footnote>
<bodyText confidence="0.999852705882353">
mention detection systems (e.g., see (ACE, 2005;
ACE, 2007; ACE, 2008)) require tagging of NPs
that correspond to 5-7 general semantic classes.
These systems are typically trained with super-
vised learning using annotated corpora, although
techniques have been developed to use resources
for one language to train systems for different lan-
guages (e.g., (Zitouni and Florian, 2009)).
Another line of relevant work is semantic class
induction (e.g., (Riloff and Shepherd, 1997; Roark
and Charniak, 1998; Thelen and Riloff, 2002; Ng,
2007; McIntosh and Curran, 2009), where the goal
is to induce a stand-alone dictionary of words with
semantic class labels. These techniques are of-
ten designed to learn specialized terminology from
unannotated domain-specific texts via bootstrap-
ping. Our work, however, focuses on classifica-
tion of NP instances in context, so the same phrase
may be assigned to different semantic classes in
different contexts. Consequently, our classifier
can also assign semantic class labels to pronouns.
There has also been work on extracting seman-
tically related terms or category members from
the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005;
Kozareva et al., 2008; Carlson et al., 2009)). These
techniques harvest broad-coverage semantic infor-
mation from the Web using patterns and statistics,
typically for the purpose of knowledge acquisi-
tion. Importantly, our goal is to classify instances
in context, rather than generate lists of terms. In
addition, the goal of our research is to learn spe-
cialized terms and jargon that may not be common
on the Web, as well as domain-specific usages that
may differ from the norm (e.g., “mix” and “lab”
are usually ANIMALS in our domain).
The idea of simulataneously learning multiple
semantic categories to prevent semantic drift has
been explored for other tasks, such as semantic
lexicon induction (Thelen and Riloff, 2002; McIn-
tosh and Curran, 2009) and pattern learning (Yan-
garber, 2003). Our bootstrapping model can be
viewed as a form of self-training (e.g., (Ng and
Cardie, 2003; Mihalcea, 2004; McClosky et al.,
2006)), and cross-category training is similar in
spirit to co-training (e.g., (Blum and Mitchell,
1998; Collins and Singer, 1999; Riloff and Jones,
1999; Mueller et al., 2002; Phillips and Riloff,
2002)). But, importantly, our classifiers all use the
same feature set so they do not represent indepen-
dent views of the data. They do, however, offer
slightly different perspectives because each is at-
</bodyText>
<page confidence="0.996378">
276
</page>
<bodyText confidence="0.981657">
tempting to recognize a different semantic class.
</bodyText>
<subsectionHeader confidence="0.880561">
3 Bootstrapping an Instance-based
Semantic Class Tagger from Seeds
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999925508474576">
Our goal is to create a bootstrapping model that
can rapidly create semantic class taggers using
just a small set of seed words and an unanno-
tated domain-specific corpus. Our motivation
comes from specialized domains that cannot be
adequately handled by general-purpose NLP sys-
tems. As an example of such a domain, we have
been working with a collection of message board
posts in the field of veterinary medicine. Given a
document, we want a semantic class tagger to label
every NP with a semantic category, for example:
[A 14yo doxy]ANIMAL owned by
[a reputable breeder]HUMAN is be-
ing treated for [IBD]DISEASE with
[pred]DRUG.
When we began working with these texts, we
were immediately confronted by a dizzying array
of non-standard words and word uses. In addition
to formal veterinary vocabulary (e.g., animal dis-
eases), veterinarians often use informal, shorthand
terms when posting on-line. For example, they
frequently refer to breeds using “nicknames” or
shortened terms (e.g., gshep for German shepherd,
doxy for dachsund, bxr for boxer, labx for labrador
cross). Often, they refer to animals based solely on
their physical characteristics, for example “a dlh”
(domestic long hair), “a m/n” (male, neutered), or
“a 2yo” (2 year old). This phenomenon occurs
with other semantic categories as well, such as
drugs and medical tests (e.g., pred for prednisone,
and rads for radiographs).
Nearly all semantic class taggers are trained us-
ing supervised learning with manually annotated
data. However, annotated data is rarely available
for specialized domains, and it is expensive to ob-
tain because domain experts must do the annota-
tion work. So we set out to create a bootstrapping
model that can rapidly create domain-specific se-
mantic taggers using just a few seed words and a
domain-specific text collection.
Our bootstrapping model consists of two dis-
tinct phases. First, we train strictly contextual
classifiers from the seed annotations. We then ap-
ply the classifiers to the unlabeled data to gener-
ate new annotated instances that are added to the
training set. Second, we employ a cross-category
bootstrapping process that simultaneously trains
a suite of classifiers for multiple semantic cate-
gories, using the positive instances for one se-
mantic class as negative instances for the oth-
ers. This cross-category training process gives
the learner sustained momentum over many boot-
strapping iterations. Finally, we explore two ad-
ditional enhancements: (1) a one-semantic-class-
per-discourse heuristic to automatically generate
new training instances, and (2) dynamically cre-
ated semantic features produced by the classifiers
themselves. In the following sections, we explain
each of these steps in detail.
</bodyText>
<subsectionHeader confidence="0.999299">
3.2 Phase 1: Inducing a Contextual Classifier
</subsectionHeader>
<bodyText confidence="0.999992714285714">
The main challenge that we faced was how to train
an instance-based classifier using seed words as
the only form of human supervision. First, the user
must provide a small set of seed words that are
relatively unambiguous (e.g., “dog” will nearly
always refer to an animal in our domain). But
even so, training a traditional classifier from seed-
based instances would likely produce a classifier
that learns to recognize the seeds but is unable to
classify new examples. We needed to force the
classifier to generalize beyond the seed words.
Our solution was to introduce an initial train-
ing step that induces a strictly contextual classifier.
First, we generate training instances by automati-
cally labeling each instance of a seed word with
its designated semantic class. However, when we
create feature vectors for the classifier, the seeds
themselves are hidden and only contextual fea-
tures are used to represent each training instance.
By essentially “masking” the seed words so the
classifier can only see the contexts around them,
we force the classifier to generalize.
We create a suite of strictly contextual classi-
fiers, one for each semantic category. Each classi-
fier makes a binary decision as to whether a noun
phrase belongs to its semantic category. We use
the seed words for category Ck to generate posi-
tive training instances for the Ck classifier, and the
seed words for all other categories to generate the
negative training instances for Ck.
We use an in-house sentence segmenter and NP
chunker to identify the base NPs in each sentence
and create feature vectors that represent each con-
stituent in the sentence as either an NP or an in-
dividual word. For each seed word, the feature
</bodyText>
<page confidence="0.976407">
277
</page>
<bodyText confidence="0.999790052631579">
vector captures a context window of 3 constituents
(word or NP) to its left and 3 constituents (word
or NP) to its right. Each constituent is represented
with a lexical feature: for NPs, we use its head
noun; for individual words, we use the word itself.
The seed word, however, is discarded so that the
classifier is essentially blind-folded and cannot see
the seed that produced the training instance. We
also create a feature for every modifier that pre-
cedes the head noun in the target NP, except for
articles which are discarded. As an example, con-
sider the following sentence:
Fluffy was diagnosed with FELV after a
blood test showed that he tested positive.
Suppose that “FELV” is a seed for the DISEASE
category and “test” is a seed for the TEST cate-
gory. Two training instances would be created,
with feature vectors that look like this, where M
represents a modifier inside the target NP:
</bodyText>
<equation confidence="0.99906075">
was−3 diagnosed−2 with−1 after1 test2
showed3 ==&gt;- DISEASE
with−3 FELV−2 after−1 bloodM showed1
that2 he3 ==&gt;- TEST
</equation>
<bodyText confidence="0.999866083333333">
The contextual classifiers are then applied to the
corpus to automatically label new instances. We
use a confidence score to label only the instances
that the classifiers are most certain about. We com-
pute a confidence score for instance i with respect
to semantic class Ck by considering both the score
of the Ck classifier as well as the scores of the
competing classifiers. Intuitively, we have confi-
dence in labeling an instance as category Ck if the
Ck classifier gave it a positive score, and its score
is much higher than the score of any other classi-
fier. We use the following scoring function:
</bodyText>
<equation confidence="0.999705">
Confidence(i,Ck) =
score(i,Ck) - max(Vj6�k score(i,Cj))
</equation>
<bodyText confidence="0.999979764705882">
We employ support vector machines (SVMs)
(Joachims, 1999) with a linear kernel as our classi-
fiers, using the SVMlin software (Keerthi and De-
Coste, 2005). We use the value produced by the
decision function (essentially the distance from
the hyperplane) as the score for a classifier. We
specify a threshold Bcf and only assign a semantic
tag Ck to an instance i if Confidence(i,Ck) &gt; Bcf.
All instances that pass the confidence thresh-
old are labeled and added to the training set.
This process greatly enhances the diversity of
the training data. In this initial learning step,
the strictly contextual classifiers substantially in-
crease the number of training instances for each
semantic category, producing a more diverse mix
of seed-generated instances and context-generated
instances.
</bodyText>
<subsectionHeader confidence="0.998942">
3.3 Phase 2: Cross-Category Bootstrapping
</subsectionHeader>
<bodyText confidence="0.976071">
The next phase of the learning process is an iter-
ative bootstrapping procedure. The key challenge
was to design a bootstrapping model that would
not succumb to semantic drift and would have sus-
tained momentum to continue learning over many
iterations.
Figure 1 shows the design of our cross-category
bootstrapping model.2 We simultaneously train a
suite of binary classifiers, one for each semantic
category, C1 ... Cn. After each training cycle,
all of the classifiers are applied to the remaining
unlabeled instances and each classifier labels the
(positive) instances that it is most confident about
(i.e., the instances that it classifies with a confi-
dence score &gt; Bcf). The set of instances positively
labeled by classifier Ck are shown as Ck in Figure
1. All of the new instances produced by classifier
Ck are then added to the set of positive training
instances for Ck and to the set of negative training
instances for all of the other classifiers.
One potential problem with this scheme is that
some categories are more prolific than others, plus
we are collecting negative instances from a set
of competing classifiers. Consequently, this ap-
proach can produce highly imbalanced training
sets. Therefore we enforced a 3:1 ratio of nega-
tives to positives by randomly selecting a subset
of the possible negative instances. We discuss this
issue further in Section 4.4.
</bodyText>
<footnote confidence="0.894596">
2For simplicity, this picture does not depict the initial con-
textual training step, but that can be viewed as the first itera-
tion in this general framework.
</footnote>
<figure confidence="0.99460024137931">
labeled
C�
2
C�
1
C�
n
seeds
C�
1
(+)
C�
i=1
( )
_
C�
i=n
( )
_
unlabeled
C�2 C�i=2
(+) ( )
_
C�
n
(+)
C1
C2
Cn
</figure>
<figureCaption confidence="0.998884">
Figure 1: Cross-Category Bootstrapping
</figureCaption>
<page confidence="0.975572">
278
</page>
<bodyText confidence="0.999941714285714">
Cross-category training has two advantages
over independent self-training. First, as oth-
ers have shown for pattern learning and lexicon
induction (Thelen and Riloff, 2002; Yangarber,
2003; McIntosh and Curran, 2009), simultane-
ously training classifiers for multiple categories
reduces semantic drift because each classifier is
deterred from encroaching on another one’s terri-
tory (i.e., claiming the instances from a compet-
ing class as its own). Second, similar in spirit to
co-training3, this approach allows each classifier
to obtain new training instances from an outside
source that has a slightly different perspective.
While independent self-training can quickly run
out of steam, cross-category training supplies each
classifier with a constant stream of new (negative)
instances produced by competing classifiers. In
Section 4, we will show that cross-category boot-
strapping performs substantially better than an in-
dependent self-training model, where each classi-
fier is bootstrapped separately.
The feature set for these classifiers is exactly the
same as described in Section 3.2, except that we
add a new lexical feature that represents the head
noun of the target NP (i.e., the NP that needs to be
tagged). This allows the classifiers to consider the
local context as well as the target word itself when
making decisions.
</bodyText>
<subsectionHeader confidence="0.985621">
3.4 One Semantic Class Per Discourse
</subsectionHeader>
<bodyText confidence="0.999987">
We also explored the idea of using a one semantic
class per discourse (OSCPD) heuristic to gener-
ate additional training instances during bootstrap-
ping. Inspired by Yarowsky’s one sense per dis-
course heuristic for word sense disambiguation
(Yarowsky, 1995), we make the assumption that
multiple instances of a word in the same discourse
will nearly always correspond to the same seman-
tic class. Since our data set consists of message
board posts organized as threads, we consider all
posts in the same thread to be a single discourse.
After each training step, we apply the classi-
fiers to the unlabeled data to label some new in-
stances. For each newly labeled instance, the OS-
CPD heuristic collects all instances with the same
head noun in the same discourse (thread) and uni-
laterally labels them with the same semantic class.
This heuristic serves as meta-knowledge to label
instances that (potentially) occur in very different
</bodyText>
<footnote confidence="0.7489095">
3But technically this is not co-training because our feature
sets are all the same.
</footnote>
<bodyText confidence="0.999429384615385">
contexts, thereby infusing the bootstrapping pro-
cess with “fresh” training examples.
In early experiments, we found that OSCPD can
be aggressive, pulling in many new instances. If
the classifier labels a word incorrectly, however,
then the OSCPD heuristic will compound the er-
ror and mislabel even more instances incorrectly.
Therefore we only apply this heuristic to instances
that are labeled with extremely high confidence
(Bcf &gt; 2.5) and that pass a global sanity check,
gsc(w) &gt; 0.2, which ensures that a relatively high
proportion of labeled instances with the same head
noun have been assigned to the same semantic
class. Specifically, gsc(w) = 0.1* wl/c
wl +0.9*wu/c
wu
where wl and wu are the # of labeled and unla-
beled instances, respectively, wl/c is the # of in-
stances labeled as c, and wu/c is the # of unlabeled
instances that receive a positive confidence score
for c when given to the classifier. The intuition
behind the second term is that most instances are
initially unlabeled and we want to make sure that
many of the unlabeled instances are likely to be-
long to the same semantic class (even though the
classifier isn’t ready to commit to them yet).
</bodyText>
<subsectionHeader confidence="0.775988">
3.5 Dynamic Semantic Features
</subsectionHeader>
<bodyText confidence="0.998935565217391">
For many NLP tasks, classifiers use semantic fea-
tures to represent the semantic class of words.
These features are typically obtained from exter-
nal resources such as Wordnet (Miller, 1990). Our
bootstrapping model incrementally trains seman-
tic class taggers, so we explored the idea of using
the labels assigned by the classifiers to create en-
hanced feature vectors by dynamically adding se-
mantic features. This process allows later stages
of bootstrapping to directly benefit from earlier
stages. For example, consider the sentence:
He started the doxy on Vetsulin today.
If “Vetsulin” was labeled as a DRUG in a previ-
ous bootstrapping iteration, then the feature vector
representing the context around “doxy” can be en-
hanced to include an additional semantic feature
identifying Vetsulin as a DRUG, which would look
like this:
He−2 started−1 on1 Vetsulin2 DRUG2 today3
Intuitively, the semantic features should help the
classifier identify more general contextual pat-
terns, such as “started GX&gt; on DRUG”. To create
semantic features, we use the semantic tags that
</bodyText>
<page confidence="0.990655">
279
</page>
<bodyText confidence="0.999940125">
have been assigned to the current set of labeled in-
stances. When a feature vector is created for a tar-
get NP, we check every noun instance in its context
window to see if it has been assigned a semantic
tag, and if so, then we add a semantic feature. In
the early stages of bootstrapping, however, rela-
tively few nouns will be assigned semantic tags,
so these features are often missing.
</bodyText>
<subsectionHeader confidence="0.989853">
3.6 Thresholds and Stopping Criterion
</subsectionHeader>
<bodyText confidence="0.999958476190476">
When new instances are automatically labeled
during bootstrapping, it is critically important that
most of the labels are correct or performance
rapidly deteriorates. This suggests that we should
only label instances in which the classifier has
high confidence. On the other hand, a high thresh-
old often yields few new instances, which can
cause the bootstrapping process to sputter and halt.
To balance these competing demands, we used
a sliding threshold that begins conservatively but
gradually loosens the reins as bootstrapping pro-
gresses. Initially, we set Ocf = 2.0, which only
labels instances that the classifier is highly confi-
dent about. When fewer than min new instances
can be labeled, we automatically decrease Ocf by
0.2, allowing another batch of new instances to be
labeled, albeit with slightly less confidence. We
continue decreasing the threshold, as needed, un-
til Ocf &lt; 1.0, when we end the bootstrapping
process. In Section 4, we show that this sliding
threshold outperforms fixed threshold values.
</bodyText>
<sectionHeader confidence="0.999066" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.933064">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999901027777778">
Our data set consists of message board posts from
the Veterinary Information Network (VIN), which
is a web site (www.vin.com) for professionals in
veterinary medicine. Among other things, VIN
hosts forums where veterinarians engage in dis-
cussions about medical issues, cases in their prac-
tices, etc. Over half of the small animal veterinar-
ians in the U.S. and Canada use VIN. Analysis of
veterinary data could not only improve pet health
care, but also provide early warning signs of in-
fectious disease outbreaks, emerging zoonotic dis-
eases, exposures to environmental toxins, and con-
tamination in the food chain.
We obtained over 15,000 VIN message board
threads representing three topics: cardiology, en-
docrinology, and feline internal medicine. We did
basic cleaning, removing html tags and tokeniz-
ing numbers. For training, we used 4,629 threads,
consisting of 25,944 individual posts. We devel-
oped classifiers to identify six semantic categories:
ANIMAL, DISEASE/SYMPTOM4, DRUG, HUMAN,
TEST, and OTHER.
The message board posts contain an abundance
of veterinary terminology and jargon, so two do-
main experts5 from VIN created a test set (answer
key) for our evaluation. We defined annotation
guidelines6 for each semantic category and con-
ducted an inter-annotator agreement study to mea-
sure the consistency of the two domain experts on
30 message board posts, which contained 1,473
noun phrases. The annotators achieved a relatively
high r. score of .80. Each annotator then labeled an
additional 35 documents, which gave us a test set
containing 100 manually annotated message board
posts. The table below shows the distribution of
semantic classes in the test set.
</bodyText>
<subsectionHeader confidence="0.509742">
Animal Dis/Sym Drug Test Human Other
</subsectionHeader>
<bodyText confidence="0.978062818181818">
612 900 369 404 818 1723
To select seed words, we used the procedure
proposed by Roark and Charniak (1998), ranking
all of the head nouns in the training corpus by fre-
quency and manually selecting the first 10 nouns
that unambiguously belong to each category.7 This
process is fast, relatively objective, and guaranteed
to yield high-frequency terms, which is important
for bootstrapping. We used the Stanford part-of-
speech tagger (Toutanova et al., 2003) to identify
nouns, and our own simple rule-based NP chunker.
</bodyText>
<subsectionHeader confidence="0.981895">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.942181214285714">
To assess the difficulty of our data set and task,
we evaluated several baselines. The first baseline
searches for each head noun in WordNet and la-
bels the noun as category Ck if it has a hypernym
synset corresponding to that category. We manu-
ally identified the WordNet synsets that, to the best
of our ability, seem to most closely correspond
4We used a single category for diseases and symptoms
because our domain experts had difficulty distinguishing be-
tween them. A veterinary consultant explained that the same
term (e.g., diabetes) may be considered a symptom in one
context if it is secondary to another condition (e.g., pancre-
atitis) and a disease in a different context if it is the primary
diagnosis.
</bodyText>
<footnote confidence="0.989751428571429">
5One annotator is a veterinarian and the other is a veteri-
nary technician.
6The annotators were also allowed to label an NP as
POS Error if it was clearly misparsed. These cases were not
used in the evaluation.
7We used 20 seeds for DIS/SYM because we merged two
categories and for OTHER because it is a broad catch-all class.
</footnote>
<page confidence="0.983904">
280
</page>
<table confidence="0.999871636363636">
Method Animal Dis/Sym Drug Test Human Other Avg
BASELINES
WordNet 32/80/46 21/81/34 25/35/29 NA 62/66/64 NA 35/66/45.8
Seeds 38/100/55 14/99/25 21/97/35 29/94/45 80/99/88 18/93/30 37/98/53.1
Supervised 67/94/78 20/88/33 24/96/39 34/90/49 79/99/88 31/91/46 45/94/60.7
Ind. Self-Train I.13 61/84/71 39/80/52 53/77/62 55/70/61 81/96/88 30/82/44 58/81/67.4
CROSS-CATEGORY BOOTSTRAPPED CLASSIFIERS
Contextual I.1 59/77/67 33/84/47 42/80/55 49/77/59 82/93/87 33/80/47 53/82/64.3
XCategory I.45 86/71/78 57/82/67 70/78/74 73/65/69 85/92/89 46/82/59 75/78/76.1
XCat+OSCPD I.40 86/69/77 59/81/68 72/70/71 72/69/71 86/92/89 50/81/62 75/76/75.6
XCat+OSCPD+SF I.39 86/70/77 60/81/69 69/81/75 73/69/71 86/91/89 50/81/62 75/78/76.6
</table>
<tableCaption confidence="0.999962">
Table 1: Experimental results, reported as Recall/Precision/F score
</tableCaption>
<bodyText confidence="0.999758225806452">
to each semantic class. We do not report Word-
Net results for TEST because there did not seem
be an appropriate synset, or for the OTHER cate-
gory because that is a catch-all class. The first row
of Table 1 shows the results, which are reported
as Recall/Precision/F score8. The WordNet base-
line yields low recall (21-32%) for every category
except HUMAN, which confirms that many veteri-
nary terms are not present in WordNet. The sur-
prisingly low precision for some categories is due
to atypical word uses (e.g., patient, boy, and girl
are HUMAN in WordNet but nearly always ANI-
MALS in our domain), and overgeneralities (e.g.,
WordNet lists calcium as a DRUG).
The second baseline simply labels every in-
stance of a seed with its designated semantic class.
All non-seed instances remain unlabeled. As ex-
pected, the seeds produce high precision but low
recall. The exception is HUMAN, where 80% of
the instances match a seed word, undoubtedly be-
cause five of the ten HUMAN seeds are 1st and 2nd
person pronouns, which are extremely common.
A third baseline trains semantic classifiers using
supervised learning by performing 10-fold cross-
validation on the test set. The feature set and
classifier settings are exactly the same as with
our bootstrapped classifiers.9 Supervised learning
achieves good precision but low recall for all cate-
gories except ANIMAL and HUMAN. In the next
section, we present the experimental results for
our bootstrapped classifiers.
</bodyText>
<subsectionHeader confidence="0.747286">
4.3 Results for Bootstrapped Classifiers
</subsectionHeader>
<bodyText confidence="0.999656333333333">
The bottom section of Table 1 displays the results
for our bootstrapped classifiers. The Contextual
I.1 row shows results after just the first iteration,
</bodyText>
<footnote confidence="0.9951894">
8We use an F(1) score, where recall and precision are
equally weighted.
9For all of our classifiers, supervised and bootstrapped,
we label all instances of the seed words first and then apply
the classifiers to the unlabeled (non-seed) instances.
</footnote>
<bodyText confidence="0.9997757">
which trains only the strictly contextual classi-
fiers. The average F score improved from 53.1 for
the seeds alone to 64.3 with the contextual classi-
fiers. The next row, XCategory I.45, shows the
results after cross-category bootstrapping, which
ended after 45 iterations. (We indicate the num-
ber of iterations until bootstrapping ended using
the notation I.#.) With cross-category bootstrap-
ping, the average F score increased from 64.3 to
76.1. A closer inspection reveals that all of the se-
mantic categories except HUMAN achieved large
recall gains. And importantly, these recall gains
were obtained with relatively little loss of preci-
sion, with the exception of TEST.
Next, we measured the impact of the one-
semantic-class-per-discourse heuristic, shown as
XCat+OSCPD I.40. From Table 1, it appears that
OSCPD produced mixed results: recall increased
by 1-4 points for DIS/SYM, DRUG, HUMAN, and
OTHER, but precision was inconsistent, improv-
ing by +4 for TEST but dropping by -8 for DRUG.
However, this single snapshot in time does not tell
the full story. Figure 2 shows the performance
of the classifiers during the course of bootstrap-
ping. The OSCPD heuristic produced a steeper
learning curve, and consistently improved perfor-
mance until the last few iterations when its perfor-
mance dipped. This is probably due to the fact that
noise gradually increases during bootstrapping, so
incorrect labels are more likely and OSCPD will
compound any mistakes by the classifier. A good
future strategy might be to use the OSCPD heuris-
tic only during the early stages of bootstrapping
when the classifier’s decisions are most reliable.
We also evaluated the effect of dynamically cre-
ated semantic features. When added to the ba-
sic XCategory system, they had almost no ef-
fect. We suspect this is because the semantic fea-
tures are sparse during most of the bootstrapping
process. However, the semantic features did im-
</bodyText>
<page confidence="0.991947">
281
</page>
<figure confidence="0.989158555555556">
78
76
74
72
70
68
66
640 5 10 15 20 25 30 35 40 45
# of iterations
</figure>
<figureCaption confidence="0.999709">
Figure 2: Average F scores after each iteration
</figureCaption>
<bodyText confidence="0.999903580645161">
prove performance when coupled with the OSCPD
heuristic, presumably because the OSCPD heuris-
tic aggressively labels more instances in the earlier
stages of bootstrapping, increasing the prevalence
of semantic class tags. The XCat+OSCPD+SF
I.39 row in Table 1 shows that the semantic fea-
tures coupled with OSCPD dramatically increased
the precision for DRUG, yielding the best overall F
score of 76.6.
We conducted one additional experiment to as-
sess the benefits of cross-category bootstrapping.
We created an analogous suite of classifiers using
self-training, where each classifier independently
labels the instances that it is most confident about,
adds them only to its own training set, and then
retrains itself. The Ind. Self-Train I.13 row in
Table 1 shows that these classifiers achieved only
58% recall (compared to 75% for XCategory) and
an average F score of 67.4 (compared to 76.1 for
XCategory). One reason for the disparity is that
the self-training model ended after just 13 boot-
strapping cycles (I.13), given the same threshold
values. To see if we could push it further, we low-
ered the confidence threshold to 0 and it continued
learning through 35 iterations. Even so, its final
score was 65% recall with 79% precision, which is
still well below the 75% recall with 78% precision
produced by the XCategory model. These results
support our claim that cross-category bootstrap-
ping is more effective than independently self-
trained models.
</bodyText>
<figureCaption confidence="0.422604">
Figure 3 tracks the recall and precision scores
of the XCat+OSCPD+SF system as bootstrap-
ping progresses. This graph shows the sustained
momentum of cross-category bootstrapping: re-
</figureCaption>
<figure confidence="0.529879">
# of iterations
</figure>
<figureCaption confidence="0.974903">
Figure 3: Recall and Precision scores during
cross-category bootstrapping
</figureCaption>
<bodyText confidence="0.943633">
call steadily improves while precision stays con-
sistently high with only a slight dropoff at the end.
</bodyText>
<subsectionHeader confidence="0.99812">
4.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999991818181818">
To assess the impact of corpus size, we generated
a learning curve with randomly selected subsets
of the training texts. Figure 4 shows the average F
score of our best system using 116, 1�, 1�, 1�, 3�, and
all of the data. With just 1�6th of the training set,
the system has about 1,600 message board posts
to use for training, which yields a similar F score
(roughly 61%) as the supervised baseline that used
100 manually annotated posts via 10-fold cross-
validation. So with 16 times more text, seed-based
bootstrapping achieves roughly the same results as
supervised learning. This result reflects the natural
trade-off between supervised learning and seed-
based bootstrapping. Supervised learning exploits
manually annotated data, but must make do with
a relatively small amount of training text because
manual annotations are expensive. In contrast,
seed-based bootstrapping exploits a small number
of human-provided seeds, but needs a larger set of
(unannotated) texts for training because the seeds
produce relatively sparse annotations of the texts.
An additional advantage of seed-based boot-
strapping methods is that they can easily exploit
unlimited amounts of training text. For many do-
mains, large text collections are readily available.
Figure 4 shows a steady improvement in perfor-
mance as the amount of training text grows. Over-
all, the F score improves from roughly 61% to
nearly 77% simply by giving the system access to
more unannotated text during bootstrapping.
We also evaluated the effectiveness of our slid-
ing confidence threshold (Section 3.6). The ta-
ble below shows the results using fixed thresholds
</bodyText>
<figure confidence="0.97770636">
85
80
75
70
65
60
Precision
Recall
50
0 5 10 15 20 25 30 35 40
55
F measure (%)
independent self−training
cross−category bootstrapping
+OSCPD
+OSCPD+SemFeat
282
80
75
70
65
60
F measure (%) 40
20
0
</figure>
<figureCaption confidence="0.999978">
Figure 4: Learning Curve
</figureCaption>
<bodyText confidence="0.9999076">
of 1.0, 1.5, 2.0, as well as the sliding threshold
(which begins at 2.0 and ends at 1.0 decreasing by
0.2 when the number of newly labeled instances
falls below 3000 (i.e., &lt; 500 per category, on av-
erage). This table depicts the expected trade-off
between recall and precision for the fixed thresh-
olds, with higher thresholds producing higher pre-
cision but lower recall. The sliding threshold pro-
duces the best F score, achieving the best balance
of high recall and precision.
</bodyText>
<figure confidence="0.8883854">
Bmf R/P/F
1.0 71/77/74.1
1.5 69/81/74.7
2.0 65/82/72.4
Sliding 75/78/76.6
</figure>
<bodyText confidence="0.999819958333334">
As mentioned in Section 3.3, we used 3 times
as many negative instances as positive instances
for every semantic category during bootstrap-
ping. This ratio was based on early experiments
where we needed to limit the number of neg-
ative instances per category because the cross-
category framework naturally produces an ex-
tremely skewed negative/positive training set. We
revisited this issue to empirically assess the impact
of the negative/positive ratio on performance. The
table below shows recall, precision, and F score
results when we vary the ratio from 1:1 to 5:1. A
1:1 ratio seems to be too conservative, improving
precision a bit but lowering recall. However the
difference in performance between the other ra-
tios is small. Our conclusion is that a 1:1 ratio is
too restrictive but, in general, the cross-category
bootstrapping process is relatively insensitive to
the specific negative/positive ratio used. Our ob-
servation from preliminary experiments, however,
is that the negative/positive ratio does need to be
controlled, or else the dominant categories over-
whelm the less frequent categories with negative
instances.
</bodyText>
<table confidence="0.478064166666667">
Neg:Pos R/P/F
1:1 72/79/75.2
2:1 74/78/76.1
3:1 75/78/76.6
4:1 75/77/76.0
5:1 76/77/76.4
</table>
<bodyText confidence="0.999340384615385">
Finally, we examined performance on gendered
pronouns (he/she/him/her), which can refer to ei-
ther animals or people in the veterinary domain.
84% (220/261) of the gendered pronouns were an-
notated as ANIMAL in the test set. Our classi-
fier achieved 95% recall (209/220) and 90% preci-
sion (209/232) for ANIMAL and 15% recall (6/41)
and 100% precision (6/6) for HUMAN. So while
it failed to recognize most of the (relatively few)
gendered pronouns that refer to a person, it was
highly effective at identifying the ANIMAL refer-
ences and it was always correct when it did assign
a HUMAN tag to a pronoun.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999991909090909">
We presented a novel technique for inducing
domain-specific semantic class taggers from a
handful of seed words and an unannotated text
collection. Our results showed that the induced
taggers achieve good performance on six seman-
tic categories associated with the domain of vet-
erinary medicine. Our technique allows seman-
tic class taggers to be rapidly created for special-
ized domains with minimal human effort. In future
work, we plan to investigate whether these seman-
tic taggers can be used to improve other tasks.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.991940090909091">
We are very grateful to the people at the Veterinary
Information Network for providing us access to
their resources. Special thanks to Paul Pion, DVM
and Nicky Mastin, DVM for making their data
available to us, and to Sherri Lofing and Becky
Lundgren, DVM for their time and expertise in
creating the gold standard annotations. This re-
search was supported in part by Department of
Homeland Security Grant N0014-07-1-0152 and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
</bodyText>
<sectionHeader confidence="0.99055" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.6351555">
ACE. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
</reference>
<figure confidence="0.876432333333333">
0 1/16 1/8 1/4 1/2
ration of data
3/4 1
</figure>
<page confidence="0.960469">
283
</page>
<bodyText confidence="0.8951214">
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-08).
D. McClosky, E. Charniak, and M Johnson. 2006. Ef-
fective self-training for parsing. In HLT-NAACL-
2006.
</bodyText>
<note confidence="0.834495">
ACE. 2007. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2007.
ACE. 2008. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2008.
</note>
<reference confidence="0.999837686868687">
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings
ofANLP-97, pages 194–201.
A. Blum and T. Mitchell. 1998. Combining Labeled
and Unlabeled Data with Co-Training. In Proceed-
ings of the 11th Annual Conference on Computa-
tional Learning Theory (COLT-98).
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In HLT-NAACL 2009 Workshop on Semi-Supervised
Learning for NLP.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language In-
dependent Named Entity Recognition Combining
Morphologi cal and Contextual Evidence. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-99).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: an experimental study. Artificial Intelli-
gence, 165(1):91–134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of
the COLING conference, August.
T. Joachims. 1999. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Sch¨olkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale
Linear SVMs. Journal of Machine Learning Re-
search.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the As-
sociation for Computational Linguistics.
R. Mihalcea. 2004. Co-training and Self-training for
Word Sense Disambiguation. In CoNLL-2004.
G. Miller. 1990. Wordnet: An On-line Lexical
Database. International Journal of Lexicography,
3(4).
C. Mueller, S. Rapp, and M. Strube. 2002. Applying
co-training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL-2003.
V. Ng. 2007. Semantic Class Induction and Corefer-
ence Resolution. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Sri-
hari. 2003. A bootstrapping approach to named
entity classification using successive learners. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics (ACL-03), pages
335–342.
M. Pas¸ca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the Thirteenth
ACM International Conference on Information and
Knowledge Management, pages 137–145.
W. Phillips and E. Riloff. 2002. Exploiting Strong
Syntactic Heuristics and Co-Training to Learn Se-
mantic Lexicons. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing, pages 125–132.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117–
124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1110–1116.
</reference>
<page confidence="0.976626">
284
</page>
<reference confidence="0.999580476190476">
M. Thelen and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons Using Ex-
traction Pa ttern Contexts. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 214–221.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-Rich Part-of-Speech Tagging with
a Cyclic Dependency Network. In Proceedings of
HLT-NAACL 2003.
R. Yangarber. 2003. Counter-training in the discovery
of semantic patterns. In Proceedings of the 41th An-
nual Meeting of the Association for Computational
Linguistics.
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics.
Imed Zitouni and Radu Florian. 2009. Cross-language
information propagation for arabic mention detec-
tion. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 8(4):1–21.
</reference>
<page confidence="0.998522">
285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986040">
<title confidence="0.999649">Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing</title>
<author confidence="0.999709">Ruihong Huang</author>
<author confidence="0.999709">Ellen Riloff</author>
<affiliation confidence="0.9998915">School of Computing University of Utah</affiliation>
<address confidence="0.996111">Salt Lake City, UT 84112</address>
<abstract confidence="0.999574727272727">This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. a bootstrapping protrains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynamically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>NIST ACE evaluation website.</title>
<date>2005</date>
<booktitle>In http://www.nist.gov/speech/tests/ace/2005.</booktitle>
<contexts>
<context position="6430" citStr="ACE, 2005" startWordPosition="976" endWordPosition="977">1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class l</context>
</contexts>
<marker>ACE, 2005</marker>
<rawString>ACE. 2005. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP-97,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="5799" citStr="Bikel et al., 1997" startWordPosition="880" endWordPosition="883">eviously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detectio</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a highperformance learning name-finder. In Proceedings ofANLP-97, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).</booktitle>
<contexts>
<context position="8570" citStr="Blum and Mitchell, 1998" startWordPosition="1308" endWordPosition="1311">on on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an Instance-based Semantic Class Tagger from Seeds 3.1 Motivation Our goal is to create a bootstrapping model that can rapidly create semantic class taggers using just a small set of seed words and an unannotated domain-specific corpu</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupling semi-supervised learning of categories and relations.</title>
<date>2009</date>
<booktitle>In HLT-NAACL 2009 Workshop on Semi-Supervised Learning for NLP.</booktitle>
<contexts>
<context position="7592" citStr="Carlson et al., 2009" startWordPosition="1153" endWordPosition="1156">uce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explor</context>
</contexts>
<marker>Carlson, Betteridge, Jr, Mitchell, 2009</marker>
<rawString>Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2009. Coupling semi-supervised learning of categories and relations. In HLT-NAACL 2009 Workshop on Semi-Supervised Learning for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</booktitle>
<contexts>
<context position="5825" citStr="Collins and Singer, 1999" startWordPosition="884" endWordPosition="887"> the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE,</context>
<context position="8596" citStr="Collins and Singer, 1999" startWordPosition="1312" endWordPosition="1315"> domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an Instance-based Semantic Class Tagger from Seeds 3.1 Motivation Our goal is to create a bootstrapping model that can rapidly create semantic class taggers using just a small set of seed words and an unannotated domain-specific corpus. Our motivation comes fr</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language Independent Named Entity Recognition Combining Morphologi cal and Contextual Evidence.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</booktitle>
<contexts>
<context position="5854" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="888" endWordPosition="891">e classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008))</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language Independent Named Entity Recognition Combining Morphologi cal and Contextual Evidence. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="7546" citStr="Etzioni et al., 2005" startWordPosition="1145" endWordPosition="1148">h and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic catego</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>E H Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING conference,</booktitle>
<contexts>
<context position="5882" citStr="Fleischman and Hovy, 2002" startWordPosition="892" endWordPosition="895">create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>M.B. Fleischman and E.H. Hovy. 2002. Fine grained classification of named entities. In Proceedings of the COLING conference, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale Support Vector Machine Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Machines.</booktitle>
<editor>In A. Smola B. Sch¨olkopf, C. Burges, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15199" citStr="Joachims, 1999" startWordPosition="2386" endWordPosition="2387">e a confidence score to label only the instances that the classifiers are most certain about. We compute a confidence score for instance i with respect to semantic class Ck by considering both the score of the Ck classifier as well as the scores of the competing classifiers. Intuitively, we have confidence in labeling an instance as category Ck if the Ck classifier gave it a positive score, and its score is much higher than the score of any other classifier. We use the following scoring function: Confidence(i,Ck) = score(i,Ck) - max(Vj6�k score(i,Cj)) We employ support vector machines (SVMs) (Joachims, 1999) with a linear kernel as our classifiers, using the SVMlin software (Keerthi and DeCoste, 2005). We use the value produced by the decision function (essentially the distance from the hyperplane) as the score for a classifier. We specify a threshold Bcf and only assign a semantic tag Ck to an instance i if Confidence(i,Ck) &gt; Bcf. All instances that pass the confidence threshold are labeled and added to the training set. This process greatly enhances the diversity of the training data. In this initial learning step, the strictly contextual classifiers substantially increase the number of trainin</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making Large-Scale Support Vector Machine Learning Practical. In A. Smola B. Sch¨olkopf, C. Burges, editor, Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Keerthi</author>
<author>D DeCoste</author>
</authors>
<title>A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="15294" citStr="Keerthi and DeCoste, 2005" startWordPosition="2400" endWordPosition="2404">in about. We compute a confidence score for instance i with respect to semantic class Ck by considering both the score of the Ck classifier as well as the scores of the competing classifiers. Intuitively, we have confidence in labeling an instance as category Ck if the Ck classifier gave it a positive score, and its score is much higher than the score of any other classifier. We use the following scoring function: Confidence(i,Ck) = score(i,Ck) - max(Vj6�k score(i,Cj)) We employ support vector machines (SVMs) (Joachims, 1999) with a linear kernel as our classifiers, using the SVMlin software (Keerthi and DeCoste, 2005). We use the value produced by the decision function (essentially the distance from the hyperplane) as the score for a classifier. We specify a threshold Bcf and only assign a semantic tag Ck to an instance i if Confidence(i,Ck) &gt; Bcf. All instances that pass the confidence threshold are labeled and added to the training set. This process greatly enhances the diversity of the training data. In this initial learning step, the strictly contextual classifiers substantially increase the number of training instances for each semantic category, producing a more diverse mix of seed-generated instance</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>S. Keerthi and D. DeCoste. 2005. A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Taku Kudo</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Graph-based analysis of semantic drift in espresso-like bootstrapping algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4648" citStr="Komachi et al., 2008" startWordPosition="709" endWordPosition="712">Computational Linguistics has access to contextual features and cannot see the seed. Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. This process expands and diversifies the training set to fuel subsequent bootstrapping. Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. Iterative self-training, however, often has difficulty sustaining momentum or it succumbs to semantic drift (Komachi et al., 2008; McIntosh and Curran, 2009). To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dyna</context>
</contexts>
<marker>Komachi, Kudo, Shimbo, Matsumoto, 2008</marker>
<rawString>Mamoru Komachi, Taku Kudo, Masashi Shimbo, and Yuji Matsumoto. 2008. Graph-based analysis of semantic drift in espresso-like bootstrapping algorithms. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual</booktitle>
<contexts>
<context position="7569" citStr="Kozareva et al., 2008" startWordPosition="1149" endWordPosition="1152">here the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semanti</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proceedings of the 46th Annual</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McIntosh</author>
<author>J Curran</author>
</authors>
<title>Reducing Semantic Drift with Bagging and Distributional Similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational</booktitle>
<contexts>
<context position="4676" citStr="McIntosh and Curran, 2009" startWordPosition="713" endWordPosition="716">ics has access to contextual features and cannot see the seed. Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. This process expands and diversifies the training set to fuel subsequent bootstrapping. Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. Iterative self-training, however, often has difficulty sustaining momentum or it succumbs to semantic drift (Komachi et al., 2008; McIntosh and Curran, 2009). To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic feat</context>
<context position="6945" citStr="McIntosh and Curran, 2009" startWordPosition="1052" endWordPosition="1055">ndle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 200</context>
<context position="8299" citStr="McIntosh and Curran, 2009" startWordPosition="1265" endWordPosition="1269">g patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a diffe</context>
<context position="17873" citStr="McIntosh and Curran, 2009" startWordPosition="2826" endWordPosition="2829">lecting a subset of the possible negative instances. We discuss this issue further in Section 4.4. 2For simplicity, this picture does not depict the initial contextual training step, but that can be viewed as the first iteration in this general framework. labeled C� 2 C� 1 C� n seeds C� 1 (+) C� i=1 ( ) _ C� i=n ( ) _ unlabeled C�2 C�i=2 (+) ( ) _ C� n (+) C1 C2 Cn Figure 1: Cross-Category Bootstrapping 278 Cross-category training has two advantages over independent self-training. First, as others have shown for pattern learning and lexicon induction (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh and Curran, 2009), simultaneously training classifiers for multiple categories reduces semantic drift because each classifier is deterred from encroaching on another one’s territory (i.e., claiming the instances from a competing class as its own). Second, similar in spirit to co-training3, this approach allows each classifier to obtain new training instances from an outside source that has a slightly different perspective. While independent self-training can quickly run out of steam, cross-category training supplies each classifier with a constant stream of new (negative) instances produced by competing classi</context>
</contexts>
<marker>McIntosh, Curran, 2009</marker>
<rawString>T. McIntosh and J. Curran. 2009. Reducing Semantic Drift with Bagging and Distributional Similarity. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics. R. Mihalcea. 2004. Co-training and Self-training for Word Sense Disambiguation. In CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="21430" citStr="Miller, 1990" startWordPosition="3405" endWordPosition="3406">nces labeled as c, and wu/c is the # of unlabeled instances that receive a positive confidence score for c when given to the classifier. The intuition behind the second term is that most instances are initially unlabeled and we want to make sure that many of the unlabeled instances are likely to belong to the same semantic class (even though the classifier isn’t ready to commit to them yet). 3.5 Dynamic Semantic Features For many NLP tasks, classifiers use semantic features to represent the semantic class of words. These features are typically obtained from external resources such as Wordnet (Miller, 1990). Our bootstrapping model incrementally trains semantic class taggers, so we explored the idea of using the labels assigned by the classifiers to create enhanced feature vectors by dynamically adding semantic features. This process allows later stages of bootstrapping to directly benefit from earlier stages. For example, consider the sentence: He started the doxy on Vetsulin today. If “Vetsulin” was labeled as a DRUG in a previous bootstrapping iteration, then the feature vector representing the context around “doxy” can be enhanced to include an additional semantic feature identifying Vetsuli</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An On-line Lexical Database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mueller</author>
<author>S Rapp</author>
<author>M Strube</author>
</authors>
<title>Applying co-training to reference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8642" citStr="Mueller et al., 2002" startWordPosition="1320" endWordPosition="1323">orm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an Instance-based Semantic Class Tagger from Seeds 3.1 Motivation Our goal is to create a bootstrapping model that can rapidly create semantic class taggers using just a small set of seed words and an unannotated domain-specific corpus. Our motivation comes from specialized domains that cannot be adequate</context>
</contexts>
<marker>Mueller, Rapp, Strube, 2002</marker>
<rawString>C. Mueller, S. Rapp, and M. Strube. 2002. Applying co-training to reference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In HLTNAACL-2003.</booktitle>
<contexts>
<context position="8432" citStr="Ng and Cardie, 2003" startWordPosition="1288" endWordPosition="1291">ther than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an Instance-based Semantic Class Tagger from Seeds 3.1 Motivation Our goal is to create a bootst</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>V. Ng and C. Cardie. 2003. Weakly supervised natural language learning without redundant views. In HLTNAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Semantic Class Induction and Coreference Resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6917" citStr="Ng, 2007" startWordPosition="1050" endWordPosition="1051">ms also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸c</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>V. Ng. 2007. Semantic Class Induction and Coreference Resolution. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng Niu</author>
<author>Wei Li</author>
<author>Jihong Ding</author>
<author>Rohini K Srihari</author>
</authors>
<title>A bootstrapping approach to named entity classification using successive learners.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL-03),</booktitle>
<pages>335--342</pages>
<contexts>
<context position="6081" citStr="Niu et al., 2003" startWordPosition="921" endWordPosition="924">-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one lan</context>
</contexts>
<marker>Niu, Li, Ding, Srihari, 2003</marker>
<rawString>Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Srihari. 2003. A bootstrapping approach to named entity classification using successive learners. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL-03), pages 335–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proc. of the Thirteenth ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>137--145</pages>
<marker>Pas¸ca, 2004</marker>
<rawString>M. Pas¸ca. 2004. Acquisition of categorized named entities for web search. In Proc. of the Thirteenth ACM International Conference on Information and Knowledge Management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Phillips</author>
<author>E Riloff</author>
</authors>
<title>Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>125--132</pages>
<contexts>
<context position="8670" citStr="Phillips and Riloff, 2002" startWordPosition="1324" endWordPosition="1327">lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an Instance-based Semantic Class Tagger from Seeds 3.1 Motivation Our goal is to create a bootstrapping model that can rapidly create semantic class taggers using just a small set of seed words and an unannotated domain-specific corpus. Our motivation comes from specialized domains that cannot be adequately handled by general-purpos</context>
</contexts>
<marker>Phillips, Riloff, 2002</marker>
<rawString>W. Phillips and E. Riloff. 2002. Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8620" citStr="Riloff and Jones, 1999" startWordPosition="1316" endWordPosition="1319">at may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an Instance-based Semantic Class Tagger from Seeds 3.1 Motivation Our goal is to create a bootstrapping model that can rapidly create semantic class taggers using just a small set of seed words and an unannotated domain-specific corpus. Our motivation comes from specialized domains t</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="6856" citStr="Riloff and Shepherd, 1997" startWordPosition="1038" endWordPosition="1041">t associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1110--1116</pages>
<contexts>
<context position="6882" citStr="Roark and Charniak, 1998" startWordPosition="1042" endWordPosition="1045">med entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category</context>
<context position="25596" citStr="Roark and Charniak (1998)" startWordPosition="4070" endWordPosition="4073">tation guidelines6 for each semantic category and conducted an inter-annotator agreement study to measure the consistency of the two domain experts on 30 message board posts, which contained 1,473 noun phrases. The annotators achieved a relatively high r. score of .80. Each annotator then labeled an additional 35 documents, which gave us a test set containing 100 manually annotated message board posts. The table below shows the distribution of semantic classes in the test set. Animal Dis/Sym Drug Test Human Other 612 900 369 404 818 1723 To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.7 This process is fast, relatively objective, and guaranteed to yield high-frequency terms, which is important for bootstrapping. We used the Stanford part-ofspeech tagger (Toutanova et al., 2003) to identify nouns, and our own simple rule-based NP chunker. 4.2 Baselines To assess the difficulty of our data set and task, we evaluated several baselines. The first baseline searches for each head noun in WordNet and labels the noun as category Ck</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1110–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="6907" citStr="Thelen and Riloff, 2002" startWordPosition="1046" endWordPosition="1049">ms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.</context>
<context position="8271" citStr="Thelen and Riloff, 2002" startWordPosition="1261" endWordPosition="1264">rmation from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 t</context>
<context position="17828" citStr="Thelen and Riloff, 2002" startWordPosition="2820" endWordPosition="2823">o of negatives to positives by randomly selecting a subset of the possible negative instances. We discuss this issue further in Section 4.4. 2For simplicity, this picture does not depict the initial contextual training step, but that can be viewed as the first iteration in this general framework. labeled C� 2 C� 1 C� n seeds C� 1 (+) C� i=1 ( ) _ C� i=n ( ) _ unlabeled C�2 C�i=2 (+) ( ) _ C� n (+) C1 C2 Cn Figure 1: Cross-Category Bootstrapping 278 Cross-category training has two advantages over independent self-training. First, as others have shown for pattern learning and lexicon induction (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh and Curran, 2009), simultaneously training classifiers for multiple categories reduces semantic drift because each classifier is deterred from encroaching on another one’s territory (i.e., claiming the instances from a competing class as its own). Second, similar in spirit to co-training3, this approach allows each classifier to obtain new training instances from an outside source that has a slightly different perspective. While independent self-training can quickly run out of steam, cross-category training supplies each classifier with a constant stream of new (neg</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="25945" citStr="Toutanova et al., 2003" startWordPosition="4124" endWordPosition="4127">containing 100 manually annotated message board posts. The table below shows the distribution of semantic classes in the test set. Animal Dis/Sym Drug Test Human Other 612 900 369 404 818 1723 To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.7 This process is fast, relatively objective, and guaranteed to yield high-frequency terms, which is important for bootstrapping. We used the Stanford part-ofspeech tagger (Toutanova et al., 2003) to identify nouns, and our own simple rule-based NP chunker. 4.2 Baselines To assess the difficulty of our data set and task, we evaluated several baselines. The first baseline searches for each head noun in WordNet and labels the noun as category Ck if it has a hypernym synset corresponding to that category. We manually identified the WordNet synsets that, to the best of our ability, seem to most closely correspond 4We used a single category for diseases and symptoms because our domain experts had difficulty distinguishing between them. A veterinary consultant explained that the same term (e</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
</authors>
<title>Counter-training in the discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8338" citStr="Yangarber, 2003" startWordPosition="1273" endWordPosition="1275">ose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at276 tempting to recognize a different semantic class. 3 Bootstrapping an</context>
<context position="17845" citStr="Yangarber, 2003" startWordPosition="2824" endWordPosition="2825">es by randomly selecting a subset of the possible negative instances. We discuss this issue further in Section 4.4. 2For simplicity, this picture does not depict the initial contextual training step, but that can be viewed as the first iteration in this general framework. labeled C� 2 C� 1 C� n seeds C� 1 (+) C� i=1 ( ) _ C� i=n ( ) _ unlabeled C�2 C�i=2 (+) ( ) _ C� n (+) C1 C2 Cn Figure 1: Cross-Category Bootstrapping 278 Cross-category training has two advantages over independent self-training. First, as others have shown for pattern learning and lexicon induction (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh and Curran, 2009), simultaneously training classifiers for multiple categories reduces semantic drift because each classifier is deterred from encroaching on another one’s territory (i.e., claiming the instances from a competing class as its own). Second, similar in spirit to co-training3, this approach allows each classifier to obtain new training instances from an outside source that has a slightly different perspective. While independent self-training can quickly run out of steam, cross-category training supplies each classifier with a constant stream of new (negative) instances </context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>R. Yangarber. 2003. Counter-training in the discovery of semantic patterns. In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19284" citStr="Yarowsky, 1995" startWordPosition="3044" endWordPosition="3045">ture set for these classifiers is exactly the same as described in Section 3.2, except that we add a new lexical feature that represents the head noun of the target NP (i.e., the NP that needs to be tagged). This allows the classifiers to consider the local context as well as the target word itself when making decisions. 3.4 One Semantic Class Per Discourse We also explored the idea of using a one semantic class per discourse (OSCPD) heuristic to generate additional training instances during bootstrapping. Inspired by Yarowsky’s one sense per discourse heuristic for word sense disambiguation (Yarowsky, 1995), we make the assumption that multiple instances of a word in the same discourse will nearly always correspond to the same semantic class. Since our data set consists of message board posts organized as threads, we consider all posts in the same thread to be a single discourse. After each training step, we apply the classifiers to the unlabeled data to label some new instances. For each newly labeled instance, the OSCPD heuristic collects all instances with the same head noun in the same discourse (thread) and unilaterally labels them with the same semantic class. This heuristic serves as meta</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Radu Florian</author>
</authors>
<title>Cross-language information propagation for arabic mention detection.</title>
<date>2009</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="6762" citStr="Zitouni and Florian, 2009" startWordPosition="1024" endWordPosition="1027">tances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE 1Some NER systems also handle specialized constructs such as dates and monetary amounts. mention detection systems (e.g., see (ACE, 2005; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with supervised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different languages (e.g., (Zitouni and Florian, 2009)). Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also</context>
</contexts>
<marker>Zitouni, Florian, 2009</marker>
<rawString>Imed Zitouni and Radu Florian. 2009. Cross-language information propagation for arabic mention detection. ACM Transactions on Asian Language Information Processing (TALIP), 8(4):1–21.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>