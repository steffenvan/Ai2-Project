<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.991523">
Constraints on strong generative power
</title>
<author confidence="0.999084">
David Chiang
</author>
<affiliation confidence="0.998675">
University of Pennsylvania
Dept of Computer and Information Science
</affiliation>
<address confidence="0.932925">
200 S 33rd St
Philadelphia, PA 19104 USA
</address>
<email confidence="0.999595">
dchiang@cis.upenn.edu
</email>
<sectionHeader confidence="0.994817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895833333333">
We consider the question “How
much strong generative power can
be squeezed out of a formal system
without increasing its weak generative
power?” and propose some theoret-
ical and practical constraints on this
problem. We then introduce a formal-
ism which, under these constraints,
maximally squeezes strong generative
power out of context-free grammar.
Finally, we generalize this result to
formalisms beyond CFG.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980328">
“How much strong generative power can be
squeezed out of a formal system without increas-
ing its weak generative power?” This question,
posed by Joshi (2000), is important for both lin-
guistic description and natural language process-
ing. The extension of tree adjoining grammar
(TAG) to tree-local multicomponent TAG (Joshi,
1987), or the extension of context free gram-
mar (CFG) to tree insertion grammar (Schabes
and Waters, 1993) or regular form TAG (Rogers,
1994) can be seen as steps toward answering this
question. But this question is difficult to answer
with much finality unless we pin its terms down
more precisely.
First, what is meant by strong generative
power? In the standard definition (Chomsky,
1965) a grammar G weakly generates a set of
sentences L(G) and strongly generates a set of
structural descriptions E(G); the strong genera-
tive capacity of a formalism Fis then {E(G) |
F provides G}. There is some vagueness in the
literature, however, over what structural descrip-
tions are and how they can reasonably be com-
pared across theories (Miller (1999) gives a good
synopsis).
</bodyText>
<figureCaption confidence="0.999534">
Figure 1: Example of weakly context-free TAG.
</figureCaption>
<bodyText confidence="0.999929636363636">
The approach that Vijay-Shanker et al. (1987)
and Weir (1988) take, elaborated on by Becker
et al. (1992), is to identify a very general class
of formalisms, which they call linear context-
free rewriting systems (CFRSs), and define for
this class a large space of structural descriptions
which serves as a common ground in which the
strong generative capacities of these formalisms
can be compared. Similarly, if we want to talk
about squeezing strong generative power out of
a formal system, we need to do so in the context
of some larger space of structural descriptions.
Second, why is preservation of weak generative
power important? If we interpret this constraint to
the letter, it is almost vacuous. For example, the
class of all tree adjoining grammars which gen-
erate context-free languages includes the gram-
mar shown in Figure 1a (which generates the lan-
guage {a, b}*). We can also add the tree shown in
Figure 1b without increasing the grammar’s weak
generative capacity; indeed, we can add any trees
we please, provided they yield only as and bs. In-
tuitively, the constraint of weak context-freeness
has little force.
This intuition is verified if we consider that
weak context-freeness is desirable for computa-
tional efficiency. Though a weakly context-free
TAG might be recognizable in cubic time (if we
know the equivalent CFG), it need not be parsable
in cubic time—that is, given a string, to compute
all its possible structural descriptions will take
O(n6) time in general. If we are interested in com-
puting structural descriptions from strings, then
</bodyText>
<figure confidence="0.999528666666667">
(a) X
E
X
X* a
X
X* b
(b) XNA
a X
b X* a
b
Structural
Derivations
descriptions
X XNA
Y a X* d
YNA
b Y* c
Sentences
</figure>
<figureCaption confidence="0.997044">
Figure 2: Simulation: structural descriptions as
derived structures.
</figureCaption>
<bodyText confidence="0.9972917">
we need a tighter constraint than preservation of
weak generative power.
In Section 3 below we examine some restric-
tions on tree adjoining grammar which are weakly
context-free, and observe that their parsers all
work in the same way: though given a TAG G,
they implicitly parse using a CFG G&apos; which de-
rives the same strings as G, but also their corre-
sponding structural descriptions under G, in such
a way that preserves the dynamic-programming
structure of the parsing algorithm.
Based on this observation, we replace the con-
straint of preservation of weak generative power
with a constraint of simulability: essentially, a
grammar G&apos; simulates another grammar G if it
generates the same strings that G does, as well as
their corresponding structural descriptions under
G (see Figure 2).
So then, within the class of context-free rewrit-
ing systems, how does this constraint of simu-
lability limit strong generative power? In Sec-
tion 4.1 we define a formalism called multicom-
ponent multifoot TAG (MMTAG) which, when
restricted to a regular form, characterizes pre-
cisely those CFRSs which are simulable by a
CFG. Thus, in the sense we have set forth, this
formalism can be said to squeeze as much strong
generative power out of CFG as is possible. Fi-
nally, we generalize this result to formalisms be-
yond CFG.
</bodyText>
<sectionHeader confidence="0.717968" genericHeader="method">
2 Characterizing structural descriptions
</sectionHeader>
<bodyText confidence="0.860628875">
First we define context-free rewriting systems.
What these formalisms have in common is that
their derivation sets are all local sets (that is, gen-
erable by a CFG). These derivations are taken as
structural descriptions. The following definitions
are adapted from Weir (1988).
Definition 1 A generalized context-free gram-
mar G is a tuple (V, S, F, P), where
</bodyText>
<equation confidence="0.984460111111111">
1. V is a finite set of variables,
2. S E V is a distinguished start symbol,
3. F is a finite set of function symbols, and
E
S —� α(X, Y) α((x1, x2), (y1,y2)) = x1y1y2x2
X —� P1(X) P1((x1, x2)) = (ax1, x2d)
X —� E() E() = (E, E)
Y —� P2(Y) P2((y1,y2)) = (by1,y2c)
Y —� E() E() = (E, E)
</equation>
<figureCaption confidence="0.996639333333333">
Figure 3: Example of TAG with corresponding
GCFG and interpretation. Here adjunction at foot
nodes is allowed.
</figureCaption>
<bodyText confidence="0.968149266666667">
4. P is a finite set of productions of the form
A —� f(A1, ..., An)
where n &gt;_ 0, f E F, and A, Ai E V.
A generalized CFG G generates a set T(G) of
terms, which are interpreted as derivations under
some formalism. In this paper we require that G
be free of spurious ambiguity, that is, that each
term be uniquely generated.
Definition 2 We say that a formalism 9-is a
context-free rewriting system (CFRS) if its deriva-
tion sets can be characterized by generalized
CFGs, and its derived structures are produced by
a function ~+, from terms to strings such that for
each function symbol f, there is a yield function
f9- such that
</bodyText>
<equation confidence="0.854327">
~f(t1,...,tn)•9- = f9- (~t1•9- ,...,~tn•9- )
</equation>
<bodyText confidence="0.9768460625">
(A linear CFRS is subject to further restrictions,
which we do not make use of.)
As an example, Figure 3 shows a simple TAG
with a corresponding GCFG and interpretation.
A nice property of CFRS is that any formal-
ism which can be defined as a CFRS immedi-
ately lends itself to several extensions, which arise
when we give additional interpretations to the
function symbols. For example, we can interpret
the functions as ranging over probabilities, cre-
ating a stochastic grammar; or we can interpret
them as yield functions of another grammar, cre-
ating a synchronous grammar.
Now we define strong generative capacity as
the relationship between strings and structural de-
scriptions.1
</bodyText>
<footnote confidence="0.636686">
1This is similar in spirit, but not the same as, the notion
of derivational generative capacity (Becker et al., 1992).
</footnote>
<bodyText confidence="0.79734">
Definition 3 The strong generative capacity of a
grammar G a CFRS Fis the relation
</bodyText>
<equation confidence="0.532575">
{h~t•F , ti  |t ∈ T(G)}.
</equation>
<bodyText confidence="0.950225111111111">
For example, the strong generative capacity of the
grammar of Figure 3 is
{hambncndm, α(βm 1 (�()), βn 2(~()))i}
whereas any equivalent CFG must have a strong
generative capacity of the form
{hambncndm, fm(gn(e()))i}
That is, in a CFG the n bs and cs must appear later
in the derivation than the m as and ds, whereas in
our example they appear in parallel.
</bodyText>
<sectionHeader confidence="0.895335" genericHeader="method">
3 Simulating structural descriptions
</sectionHeader>
<bodyText confidence="0.99993775">
We now take a closer look at some examples of
“squeezed” context-free formalisms to illustrate
how a CFG can be used to simulate formalisms
with greater strong generative power than CFG.
</bodyText>
<subsectionHeader confidence="0.998531">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.9758093">
Tree substitution grammar (TSG), tree insertion
grammar (TIG), and regular-form TAG (RF-TAG)
are all weakly context free formalisms which can
additionally be parsed in cubic time (with a caveat
for RF-TAG below). For each of these formalisms
a CKY-style parser can be written whose items are
of the form [X, i, j] and are combined in various
ways, but always according to the schema
[X, i, j] [Y, j, k]
[Z, i, k]
just as in the CKY parser for CFG. In effect the
parser dynamically converts the TSG, TIG, or RF-
TAG into an equivalent CFG—each parser rule of
the above form corresponds to the rule schema
Z → XY.
More importantly, given a grammar G and a
string w, a parser can reconstruct all possible
derivations of w under G by storing inside each
chart item how that item was inferred. If we think
of the parser as dynamically converting G into a
CFG G0, then this CFG is likewise able to com-
positionally reconstruct TSG, TIG, or RF-TAG
derivations—we say that G0 simulates G.
Note that the parser specifies how to convert G
into G0, but G0 is not itself a parser. Thus these
three formalisms have a special relationship to
CFG that is independent of any particular pars-
ing algorithm: for any TSG, TIG, or RF-TAG G,
there is a CFG that simulates G. We make this no-
tion more precise below.
</bodyText>
<subsectionHeader confidence="0.999169">
3.2 Excursus: regular form TAG
</subsectionHeader>
<bodyText confidence="0.999963727272727">
Strictly speaking, the recognition algorithm
Rogers gives cannot be extended to parsing; that
is, it generates all possible derived trees for a
given string, but not all possible derivations. It
is correct, however, as a parser for a further re-
stricted subclass of TAGs:
Definition 4 We say that a TAG is in strict reg-
ular form if there exists some partial ordering &lt;
over the nonterminal alphabet such that for ev-
ery auxiliary tree β, if the root and foot of β are
labeled X, then for every node η along β’s spine
where adjunction is allowed, X &lt; label(η), and
X = label(η) only if η is a foot node. (In this vari-
ant adjunction at foot nodes is permitted.)
Thus the only kinds of adjunction which can oc-
cur to unbounded depth are off-spine adjunction
and adjunction at foot nodes.
This stricter definition still has greater strong
generative capacity than CFG. For example, the
TAG in Figure 3 is in strict regular form, because
the only nodes along spines where adjunction is
allowed are foot nodes.
</bodyText>
<subsectionHeader confidence="0.997999">
3.3 Simulability
</subsectionHeader>
<bodyText confidence="0.9812378">
So far we have not placed any restrictions on
how these structural descriptions are computed.
Even though we might imagine attaching arbi-
trary functions to the rules of a parser, an algo-
rithm like CKY is only really capable of com-
puting values of bounded size, or else structure-
sharing in the chart will be lost, increasing the
complexity of the algorithm possibly to exponen-
tial complexity.
For a parser to compute arbitrary-sized objects,
such as the derivations themselves, it must use
back-pointers, references to the values of sub-
computations but not the values themselves. The
only functions on a back-pointer the parser can
compute online are the identity function (by copy-
ing the back-pointer) and constant functions (by
replacing the back-pointer); any other function
would have to dereference the back-pointer and
destroy the structure of the algorithm. Therefore
such functions must be computed offline.
Definition 5 A simulating interpretation ~·• is a
bijection between two recognizable sets of terms
such that
1. For each function symbol φ, there is a func-
tion φ¯ such that
</bodyText>
<equation confidence="0.9918685">
~φ(t1,...,tn)• = φ(~t1•, ... , ~tn•)
2. Each φ¯ is definable as:
¯φ(hx11, ... , x1m1)i), ... , hxn1, ... , xmnmi) =
hw1, ... , wmi
</equation>
<bodyText confidence="0.9973545">
where each wi can take one of the following
forms:
</bodyText>
<listItem confidence="0.9223942">
(a) a variable xij, or
(b) a function application f(xi1 j1, ... xin jn),
n ≥ 0
3. Furthermore, we require that for any recog-
nizable set T, [T] is also a recognizable set.
</listItem>
<bodyText confidence="0.999283235294118">
We say that [·] is trivial if every φ¯ is definable as
¯
φ(x1,... xn) = f(xπ(1),... xπ(n))
where π is a permutation of {1, ... , n}.2
The rationale for requirement (3) is that it
should not be possible, simply by imposing local
constraints on the simulating grammar, to produce
a simulated grammar which does not even come
from a CFRS.3
Definition 6 We say that a grammar G from a
CFRS Fis (trivially) simulable by a grammar G’
from another CFRS F if there is a (trivial) simu-
lating interpretation [·]s : T(G0) → T(G) which
satisfies [t]F0 = [[t]s]F for all t ∈ T(G0).
As an example, a CFG which simulates the
TAG of Figure 3 is shown in Figure 4. Note that
if we give additional interpretations to the simu-
lated yield functions α, β1, and β2, this CFG can
compute any probabilities, translations, etc., that
the original TAG can.
Note that if G0 trivially simulates G, they are
very nearly strongly equivalent, except that the
yield functions of G0 might take their arguments
in a different order than G, and there might be sev-
eral yield functions of G0 which correspond to a
single yield function of G used in several different
contexts. In fact, for technical reasons we will use
this notion instead of strong equivalence for test-
ing the strong generative power of a formal sys-
tem.
Thus the original problem, which was, given
a formalism F , to find a formalism that has as
much strong generative power as possible but re-
mains weakly equivalent to F , is now recast as
</bodyText>
<footnote confidence="0.998920428571429">
2Simulating interpretations and trivial simulating inter-
pretations are similar to the generalized and “ungeneralized”
syntax-directed translations, respectively, of Aho and Ull-
man (1969; 1971).
3Without this requirement, there are certain pathological
cases that cause the construction of Section 4.2 to produce
infinite MM-TAGs.
</footnote>
<equation confidence="0.997488615384616">
S → α0• α(x1, x2) ←� hx1, x2i
α0• → α0• h6(), x2i ←� h−, x2i
α0• → α1 • h−, x2i ←� h−, x2i
h−, 6()i ←� h−, −i
h−, −i ←� h−, −i
α0• → β01[α0] hβ1(x1), x2i ←� hx1, x2i
β01[α0]°� a β21[αo] d hx1, x2i H hx1, x2i
β21[α ] → β01[α ] hβ1(x1), x2i ←� hx1, x2i
β21[α0] → α0• h6(), x2i ←� h−, x2i
α1• → β02[α1] h−,β2(x2)i ←� h−, x2i
β02[α1] → bβ2[α1] c h−, x2i H h−, x2i
β22[α1] → βZ[α1] h−,β2(x2)i ←� h−, x2i
β2 2[α1] → α1• h−, ~()i ←� h−, −i
</equation>
<figureCaption confidence="0.99855475">
Figure 4: CFG which simulates the grammar
of Figure 3. Here we leave the yield functions
anonymous; y ←� x denotes the function which
maps x to y.
</figureCaption>
<bodyText confidence="0.999638666666667">
the following problem: find a formalism that triv-
ially simulates as many grammars as possible but
remains simulable by F .
</bodyText>
<sectionHeader confidence="0.862536" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.998562041666667">
The following is easy to show:
Proposition 1 Simulability is reflexive and tran-
sitive.
Because of transitivity, it is impossible that a for-
malism which is simulable by F could simulate
a grammar that is not simulable by F . So we are
looking for a formalism that can trivially simulate
exactly those grammars that F can.
In Section 4.1 we define a formalism called
multicomponent multifoot TAG (MMTAG), and
then in Section 4.2 we prove the following result:
Proposition 2 A grammar G from a CFRS is
simulable by a CFG if and only if it is trivially
simulable by an MMTAG in regular form.
The “if” direction (⇐) implies (because simu-
lability is reflexive) that RF-MMTAG is simula-
ble by a CFG, and therefore cubic-time parsable.
(The proof below does give an effective proce-
dure for constructing a simulating CFG for any
RF-MMTAG.) The “only if” direction (⇒) shows
that, in the sense we have defined, RF-MMTAG
is the most powerful such formalism.
We can generalize this result using the notion
of a meta-level grammar (Dras, 1999).
</bodyText>
<figure confidence="0.856054428571428">
1• 1
→
α α •
1
α • → E
A
* A
Y 3 X 1
Y 2 *
X 1
*
Definition 7 If f1 and f2 are two CFRSs, f2 o
f1 is the CFRS characterized by the interpretation
function [&apos;]f2of1 = [&apos;]f2 o [&apos;]f1.
</figure>
<bodyText confidence="0.999613769230769">
f1 is the meta-level formalism, which generates
derivations for f2. Obviously f1 must be a tree-
rewriting system.
Proposition 3 For any CFRS f&apos;, a grammar G
from a (possibly different) CFRS is simulable by
a grammar in f&apos; if and only if it is trivially simu-
lable by a grammar in f&apos; o RF-MMTAG.
The “only if&apos; direction (=&gt;) follows from the
fact that the MMTAG constructed in the proof of
Proposition 2 generates the same derived trees as
the CFG. The “if&apos; direction (,t=) is a little trickier
because the constructed CFG inserts and relabels
nodes.
</bodyText>
<sectionHeader confidence="0.990538" genericHeader="method">
4 Multicomponent multifoot TAG
</sectionHeader>
<subsectionHeader confidence="0.931936">
4.1 Definitions
</subsectionHeader>
<bodyText confidence="0.999403">
MMTAG resembles a cross between set-local
multicomponent TAG (Joshi, 1987) and ranked
node rewriting grammar (Abe, 1988), a variant of
TAG in which auxiliary trees may have multiple
foot nodes. It also has much in common with d-
tree substitution grammar (Rambow et al., 1995).
Definition 8 An elementary tree set αl is a finite
set of trees (called the components of lα) with the
following properties:
</bodyText>
<listItem confidence="0.968794222222222">
1. Zero or more frontier nodes are designated
foot nodes, which lack labels (following
Abe), but are marked with the diacritic *;
2. Zero or more (non-foot) nodes are desig-
nated adjunction nodes, which are parti-
tioned into one or more disjoint sets called
adjunction sites. We notate this by assigning
an index i to each adjunction site and mark-
ing each node of site i with the diacritic i.
3. Each component is associated with a sym-
bol called its type. This is analogous to the
left-hand side of a CFG rule (again, follow-
ing Abe).
4. The components of αl are connected by d-
edges from foot nodes to root nodes (notated
by dotted lines) to form a single tree struc-
ture. A single foot node may have multiple
d-children, and their order is significant. (See
</listItem>
<figureCaption confidence="0.414988">
Figure 5 for an example.)
</figureCaption>
<bodyText confidence="0.8519775">
A multicomponent multifoot tree adjoining gram-
mar is a tuple (E, P, S), where:
</bodyText>
<figure confidence="0.549466">
X 1
*
</figure>
<figureCaption confidence="0.959260666666667">
Figure 5: Example of MMTAG adjunction. The
types of the components, not shown in the figure,
are all X.
</figureCaption>
<listItem confidence="0.745991333333333">
1. E is a finite alphabet;
2. P is a finite set of tree sets; and
3. S E E is a distinguished start symbol.
</listItem>
<bodyText confidence="0.9946976875">
Definition 9 A component α is adjoinable at a
node rl if rl is an adjunction node and the type of
α equals the label of rl.
The result of adjoining a component α at a node
rl is the tree set formed by separating rl from its
children, replacing rl with the root of α, and re-
placing the ith foot node of α with the ith child
of rl. (Thus adjunction of a one-foot component
is analogous to TAG adjunction, and adjunction
of a zero-foot component is analogous to substi-
tution.)
A tree set αl is adjoinable at an adjunction site
rll if there is a way to adjoin each component of αl
at a different node of rll (with no nodes left over)
such that the dominance and precedence relations
within αl are preserved. (See Figure 5 for an ex-
ample.)
We now define a regular form for MMTAG that
is analogous to strict regular form for TAG. A
spine is the path from the root to a foot of a sin-
gle component. Whenever adjunction takes place,
several spines are inserted inside or concatenated
with other spines. To ensure that unbounded in-
sertion does not take place, we impose an order-
ing on spines, by means of functions pi that map
the type of a component to the rank of that com-
ponent’s ith spine.
Definition 10 We say that an adjunction node rl E
rll is safe in a spine if it is the lowest node (except
the foot) in that spine, and if each component un-
der that spine consists only of a member of rll and
zero or more foot nodes.
</bodyText>
<figure confidence="0.888968666666667">
*
*
*
*
A
�
X 1
X 1
A
</figure>
<equation confidence="0.955926833333333">
* X 1
Y 2 *
X 1
* X 1
Y 3 *
X 1
</equation>
<bodyText confidence="0.999982076923077">
We say that an MMTAG G is in regular form if
there are functions ρi from E into the domain of
some partial ordering &lt; such that for each com-
ponent α of type X, for each adjunction node
η ∈ α, if the jth child of η dominates the ith foot
node of α (that is, another component’s jth spine
would adjoin into the ith spine), then ρi(X) &lt;
ρj(label(η)), and ρi(X) = ρj(label(η)) only if η
is safe in the ith spine.
Thus the only kinds of adjunction which can oc-
cur to unbounded depth are off-spine adjunction
and safe adjunction. The adjunction shown in Fig-
ure 5 is an example of safe adjunction.
</bodyText>
<subsectionHeader confidence="0.999631">
4.2 Proof of Proposition 2
</subsectionHeader>
<bodyText confidence="0.999654">
(⇐) First we describe how to construct a simu-
lating CFG for any RF-MMTAG; then this direc-
tion of the proof follows from the transitivity of
simulability.
When a CFG simulates a regular form TAG,
each nonterminal must encapsulate a stack (of
bounded depth) to keep track of adjunctions. In
the multicomponent case, these stacks must be
generalized to trees (again, of bounded size).
So the nonterminals of G0 are of the form [η, t],
where t is a derivation fragment of G with a dot
(·) at exactly one node ~α, and η is a node of ~α. Let
η¯ be the node in the derived tree where η ends up.
A fragment t can be put into a normal form as
follows:
</bodyText>
<listItem confidence="0.996611125">
1. For every α~ above the dot, if η¯ does not lie
along a spine of ~α, delete everything above
~α.
2. For every α~ not above or at the dot, if η¯ does
not lie along a d-edge of ~α, delete α~ and
everything below and replace it with &gt; if η¯
dominates ~α; otherwise replace it with ⊥.
3. If there are two nodes ~α1 and ~α2 along a
</listItem>
<bodyText confidence="0.898422611111111">
path which name the same tree set and η¯ lies
along the same spine or same d-edge in both
of them, collapse ~α1 and ~α2, deleting every-
thing in between.
Basically this process removes all unboundedly
long paths, so that the set of normal forms is finite.
In the rule schemata below, the terms in the left-
hand sides range over normalized terms, and their
corresponding right-hand sides are renormalized.
Let up(t) denote the tree that results from moving
the dot in t up one step.
The value of a subderivation t0 of G0 under ~·•s
is a tuple of partial derivations of G, one for each
&gt; symbol in the root label of t0, in order. Where
we do not define a yield function for a production
below, the identity function is understood.
For every set α~ with a single, S-type compo-
nent rooted by η, add the rule
</bodyText>
<equation confidence="0.943689">
S → [η, ·~α(&gt;, ... , &gt;)]
~α(x1, ... , xn) ←� hx1, ... , xni
</equation>
<bodyText confidence="0.91770775">
For every non-adjunction, non-foot node η with
children η1, ... ,ηn (n ≥ 0),
[η, t] → [η1, t] ··· [ηn, t]
For every component with root η0 that is adjoin-
able at η,
[η, up(t)] → [η0, t]
If η0 is the root of the whole set ~α0, this rule
rewrites a &gt; to several &gt; symbols; the corre-
sponding yield function is then
h... , ~α0(x1, ... , xn), ...i ←� h... , x1, ... , xn, ...i
For every component with ith foot η0i that is ad-
joinable at a node with ith child ηi,
</bodyText>
<equation confidence="0.59707">
[η0i,t] → [ηi,up(t)]
</equation>
<bodyText confidence="0.999945">
This last rule skips over deleted parts of the
derivation tree, but this is harmless in a regular
form MMTAG, because all the skipped adjunc-
tions are safe.
(⇒) First we describe how to decompose any
given derivation t0 of G0 into a set of elementary
tree sets.
Let t = ~t0•s. (Note the convention that primed
variables always pertain to the simulating gram-
mar, unprimed variables to the simulated gram-
mar.) If, during the computation of t, a node η0
creates the node η, we say that η0 is productive
and produces η. Without loss of generality, let us
assume that there is a one-to-one correspondence
between productive nodes and nodes of t.4
To start, let η be the root of t, and η1, ... , ηn its
children.
Define the domain of ηi as follows: any node
in t0 that produces ηi or any of its descendants is
in the domain of ηi, and any non-productive node
whose parent is in the domain of ηi is also in the
domain of ηi.
For each ηi, excise each connected component
of the domain of ηi. This operation is the reverse
of adjunction (see Figure 6): each component gets
</bodyText>
<footnote confidence="0.944026">
4If G0 does not have this property, it can be modified
so that it does. This may change the derived trees slightly,
which makes the proof of Proposition 3 trickier.
</footnote>
<figure confidence="0.627785">
E
</figure>
<figureCaption confidence="0.928613">
Figure 6: Example derivation (left) of the gram-
</figureCaption>
<bodyText confidence="0.993883936708861">
mar of Figure 4, and first step of decomposition.
Non-adjunction nodes are shown with the place-
holder • (because the yield functions in the origi-
nal grammar were anonymous), the Greek letters
indicating what is produced by each node. Ad-
junction nodes are shown with labels Qi in place
of the (very long) true labels.
Figure 7: MMTAG converted from CFG of Fig-
ure 4 (cf. the original TAG in Figure 3). Each
components’ type is written to its left.
foot nodes to replace its lost children, and the
components are connected by d-edges according
to their original configuration.
Meanwhile an adjunction node is created in
place of each component. This node is given a la-
bel (which also becomes the type of the excised
component) whose job is to make sure the final
grammar does not overgenerate; we describe how
the label is chosen below. The adjunction nodes
are partitioned such that the ith site contains all
the adjunction nodes created when removing z7i.
The tree set that is left behind is the elementary
tree set corresponding to z7 (rather, the function
symbol that labels z7); this process is repeated re-
cursively on the children of z7, if any.
Thus any derivation of G&apos; can be decomposed
into elementary tree sets. Let Gˆ be the union of
the decompositions of all possible derivations of
G&apos; (see Figure 7 for an example).
Labeling adjunction nodes For any node z7&apos;,
and any list of nodes (z7&apos;1, ... , z7&apos;n), let the sig-
nature of z7&apos; with respect to (z7&apos;1, ... , z7&apos;n) be
(A, a1, ... , am), where A is the left-hand side of
the GCFG production that generated z7&apos;, and ai =
(j, k) if z7&apos; gets its ith field from the kth field of
z7&apos;j, or * if z7&apos; produces a function symbol in its ith
field.
So when we excise the domain of z7i, the la-
bel of the node left behind by a component α is
(s, s1, ... , sn), where s is the signature of the root
of α with respect to the foot nodes and s1, ... , sn
are the signatures of the foot nodes with respect to
their d-children. Note that the number of possible
adjunction labels is finite, though large.
Gˆ trivially simulates G. Since each tree of Gˆ
corresponds to a function symbol (though not
necessarily one-to-one), it is easy to write a triv-
ial simulating interpretation Q·1 : T( ˆG) → T(G).
To see that Gˆ does not overgenerate, observe that
the nonterminal labels inside the signatures en-
sure that every derivation of Gˆ corresponds to a
valid derivation of G&apos;, and therefore G. To see that
[·] is one-to-one, observe that the adjunction la-
bels keep track of how G&apos; constructed its simu-
lated derivations, ensuring that for any derivation
tˆ of ˆG, the decomposition of the derived tree of tˆ
is tˆ itself. Therefore two derivations of Gˆ cannot
correspond to the same derivation of G&apos;, nor of G.
Gˆ is finite. Briefly, suppose that the number of
components per tree set is unbounded. Then it is
possible, by intersecting G&apos; with a recognizable
set, to obtain a grammar whose simulated deriva-
tion set is non-recognizable. The idea is that mul-
ticomponent tree sets give rise to dependent paths
in the derivation set, so if there is no bound on
the number of components in a tree set, neither is
there a bound on the length of dependent paths.
This contradicts the requirement that a simulating
interpretation map recognizable sets to recogniz-
able sets.
Suppose that the number of nodes per compo-
nent is unbounded. If the number of components
per tree set is bounded, so must the number of ad-
junction nodes per component; then it is possible,
again by intersecting G&apos; with a recognizable set,
to obtain a grammar which is infinitely ambigu-
ous with respect to simulated derivations, which
contradicts the requirement that simulating inter-
pretations be bijective.
</bodyText>
<figure confidence="0.99992365625">
• α
• R1
•
a • E
• E
d
�
*
E
Q1 : • R1
•
a • E
d
• α
Q1 1
• E
Q2 : •
E
Q2 : •
S : •
Q1 1
Q2 2
E
Q1 : •
•
a Q1 1
*
d
Q1 : •
*
•
b Q2 2 c
</figure>
<bodyText confidence="0.999135">
Gˆ is in regular form. A component of Gˆ corre-
sponds to a derivation fragment of G&apos; which takes
fields from several subderivations and processes
them, combining some into a larger structure and
copying some straight through to the root. Let
ρi(X) be the number of fields that a component
of type X copies from its ith foot up to its root.
This information is encoded in X, in the signa-
ture of the root. Then Gˆ satisfies the regular form
constraint, because when adjunction inserts one
spine into another spine, the the inserted spine
must copy at least as many fields as the outer
one. Furthermore, if the adjunction site is not safe,
then the inserted spine must additionally copy the
value produced by some lower node.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999987695652174">
We have proposed a more constrained version of
Joshi’s question, “How much strong generative
power can be squeezed out of a formal system
without increasing its weak generative power,”
and shown that within these constraints, a vari-
ant of TAG called MMTAG characterizes the limit
of how much strong generative power can be
squeezed out of CFG. Moreover, using the notion
of a meta-level grammar, this result is extended to
formalisms beyond CFG.
It remains to be seen whether RF-MMTAG,
whether used directly or for specifying meta-level
grammars, provides further practical benefits on
top of existing “squeezed” grammar formalisms
like tree-local MCTAG, tree insertion grammar,
or regular form TAG.
This way of approaching Joshi’s question is by
no means the only way, but we hope that this work
will contribute to a better understanding of the
strong generative capacity of constrained gram-
mar formalisms as well as reveal more powerful
formalisms for linguistic analysis and natural lan-
guage processing.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9362612">
This research is supported in part by NSF
grant SBR-89-20230-15. Thanks to Mark Dras,
William Schuler, Anoop Sarkar, Aravind Joshi,
and the anonymous reviewers for their valuable
help. S. D. G.
</bodyText>
<sectionHeader confidence="0.991852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998959355932204">
Naoki Abe. 1988. Feasible learnability of formal
grammars and the theory of natural language ac-
quisition. In Proceedings of the Twelfth Inter-
national Conference on Computational Linguistics
(COLING-88), pages 1–6, Budapest.
A. V. Aho and J. D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comp.
Sys. Sci, 3:37–56.
A. V. Aho and J. D. Ullman. 1971. Translations on
a context free grammar. Information and Control,
19:439–475.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The derivational generative power of formal
systems, or, Scrambling is beyond LCFRS. Tech-
nical Report IRCS-92-38, Institute for Research in
Cognitive Science, University of Pennsylvania.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. MIT Press, Cambridge, MA.
Mark Dras. 1999. A meta-level grammar: redefining
synchronous TAG for translation and paraphrase.
In Proceedings of the 37th Annual Meeting of the
Assocation for Computational Linguistics, pages
80–87, College Park, MD.
Aravind K. Joshi. 1987. An introduction to tree ad-
joining grammars. In Alexis Manaster-Ramer, ed-
itor, Mathematics of Language. John Benjamins,
Amsterdam.
Aravind K. Joshi. 2000. Relationship between strong
and weak generative power of formal systems. In
Proceedings of the Fifth International Workshop on
TAG and Related Formalisms (TAG+5), pages 107–
113.
Philip H. Miller. 1999. Strong Generative Capacity:
The Semantics of Linguistic Formalism. Number
103 in CSLI lecture notes. CSLI Publications, Stan-
ford.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-tree grammars. In Proceedings of the
33rd Annual Meeting of the Assocation for Com-
putational Linguistics, pages 151–158, Cambridge,
MA.
James Rogers. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd An-
nual Meeting of the Assocation for Computational
Linguistics, pages 155–162, Las Cruces, NM.
Yves Schabes and Richard C. Waters. 1993. Lexical-
ized context-free grammars. In Proceedings of the
31st Annual Meeting of the Association for Com-
putational Linguistics, pages 121–129, Columbus,
OH.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 104–111,
Stanford, CA.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univ.
of Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833187">
<title confidence="0.999369">Constraints on strong generative power</title>
<author confidence="0.999987">David Chiang</author>
<affiliation confidence="0.9998555">University of Pennsylvania Dept of Computer and Information Science</affiliation>
<address confidence="0.9554595">200 S 33rd St Philadelphia, PA 19104 USA</address>
<email confidence="0.999849">dchiang@cis.upenn.edu</email>
<abstract confidence="0.992169307692308">We consider the question “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” and propose some theoretical and practical constraints on this problem. We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
</authors>
<title>Feasible learnability of formal grammars and the theory of natural language acquisition.</title>
<date>1988</date>
<booktitle>In Proceedings of the Twelfth International Conference on Computational Linguistics (COLING-88),</booktitle>
<pages>1--6</pages>
<location>Budapest.</location>
<contexts>
<context position="16014" citStr="Abe, 1988" startWordPosition="2799" endWordPosition="2800">system. Proposition 3 For any CFRS f&apos;, a grammar G from a (possibly different) CFRS is simulable by a grammar in f&apos; if and only if it is trivially simulable by a grammar in f&apos; o RF-MMTAG. The “only if&apos; direction (=&gt;) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG. The “if&apos; direction (,t=) is a little trickier because the constructed CFG inserts and relabels nodes. 4 Multicomponent multifoot TAG 4.1 Definitions MMTAG resembles a cross between set-local multicomponent TAG (Joshi, 1987) and ranked node rewriting grammar (Abe, 1988), a variant of TAG in which auxiliary trees may have multiple foot nodes. It also has much in common with dtree substitution grammar (Rambow et al., 1995). Definition 8 An elementary tree set αl is a finite set of trees (called the components of lα) with the following properties: 1. Zero or more frontier nodes are designated foot nodes, which lack labels (following Abe), but are marked with the diacritic *; 2. Zero or more (non-foot) nodes are designated adjunction nodes, which are partitioned into one or more disjoint sets called adjunction sites. We notate this by assigning an index i to eac</context>
</contexts>
<marker>Abe, 1988</marker>
<rawString>Naoki Abe. 1988. Feasible learnability of formal grammars and the theory of natural language acquisition. In Proceedings of the Twelfth International Conference on Computational Linguistics (COLING-88), pages 1–6, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>J. Comp. Sys. Sci,</journal>
<pages>3--37</pages>
<contexts>
<context position="13227" citStr="Aho and Ullman (1969" startWordPosition="2274" endWordPosition="2278">d functions of G0 which correspond to a single yield function of G used in several different contexts. In fact, for technical reasons we will use this notion instead of strong equivalence for testing the strong generative power of a formal system. Thus the original problem, which was, given a formalism F , to find a formalism that has as much strong generative power as possible but remains weakly equivalent to F , is now recast as 2Simulating interpretations and trivial simulating interpretations are similar to the generalized and “ungeneralized” syntax-directed translations, respectively, of Aho and Ullman (1969; 1971). 3Without this requirement, there are certain pathological cases that cause the construction of Section 4.2 to produce infinite MM-TAGs. S → α0• α(x1, x2) ←� hx1, x2i α0• → α0• h6(), x2i ←� h−, x2i α0• → α1 • h−, x2i ←� h−, x2i h−, 6()i ←� h−, −i h−, −i ←� h−, −i α0• → β01[α0] hβ1(x1), x2i ←� hx1, x2i β01[α0]°� a β21[αo] d hx1, x2i H hx1, x2i β21[α ] → β01[α ] hβ1(x1), x2i ←� hx1, x2i β21[α0] → α0• h6(), x2i ←� h−, x2i α1• → β02[α1] h−,β2(x2)i ←� h−, x2i β02[α1] → bβ2[α1] c h−, x2i H h−, x2i β22[α1] → βZ[α1] h−,β2(x2)i ←� h−, x2i β2 2[α1] → α1• h−, ~()i ←� h−, −i Figure 4: CFG which si</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>A. V. Aho and J. D. Ullman. 1969. Syntax directed translations and the pushdown assembler. J. Comp. Sys. Sci, 3:37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Translations on a context free grammar. Information and Control,</title>
<date>1971</date>
<marker>Aho, Ullman, 1971</marker>
<rawString>A. V. Aho and J. D. Ullman. 1971. Translations on a context free grammar. Information and Control, 19:439–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
<author>Owen Rambow</author>
<author>Michael Niv</author>
</authors>
<title>The derivational generative power of formal systems, or, Scrambling is beyond LCFRS.</title>
<date>1992</date>
<tech>Technical Report IRCS-92-38,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="1863" citStr="Becker et al. (1992)" startWordPosition="290" endWordPosition="293">irst, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1999) gives a good synopsis). Figure 1: Example of weakly context-free TAG. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear contextfree rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Similarly, if we want to talk about squeezing strong generative power out of a formal system, we need to do so in the context of some larger space of structural descriptions. Second, why is preservation of weak generative power important? If we interpret this constraint to the letter, it is almost vacuous.</context>
<context position="7084" citStr="Becker et al., 1992" startWordPosition="1191" endWordPosition="1194">roperty of CFRS is that any formalism which can be defined as a CFRS immediately lends itself to several extensions, which arise when we give additional interpretations to the function symbols. For example, we can interpret the functions as ranging over probabilities, creating a stochastic grammar; or we can interpret them as yield functions of another grammar, creating a synchronous grammar. Now we define strong generative capacity as the relationship between strings and structural descriptions.1 1This is similar in spirit, but not the same as, the notion of derivational generative capacity (Becker et al., 1992). Definition 3 The strong generative capacity of a grammar G a CFRS Fis the relation {h~t•F , ti |t ∈ T(G)}. For example, the strong generative capacity of the grammar of Figure 3 is {hambncndm, α(βm 1 (�()), βn 2(~()))i} whereas any equivalent CFG must have a strong generative capacity of the form {hambncndm, fm(gn(e()))i} That is, in a CFG the n bs and cs must appear later in the derivation than the m as and ds, whereas in our example they appear in parallel. 3 Simulating structural descriptions We now take a closer look at some examples of “squeezed” context-free formalisms to illustrate ho</context>
</contexts>
<marker>Becker, Rambow, Niv, 1992</marker>
<rawString>Tilman Becker, Owen Rambow, and Michael Niv. 1992. The derivational generative power of formal systems, or, Scrambling is beyond LCFRS. Technical Report IRCS-92-38, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1333" citStr="Chomsky, 1965" startWordPosition="203" endWordPosition="204"> generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1999) gives a good synopsis). Figure 1: Example of weakly context-free TAG. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call l</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>A meta-level grammar: redefining synchronous TAG for translation and paraphrase.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Assocation for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>College Park, MD.</location>
<contexts>
<context position="15116" citStr="Dras, 1999" startWordPosition="2629" endWordPosition="2630">tion 4.2 we prove the following result: Proposition 2 A grammar G from a CFRS is simulable by a CFG if and only if it is trivially simulable by an MMTAG in regular form. The “if” direction (⇐) implies (because simulability is reflexive) that RF-MMTAG is simulable by a CFG, and therefore cubic-time parsable. (The proof below does give an effective procedure for constructing a simulating CFG for any RF-MMTAG.) The “only if” direction (⇒) shows that, in the sense we have defined, RF-MMTAG is the most powerful such formalism. We can generalize this result using the notion of a meta-level grammar (Dras, 1999). 1• 1 → α α • 1 α • → E A * A Y 3 X 1 Y 2 * X 1 * Definition 7 If f1 and f2 are two CFRSs, f2 o f1 is the CFRS characterized by the interpretation function [&apos;]f2of1 = [&apos;]f2 o [&apos;]f1. f1 is the meta-level formalism, which generates derivations for f2. Obviously f1 must be a treerewriting system. Proposition 3 For any CFRS f&apos;, a grammar G from a (possibly different) CFRS is simulable by a grammar in f&apos; if and only if it is trivially simulable by a grammar in f&apos; o RF-MMTAG. The “only if&apos; direction (=&gt;) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the sa</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Mark Dras. 1999. A meta-level grammar: redefining synchronous TAG for translation and paraphrase. In Proceedings of the 37th Annual Meeting of the Assocation for Computational Linguistics, pages 80–87, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<editor>In Alexis Manaster-Ramer, editor,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="947" citStr="Joshi, 1987" startWordPosition="139" endWordPosition="140">” and propose some theoretical and practical constraints on this problem. We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 1 Introduction “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vaguen</context>
<context position="15968" citStr="Joshi, 1987" startWordPosition="2792" endWordPosition="2793">ns for f2. Obviously f1 must be a treerewriting system. Proposition 3 For any CFRS f&apos;, a grammar G from a (possibly different) CFRS is simulable by a grammar in f&apos; if and only if it is trivially simulable by a grammar in f&apos; o RF-MMTAG. The “only if&apos; direction (=&gt;) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG. The “if&apos; direction (,t=) is a little trickier because the constructed CFG inserts and relabels nodes. 4 Multicomponent multifoot TAG 4.1 Definitions MMTAG resembles a cross between set-local multicomponent TAG (Joshi, 1987) and ranked node rewriting grammar (Abe, 1988), a variant of TAG in which auxiliary trees may have multiple foot nodes. It also has much in common with dtree substitution grammar (Rambow et al., 1995). Definition 8 An elementary tree set αl is a finite set of trees (called the components of lα) with the following properties: 1. Zero or more frontier nodes are designated foot nodes, which lack labels (following Abe), but are marked with the diacritic *; 2. Zero or more (non-foot) nodes are designated adjunction nodes, which are partitioned into one or more disjoint sets called adjunction sites.</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Aravind K. Joshi. 1987. An introduction to tree adjoining grammars. In Alexis Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Relationship between strong and weak generative power of formal systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth International Workshop on TAG and Related Formalisms (TAG+5),</booktitle>
<pages>107--113</pages>
<contexts>
<context position="775" citStr="Joshi (2000)" startWordPosition="114" endWordPosition="115">hiang@cis.upenn.edu Abstract We consider the question “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” and propose some theoretical and practical constraints on this problem. We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 1 Introduction “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sen</context>
</contexts>
<marker>Joshi, 2000</marker>
<rawString>Aravind K. Joshi. 2000. Relationship between strong and weak generative power of formal systems. In Proceedings of the Fifth International Workshop on TAG and Related Formalisms (TAG+5), pages 107– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip H Miller</author>
</authors>
<title>Strong Generative Capacity: The Semantics of Linguistic Formalism.</title>
<date>1999</date>
<journal>Number</journal>
<volume>103</volume>
<publisher>CSLI Publications, Stanford.</publisher>
<note>in CSLI lecture notes.</note>
<contexts>
<context position="1687" citStr="Miller (1999)" startWordPosition="263" endWordPosition="264">rs, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1999) gives a good synopsis). Figure 1: Example of weakly context-free TAG. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear contextfree rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Similarly, if we want to talk about squeezing strong generative power out of a formal system, we need to do so in the context of so</context>
</contexts>
<marker>Miller, 1999</marker>
<rawString>Philip H. Miller. 1999. Strong Generative Capacity: The Semantics of Linguistic Formalism. Number 103 in CSLI lecture notes. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Assocation for Computational Linguistics,</booktitle>
<pages>151--158</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="16168" citStr="Rambow et al., 1995" startWordPosition="2825" endWordPosition="2828">ly simulable by a grammar in f&apos; o RF-MMTAG. The “only if&apos; direction (=&gt;) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG. The “if&apos; direction (,t=) is a little trickier because the constructed CFG inserts and relabels nodes. 4 Multicomponent multifoot TAG 4.1 Definitions MMTAG resembles a cross between set-local multicomponent TAG (Joshi, 1987) and ranked node rewriting grammar (Abe, 1988), a variant of TAG in which auxiliary trees may have multiple foot nodes. It also has much in common with dtree substitution grammar (Rambow et al., 1995). Definition 8 An elementary tree set αl is a finite set of trees (called the components of lα) with the following properties: 1. Zero or more frontier nodes are designated foot nodes, which lack labels (following Abe), but are marked with the diacritic *; 2. Zero or more (non-foot) nodes are designated adjunction nodes, which are partitioned into one or more disjoint sets called adjunction sites. We notate this by assigning an index i to each adjunction site and marking each node of site i with the diacritic i. 3. Each component is associated with a symbol called its type. This is analogous t</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>Owen Rambow, K. Vijay-Shanker, and David Weir. 1995. D-tree grammars. In Proceedings of the 33rd Annual Meeting of the Assocation for Computational Linguistics, pages 151–158, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
</authors>
<title>Capturing CFLs with tree adjoining grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Assocation for Computational Linguistics,</booktitle>
<pages>155--162</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="1083" citStr="Rogers, 1994" startWordPosition="162" endWordPosition="163"> maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 1 Introduction “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1</context>
</contexts>
<marker>Rogers, 1994</marker>
<rawString>James Rogers. 1994. Capturing CFLs with tree adjoining grammars. In Proceedings of the 32nd Annual Meeting of the Assocation for Computational Linguistics, pages 155–162, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Lexicalized context-free grammars.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>121--129</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="1048" citStr="Schabes and Waters, 1993" startWordPosition="154" endWordPosition="157">uce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 1 Introduction “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be </context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Yves Schabes and Richard C. Waters. 1993. Lexicalized context-free grammars. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 121–129, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
<author>Aravind Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="1803" citStr="Vijay-Shanker et al. (1987)" startWordPosition="279" endWordPosition="282">r with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1999) gives a good synopsis). Figure 1: Example of weakly context-free TAG. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear contextfree rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Similarly, if we want to talk about squeezing strong generative power out of a formal system, we need to do so in the context of some larger space of structural descriptions. Second, why is preservation of weak generative power important? If we in</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David Weir, and Aravind Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 104–111, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly ContextSensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="1819" citStr="Weir (1988)" startWordPosition="284" endWordPosition="285">in its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions E(G); the strong generative capacity of a formalism Fis then {E(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1999) gives a good synopsis). Figure 1: Example of weakly context-free TAG. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear contextfree rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Similarly, if we want to talk about squeezing strong generative power out of a formal system, we need to do so in the context of some larger space of structural descriptions. Second, why is preservation of weak generative power important? If we interpret this con</context>
<context position="5133" citStr="Weir (1988)" startWordPosition="838" endWordPosition="839">en restricted to a regular form, characterizes precisely those CFRSs which are simulable by a CFG. Thus, in the sense we have set forth, this formalism can be said to squeeze as much strong generative power out of CFG as is possible. Finally, we generalize this result to formalisms beyond CFG. 2 Characterizing structural descriptions First we define context-free rewriting systems. What these formalisms have in common is that their derivation sets are all local sets (that is, generable by a CFG). These derivations are taken as structural descriptions. The following definitions are adapted from Weir (1988). Definition 1 A generalized context-free grammar G is a tuple (V, S, F, P), where 1. V is a finite set of variables, 2. S E V is a distinguished start symbol, 3. F is a finite set of function symbols, and E S —� α(X, Y) α((x1, x2), (y1,y2)) = x1y1y2x2 X —� P1(X) P1((x1, x2)) = (ax1, x2d) X —� E() E() = (E, E) Y —� P2(Y) P2((y1,y2)) = (by1,y2c) Y —� E() E() = (E, E) Figure 3: Example of TAG with corresponding GCFG and interpretation. Here adjunction at foot nodes is allowed. 4. P is a finite set of productions of the form A —� f(A1, ..., An) where n &gt;_ 0, f E F, and A, Ai E V. A generalized CF</context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David J. Weir. 1988. Characterizing Mildly ContextSensitive Grammar Formalisms. Ph.D. thesis, Univ. of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>