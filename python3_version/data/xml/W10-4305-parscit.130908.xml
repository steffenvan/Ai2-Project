<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.967236">
Evaluation Metrics For End-to-End Coreference Resolution Systems
</title>
<author confidence="0.604767">
Jie Cai and Michael Strube
</author>
<affiliation confidence="0.530722">
Heidelberg Institute for Theoretical Studies gGmbH
</affiliation>
<address confidence="0.394988">
Schloß-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</address>
<email confidence="0.613546">
(jie.cai|michael.strube)@h-its.org
</email>
<sectionHeader confidence="0.98648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998752636363637">
Commonly used coreference resolution
evaluation metrics can only be applied to
key mentions, i.e. already annotated men-
tions. We here propose two variants of the
B3 and CEAF coreference resolution eval-
uation algorithms which can be applied
to coreference resolution systems dealing
with system mentions, i.e. automatically
determined mentions. Our experiments
show that our variants lead to intuitive and
reliable results.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958574468086">
The coreference resolution problem can be di-
vided into two steps: (1) determining mentions,
i.e., whether an expression is referential and
can take part in a coreferential relationship, and
(2) deciding whether mentions are coreferent or
not. Most recent research on coreference res-
olution simplifies the resolution task by provid-
ing the system with key mentions, i.e. already an-
notated mentions (Luo et al. (2004), Denis &amp;
Baldridge (2007), Culotta et al. (2007), Haghighi
&amp; Klein (2007), inter alia; see also the task de-
scription of the recent SemEval task on coref-
erence resolution at http://stel.ub.edu/
semeval2010-coref), or ignores an impor-
tant part of the problem by evaluating on key men-
tions only (Ponzetto &amp; Strube, 2006; Bengtson &amp;
Roth, 2008, inter alia). We follow here Stoyanov
et al. (2009, p.657) in arguing that such evalua-
tions are “an unrealistic surrogate for the original
problem” and ask researchers to evaluate end-to-
end coreference resolution systems.
However, the evaluation of end-to-end coref-
erence resolution systems has been inconsistent
making it impossible to compare the results. Nico-
lae &amp; Nicolae (2006) evaluate using the MUC
score (Vilain et al., 1995) and the CEAF algorithm
(Luo, 2005) without modifications. Yang et al.
(2008) use only the MUC score. Bengtson &amp; Roth
(2008) and Stoyanov et al. (2009) derive variants
from the B3 algorithm (Bagga &amp; Baldwin, 1998).
Rahman &amp; Ng (2009) propose their own variants
of B3 and CEAF. Unfortunately, some of the met-
rics’ descriptions are so concise that they leave too
much room for interpretation. Also, some of the
metrics proposed are too lenient or are more sen-
sitive to mention detection than to coreference res-
olution. Hence, though standard corpora are used,
the results are not comparable.
This paper attempts to fill that desideratum by
analysing several variants of the B3 and CEAF al-
gorithms. We propose two new variants, namely
B3sys and CEAFsys, and provide algorithmic de-
tails in Section 2. We describe two experiments in
Section 3 showing that B3sys and CEAFsys lead to
intuitive and reliable results. Implementations of
B3sys and CEAFsys are available open source along
with extended examples1.
</bodyText>
<sectionHeader confidence="0.974665" genericHeader="method">
2 Coreference Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.9999718">
We discuss the problems which arise when apply-
ing the most prevalent coreference resolution eval-
uation metrics to end-to-end systems and propose
our variants which overcome those problems. We
provide detailed analyses of illustrative examples.
</bodyText>
<subsectionHeader confidence="0.745213">
2.1 MUC
</subsectionHeader>
<bodyText confidence="0.999360888888889">
The MUC score (Vilain et al., 1995) counts
the minimum number of links between mentions
to be inserted or deleted when mapping a sys-
tem response to a gold standard key set. Al-
though pairwise links capture the information
in a set, they cannot represent singleton en-
tities, i.e. entities, which are mentioned only
once. Therefore, the MUC score is not suitable
for the ACE data (http://www.itl.nist.
</bodyText>
<footnote confidence="0.987774">
1http://www.h-its.org/nlp/download
</footnote>
<note confidence="0.786272">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28–36,
The University of Tokyo, September 24-25, 2010. p@c 2010 Association for Computational Linguistics
</note>
<page confidence="0.998198">
28
</page>
<bodyText confidence="0.999864833333333">
gov/iad/mig/tests/ace/), which includes
singleton entities in the keys. Moreover, the MUC
score does not give credit for separating singleton
entities from other chains. This becomes problem-
atic in a realistic system setup, when mentions are
extracted automatically.
</bodyText>
<subsectionHeader confidence="0.780431">
2.2 B3
</subsectionHeader>
<bodyText confidence="0.997214375">
The B3 algorithm (Bagga &amp; Baldwin, 1998) over-
comes the shortcomings of the MUC score. In-
stead of looking at the links, B3 computes preci-
sion and recall for all mentions in the document,
which are then combined to produce the final pre-
cision and recall numbers for the entire output.
For each mention, the B3 algorithm computes a
precision and recall score using equations 1 and 2:
</bodyText>
<equation confidence="0.99459925">
Precision(mi) = |Rmi ∩ Kmi |(1)
|Rmi|
Recall(mi) = |Rmi ∩ Kmi |(2)
|Kmi|
</equation>
<bodyText confidence="0.999915285714286">
where Rmi is the response chain (i.e. the system
output) which includes the mention mi, and Kmi
is the key chain (manually annotated gold stan-
dard) with mi. The overall precision and recall are
computed by averaging them over all mentions.
Since B3’s calculations are based on mentions,
singletons are taken into account. However, a
problematic issue arises when system mentions
have to be dealt with: B3 assumes the mentions in
the key and in the response to be identical. Hence,
B3 has to be extended to deal with system men-
tions which are not in the key and key mentions
not extracted by the system, so called twinless
mentions (Stoyanov et al., 2009).
</bodyText>
<subsectionHeader confidence="0.972865">
2.2.1 Existing B3 variants
</subsectionHeader>
<bodyText confidence="0.968612333333334">
A few variants of the B3 algorithm for dealing with
system mentions have been introduced recently.
Stoyanov et al. (2009) suggest two variants of the
B3 algorithm to deal with system mentions, B30 and
2. For example, a key and a response are pro-
vided as below:
</bodyText>
<construct confidence="0.8413215">
Key: {a b c}
Response: {a b d}
</construct>
<bodyText confidence="0.97944325">
B30 discards all twinless system mentions (i.e.
mention d) and penalizes recall by setting
recallmi = 0 for all twinless key mentions (i.e.
mention c). The B30 precision, recall and F-score
</bodyText>
<footnote confidence="0.922379333333333">
2Our discussion of B30 and B3 all is based on the analysis
of the source code available at http://www.cs.utah.
edu/nlp/reconcile/.
</footnote>
<table confidence="0.99792408">
Set 1
System 1 key {a b c}
response {a b d}
P R F
B3 1.0 0.444 0.615
0
B3 0.556 0.556 0.556
all
B3 0.556 0.556 0.556
r&amp;n
B3 0.667 0.556 0.606
sys
CEAFsys 0.5 0.667 0.572
System 2 key {a b c}
response {a b d e}
P R F
B3 1.0 0.444 0.615
0
B3 0.375 0.556 0.448
all
B3 0.375 0.556 0.448
r&amp;n
B3 0.5 0.556 0.527
sys
CEAFsys 0.4 0.667 0.500
</table>
<tableCaption confidence="0.999443">
Table 1: Problems of B30
</tableCaption>
<equation confidence="0.98981725">
(i.e. F = 2 · Precision-Recall ) for the example are
Precision+Recall
calculated as:
1
PrB3 = 2 (22 + 22) = 1.0
0
1
RecB3 = 3(2 3 + 2 3 + 0) . = 0.444
0
.
FB×.
0 = 2 × i.0+0.444 = 0.615
</equation>
<bodyText confidence="0.995753">
B3all retains twinless system mentions. It assigns
1/|Rmi |to a twinless system mention as its preci-
sion and similarly 1/|Kmi |to a twinless key men-
tion as its recall. For the same example above, the
B3all precision, recall and F-score are given by:
</bodyText>
<equation confidence="0.993279285714286">
1 3) .
= 3(2 3 + 2 3 + 1 = 0.556
RecB3all = 13 (23 + 23 + 13) = . 0.556
.
FB 0.556 × 0.556
all = 2 × = 0.556
0.556+0.444
</equation>
<bodyText confidence="0.997451142857143">
Tables 1, 2 and 3 illustrate the problems with B30
and B3all. The rows labeled System give the origi-
nal keys and system responses while the rows la-
beled B30, B3all and B3sys show the performance gen-
erated by Stoyanov et al.’s variants and the one
we introduce in this paper, B3 sys (the row labeled
CEAFsys is discussed in Subsection 2.3).
In Table 1, there are two system outputs (i.e.
System 1 and System 2). Mentions d and e are
the twinless system mentions erroneously resolved
and c a twinless key mention. System 1 is sup-
posed to be slightly better with respect to preci-
sion, because System 2 produces one more spu-
rious resolution (i.e. for mention e ). However,
B30 computes exactly the same numbers for both
systems. Hence, there is no penalty for erroneous
coreference relations in B30, if the mentions do not
appear in the key, e.g. putting mentions d or e in
Set 1 does not count as precision errors. — B30
is too lenient by only evaluating the correctly ex-
tacted mentions.
</bodyText>
<figure confidence="0.816935">
3
Ball
PrB3
all
</figure>
<page confidence="0.996402">
29
</page>
<table confidence="0.999366809523809">
Set 1 Singletons
System 1 key {a b c}
response {a b d}
P R F
B3 0.556 0.556 0.556
all
B3 0.556 0.556 0.556
r&amp;n
B3 0.667 0.556 0.606
sys
CEAFsys 0.5 0.667 0.572
System 2 key {a b c} {c}
response {a b d}
P R F
B3 0.667 0.556 0.606
all
B3 0.667 0.556 0.606
r&amp;n
B3 0.667 0.556 0.606
sys
CEAFsys 0.5 0.667 0.572
</table>
<tableCaption confidence="0.96636">
Table 2: Problems of B3all (1)
</tableCaption>
<table confidence="0.999744095238095">
Set 1 Singletons
System 1 key {a b}
response {a b d}
P R F
B3 0.556 1.0 0.715
all
B3 0.556 1.0 0.715
r&amp;n
B3 0.556 1.0 0.715
sys
CEAFsys 0.667 1.0 0.800
System 2 key {a b} {i} {j} {k}
response {a b d}
P R F
B3 0.778 1.0 0.875
all
B3 0.556 1.0 0.715
r&amp;n
B3 0.556 1.0 0.715
sys
CEAFsys 0.667 1.0 0.800
</table>
<tableCaption confidence="0.999817">
Table 3: Problems of B3all (2)
</tableCaption>
<bodyText confidence="0.998452238095238">
B3all deals well with the problem illustrated in
Table 1, the figures reported correspond to in-
tuition. However, B3all can output different re-
sults for identical coreference resolutions when
exposed to different mention taggers as shown in
Tables 2 and 3. B3all manages to penalize erro-
neous resolutions for twinless system mentions,
however, it ignores twinless key mentions when
measuring precision. In Table 2, System 1 and Sys-
tem 2 generate the same outputs, except that the
mention tagger in System 2 also extracts mention
c. Intuitively, the same numbers are expected for
both systems. However, B3all gives a higher preci-
sion to System 2, which results in a higher F-score.
B3all retains all twinless system mentions, as can
be seen in Table 3. System 2’s mention tagger tags
more mentions (i.e. the mentions i, j and k), while
both System 1 and System 2 have identical coref-
erence resolution performance. Still, B3all outputs
quite different results for precision and thus for F-
score. This is due to the credit B3all takes from un-
resolved singleton twinless system mentions (i.e.
mention i, j, k in System 2). Since the metric is ex-
pected to evaluate the end-to-end coreference sys-
tem performance rather than the mention tagging
quality, it is not satisfying to observe that B3all’s
numbers actually fluctuate when the system is ex-
posed to different mention taggers.
Rahman &amp; Ng (2009) apply another variant, de-
noted here as B3r&amp;n. They remove only those twin-
less system mentions that are singletons before ap-
plying the B3 algorithm. So, a system would not
be rewarded by the the spurious mentions which
are correctly identified as singletons during reso-
lution (as has been the case with B3all’s higher pre-
cision for System 2 as can be seen in Table 3).
We assume that Rahman &amp; Ng apply a strategy
similar to B3all after the removing step (this is not
clear in Rahman &amp; Ng (2009)). While it avoids the
problem with singleton twinless system mentions,
B3r&amp;n still suffers from the problem dealing with
twinless key mentions, as illustrated in Table 2.
</bodyText>
<subsectionHeader confidence="0.886954">
2.2.2 B3sys
</subsectionHeader>
<bodyText confidence="0.999936304347826">
We here propose a coreference resolution evalua-
tion metric, B3sys, which deals with system men-
tions more adequately (see the rows labeled B3sys
in Tables 1, 2, 3, 4 and 5). We put all twinless key
mentions into the response as singletons which en-
ables B3sys to penalize non-resolved coreferent key
mentions without penalizing non-resolved single-
ton key mentions, and also avoids the problem B3all
and B3r&amp;n have as shown in Table 2. All twinless
system mentions which were deemed not coref-
erent (hence being singletons) are discarded. To
calculate B3sys precision, all twinless system men-
tions which were mistakenly resolved are put into
the key since they are spurious resolutions (equiv-
alent to the assignment operations in B3all), which
should be penalized by precision. Unlike B3all,
B3sys does not benefit from unresolved twinless
system mentions (i.e. the twinless singleton sys-
tem mentions). For recall, the algorithm only goes
through the original key sets, similar to B3all and
B3r&amp;n. Details are given in Algorithm 1.
For example, a coreference resolution system
has the following key and response:
</bodyText>
<construct confidence="0.629641">
Key: {a b c}
Response: {a b d} {i j}
</construct>
<bodyText confidence="0.908867333333333">
To calculate the precision of B3
sys, the key and re-
sponse are altered to:
</bodyText>
<footnote confidence="0.38559">
Keyp : {a b c} {d} {i} {j}
Responsep: {a b d} {i j} {c}
</footnote>
<page confidence="0.98841">
30
</page>
<bodyText confidence="0.859772666666667">
Algorithm 1 B3sys
Input: key sets key, response sets response
Output: precision P, recall R and F-score F
</bodyText>
<listItem confidence="0.959755222222222">
1: Discard all the singleton twinless system mentions in
response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions
with key to form keyp;
5: Use response to form responsep
6: Through keyp and responsep;
7: Calculate B3 precision P.
8: end if
9: if calculating recall then
10: Discard all the remaining twinless system mentions in
response to from responser;
11: Use key to form keyr
12: Through key3r and responser;
13: Calculate B recall R
14: end if
15: Calculate F-score F
</listItem>
<bodyText confidence="0.586122">
So, the precision of B3sys is given by:
</bodyText>
<equation confidence="0.661392777777778">
PrB3sys = 16(23+23+31+21+21+1)=.0.611
The modified key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}
The resulting recall of B3sys is:
RecB3 sys = 13(23 + 23 + 13) .= 0.556
Thus the F-score number is calculated as:
F 3 = 2 x 0.611 × 0.556 . 0.582
Bsys 0.611+0.556
</equation>
<bodyText confidence="0.977736">
B3sys indicates more adequately the performance of
end-to-end coreference resolution systems. It is
not easily tricked by different mention taggers3.
</bodyText>
<subsectionHeader confidence="0.938048">
2.3 CEAF
</subsectionHeader>
<bodyText confidence="0.999891571428572">
Luo (2005) criticizes the B3 algorithm for using
entities more than one time, because B3 computes
precision and recall of mentions by comparing en-
tities containing that mention. Hence Luo pro-
poses the CEAF algorithm which aligns entities in
key and response. CEAF applies a similarity met-
ric (which could be either mention based or entity
based) for each pair of entities (i.e. a set of men-
tions) to measure the goodness of each possible
alignment. The best mapping is used for calculat-
ing CEAF precision, recall and F-measure.
Luo proposes two entity based similarity met-
rics (Equation 3 and 4) for an entity pair (Ki, Rj)
originating from key, Ki, and response, Rj.
</bodyText>
<equation confidence="0.99856875">
φ3(Ki,Rj) = |Ki ∩ Rj |(3)
2|Ki ∩ Rj|
φ4(Ki,Rj) = (4)
|Ki |+ |Rj|
</equation>
<footnote confidence="0.559024">
3Further example analyses can be found in Appendix A.
</footnote>
<bodyText confidence="0.997782">
The CEAF precision and recall are derived from
the alignment which has the best total similarity
(denoted as Φ(g*)), shown in Equations 5 and 6.
</bodyText>
<equation confidence="0.9906786">
*
Precision = Φ(g*) (5)
Ei φ(Ri, Ri)
*
Recall = &amp; O(Ki, KZ) (6)
</equation>
<bodyText confidence="0.87590375">
If not specified otherwise, we apply Luo’s φ3(⋆, ⋆)
in the example illustrations. We denote the origi-
nal CEAF algorithm as CEAForig.
Detailed calculations are illustrated below:
</bodyText>
<equation confidence="0.846661818181818">
Key: {a b c}
Response: {a b d}
The CEAForig φ3(⋆, ⋆) are given by:
03(K1, R1) = 2 (K1 : {abc}; R1 : {abd})
03(K1, K1) = 3
03(R1, R1) = 3
So the CEAForig evaluation numbers are:
PrCEAForig = 23 = 0.667
RecCEAForig = 23 = 0.667
FCEAF = 2 X 0.667×0.667 = 0.667
or*s 0.667+0.667
</equation>
<subsectionHeader confidence="0.887296">
2.3.1 Problems of CEAForig
</subsectionHeader>
<bodyText confidence="0.999944052631579">
CEAForig was intended to deal with key mentions.
Its adaptation to system mentions has not been ad-
dressed explicitly. Although CEAForig theoreti-
cally does not require to have the same number of
mentions in key and response, it still cannot be di-
rectly applied to end-to-end systems, because the
entity alignments are based on mention mappings.
As can be seen from Table 4, CEAForig fails
to produce intuitive results for system mentions.
System 2 outputs one more spurious entity (con-
taining mention i and j) than System 1 does, how-
ever, achieves a same CEAForig precision. Since
twinless system mentions do not have mappings in
key, they contribute nothing to the mapping simi-
larity. So, resolution mistakes for system mentions
are not calculated, and moreover, the precision is
easily skewed by the number of output entities.
CEAForig reports very low precision for system
mentions (see also Stoyanov et al. (2009)).
</bodyText>
<subsectionHeader confidence="0.763801">
2.3.2 Existing CEAF variants
</subsectionHeader>
<bodyText confidence="0.999671428571429">
Rahman &amp; Ng (2009) briefly introduce their
CEAF variant, which is denoted as CEAFr&amp;n
here. They use φ3(⋆, ⋆), which results in equal
CEAFr&amp;n precision and recall figures when using
true mentions. Since Rahman &amp; Ng’s experiments
using system mentions produce unequal precision
and recall figures, we assume that, after removing
</bodyText>
<page confidence="0.999361">
31
</page>
<table confidence="0.9995418">
Set 1 Set 2 Singletons
System 1 key {a b c} {c} {i} {j}
response {a b}
P R F
CEAForig 0.4 0.667 0.500
B3 1.0 0.556 0.715
sys
CEAFsys 0.667 0.667 0.667
System 2 key {a b c} {i j} {c}
response {a b}
P R F
CEAForig 0.4 0.667 0.500
B3 0.8 0.556 0.656
sys
CEAFsys 0.6 0.667 0.632
</table>
<tableCaption confidence="0.97817">
Table 4: Problems of CEAForig
</tableCaption>
<table confidence="0.999866666666667">
Set 1 Set 2 Set 3 Singletons
System 1 key {a b c} {i j} {k l} {c}
response {a b}
P R F
CEAFr&amp;n 0.286 0.667 0.400
B3 0.714 0.556 0.625
sys
CEAFsys 0.571 0.667 0.615
System 2 key {a b c} {i j k l} {c}
response {a b}
P R F
CEAFr&amp;n 0.286 0.667 0.400
B3 0.571 0.556 0.563
sys
CEAFsys 0.429 0.667 0.522
</table>
<tableCaption confidence="0.999506">
Table 5: Problems of CEAFr&amp;n
</tableCaption>
<bodyText confidence="0.998652555555556">
twinless singleton system mentions, they do not
put any twinless mentions into the other set. In the
example in Table 5, CEAFr&amp;n does not penalize
adequately the incorrectly resolved entities con-
sisting of twinless sytem mentions. So CEAFr&amp;n
does not tell the difference between System 1 and
System 2. It can be concluded from the examples
that the same number of mentions in key and re-
sponse is needed for computing the CEAF score.
</bodyText>
<subsectionHeader confidence="0.733262">
2.3.3 CEAFsys
</subsectionHeader>
<bodyText confidence="0.957327615384615">
We propose to adjust CEAF in the same way as
we did for B3 sys, resulting in CEAFsys. We put
all twinless key mentions into the response as sin-
gletons. All singleton twinless system mentions
are discarded. For calculating CEAFsys precision,
all twinless system mentions which were mistak-
enly resolved are put into the key. For computing
CEAFsys recall, only the original key sets are con-
sidered. That way CEAFsys deals adequately with
system mentions (see Algorithm 2 for details).
Algorithm 2 CEAFsys
Input: key sets key, response sets response
Output: precision P, recall R and F-score F
</bodyText>
<listItem confidence="0.978280235294118">
1: Discard all the singleton twinless system mentions in
response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions
with key to form keyp;
5: Use response to form responsep
6: Form Map g⋆ between keyp and responsep
7: Calculate CEAF precision P using 03(*, *)
8: end if
9: if calculating recall then
10: Discard all the remaining twinless system mentions in
response to form responser;
11: Use key to form keyr
12: Form Map g⋆ between keyr and responser
13: Calculate CEAF recall R using 03(*, *)
14: end if
</listItem>
<page confidence="0.617763">
15: Calculate F-score F
</page>
<bodyText confidence="0.868308">
Taking System 2 in Table 4 as an example, key and
response are altered for precision:
</bodyText>
<equation confidence="0.973261416666667">
Keyp : {a b c} {i} {j}
Responsep: {a b d} {i j} {c}
So the 03(*, *) are as below, only listing the best
mappings:
03(K1, R1) = 2 (K1 : {abc}; R1 : {abd})
03(K2, R2) = 1 (K2 : {i}; R2 : {ij})
03(∅, R3) = 0 (R3 : {c})
03(R1, R1) = 3
03(R2, R2) = 2
03(R3, R3) = 1
The precision is thus give by:
PrCEAFsys = 2+1+0 3+2+1 = 0.6
</equation>
<bodyText confidence="0.688243">
The key and response for recall are:
</bodyText>
<equation confidence="0.99681475">
Keyr : {a b c}
Responser: {a b} {c}
The resulting 03(*, *) are:
03(K1, R1) = 2(K1 : {abc}; R1 : {ab})
03(∅, R2) = 0(R2 : {c})
03(K1, K1) = 3
03(R1, R1) = 2
03(R2, R2) = 1
</equation>
<bodyText confidence="0.578209">
The recall and F-score are thus calculated as:
</bodyText>
<equation confidence="0.996166666666667">
RecCEAFsys = 23 = 0.667
FCEAFsys = 2 × 0.6×0.667
0.6+0.667 = 0.632
</equation>
<bodyText confidence="0.999929857142857">
However, one additional complication arises
with regard to the similarity metrics used by
CEAF. It turns out that only 03(*, *) is suitable
for dealing with system mentions while 04(*,*)
produces uninituitive results (see Table 6).
04(*, *) computes a normalized similarity for
each entity pair using the summed number of men-
tions in the key and the response. CEAF precision
then distributes that similarity evenly over the re-
sponse set. Spurious system entities, such as the
one with mention i and j in Table 6, are not pe-
nalized. 03(*, *) calculates unnormalized similar-
ities. It compares the two systems in Table 6 ade-
quately. Hence we use only 03(*, *) in CEAFsys.
</bodyText>
<page confidence="0.998414">
32
</page>
<table confidence="0.999337181818182">
Set 1 Singletons
System 1 key {a b c} {c} {i} {j}
response {a b}
P R F
φ�(⋆, ⋆) 0.4 0.8 0.533
φ3(⋆, ⋆) 0.667 0.667 0.667
System 2 key {a b c} {c}
response {a b} {i j}
P R F
φ�(⋆, ⋆) 0.489 0.8 0.607
φ3(⋆, ⋆) 0.6 0.667 0.632
</table>
<tableCaption confidence="0.998861">
Table 6: Problems of φ4(⋆, ⋆)
</tableCaption>
<bodyText confidence="0.999949625">
When normalizing the similarities by the num-
ber of entities or mentions in the key (for recall)
and the response (for precision), the CEAF al-
gorithm considers all entities or mentions to be
equally important. Hence CEAF tends to compute
quite low precision for system mentions which
does not represent the system performance ade-
quately. Here, we do not address this issue.
</bodyText>
<subsectionHeader confidence="0.896922">
2.4 BLANC
</subsectionHeader>
<bodyText confidence="0.999981">
Recently, a new coreference resolution evalua-
tion algorithm, BLANC, has been introduced (Re-
casens &amp; Hovy, 2010). This measure implements
the Rand index (Rand, 1971) which has been orig-
inally developed to evaluate clustering methods.
The BLANC algorithm deals correctly with sin-
gleton entities and rewards correct entities accord-
ing to the number of mentions. However, a ba-
sic assumption behind BLANC is, that the sum of
all coreferential and non-coreferential links is con-
stant for a given set of mentions. This implies that
BLANC assumes identical mentions in key and re-
sponse. It is not clear how to adapt BLANC to sys-
tem mentions. We do not address this issue here.
</bodyText>
<sectionHeader confidence="0.998478" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999265666666667">
While Section 2 used toy examples to motivate our
metrics B3sys and CEAFsys, we here report results
on two larger experiments using ACE2004 data.
</bodyText>
<subsectionHeader confidence="0.99959">
3.1 Data and Mention Taggers
</subsectionHeader>
<bodyText confidence="0.999992111111111">
We use the ACE2004 (Mitchell et al., 2004) En-
glish training data which we split into three sets
following Bengtson &amp; Roth (2008): Train (268
docs), Dev (76), and Test (107). We use two in-
house mention taggers. The first (SM1) imple-
ments a heuristic aiming at high recall. The second
(SM2) uses the J48 decision tree classifier (Wit-
ten &amp; Frank, 2005). The number of detected men-
tions, head coverage, and accuracy on testing data
</bodyText>
<equation confidence="0.648073666666667">
SM1 SM2
31,370 16,081
13,072 14,179
development mentions
twin mentions
8,387 4,956
4,242 4,212
79.3% 73.3%
57.3% 81.2%
</equation>
<tableCaption confidence="0.948038">
Table 7: Mention Taggers on ACE2004 Data
are shown in Table 7.
</tableCaption>
<subsectionHeader confidence="0.999618">
3.2 Artificial Setting
</subsectionHeader>
<bodyText confidence="0.9993988">
For the artificial setting we report results on the
development data using the SM1 tagger. To illus-
trate the stability of the evaluation metrics with
respect to different mention taggers, we reduce
the number of twinless system mentions in inter-
vals of 10%, while correct (non-twinless) ones are
kept untouched. The coreference resolution sys-
tem used is the BART (Versley et al., 2008) reim-
plementation of Soon et al. (2001). The results are
plotted in Figures 1 and 2.
</bodyText>
<subsectionHeader confidence="0.385797">
Proportion of twinless system mentions used in the experiment
</subsectionHeader>
<figureCaption confidence="0.9999865">
Figure 1: Artificial Setting B3 Variants
Figure 2: Artificial Setting CEAF Variants
</figureCaption>
<figure confidence="0.990796972972973">
F-score for ACE04 Development Data 0.75
0.7
0.65
0.6
0.55
0.85
0.8
MUC
BCubedsys
BCubed0
BCubedall
BCubedng
1 0.8 0.6 0.4 0.2 0
training mentions
twin mentions
test mentions
twin mentions
head coverage
accuracy
8,045
3,371
1 0.8 0.6 0.4 0.2 0
F-score for ACE04 Development Data
0.75
0.65
0.55
0.45
0.8
0.7
0.6
0.5
0.4
MUC
CEAFsys
CEAForig
CEAFng
Proportion of twinless system mentions used in the experiment
</figure>
<page confidence="0.972068">
33
</page>
<table confidence="0.9998945">
R MUC F
Pr
Soon (SM1) 51.7 53.1 52.4
Soon (SM2) 49.1 69.9 57.7
</table>
<tableCaption confidence="0.877262">
Table 8: Realistic Setting MUC
</tableCaption>
<table confidence="0.9999728">
R B3 F R B3 F
sys 0
Pr Pr
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4
Bengtson 66.1 81.9 73.1 69.5 74.7 72.0
</table>
<tableCaption confidence="0.997358">
Table 11: Realistic Setting
</tableCaption>
<bodyText confidence="0.999953">
Omitting twinless system mentions from the
training data while keeping the number of cor-
rect mentions constant should improve the corefer-
ence resolution performance, because a more pre-
cise coreference resolution model is obtained. As
can be seen from Figures 1 and 2, the MUC-score,
B3sys and CEAFsys follow this intuition.
B30 is almost constant. It does not take twinless
mentions into account. B3all ’s curve, also, has a
lower slope in comparison to B3sys and MUC (i.e.
B3all computes similar numbers for worse models).
This shows that the B3all score can be tricked by
using a high recall mention tagger, e.g. in cases
with the worse models (i.e. ones on the left side
of the figures) which have much more twinless
system mentions. The original CEAF algorithm,
CEAForig, is too sensitive to the input system
mentions making it less reliable. CEAFsys is par-
allel to B3sys. Thus both of our metrics exhibit the
same intuition.
</bodyText>
<subsectionHeader confidence="0.9389385">
3.3 Realistic Setting
3.3.1 Experiment 1
</subsectionHeader>
<bodyText confidence="0.999887606060606">
For the realistic setting we compare SM1 and SM2
as preprocessing components for the BART (Ver-
sley et al., 2008) reimplementation of Soon et al.
(2001). The coreference resolution system with
the SM2 tagger performs better, because a better
coreference model is achieved from system men-
tions with higher accuracy.
The MUC, B3sys and CEAFsys metrics have the
same tendency when applied to systems with dif-
ferent mention taggers (Table 8, 9 and 10 and the
bold numbers are higher with a p-value of 0.05,
by a paired-t test). Since the MUC scorer does
not evaluate singleton entities, it produces too low
numbers which are not informative any more.
As shown in Table 9, B3all reports counter-
intuitive results when a system is fed with system
mentions generated by different mention taggers.
B3all cannot be used to evaluate two different end-
to-end coreference resolution systems, because the
mention tagger is likely to have bigger impact than
the coreference resolution system. B30 fails to gen-
erate the right comparison too, because it is too
lenient by ignoring all twinless mentions.
The CEAForig numbers in Table 10 illustrate the
big influence the system mentions have on preci-
sion (e.g. the very low precision number for Soon
(SM1)). The big improvement for Soon (SM2) is
largely due to the system mentions it uses, rather
than to different coreference models.
Both B3r&amp;n and CEAFr&amp;n show no serious prob-
lems in the experimental results. However, as dis-
cussed before, they fail to penalize the spurious
entities with twinless system mentions adequately.
</bodyText>
<sectionHeader confidence="0.753653" genericHeader="method">
3.3.2 Experiment 2
</sectionHeader>
<bodyText confidence="0.999954470588235">
We compare results of Bengtson &amp; Roth’s (2008)
system with our Soon (SM2) system. Bengtson &amp;
Roth’s embedded mention tagger aims at high pre-
cision, generating half of the mentions SM1 gen-
erates (explicit statistics are not available to us).
Bengtson &amp; Roth report a B3 F-score for sys-
tem mentions, which is very close to the one for
true mentions. Their B3-variant does not impute
errors of twinless mentions and is assumed to be
quite similar to the B3 0 strategy.
We integrate both the B30 and B3sys variants into
their system and show results in Table 11 (we can-
not report significance, because we do not have ac-
cess to results for single documents in Bengtson &amp;
Roth’s system). It can be seen that, when different
variants of evaluation metrics are applied, the per-
formance of the systems vary wildly.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999947">
In this paper, we address problems of commonly
used evaluation metrics for coreference resolution
and suggest two variants for B3 and CEAF, called
B3sys and CEAFsys. In contrast to the variants
proposed by Stoyanov et al. (2009), B3sys and
CEAFsys are able to deal with end-to-end systems
which do not use any gold information. The num-
bers produced by B3sys and CEAFsys are able to
indicate the resolution performance of a system
more adequately, without being tricked easily by
twisting preprocessing components. We believe
that the explicit description of evaluation metrics,
as given in this paper, is a precondition for the re-
</bodyText>
<page confidence="0.995391">
34
</page>
<table confidence="0.9994506">
B3 B3 B3 B3
sys 0 all r&amp;n
R Pr F R Pr F R Pr F R Pr F
Soon (SM1) 65.7 76.8 70.8 57.0 91.1 70.1 65.1 85.8 74.0 65.1 78.7 71.2
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4 64.3 87.1 73.9 64.3 84.9 73.2
</table>
<tableCaption confidence="0.86733">
Table 9: Realistic Setting B3 Variants
</tableCaption>
<table confidence="0.99988875">
R CEAFsys F R CEAForig F R CEAFr&amp;n F
Pr Pr Pr
Soon (SM1) 66.4 61.2 63.7 62.0 39.9 48.5 62.1 59.8 60.9
Soon (SM2) 67.4 65.2 66.3 60.0 56.6 58.2 60.0 66.2 62.9
</table>
<tableCaption confidence="0.998855">
Table 10: Realistic Setting CEAF Variants
</tableCaption>
<bodyText confidence="0.98595075">
liabe comparison of end-to-end coreference reso-
lution systems.
Acknowledgements. This work has been
funded by the Klaus Tschira Foundation, Hei-
delberg, Germany. The first author has been
supported by a HITS PhD. scholarship. We
would like to thank ´Eva M´ujdricza-Maydt for
implementing the mention taggers.
</bodyText>
<sectionHeader confidence="0.997045" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999622663043478">
Bagga, Amit &amp; Breck Baldwin (1998). Algorithms for scor-
ing coreference chains. In Proceedings of the 1st Inter-
national Conference on Language Resources and Evalua-
tion, Granada, Spain, 28–30 May 1998, pp. 563–566.
Bengtson, Eric &amp; Dan Roth (2008). Understanding the value
of features for coreference resolution. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-27
October 2008, pp. 294–303.
Culotta, Aron, Michael Wick &amp; Andrew McCallum (2007).
First-order probabilistic models for coreference resolu-
tion. In Proceedings of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics, Rochester,
N.Y., 22–27 April 2007, pp. 81–88.
Denis, Pascal &amp; Jason Baldridge (2007). Joint determination
of anaphoricity and coreference resolution using integer
programming. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
Rochester, N.Y., 22–27 April 2007, pp. 236–243.
Haghighi, Aria &amp; Dan Klein (2007). Unsupervised coref-
erence resolution in a nonparametric Bayesian model. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Republic,
23–30 June 2007, pp. 848–855.
Luo, Xiaoqiang (2005). On coreference resolution perfor-
mance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Em-
pirical Methods in Natural Language Processing, Vancou-
ver, B.C., Canada, 6–8 October 2005, pp. 25–32.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla &amp; Salim Roukos (2004). A mention-
synchronous coreference resolution algorithm based on
the Bell Tree. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21–26 July 2004, pp. 136–143.
Mitchell, Alexis, Stephanie Strassel, Shudong Huang &amp;
Ramez Zakhary (2004). ACE 2004 Multilingual Training
Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic
Data Consortium.
Nicolae, Cristina &amp; Gabriel Nicolae (2006). BestCut: A
graph algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, Sydney, Australia, 22–23 July
2006, pp. 275–283.
Ponzetto, Simone Paolo &amp; Michael Strube (2006). Exploiting
semantic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proceedings of the Human Language
Technology Conference of the North American Chapter of
the Association for Computational Linguistics, New York,
N.Y., 4–9 June 2006, pp. 192–199.
Rahman, Altaf &amp; Vincent Ng (2009). Supervised models for
coreference resolution. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6-7 August 2009, pp. 968–977.
Rand, William R. (1971). Objective criteria for the evaluation
of clustering methods. Journal of the American Statistical
Association, 66(336):846–850.
Recasens, Marta &amp; Eduard Hovy (2010). BLANC: Imple-
menting the Rand index for coreference evaluation. Sub-
mitted.
Soon, Wee Meng, Hwee Tou Ng &amp; Daniel Chung Yong
Lim (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521–544.
Stoyanov, Veselin, Nathan Gilbert, Claire Cardie &amp; Ellen
Riloff (2009). Conundrums in noun phrase coreference
resolution: Making sense of the state-of-the-art. In Pro-
ceedings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics and
the 4th International Joint Conference on Natural Lan-
guage Processing, Singapore, 2–7 August 2009, pp. 656–
664.
Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang &amp; Alessandro Moschitti (2008). BART: A modular
toolkit for coreference resolution. In Companion Volume
to the Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, Columbus, Ohio,
15–20 June 2008, pp. 9–12.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly
&amp; Lynette Hirschman (1995). A model-theoretic corefer-
ence scoring scheme. In Proceedings of the 6th Message
Understanding Conference (MUC-6), pp. 45–52. San Ma-
teo, Cal.: Morgan Kaufmann.
Witten, Ian H. &amp; Eibe Frank (2005). Data Mining: Practical
Machine Learning Tools and Techniques (2nd ed.). San
Francisco, Cal.: Morgan Kaufmann.
Yang, Xiaofeng, Jian Su &amp; Chew Lim Tan (2008). A twin-
candidate model for learning-based anaphora resolution.
Computational Linguistics, 34(3):327–356.
</reference>
<page confidence="0.999449">
35
</page>
<bodyText confidence="0.95692765">
A B3sys Example Output
Here, we provide additional examples for analyzing the behavior of B3sys where we systematically vary
system outputs. Since we proposed B3sys for dealing with end-to-end systems, we consider only examples
also containing twinless mentions. The systems in Table 12 and 14 generate different twinless key
mentions while keeping the twinless system mentions untouched. In Table 13 and 15, the number of
twinless system mentions changes through different responses and the number of twinless key mentions
is fixed.
In Table 12, B3 sys recall goes up when more key mentions are resolved into the correct set. And the
precision stays the same, because there is no change in the number of the erroneous resolutoins (i.e. the
spurious cluster with mentions i and j). For the examples in Tables 13 and 15, B3 sys gives worse precision
to the outputs with more spurious resolutions, and the same recall if the systems resolve key mentions in
the same way. Since the set of key mentions intersects with the set of twinless system mentions in Table
14, we do not have an intuitive explanation for the decrease in precision from responses to response4.
However, both the F-score and the recall still show the right tendency.
Set 1 Set 2 B3sys
key {abcde} P R F
response, {a b} {i j} 0.857 0.280 0.422
response2 {a b c} {i j} 0.857 0.440 0.581
response3 {a b c d} {i j} 0.857 0.68 0.784
response4 {a b c d e} {i j} 0.857 1.0 0.923
</bodyText>
<tableCaption confidence="0.894843">
Table 12: Analysis of B3sys 1
</tableCaption>
<bodyText confidence="0.666772666666667">
Set 1 Set 2 B3sys
key {abcde} P R F
response, {a b c} {i j} 0.857 0.440 0.581
response2 {a b c} {i j k} 0.75 0.440 0.555
response3 {a b c} {i j k l} 0.667 0.440 0.530
response4 {a b c} {i j k l m} 0.6 0.440 0.508
</bodyText>
<tableCaption confidence="0.915968">
Table 13: Analysis of B3sys 2
</tableCaption>
<bodyText confidence="0.884699">
Set 1 B3sys
key {a b c d e} P R F
response, {a b i j} 0.643 0.280 0.390
response2 {a b c i j} 0.6 0.440 0.508
response3 {a b c d i j} 0.571 0.68 0.621
response4 {a b c d e i j} 0.551 1.0 0.711
</bodyText>
<tableCaption confidence="0.953816">
Table 14: Analysis of B3sys 3
</tableCaption>
<table confidence="0.469335166666667">
Set 1 B3sys
key {a b c d e} P R F
response, {a b c i j} 0.6 0.440 0.508
response2 {a b c i j k} 0.5 0.440 0.468
response3 {a b c i j k l} 0.429 0.440 0.434
response4 {a b c i j k l m} 0.375 0.440 0.405
</table>
<tableCaption confidence="0.979393">
Table 15: Analysis of B3sys 4
</tableCaption>
<page confidence="0.997831">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.542521">
<title confidence="0.999909">Evaluation Metrics For End-to-End Coreference Resolution Systems</title>
<author confidence="0.935488">Cai</author>
<affiliation confidence="0.778386">Heidelberg Institute for Theoretical Studies Schloß-Wolfsbrunnenweg</affiliation>
<address confidence="0.999791">69118 Heidelberg, Germany</address>
<email confidence="0.999049">(jie.cai|michael.strube)@h-its.org</email>
<abstract confidence="0.999273083333333">Commonly used coreference resolution evaluation metrics can only be applied to key mentions, i.e. already annotated mentions. We here propose two variants of the resolution evaluation algorithms which can be applied to coreference resolution systems dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation,</booktitle>
<pages>563--566</pages>
<location>Granada,</location>
<contexts>
<context position="2078" citStr="Bagga &amp; Baldwin, 1998" startWordPosition="308" endWordPosition="311">here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variants from the B3 algorithm (Bagga &amp; Baldwin, 1998). Rahman &amp; Ng (2009) propose their own variants of B3 and CEAF. Unfortunately, some of the metrics’ descriptions are so concise that they leave too much room for interpretation. Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution. Hence, though standard corpora are used, the results are not comparable. This paper attempts to fill that desideratum by analysing several variants of the B3 and CEAF algorithms. We propose two new variants, namely B3sys and CEAFsys, and provide algorithmic details in Section 2. We describe two </context>
<context position="4126" citStr="Bagga &amp; Baldwin, 1998" startWordPosition="625" endWordPosition="628">suitable for the ACE data (http://www.itl.nist. 1http://www.h-its.org/nlp/download Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28–36, The University of Tokyo, September 24-25, 2010. p@c 2010 Association for Computational Linguistics 28 gov/iad/mig/tests/ace/), which includes singleton entities in the keys. Moreover, the MUC score does not give credit for separating singleton entities from other chains. This becomes problematic in a realistic system setup, when mentions are extracted automatically. 2.2 B3 The B3 algorithm (Bagga &amp; Baldwin, 1998) overcomes the shortcomings of the MUC score. Instead of looking at the links, B3 computes precision and recall for all mentions in the document, which are then combined to produce the final precision and recall numbers for the entire output. For each mention, the B3 algorithm computes a precision and recall score using equations 1 and 2: Precision(mi) = |Rmi ∩ Kmi |(1) |Rmi| Recall(mi) = |Rmi ∩ Kmi |(2) |Kmi| where Rmi is the response chain (i.e. the system output) which includes the mention mi, and Kmi is the key chain (manually annotated gold standard) with mi. The overall precision and rec</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit &amp; Breck Baldwin (1998). Algorithms for scoring coreference chains. In Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28–30 May 1998, pp. 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>294--303</pages>
<location>Waikiki, Honolulu, Hawaii,</location>
<contexts>
<context position="1431" citStr="Bengtson &amp; Roth, 2008" startWordPosition="205" endWordPosition="208">l and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis &amp; Baldridge (2007), Culotta et al. (2007), Haghighi &amp; Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variant</context>
<context position="21005" citStr="Bengtson &amp; Roth (2008)" startWordPosition="3693" endWordPosition="3696">er, a basic assumption behind BLANC is, that the sum of all coreferential and non-coreferential links is constant for a given set of mentions. This implies that BLANC assumes identical mentions in key and response. It is not clear how to adapt BLANC to system mentions. We do not address this issue here. 3 Experiments While Section 2 used toy examples to motivate our metrics B3sys and CEAFsys, we here report results on two larger experiments using ACE2004 data. 3.1 Data and Mention Taggers We use the ACE2004 (Mitchell et al., 2004) English training data which we split into three sets following Bengtson &amp; Roth (2008): Train (268 docs), Dev (76), and Test (107). We use two inhouse mention taggers. The first (SM1) implements a heuristic aiming at high recall. The second (SM2) uses the J48 decision tree classifier (Witten &amp; Frank, 2005). The number of detected mentions, head coverage, and accuracy on testing data SM1 SM2 31,370 16,081 13,072 14,179 development mentions twin mentions 8,387 4,956 4,242 4,212 79.3% 73.3% 57.3% 81.2% Table 7: Mention Taggers on ACE2004 Data are shown in Table 7. 3.2 Artificial Setting For the artificial setting we report results on the development data using the SM1 tagger. To i</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Bengtson, Eric &amp; Dan Roth (2008). Understanding the value of features for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25-27 October 2008, pp. 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>81--88</pages>
<location>Rochester, N.Y.,</location>
<contexts>
<context position="1140" citStr="Culotta et al. (2007)" startWordPosition="157" endWordPosition="160">g with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis &amp; Baldridge (2007), Culotta et al. (2007), Haghighi &amp; Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsis</context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>Culotta, Aron, Michael Wick &amp; Andrew McCallum (2007). First-order probabilistic models for coreference resolution. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, N.Y., 22–27 April 2007, pp. 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>236--243</pages>
<location>Rochester, N.Y.,</location>
<contexts>
<context position="1117" citStr="Denis &amp; Baldridge (2007)" startWordPosition="153" endWordPosition="156"> resolution systems dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis &amp; Baldridge (2007), Culotta et al. (2007), Haghighi &amp; Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution sy</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Denis, Pascal &amp; Jason Baldridge (2007). Joint determination of anaphoricity and coreference resolution using integer programming. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, N.Y., 22–27 April 2007, pp. 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>848--855</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1165" citStr="Haghighi &amp; Klein (2007)" startWordPosition="161" endWordPosition="164"> i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis &amp; Baldridge (2007), Culotta et al. (2007), Haghighi &amp; Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Haghighi, Aria &amp; Dan Klein (2007). Unsupervised coreference resolution in a nonparametric Bayesian model. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic, 23–30 June 2007, pp. 848–855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="1900" citStr="Luo, 2005" startWordPosition="280" endWordPosition="281">al2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variants from the B3 algorithm (Bagga &amp; Baldwin, 1998). Rahman &amp; Ng (2009) propose their own variants of B3 and CEAF. Unfortunately, some of the metrics’ descriptions are so concise that they leave too much room for interpretation. Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution. Hence, though standard corpora are used, the results are not comparable. This paper attempts to fill that desideratum </context>
<context position="12943" citStr="Luo (2005)" startWordPosition="2237" endWordPosition="2238">11: Use key to form keyr 12: Through key3r and responser; 13: Calculate B recall R 14: end if 15: Calculate F-score F So, the precision of B3sys is given by: PrB3sys = 16(23+23+31+21+21+1)=.0.611 The modified key and response for recall are: Keyr : {a b c} Responser: {a b} {c} The resulting recall of B3sys is: RecB3 sys = 13(23 + 23 + 13) .= 0.556 Thus the F-score number is calculated as: F 3 = 2 x 0.611 × 0.556 . 0.582 Bsys 0.611+0.556 B3sys indicates more adequately the performance of end-to-end coreference resolution systems. It is not easily tricked by different mention taggers3. 2.3 CEAF Luo (2005) criticizes the B3 algorithm for using entities more than one time, because B3 computes precision and recall of mentions by comparing entities containing that mention. Hence Luo proposes the CEAF algorithm which aligns entities in key and response. CEAF applies a similarity metric (which could be either mention based or entity based) for each pair of entities (i.e. a set of mentions) to measure the goodness of each possible alignment. The best mapping is used for calculating CEAF precision, recall and F-measure. Luo proposes two entity based similarity metrics (Equation 3 and 4) for an entity </context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Luo, Xiaoqiang (2005). On coreference resolution performance metrics. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8 October 2005, pp. 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla &amp; Salim Roukos</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>136--143</pages>
<location>Barcelona,</location>
<marker>Luo, 2004</marker>
<rawString>Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla &amp; Salim Roukos (2004). A mentionsynchronous coreference resolution algorithm based on the Bell Tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, 21–26 July 2004, pp. 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Mitchell</author>
<author>Stephanie Strassel</author>
</authors>
<title>Shudong Huang &amp; Ramez Zakhary</title>
<date>2004</date>
<booktitle>ACE 2004 Multilingual Training Corpus. LDC2005T09,</booktitle>
<institution>Linguistic Data Consortium.</institution>
<location>Philadelphia, Penn.:</location>
<marker>Mitchell, Strassel, 2004</marker>
<rawString>Mitchell, Alexis, Stephanie Strassel, Shudong Huang &amp; Ramez Zakhary (2004). ACE 2004 Multilingual Training Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Nicolae</author>
<author>Gabriel Nicolae</author>
</authors>
<title>BestCut: A graph algorithm for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>275--283</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1814" citStr="Nicolae &amp; Nicolae (2006)" startWordPosition="262" endWordPosition="266">e task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variants from the B3 algorithm (Bagga &amp; Baldwin, 1998). Rahman &amp; Ng (2009) propose their own variants of B3 and CEAF. Unfortunately, some of the metrics’ descriptions are so concise that they leave too much room for interpretation. Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution. Hence, though standard corpora a</context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>Nicolae, Cristina &amp; Gabriel Nicolae (2006). BestCut: A graph algorithm for coreference resolution. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia, 22–23 July 2006, pp. 275–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<location>New York, N.Y., 4–9</location>
<contexts>
<context position="1408" citStr="Ponzetto &amp; Strube, 2006" startWordPosition="201" endWordPosition="204"> expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis &amp; Baldridge (2007), Culotta et al. (2007), Haghighi &amp; Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Ponzetto, Simone Paolo &amp; Michael Strube (2006). Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, New York, N.Y., 4–9 June 2006, pp. 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="2098" citStr="Rahman &amp; Ng (2009)" startWordPosition="312" endWordPosition="315">09, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variants from the B3 algorithm (Bagga &amp; Baldwin, 1998). Rahman &amp; Ng (2009) propose their own variants of B3 and CEAF. Unfortunately, some of the metrics’ descriptions are so concise that they leave too much room for interpretation. Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution. Hence, though standard corpora are used, the results are not comparable. This paper attempts to fill that desideratum by analysing several variants of the B3 and CEAF algorithms. We propose two new variants, namely B3sys and CEAFsys, and provide algorithmic details in Section 2. We describe two experiments in Secti</context>
<context position="9816" citStr="Rahman &amp; Ng (2009)" startWordPosition="1696" endWordPosition="1699">er tags more mentions (i.e. the mentions i, j and k), while both System 1 and System 2 have identical coreference resolution performance. Still, B3all outputs quite different results for precision and thus for Fscore. This is due to the credit B3all takes from unresolved singleton twinless system mentions (i.e. mention i, j, k in System 2). Since the metric is expected to evaluate the end-to-end coreference system performance rather than the mention tagging quality, it is not satisfying to observe that B3all’s numbers actually fluctuate when the system is exposed to different mention taggers. Rahman &amp; Ng (2009) apply another variant, denoted here as B3r&amp;n. They remove only those twinless system mentions that are singletons before applying the B3 algorithm. So, a system would not be rewarded by the the spurious mentions which are correctly identified as singletons during resolution (as has been the case with B3all’s higher precision for System 2 as can be seen in Table 3). We assume that Rahman &amp; Ng apply a strategy similar to B3all after the removing step (this is not clear in Rahman &amp; Ng (2009)). While it avoids the problem with singleton twinless system mentions, B3r&amp;n still suffers from the probl</context>
<context position="15375" citStr="Rahman &amp; Ng (2009)" startWordPosition="2658" endWordPosition="2661">an be seen from Table 4, CEAForig fails to produce intuitive results for system mentions. System 2 outputs one more spurious entity (containing mention i and j) than System 1 does, however, achieves a same CEAForig precision. Since twinless system mentions do not have mappings in key, they contribute nothing to the mapping similarity. So, resolution mistakes for system mentions are not calculated, and moreover, the precision is easily skewed by the number of output entities. CEAForig reports very low precision for system mentions (see also Stoyanov et al. (2009)). 2.3.2 Existing CEAF variants Rahman &amp; Ng (2009) briefly introduce their CEAF variant, which is denoted as CEAFr&amp;n here. They use φ3(⋆, ⋆), which results in equal CEAFr&amp;n precision and recall figures when using true mentions. Since Rahman &amp; Ng’s experiments using system mentions produce unequal precision and recall figures, we assume that, after removing 31 Set 1 Set 2 Singletons System 1 key {a b c} {c} {i} {j} response {a b} P R F CEAForig 0.4 0.667 0.500 B3 1.0 0.556 0.715 sys CEAFsys 0.667 0.667 0.667 System 2 key {a b c} {i j} {c} response {a b} P R F CEAForig 0.4 0.667 0.500 B3 0.8 0.556 0.656 sys CEAFsys 0.6 0.667 0.632 Table 4: Prob</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Rahman, Altaf &amp; Vincent Ng (2009). Supervised models for coreference resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore, 6-7 August 2009, pp. 968–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William R Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="20183" citStr="Rand, 1971" startWordPosition="3554" endWordPosition="3555">.8 0.607 φ3(⋆, ⋆) 0.6 0.667 0.632 Table 6: Problems of φ4(⋆, ⋆) When normalizing the similarities by the number of entities or mentions in the key (for recall) and the response (for precision), the CEAF algorithm considers all entities or mentions to be equally important. Hence CEAF tends to compute quite low precision for system mentions which does not represent the system performance adequately. Here, we do not address this issue. 2.4 BLANC Recently, a new coreference resolution evaluation algorithm, BLANC, has been introduced (Recasens &amp; Hovy, 2010). This measure implements the Rand index (Rand, 1971) which has been originally developed to evaluate clustering methods. The BLANC algorithm deals correctly with singleton entities and rewards correct entities according to the number of mentions. However, a basic assumption behind BLANC is, that the sum of all coreferential and non-coreferential links is constant for a given set of mentions. This implies that BLANC assumes identical mentions in key and response. It is not clear how to adapt BLANC to system mentions. We do not address this issue here. 3 Experiments While Section 2 used toy examples to motivate our metrics B3sys and CEAFsys, we h</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>Rand, William R. (1971). Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2010</date>
<note>Submitted.</note>
<contexts>
<context position="20130" citStr="Recasens &amp; Hovy, 2010" startWordPosition="3543" endWordPosition="3547">em 2 key {a b c} {c} response {a b} {i j} P R F φ�(⋆, ⋆) 0.489 0.8 0.607 φ3(⋆, ⋆) 0.6 0.667 0.632 Table 6: Problems of φ4(⋆, ⋆) When normalizing the similarities by the number of entities or mentions in the key (for recall) and the response (for precision), the CEAF algorithm considers all entities or mentions to be equally important. Hence CEAF tends to compute quite low precision for system mentions which does not represent the system performance adequately. Here, we do not address this issue. 2.4 BLANC Recently, a new coreference resolution evaluation algorithm, BLANC, has been introduced (Recasens &amp; Hovy, 2010). This measure implements the Rand index (Rand, 1971) which has been originally developed to evaluate clustering methods. The BLANC algorithm deals correctly with singleton entities and rewards correct entities according to the number of mentions. However, a basic assumption behind BLANC is, that the sum of all coreferential and non-coreferential links is constant for a given set of mentions. This implies that BLANC assumes identical mentions in key and response. It is not clear how to adapt BLANC to system mentions. We do not address this issue here. 3 Experiments While Section 2 used toy exa</context>
</contexts>
<marker>Recasens, Hovy, 2010</marker>
<rawString>Recasens, Marta &amp; Eduard Hovy (2010). BLANC: Implementing the Rand index for coreference evaluation. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
</authors>
<title>Hwee Tou Ng &amp; Daniel Chung Yong Lim</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Soon, 2001</marker>
<rawString>Soon, Wee Meng, Hwee Tou Ng &amp; Daniel Chung Yong Lim (2001). A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
</authors>
<title>Claire Cardie &amp; Ellen Riloff (2009). Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing,</booktitle>
<pages>656--664</pages>
<marker>Stoyanov, Gilbert, 2009</marker>
<rawString>Stoyanov, Veselin, Nathan Gilbert, Claire Cardie &amp; Ellen Riloff (2009). Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing, Singapore, 2–7 August 2009, pp. 656– 664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
</authors>
<title>Xiaofeng Yang &amp; Alessandro Moschitti</title>
<date>2008</date>
<booktitle>In Companion Volume to the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--12</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="21895" citStr="Versley et al., 2008" startWordPosition="3842" endWordPosition="3845">, and accuracy on testing data SM1 SM2 31,370 16,081 13,072 14,179 development mentions twin mentions 8,387 4,956 4,242 4,212 79.3% 73.3% 57.3% 81.2% Table 7: Mention Taggers on ACE2004 Data are shown in Table 7. 3.2 Artificial Setting For the artificial setting we report results on the development data using the SM1 tagger. To illustrate the stability of the evaluation metrics with respect to different mention taggers, we reduce the number of twinless system mentions in intervals of 10%, while correct (non-twinless) ones are kept untouched. The coreference resolution system used is the BART (Versley et al., 2008) reimplementation of Soon et al. (2001). The results are plotted in Figures 1 and 2. Proportion of twinless system mentions used in the experiment Figure 1: Artificial Setting B3 Variants Figure 2: Artificial Setting CEAF Variants F-score for ACE04 Development Data 0.75 0.7 0.65 0.6 0.55 0.85 0.8 MUC BCubedsys BCubed0 BCubedall BCubedng 1 0.8 0.6 0.4 0.2 0 training mentions twin mentions test mentions twin mentions head coverage accuracy 8,045 3,371 1 0.8 0.6 0.4 0.2 0 F-score for ACE04 Development Data 0.75 0.65 0.55 0.45 0.8 0.7 0.6 0.5 0.4 MUC CEAFsys CEAForig CEAFng Proportion of twinless </context>
<context position="23848" citStr="Versley et al., 2008" startWordPosition="4178" endWordPosition="4182">d MUC (i.e. B3all computes similar numbers for worse models). This shows that the B3all score can be tricked by using a high recall mention tagger, e.g. in cases with the worse models (i.e. ones on the left side of the figures) which have much more twinless system mentions. The original CEAF algorithm, CEAForig, is too sensitive to the input system mentions making it less reliable. CEAFsys is parallel to B3sys. Thus both of our metrics exhibit the same intuition. 3.3 Realistic Setting 3.3.1 Experiment 1 For the realistic setting we compare SM1 and SM2 as preprocessing components for the BART (Versley et al., 2008) reimplementation of Soon et al. (2001). The coreference resolution system with the SM2 tagger performs better, because a better coreference model is achieved from system mentions with higher accuracy. The MUC, B3sys and CEAFsys metrics have the same tendency when applied to systems with different mention taggers (Table 8, 9 and 10 and the bold numbers are higher with a p-value of 0.05, by a paired-t test). Since the MUC scorer does not evaluate singleton entities, it produces too low numbers which are not informative any more. As shown in Table 9, B3all reports counterintuitive results when a</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, 2008</marker>
<rawString>Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang &amp; Alessandro Moschitti (2008). BART: A modular toolkit for coreference resolution. In Companion Volume to the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, 15–20 June 2008, pp. 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, Cal.:</location>
<contexts>
<context position="1865" citStr="Vilain et al., 1995" startWordPosition="272" endWordPosition="275">rence resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variants from the B3 algorithm (Bagga &amp; Baldwin, 1998). Rahman &amp; Ng (2009) propose their own variants of B3 and CEAF. Unfortunately, some of the metrics’ descriptions are so concise that they leave too much room for interpretation. Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution. Hence, though standard corpora are used, the results are not comparable. This paper</context>
<context position="3188" citStr="Vilain et al., 1995" startWordPosition="485" endWordPosition="488">ose two new variants, namely B3sys and CEAFsys, and provide algorithmic details in Section 2. We describe two experiments in Section 3 showing that B3sys and CEAFsys lead to intuitive and reliable results. Implementations of B3sys and CEAFsys are available open source along with extended examples1. 2 Coreference Evaluation Metrics We discuss the problems which arise when applying the most prevalent coreference resolution evaluation metrics to end-to-end systems and propose our variants which overcome those problems. We provide detailed analyses of illustrative examples. 2.1 MUC The MUC score (Vilain et al., 1995) counts the minimum number of links between mentions to be inserted or deleted when mapping a system response to a gold standard key set. Although pairwise links capture the information in a set, they cannot represent singleton entities, i.e. entities, which are mentioned only once. Therefore, the MUC score is not suitable for the ACE data (http://www.itl.nist. 1http://www.h-its.org/nlp/download Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28–36, The University of Tokyo, September 24-25, 2010. p@c 2010 Association for Compu</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly &amp; Lynette Hirschman (1995). A model-theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pp. 45–52. San Mateo, Cal.: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
</authors>
<title>Eibe Frank</title>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques (2nd ed.).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, Cal.:</location>
<marker>Witten, 2005</marker>
<rawString>Witten, Ian H. &amp; Eibe Frank (2005). Data Mining: Practical Machine Learning Tools and Techniques (2nd ed.). San Francisco, Cal.: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>A twincandidate model for learning-based anaphora resolution.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="1942" citStr="Yang et al. (2008)" startWordPosition="284" endWordPosition="287">tant part of the problem by evaluating on key mentions only (Ponzetto &amp; Strube, 2006; Bengtson &amp; Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae &amp; Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm (Luo, 2005) without modifications. Yang et al. (2008) use only the MUC score. Bengtson &amp; Roth (2008) and Stoyanov et al. (2009) derive variants from the B3 algorithm (Bagga &amp; Baldwin, 1998). Rahman &amp; Ng (2009) propose their own variants of B3 and CEAF. Unfortunately, some of the metrics’ descriptions are so concise that they leave too much room for interpretation. Also, some of the metrics proposed are too lenient or are more sensitive to mention detection than to coreference resolution. Hence, though standard corpora are used, the results are not comparable. This paper attempts to fill that desideratum by analysing several variants of the B3 an</context>
</contexts>
<marker>Yang, Su, Tan, 2008</marker>
<rawString>Yang, Xiaofeng, Jian Su &amp; Chew Lim Tan (2008). A twincandidate model for learning-based anaphora resolution. Computational Linguistics, 34(3):327–356.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>