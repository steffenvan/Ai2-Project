<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007681">
<title confidence="0.9992745">
Mixture Model POMDPs for Efficient Handling of Uncertainty
in Dialogue Management
</title>
<author confidence="0.998409">
James Henderson Oliver Lemon
</author>
<affiliation confidence="0.9998205">
University of Geneva University of Edinburgh
Department of Computer Science School of Informatics
</affiliation>
<email confidence="0.987307">
James.Henderson@cui.unige.ch olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997126" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814625">
In spoken dialogue systems, Partially Observ-
able Markov Decision Processes (POMDPs)
provide a formal framework for making di-
alogue management decisions under uncer-
tainty, but efficiency and interpretability con-
siderations mean that most current statistical
dialogue managers are only MDPs. These
MDP systems encode uncertainty explicitly in
a single state representation. We formalise
such MDP states in terms of distributions
over POMDP states, and propose a new di-
alogue system architecture (Mixture Model
POMDPs) which uses mixtures of these dis-
tributions to efficiently represent uncertainty.
We also provide initial evaluation results (with
real users) for this architecture.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995358117647059">
Partially Observable Markov Decision Processes
(POMDPs) provide a formal framework for mak-
ing decisions under uncertainty. Recent research
in spoken dialogue systems has used POMDPs for
dialogue management (Williams and Young, 2007;
Young et al., 2007). These systems represent the
uncertainty about the dialogue history using a prob-
ability distribution over dialogue states, known as
the POMDP’s belief state, and they use approxi-
mate POMDP inference procedures to make dia-
logue management decisions. However, these infer-
ence procedures are too computationally intensive
for most domains, and the system’s behaviour can be
difficult to predict. Instead, most current statistical
dialogue managers use a single state to represent the
dialogue history, thereby making them only Markov
Decision Process models (MDPs). These state rep-
</bodyText>
<page confidence="0.982839">
73
</page>
<bodyText confidence="0.9999159375">
resentations have been fine-tuned over many devel-
opment cycles so that common types of uncertainty
can be encoded in a single state. Examples of such
representations include unspecified values, confi-
dence scores, and confirmed/unconfirmed features.
We formalise such MDP systems as compact encod-
ings of POMDPs, where each MDP state represents
a probability distribution over POMDP states. We
call these distributions “MDP belief states”.
Given this understanding of MDP dialogue man-
agers, we propose a new POMDP spoken dialogue
system architecture which uses mixtures of MDP be-
lief states to encode uncertainty. A Mixture Model
POMDP represents its belief state as a probability
distribution over a finite set of MDP states. This
extends the compact representations of uncertainty
in MDP states to include arbitrary disjunction be-
tween MDP states. Efficiency is maintained because
such arbitrary disjunction is not needed to encode
the most common forms of uncertainty, and thus the
number of MDP states in the set can be kept small
without losing accuracy. On the other hand, allow-
ing multiple MDP states provides the representa-
tional mechanism necessary to incorporate multiple
speech recognition hypotheses into the belief state
representation. In spoken dialogue systems, speech
recognition is by far the most important source of
uncertainty. By providing a mechanism to incorpo-
rate multiple arbitrary speech recognition hypothe-
ses, the proposed architecture leverages the main ad-
vantage of POMDP systems while still maintaining
the efficiency of MDP-based dialogue managers.
</bodyText>
<sectionHeader confidence="0.915156" genericHeader="method">
2 Mixture Model POMDPs
</sectionHeader>
<footnote confidence="0.569934">
A POMDP belief state bt is a probability distribution
P(st|V_1i ut) over POMDP states st given the dia-
</footnote>
<note confidence="0.391144">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 73–76,
</note>
<page confidence="0.459361">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.997307142857143">
logue history Vt−1 and the most recent observation
(i.e. user utterance) ut. We formalise the meaning
of an MDP state representation rt as a distribution
b(rt) = P(st|rt) over POMDP states. We represent
the belief state bt as a list of pairs hrit, piti such that
Pi pit = 1. This list is interpreted as a mixture of
the b(rit).
</bodyText>
<equation confidence="0.9971835">
Xbt = pitb(rit) (1)
i
</equation>
<bodyText confidence="0.9998521875">
State transitions in MDPs are specified with an
update function, rt = f(rt−1, at−1, ht), which maps
the preceding state rt−1, system action at−1, and
user input ht to a new state rt. This function is in-
tended to encode in rt all the new information pro-
vided by at−1 and ht. The user input ht is the result
of automatic speech recognition (ASR) plus spoken
language understanding (SLU) applied to ut. Be-
cause there is no method for handling ambiguity in
ht, ht is computed from the single best ASR-SLU
hypothesis, plus some measure of ASR confidence.
In POMDPs, belief state transitions are done by
changing the distribution over states to take into ac-
count the new information from the system action
at−1 and an n-best list of ASR-SLU hypotheses hjt.
This new belief state can be estimated as
</bodyText>
<equation confidence="0.999340125">
bt = P(st|Vt−1,ut)
P(st−1|Vt−1)P(hjt|Vt−1, st−1)
P(ut|Vt−1, st−1, hjt)
P(st|Vt−1, st−1, hjt, ut)
P(ut|Vt−1)
P(st−1|Vt−2, ut−1)P(hjt|at−1, st−1)
P(hjt|ut)P(st|at−1, st−1, hjt)
P(hjt)Z(Vt)
</equation>
<bodyText confidence="0.997839272727273">
where Z(Vt) is a normalising constant.
P(st−1|Vt−2, ut−1) is the previous belief state.
P(hjt|ut) reflects the confidence of ASR-SLU in
hypothesis hjt. P(st|at−1, st−1, hjt) is normally 1
for st = st−1, but can be used to allow users to
change their mind mid-dialogue. P(hjt|at−1, st−1)
is a user model. P(hjt) is a prior over ASR-SLU
outputs.
Putting these two approaches together, we get the
following update equation for our mixture of MDP
belief states:
</bodyText>
<equation confidence="0.95085825">
bt = P(st|Vt−1, ut)
pit−1P (hjt|at−1, rit−1)
P(hjt|ut)b(f(rit−1, at−1, hj t)) (2)
P(hj t)Z(Vt)
i0 i
rt = f (rt−1,at−1, hjt)
pi
0 t−1P(hjt at−1,rit−1)P (hj t ut)
.
P(hjt)Z(Vt)
For equation (2) to be true, we require that
b(f(rit−1,at−1,hjt)) ≈ P(st|at−1,rit−1,hjt) (4)
</equation>
<bodyText confidence="0.999793">
which simply ensures that the meaning assigned to
MDP state representations and the MDP state tran-
sition function are compatible.
From equation (3), we see that the number
of MDP states will grow exponentially with the
length of the dialogue, proportionately to the num-
ber of ASR-SLU hypotheses. Some of the state-
hypothesis pairs rit−1, hjt may lead to equivalent
states f(rit−1, at−1, hjt), but in general pruning is
necessary. Pruning should be done so as to min-
imise the change to the belief state distribution, for
example by minimising the KL divergence between
the pre- and post- pruning belief states. We use two
heuristic approximations to this optimisation prob-
lem. First, if two states share the same core features
(e.g. filled slots, but not the history of user inputs),
then the state with the lower probability is pruned,
and its probability is added to the other state. Sec-
ond, a fixed beam of the k most probable states is
kept, and the other states are pruned. The probabil-
ity pit from a pruned state rit is redistributed to un-
pruned states which are less informative than ri t in
their core features.1
The interface between the ASR-SLU module and
the dialogue manager is a set of hypotheses hjt paired
with their confidence scores P(hjt|ut). These pairs
are analogous to the state-probability pairs rit, pit
within the dialogue manager, and we can extend our
mixture model architecture to cover these pairs as
well. Interpreting the set of hjt, P(hjt|ut) pairs as a
</bodyText>
<footnote confidence="0.722772666666667">
1In the current implementation, these pruned state probabil-
ities are simply added to an uninformative “null” state, but in
general we could check for logical subsumption between states.
</footnote>
<equation confidence="0.927241">
X=
hj
t
X≈
hj
t
X
st−1
X
st−1
X≈
hj
t
X
ri
t−1
X= pi0
i0 t b(ri0
t )
</equation>
<bodyText confidence="0.557777">
where, for each i&apos; there is one pair i, j such that
</bodyText>
<equation confidence="0.7078855">
(3)
pit =
</equation>
<page confidence="0.991012">
74
</page>
<bodyText confidence="0.999990388888889">
mixture of distributions over more specific hypothe-
ses becomes important when we consider pruning
this set before passing it to the dialogue manager. As
with the pruning of states, pruning should not sim-
ply remove a hypothesis and renormalise, it should
redistribute the probability of a pruned hypothesis to
similar hypotheses. This is not always computation-
ally feasible, but all interfaces within the Mixture
Model POMDP architecture are sets of hypothesis-
probability pairs which can be interpreted as finite
mixtures in some underlying hypothesis space.
Given an MDP state representation, this formali-
sation allows us to convert it into a Mixture Model
POMDP. The only additional components of the
model are the user model P(hjt|at−1, rit−1), the
ASR-SLU prior P(hjt), and the ASR-SLU confi-
dence score P(hjt|ut). Note that there is no need
to actually define b(rit), provided equation (4) holds.
</bodyText>
<sectionHeader confidence="0.995394" genericHeader="method">
3 Decision Making with MM POMDPs
</sectionHeader>
<bodyText confidence="0.999987142857143">
Given this representation of the uncertainty in the
current dialogue state, the spoken dialogue system
needs to decide what system action to perform.
There are several approaches to POMDP decision
making which could be adapted to this representa-
tion, but to date we have only considered a method
which allows us to directly derive a POMDP policy
from the policy of the original MDP.
Here again we exploit the fact that the most fre-
quent forms of uncertainty are already effectively
handled in the MDP system (e.g. by filled vs. con-
firmed slot values). We propose that an effective di-
alogue management policy can be created by sim-
ply computing a mixture of the MDP policy applied
to the MDP states in the belief state list. More
precisely, we assume that the original MDP system
specifies a Q function QMDP(at, rt) which estimates
the expected future reward of performing action at
in state rt. We then estimate the expected future re-
ward of performing action at in belief state bt as the
mixture of these MDP estimates.
</bodyText>
<equation confidence="0.97686">
Q(at, bt) � � ptQMDP(at, rit) (5)
i
</equation>
<bodyText confidence="0.99981775">
The dialogue management policy is to choose the
action at with the largest value for Q(at, bt). This is
known as a Q-MDP model (Littman et al., 1995), so
we call this proposal a Mixture Model Q-MDP.
</bodyText>
<sectionHeader confidence="0.999871" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9999296875">
Our representation of POMDP belief states using a
set of distributions over POMDP states is similar to
the approach in (Young et al., 2007), where POMDP
belief states are represented using a set of partitions
of POMDP states. For any set of partitions, the mix-
ture model approach could express the same model
by defining one MDP state per partition and giving
it a uniform distribution inside its partition and zero
probability outside. However, the mixture model ap-
proach is more flexible, because the distributions in
the mixture do not have to be uniform within their
non-zero region, and these regions do not have to
be disjoint. A list of states was also used in (Hi-
gashinaka et al., 2003) to represent uncertainty, but
no formal semantics was provided for this list, and
therefore only heuristic uses were suggested for it.
</bodyText>
<sectionHeader confidence="0.998213" genericHeader="method">
5 Initial Experiments
</sectionHeader>
<bodyText confidence="0.999937142857143">
We have implemented a Mixture Model POMDP ar-
chitecture as a multi-state version of the DIPPER
“Information State Update” dialogue manager (Bos
et al., 2003). It uses equation (3) to compute belief
state updates, given separate models for MDP state
updates (for f(rit−1, at−1, hjt)), statistical ASR-SLU
(for P(hjt|ut)/P(hjt)), and a statistical user model
(for P(hjt|at−1, rit−1)). The state list is pruned as
described in section 2, where the “core features”
are the filled information slot values and whether
they have been confirmed. For example, the sys-
tem will merge two states which agree that the user
only wants a cheap hotel, even if they disagree on
the sequence of dialogue acts which lead to this in-
formation. It also never prunes the “null” state, so
that there is always some probability that the system
knows nothing.
The system used in the experiments described
below uses the MDP state representation and up-
date function from (Lemon and Liu, 2007), which
is designed for standard slot-filling dialogues. For
the ASR model, it uses the HTK speech recogniser
(Young et al., 2002) and an n-best list of three ASR
hypotheses on each user turn. The prior over user in-
puts is assumed to be uniform. The ASR hypotheses
are passed to the SLU model from (Meza-Ruiz et al.,
2008), which produces a single user input for each
ASR hypothesis. This SLU model was trained on
</bodyText>
<page confidence="0.995786">
75
</page>
<table confidence="0.99834975">
TC % Av. length (std. deviation)
Handcoded 56.0 7.2 (4.6)
MDP 66.6 7.2 (4.0)
MM Q-MDP 73.3 7.3 (3.7)
</table>
<tableCaption confidence="0.9697295">
Table 1: Initial test results for human-machine dialogues,
showing task completion and average length.
</tableCaption>
<bodyText confidence="0.999924736842105">
the TownInfo corpus of dialogues, which was col-
lected using the TownInfo human-machine dialogue
systems of (Lemon et al., 2006), transcribed, and
hand annotated. ASR hypotheses which result in the
same user input are merged (summing their proba-
bilities), and the resulting list of at most three ASR-
SLU hypotheses are passed to the dialogue manager.
Thus the number of MDP states in the dialogue man-
ager grows by up to three times at each step, before
pruning. For the user model, the system uses an n-
gram user model, as described in (Georgila et al.,
2005), trained on the annotated TownInfo corpus.2
The system’s dialogue management policy is a
Mixture Model Q-MDP (MM Q-MDP) policy. As
with the MDP states, the MDP Q function is from
(Lemon and Liu, 2007). It was trained in an MDP
system using reinforcement learning with simulated
users (Lemon and Liu, 2007), and was not modified
for use in our MM Q-MDP policy.
We tested this system with 10 different users, each
attempting 9 tasks in the TownInfo domain (search-
ing for hotels and restaurants in a fictitious town),
resulting in 90 test dialogues. The users each at-
tempted 3 tasks with the MDP system of (Lemon
and Liu, 2007), 3 tasks with a state-of-the-art hand-
coded system (see (Lemon et al., 2006)), and 3 tasks
with the MM Q-MDP system. Ordering of sys-
tems and tasks was controlled, and 3 of the users
were not native speakers of English. We collected
the Task Completion (TC), and dialogue length for
each system, as reported in table 1. Task Comple-
tion is counted from the system logs when the user
replies that they are happy with their chosen option.
Such a small sample size means that these results are
not statistically significant, but there is a clear trend
showing the superiority of the the MM Q-MDP sys-
tem, both in terms of more tasks being completed
and less variability in overall dialogue length.
</bodyText>
<note confidence="0.343846">
2Thanks to K. Georgilla for training this model.
</note>
<sectionHeader confidence="0.999153" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999917888888889">
Mixture Model POMDPs combine the efficiency of
MDP spoken dialogue systems with the ability of
POMDP models to make use of multiple ASR hy-
potheses. They can also be constructed from MDP
models without additional training, using the Q-
MDP approximation for the dialogue management
policy. Initial results suggest that, despite its sim-
plicity, this approach does lead to better spoken dia-
logue systems than MDP and hand-coded models.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99938275">
This research received funding from UK EPSRC
grant EP/E019501/1 and the European Community’s
FP7 under grant no 216594 (CLASSIC project:
www.classic-project.org).
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994453125">
J Bos, E Klein, O Lemon, and T Oka. 2003. DIPPER:
Description and Formalisation of an Information-State
Update Dialogue System Architecture. In Proc. SIG-
dial Workshop on Discourse and Dialogue, Sapporo.
K Georgila, J Henderson, and O Lemon. 2005. Learning
User Simulations for Information State Update Dia-
logue Systems. In Proc. Eurospeech.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken dia-
logue systems. In Proc. ACL, Sapporo.
O Lemon and X Liu. 2007. Dialogue policy learning
for combinations of noise and user simulation: transfer
results. In Proc. SIGdial.
O Lemon, K Georgila, and J Henderson. 2006. Evalu-
ating Effectiveness and Portability of Reinforcement
Learned Dialogue Strategies with real users: the
TALK TownInfo Evaluation. In Proc. ACL/IEEE SLT.
ML Littman, AR Cassandra, and LP Kaelbling. 1995.
Learning policies for partially observable environ-
ments: Scaling up. In Proc. ICML, pages 362–370.
I Meza-Ruiz, S Riedel, and O Lemon. 2008. Accurate
statistical spoken language understanding from limited
development resources. In Proc. ICASSP. (to appear).
JD Williams and SJ Young. 2007. Partially Observ-
able Markov Decision Processes for Spoken Dialog Systems.
Computer Speech and Language, 21(2):231–422.
S Young, G Evermann, D Kershaw, G Moore, J Odell,
D Ollason, D Povey, V Valtchev, and P Woodland.
2002. The HTK Book. Cambridge Univ. Eng. Dept.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Di-
alog Management. In Proc. ICASSP, Honolulu.
</reference>
<page confidence="0.991827">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.385940">
<title confidence="0.9960795">Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management</title>
<author confidence="0.999861">James Henderson Oliver Lemon</author>
<affiliation confidence="0.9976895">University of Geneva University of Edinburgh Department of Computer Science School of Informatics</affiliation>
<author confidence="0.396508">James Hendersoncui unige ch olemoninf ed ac uk</author>
<abstract confidence="0.998863411764706">In spoken dialogue systems, Partially Observable Markov Decision Processes (POMDPs) provide a formal framework for making dialogue management decisions under uncertainty, but efficiency and interpretability considerations mean that most current statistical dialogue managers are only MDPs. These MDP systems encode uncertainty explicitly in a single state representation. We formalise such MDP states in terms of distributions over POMDP states, and propose a new dialogue system architecture (Mixture Model POMDPs) which uses mixtures of these distributions to efficiently represent uncertainty. We also provide initial evaluation results (with real users) for this architecture.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>E Klein</author>
<author>O Lemon</author>
<author>T Oka</author>
</authors>
<title>DIPPER: Description and Formalisation of an Information-State Update Dialogue System Architecture.</title>
<date>2003</date>
<booktitle>In Proc. SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="10727" citStr="Bos et al., 2003" startWordPosition="1731" endWordPosition="1734">e its partition and zero probability outside. However, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal semantics was provided for this list, and therefore only heuristic uses were suggested for it. 5 Initial Experiments We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER “Information State Update” dialogue manager (Bos et al., 2003). It uses equation (3) to compute belief state updates, given separate models for MDP state updates (for f(rit−1, at−1, hjt)), statistical ASR-SLU (for P(hjt|ut)/P(hjt)), and a statistical user model (for P(hjt|at−1, rit−1)). The state list is pruned as described in section 2, where the “core features” are the filled information slot values and whether they have been confirmed. For example, the system will merge two states which agree that the user only wants a cheap hotel, even if they disagree on the sequence of dialogue acts which lead to this information. It also never prunes the “null” st</context>
</contexts>
<marker>Bos, Klein, Lemon, Oka, 2003</marker>
<rawString>J Bos, E Klein, O Lemon, and T Oka. 2003. DIPPER: Description and Formalisation of an Information-State Update Dialogue System Architecture. In Proc. SIGdial Workshop on Discourse and Dialogue, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>Learning User Simulations for Information State Update Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proc. Eurospeech.</booktitle>
<contexts>
<context position="12712" citStr="Georgila et al., 2005" startWordPosition="2070" endWordPosition="2073">an-machine dialogues, showing task completion and average length. the TownInfo corpus of dialogues, which was collected using the TownInfo human-machine dialogue systems of (Lemon et al., 2006), transcribed, and hand annotated. ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU hypotheses are passed to the dialogue manager. Thus the number of MDP states in the dialogue manager grows by up to three times at each step, before pruning. For the user model, the system uses an ngram user model, as described in (Georgila et al., 2005), trained on the annotated TownInfo corpus.2 The system’s dialogue management policy is a Mixture Model Q-MDP (MM Q-MDP) policy. As with the MDP states, the MDP Q function is from (Lemon and Liu, 2007). It was trained in an MDP system using reinforcement learning with simulated users (Lemon and Liu, 2007), and was not modified for use in our MM Q-MDP policy. We tested this system with 10 different users, each attempting 9 tasks in the TownInfo domain (searching for hotels and restaurants in a fictitious town), resulting in 90 test dialogues. The users each attempted 3 tasks with the MDP system</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>K Georgila, J Henderson, and O Lemon. 2005. Learning User Simulations for Information State Update Dialogue Systems. In Proc. Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Higashinaka</author>
<author>M Nakano</author>
<author>K Aikawa</author>
</authors>
<title>Corpus-based discourse understanding in spoken dialogue systems.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="10413" citStr="Higashinaka et al., 2003" startWordPosition="1681" endWordPosition="1685">s over POMDP states is similar to the approach in (Young et al., 2007), where POMDP belief states are represented using a set of partitions of POMDP states. For any set of partitions, the mixture model approach could express the same model by defining one MDP state per partition and giving it a uniform distribution inside its partition and zero probability outside. However, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal semantics was provided for this list, and therefore only heuristic uses were suggested for it. 5 Initial Experiments We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER “Information State Update” dialogue manager (Bos et al., 2003). It uses equation (3) to compute belief state updates, given separate models for MDP state updates (for f(rit−1, at−1, hjt)), statistical ASR-SLU (for P(hjt|ut)/P(hjt)), and a statistical user model (for P(hjt|at−1, rit−1)). The state list is pruned as described in section 2, where th</context>
</contexts>
<marker>Higashinaka, Nakano, Aikawa, 2003</marker>
<rawString>H Higashinaka, M Nakano, and K Aikawa. 2003. Corpus-based discourse understanding in spoken dialogue systems. In Proc. ACL, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>X Liu</author>
</authors>
<title>Dialogue policy learning for combinations of noise and user simulation: transfer results.</title>
<date>2007</date>
<booktitle>In Proc. SIGdial.</booktitle>
<contexts>
<context position="11535" citStr="Lemon and Liu, 2007" startWordPosition="1865" endWordPosition="1868"> user model (for P(hjt|at−1, rit−1)). The state list is pruned as described in section 2, where the “core features” are the filled information slot values and whether they have been confirmed. For example, the system will merge two states which agree that the user only wants a cheap hotel, even if they disagree on the sequence of dialogue acts which lead to this information. It also never prunes the “null” state, so that there is always some probability that the system knows nothing. The system used in the experiments described below uses the MDP state representation and update function from (Lemon and Liu, 2007), which is designed for standard slot-filling dialogues. For the ASR model, it uses the HTK speech recogniser (Young et al., 2002) and an n-best list of three ASR hypotheses on each user turn. The prior over user inputs is assumed to be uniform. The ASR hypotheses are passed to the SLU model from (Meza-Ruiz et al., 2008), which produces a single user input for each ASR hypothesis. This SLU model was trained on 75 TC % Av. length (std. deviation) Handcoded 56.0 7.2 (4.6) MDP 66.6 7.2 (4.0) MM Q-MDP 73.3 7.3 (3.7) Table 1: Initial test results for human-machine dialogues, showing task completion</context>
<context position="12913" citStr="Lemon and Liu, 2007" startWordPosition="2104" endWordPosition="2107">bed, and hand annotated. ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU hypotheses are passed to the dialogue manager. Thus the number of MDP states in the dialogue manager grows by up to three times at each step, before pruning. For the user model, the system uses an ngram user model, as described in (Georgila et al., 2005), trained on the annotated TownInfo corpus.2 The system’s dialogue management policy is a Mixture Model Q-MDP (MM Q-MDP) policy. As with the MDP states, the MDP Q function is from (Lemon and Liu, 2007). It was trained in an MDP system using reinforcement learning with simulated users (Lemon and Liu, 2007), and was not modified for use in our MM Q-MDP policy. We tested this system with 10 different users, each attempting 9 tasks in the TownInfo domain (searching for hotels and restaurants in a fictitious town), resulting in 90 test dialogues. The users each attempted 3 tasks with the MDP system of (Lemon and Liu, 2007), 3 tasks with a state-of-the-art handcoded system (see (Lemon et al., 2006)), and 3 tasks with the MM Q-MDP system. Ordering of systems and tasks was controlled, and 3 of the </context>
</contexts>
<marker>Lemon, Liu, 2007</marker>
<rawString>O Lemon and X Liu. 2007. Dialogue policy learning for combinations of noise and user simulation: transfer results. In Proc. SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>K Georgila</author>
<author>J Henderson</author>
</authors>
<title>Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation.</title>
<date>2006</date>
<booktitle>In Proc. ACL/IEEE SLT.</booktitle>
<contexts>
<context position="12283" citStr="Lemon et al., 2006" startWordPosition="1993" endWordPosition="1996">) and an n-best list of three ASR hypotheses on each user turn. The prior over user inputs is assumed to be uniform. The ASR hypotheses are passed to the SLU model from (Meza-Ruiz et al., 2008), which produces a single user input for each ASR hypothesis. This SLU model was trained on 75 TC % Av. length (std. deviation) Handcoded 56.0 7.2 (4.6) MDP 66.6 7.2 (4.0) MM Q-MDP 73.3 7.3 (3.7) Table 1: Initial test results for human-machine dialogues, showing task completion and average length. the TownInfo corpus of dialogues, which was collected using the TownInfo human-machine dialogue systems of (Lemon et al., 2006), transcribed, and hand annotated. ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU hypotheses are passed to the dialogue manager. Thus the number of MDP states in the dialogue manager grows by up to three times at each step, before pruning. For the user model, the system uses an ngram user model, as described in (Georgila et al., 2005), trained on the annotated TownInfo corpus.2 The system’s dialogue management policy is a Mixture Model Q-MDP (MM Q-MDP) policy. As with the MDP states, the MDP Q function</context>
</contexts>
<marker>Lemon, Georgila, Henderson, 2006</marker>
<rawString>O Lemon, K Georgila, and J Henderson. 2006. Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation. In Proc. ACL/IEEE SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ML Littman</author>
<author>AR Cassandra</author>
<author>LP Kaelbling</author>
</authors>
<title>Learning policies for partially observable environments: Scaling up.</title>
<date>1995</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="9654" citStr="Littman et al., 1995" startWordPosition="1550" endWordPosition="1553">logue management policy can be created by simply computing a mixture of the MDP policy applied to the MDP states in the belief state list. More precisely, we assume that the original MDP system specifies a Q function QMDP(at, rt) which estimates the expected future reward of performing action at in state rt. We then estimate the expected future reward of performing action at in belief state bt as the mixture of these MDP estimates. Q(at, bt) � � ptQMDP(at, rit) (5) i The dialogue management policy is to choose the action at with the largest value for Q(at, bt). This is known as a Q-MDP model (Littman et al., 1995), so we call this proposal a Mixture Model Q-MDP. 4 Related Work Our representation of POMDP belief states using a set of distributions over POMDP states is similar to the approach in (Young et al., 2007), where POMDP belief states are represented using a set of partitions of POMDP states. For any set of partitions, the mixture model approach could express the same model by defining one MDP state per partition and giving it a uniform distribution inside its partition and zero probability outside. However, the mixture model approach is more flexible, because the distributions in the mixture do </context>
</contexts>
<marker>Littman, Cassandra, Kaelbling, 1995</marker>
<rawString>ML Littman, AR Cassandra, and LP Kaelbling. 1995. Learning policies for partially observable environments: Scaling up. In Proc. ICML, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Meza-Ruiz</author>
<author>S Riedel</author>
<author>O Lemon</author>
</authors>
<title>Accurate statistical spoken language understanding from limited development resources.</title>
<date>2008</date>
<booktitle>In Proc. ICASSP.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="11857" citStr="Meza-Ruiz et al., 2008" startWordPosition="1923" endWordPosition="1926"> sequence of dialogue acts which lead to this information. It also never prunes the “null” state, so that there is always some probability that the system knows nothing. The system used in the experiments described below uses the MDP state representation and update function from (Lemon and Liu, 2007), which is designed for standard slot-filling dialogues. For the ASR model, it uses the HTK speech recogniser (Young et al., 2002) and an n-best list of three ASR hypotheses on each user turn. The prior over user inputs is assumed to be uniform. The ASR hypotheses are passed to the SLU model from (Meza-Ruiz et al., 2008), which produces a single user input for each ASR hypothesis. This SLU model was trained on 75 TC % Av. length (std. deviation) Handcoded 56.0 7.2 (4.6) MDP 66.6 7.2 (4.0) MM Q-MDP 73.3 7.3 (3.7) Table 1: Initial test results for human-machine dialogues, showing task completion and average length. the TownInfo corpus of dialogues, which was collected using the TownInfo human-machine dialogue systems of (Lemon et al., 2006), transcribed, and hand annotated. ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU</context>
</contexts>
<marker>Meza-Ruiz, Riedel, Lemon, 2008</marker>
<rawString>I Meza-Ruiz, S Riedel, and O Lemon. 2008. Accurate statistical spoken language understanding from limited development resources. In Proc. ICASSP. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1194" citStr="Williams and Young, 2007" startWordPosition="158" endWordPosition="161">tems encode uncertainty explicitly in a single state representation. We formalise such MDP states in terms of distributions over POMDP states, and propose a new dialogue system architecture (Mixture Model POMDPs) which uses mixtures of these distributions to efficiently represent uncertainty. We also provide initial evaluation results (with real users) for this architecture. 1 Introduction Partially Observable Markov Decision Processes (POMDPs) provide a formal framework for making decisions under uncertainty. Recent research in spoken dialogue systems has used POMDPs for dialogue management (Williams and Young, 2007; Young et al., 2007). These systems represent the uncertainty about the dialogue history using a probability distribution over dialogue states, known as the POMDP’s belief state, and they use approximate POMDP inference procedures to make dialogue management decisions. However, these inference procedures are too computationally intensive for most domains, and the system’s behaviour can be difficult to predict. Instead, most current statistical dialogue managers use a single state to represent the dialogue history, thereby making them only Markov Decision Process models (MDPs). These state rep</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2):231–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>G Evermann</author>
<author>D Kershaw</author>
<author>G Moore</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>D Povey</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<title>The HTK Book. Cambridge Univ.</title>
<date>2002</date>
<journal>Eng. Dept.</journal>
<contexts>
<context position="11665" citStr="Young et al., 2002" startWordPosition="1886" endWordPosition="1889">information slot values and whether they have been confirmed. For example, the system will merge two states which agree that the user only wants a cheap hotel, even if they disagree on the sequence of dialogue acts which lead to this information. It also never prunes the “null” state, so that there is always some probability that the system knows nothing. The system used in the experiments described below uses the MDP state representation and update function from (Lemon and Liu, 2007), which is designed for standard slot-filling dialogues. For the ASR model, it uses the HTK speech recogniser (Young et al., 2002) and an n-best list of three ASR hypotheses on each user turn. The prior over user inputs is assumed to be uniform. The ASR hypotheses are passed to the SLU model from (Meza-Ruiz et al., 2008), which produces a single user input for each ASR hypothesis. This SLU model was trained on 75 TC % Av. length (std. deviation) Handcoded 56.0 7.2 (4.6) MDP 66.6 7.2 (4.0) MM Q-MDP 73.3 7.3 (3.7) Table 1: Initial test results for human-machine dialogues, showing task completion and average length. the TownInfo corpus of dialogues, which was collected using the TownInfo human-machine dialogue systems of (L</context>
</contexts>
<marker>Young, Evermann, Kershaw, Moore, Odell, Ollason, Povey, Valtchev, Woodland, 2002</marker>
<rawString>S Young, G Evermann, D Kershaw, G Moore, J Odell, D Ollason, D Povey, V Valtchev, and P Woodland. 2002. The HTK Book. Cambridge Univ. Eng. Dept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
</authors>
<title>The Hidden Information State Approach to Dialog Management.</title>
<date>2007</date>
<booktitle>In Proc. ICASSP,</booktitle>
<location>Honolulu.</location>
<contexts>
<context position="1215" citStr="Young et al., 2007" startWordPosition="162" endWordPosition="165">plicitly in a single state representation. We formalise such MDP states in terms of distributions over POMDP states, and propose a new dialogue system architecture (Mixture Model POMDPs) which uses mixtures of these distributions to efficiently represent uncertainty. We also provide initial evaluation results (with real users) for this architecture. 1 Introduction Partially Observable Markov Decision Processes (POMDPs) provide a formal framework for making decisions under uncertainty. Recent research in spoken dialogue systems has used POMDPs for dialogue management (Williams and Young, 2007; Young et al., 2007). These systems represent the uncertainty about the dialogue history using a probability distribution over dialogue states, known as the POMDP’s belief state, and they use approximate POMDP inference procedures to make dialogue management decisions. However, these inference procedures are too computationally intensive for most domains, and the system’s behaviour can be difficult to predict. Instead, most current statistical dialogue managers use a single state to represent the dialogue history, thereby making them only Markov Decision Process models (MDPs). These state rep73 resentations have </context>
<context position="9858" citStr="Young et al., 2007" startWordPosition="1586" endWordPosition="1589">Q function QMDP(at, rt) which estimates the expected future reward of performing action at in state rt. We then estimate the expected future reward of performing action at in belief state bt as the mixture of these MDP estimates. Q(at, bt) � � ptQMDP(at, rit) (5) i The dialogue management policy is to choose the action at with the largest value for Q(at, bt). This is known as a Q-MDP model (Littman et al., 1995), so we call this proposal a Mixture Model Q-MDP. 4 Related Work Our representation of POMDP belief states using a set of distributions over POMDP states is similar to the approach in (Young et al., 2007), where POMDP belief states are represented using a set of partitions of POMDP states. For any set of partitions, the mixture model approach could express the same model by defining one MDP state per partition and giving it a uniform distribution inside its partition and zero probability outside. However, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal sema</context>
</contexts>
<marker>Young, Schatzmann, Weilhammer, Ye, 2007</marker>
<rawString>SJ Young, J Schatzmann, K Weilhammer, and H Ye. 2007. The Hidden Information State Approach to Dialog Management. In Proc. ICASSP, Honolulu.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>