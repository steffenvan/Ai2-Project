<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999052">
Unsupervised Relation Extraction by Mining Wikipedia Texts Using
Information from the Web
</title>
<author confidence="0.994082">
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang and Mitsuru Ishizuka
</author>
<affiliation confidence="0.98818">
The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
</affiliation>
<email confidence="0.79362">
yulan@mi.ci.i.u-tokyo.ac.jp
okazaki@is.s.u-tokyo.ac.jp
</email>
<note confidence="0.581893">
matsuo@biz-model.t.utokyo.ac.jp
</note>
<email confidence="0.9690085">
yangzl@tkl.iis.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.993571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925894736842">
This paper presents an unsupervised rela-
tion extraction method for discovering and
enhancing relations in which a specified
concept in Wikipedia participates. Using
respective characteristics of Wikipedia ar-
ticles and Web corpus, we develop a clus-
tering approach based on combinations of
patterns: dependency patterns from depen-
dency analysis of texts in Wikipedia, and
surface patterns generated from highly re-
dundant information related to the Web.
Evaluations of the proposed approach on
two different domains demonstrate the su-
periority of the pattern combination over
existing approaches. Fundamentally, our
method demonstrates how deep linguistic
patterns contribute complementarily with
Web surface patterns to the generation of
various relations.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941830508475">
Machine learning approaches for relation extrac-
tion tasks require substantial human effort, partic-
ularly when applied to the broad range of docu-
ments, entities, and relations existing on the Web.
Even with semi-supervised approaches, which use
a large unlabeled corpus, manual construction of a
small set of seeds known as true instances of the
target entity or relation is susceptible to arbitrary
human decisions. Consequently, a need exists for
development of semantic information-retrieval al-
gorithms that can operate in a manner that is as
unsupervised as possible.
Currently, the leading methods in unsupervised
information extraction collect redundancy infor-
mation from a local corpus or use the Web as a
corpus (Pantel and Pennacchiotti, 2006); (Banko
et al., 2007); (Bollegala et al., 2007): (Fan et
al., 2008); (Davidov and Rappoport, 2008). The
standard process is to scan or search the cor-
pus to collect co-occurrences of word pairs with
strings between them, and then to calculate term
co-occurrence or generate surface patterns. The
method is used widely. However, even when pat-
terns are generated from well-written texts, fre-
quent pattern mining is non-trivial because the
number of unique patterns is loose, but many pat-
terns are non-discriminative and correlated. A
salient challenge and research interest for frequent
pattern mining is abstraction away from different
surface realizations of semantic relations to dis-
cover discriminative patterns efficiently.
Linguistic analysis is another effective tech-
nology for semantic relation extraction, as de-
scribed in many reports such as (Kambhatla,
2004); (Bunescu and Mooney, 2005); (Harabagiu
et al., 2005); (Nguyen et al., 2007). Currently, lin-
guistic approaches for semantic relation extraction
are mostly supervised, relying on pre-specification
of the desired relation or initial seed words or pat-
terns from hand-coding. The common process is
to generate linguistic features based on analyses of
the syntactic features, dependency, or shallow se-
mantic structure of text. Then the system is trained
to identify entity pairs that assume a relation and
to classify them into pre-defined relations. The ad-
vantage of these methods is that they use linguistic
technologies to learn semantic information from
different surface expressions.
As described herein, we consider integrating
linguistic analysis with Web frequency informa-
tion to improve the performance of unsupervised
relation extraction. As (Banko et al., 2007)
reported, “deep” linguistic technology presents
problems when applied to heterogeneous text on
the Web. Therefore, we do not parse informa-
tion from the Web corpus, but from well written
texts. Particularly, we specifically examine unsu-
pervised relation extraction from existing texts of
Wikipedia articles. Wikipedia resources of a fun-
</bodyText>
<page confidence="0.957637">
1021
</page>
<note confidence="0.9996065">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021–1029,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999211272727273">
damental type are of concepts (e.g., represented
by Wikipedia articles as a special case) and their
mutual relations. We propose our method, which
groups concept pairs into several clusters based on
the similarity of their contexts. Contexts are col-
lected as patterns of two kinds: dependency pat-
terns from dependency analysis of sentences in
Wikipedia, and surface patterns generated from
highly redundant information from the Web.
The main contributions of this paper are as fol-
lows:
</bodyText>
<listItem confidence="0.804124235294118">
• Using characteristics of Wikipedia articles
and the Web corpus respectively, our study
yields an example of bridging the gap sep-
arating “deep” linguistic technology and re-
dundant Web information for Information
Extraction tasks.
• Our experimental results reveal that relations
are extractable with good precision using
linguistic patterns, whereas surface patterns
from Web frequency information contribute
greatly to the coverage of relation extraction.
• The combination of these patterns produces
a clustering method to achieve high pre-
cision for different Information Extraction
applications, especially for bootstrapping a
high-recall semi-supervised relation extrac-
tion system.
</listItem>
<sectionHeader confidence="0.999218" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999943490909091">
(Hasegawa et al., 2004) introduced a method for
discovering a relation by clustering pairs of co-
occurring entities represented as vectors of con-
text features. They used a simple representation
of contexts; the features were words in sentences
between the entities of the candidate pairs.
(Turney, 2006) presented an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. Given a word pair, the
output list of lexicon-syntactic patterns was ranked
by pertinence, which showed how well each pat-
tern expresses the relations between word pairs.
(Davidov et al., 2007) proposed a method for
unsupervised discovery of concept specific rela-
tions, requiring initial word seeds. That method
used pattern clusters to define general relations,
specific to a given concept. (Davidov and Rap-
poport, 2008) presented an approach to discover
and represent general relations present in an arbi-
trary corpus. That approach incorporated a fully
unsupervised algorithm for pattern cluster discov-
ery, which searches, clusters, and merges high-
frequency patterns around randomly selected con-
cepts.
The field of Unsupervised Relation Identifica-
tion (URI)—the task of automatically discover-
ing interesting relations between entities in large
text corpora—was introduced by (Hasegawa et
al., 2004). Relations are discovered by cluster-
ing pairs of co-occurring entities represented as
vectors of context features. (Rosenfeld and Feld-
man, 2006) showed that the clusters discovered by
URI are useful for seeding a semi-supervised rela-
tion extraction system. To compare different clus-
tering algorithms, feature extraction and selection
method, (Rosenfeld and Feldman, 2007) presented
a URI system that used surface patterns of two
kinds: patterns that test two entities together and
patterns that test either of two entities.
In this paper, we propose an unsupervised rela-
tion extraction method that combines patterns of
two types: surface patterns and dependency pat-
terns. Surface patterns are generated from the Web
corpus to provide redundancy information for re-
lation extraction. In addition, to obtain seman-
tic information for concept pairs, we generate de-
pendency patterns to abstract away from different
surface realizations of semantic relations. Depen-
dency patterns are expected to be more accurate
and less spam-prone than surface patterns from the
Web corpus. Surface patterns from redundancy
Web information are expected to address the data
sparseness problem. Wikipedia is currently widely
used information extraction as a local corpus; the
Web is used as a global corpus.
</bodyText>
<sectionHeader confidence="0.803839" genericHeader="method">
3 Characteristics of Wikipedia articles
</sectionHeader>
<bodyText confidence="0.999966428571428">
Wikipedia, unlike the whole Web corpus, has
several characteristics that markedly facilitate in-
formation extraction. First, as an earlier report
(Giles, 2005) explained, Wikipedia articles are
much cleaner than typical Web pages. Because
the quality is not so different from standard writ-
ten English, we can use “deep” linguistic tech-
nologies, such as syntactic or dependency parsing.
Secondly, Wikipedia articles are heavily cross-
linked, in a manner resembling cross-linking of
the Web pages. (Gabrilovich and Markovitch,
2006) assumed that these links encode numerous
interesting relations among concepts, and that they
provide an important source of information in ad-
</bodyText>
<page confidence="0.993285">
1022
</page>
<bodyText confidence="0.9999334">
dition to the article texts.
To establish the background for this paper, we
start by defining the problem under consideration:
relation extraction from Wikipedia. We use the en-
cyclopedic nature of the corpus by specifically ex-
amining the relation extraction between the enti-
tled concept (ec) and a related concept (rc), which
are described in anchor text in this article. A com-
mon assumption is that, when investigating the se-
mantics in articles such as those in Wikipedia (e.g.
semantic Wikipedia (Volkel et al., 2006)), key in-
formation related to a concept described on a page
p lies within the set of links l(p) on that page; par-
ticularly, it is likely that a salient semantic relation
r exists between p and a related page p&apos; E l(p).
Given the scenario we described along with
earlier related works, the challenges we face are
these: 1) enumerating all potential relation types
of interest for extraction is highly problematic for
corpora as large and varied as Wikipedia; 2) train-
ing data or seed data are difficult to label. Consid-
ering (Davidov and Rappoport, 2008), which de-
scribes work to get the target word and relation
cluster given a single (‘hook’) word, their method
depends mainly on frequency information from
the Web to obtain a target and clusters. Attempt-
ing to improve the performance, our solution for
these challenges is to combine frequency informa-
tion from the Web and the “high quality” charac-
teristic of Wikipedia text.
</bodyText>
<sectionHeader confidence="0.9621475" genericHeader="method">
4 Pattern Combination Method for
Relation Extraction
</sectionHeader>
<bodyText confidence="0.999990555555555">
With the scene and challenges stated, we propose a
solution in the following way. The intuitive idea is
that we integrate linguistic technologies on high-
quality text in Wikipedia and Web mining tech-
nologies on a large-scale Web corpus. In this sec-
tion, we first provide an overview of our method
along with the function of the main modules. Sub-
sequently, we explain each module in the method
in detail.
</bodyText>
<subsectionHeader confidence="0.99875">
4.1 Overview of the Method
</subsectionHeader>
<bodyText confidence="0.9999724">
Given a set of Wikipedia articles as input, our
method outputs a list of concept pairs for each ar-
ticle with a relation label assigned to each concept
pair. Briefly, the proposed approach has four main
modules, as depicted in Fig. 1.
</bodyText>
<listItem confidence="0.986244">
• Text Preprocessor and Concept Pair Col-
lector preprocesses Wikipedia articles to
</listItem>
<figureCaption confidence="0.999349">
Figure 1: Framework of the proposed approach
</figureCaption>
<bodyText confidence="0.999280666666667">
split text and filter sentences. It outputs con-
cept pairs, each of which has an accompany-
ing sentence.
</bodyText>
<listItem confidence="0.97883425">
• Web Context Collector collects context in-
formation from the Web and generates ranked
relational terms and surface patterns for each
concept pair.
• Dependency Pattern Extractor generates
dependency patterns for each concept pair
from corresponding sentences in Wikipedia
articles.
• Clustering Algorithm clusters concept pairs
based on their context. It consists of the two
sub-modules described below.
– Depend Clustering, which merges con-
</listItem>
<bodyText confidence="0.9046827">
cept pairs using dependency patterns
alone, aiming at obtaining clusters of
concept pairs with good precision;
– Surface Clustering, which clusters
concept pairs using surface patterns
based on the resultant clusters of depend
clustering. The aim is to merge more
concept pairs into existing clusters with
surface patterns to improve the coverage
of clusters.
</bodyText>
<page confidence="0.922407">
1023
</page>
<subsectionHeader confidence="0.961877">
4.2 Text Preprocessor and Concept Pair
Collector
</subsectionHeader>
<bodyText confidence="0.999953857142857">
This module pre-processes Wikipedia article texts
to collect concept pairs and corresponding sen-
tences. Given a concept described in a Wikipedia
article, our idea of preprocessing executes initial
consideration of all anchor-text concepts linking
to other Wikipedia articles in the article as related
concepts that might share a semantic relation with
the entitled concept. The link structure, more par-
ticularly, the structure of outgoing links, provides
a simple mechanism for identifying relevant arti-
cles. We split text into sentences and select sen-
tences containing one reference of an entitled con-
cept and one of the linked texts for the dependency
pattern extractor module.
</bodyText>
<subsectionHeader confidence="0.99661">
4.3 Web Context Collector
</subsectionHeader>
<bodyText confidence="0.999981090909091">
Querying a concept pair using a search engine
(Google), we characterize the semantic relation
between the pair by leveraging the vast size of the
Web. Our hypothesis is that there exist some key
terms and patterns that provide clues to the rela-
tions between pairs. From the snippets retrieved
by the search engine, we extract relational infor-
mation of two kinds: ranked relational terms as
keywords and surface patterns. Here surface pat-
terns are generated with support of ranked rela-
tional terms.
</bodyText>
<subsectionHeader confidence="0.609419">
4.3.1 Relational Term Ranking
</subsectionHeader>
<bodyText confidence="0.999914523809524">
To collect relational terms as indicators for each
concept pair, we look for verbs and nouns from
qualified sentences in the snippets instead of sim-
ply finding verbs. Using only verbs as relational
terms might engender the loss of various important
relations, e.g. noun relations “CEO”, “founder”
between a person and a company. Therefore, for
each concept pair, a list of relational terms is col-
lected. Then all the collected terms of all concept
pairs are combined and ranked using an entropy-
based algorithm which is described in (Chen et al.,
2005). With their algorithm, the importance of
terms can be assessed using the entropy criterion,
which is based on the assumption that a term is ir-
relevant if its presence obscures the separability of
the dataset. After the ranking, we obtain a global
ranked list of relational terms Tall for the whole
dataset (all the concept pairs). For each concept
pair, a local list of relational terms Tcp is sorted ac-
cording to the terms’ order in Tall. Then from the
relational term list Tcp, a keyword tcp is selected
</bodyText>
<tableCaption confidence="0.993676">
Table 1: Surface patterns for a concept pair
</tableCaption>
<subsectionHeader confidence="0.356409">
Pattern Pattern
</subsectionHeader>
<bodyText confidence="0.96140725">
ec ceo rc rc found ec
ceo rc found ec rc succeed as ceo of ec
rc be ceo of ec ec ceo of rc
ec assign rc as ceo ec found by ceo rc
ceo of ec rc ec found in by rc
for each concept pair cp as the first term appearing
in the term list Tcp. Keyword tcp will be used to
initialize the clustering algorithm in Section 4.5.1.
</bodyText>
<subsectionHeader confidence="0.749222">
4.3.2 Surface Pattern Generation
</subsectionHeader>
<bodyText confidence="0.999758357142857">
Because simply taking the entire string between
two concept words captures an excess of extra-
neous and incoherent information, we use Tcp of
each concept pair as a key for surface pattern gen-
eration. We classified words into Content Words
(CWs) and Functional Words (FWs). From each
snippet sentence, the entitled concept, related con-
cept, or the keyword kcp is considered to be a Con-
tent Word (CW). Our idea of obtaining FWs is to
look for verbs, nouns, prepositions, and coordinat-
ing conjunctions that can help make explicit the
hidden relations between the target nouns.
Surface patterns have the following general
form.
</bodyText>
<equation confidence="0.799314">
[CW1] Infix1 [CW2] Infix2 [CW3] (1)
</equation>
<bodyText confidence="0.99987994117647">
Therein, Infix1 and Infix2 respectively con-
tain only and any number of FWs. A pattern ex-
ample is “ec assign rc as ceo (keyword)”. All gen-
erated patterns are sorted by their frequency, and
all occurrences of the entitled concept and related
concept are replaced with “ec” and “rc”, respec-
tively for pattern matching of different concept
pairs.
Table 1 presents examples of surface patterns
for a sample concept pair. Pattern windows are
bounded by CWs to obtain patterns more precisely
because 1) if we use only the string between two
concepts, it may not contain some important re-
lational information, such as “ceo ec resign rc”
in Table 1; 2) if we generate patterns by setting
a windows surrounding two concepts, the number
of unique patterns is often exponential.
</bodyText>
<subsectionHeader confidence="0.975379">
4.4 Dependency Pattern Extractor
</subsectionHeader>
<bodyText confidence="0.998614333333333">
In this section, we describe how to obtain depen-
dency patterns for relation clustering. After pre-
processing, selected sentences that contain at least
</bodyText>
<page confidence="0.985163">
1024
</page>
<bodyText confidence="0.9999394">
one mention of an entitled concept or related con-
cept are parsed into dependency structures. We de-
fine dependency patterns as sub-paths of the short-
est dependency path between a concept pair for
two reasons. One is that the shortest path de-
pendency kernels outperform dependency tree ker-
nels by offering a highly condensed representation
of the information needed to assess their relation
(Bunescu and Mooney, 2005). The other reason
is that embedded structures of the linguistic repre-
sentation are important for obtaining good cover-
age of the pattern acquisition, as explained in (Cu-
lotta and Sorensen, 2005); (Zhang et al., 2006).
The process of inducing dependency patterns has
two steps.
</bodyText>
<listItem confidence="0.9678003">
1. Shortest dependency path inducement. From
the original dependency tree structure by parsing
the selected sentence for each concept pair, we
first induce the shortest dependency path with the
entitled concept and related concept.
2. Dependency pattern generation. We use
a frequent tree-mining algorithm (Zaki, 2002) to
generate sub-paths as dependency patterns from
the shortest dependency path for relation cluster-
ing.
</listItem>
<subsectionHeader confidence="0.898275">
4.5 Clustering Algorithm for Relation
Extraction
</subsectionHeader>
<bodyText confidence="0.999959785714286">
In this subsection, we present a clustering algo-
rithm that merges concept pairs based on depen-
dency patterns and surface patterns. The algorithm
is based on k-means clustering for relation cluster-
ing.
The dependency pattern has the properties of
being more accurate, but the Web context has the
advantage of containing much more redundant in-
formation than Wikipedia. Our idea of concept
pair clustering is a two-step clustering process:
first it clusters concept pairs into clusters with
good precision using dependency patterns; then it
improves the coverage of the clusters using surface
patterns.
</bodyText>
<sectionHeader confidence="0.595087" genericHeader="method">
4.5.1 Initial Centroid Selection and Distance
Function Definition
</sectionHeader>
<bodyText confidence="0.999028529411765">
The standard k-means algorithm is affected by
the choice of seeds and the number of clusters
k. However, as we claimed in the Introduc-
tion section, because we aim to extract relations
from Wikipedia articles in an unsupervised man-
ner, cluster number k is unknown and no good
centroids can be predicted. As described in this
paper, we select centroids based on the keyword
tcp of each concept pair.
First of all, all concept pairs are grouped by
their keywords tcp. Let G = {G1, G2, ...G,,,}
be the resultant groups, where each Gi =
{cpi1, cpi2, ...} identify a group of concept pairs
sharing the same keyword tcp (such as “CEO”).
We rank all the groups by their number of concept
pairs and then choose the top k groups. Then a
centroid ci is selected for each group Gi by Eq. 2.
</bodyText>
<equation confidence="0.990239666666667">
ci = arg max |{cpij|(dis1(cpij, cp)+
cp�Gi
A * dis2(cpij, cp)) &lt;= Dz, 1 :5 j :5 |Gi|} |(2)
</equation>
<bodyText confidence="0.999985727272727">
We assume a centroid for each group to be the
concept pair which has the most other concept
pairs in the same group that have distance less
than Dz with it. Also, Dz is a threshold to avoid
noisy concept pairs: we assign it 1/3. To balance
the contribution between dependency patterns and
surface patterns, A is used. The distance function
to calculate the distance between dependency pat-
tern sets DPi, DPj of two concept pairs cpi and
cpj is dis1. The distance is decided by the number
of overlapped dependency patterns with Eq. 3.
</bodyText>
<equation confidence="0.998934">
dis1(cpi, cpj) = 1 − |DPi n DPj |(3)
V/ (|DPi |* |DPj|)
</equation>
<bodyText confidence="0.9946924">
Actually, dis2 is the distance function to calcu-
late distance between two surface pattern sets of
two concept pairs. To compute the distance over
surface patterns, we implement the distance func-
tion dis2(cpi, cpj) in Fig. 2.
</bodyText>
<equation confidence="0.954014357142857">
Algorithm 1: distance function dis2(cpi, cpj)
Input: SP1 = {sp11, ..., sp1m}(surface patterns of
cpi)
SP2 = {sp21, ..., sp2n} (surface patterns of cpj)
Output: dis (distance between SP1 and SP2)
define a m × n distance matrix A:
{Aij = LD(sp1i,sp2j)
Max(|sp1i|,|sp2j|), 1≤i≤m; 1≤j≤n};
dis ← 0
for min(m, n) times do
(x, y) ← argmin0&lt;i&lt;m;0&lt;j&lt;nAij;
dis ← dis + Axy/min(m, n);
Ax∗ ← 1; A∗y ← 1;
return dis
</equation>
<figureCaption confidence="0.994108">
Figure 2: Distance function over surface patterns
</figureCaption>
<bodyText confidence="0.999773">
As shown in Fig. 2, the distance algorithm per-
forms as: firstly it defines a m x n distance matrix
A, then repeatedly selects two nearest sequences
and sums up their distances. While computing
</bodyText>
<page confidence="0.987491">
1025
</page>
<bodyText confidence="0.999953583333333">
dist, we use the Levenshtein distance LD to mea-
sure the difference of two surface patterns. The
Levenshtein distance is a metric for measuring the
amount of difference between two sequences (i.e.,
the so-called edit distance). Each generated sur-
face pattern is a sequence of words. The distance
of two surface patterns is defined as the fraction of
the LD value to the length of the longer sequence.
For estimating the number of clusters k, we ap-
ply the stability-based criteria from (Chen et al.,
2005) to decide the number of optimal clusters k
automatically.
</bodyText>
<subsectionHeader confidence="0.9247125">
4.5.2 Concept Pair Clustering with
Dependency Patterns
</subsectionHeader>
<bodyText confidence="0.999612533333333">
Given the initial seed concept pairs and cluster
number k, this stage merges concept pairs over de-
pendency patterns into k clusters. Each concept
pair cpi has a set of dependency patterns DPi. We
calculate distances between two pairs cpi and cpi
using above the function disc(cpi, cpi). The clus-
tering algorithm is portrayed in Fig. 3. The pro-
cess of depend clustering is to assign each concept
pair to the cluster with the closest centroid and
then recomputing each centroid based on the cur-
rent members of its cluster. As shown in Figure 3,
this is done iteratively by repeating both two steps
until a stopping criterion is met. We apply the ter-
mination condition as: centroids do not change be-
tween iterations.
</bodyText>
<figure confidence="0.847023">
Algorithm 2: Depend Clustering
Ir — C0
return C and Cd
</figure>
<figureCaption confidence="0.999399">
Figure 3: Clustering with dependency patterns
</figureCaption>
<bodyText confidence="0.99459675">
Because many concept pairs are scattered and
do not belong to any of the top k clusters, we
filter concept pairs with distance larger than Di
with the seed concept pairs. Such concept pairs
</bodyText>
<figureCaption confidence="0.9827375">
Figure 4: Example showing why surface cluster-
ing is needed
</figureCaption>
<bodyText confidence="0.997418">
are stored in Co. We named the cluster of concept
pairs Ir which are left to be clustered in the next
step of clustering. After this step, concept pairs
with similar dependency patterns are merged into
same clusters, see Fig. 4 (ST1, ST2).
</bodyText>
<subsectionHeader confidence="0.869086">
4.5.3 Concept Pair Clustering with Surface
Patterns
</subsectionHeader>
<bodyText confidence="0.999994346153846">
A salient difficulty posed by dependency pattern
clustering is that concept pairs of the same se-
mantic relation cannot be merged if they are ex-
pressed in different dependency structures. Fig-
ure 4 presents an example demonstrating why we
perform surface pattern clustering. As depicted
in Fig. 4, ST1, ST2, ST3, and ST4 are depen-
dency structures for four concept pairs that should
be classified as the same relation “CEO”. However
ST3 and ST4 can not be merged with ST1 and
ST2 using the dependency patterns because their
dependency structures are too diverse to share suf-
ficient dependency patterns.
In this step, we use surface patterns to merge
more concept pairs for each cluster to improve the
coverage. Figure 5 portrays the algorithm. We
assume that each concept pair has a set of sur-
face patterns from the Web context collector mod-
ule. As shown in Figure 5, surface clustering is
done iteratively by repeating two steps until a stop-
ping criterion is met: using the distance function
dist explained in the preceding section, assign
each concept pair to the cluster with the closest
centroid and recomputing each centroid based on
the current members of its cluster. We apply the
same termination condition as depend clustering.
</bodyText>
<equation confidence="0.878570071428571">
Input: I = {cp1, ..., cpn}(all concept pairs)
C = {c1, ..., ck} (k initial centroids)
Output: Md : I --+ C (cluster membership)
Ir (rest of concept pairs not clustered)
Cd = {c1, ..., ck} (recomputed centroids)
while stopping criterion has not been met do
for each cpi E I do
if minsE1..k dis1(cpi, cs) &lt;= Dl then
Md(cpi) argminsE1..k dis1(cpi, cs)
else
Md(cpi) �0
for each j E {1..k} do
recompute cj as the centroid of
{cpi|Mloc(cpi) = j}
</equation>
<page confidence="0.974927">
1026
</page>
<table confidence="0.509423">
Additionally, we filter concept pairs with distance
greater than Dg with the centroid concept pairs.
Table 2: Results for the category: “American chief
executives”
Algorithm 3: Surface Clustering
return clusters C
</table>
<figureCaption confidence="0.993306">
Figure 5: Clustering with surface patterns
</figureCaption>
<bodyText confidence="0.99984175">
Finally we have k clusters of concept pairs, each
of which has a centroid concept pair. To attach
a single relation label to each cluster, we use the
centroid concept pair.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999705461538461">
We apply our algorithm to two categories in
Wikipedia: “American chief executives” and
“Companies”. Both categories are well defined
and closed. We conduct experiments for extract-
ing various relations and for measuring the quality
of these relations in terms of precision and cover-
age. We use coverage as an evaluation instead of
using recall as a measure. The coverage is used to
evaluate all correctly extracted concept pairs. It is
defined as the fraction of all the correctly extracted
concept pairs to the whole set of concept pairs. To
balance between precision and coverage of clus-
tering, we integrate two parameters: Dl, Dg.
We downloaded the Wikipedia dump as of De-
cember 3, 2008. The performance of the pro-
posed method is evaluated using different pattern
types: dependency patterns, surface patterns, and
their combination. We compare our method with
(Rosenfeld and Feldman, 2007)’s URI method.
Their algorithm outperformed that presented in the
earlier work using surface features of two kinds for
unsupervised relation extraction: features that test
two entities together and features that test only one
entity each. For comparison, we use a k-means
clustering algorithm using the same cluster num-
ber k.
</bodyText>
<table confidence="0.842249">
method Existing method Proposed method
(Rosenfeld et al.) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
</table>
<subsectionHeader confidence="0.4547565">
5.1 Wikipedia Category: “American chief
executives”
</subsectionHeader>
<bodyText confidence="0.969885666666667">
We choose appropriate Dl(concept pair filter in
depend clustering) and Dg(concept pair filter in
surface clustering) in a development set. To bal-
ance precision and coverage, we set 1/3 for both
Dl and Dg.
The 526 articles in this category are used for
evaluation. We obtain 7310 concept pairs from
the articles as our dataset. The top 18 groups are
chosen to obtain the centroid concept pairs. Of
these, 15 binary relations are the clearly identifi-
able relations shown in Table 2, where # Ins. rep-
resents the number of concept pairs clustered us-
ing each method, and pre denotes the precision of
each cluster.
The proposed approach shows higher precision
and better coverage than URI in Table 2. This
result demonstrates that adding dependency pat-
terns from linguistic analysis contributes more to
the precision and coverage of the clustering task
than the sole use of surface patterns.
Input: Ir (rest of concept pairs)
</bodyText>
<equation confidence="0.592564333333333">
Cd = {c1, ..., ck} (initial centroids)
Output: Ms : Ir --+ C (cluster membership)
Cs = {c1, ..., ck} (final centroids)
while stopping criterion has not been met do
for each cpi E Ir do
if minsE1..k dis2(cpi, cs) &lt;= Dg then
Ms(cpi) argminsE1..k dis2(cpi, cs)
else
Ms(cpi) 0
for each j E L.k do
recompute cj as the centroid of cluster
{cpi|Md(cpi) = j V Ms(cpi) = j}
</equation>
<bodyText confidence="0.5522975">
chairman 434 63.52
(x be chairman of y)
ceo 396 73.74
(x be ceo of y)
</bodyText>
<figure confidence="0.939845837209302">
bear 138 83.33
(x be bear in y)
attend 225 67.11
(x attend y)
member 14 85.71
(x be member of y)
receive 97 67.97
(x receive y)
graduate 18 83.33
(x graduate from y)
degree 5 80.00
(x obtain y degree)
marry 55 41.67
(x marry y)
earn 23 86.96
(x earn y)
award 23 43.47
(x won y award)
hold 5 80.00
(x hold y degree)
become 35 74.29
(x become y)
director 24 67.35
(x be director of y)
die 18 77.78
(x die in y)
all 1510 68.27
547 68.37
423 77.54
276 86.96
313 70.28
175 91.43
117 73.53
92 88.04
78 82.05
74 61.25
51 88.24
46 84.78
37 72.97
37 81.08
29 79.31
19 84.21
2314 75.63
</figure>
<page confidence="0.987283">
1027
</page>
<tableCaption confidence="0.999647">
Table 3: Performance of different pattern types
</tableCaption>
<table confidence="0.99988375">
Pattern type #Instance Precision Coverage
dependency 1127 84.29 13.00%
surface 1510 68.27 14.10%
Combined 2314 75.63 23.94%
</table>
<tableCaption confidence="0.893635">
Table 4: Results for the category: “Companies”
</tableCaption>
<table confidence="0.996381827586207">
Method Existing method Proposed method
(Rosenfeld et al.) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
found 82 75.61 163 84.05
(found x in y) 82 76.83 122 82.79
base 23 86.97 120 89.34
(x be base in y)
headquarter
(x be headquarter in y)
service 37 51.35 108 69.44
(x offer y service) 113 77.88 88 72.72
store 59 62.71 70 64.28
(x open store in y) 51 64.71 67 70.15
acquire 25 76.00 57 77.19
(x acquire y)
list
(x list on y)
product
(x produce y)
CEO 37 64.86 39 66.67
(ceo x found y) 53 62.26 37 56.76
buy 35 82.86 26 80.77
(x buy y) 14 50.00 24 75.00
establish
(x be establish in y)
locate
(x be locate in y)
all 685 71.03 1039 76.87
</table>
<bodyText confidence="0.9999108">
To examine the contribution of dependency pat-
terns, we compare results obtained with patterns
of different kinds. Table 3 shows the precision and
coverage scores. The best precision is achieved by
dependency patterns. The precision is markedly
better than that of surface patterns. However, the
coverage is worse than that by surface patterns. As
we reported, many concept pairs are scattered and
do not belong to any of the top k clusters, the cov-
erage is low.
</bodyText>
<subsectionHeader confidence="0.973241">
5.2 Wikipedia Category: “Companies”
</subsectionHeader>
<bodyText confidence="0.999515666666667">
We also evaluate the performance for the “Com-
panies” category. Instead of using all the arti-
cles, we randomly select 434 articles for evalua-
tion and 4073 concept pairs from the articles form
our dataset for this category. We also set Dl and
Dy to 1/3. Then 28 groups are chosen. For each
group, a centroid concept pair is obtained. Finally,
of 28 clusters, 25 binary relations are clearly iden-
tifiable relations. Table 4 presents some relations.
</bodyText>
<tableCaption confidence="0.996996">
Table 5: Performance of different pattern types
</tableCaption>
<table confidence="0.99756175">
Pattern type #Instance Precision Coverage
dependency 551 82.58 11.17%
surface 685 71.03 11.95%
Combined 1039 76.87 19.61%
</table>
<bodyText confidence="0.999354608695652">
Our clustering algorithms use two filters Dl and
Dy to filter scattering concept pairs. In Table 4, we
present that concept pairs are clustered with good
precision. As in the first experiments, the combi-
nation of dependency patterns and surface patterns
contribute greatly to the precision and coverage.
Table 5 shows that, using dependency patterns,
the precision is the highest (82.58%), although the
coverage is the lowest.
All experimental results support our idea
mainly in two aspects: 1) Dependency analysis
can abstract away from different surface realiza-
tions of text. In addition, embedded structures of
the dependency representation are important for
obtaining a good coverage of the pattern acqui-
sition. Furthermore, the precision is better than
that of the string surface patterns from Web pages
of various kinds. 2) Surface patterns are used to
merge concept pairs with relations represented in
different dependency structures with redundancy
information from the vast size of Web pages. Us-
ing surface patterns, more concept pairs are clus-
tered, and the coverage is improved.
</bodyText>
<sectionHeader confidence="0.999784" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999645">
To discover a range of semantic relations from
a large corpus, we present an unsupervised rela-
tion extraction method using deep linguistic in-
formation to alleviate surface and noisy surface
patterns generated from a large corpus, and use
Web frequency information to ease the sparse-
ness of linguistic information. We specifically ex-
amine texts from Wikipedia articles. Relations
are gathered in an unsupervised way over pat-
terns of two types: dependency patterns by parsing
sentences in Wikipedia articles using a linguistic
parser, and surface patterns from redundancy in-
formation from the Web corpus using a search en-
gine. We report our experimental results in com-
parison to those of previous works. The results
show that the best performance arises from a com-
bination of dependency patterns and surface pat-
terns.
</bodyText>
<page confidence="0.993687">
1028
</page>
<sectionHeader confidence="0.995881" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905342465753">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of IJCAI-2007.
Danushka Bollegala, Yutaka Matsuo and Mitsuru
Ishizuka. 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Pro-
ceedings of WWW-2007.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT/EMLNP-2005.
Jinxiu Chen, Donghong Ji, Chew Lim Tan and
Zhengyu Niu. 2005. Unsupervised Feature Se-
lection for Relation Extraction. In Proceedings of
IJCNLP-2005.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL-2004.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by Web mining. In Proceed-
ings of ACL-2007.
Dmitry Davidov and Ari Rappoport. 2008. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. In Proceedings of ACL-
2008.
Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng
Yan, Jiawei Han, Philip S. Yu and Olivier Ver-
scheure. 2008. Direct Mining of Discriminative and
Essential Frequent Patterns via Model-based Search
Tree. In Proceedings of KDD-2008.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proceedings of
AAAI-2006.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature 438:900C901.
Sanda Harabagiu, Cosmin Adrian Bejan and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In Proceedings of IJCAI-2005.
Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In Proceedings of
ACL-2004.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els. In Proceedings of ACL-2004.
Dat P.T. Nguyen, Yutaka Matsuo and Mitsuru Ishizuka.
2007. Relation extraction from Wikipedia using sub-
tree mining. In Proceedings of AAAI-2007.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceedings
of ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2006.
URES: an Unsupervised Web Relation Extraction
System. In Proceedings of COLING/ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In
Proceedings of CIKM-2007.
Peter D. Turney. 2006. Expressing implicit seman-
tic relations without supervision. In Proceedings of
ACL-2006.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
wikipedia. In Proceedings of WWW-2006.
Mohammed J. Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of SIGKDD-2002.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-2006.
</reference>
<page confidence="0.996229">
1029
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455410">
<title confidence="0.9949025">Unsupervised Relation Extraction by Mining Wikipedia Texts Using Information from the Web</title>
<author confidence="0.978003">Yulan Yan</author>
<author confidence="0.978003">Naoaki Okazaki</author>
<author confidence="0.978003">Yutaka Matsuo</author>
<author confidence="0.978003">Zhenglu Yang</author>
<author confidence="0.978003">Mitsuru Ishizuka</author>
<affiliation confidence="0.719652">The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan</affiliation>
<abstract confidence="0.9857912">yulan@mi.ci.i.u-tokyo.ac.jp okazaki@is.s.u-tokyo.ac.jp matsuo@biz-model.t.utokyo.ac.jp yangzl@tkl.iis.u-tokyo.ac.jp ishizuka@i.u-tokyo.ac.jp Abstract This paper presents an unsupervised relation extraction method for discovering and enhancing relations in which a specified concept in Wikipedia participates. Using respective characteristics of Wikipedia articles and Web corpus, we develop a clustering approach based on combinations of patterns: dependency patterns from dependency analysis of texts in Wikipedia, and surface patterns generated from highly redundant information related to the Web. Evaluations of the proposed approach on two different domains demonstrate the superiority of the pattern combination over existing approaches. Fundamentally, our method demonstrates how deep linguistic patterns contribute complementarily with Web surface patterns to the generation of various relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of IJCAI-2007.</booktitle>
<contexts>
<context position="1930" citStr="Banko et al., 2007" startWordPosition="261" endWordPosition="264">ities, and relations existing on the Web. Even with semi-supervised approaches, which use a large unlabeled corpus, manual construction of a small set of seeds known as true instances of the target entity or relation is susceptible to arbitrary human decisions. Consequently, a need exists for development of semantic information-retrieval algorithms that can operate in a manner that is as unsupervised as possible. Currently, the leading methods in unsupervised information extraction collect redundancy information from a local corpus or use the Web as a corpus (Pantel and Pennacchiotti, 2006); (Banko et al., 2007); (Bollegala et al., 2007): (Fan et al., 2008); (Davidov and Rappoport, 2008). The standard process is to scan or search the corpus to collect co-occurrences of word pairs with strings between them, and then to calculate term co-occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away fr</context>
<context position="3643" citStr="Banko et al., 2007" startWordPosition="516" endWordPosition="519">terns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use linguistic technologies to learn semantic information from different surface expressions. As described herein, we consider integrating linguistic analysis with Web frequency information to improve the performance of unsupervised relation extraction. As (Banko et al., 2007) reported, “deep” linguistic technology presents problems when applied to heterogeneous text on the Web. Therefore, we do not parse information from the Web corpus, but from well written texts. Particularly, we specifically examine unsupervised relation extraction from existing texts of Wikipedia articles. Wikipedia resources of a fun1021 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021–1029, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP damental type are of concepts (e.g., represented by Wikipedia articles as a special case) and their mu</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead and Oren Etzioni. 2007. Open information extraction from the Web. In Proceedings of IJCAI-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
</authors>
<title>Yutaka Matsuo and Mitsuru Ishizuka.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW-2007.</booktitle>
<marker>Bollegala, 2007</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo and Mitsuru Ishizuka. 2007. Measuring Semantic Similarity between Words Using Web Search Engines. In Proceedings of WWW-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMLNP-2005.</booktitle>
<contexts>
<context position="2803" citStr="Bunescu and Mooney, 2005" startWordPosition="392" endWordPosition="395">ate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use linguistic technologies to learn </context>
<context position="16623" citStr="Bunescu and Mooney, 2005" startWordPosition="2583" endWordPosition="2586">is often exponential. 4.4 Dependency Pattern Extractor In this section, we describe how to obtain dependency patterns for relation clustering. After preprocessing, selected sentences that contain at least 1024 one mention of an entitled concept or related concept are parsed into dependency structures. We define dependency patterns as sub-paths of the shortest dependency path between a concept pair for two reasons. One is that the shortest path dependency kernels outperform dependency tree kernels by offering a highly condensed representation of the information needed to assess their relation (Bunescu and Mooney, 2005). The other reason is that embedded structures of the linguistic representation are important for obtaining good coverage of the pattern acquisition, as explained in (Culotta and Sorensen, 2005); (Zhang et al., 2006). The process of inducing dependency patterns has two steps. 1. Shortest dependency path inducement. From the original dependency tree structure by parsing the selected sentence for each concept pair, we first induce the shortest dependency path with the entitled concept and related concept. 2. Dependency pattern generation. We use a frequent tree-mining algorithm (Zaki, 2002) to g</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of HLT/EMLNP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxiu Chen</author>
<author>Donghong Ji</author>
<author>Chew Lim Tan</author>
<author>Zhengyu Niu</author>
</authors>
<title>Unsupervised Feature Selection for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP-2005.</booktitle>
<contexts>
<context position="13674" citStr="Chen et al., 2005" startWordPosition="2074" endWordPosition="2077"> support of ranked relational terms. 4.3.1 Relational Term Ranking To collect relational terms as indicators for each concept pair, we look for verbs and nouns from qualified sentences in the snippets instead of simply finding verbs. Using only verbs as relational terms might engender the loss of various important relations, e.g. noun relations “CEO”, “founder” between a person and a company. Therefore, for each concept pair, a list of relational terms is collected. Then all the collected terms of all concept pairs are combined and ranked using an entropybased algorithm which is described in (Chen et al., 2005). With their algorithm, the importance of terms can be assessed using the entropy criterion, which is based on the assumption that a term is irrelevant if its presence obscures the separability of the dataset. After the ranking, we obtain a global ranked list of relational terms Tall for the whole dataset (all the concept pairs). For each concept pair, a local list of relational terms Tcp is sorted according to the terms’ order in Tall. Then from the relational term list Tcp, a keyword tcp is selected Table 1: Surface patterns for a concept pair Pattern Pattern ec ceo rc rc found ec ceo rc fou</context>
<context position="20876" citStr="Chen et al., 2005" startWordPosition="3301" endWordPosition="3304">distance matrix A, then repeatedly selects two nearest sequences and sums up their distances. While computing 1025 dist, we use the Levenshtein distance LD to measure the difference of two surface patterns. The Levenshtein distance is a metric for measuring the amount of difference between two sequences (i.e., the so-called edit distance). Each generated surface pattern is a sequence of words. The distance of two surface patterns is defined as the fraction of the LD value to the length of the longer sequence. For estimating the number of clusters k, we apply the stability-based criteria from (Chen et al., 2005) to decide the number of optimal clusters k automatically. 4.5.2 Concept Pair Clustering with Dependency Patterns Given the initial seed concept pairs and cluster number k, this stage merges concept pairs over dependency patterns into k clusters. Each concept pair cpi has a set of dependency patterns DPi. We calculate distances between two pairs cpi and cpi using above the function disc(cpi, cpi). The clustering algorithm is portrayed in Fig. 3. The process of depend clustering is to assign each concept pair to the cluster with the closest centroid and then recomputing each centroid based on t</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2005</marker>
<rawString>Jinxiu Chen, Donghong Ji, Chew Lim Tan and Zhengyu Niu. 2005. Unsupervised Feature Selection for Relation Extraction. In Proceedings of IJCNLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL-2004.</booktitle>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by Web mining.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-2007.</booktitle>
<contexts>
<context position="5932" citStr="Davidov et al., 2007" startWordPosition="859" endWordPosition="862">m. 2 Related Work (Hasegawa et al., 2004) introduced a method for discovering a relation by clustering pairs of cooccurring entities represented as vectors of context features. They used a simple representation of contexts; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. That method used pattern clusters to define general relations, specific to a given concept. (Davidov and Rappoport, 2008) presented an approach to discover and represent general relations present in an arbitrary corpus. That approach incorporated a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters, and merges highfrequency patterns around randomly selected concepts. The field of Unsupervised Relation Identification (URI)—the task of automatically discove</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport and Moshe Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by Web mining. In Proceedings of ACL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Classification of Semantic Relationships between Nominals Using Pattern Clusters.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL2008.</booktitle>
<contexts>
<context position="2007" citStr="Davidov and Rappoport, 2008" startWordPosition="273" endWordPosition="276">approaches, which use a large unlabeled corpus, manual construction of a small set of seeds known as true instances of the target entity or relation is susceptible to arbitrary human decisions. Consequently, a need exists for development of semantic information-retrieval algorithms that can operate in a manner that is as unsupervised as possible. Currently, the leading methods in unsupervised information extraction collect redundancy information from a local corpus or use the Web as a corpus (Pantel and Pennacchiotti, 2006); (Banko et al., 2007); (Bollegala et al., 2007): (Fan et al., 2008); (Davidov and Rappoport, 2008). The standard process is to scan or search the corpus to collect co-occurrences of word pairs with strings between them, and then to calculate term co-occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discrimin</context>
<context position="6160" citStr="Davidov and Rappoport, 2008" startWordPosition="892" endWordPosition="896">s; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. That method used pattern clusters to define general relations, specific to a given concept. (Davidov and Rappoport, 2008) presented an approach to discover and represent general relations present in an arbitrary corpus. That approach incorporated a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters, and merges highfrequency patterns around randomly selected concepts. The field of Unsupervised Relation Identification (URI)—the task of automatically discovering interesting relations between entities in large text corpora—was introduced by (Hasegawa et al., 2004). Relations are discovered by clustering pairs of co-occurring entities represented as vectors of context features. (Rose</context>
<context position="9711" citStr="Davidov and Rappoport, 2008" startWordPosition="1440" endWordPosition="1443">s such as those in Wikipedia (e.g. semantic Wikipedia (Volkel et al., 2006)), key information related to a concept described on a page p lies within the set of links l(p) on that page; particularly, it is likely that a salient semantic relation r exists between p and a related page p&apos; E l(p). Given the scenario we described along with earlier related works, the challenges we face are these: 1) enumerating all potential relation types of interest for extraction is highly problematic for corpora as large and varied as Wikipedia; 2) training data or seed data are difficult to label. Considering (Davidov and Rappoport, 2008), which describes work to get the target word and relation cluster given a single (‘hook’) word, their method depends mainly on frequency information from the Web to obtain a target and clusters. Attempting to improve the performance, our solution for these challenges is to combine frequency information from the Web and the “high quality” characteristic of Wikipedia text. 4 Pattern Combination Method for Relation Extraction With the scene and challenges stated, we propose a solution in the following way. The intuitive idea is that we integrate linguistic technologies on highquality text in Wik</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008. Classification of Semantic Relationships between Nominals Using Pattern Clusters. In Proceedings of ACL2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Fan</author>
<author>Kun Zhang</author>
<author>Hong Cheng</author>
<author>Jing Gao</author>
<author>Xifeng Yan</author>
<author>Jiawei Han</author>
<author>Philip S Yu</author>
<author>Olivier Verscheure</author>
</authors>
<title>Direct Mining of Discriminative and Essential Frequent Patterns via Model-based Search Tree.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD-2008.</booktitle>
<contexts>
<context position="1976" citStr="Fan et al., 2008" startWordPosition="269" endWordPosition="272">ith semi-supervised approaches, which use a large unlabeled corpus, manual construction of a small set of seeds known as true instances of the target entity or relation is susceptible to arbitrary human decisions. Consequently, a need exists for development of semantic information-retrieval algorithms that can operate in a manner that is as unsupervised as possible. Currently, the leading methods in unsupervised information extraction collect redundancy information from a local corpus or use the Web as a corpus (Pantel and Pennacchiotti, 2006); (Banko et al., 2007); (Bollegala et al., 2007): (Fan et al., 2008); (Davidov and Rappoport, 2008). The standard process is to scan or search the corpus to collect co-occurrences of word pairs with strings between them, and then to calculate term co-occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic </context>
</contexts>
<marker>Fan, Zhang, Cheng, Gao, Yan, Han, Yu, Verscheure, 2008</marker>
<rawString>Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng Yan, Jiawei Han, Philip S. Yu and Olivier Verscheure. 2008. Direct Mining of Discriminative and Essential Frequent Patterns via Model-based Search Tree. In Proceedings of KDD-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Overcoming the brittleness bottleneck using wikipedia: Enhancing text categorization with encyclopedic knowledge.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-2006.</booktitle>
<contexts>
<context position="8492" citStr="Gabrilovich and Markovitch, 2006" startWordPosition="1236" endWordPosition="1239">xtraction as a local corpus; the Web is used as a global corpus. 3 Characteristics of Wikipedia articles Wikipedia, unlike the whole Web corpus, has several characteristics that markedly facilitate information extraction. First, as an earlier report (Giles, 2005) explained, Wikipedia articles are much cleaner than typical Web pages. Because the quality is not so different from standard written English, we can use “deep” linguistic technologies, such as syntactic or dependency parsing. Secondly, Wikipedia articles are heavily crosslinked, in a manner resembling cross-linking of the Web pages. (Gabrilovich and Markovitch, 2006) assumed that these links encode numerous interesting relations among concepts, and that they provide an important source of information in ad1022 dition to the article texts. To establish the background for this paper, we start by defining the problem under consideration: relation extraction from Wikipedia. We use the encyclopedic nature of the corpus by specifically examining the relation extraction between the entitled concept (ec) and a related concept (rc), which are described in anchor text in this article. A common assumption is that, when investigating the semantics in articles such as</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2006</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2006. Overcoming the brittleness bottleneck using wikipedia: Enhancing text categorization with encyclopedic knowledge. In Proceedings of AAAI-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Giles</author>
</authors>
<date>2005</date>
<note>Internet encyclopaedias go head to head. Nature 438:900C901.</note>
<contexts>
<context position="8122" citStr="Giles, 2005" startWordPosition="1183" endWordPosition="1184">tterns to abstract away from different surface realizations of semantic relations. Dependency patterns are expected to be more accurate and less spam-prone than surface patterns from the Web corpus. Surface patterns from redundancy Web information are expected to address the data sparseness problem. Wikipedia is currently widely used information extraction as a local corpus; the Web is used as a global corpus. 3 Characteristics of Wikipedia articles Wikipedia, unlike the whole Web corpus, has several characteristics that markedly facilitate information extraction. First, as an earlier report (Giles, 2005) explained, Wikipedia articles are much cleaner than typical Web pages. Because the quality is not so different from standard written English, we can use “deep” linguistic technologies, such as syntactic or dependency parsing. Secondly, Wikipedia articles are heavily crosslinked, in a manner resembling cross-linking of the Web pages. (Gabrilovich and Markovitch, 2006) assumed that these links encode numerous interesting relations among concepts, and that they provide an important source of information in ad1022 dition to the article texts. To establish the background for this paper, we start b</context>
</contexts>
<marker>Giles, 2005</marker>
<rawString>Jim Giles. 2005. Internet encyclopaedias go head to head. Nature 438:900C901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Cosmin Adrian Bejan</author>
<author>Paul Morarescu</author>
</authors>
<title>Shallow semantics for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-2005.</booktitle>
<contexts>
<context position="2829" citStr="Harabagiu et al., 2005" startWordPosition="396" endWordPosition="399">thod is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use linguistic technologies to learn semantic information from </context>
</contexts>
<marker>Harabagiu, Bejan, Morarescu, 2005</marker>
<rawString>Sanda Harabagiu, Cosmin Adrian Bejan and Paul Morarescu. 2005. Shallow semantics for relation extraction. In Proceedings of IJCAI-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004.</booktitle>
<contexts>
<context position="5352" citStr="Hasegawa et al., 2004" startWordPosition="770" endWordPosition="773">ple of bridging the gap separating “deep” linguistic technology and redundant Web information for Information Extraction tasks. • Our experimental results reveal that relations are extractable with good precision using linguistic patterns, whereas surface patterns from Web frequency information contribute greatly to the coverage of relation extraction. • The combination of these patterns produces a clustering method to achieve high precision for different Information Extraction applications, especially for bootstrapping a high-recall semi-supervised relation extraction system. 2 Related Work (Hasegawa et al., 2004) introduced a method for discovering a relation by clustering pairs of cooccurring entities represented as vectors of context features. They used a simple representation of contexts; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method f</context>
<context position="6639" citStr="Hasegawa et al., 2004" startWordPosition="960" endWordPosition="963">g initial word seeds. That method used pattern clusters to define general relations, specific to a given concept. (Davidov and Rappoport, 2008) presented an approach to discover and represent general relations present in an arbitrary corpus. That approach incorporated a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters, and merges highfrequency patterns around randomly selected concepts. The field of Unsupervised Relation Identification (URI)—the task of automatically discovering interesting relations between entities in large text corpora—was introduced by (Hasegawa et al., 2004). Relations are discovered by clustering pairs of co-occurring entities represented as vectors of context features. (Rosenfeld and Feldman, 2006) showed that the clusters discovered by URI are useful for seeding a semi-supervised relation extraction system. To compare different clustering algorithms, feature extraction and selection method, (Rosenfeld and Feldman, 2007) presented a URI system that used surface patterns of two kinds: patterns that test two entities together and patterns that test either of two entities. In this paper, we propose an unsupervised relation extraction method that c</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine and Ralph Grishman. 2004. Discovering Relations among Named Entities from Large Corpora. In Proceedings of ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic and semantic features with maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004.</booktitle>
<contexts>
<context position="2775" citStr="Kambhatla, 2004" startWordPosition="390" endWordPosition="391">occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use lingu</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic and semantic features with maximum entropy models. In Proceedings of ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dat P T Nguyen</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Relation extraction from Wikipedia using subtree mining.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI-2007.</booktitle>
<contexts>
<context position="2852" citStr="Nguyen et al., 2007" startWordPosition="400" endWordPosition="403">er, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining is abstraction away from different surface realizations of semantic relations to discover discriminative patterns efficiently. Linguistic analysis is another effective technology for semantic relation extraction, as described in many reports such as (Kambhatla, 2004); (Bunescu and Mooney, 2005); (Harabagiu et al., 2005); (Nguyen et al., 2007). Currently, linguistic approaches for semantic relation extraction are mostly supervised, relying on pre-specification of the desired relation or initial seed words or patterns from hand-coding. The common process is to generate linguistic features based on analyses of the syntactic features, dependency, or shallow semantic structure of text. Then the system is trained to identify entity pairs that assume a relation and to classify them into pre-defined relations. The advantage of these methods is that they use linguistic technologies to learn semantic information from different surface expre</context>
</contexts>
<marker>Nguyen, Matsuo, Ishizuka, 2007</marker>
<rawString>Dat P.T. Nguyen, Yutaka Matsuo and Mitsuru Ishizuka. 2007. Relation extraction from Wikipedia using subtree mining. In Proceedings of AAAI-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-2006.</booktitle>
<contexts>
<context position="1908" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="257" endWordPosition="260"> the broad range of documents, entities, and relations existing on the Web. Even with semi-supervised approaches, which use a large unlabeled corpus, manual construction of a small set of seeds known as true instances of the target entity or relation is susceptible to arbitrary human decisions. Consequently, a need exists for development of semantic information-retrieval algorithms that can operate in a manner that is as unsupervised as possible. Currently, the leading methods in unsupervised information extraction collect redundancy information from a local corpus or use the Web as a corpus (Pantel and Pennacchiotti, 2006); (Banko et al., 2007); (Bollegala et al., 2007): (Fan et al., 2008); (Davidov and Rappoport, 2008). The standard process is to scan or search the corpus to collect co-occurrences of word pairs with strings between them, and then to calculate term co-occurrence or generate surface patterns. The method is used widely. However, even when patterns are generated from well-written texts, frequent pattern mining is non-trivial because the number of unique patterns is loose, but many patterns are non-discriminative and correlated. A salient challenge and research interest for frequent pattern mining </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of ACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>URES: an Unsupervised Web Relation Extraction System. In</title>
<date>2006</date>
<booktitle>Proceedings of COLING/ACL-2006.</booktitle>
<contexts>
<context position="6784" citStr="Rosenfeld and Feldman, 2006" startWordPosition="980" endWordPosition="984">008) presented an approach to discover and represent general relations present in an arbitrary corpus. That approach incorporated a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters, and merges highfrequency patterns around randomly selected concepts. The field of Unsupervised Relation Identification (URI)—the task of automatically discovering interesting relations between entities in large text corpora—was introduced by (Hasegawa et al., 2004). Relations are discovered by clustering pairs of co-occurring entities represented as vectors of context features. (Rosenfeld and Feldman, 2006) showed that the clusters discovered by URI are useful for seeding a semi-supervised relation extraction system. To compare different clustering algorithms, feature extraction and selection method, (Rosenfeld and Feldman, 2007) presented a URI system that used surface patterns of two kinds: patterns that test two entities together and patterns that test either of two entities. In this paper, we propose an unsupervised relation extraction method that combines patterns of two types: surface patterns and dependency patterns. Surface patterns are generated from the Web corpus to provide redundancy</context>
</contexts>
<marker>Rosenfeld, Feldman, 2006</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman. 2006. URES: an Unsupervised Web Relation Extraction System. In Proceedings of COLING/ACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Clustering for Unsupervised Relation Identification.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM-2007.</booktitle>
<contexts>
<context position="7011" citStr="Rosenfeld and Feldman, 2007" startWordPosition="1013" endWordPosition="1016">s highfrequency patterns around randomly selected concepts. The field of Unsupervised Relation Identification (URI)—the task of automatically discovering interesting relations between entities in large text corpora—was introduced by (Hasegawa et al., 2004). Relations are discovered by clustering pairs of co-occurring entities represented as vectors of context features. (Rosenfeld and Feldman, 2006) showed that the clusters discovered by URI are useful for seeding a semi-supervised relation extraction system. To compare different clustering algorithms, feature extraction and selection method, (Rosenfeld and Feldman, 2007) presented a URI system that used surface patterns of two kinds: patterns that test two entities together and patterns that test either of two entities. In this paper, we propose an unsupervised relation extraction method that combines patterns of two types: surface patterns and dependency patterns. Surface patterns are generated from the Web corpus to provide redundancy information for relation extraction. In addition, to obtain semantic information for concept pairs, we generate dependency patterns to abstract away from different surface realizations of semantic relations. Dependency pattern</context>
<context position="25360" citStr="Rosenfeld and Feldman, 2007" startWordPosition="4053" endWordPosition="4056">recision and coverage. We use coverage as an evaluation instead of using recall as a measure. The coverage is used to evaluate all correctly extracted concept pairs. It is defined as the fraction of all the correctly extracted concept pairs to the whole set of concept pairs. To balance between precision and coverage of clustering, we integrate two parameters: Dl, Dg. We downloaded the Wikipedia dump as of December 3, 2008. The performance of the proposed method is evaluated using different pattern types: dependency patterns, surface patterns, and their combination. We compare our method with (Rosenfeld and Feldman, 2007)’s URI method. Their algorithm outperformed that presented in the earlier work using surface features of two kinds for unsupervised relation extraction: features that test two entities together and features that test only one entity each. For comparison, we use a k-means clustering algorithm using the same cluster number k. method Existing method Proposed method (Rosenfeld et al.) (Our method) Relation # Ins. pre # Ins. pre (sample) 5.1 Wikipedia Category: “American chief executives” We choose appropriate Dl(concept pair filter in depend clustering) and Dg(concept pair filter in surface cluste</context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman. 2007. Clustering for Unsupervised Relation Identification. In Proceedings of CIKM-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-2006.</booktitle>
<contexts>
<context position="5631" citStr="Turney, 2006" startWordPosition="815" endWordPosition="816">rmation contribute greatly to the coverage of relation extraction. • The combination of these patterns produces a clustering method to achieve high precision for different Information Extraction applications, especially for bootstrapping a high-recall semi-supervised relation extraction system. 2 Related Work (Hasegawa et al., 2004) introduced a method for discovering a relation by clustering pairs of cooccurring entities represented as vectors of context features. They used a simple representation of contexts; the features were words in sentences between the entities of the candidate pairs. (Turney, 2006) presented an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. Given a word pair, the output list of lexicon-syntactic patterns was ranked by pertinence, which showed how well each pattern expresses the relations between word pairs. (Davidov et al., 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. That method used pattern clusters to define general relations, specific to a given concept. (Davidov and Rappoport, 2008) presented an approach to discover and represent general relations pres</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Expressing implicit semantic relations without supervision. In Proceedings of ACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Volkel</author>
<author>Markus Krotzsch</author>
<author>Denny Vrandecic</author>
<author>Heiko Haller</author>
<author>Rudi Studer</author>
</authors>
<title>Semantic wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of WWW-2006.</booktitle>
<contexts>
<context position="9158" citStr="Volkel et al., 2006" startWordPosition="1343" endWordPosition="1346">resting relations among concepts, and that they provide an important source of information in ad1022 dition to the article texts. To establish the background for this paper, we start by defining the problem under consideration: relation extraction from Wikipedia. We use the encyclopedic nature of the corpus by specifically examining the relation extraction between the entitled concept (ec) and a related concept (rc), which are described in anchor text in this article. A common assumption is that, when investigating the semantics in articles such as those in Wikipedia (e.g. semantic Wikipedia (Volkel et al., 2006)), key information related to a concept described on a page p lies within the set of links l(p) on that page; particularly, it is likely that a salient semantic relation r exists between p and a related page p&apos; E l(p). Given the scenario we described along with earlier related works, the challenges we face are these: 1) enumerating all potential relation types of interest for extraction is highly problematic for corpora as large and varied as Wikipedia; 2) training data or seed data are difficult to label. Considering (Davidov and Rappoport, 2008), which describes work to get the target word a</context>
</contexts>
<marker>Volkel, Krotzsch, Vrandecic, Haller, Studer, 2006</marker>
<rawString>Max Volkel, Markus Krotzsch, Denny Vrandecic, Heiko Haller and Rudi Studer. 2006. Semantic wikipedia. In Proceedings of WWW-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed J Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGKDD-2002.</booktitle>
<contexts>
<context position="17218" citStr="Zaki, 2002" startWordPosition="2675" endWordPosition="2676">nd Mooney, 2005). The other reason is that embedded structures of the linguistic representation are important for obtaining good coverage of the pattern acquisition, as explained in (Culotta and Sorensen, 2005); (Zhang et al., 2006). The process of inducing dependency patterns has two steps. 1. Shortest dependency path inducement. From the original dependency tree structure by parsing the selected sentence for each concept pair, we first induce the shortest dependency path with the entitled concept and related concept. 2. Dependency pattern generation. We use a frequent tree-mining algorithm (Zaki, 2002) to generate sub-paths as dependency patterns from the shortest dependency path for relation clustering. 4.5 Clustering Algorithm for Relation Extraction In this subsection, we present a clustering algorithm that merges concept pairs based on dependency patterns and surface patterns. The algorithm is based on k-means clustering for relation clustering. The dependency pattern has the properties of being more accurate, but the Web context has the advantage of containing much more redundant information than Wikipedia. Our idea of concept pair clustering is a two-step clustering process: first it </context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed J. Zaki. 2002. Efficiently mining frequent trees in a forest. In Proceedings of SIGKDD-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-2006.</booktitle>
<contexts>
<context position="16839" citStr="Zhang et al., 2006" startWordPosition="2618" endWordPosition="2621">of an entitled concept or related concept are parsed into dependency structures. We define dependency patterns as sub-paths of the shortest dependency path between a concept pair for two reasons. One is that the shortest path dependency kernels outperform dependency tree kernels by offering a highly condensed representation of the information needed to assess their relation (Bunescu and Mooney, 2005). The other reason is that embedded structures of the linguistic representation are important for obtaining good coverage of the pattern acquisition, as explained in (Culotta and Sorensen, 2005); (Zhang et al., 2006). The process of inducing dependency patterns has two steps. 1. Shortest dependency path inducement. From the original dependency tree structure by parsing the selected sentence for each concept pair, we first induce the shortest dependency path with the entitled concept and related concept. 2. Dependency pattern generation. We use a frequent tree-mining algorithm (Zaki, 2002) to generate sub-paths as dependency patterns from the shortest dependency path for relation clustering. 4.5 Clustering Algorithm for Relation Extraction In this subsection, we present a clustering algorithm that merges c</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. In Proceedings of ACL-2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>