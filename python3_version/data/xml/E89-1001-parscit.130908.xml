<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.765380333333333">
Parsing Idioms in Lexicalized TAGs *
Anne Abeille and Yves Schabes
Laboratoire Automatique Documentaire et Linguistique
</note>
<affiliation confidence="0.850332666666667">
University Paris 7, 2 place Jussieu, 75005 Paris France
and Department of Computer and Information Science
University of Pennsylvania, Philadelphia PA 19104-6389 USA
</affiliation>
<email confidence="0.994801">
abeilleischabes@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.988859" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9699425">
We show how idioms can be parsed in lexical-
ized TAGs. We rely on extensive studies of frozen
phrases pursued at L.A.D.L.1 that show that id-
ioms are pervasive in natural language and obey,
generally speaking, the same morphological and
syntactical patterns as &apos;free&apos; structures. By id-
iom we mean a structure in which some items are
lexically frozen and have a semantics that is not
compositional. We thus consider idioms of differ-
ent syntactic categories : NP, S, adverbials, com-
pound prepositions... in both English and French.
In lexicalized TAGs, the same grammar is used
for idioms as for &apos;free&apos; sentences. We assign
them regular syntactic structures while represent-
ing them semantically as one non-compositional
entry. Syntactic transformations and insertion of
modifiers may thus apply to them as to any &apos;free&apos;
structures. Unlike previous approaches, their vari-
ability becomes the general case and their being
totally frozen the exception. Idioms are gener-
ally represented by extended elementary trees with
&apos;heads&apos; made out of several items (that need not
be contiguous) with one of the items serving as an
index. When an idiomatic tree is selected by this
index, lexical items are attached to some nodes in
the tree. Idiomatic trees are selected by a single
head node however the head value imposes lexical
values on other nodes in the tree. This operation
of attaching the head item of an idiom and its
lexical parts is called lexical attachment. The
resulting tree has the lexical items corresponding
to the pieces of the idiom already attached to it.
This work is partially supported (for the second au-
thor) by ARO grant DAA29-84-9-007, DARPA grant
N0014-85-K0018, NSF grants MCS-82-191169 and DCR-
84-10413. We have benefitted immensely from our discus-
sions with Aravind Joshi, Maurice Gross and Mitch Mar-
cus. We want also to thank Kathleen Bishop, and Sharon
Cote.
iLaboratoire d&apos;Automatique Documentaire et Linguis-
tique, University of Paris 7.
We generalize the parsing strategy defined for
lexicalized TAG to the case of &apos;heads&apos; made out
of several items. We propose to parse idioms in
two steps which are merged in the two steps pars-
ing strategy that is defined for &apos;free&apos; sentences.
The first step performed during the lexical pass
selects trees corresponding to the literal and id-
iomatic interpretation. However it is not always
the case that the idiomatic trees are selected as
possible candidates. We require that all basic
pieces building the minimal idiomatic expression
must be present in the input string (with possibly
some order constraints). This condition is a nec-
essary condition for the idiomatic reading but of
course it is not sufficient. The second step per-
forms the syntax analysis as in the usual case.
During the second step, idiomatic reading might
be rejected. Idioms are thus parsed as any &apos;free&apos;
sentences. Except during the selection process,
idioms do not require any special parsing mech-
anism. We are also able to account for cases of
ambiguity between idiomatic and literal interpre-
tations.
Factoring recursion from dependencies in TAGs
allows discontinuous constituents to be parsed in
an elegant way. We also show how regular &apos;trans-
formations&apos; are taken into account by the parser.
</bodyText>
<keyword confidence="0.488324">
Topics: Parsing, Idioms.
</keyword>
<sectionHeader confidence="0.92029" genericHeader="keywords">
1 Introduction to Tree Ad-
joining Grammars
</sectionHeader>
<bodyText confidence="0.999568">
Tree Adjoining Grammars (TAGs) were intro-
duced by Joshi et al. 1975 and Joshi 1985 as
a formalism for linguistic description. Their lin-
guistic relevance was shown by Kroch and Joshi
1985 and Abellle 1988. A lexicalized version of the
formalism was presented in Schabes, Abeille and
Joshi 1988 that makes them attractive for writing
computational grammars. They were proved to be
</bodyText>
<page confidence="0.975641">
1
</page>
<bodyText confidence="0.999881727272727">
parsable in polynomial time (worst case) by Vijay
Shanker and Joshi 1985 and an Earley-type parser
was presented by Schabes and Joshi 1988.
The basic component of a TAG is a finite set
of elementary trees that have two types: initial
trees or auxiliary trees (See Figure 1). Both are
minimal (but complete) linguistic structures and
have at least one terminal at their frontier (that is
their &apos;head). Auxiliary trees are also constrained
to have exactly one leaf node labeled with a non-
terminal of the same category as their root node.
</bodyText>
<figure confidence="0.744550333333333">
Initial tree: Auxiliary trem
terminals or
substitution nodes
</figure>
<figureCaption confidence="0.999674">
Figure 1: Schematic initial and auxiliary trees
</figureCaption>
<bodyText confidence="0.999814375">
Sentences of the language of a TAG are derived
from the composition of an S-rooted initial tree
with elementary trees by two operations: substi-
tution or adjunction.
Substitution inserts an initial tree (or a tree de-
rived from an initial tree) at a leaf node bearing
the same label in an elementary tree (See Fig-
ure 2).2 It is the operation used by CFGs.
</bodyText>
<figureCaption confidence="0.998242">
Figure 2: Mechanism of substitution
</figureCaption>
<bodyText confidence="0.999911666666667">
Adjunction is a more powerful operation: it in-
serts an auxiliary tree at one of the corresponding
node of an elementary tree (See Figure 3).3
TAGs are more powerful than CFGs but only
mildly so (Joshi 1983). Most of the linguistic ad-
vantages of the formalism come from the fact that
it factors recursion from dependencies. Kroch and
Joshi 1985 show how unbounded dependencies can
be &apos;localized&apos; by having filler and gap as part of
</bodyText>
<footnote confidence="0.9602824">
2j is the mark for substitution.
3At each node of an elementary tree, there is a feature
structure associated with it (Vijayshanker and Joshi, 1988).
Adjunction constraints can be defined in terms of feature
structures and the success or failure of unification.
</footnote>
<figureCaption confidence="0.999019">
Figure 3: Adjoining
</figureCaption>
<bodyText confidence="0.999446615384616">
the same elementary tree and having insertion of
matrix clauses provided by recursive adjunctions.
Another interesting property of the formalism is
its extended domain of locality, as compared to
that of usual phrase structure rules in CFG. This
was used by Abeille 1988 to account for the prop-
erties of &apos;light&apos; verb (often called &apos;support&apos; verb for
Romance languages) constructions with only one
basic structure (instead of the double analysis or
reanalysis usually proposed).
We now define by an example the notion of
derivation in a TAG.
Take for example the derived tree in Figure 4.
</bodyText>
<figure confidence="0.995366090909091">
S
....../.%
Ad S
1 VN
yesterday NP VP
AA
D N V NP
1 1 1 1
a man saw N
1
Mary
</figure>
<figureCaption confidence="0.912594">
Figure 4: Derived tree for: yesterday a man saw
Mary
It has been built with the elementary trees in
Figure 5.
Figure 5: Some elementary trees
</figureCaption>
<figure confidence="0.956934083333333">
Unlike CFGs, from the tree obtained by deriva-
(7)
(a)
(3)
S S NI
A A I
Ad S NP NP01. VP N
I A A I
yesterday D DI N V NPII aNPn[i
13adS[yesterday] I I I
a man saw
aD[a] aNPdn[man] atnl[saw]
</figure>
<page confidence="0.997637">
2
</page>
<bodyText confidence="0.999578052631579">
tion (called the derived tree) it is not always pos-
sible to know how it was constructed. The deriva-
tion tree is an object that specifies uniquely how
a derived tree was constructed.
The root of the derivation tree is labeled by an
S-type initial tree. All other nodes in the deriva-
tion tree are labeled by auxiliary trees in the case
of adjunction or initial trees in the case of sub-
stitution. A tree address is associated with each
node (except the root node) in the derivation tree.
This tree address is the address of the node in the
parent tree to which the adjunction or substitu-
tion has been performed. We use the following
convention: trees that are adjoined to their par-
ent tree are linked by an unbroken line to their
parent, and trees that are substituted are linked
by dashed lines.
The derivation tree in Figure 6 specifies how the
derived tree was obtained:
</bodyText>
<equation confidence="0.842421">
canl[saw]
aNPdn[man] (1) aNPn[Mary] (2.2) DadS[yesterday] (0)
aD[8] (1)
</equation>
<figureCaption confidence="0.7274115">
Figure 6: Derivation tree for Yesterday a man saw
Mary
</figureCaption>
<bodyText confidence="0.881409421052632">
aD[a] is substituted in the tree aNPdn[man] at
node of address 1, aNPdn[man] is substituted in
the tree atnl[saw] at address 1, aNPn[Mary] is
substituted in the tree ottnl[saw] at node 2.2 and
the tree fladS[yesterdayj is adjoined in the tree
atnl[satd at node 0.
In a `lexicalized&apos; TAG, the &apos;category&apos; of each
word in the lexicon is in fact the tree structure(s)
it selects.4 Elementary trees that can be linked by
a syntactic or a lexical rule are gathered in a Tree
Family, that is selected as a whole by the head
of the structure. A novel parsing strategy follows
(Schabes, Abeille, Joshi 1988). In a first step, the
parser scans the input string and selects the dif-
ferent tree structures associated with the lexical
items of the string by looking up the lexicon. In
a second step, these structures are combined to-
gether to produce a sentence. Thus the parser uses
only a subset of the entire (lexicalized) grammar.
</bodyText>
<footnote confidence="0.5639805">
4 The nodes of the tree structures have feature structures
associated with them, see footnote 3.
</footnote>
<sectionHeader confidence="0.752636" genericHeader="method">
2 Linguistic Properties of Id-
ioms
</sectionHeader>
<bodyText confidence="0.999670333333333">
Idioms have been at stake in many linguistic dis-
cussions since the early transformational gram-
mars, but no exhaustive work based on exten-
sive listings of idioms have been pursued before
Gross 1982. We rely on L.A.D.L.&apos;s work for French
that studied 8000 frozen sentences, 20, 000 frozen
nouns and 6000 frozen adverbs. For English, we
made use of Freckelton&apos;s thesis (1984) that listed
more than 3000 sentential idioms. They show
that, for a given structure, idiomatic phrases are
usually more numerous in the language than &apos;free&apos;
ones. As is well known, idioms are made of the
same lexicon and consist of the same sequences of
categories as &apos;free&apos; structures. An interesting ex-
ception is the case of &apos;words&apos; existing only as part
of an idiomatic phrase, such as escampette in pren-
dre la poudre d&apos;escampette (to leave furtively) or
umbrage in to take umbrage at NP.
The specificity of idioms is their semantic non-
compositionality. The meaning of casser so pipe
(to die), cannot be derived from that of casser (to
break) and that of pipe (pipe). They behave se-
mantically as one predicate, and for example the
whole VP casser sa pipe selects the subject of the
sentence and all possible modifiers. We therefore
consider an idiom as one entity in the lexicon.
It would not make sense to have its parts listed in
the lexicon as regular categories and to have spe-
cial rules to limit their distribution to this unique
context. If they are already listed in the lexi-
con, these existing entries are considered as mere
homonyms. Furthermore, usually idioms are am-
biguous between literal and idiomatic read-
ings.
Idioms do not appear necessarily as con-
tinuous strings in texts. As shown by M. Gross
for French and P. Freckelton for English, more
than 15% of sentential idioms are made up of un-
bounded arguments, (e.g. NA) prendre NPi en
compte, NPo take NP1 into account, Butter would
not melt in NP&apos;s mouth). Discontinuities can also
come from the regular application of syntactic
rules. For example, interposition of adverbs be-
tween verb and object in compound V-NP phrases,
and interposition of modals or auxiliaries between
subject and verb in compound NP-V phrases are
very general (Laporte 1988).
As shown by Gazdar et al. 1985 for English,
and Gross 1982 for French, most sentential id-
ioms are not completely frozen and &apos;transfor-
mations&apos; apply to them much more regularly
</bodyText>
<page confidence="0.99496">
3
</page>
<bodyText confidence="0.999677555555555">
than is usually thought. Freckelton 1984&apos;s list-
ings of idiomatic sentences exhibit passivization
for about 50% of the idioms comprised of a verb
(different from be and have) and a frozen direct
argument. Looking at a representative sample of
2000 idiomatic sentences with frozen objects (from
Gross&apos;s listings at LADL) yields similar results for
passivization and relativization of the frozen argu-
ment for French. This is usually considered a prob-
lem for parsing, since the order in which the frozen
elements of an idiom appear might thus vary.
Recognizing idioms is thus dependent on the
whole syntactic analysis and it is not realistic to
reanalyze them as simple categories in a prepro-
cessing step.
corresponding tree must be expanded up to the
D1 and N1 level. the (resp. bucket) is directly
attached to the D1 (resp. N1) node (See Figure 8).
</bodyText>
<figure confidence="0.997961375">
S
A
&apos;,poi VP
V NPI
I /\
kicked DI N1
I I
the bucket
</figure>
<figureCaption confidence="0.999665">
Figure 8: Tree for N130 kicked the bucket
</figureCaption>
<sectionHeader confidence="0.6618305" genericHeader="method">
3 Representing Idioms in
Lexicalized TAGs
</sectionHeader>
<bodyText confidence="0.999953555555556">
We represent idioms with the same elementary
trees as &apos;free&apos; structures. The values of the argu-
ments of trees that correspond to a literal expres-
sion are introduced via syntactic categories and
semantic features. However, the values of argu-
ments of trees that correspond to an idiomatic
expression are not only introduced via syntactic
categories and semantic features but also directly
specified.
</bodyText>
<subsectionHeader confidence="0.999356">
3.1 Extended Elementary Trees
</subsectionHeader>
<bodyText confidence="0.99830175">
Some idioms select the same elementary tree struc-
tures as &apos;free&apos; sentences. For example, a sentential
idiom with a frozen subject il faut Si selects the
same tree family as any verb taking a sentential
complement (ex: NP0 dit Si), except that il is
directly attached in subject position, whereas a
&apos;free&apos; NP is inserted in NP0 in the case of `dit&apos;
(See Figure 7).
</bodyText>
<figure confidence="0.88796725">
NP0 VP NPol VP
IA A
v si v s,
taut dit
</figure>
<figureCaption confidence="0.99938">
Figure 7: trees for il faut and dit
</figureCaption>
<bodyText confidence="0.999089333333333">
Usually idioms require elementary trees that are
more expanded. Take now as another example
the sentential idiom NP0 kicked the bucket. The
</bodyText>
<subsectionHeader confidence="0.990699">
3.2 Multicomponent Heads
</subsectionHeader>
<bodyText confidence="0.983254111111111">
In the lexicon, idiomatic trees are represented by
specifying the elements of the idiom. An idiom
as NP0 kicked the bucket is indexed by a &apos;head&apos;
(kicked) which specifies the other pieces of the id-
iom. Although the idiom is indexed by one item
the pieces are considered as its multicomponent
heads.5
We have, among others, the following entries in
the lexicon:6
</bodyText>
<figure confidence="0.767666730769231">
kicked ,V
kicked ,V
the ,D
bucket , N
John ,N
: Tn1 (transitive verb)
: Tdnl[Di = the, N1 = bucket] (idiom)
: aD
: aNPdn
: aNP
The trees aNPdn and aNPn are:7
NP NP
(aNPn) A (aNPdn)
NO DI NO
Among other trees, the tree atnl is in the family
Tnl and the tree atdnl is in the family Tdnl:
A
NP04. VP (atnl)
A
vo
A
NPo.i. VP
A
vo NPI (atdn1)
/\
D11 Nil
</figure>
<footnote confidence="0.9264372">
5 The choice of the item under which the idiom is indexe
is most of the time arbitrary.
6The lexical entries are simplified to just illustrate hos
idiom are handled.
o marks the node under which the head is attached.
</footnote>
<page confidence="0.94455">
4
</page>
<figure confidence="0.997478">
NP NP
A
N
John the bucket
(aNPn[John]) (a D[the]) (aNPdn[bucket])
A
NPeri, VP
NP04. VP V NPi
I /\
kicked DI N1
kicked the bucket
(atn1 [kicked]) (a tdn1 [kicked-the-bucket])
</figure>
<figureCaption confidence="0.998326">
Figure 9: &apos;Drees selected for the input
</figureCaption>
<bodyText confidence="0.98431844">
John kicked the bucket
Suppose that the input sentence is John kicked
the bucket. The first entry for kicked (a) speci-
fies that kicked can be attached under the V node
in the tree atdnl (See the tree atnl[kicked] in
Figure 9). However the second entry for kicked
(b) specifies that kicked can be attached under
the V node and that the must be attached un-
der the node labeled by D1 and that bucket must
be attached under the node labeled N1 in the
tree atnl (See the tree atdnl[kicked-the-bucket]
in Figure 9).
In the first pass, the trees in Figure 9 are be
selected (among others).
Some idioms allow some lexical variation, usu-
ally between a more familiar and a regular use of
the same idiom, for example in French NP0 per-
dre Ia tete and NP0 perdre la bottle (to get mad).
This is represented by allowing disjunction on the
string that gets directly attached at a certain posi-
tion in the idiomatic tree. NP0 perdre la tete/boule
will thus be one entry in the lexicon, and we do
not have to specify that tete and boule are synony-
mous (and restrict this synonymy to hold only for
this context).
</bodyText>
<subsectionHeader confidence="0.999967">
3.3 Selection of Idiomatic Trees
</subsectionHeader>
<bodyText confidence="0.986139741935484">
We now explain how the first pass of the parser
is modified to select the appropriate possible can-
didates for idiomatic readings. Take the previ-
ous example, John kicked the bucket. The verb
kicked will select the tree atdnl[kicked-the-bucket]
for an idiomatic reading. However, the values of
the determiner and the noun of the object noun
phrase are imposed to be respectively the and
bucket. The determiner and the noun are at-
tached to the tree atdnl[kicked-the-bucket], how-
ever the tree atdnl[kicked-the-bucket] is selected
if the words kicked, the and bucket appear in the
input string at position compatible with the tree
atdnl[kicked-the-bucket]. Therefore they must re-
spectively appear in the input string at some po-
sition i, j and k such that i &lt;j &lt; k. If it is not
the case, the tree atdnl[kicked-the-bucket] is not
selected. This process is called lexical attach-
ment.
For example the word kicked in the fol-
lowing sentences will select the idiomatic tree
atdnl[kicked-the-bucket]:
John kicked the bucket (s
John kicked the proverbial bucket (s2)
John kicked the man who was
carrying the bucket (s3)
The parser will accept sentences s/ and s2 as id-
iomatic reading but not the sentence s3 since the
tree atdnl[kicked-the-bucket] will fail in the parse.
In the following sentence the word kicked will not
select the idiomatic tree atdnl[kicked-the-bucket]:
</bodyText>
<figure confidence="0.869179">
John kicked Mark (s4)
John kicked a bucket (s5)
John who was carrying a bucket
kicked the child (s6)
What did John kick? (s7)
</figure>
<bodyText confidence="0.999922045454545">
This test cuts down the number of idiomatic
trees that are given to the parser as possible can-
didates. Thus a lot of idioms are ruled out before
starting the syntactic analysis because we know
all the lexical items at the end of the first pass.
This is important because a given item (e.g. a
verb) can be the head of a large number of idioms
(Gross 82 has listed more than 50 of them for the
verb manger, and prendre or avoir yield thousands
of them). However, as sentence s3 illustrates, the
test is not sufficient.
What TAGs allow us to do is to define mul-
ticomponent heads for idiomatic structures with-
out requiring their being contiguous in the input
string. The formalism also allows us to access
directly the different elements of the compound
without flattening the structure. As opposed to
CFGs, for example, direct dependencies can be
expressed between arguments that are at differ-
ent levels of depth in the tree without having to
pass features across local domains. For example,
in NP0 vider DET sac (to express all of one&apos;s se-
</bodyText>
<page confidence="0.984999">
5
</page>
<bodyText confidence="0.996904125">
cret thoughts), the determiner of the object sac
has to be a possessive and agree in person with
the subject : je vide mon sac, is vides ton sac...
In NP0 dire DET quatre verites a NP2 (to tell
someone what he really is), the determiner of the
object verites has to be a possessive and agree in
person with the second object NP2 : je te dis tes
quatre verites, je lui dis ses quatre verites.
</bodyText>
<sectionHeader confidence="0.991949" genericHeader="method">
4 Literal and Idiomatic
Readings
</sectionHeader>
<bodyText confidence="0.984302615384615">
Our representation expresses correctly that id-
ioms are semantically non-compositional. Trees
obtained by lexical attachment of several lexical
items act as one syntactic unit and also one se-
mantic unit.
For example, the sentence John kicked the
bucket can be parsed in two different ways. One
derivation is built with the trees: atnl[kicked]
(transitive verb), aNPn[Johnj, aD[the] and
aNPn[bucket] . It corresponds to the literal in-
terpretation; the other derivation is built with the
trees: atdnl[kicked-the-bucket] (idiomatic tree)
and aNPn[John] (John):
</bodyText>
<figure confidence="0.805798">
atnl[kicked]
aNPn[john] (1) a.NPdn[bucket] (2.2) atdnl[kicket-the-bucket]
•
aD[the] (1) aNPn[John] (1)
literal derivation idiomatic derivation
However, both derivations have the same de-
rived tree:
Sr
NP VP
I ZX
N V NP
John kicked D N
I I
the bucket
</figure>
<bodyText confidence="0.998905163934427">
The meaning of kicked the bucket in its idiomatic
reading cannot be derived from that of kicked and
the bucket. However, by allowing arguments to be
inserted by substitution or adjunction (in for ex-
ample atdnl[kicked-the-bucket]), we represent the
fact that N.130 kicked the bucket acts as a syntactic
and semantic unit expecting one argument NP0.
Similarly, NP0 kicked N Pi in cttnl[kicked] acts as
a syntactic and semantic unit expecting two argu-
ments NP0 and NPi. This fact is reflected in the
two derivation trees of John kicked the bucket.
However, the sentential idiom `il faut S1&apos;, is not
parsed as ambiguous, since faut has only one en-
try (that is idiomatic) in the lexicon. When a
certain item does not exist except in a specific
idiom, for example umbrage in English, the cor-
responding idiom to take umbrage of NP will not
be parsed as ambiguous. The same holds when
a item selects a construction only in an idiomatic
expression. Aller, for example, takes an obligatory
PP (or adverbial) argument in its non-idiomatic
sense. Thus the idiom:
aller son train (to follow one&apos;s way)
is not parsed as ambiguous since there is no free
NP0 aller NP1 structure in the lexicon.
We also have ambiguities for compound nom-
inals such as carte bleue, meaning either credil
card (idiomatic) or blue card (literal), and for com-
pound adverbials like on a dime: John stopped or
a dime will mean either that he stopped in a con-
trolled way or on a 10 cent coin.
Structures for literal and idiomatic readings are
both selected by the parser in the first step. Since
syntax and semantics are processed at the sam(
time, the sentence is analyzed as ambiguous be-
tween literal and idiomatic interpretations. Th(
derived trees are the same but the derivation tree:
are different. For example, the adjective bleue se-
lects an auxiliary tree that is adjoined to carte ir
the literal derivation tree, whereas it is direct13
attached in a complex initial tree in the case o
idiomatic interpretation.
All frozen elements of the idiom are direct13
attached in the corresponding elementary trees
and do not have to exist in the lexicon. The3
are thus distinguished from &apos;free&apos; arguments that
select their own trees (and their own semantics&apos;
to be substituted in a standard sentential tree
Therefore we distinguish two kinds of semantic op
erations: substitution (or adjunction) correspondt
to a compositional semantics; direct attachment
on the other hand, makes different items behavo
as one semantic unit.
One should notice that non-idiomatic reading
are not necessarily literal readings. Since featurl
structures are used for selectional restrictions o
arguments, metaphoric readings can be taken int&lt;
account (Bishop, Cote and Abeille 1989).
We are able to handle different kinds of seman
tic non-compositionality, and we do not treat a
idiomatic all cases of non-literal readings.
</bodyText>
<page confidence="0.972796">
6
</page>
<figure confidence="0.999353035714286">
NP0 VP
No V NP1
Jean
A
DI N1
I I
sa pipe
Aux V
I I
a casse
literal
A
NP0!. VP
71
V NPvi. PP2NA
A
takes P2 NP2N A
1 1
into N2NA
account
NP0 VP
I VN
No V NEVA
IAA
Jean Aux V DI NINA
1111
a casse sa pipe
idiom
</figure>
<figureCaption confidence="0.987978">
Figure 10: Tree for NP0 takes NP1 into account
</figureCaption>
<sectionHeader confidence="0.993952" genericHeader="method">
5 Recognizing
</sectionHeader>
<subsectionHeader confidence="0.910926">
Discontinuous Idioms
</subsectionHeader>
<bodyText confidence="0.999939555555555">
Parsing flexible idioms has received only partial
solutions so far (Stock 1987, Laporte 1988). Since
TAGs factor recursion from dependencies, discon-
tinuities are captured straightforwardly without
special devices (as opposed to Johnson 1985 or
Bunt et al. 1987). We distinguish two kinds of dis-
continuities: discontinuities that come from inter-
nal structures and discontinuities that come from
the insertion of modifiers.
</bodyText>
<subsectionHeader confidence="0.984685">
5.1 Internal Discontinuities
</subsectionHeader>
<bodyText confidence="0.999868625">
Some idioms are internally discontinuous. Take for
example the idioms NP0 prendre NP1 en compte
and NP0 takes NP1 into account (see Figure 10).8
The discontinuity is handled simply by argu-
ments (here NP0 and NP1) to be substituted
(or adjoined in some cases) as any free sentences.
The internal structures of arguments can be un-
bounded.
</bodyText>
<subsectionHeader confidence="0.9939655">
5.2 Recursive Insertions of Modi-
fiers
</subsectionHeader>
<bodyText confidence="0.999776444444444">
Some adjunctions of modifiers may be ruled out
in idioms or some new ones may be valid only
in idioms. If the sentence is possibly ambiguous
between idiomatic and literal reading, the adjunc-
tion of such modifiers force the literal interpre-
tation. For example, in N130 casser sa pipe (to
die) , the NP1 node in the idiomatic tree bears a
null adjunction constraint (NA). The sentence Il a
casse sa pipe en bois (he broke his wooden pipe) is
</bodyText>
<footnote confidence="0.7438215">
8 NA expresses the fact that the node has null adjunction
constraint
</footnote>
<figureCaption confidence="0.964253">
Figure 11: Jean a casse sa pipe
</figureCaption>
<bodyText confidence="0.992275666666667">
then parsed as non-idiomatic. This NA constraint
will be the only difference between the two derived
trees (See Figure 11): Jean a casse sa pipe (literal)
and Jean a casse sa pipe (idiomatic).
But most idioms allow modifiers to be inserted
in them. Each modifier can be unbounded (e.g.
with embedded adjunct clauses) and their inser-
tion is recursive. We treat these insertion by ad-
junction of modifiers in the idiomatic tree. How-
ever constraint of adjunction and feature structure
constraints filter out partially or totally the inser-
tion of modifiers at each node of an idiomatic tree.
In a TAG, the internal structure of idioms is spec-
ified in terms of a tree, and we can get a unified
representation for such compound adverbials as a
la limite and a reztreme limite (if there is no other
way) or such complex determiners as a bunch of
(or la majorite de NP) and a whole bunch of NP
(resp. la grande majorite de NP) that will not have
to be listed as separate entries in the lexicon. The
adjective whole (resp. grande) adjoins to the noun
bunch (resp. majorite ), as to any noun. Take a
bunch of NP. The adjective whole adjoins to the
noun bunch as to any noun (See Figure 12) and
builds a whole bunch of.
In order to have a modifier with the right fea-
tures adjoining at a certain node in the idiom, we
associate some features with the head of the id-
iom (as for heads of &apos;free&apos; structures) but also with
elements of the idiom that are directly attached.
Unification equations, such as those constraining
agreement, are the same for trees selected by id-
ioms and trees selected by &apos;free&apos; structures. Thus
only grande that is feminine singular, and not
grand for example, can adjoin to majorite that
is feminine singular. In il falloir NP, the frozen
subject il is marked 3rd person singular, and only
an auxiliary like va (that is 3rd person singular)
and not vont (3rd person plural) will be allowed
</bodyText>
<page confidence="0.99731">
7
</page>
<figure confidence="0.567856583333333">
NP AN Therefore parsing flexible idioms is reduced to
D N PP whole the general parsing of TAGs (Schabes and Joshi
I I A 1988).
a bunch P NP 6 Tree Families and Appli-
of cation of &apos;Transformations&apos;
to Idioms
NP
D N PP
by adjunction: I /.\ A
aA N PNP
1 1 1
whole bunch of
</figure>
<figureCaption confidence="0.976085">
Figure 12: Trees for a whole bunch of
</figureCaption>
<bodyText confidence="0.994122178082192">
to adjoin to the VP: il va falloir S1 and not il vont
falloir S1.
As another example, an idiom such as /a
moutarde monte au nez de NP (NP looses his tem-
per) can be represented as contiguous in the ele-
mentary tree. Adjunction takes place at any inter-
nal node without breaking the semantic unity of
the idiom. For example, an adjunct clause headed
by aussitot can adjoin between the frozen subject
and the rest of the the idiom in /a moutarde mon-
ter au nez de NP2 : la moutarde, aussitot que
Marie entrap monta au nez de Max (Max, as soon
as Marie got in, lost his temper). Similarly, aux-
iliaries adjoin between frozen subjects and verbs
as they do to &apos;free&apos; VPs: There might have been
a box on the table is parsed as being derived from
the idiom: there be NPi P NP2.
It should be noted that when a modifier adjoins
to an interior node of an idiom, there is a semantic
composition between the semantics of the modi-
fier and that of the idiom as a whole, no matter
at which interior node the adjunction takes place.
For example, in John kicked the proverbial bucket
semantic composition happens between the 3 units
John, kick-the-bucket, and proverbia1.9 Semantic
composition will be done the same way if an ad-
junct clause were adjoined into the VP. In John
kicked the bucket, as the proverb says, composi-
tion will happen between John, kick-the-bucket,
and the adjunct clause considered as one predi-
cate as-proverb-say:
As in the case of predicates in lexicalized TAGs,
sentential idioms are represented as selecting a set
of elementary trees and not only one tree. These
tree families gather all elementary trees that are
possible syntactic realizations of a given argument
structure. The family for transitive verbs, for ex-
ample, is comprised of trees for wh-question on the
subject, wh-question on the object, relativization
on the subject, relativization on the object, and so
on. In the first pass, the parser loads all the trees
in the tree family corresponding to an item in the
input string (unless certain trees in that family do
not match with the feature of the head in the input
string).
The same tree families are used with idioms.
However some trees in a family might be ruled
out by an idiom if it does not satisfy one of the
three following requirements.
First, the tree must have slots in which the
pieces of the idiom can be attached.1° If one
distinguishes syntactic rules that keep the lexical
value of an argument in a sentence (e.g. topical-
ization, cleft extraction, relativization...), and syn-
tactic rules that do not (deleting the node for that
argument, or replacing it by a pronoun or a wh-
element; e.g.: wh-question, pronominalization), it
can be shown that usually only the former applies
to frozen elements of an idiom. If you take the id-
iom bruler an feu (to run a (red) light), relativiza-
tion and cleft extraction, but not wh-question, are
possible on the noun feu, with the idiomatic read-
ing:
Le feu que Jean a brule.
C&apos;est an feu que Jean a brule.
* Que brule Jean ?
Second, if all the pieces of an idiom can be at-
tached in a tree, the order imposed by the tree
must match with the order in which the pieces ap-
pear in the input string. Thus, if enfant appears
before attendre in the input string, the hypothe-
sis for an idiomatic reading will be made but only
the trees corresponding to relativization, cleft ex-
</bodyText>
<footnote confidence="0.802081">
9This is the case of a modifier where adjoining is valid
only for the idiom. &amp;quot;This requirement is independent of the input string.
</footnote>
<page confidence="0.975288">
8
</page>
<bodyText confidence="0.999751142857143">
traction, topicalization in which enfant is required
to appear before attendre will be selected. But if
the string enfant is not present at all in the input
string, the idiomatic reading will not be hypoth-
esized, and trees corresponding to qui attend-elle
will never be selected as part of the family of the
idiom attendre un enfant.
Third, the features of the heads of an idiom
must unify with those imposed on the tree (as
for &apos;free&apos; sentences). For example, it has to be
specified that bucket in to kick the bucket does not
undergo relativization nor passivization, whereas
tabs in to keep tabs on NP does. It is well known
that even for &apos;free&apos; sentences application of the
passive, for example, has somehow to be speci-
fied for each transitive verbs since there are lexical
idiosyncrasies.11 The semantics of the passive tabs
were kept on NP by NP is exactly the same as that
of the active NP keep tabs on NP, since different
trees in the same tree families are considered as
(semantically) synonymous.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99999705">
We have shown how idioms can be processed in
lexicalized TAGs. We can access simultaneously
frozen elements at different levels of depths where
CFGs would either have to flatten the idiomatic
structure (and lose the possibility of regular in-
sertion of modifiers) or to use specific devices to
check the presence of an idiom. We can also put
sentential idioms in the same grammar as free
sentences. The two pass parsing strategy we use
combining with an operation of direct attachment
of lexical items in idiomatic trees, enables us to
cut down the number of idiomatic trees that the
parser takes as possible candidates. We easily get
possibly idiomatic and literal reading for a given
sentence. The only distinctive property of idioms
is the non-compositional semantics of their frozen
constituents. The extended domain of locality of
TAGs allows the two problems of internal discon-
tinuity and of unbounded interpositions to be han-
dled in a nice way.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708783333333">
Abellle, Anne, 1988. Parsing French with Tree Adjoining
Grammar: some Linguistic Accounts. In Proceedings of the
12th International Conference on Computational Linguis-
tics (Coling &apos;88). Budapest.
&amp;quot;Unless one thinks that some regularity might show up
if one distinguishes different kinds of direct complements
with thematic roles.
Bishop, Kathleen M.; Cote, Sharon; and Abeille, Anne,
1989. A Lexicalized Tree Adjoining Grammar for English.
Technical Report, Department of Computer and Informa-
tion Science, University of Pennsylvania.
Bunt, et al., 1987. Discontinuous Constituents in Trees,
Rules and Parsing. In Proceedings of European Chapter of
the A CL &apos;87. Copenhagen.
Frecicelton, P., 1984. Une Etude Comparative des Expres-
sions Idiomatiques de l&apos;Anglais et du Francais. PhD thesis,
These de troisieme cycle, University Paris 7.
Gazdar, G.; Klein, E.; Pulhun, G. K.; and Sag, L A.,
1985. Generalized Phrase Structure Grammars. Blackwell
Publishing, Oxford. Also published by Harvard University
Press, Cambridge, MA.
Gross, Maurice, 1982. Classification des phrases figees en
Francais. Revue Quebecoise de Linguistique 11(2).
Johnson, M., 1985. Parsing with discontinuous elements.
In Proceedings of the 23rd A CL meeting. Chicago.
Joshi, Aravind K., 1985. How Much Context-Sensitivity
is Necessary for Characterizing Structural Descriptions—
Tree Adjoining Granunars. In Dowty, D.; Karttunen, L.;
and Zwicky, A. (editors), Natural Language Processing—
Theoretical, Computational and Psychological Perspec-
tives. Cambridge University Press, New York. Originally
presented in a Workshop on Natural Language Parsing at
Ohio State University, Columbus, Ohio, May 1983.
Joshi, A. K.; Levy, L. S.; and Takahashi, M., 1975. Tree
Adjunct Grammars. J. Comput. Syst. Sci. 10(1).
Kroch, A. and Joshi, A. K., 1985. Linguistic Relevance
of Tree Adjoining Grammars. Technical Report MS-CIS-
85-18, Department of Computer and Information Science,
University of Pennsylvania.
Laporte, E., 1988. Reconnaissance des expressions figees
lors de l&apos;analyse autornatique. Langages Larousse, Paris.
Schabes, Yves and Joshi, Aravind K., 1988. An Earley-
Type Parsing Algorithm for Tree Adjoining Grammars. In
26th Meeting of the Association for Computational Lin-
guistics. Buffalo.
Sdiabes, Yves; Abeille, Anne; and Joshi, Aravind K., 1988.
Parsing Strategies with `Lexicalized&apos; Grammars: Applica-
tion to &apos;five Adjoining Grammars. In Proceedings of the
12th International Conference on Computational Linguis-
tics.
Stock, 0., 1987. Getting Idioms in a Lexicon Based
Parser&apos;s Head. In Proceedings of ACL&apos;87. Stanford.
Vijay-Shanker, K. and Joshi, A. K., 1985. Some Compu-
tational Properties of Tree Adjoining Grammars. In 23&apos;d
Meeting of the Association for Computational Linguistics,
pages 82-93.
Vijay-Shanker, K. and Joshi, A.K., 1988. Feature Struc-
ture Based Tree Adjoining Grammars. In Proceedings of
the 12th International Conference on Computational Lin-
guistics (Coling &apos;88). Budapest.
</reference>
<page confidence="0.997106">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.227207">
<title confidence="0.999482">Parsing Idioms in Lexicalized TAGs *</title>
<author confidence="0.981478">Anne Abeille</author>
<author confidence="0.981478">Yves Schabes</author>
<affiliation confidence="0.911977">Laboratoire Automatique Documentaire et Linguistique University Paris 7, 2 place Jussieu, 75005 Paris France and Department of Computer and Information Science University of Pennsylvania, Philadelphia PA 19104-6389 USA</affiliation>
<email confidence="0.998959">abeilleischabes@linc.cis.upenn.edu</email>
<abstract confidence="0.99987293939394">We show how idioms can be parsed in lexicalized TAGs. We rely on extensive studies of frozen pursued at that show that idioms are pervasive in natural language and obey, generally speaking, the same morphological and syntactical patterns as &apos;free&apos; structures. By idiom we mean a structure in which some items are lexically frozen and have a semantics that is not compositional. We thus consider idioms of different syntactic categories : NP, S, adverbials, compound prepositions... in both English and French. In lexicalized TAGs, the same grammar is used for idioms as for &apos;free&apos; sentences. We assign them regular syntactic structures while representing them semantically as one non-compositional entry. Syntactic transformations and insertion of modifiers may thus apply to them as to any &apos;free&apos; structures. Unlike previous approaches, their variability becomes the general case and their being totally frozen the exception. Idioms are generally represented by extended elementary trees with &apos;heads&apos; made out of several items (that need not be contiguous) with one of the items serving as an index. When an idiomatic tree is selected by this index, lexical items are attached to some nodes in the tree. Idiomatic trees are selected by a single head node however the head value imposes lexical values on other nodes in the tree. This operation of attaching the head item of an idiom and its parts is called attachment. resulting tree has the lexical items corresponding to the pieces of the idiom already attached to it.</abstract>
<note confidence="0.739060666666667">This work is partially supported (for the second author) by ARO grant DAA29-84-9-007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169 and DCR-</note>
<abstract confidence="0.977462878787879">84-10413. We have benefitted immensely from our discussions with Aravind Joshi, Maurice Gross and Mitch Marcus. We want also to thank Kathleen Bishop, and Sharon Cote. iLaboratoire d&apos;Automatique Documentaire et Linguistique, University of Paris 7. We generalize the parsing strategy defined for lexicalized TAG to the case of &apos;heads&apos; made out of several items. We propose to parse idioms in two steps which are merged in the two steps parsing strategy that is defined for &apos;free&apos; sentences. The first step performed during the lexical pass selects trees corresponding to the literal and idiomatic interpretation. However it is not always the case that the idiomatic trees are selected as possible candidates. We require that all basic pieces building the minimal idiomatic expression must be present in the input string (with possibly some order constraints). This condition is a necessary condition for the idiomatic reading but of course it is not sufficient. The second step performs the syntax analysis as in the usual case. During the second step, idiomatic reading might be rejected. Idioms are thus parsed as any &apos;free&apos; sentences. Except during the selection process, idioms do not require any special parsing mechanism. We are also able to account for cases of ambiguity between idiomatic and literal interpretations. Factoring recursion from dependencies in TAGs allows discontinuous constituents to be parsed in an elegant way. We also show how regular &apos;transformations&apos; are taken into account by the parser.</abstract>
<intro confidence="0.831261">Idioms.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abellle</author>
</authors>
<title>Parsing French with Tree Adjoining Grammar: some Linguistic Accounts.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (Coling &apos;88).</booktitle>
<location>Budapest.</location>
<contexts>
<context position="3824" citStr="Abellle 1988" startWordPosition="609" endWordPosition="610">ioms do not require any special parsing mechanism. We are also able to account for cases of ambiguity between idiomatic and literal interpretations. Factoring recursion from dependencies in TAGs allows discontinuous constituents to be parsed in an elegant way. We also show how regular &apos;transformations&apos; are taken into account by the parser. Topics: Parsing, Idioms. 1 Introduction to Tree Adjoining Grammars Tree Adjoining Grammars (TAGs) were introduced by Joshi et al. 1975 and Joshi 1985 as a formalism for linguistic description. Their linguistic relevance was shown by Kroch and Joshi 1985 and Abellle 1988. A lexicalized version of the formalism was presented in Schabes, Abeille and Joshi 1988 that makes them attractive for writing computational grammars. They were proved to be 1 parsable in polynomial time (worst case) by Vijay Shanker and Joshi 1985 and an Earley-type parser was presented by Schabes and Joshi 1988. The basic component of a TAG is a finite set of elementary trees that have two types: initial trees or auxiliary trees (See Figure 1). Both are minimal (but complete) linguistic structures and have at least one terminal at their frontier (that is their &apos;head). Auxiliary trees are a</context>
</contexts>
<marker>Abellle, 1988</marker>
<rawString>Abellle, Anne, 1988. Parsing French with Tree Adjoining Grammar: some Linguistic Accounts. In Proceedings of the 12th International Conference on Computational Linguistics (Coling &apos;88). Budapest.</rawString>
</citation>
<citation valid="false">
<title>Unless one thinks that some regularity might show up if one distinguishes different kinds of direct complements with thematic roles.</title>
<marker></marker>
<rawString>&amp;quot;Unless one thinks that some regularity might show up if one distinguishes different kinds of direct complements with thematic roles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen M Bishop</author>
<author>Sharon Cote</author>
<author>Anne Abeille</author>
</authors>
<title>A Lexicalized Tree Adjoining Grammar for English.</title>
<date>1989</date>
<tech>Technical Report,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<marker>Bishop, Cote, Abeille, 1989</marker>
<rawString>Bishop, Kathleen M.; Cote, Sharon; and Abeille, Anne, 1989. A Lexicalized Tree Adjoining Grammar for English. Technical Report, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bunt</author>
</authors>
<title>Discontinuous Constituents in Trees, Rules and Parsing.</title>
<date>1987</date>
<booktitle>In Proceedings of European Chapter of the A CL &apos;87.</booktitle>
<location>Copenhagen.</location>
<marker>Bunt, 1987</marker>
<rawString>Bunt, et al., 1987. Discontinuous Constituents in Trees, Rules and Parsing. In Proceedings of European Chapter of the A CL &apos;87. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Frecicelton</author>
</authors>
<title>Une Etude Comparative des Expressions Idiomatiques de l&apos;Anglais et du Francais. PhD thesis, These de troisieme cycle,</title>
<date>1984</date>
<location>University Paris</location>
<marker>Frecicelton, 1984</marker>
<rawString>Frecicelton, P., 1984. Une Etude Comparative des Expressions Idiomatiques de l&apos;Anglais et du Francais. PhD thesis, These de troisieme cycle, University Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G K Pulhun</author>
<author>L A Sag</author>
</authors>
<title>Generalized Phrase Structure Grammars.</title>
<date>1985</date>
<publisher>Blackwell Publishing,</publisher>
<location>Oxford.</location>
<note>Also published by</note>
<contexts>
<context position="11017" citStr="Gazdar et al. 1985" startWordPosition="1861" endWordPosition="1864">ioms do not appear necessarily as continuous strings in texts. As shown by M. Gross for French and P. Freckelton for English, more than 15% of sentential idioms are made up of unbounded arguments, (e.g. NA) prendre NPi en compte, NPo take NP1 into account, Butter would not melt in NP&apos;s mouth). Discontinuities can also come from the regular application of syntactic rules. For example, interposition of adverbs between verb and object in compound V-NP phrases, and interposition of modals or auxiliaries between subject and verb in compound NP-V phrases are very general (Laporte 1988). As shown by Gazdar et al. 1985 for English, and Gross 1982 for French, most sentential idioms are not completely frozen and &apos;transformations&apos; apply to them much more regularly 3 than is usually thought. Freckelton 1984&apos;s listings of idiomatic sentences exhibit passivization for about 50% of the idioms comprised of a verb (different from be and have) and a frozen direct argument. Looking at a representative sample of 2000 idiomatic sentences with frozen objects (from Gross&apos;s listings at LADL) yields similar results for passivization and relativization of the frozen argument for French. This is usually considered a problem f</context>
</contexts>
<marker>Gazdar, Klein, Pulhun, Sag, 1985</marker>
<rawString>Gazdar, G.; Klein, E.; Pulhun, G. K.; and Sag, L A., 1985. Generalized Phrase Structure Grammars. Blackwell Publishing, Oxford. Also published by Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Classification des phrases figees en Francais. Revue Quebecoise de Linguistique 11(2).</title>
<date>1982</date>
<contexts>
<context position="8992" citStr="Gross 1982" startWordPosition="1517" endWordPosition="1518"> input string and selects the different tree structures associated with the lexical items of the string by looking up the lexicon. In a second step, these structures are combined together to produce a sentence. Thus the parser uses only a subset of the entire (lexicalized) grammar. 4 The nodes of the tree structures have feature structures associated with them, see footnote 3. 2 Linguistic Properties of Idioms Idioms have been at stake in many linguistic discussions since the early transformational grammars, but no exhaustive work based on extensive listings of idioms have been pursued before Gross 1982. We rely on L.A.D.L.&apos;s work for French that studied 8000 frozen sentences, 20, 000 frozen nouns and 6000 frozen adverbs. For English, we made use of Freckelton&apos;s thesis (1984) that listed more than 3000 sentential idioms. They show that, for a given structure, idiomatic phrases are usually more numerous in the language than &apos;free&apos; ones. As is well known, idioms are made of the same lexicon and consist of the same sequences of categories as &apos;free&apos; structures. An interesting exception is the case of &apos;words&apos; existing only as part of an idiomatic phrase, such as escampette in prendre la poudre d&apos;</context>
<context position="11045" citStr="Gross 1982" startWordPosition="1868" endWordPosition="1869">ntinuous strings in texts. As shown by M. Gross for French and P. Freckelton for English, more than 15% of sentential idioms are made up of unbounded arguments, (e.g. NA) prendre NPi en compte, NPo take NP1 into account, Butter would not melt in NP&apos;s mouth). Discontinuities can also come from the regular application of syntactic rules. For example, interposition of adverbs between verb and object in compound V-NP phrases, and interposition of modals or auxiliaries between subject and verb in compound NP-V phrases are very general (Laporte 1988). As shown by Gazdar et al. 1985 for English, and Gross 1982 for French, most sentential idioms are not completely frozen and &apos;transformations&apos; apply to them much more regularly 3 than is usually thought. Freckelton 1984&apos;s listings of idiomatic sentences exhibit passivization for about 50% of the idioms comprised of a verb (different from be and have) and a frozen direct argument. Looking at a representative sample of 2000 idiomatic sentences with frozen objects (from Gross&apos;s listings at LADL) yields similar results for passivization and relativization of the frozen argument for French. This is usually considered a problem for parsing, since the order </context>
</contexts>
<marker>Gross, 1982</marker>
<rawString>Gross, Maurice, 1982. Classification des phrases figees en Francais. Revue Quebecoise de Linguistique 11(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Parsing with discontinuous elements.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd A CL meeting.</booktitle>
<location>Chicago.</location>
<contexts>
<context position="22602" citStr="Johnson 1985" startWordPosition="3876" endWordPosition="3877">ositionality, and we do not treat a idiomatic all cases of non-literal readings. 6 NP0 VP No V NP1 Jean A DI N1 I I sa pipe Aux V I I a casse literal A NP0!. VP 71 V NPvi. PP2NA A takes P2 NP2N A 1 1 into N2NA account NP0 VP I VN No V NEVA IAA Jean Aux V DI NINA 1111 a casse sa pipe idiom Figure 10: Tree for NP0 takes NP1 into account 5 Recognizing Discontinuous Idioms Parsing flexible idioms has received only partial solutions so far (Stock 1987, Laporte 1988). Since TAGs factor recursion from dependencies, discontinuities are captured straightforwardly without special devices (as opposed to Johnson 1985 or Bunt et al. 1987). We distinguish two kinds of discontinuities: discontinuities that come from internal structures and discontinuities that come from the insertion of modifiers. 5.1 Internal Discontinuities Some idioms are internally discontinuous. Take for example the idioms NP0 prendre NP1 en compte and NP0 takes NP1 into account (see Figure 10).8 The discontinuity is handled simply by arguments (here NP0 and NP1) to be substituted (or adjoined in some cases) as any free sentences. The internal structures of arguments can be unbounded. 5.2 Recursive Insertions of Modifiers Some adjunctio</context>
</contexts>
<marker>Johnson, 1985</marker>
<rawString>Johnson, M., 1985. Parsing with discontinuous elements. In Proceedings of the 23rd A CL meeting. Chicago.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>How Much Context-Sensitivity is Necessary for Characterizing Structural Descriptions— Tree Adjoining Granunars.</title>
<date>1985</date>
<booktitle>Natural Language Processing— Theoretical, Computational and Psychological Perspectives.</booktitle>
<editor>In Dowty, D.; Karttunen, L.; and Zwicky, A. (editors),</editor>
<publisher>Cambridge University Press,</publisher>
<institution>Ohio State University,</institution>
<location>New York.</location>
<contexts>
<context position="3703" citStr="Joshi 1985" startWordPosition="589" endWordPosition="590">atic reading might be rejected. Idioms are thus parsed as any &apos;free&apos; sentences. Except during the selection process, idioms do not require any special parsing mechanism. We are also able to account for cases of ambiguity between idiomatic and literal interpretations. Factoring recursion from dependencies in TAGs allows discontinuous constituents to be parsed in an elegant way. We also show how regular &apos;transformations&apos; are taken into account by the parser. Topics: Parsing, Idioms. 1 Introduction to Tree Adjoining Grammars Tree Adjoining Grammars (TAGs) were introduced by Joshi et al. 1975 and Joshi 1985 as a formalism for linguistic description. Their linguistic relevance was shown by Kroch and Joshi 1985 and Abellle 1988. A lexicalized version of the formalism was presented in Schabes, Abeille and Joshi 1988 that makes them attractive for writing computational grammars. They were proved to be 1 parsable in polynomial time (worst case) by Vijay Shanker and Joshi 1985 and an Earley-type parser was presented by Schabes and Joshi 1988. The basic component of a TAG is a finite set of elementary trees that have two types: initial trees or auxiliary trees (See Figure 1). Both are minimal (but comp</context>
<context position="5380" citStr="Joshi 1985" startWordPosition="873" endWordPosition="874">o operations: substitution or adjunction. Substitution inserts an initial tree (or a tree derived from an initial tree) at a leaf node bearing the same label in an elementary tree (See Figure 2).2 It is the operation used by CFGs. Figure 2: Mechanism of substitution Adjunction is a more powerful operation: it inserts an auxiliary tree at one of the corresponding node of an elementary tree (See Figure 3).3 TAGs are more powerful than CFGs but only mildly so (Joshi 1983). Most of the linguistic advantages of the formalism come from the fact that it factors recursion from dependencies. Kroch and Joshi 1985 show how unbounded dependencies can be &apos;localized&apos; by having filler and gap as part of 2j is the mark for substitution. 3At each node of an elementary tree, there is a feature structure associated with it (Vijayshanker and Joshi, 1988). Adjunction constraints can be defined in terms of feature structures and the success or failure of unification. Figure 3: Adjoining the same elementary tree and having insertion of matrix clauses provided by recursive adjunctions. Another interesting property of the formalism is its extended domain of locality, as compared to that of usual phrase structure rul</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, Aravind K., 1985. How Much Context-Sensitivity is Necessary for Characterizing Structural Descriptions— Tree Adjoining Granunars. In Dowty, D.; Karttunen, L.; and Zwicky, A. (editors), Natural Language Processing— Theoretical, Computational and Psychological Perspectives. Cambridge University Press, New York. Originally presented in a Workshop on Natural Language Parsing at Ohio State University, Columbus, Ohio, May 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<date>1975</date>
<journal>Tree Adjunct Grammars. J. Comput. Syst. Sci.</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="3688" citStr="Joshi et al. 1975" startWordPosition="584" endWordPosition="587">the second step, idiomatic reading might be rejected. Idioms are thus parsed as any &apos;free&apos; sentences. Except during the selection process, idioms do not require any special parsing mechanism. We are also able to account for cases of ambiguity between idiomatic and literal interpretations. Factoring recursion from dependencies in TAGs allows discontinuous constituents to be parsed in an elegant way. We also show how regular &apos;transformations&apos; are taken into account by the parser. Topics: Parsing, Idioms. 1 Introduction to Tree Adjoining Grammars Tree Adjoining Grammars (TAGs) were introduced by Joshi et al. 1975 and Joshi 1985 as a formalism for linguistic description. Their linguistic relevance was shown by Kroch and Joshi 1985 and Abellle 1988. A lexicalized version of the formalism was presented in Schabes, Abeille and Joshi 1988 that makes them attractive for writing computational grammars. They were proved to be 1 parsable in polynomial time (worst case) by Vijay Shanker and Joshi 1985 and an Earley-type parser was presented by Schabes and Joshi 1988. The basic component of a TAG is a finite set of elementary trees that have two types: initial trees or auxiliary trees (See Figure 1). Both are mi</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, A. K.; Levy, L. S.; and Takahashi, M., 1975. Tree Adjunct Grammars. J. Comput. Syst. Sci. 10(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kroch</author>
<author>A K Joshi</author>
</authors>
<title>Linguistic Relevance of Tree Adjoining Grammars.</title>
<date>1985</date>
<tech>Technical Report MS-CIS85-18,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="3807" citStr="Kroch and Joshi 1985" startWordPosition="604" endWordPosition="607">the selection process, idioms do not require any special parsing mechanism. We are also able to account for cases of ambiguity between idiomatic and literal interpretations. Factoring recursion from dependencies in TAGs allows discontinuous constituents to be parsed in an elegant way. We also show how regular &apos;transformations&apos; are taken into account by the parser. Topics: Parsing, Idioms. 1 Introduction to Tree Adjoining Grammars Tree Adjoining Grammars (TAGs) were introduced by Joshi et al. 1975 and Joshi 1985 as a formalism for linguistic description. Their linguistic relevance was shown by Kroch and Joshi 1985 and Abellle 1988. A lexicalized version of the formalism was presented in Schabes, Abeille and Joshi 1988 that makes them attractive for writing computational grammars. They were proved to be 1 parsable in polynomial time (worst case) by Vijay Shanker and Joshi 1985 and an Earley-type parser was presented by Schabes and Joshi 1988. The basic component of a TAG is a finite set of elementary trees that have two types: initial trees or auxiliary trees (See Figure 1). Both are minimal (but complete) linguistic structures and have at least one terminal at their frontier (that is their &apos;head). Auxi</context>
<context position="5380" citStr="Kroch and Joshi 1985" startWordPosition="871" endWordPosition="874">rees by two operations: substitution or adjunction. Substitution inserts an initial tree (or a tree derived from an initial tree) at a leaf node bearing the same label in an elementary tree (See Figure 2).2 It is the operation used by CFGs. Figure 2: Mechanism of substitution Adjunction is a more powerful operation: it inserts an auxiliary tree at one of the corresponding node of an elementary tree (See Figure 3).3 TAGs are more powerful than CFGs but only mildly so (Joshi 1983). Most of the linguistic advantages of the formalism come from the fact that it factors recursion from dependencies. Kroch and Joshi 1985 show how unbounded dependencies can be &apos;localized&apos; by having filler and gap as part of 2j is the mark for substitution. 3At each node of an elementary tree, there is a feature structure associated with it (Vijayshanker and Joshi, 1988). Adjunction constraints can be defined in terms of feature structures and the success or failure of unification. Figure 3: Adjoining the same elementary tree and having insertion of matrix clauses provided by recursive adjunctions. Another interesting property of the formalism is its extended domain of locality, as compared to that of usual phrase structure rul</context>
</contexts>
<marker>Kroch, Joshi, 1985</marker>
<rawString>Kroch, A. and Joshi, A. K., 1985. Linguistic Relevance of Tree Adjoining Grammars. Technical Report MS-CIS85-18, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Laporte</author>
</authors>
<title>Reconnaissance des expressions figees lors de l&apos;analyse autornatique. Langages Larousse,</title>
<date>1988</date>
<location>Paris.</location>
<contexts>
<context position="10985" citStr="Laporte 1988" startWordPosition="1856" endWordPosition="1857"> and idiomatic readings. Idioms do not appear necessarily as continuous strings in texts. As shown by M. Gross for French and P. Freckelton for English, more than 15% of sentential idioms are made up of unbounded arguments, (e.g. NA) prendre NPi en compte, NPo take NP1 into account, Butter would not melt in NP&apos;s mouth). Discontinuities can also come from the regular application of syntactic rules. For example, interposition of adverbs between verb and object in compound V-NP phrases, and interposition of modals or auxiliaries between subject and verb in compound NP-V phrases are very general (Laporte 1988). As shown by Gazdar et al. 1985 for English, and Gross 1982 for French, most sentential idioms are not completely frozen and &apos;transformations&apos; apply to them much more regularly 3 than is usually thought. Freckelton 1984&apos;s listings of idiomatic sentences exhibit passivization for about 50% of the idioms comprised of a verb (different from be and have) and a frozen direct argument. Looking at a representative sample of 2000 idiomatic sentences with frozen objects (from Gross&apos;s listings at LADL) yields similar results for passivization and relativization of the frozen argument for French. This i</context>
<context position="22455" citStr="Laporte 1988" startWordPosition="3857" endWordPosition="3858">rguments, metaphoric readings can be taken int&lt; account (Bishop, Cote and Abeille 1989). We are able to handle different kinds of seman tic non-compositionality, and we do not treat a idiomatic all cases of non-literal readings. 6 NP0 VP No V NP1 Jean A DI N1 I I sa pipe Aux V I I a casse literal A NP0!. VP 71 V NPvi. PP2NA A takes P2 NP2N A 1 1 into N2NA account NP0 VP I VN No V NEVA IAA Jean Aux V DI NINA 1111 a casse sa pipe idiom Figure 10: Tree for NP0 takes NP1 into account 5 Recognizing Discontinuous Idioms Parsing flexible idioms has received only partial solutions so far (Stock 1987, Laporte 1988). Since TAGs factor recursion from dependencies, discontinuities are captured straightforwardly without special devices (as opposed to Johnson 1985 or Bunt et al. 1987). We distinguish two kinds of discontinuities: discontinuities that come from internal structures and discontinuities that come from the insertion of modifiers. 5.1 Internal Discontinuities Some idioms are internally discontinuous. Take for example the idioms NP0 prendre NP1 en compte and NP0 takes NP1 into account (see Figure 10).8 The discontinuity is handled simply by arguments (here NP0 and NP1) to be substituted (or adjoine</context>
</contexts>
<marker>Laporte, 1988</marker>
<rawString>Laporte, E., 1988. Reconnaissance des expressions figees lors de l&apos;analyse autornatique. Langages Larousse, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>An EarleyType Parsing Algorithm for Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In 26th Meeting of the Association for Computational Linguistics.</booktitle>
<location>Buffalo.</location>
<contexts>
<context position="4140" citStr="Schabes and Joshi 1988" startWordPosition="658" endWordPosition="661">e taken into account by the parser. Topics: Parsing, Idioms. 1 Introduction to Tree Adjoining Grammars Tree Adjoining Grammars (TAGs) were introduced by Joshi et al. 1975 and Joshi 1985 as a formalism for linguistic description. Their linguistic relevance was shown by Kroch and Joshi 1985 and Abellle 1988. A lexicalized version of the formalism was presented in Schabes, Abeille and Joshi 1988 that makes them attractive for writing computational grammars. They were proved to be 1 parsable in polynomial time (worst case) by Vijay Shanker and Joshi 1985 and an Earley-type parser was presented by Schabes and Joshi 1988. The basic component of a TAG is a finite set of elementary trees that have two types: initial trees or auxiliary trees (See Figure 1). Both are minimal (but complete) linguistic structures and have at least one terminal at their frontier (that is their &apos;head). Auxiliary trees are also constrained to have exactly one leaf node labeled with a nonterminal of the same category as their root node. Initial tree: Auxiliary trem terminals or substitution nodes Figure 1: Schematic initial and auxiliary trees Sentences of the language of a TAG are derived from the composition of an S-rooted initial tr</context>
</contexts>
<marker>Schabes, Joshi, 1988</marker>
<rawString>Schabes, Yves and Joshi, Aravind K., 1988. An EarleyType Parsing Algorithm for Tree Adjoining Grammars. In 26th Meeting of the Association for Computational Linguistics. Buffalo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Sdiabes</author>
<author>Anne Abeille</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing Strategies with `Lexicalized&apos; Grammars: Application to &apos;five Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics.</booktitle>
<marker>Sdiabes, Abeille, Joshi, 1988</marker>
<rawString>Sdiabes, Yves; Abeille, Anne; and Joshi, Aravind K., 1988. Parsing Strategies with `Lexicalized&apos; Grammars: Application to &apos;five Adjoining Grammars. In Proceedings of the 12th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stock</author>
</authors>
<title>Getting Idioms in a Lexicon Based Parser&apos;s Head.</title>
<date>1987</date>
<booktitle>In Proceedings of ACL&apos;87.</booktitle>
<location>Stanford.</location>
<contexts>
<context position="22440" citStr="Stock 1987" startWordPosition="3855" endWordPosition="3856">rictions o arguments, metaphoric readings can be taken int&lt; account (Bishop, Cote and Abeille 1989). We are able to handle different kinds of seman tic non-compositionality, and we do not treat a idiomatic all cases of non-literal readings. 6 NP0 VP No V NP1 Jean A DI N1 I I sa pipe Aux V I I a casse literal A NP0!. VP 71 V NPvi. PP2NA A takes P2 NP2N A 1 1 into N2NA account NP0 VP I VN No V NEVA IAA Jean Aux V DI NINA 1111 a casse sa pipe idiom Figure 10: Tree for NP0 takes NP1 into account 5 Recognizing Discontinuous Idioms Parsing flexible idioms has received only partial solutions so far (Stock 1987, Laporte 1988). Since TAGs factor recursion from dependencies, discontinuities are captured straightforwardly without special devices (as opposed to Johnson 1985 or Bunt et al. 1987). We distinguish two kinds of discontinuities: discontinuities that come from internal structures and discontinuities that come from the insertion of modifiers. 5.1 Internal Discontinuities Some idioms are internally discontinuous. Take for example the idioms NP0 prendre NP1 en compte and NP0 takes NP1 into account (see Figure 10).8 The discontinuity is handled simply by arguments (here NP0 and NP1) to be substitu</context>
</contexts>
<marker>Stock, 1987</marker>
<rawString>Stock, 0., 1987. Getting Idioms in a Lexicon Based Parser&apos;s Head. In Proceedings of ACL&apos;87. Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Some Computational Properties of Tree Adjoining Grammars. In 23&apos;d Meeting of the Association for Computational Linguistics,</title>
<date>1985</date>
<pages>82--93</pages>
<marker>Vijay-Shanker, Joshi, 1985</marker>
<rawString>Vijay-Shanker, K. and Joshi, A. K., 1985. Some Computational Properties of Tree Adjoining Grammars. In 23&apos;d Meeting of the Association for Computational Linguistics, pages 82-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Feature Structure Based Tree Adjoining Grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (Coling &apos;88).</booktitle>
<location>Budapest.</location>
<marker>Vijay-Shanker, Joshi, 1988</marker>
<rawString>Vijay-Shanker, K. and Joshi, A.K., 1988. Feature Structure Based Tree Adjoining Grammars. In Proceedings of the 12th International Conference on Computational Linguistics (Coling &apos;88). Budapest.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>