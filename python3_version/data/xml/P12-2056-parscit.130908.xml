<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008133">
<title confidence="0.98118">
Enhancing Statistical Machine Translation with Character Alignment
</title>
<author confidence="0.996945">
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, Jiajun Chen
</author>
<affiliation confidence="0.979413333333333">
State Key Laboratory for Novel Software Technology,
Department of Computer Science and Technology,
Nanjing University, Nanjing, 210046, China
</affiliation>
<email confidence="0.996915">
{xin,tanggc,dxy,huangsj,chenjj}@nlp.nju.edu.cn
</email>
<sectionHeader confidence="0.993863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998408578947368">
The dominant practice of statistical machine
translation (SMT) uses the same Chinese word
segmentation specification in both alignment
and translation rule induction steps in building
Chinese-English SMT system, which may suf-
fer from a suboptimal problem that word seg-
mentation better for alignment is not necessarily
better for translation. To tackle this, we propose
a framework that uses two different segmenta-
tion specifications for alignment and translation
respectively: we use Chinese character as the
basic unit for alignment, and then convert this
alignment to conventional word alignment for
translation rule induction. Experimentally, our
approach outperformed two baselines: fully
word-based system (using word for both
alignment and translation) and fully charac-
ter-based system, in terms of alignment quality
and translation performance.
</bodyText>
<sectionHeader confidence="0.998876" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996517733333333">
Chinese Word segmentation is a necessary step in
Chinese-English statistical machine translation
(SMT) because Chinese sentences do not delimit
words by spaces. The key characteristic of a Chi-
nese word segmenter is the segmentation specifi-
cation1. As depicted in Figure 1(a), the dominant
practice of SMT uses the same word segmentation
for both word alignment and translation rule induc-
tion. For brevity, we will refer to the word seg-
mentation of the bilingual corpus as word segmen-
tation for alignment (WSA for short), because it
determines the basic tokens for alignment; and refer
to the word segmentation of the aligned corpus as
word segmentation for rules (WSR for short), be-
cause it determines the basic tokens of translation
</bodyText>
<footnote confidence="0.847174">
1 We hereafter use “word segmentation” for short.
</footnote>
<figureCaption confidence="0.999658">
Figure 1. WSA and WSR in SMT pipeline
</figureCaption>
<bodyText confidence="0.99990108">
rules2, which also determines how the translation
rules would be matched by the source sentences.
It is widely accepted that word segmentation with
a higher F-score will not necessarily yield better
translation performance (Chang et al., 2008; Zhang
et al., 2008; Xiao et al., 2010). Therefore, many
approaches have been proposed to learn word
segmentation suitable for SMT. These approaches
were either complicated (Ma et al., 2007; Chang et
al., 2008; Ma and Way, 2009; Paul et al., 2010), or
of high computational complexity (Chung and
Gildea 2009; Duan et al., 2010). Moreover, they
implicitly assumed that WSA and WSR should be
equal. This requirement may lead to a suboptimal
problem that word segmentation better for align-
ment is not necessarily better for translation.
To tackle this, we propose a framework that uses
different word segmentation specifications as WSA
and WSR respectively, as shown Figure 1(b). We
investigate a solution in this framework: first, we
use Chinese character as the basic unit for align-
ment, viz. character alignment; second, we use a
simple method (Elming and Habash, 2007) to
convert the character alignment to conventional
word alignment for translation rule induction. In the
</bodyText>
<page confidence="0.889479">
2 Interestingly, word is also a basic token in syntax-based rules.
</page>
<figure confidence="0.99939005">
Bilingual Corpus WSA
Word alignment
Aligned Corpus WSA
Rule induction
Translation Rules WSR
Decoding
Translation Results WSR
(a) WSA=WSR
Bilingual Corpus WSA
Word alignment
Conversion
Aligned Corpus
Rule induction
Translation Rules WSR
Decoding
Translation Results WSR
(b) WSA≠WSR
Aligned Corpus
WSA
WSR
</figure>
<page confidence="0.990828">
285
</page>
<note confidence="0.7002405">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285–290,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.906382083333333">
experiment, our approach consistently outper-
formed two baselines with three different word
segmenters: fully word-based system (using word
for both alignment and translation) and fully char-
acter-based system, in terms of alignment quality
and translation performance.
The remainder of this paper is structured as fol-
lows: Section 2 analyzes the influences of WSA and
WSR on SMT respectively; Section 3 discusses
how to convert character alignment to word align-
ment; Section 4 presents experimental results, fol-
lowed by conclusions and future work in section 5.
</bodyText>
<subsectionHeader confidence="0.516751">
2 Understanding WSA and WSR
</subsectionHeader>
<bodyText confidence="0.9998213125">
We propose a solution to tackle the suboptimal
problem: using Chinese character for alignment
while using Chinese word for translation. Character
alignment differs from conventional word align-
ment in the basic tokens of the Chinese side of the
training corpus3. Table 1 compares the token dis-
tributions of character-based corpus (CCorpus) and
word-based corpus (WCorpus). We see that the
WCorpus has a longer-tailed distribution than the
CCorpus. More than 70% of the unique tokens ap-
pear less than 5 times in WCorpus. However, over
half of the tokens appear more than or equal to 5
times in the CCorpus. This indicates that modeling
word alignment could suffer more from data
sparsity than modeling character alignment.
Table 2 shows the numbers of the unique tokens
(#UT) and unique bilingual token pairs (#UTP) of
the two corpora. Consider two extensively features,
fertility and translation features, which are exten-
sively used by many state-of-the-art word aligners.
The number of parameters w.r.t. fertility features
grows linearly with #UT while the number of pa-
rameters w.r.t. translation features grows linearly
with #UTP. We compare #UT and #UTP of both
corpora in Table 2. As can be seen, CCorpus has
less UT and UTP than WCorpus, i.e. character
alignment model has a compact parameterization
than word alignment model, where the compactness
of parameterization is shown very important in sta-
tistical modeling (Collins, 1999).
Another advantage of character alignment is the
reduction in alignment errors caused by word seg-
</bodyText>
<tableCaption confidence="0.5365525">
3 Several works have proposed to use character (letter) on both
sides of the parallel corpus for SMT between similar (European)
languages (Vilar et al., 2007; Tiedemann, 2009), however,
Chinese is not similar to English.
</tableCaption>
<table confidence="0.9610555">
Frequency Characters (%) Words (%)
1 27.22 45.39
2 11.13 14.61
3 6.18 6.47
4 4.26 4.32
5(+) 50.21 29.21
</table>
<tableCaption confidence="0.98806">
Table 1 Token distribution of CCorpus and WCorpus
</tableCaption>
<table confidence="0.818728">
Stats. Characters Words
#UT 9.7K 88.1K
#UTP 15.8M 24.2M
</table>
<tableCaption confidence="0.984699">
Table 2 #UT and #UTP in CCorpus and WCorpus
</tableCaption>
<bodyText confidence="0.998132605263158">
mentation errors. For example, “切尼 (Cheney)”
and “愿 (will)” are wrongly merged into one word
切 尼 愿 by the word segmenter, and 切 尼 愿
wrongly aligns to a comma in English sentence in
the word alignment; However, both 切 and 尼 align
to “Cheney” correctly in the character alignment.
However, this kind of errors cannot be fixed by
methods which learn new words by packing already
segmented words, such as word packing (Ma et al.,
2007) and Pseudo-word (Duan et al., 2010).
As character could preserve more meanings than
word in Chinese, it seems that a character can be
wrongly aligned to many English words by the
aligner. However, we found this can be avoided to a
great extent by the basic features (co-occurrence
and distortion) used by many alignment models. For
example, we observed that the four characters of the
non-compositional word “阿拉法特 (Arafat)” align
to Arafat correctly, although these characters pre-
serve different meanings from that of Arafat. This
can be attributed to the frequent co-occurrence (192
times) of these characters and Arafat in CCorpus.
Moreover,法 usually means France in Chinese,
thus it may co-occur very often with France in
CCorpus. If both France and Arafat appear in the
English sentence, 法 may wrongly align to France.
However, if 阿 aligns to Arafat, 法 will probably
align to Arafat, because aligning 法 to Arafat could
result in a lower distortion cost than aligning it to
France.
Different from alignment, translation is a pattern
matching procedure (Lopez, 2008). WSR deter-
mines how the translation rules would be matched
by the source sentences. For example, if we use
translation rules with character as WSR to translate
name entities such as the non-compositional word
阿拉法特, i.e. translating literally, we may get a
wrong translation. That’s because the linguistic
</bodyText>
<page confidence="0.98955">
286
</page>
<bodyText confidence="0.999972545454546">
knowledge that the four characters convey a spe-
cific meaning different from the characters has been
lost, which cannot always be totally recovered even
by using phrase in phrase-based SMT systems (see
Chang et al. (2008) for detail). Duan et al. (2010)
and Paul et al., (2010) further pointed out that
coarser-grained segmentation of the source sen-
tence do help capture more contexts in translation.
Therefore, rather than using character, using
coarser-grained, at least as coarser as the conven-
tional word, as WSR is quite necessary.
</bodyText>
<sectionHeader confidence="0.9826295" genericHeader="method">
3 Converting Character Alignment to Word
Alignment
</sectionHeader>
<bodyText confidence="0.970908371428571">
In order to use word as WSR, we employ the same
method as Elming and Habash (2007)4 to convert
the character alignment (CA) to its word-based
version (CA’) for translation rule induction. The
conversion is very intuitive: for every Eng-
lish-Chinese word pair (a, c) in the sentence pair,
we align c to a as a link in CA’, if and only if there
is at least one Chinese character of c aligns to a in
CA.
Given two different segmentations A and B of the
same sentence, it is easy to prove that if every word
in A is finer-grained than the word of B at the cor-
responding position, the conversion is unambiguity
(we omit the proof due to space limitation). As
character is a finer-grained than its original word,
character alignment can always be converted to
alignment based on any word segmentation.
Therefore, our approach can be naturally scaled to
syntax-based system by converting character
alignment to word alignment where the word seg-
mentation is consistent with the parsers.
We compare CA with the conventional word
alignment (WA) as follows: We hand-align some
sentence pairs as the evaluation set based on char-
acters (E5Char), and converted it to the evaluation
set based on word (E5Word) using the above con-
version method. It is worth noting that comparing
CA and WA by evaluating CA on E5Char and
evaluating WA on E5Word is meaningless, because
the basic tokens in CA and WA are different.
However, based on the conversion method, com-
paring CA with WA can be accomplished by evalu-
ating both CA’ and WA on E5Word.
4 They used this conversion for word alignment combination
only, no translation results were reported.
</bodyText>
<sectionHeader confidence="0.997603" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.546866">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999979541666667">
FBIS corpus (LDC2003E14) (210K sentence pairs)
was used for small-scale task. A large bilingual
corpus of our lab (1.9M sentence pairs) was used for
large-scale task. The NIST’06 and NIST’08 test sets
were used as the development set and test set re-
spectively. The Chinese portions of all these data
were preprocessed by character segmenter (CHAR),
ICTCLAS word segmenter 5 (ICT) and Stanford
word segmenters with CTB and PKU specifica-
tions6 respectively. The first 100 sentence pairs of
the hand-aligned set in Haghighi et al. (2009) were
hand-aligned as E5Char, which is converted to
three E5Words based on three segmentations re-
spectively. These E5Words were appended to
training corpus with the corresponding word seg-
mentation for evaluation purpose.
Both character and word alignment were per-
formed by GIZA++ (Och and Ney, 2003) enhanced
with gdf heuristics to combine bidirectional align-
ments (Koehn et al., 2003). A 5-gram language
model was trained from the Xinhua portion of
Gigaword corpus. A phrase-based MT decoder
similar to (Koehn et al., 2007) was used with the
decoding weights optimized by MERT (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.946502">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999806411764706">
We first evaluate the alignment quality. The method
discussed in section 3 was used to compare char-
acter and word alignment. As can be seen from
Table 3, the systems using character as WSA out-
performed the ones using word as WSA in both
small-scale (row 3-5) and large-scale task (row 6-8)
with all segmentations. This gain can be attributed
to the small vocabulary size (sparsity) for character
alignment. The observation is consistent with
Koehn (2005) which claimed that there is a negative
correlation between the vocabulary size and trans-
lation performance without explicitly distinguish-
ing WSA and WSR.
We then evaluated the translation performance.
The baselines are fully word-based MT systems
(Word5ys), i.e. using word as both WSA and WSR,
and fully character-based systems (Char5ys). Table
</bodyText>
<footnote confidence="0.997748">
5 http://www.ictclas.org/
6 http://nlp.stanford.edu/software/segmenter.shtml
</footnote>
<page confidence="0.987397">
287
</page>
<table confidence="0.999853">
Word alignment Character alignment
P R F P R F
CTB 76.0 81.9 78.9 78.2 85.2 81.8
S PKU 76.1 82.0 79.0 78.0 86.1 81.9
ICT 75.2 80.8 78.0 78.7 86.3 82.3
CTB 79.6 85.6 82.5 82.2 90.6 86.2
L PKU 80.0 85.4 82.6 81.3 89.5 85.2
ICT 80.0 85.0 82.4 81.3 89.7 85.3
</table>
<tableCaption confidence="0.995131">
Table 3 Alignment evaluation. Precision (P), recall (R),
and F-score (F) with a ൌ 0.5 (Fraser and Marcu, 2007)
</tableCaption>
<table confidence="0.9998114">
WSA WSR CTB PKU ICT
S word word 21.52 20.99 20.95
char word 22.04 21.98 22.04
L word word 22.07 22.86 22.23
char word 23.41 23.51 23.05
</table>
<tableCaption confidence="0.9988635">
Table 4 Translation evaluation of WordSys and pro-
posed system using BLEU-SBP (Chiang et al., 2008)
</tableCaption>
<bodyText confidence="0.999293705882353">
4 compares WordSys to our proposed system. Sig-
nificant testing was carried out using bootstrap
re-sampling method proposed by Koehn (2004)
with a 95% confidence level. We see that our pro-
posed systems outperformed WordSys in all seg-
mentation specifications settings. Table 5 lists the
results of CharSys in small-scale task. In this setting,
we gradually set the phrase length and the distortion
limits of the phrase-based decoder (context size) to
7, 9, 11 and 13, in order to remove the disadvantage
of shorter context size of using character as WSR
for fair comparison with WordSys as suggested by
Duan et al. (2010). Comparing Table 4 and 5, we
see that all CharSys underperformed WordSys. This
observation is consistent with Chang et al. (2008)
which claimed that using characters, even with
large phrase length (up to 13 in our experiment)
cannot always capture everything a Chinese word
segmenter can do, and using word for translation is
quite necessary. We also see that CharSys under-
performed our proposed systems, that’s because the
harm of using character as WSR outweighed the
benefit of using character as WSA, which indicated
that word segmentation better for alignment is not
necessarily better for translation, and vice versa.
We finally compared our approaches to Ma et al.
(2007) and Ma and Way (2009), which proposed
“packed word (PW)” and “bilingual motivated
word (BS)” respectively. Both methods iteratively
learn word segmentation and alignment alterna-
tively, with the former starting from word-based
corpus and the latter starting from characters-based
corpus. Therefore, PW can be experimented on all
segmentations. Table 6 lists their results in small-
</bodyText>
<table confidence="0.9533165">
Context Size 7 9 11 13
BLEU 20.90 21.19 20.89 21.09
</table>
<tableCaption confidence="0.98913">
Table 5 Translation evaluation of CharSys.
</tableCaption>
<table confidence="0.999839857142857">
System WSA WSR CTB PKU ICT
WordSys word word 21.52 20.99 20.95
Proposed char word 22.04 21.98 22.04
PW PW PW 21.24 21.24 21.19
Char+PW char PW 22.46 21.87 21.97
BS BS BS 19.76
Char+BS char BS 20.19
</table>
<tableCaption confidence="0.999912">
Table 6 Comparison with other works
</tableCaption>
<bodyText confidence="0.999849105263158">
scale task, we see that both PW and BS underper-
formed our approach. This may be attributed to the
low recall of the learned BS or PW in their ap-
proaches. BS underperformed both two baselines,
one reason is that Ma and Way (2009) also em-
ployed word lattice decoding techniques (Dyer et al.,
2008) to tackle the low recall of BS, which was
removed from our experiments for fair comparison.
Interestingly, we found that using character as
WSA and BS as WSR (Char+BS), a moderate gain
(+0.43 point) was achieved compared with fully
BS-based system; and using character as WSA and
PW as WSR (Char+PW), significant gains were
achieved compared with fully PW-based system,
the result of CTB segmentation in this setting even
outperformed our proposed approach (+0.42 point).
This observation indicated that in our framework,
better combinations of WSA and WSR can be found
to achieve better translation performance.
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999883333333333">
We proposed a SMT framework that uses character
for alignment and word for translation, which im-
proved both alignment quality and translation per-
formance. We believe that in this framework, using
other finer-grained segmentation, with fewer am-
biguities than character, would better parameterize
the alignment models, while using other coars-
er-grained segmentation as WSR can help capture
more linguistic knowledge than word to get better
translation. We also believe that our approach, if
integrated with combination techniques (Dyer et al.,
2008; Xi et al., 2011), can yield better results.
</bodyText>
<sectionHeader confidence="0.998307" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98150375">
We thank ACL reviewers. This work is supported
by the National Natural Science Foundation of
China (No. 61003112), the National Fundamental
Research Program of China (2010CB327903).
</bodyText>
<page confidence="0.996284">
288
</page>
<sectionHeader confidence="0.990003" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782112244897">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Peitra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2), pages
263-311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmenta-
tion for machine translation performance. In Pro-
ceedings of third workshop on SMT, pages 224-232.
David Chiang, Steve DeNeefe, Yee Seng Chan and
Hwee Tou Ng. 2008. Decomposability of Translation
Metrics for Improved Evaluation and Efficient Algo-
rithms. In Proceedings of Conference on Empirical
Methods in Natural Language Processing, pages
610-619.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing, pages 718-726.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Xiangyu Duan, Min Zhang, and Haizhou Li. 2010.
Pseudo-word for phrase-based machine translation. In
Proceedings of the Association for Computational
Linguistics, pages 148-156.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 1012-1020.
Jakob Elming and Nizar Habash. 2007. Combination of
statistical word alignments based on multiple pre-
processing schemes. In Proceedings of the Associa-
tion for Computational Linguistics, pages 25-28.
Alexander Fraser and Daniel Marcu. 2007. Squibs and
Discussions: Measuring Word Alignment Quality for
Statistical Machine Translation. In Computational
Linguistics, 33(3), pages 293-303.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of the Association for
Computational Linguistics, pages 923-931.
Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan,W. Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E.
Herbst. 2007. Moses: Open source toolkit for statis-
tical machine translation. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 177-180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
Conference on Empirical Methods on Natural Lan-
guage Processing, pages 388-395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit.
Adam David Lopez. 2008. Machine translation by pat-
tern matching. Ph.D. thesis, University of Maryland.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the Association for Computational
Linguistics, pages 304-311.
Yanjun Ma and Andy Way. 2009. Bilingually motivated
domain-adapted word segmentation for statistical
machine translation. In Proceedings of the Conference
of the European Chapter of the ACL, pages 549-557.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Association for Computational Linguistics, pages
440-447.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1), pages 19-51.
Michael Paul, Andrew Finch and Eiichiro Sumita. 2010.
Integration of multiple bilingually-learned segmenta-
tion schemes into statistical machine translation. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages
400-408.
Jörg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proceedings of the An-
nual Conference of the European Association for
machine Translation, pages 12-19.
David Vilar, Jan-T. Peter and Hermann Ney. 2007. Can
we translate letters? In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
33-39.
Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu
and Shouxun Lin. 2010. Joint tokenization and
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1200-1208.
Ning Xi, Guangchao Tang, Boyuan Li, and Yinggong
Zhao. 2011. Word alignment combination over mul-
tiple word segmentation. In Proceedings of the ACL
2011 Student Session, pages 1-5.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceedings
</reference>
<page confidence="0.979781">
289
</page>
<reference confidence="0.908126">
of the Third Workshop on Statistical Machine Trans-
lation, pages 216-223.
</reference>
<page confidence="0.996585">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.332559">
<title confidence="0.998112">Enhancing Statistical Machine Translation with Character Alignment</title>
<author confidence="0.8536605">Ning Xi</author>
<author confidence="0.8536605">Guangchao Tang</author>
<author confidence="0.8536605">Xinyu Dai</author>
<author confidence="0.8536605">Shujian Huang</author>
<author confidence="0.8536605">Jiajun State Key Laboratory for Novel Software</author>
<affiliation confidence="0.749927">Department of Computer Science and Nanjing University, Nanjing, 210046,</affiliation>
<email confidence="0.955771">xin@nlp.nju.edu.cn</email>
<email confidence="0.955771">tanggc@nlp.nju.edu.cn</email>
<email confidence="0.955771">dxy@nlp.nju.edu.cn</email>
<email confidence="0.955771">huangsj@nlp.nju.edu.cn</email>
<email confidence="0.955771">chenjj@nlp.nju.edu.cn</email>
<abstract confidence="0.99960165">The dominant practice of statistical machine translation (SMT) uses the same Chinese word segmentation specification in both alignment and translation rule induction steps in building Chinese-English SMT system, which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses two different segmentation specifications for alignment and translation respectively: we use Chinese character as the basic unit for alignment, and then convert this alignment to conventional word alignment for translation rule induction. Experimentally, our approach outperformed two baselines: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Peitra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Pietra, Peitra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Peitra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), pages 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of third workshop on SMT,</booktitle>
<pages>224--232</pages>
<contexts>
<context position="2259" citStr="Chang et al., 2008" startWordPosition="326" endWordPosition="329">f the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word s</context>
<context position="8432" citStr="Chang et al. (2008)" startWordPosition="1311" endWordPosition="1314">ance. Different from alignment, translation is a pattern matching procedure (Lopez, 2008). WSR determines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word 阿拉法特, i.e. translating literally, we may get a wrong translation. That’s because the linguistic 286 knowledge that the four characters convey a specific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sentence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conventional word, as WSR is quite necessary. 3 Converting Character Alignment to Word Alignment In order to use word as WSR, we employ the same method as Elming and Habash (2007)4 to convert the character alignment (CA) to its word-based version (CA’) for translation rule induction. The conversion is very intuitive: for every E</context>
<context position="13818" citStr="Chang et al. (2008)" startWordPosition="2208" endWordPosition="2211">n (2004) with a 95% confidence level. We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010). Comparing Table 4 and 5, we see that all CharSys underperformed WordSys. This observation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys underperformed our proposed systems, that’s because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa. We finally compared our approaches to Ma et al. (2007) and Ma and Way (2009), which proposed “packed word</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of third workshop on SMT, pages 224-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Steve DeNeefe, Yee Seng Chan and Hwee Tou Ng.</title>
<date>2008</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>610--619</pages>
<marker>Chiang, 2008</marker>
<rawString>David Chiang, Steve DeNeefe, Yee Seng Chan and Hwee Tou Ng. 2008. Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 610-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>718--726</pages>
<contexts>
<context position="2567" citStr="Chung and Gildea 2009" startWordPosition="377" endWordPosition="380">e “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 718-726.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5829" citStr="Collins, 1999" startWordPosition="881" endWordPosition="882">wo corpora. Consider two extensively features, fertility and translation features, which are extensively used by many state-of-the-art word aligners. The number of parameters w.r.t. fertility features grows linearly with #UT while the number of parameters w.r.t. translation features grows linearly with #UTP. We compare #UT and #UTP of both corpora in Table 2. As can be seen, CCorpus has less UT and UTP than WCorpus, i.e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in statistical modeling (Collins, 1999). Another advantage of character alignment is the reduction in alignment errors caused by word seg3 Several works have proposed to use character (letter) on both sides of the parallel corpus for SMT between similar (European) languages (Vilar et al., 2007; Tiedemann, 2009), however, Chinese is not similar to English. Frequency Characters (%) Words (%) 1 27.22 45.39 2 11.13 14.61 3 6.18 6.47 4 4.26 4.32 5(+) 50.21 29.21 Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For examp</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Pseudo-word for phrase-based machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="2587" citStr="Duan et al., 2010" startWordPosition="381" endWordPosition="384">or short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word a</context>
<context position="6869" citStr="Duan et al., 2010" startWordPosition="1059" endWordPosition="1062"> Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, “切尼 (Cheney)” and “愿 (will)” are wrongly merged into one word 切 尼 愿 by the word segmenter, and 切 尼 愿 wrongly aligns to a comma in English sentence in the word alignment; However, both 切 and 尼 align to “Cheney” correctly in the character alignment. However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word (Duan et al., 2010). As character could preserve more meanings than word in Chinese, it seems that a character can be wrongly aligned to many English words by the aligner. However, we found this can be avoided to a great extent by the basic features (co-occurrence and distortion) used by many alignment models. For example, we observed that the four characters of the non-compositional word “阿拉法特 (Arafat)” align to Arafat correctly, although these characters preserve different meanings from that of Arafat. This can be attributed to the frequent co-occurrence (192 times) of these characters and Arafat in CCorpus. M</context>
<context position="8464" citStr="Duan et al. (2010)" startWordPosition="1317" endWordPosition="1320">ranslation is a pattern matching procedure (Lopez, 2008). WSR determines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word 阿拉法特, i.e. translating literally, we may get a wrong translation. That’s because the linguistic 286 knowledge that the four characters convey a specific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sentence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conventional word, as WSR is quite necessary. 3 Converting Character Alignment to Word Alignment In order to use word as WSR, we employ the same method as Elming and Habash (2007)4 to convert the character alignment (CA) to its word-based version (CA’) for translation rule induction. The conversion is very intuitive: for every English-Chinese word pair (a, c) </context>
<context position="13688" citStr="Duan et al. (2010)" startWordPosition="2187" endWordPosition="2190"> compares WordSys to our proposed system. Significant testing was carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level. We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010). Comparing Table 4 and 5, we see that all CharSys underperformed WordSys. This observation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys underperformed our proposed systems, that’s because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for tran</context>
</contexts>
<marker>Duan, Zhang, Li, 2010</marker>
<rawString>Xiangyu Duan, Min Zhang, and Haizhou Li. 2010. Pseudo-word for phrase-based machine translation. In Proceedings of the Association for Computational Linguistics, pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="15371" citStr="Dyer et al., 2008" startWordPosition="2467" endWordPosition="2470">t Size 7 9 11 13 BLEU 20.90 21.19 20.89 21.09 Table 5 Translation evaluation of CharSys. System WSA WSR CTB PKU ICT WordSys word word 21.52 20.99 20.95 Proposed char word 22.04 21.98 22.04 PW PW PW 21.24 21.24 21.19 Char+PW char PW 22.46 21.87 21.97 BS BS BS 19.76 Char+BS char BS 20.19 Table 6 Comparison with other works scale task, we see that both PW and BS underperformed our approach. This may be attributed to the low recall of the learned BS or PW in their approaches. BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques (Dyer et al., 2008) to tackle the low recall of BS, which was removed from our experiments for fair comparison. Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point). This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of the Association for Computational Linguistics, pages 1012-1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of statistical word alignments based on multiple preprocessing schemes.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="3129" citStr="Elming and Habash, 2007" startWordPosition="467" endWordPosition="470">0), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word alignment for translation rule induction. In the 2 Interestingly, word is also a basic token in syntax-based rules. Bilingual Corpus WSA Word alignment Aligned Corpus WSA Rule induction Translation Rules WSR Decoding Translation Results WSR (a) WSA=WSR Bilingual Corpus WSA Word alignment Conversion Aligned Corpus Rule induction Translation Rules WSR Decoding Translation Results WSR (b) WSA≠WSR Aligned Corpus WSA WSR 285 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285–290, Jeju, Republic </context>
<context position="8881" citStr="Elming and Habash (2007)" startWordPosition="1385" endWordPosition="1388">pecific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sentence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conventional word, as WSR is quite necessary. 3 Converting Character Alignment to Word Alignment In order to use word as WSR, we employ the same method as Elming and Habash (2007)4 to convert the character alignment (CA) to its word-based version (CA’) for translation rule induction. The conversion is very intuitive: for every English-Chinese word pair (a, c) in the sentence pair, we align c to a as a link in CA’, if and only if there is at least one Chinese character of c aligns to a in CA. Given two different segmentations A and B of the same sentence, it is easy to prove that if every word in A is finer-grained than the word of B at the corresponding position, the conversion is unambiguity (we omit the proof due to space limitation). As character is a finer-grained </context>
</contexts>
<marker>Elming, Habash, 2007</marker>
<rawString>Jakob Elming and Nizar Habash. 2007. Combination of statistical word alignments based on multiple preprocessing schemes. In Proceedings of the Association for Computational Linguistics, pages 25-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Squibs and Discussions: Measuring Word Alignment Quality for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>3</issue>
<pages>293--303</pages>
<contexts>
<context position="12833" citStr="Fraser and Marcu, 2007" startWordPosition="2041" endWordPosition="2044">ed the translation performance. The baselines are fully word-based MT systems (Word5ys), i.e. using word as both WSA and WSR, and fully character-based systems (Char5ys). Table 5 http://www.ictclas.org/ 6 http://nlp.stanford.edu/software/segmenter.shtml 287 Word alignment Character alignment P R F P R F CTB 76.0 81.9 78.9 78.2 85.2 81.8 S PKU 76.1 82.0 79.0 78.0 86.1 81.9 ICT 75.2 80.8 78.0 78.7 86.3 82.3 CTB 79.6 85.6 82.5 82.2 90.6 86.2 L PKU 80.0 85.4 82.6 81.3 89.5 85.2 ICT 80.0 85.0 82.4 81.3 89.7 85.3 Table 3 Alignment evaluation. Precision (P), recall (R), and F-score (F) with a ൌ 0.5 (Fraser and Marcu, 2007) WSA WSR CTB PKU ICT S word word 21.52 20.99 20.95 char word 22.04 21.98 22.04 L word word 22.07 22.86 22.23 char word 23.41 23.51 23.05 Table 4 Translation evaluation of WordSys and proposed system using BLEU-SBP (Chiang et al., 2008) 4 compares WordSys to our proposed system. Significant testing was carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level. We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the p</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Squibs and Discussions: Measuring Word Alignment Quality for Statistical Machine Translation. In Computational Linguistics, 33(3), pages 293-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>923--931</pages>
<contexts>
<context position="10980" citStr="Haghighi et al. (2009)" startWordPosition="1741" endWordPosition="1744">ignment combination only, no translation results were reported. 4 Experiments 4.1 Setup FBIS corpus (LDC2003E14) (210K sentence pairs) was used for small-scale task. A large bilingual corpus of our lab (1.9M sentence pairs) was used for large-scale task. The NIST’06 and NIST’08 test sets were used as the development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as E5Char, which is converted to three E5Words based on three segmentations respectively. These E5Words were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evalu</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of the Association for Computational Linguistics, pages 923-931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="11504" citStr="Koehn et al., 2007" startWordPosition="1823" endWordPosition="1826">s6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as E5Char, which is converted to three E5Words based on three segmentations respectively. These E5Words were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan,W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Association for Computational Linguistics, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="13207" citStr="Koehn (2004)" startWordPosition="2107" endWordPosition="2108">0.8 78.0 78.7 86.3 82.3 CTB 79.6 85.6 82.5 82.2 90.6 86.2 L PKU 80.0 85.4 82.6 81.3 89.5 85.2 ICT 80.0 85.0 82.4 81.3 89.7 85.3 Table 3 Alignment evaluation. Precision (P), recall (R), and F-score (F) with a ൌ 0.5 (Fraser and Marcu, 2007) WSA WSR CTB PKU ICT S word word 21.52 20.99 20.95 char word 22.04 21.98 22.04 L word word 22.07 22.86 22.23 char word 23.41 23.51 23.05 Table 4 Translation evaluation of WordSys and proposed system using BLEU-SBP (Chiang et al., 2008) 4 compares WordSys to our proposed system. Significant testing was carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level. We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010). Comparing Table 4 and 5, we see that all CharSys underperformed WordSys. This observation is consistent with Chang et</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit.</booktitle>
<contexts>
<context position="12040" citStr="Koehn (2005)" startWordPosition="1914" endWordPosition="1915">f Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing WSA and WSR. We then evaluated the translation performance. The baselines are fully word-based MT systems (Word5ys), i.e. using word as both WSA and WSR, and fully character-based systems (Char5ys). Table 5 http://www.ictclas.org/ 6 http://nlp.stanford.edu/software/segmenter.shtml 287 Word alignment Character alignment P R F P R F CTB 76.0 81.9 78.9 78.2 85.2 81.8 S PKU 76.1 82.0 79.0 78.0 86.1 81.9 ICT 75.2 80.8 78.0 78.7 86.3 82.3 CTB 79.6 85.6 82.5 82</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam David Lopez</author>
</authors>
<title>Machine translation by pattern matching.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="7902" citStr="Lopez, 2008" startWordPosition="1228" endWordPosition="1229">ugh these characters preserve different meanings from that of Arafat. This can be attributed to the frequent co-occurrence (192 times) of these characters and Arafat in CCorpus. Moreover,法 usually means France in Chinese, thus it may co-occur very often with France in CCorpus. If both France and Arafat appear in the English sentence, 法 may wrongly align to France. However, if 阿 aligns to Arafat, 法 will probably align to Arafat, because aligning 法 to Arafat could result in a lower distortion cost than aligning it to France. Different from alignment, translation is a pattern matching procedure (Lopez, 2008). WSR determines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word 阿拉法特, i.e. translating literally, we may get a wrong translation. That’s because the linguistic 286 knowledge that the four characters convey a specific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further point</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam David Lopez. 2008. Machine translation by pattern matching. Ph.D. thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Nicolas Stroppa</author>
<author>Andy Way</author>
</authors>
<title>Bootstrapping word alignment via word packing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="2449" citStr="Ma et al., 2007" startWordPosition="356" endWordPosition="359"> segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, vi</context>
<context position="6833" citStr="Ma et al., 2007" startWordPosition="1053" endWordPosition="1056"> 6.47 4 4.26 4.32 5(+) 50.21 29.21 Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, “切尼 (Cheney)” and “愿 (will)” are wrongly merged into one word 切 尼 愿 by the word segmenter, and 切 尼 愿 wrongly aligns to a comma in English sentence in the word alignment; However, both 切 and 尼 align to “Cheney” correctly in the character alignment. However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word (Duan et al., 2010). As character could preserve more meanings than word in Chinese, it seems that a character can be wrongly aligned to many English words by the aligner. However, we found this can be avoided to a great extent by the basic features (co-occurrence and distortion) used by many alignment models. For example, we observed that the four characters of the non-compositional word “阿拉法特 (Arafat)” align to Arafat correctly, although these characters preserve different meanings from that of Arafat. This can be attributed to the frequent co-occurrence (192 times) of these</context>
<context position="14367" citStr="Ma et al. (2007)" startWordPosition="2296" endWordPosition="2299">ed WordSys. This observation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys underperformed our proposed systems, that’s because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa. We finally compared our approaches to Ma et al. (2007) and Ma and Way (2009), which proposed “packed word (PW)” and “bilingual motivated word (BS)” respectively. Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in smallContext Size 7 9 11 13 BLEU 20.90 21.19 20.89 21.09 Table 5 Translation evaluation of CharSys. System WSA WSR CTB PKU ICT WordSys word word 21.52 20.99 20.95 Proposed char word 22.04 21.98 22.04 PW PW PW 21.24 21.24 21.1</context>
</contexts>
<marker>Ma, Stroppa, Way, 2007</marker>
<rawString>Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the Association for Computational Linguistics, pages 304-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the ACL,</booktitle>
<pages>549--557</pages>
<contexts>
<context position="2487" citStr="Ma and Way, 2009" startWordPosition="364" endWordPosition="367">t), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use</context>
<context position="14389" citStr="Ma and Way (2009)" startWordPosition="2301" endWordPosition="2304">rvation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys underperformed our proposed systems, that’s because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa. We finally compared our approaches to Ma et al. (2007) and Ma and Way (2009), which proposed “packed word (PW)” and “bilingual motivated word (BS)” respectively. Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in smallContext Size 7 9 11 13 BLEU 20.90 21.19 20.89 21.09 Table 5 Translation evaluation of CharSys. System WSA WSR CTB PKU ICT WordSys word word 21.52 20.99 20.95 Proposed char word 22.04 21.98 22.04 PW PW PW 21.24 21.24 21.19 Char+PW char PW 22.4</context>
</contexts>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of the Conference of the European Chapter of the ACL, pages 549-557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="11569" citStr="Och, 2003" startWordPosition="1836" endWordPosition="1837">aghighi et al. (2009) were hand-aligned as E5Char, which is converted to three E5Words based on three segmentations respectively. These E5Words were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly di</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Association for Computational Linguistics, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context position="11279" citStr="Och and Ney, 2003" startWordPosition="1787" endWordPosition="1790">he development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as E5Char, which is converted to three E5Words based on three segmentations respectively. These E5Words were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) wit</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), pages 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Integration of multiple bilingually-learned segmentation schemes into statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>400--408</pages>
<contexts>
<context position="2507" citStr="Paul et al., 2010" startWordPosition="368" endWordPosition="371">ermines the basic tokens of translation 1 We hereafter use “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (El</context>
<context position="8488" citStr="Paul et al., (2010)" startWordPosition="1322" endWordPosition="1325"> matching procedure (Lopez, 2008). WSR determines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word 阿拉法特, i.e. translating literally, we may get a wrong translation. That’s because the linguistic 286 knowledge that the four characters convey a specific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sentence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conventional word, as WSR is quite necessary. 3 Converting Character Alignment to Word Alignment In order to use word as WSR, we employ the same method as Elming and Habash (2007)4 to convert the character alignment (CA) to its word-based version (CA’) for translation rule induction. The conversion is very intuitive: for every English-Chinese word pair (a, c) in the sentence pair, we</context>
</contexts>
<marker>Paul, Finch, Sumita, 2010</marker>
<rawString>Michael Paul, Andrew Finch and Eiichiro Sumita. 2010. Integration of multiple bilingually-learned segmentation schemes into statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 400-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Tiedemann</author>
</authors>
<title>Character-based PSMT for closely related languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference of the European Association for machine Translation,</booktitle>
<pages>12--19</pages>
<contexts>
<context position="6102" citStr="Tiedemann, 2009" startWordPosition="924" endWordPosition="925">ion features grows linearly with #UTP. We compare #UT and #UTP of both corpora in Table 2. As can be seen, CCorpus has less UT and UTP than WCorpus, i.e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in statistical modeling (Collins, 1999). Another advantage of character alignment is the reduction in alignment errors caused by word seg3 Several works have proposed to use character (letter) on both sides of the parallel corpus for SMT between similar (European) languages (Vilar et al., 2007; Tiedemann, 2009), however, Chinese is not similar to English. Frequency Characters (%) Words (%) 1 27.22 45.39 2 11.13 14.61 3 6.18 6.47 4 4.26 4.32 5(+) 50.21 29.21 Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, “切尼 (Cheney)” and “愿 (will)” are wrongly merged into one word 切 尼 愿 by the word segmenter, and 切 尼 愿 wrongly aligns to a comma in English sentence in the word alignment; However, both 切 and 尼 align to “Cheney” correctly in the character alignment. However, this kind of</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Jörg Tiedemann. 2009. Character-based PSMT for closely related languages. In Proceedings of the Annual Conference of the European Association for machine Translation, pages 12-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jan-T Peter</author>
<author>Hermann Ney</author>
</authors>
<title>Can we translate letters?</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>33--39</pages>
<contexts>
<context position="6084" citStr="Vilar et al., 2007" startWordPosition="920" endWordPosition="923">ters w.r.t. translation features grows linearly with #UTP. We compare #UT and #UTP of both corpora in Table 2. As can be seen, CCorpus has less UT and UTP than WCorpus, i.e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in statistical modeling (Collins, 1999). Another advantage of character alignment is the reduction in alignment errors caused by word seg3 Several works have proposed to use character (letter) on both sides of the parallel corpus for SMT between similar (European) languages (Vilar et al., 2007; Tiedemann, 2009), however, Chinese is not similar to English. Frequency Characters (%) Words (%) 1 27.22 45.39 2 11.13 14.61 3 6.18 6.47 4 4.26 4.32 5(+) 50.21 29.21 Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, “切尼 (Cheney)” and “愿 (will)” are wrongly merged into one word 切 尼 愿 by the word segmenter, and 切 尼 愿 wrongly aligns to a comma in English sentence in the word alignment; However, both 切 and 尼 align to “Cheney” correctly in the character alignment. How</context>
</contexts>
<marker>Vilar, Peter, Ney, 2007</marker>
<rawString>David Vilar, Jan-T. Peter and Hermann Ney. 2007. Can we translate letters? In Proceedings of the Second Workshop on Statistical Machine Translation, pages 33-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Yang Liu</author>
<author>Young-Sook Hwang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Joint tokenization and translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1200--1208</pages>
<contexts>
<context position="2299" citStr="Xiao et al., 2010" startWordPosition="334" endWordPosition="337">ion for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WS</context>
</contexts>
<marker>Xiao, Liu, Hwang, Liu, Lin, 2010</marker>
<rawString>Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu and Shouxun Lin. 2010. Joint tokenization and translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1200-1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ning Xi</author>
<author>Guangchao Tang</author>
<author>Boyuan Li</author>
<author>Yinggong Zhao</author>
</authors>
<title>Word alignment combination over multiple word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Student Session,</booktitle>
<pages>1--5</pages>
<marker>Xi, Tang, Li, Zhao, 2011</marker>
<rawString>Ning Xi, Guangchao Tang, Boyuan Li, and Yinggong Zhao. 2011. Word alignment combination over multiple word segmentation. In Proceedings of the ACL 2011 Student Session, pages 1-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="2279" citStr="Zhang et al., 2008" startWordPosition="330" endWordPosition="333">us as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specific</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 216-223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>