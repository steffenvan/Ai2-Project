<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.9973865">
Robust Multi-Relational Clustering via i1-Norm Symmetric Nonnegative
Matrix Factorization
</title>
<author confidence="0.994763">
Kai Liu
</author>
<affiliation confidence="0.971544">
Colorado school of Mines
Department of EECS
</affiliation>
<address confidence="0.911871">
Golden, Colorado 80401
</address>
<email confidence="0.999554">
kaliu@mines.edu
</email>
<sectionHeader confidence="0.994817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856133333333">
In this paper, we propose an `1-norm
Symmetric Nonnegative Matrix Tri-
Factorization (�1 S-NMTF) framework
to cluster multi-type relational data by
utilizing their interrelatedness. Due to
introducing the `1-norm distances in our
new objective function, the proposed ap-
proach is robust against noise and outliers,
which are inherent in multi-relational data.
We also derive the solution algorithm and
rigorously analyze its correctness and
convergence. The promising experimental
results of the algorithm applied to text
clustering on IMDB dataset validate the
proposed approach.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999425272727273">
Traditional clustering aims to partition data points
into several groups, such that the data points in
the same group can share some commonalities
whilst those from different groups are dissimilar.
With the recent progresses of Internet and compu-
tational technologies, data have started to appear
in much richer structures. To be more specific, in
many real-world problems a pair of object can be
related in several different ways, which inevitably
complicates the problem and calls for new clus-
tering algorithms for better understanding to the
data. To address this new challenge, Wang et. al.
(Wang et al., 2011c; Wang et al., 2011d) proposed
nonnegative matrix factorization (NMF) (Lee and
Seung, 1999) based computational algorithms that
have successfully solved the problems.
Due to its mathematical elegance and its equiv-
alence to K-means clustering and spectral clus-
tering (Ding et al., 2005), NMF (Lee and Seung,
1999) has been broadly studied in recent years and
successfully solved a variety of practical problems
in data mining and machine learning, such as those
</bodyText>
<note confidence="0.67960525">
Hua Wang
Colorado school of Mines
Department of EECS
Golden, Colorado 80401
</note>
<email confidence="0.976824">
HUAWANGCS@gmail.com
</email>
<bodyText confidence="0.999931033333333">
in computer vision (Wang et al., 2011b), bioinfor-
matics (Wang et al., 2013), natural language un-
derstanding (Wang et al., 2011a), to name a few.
Compared to many traditional clustering meth-
ods, such as K-means clustering, NMF has better
mathematical interpretation, which usually lead
to improved accuracy on clustering (Ding et al.,
2010). Traditional clustering algorithms concen-
trate on dealing with homogeneous data, in which
all the data belong to one single type (Wang et al.,
2011d). To deal with the richer data structures in
modern real-world applications, symmetric Non-
negative Matrix Tri-Factorization (NMTF)(Wang
et al., 2011c) have demonstrated its effectiveness
on simultaneous clustering of multi-type relational
data by utilizing the interrelatedness among differ-
ent data types.
Traditional NMF algorithms routinely use the
least square error functions, which are notably
known to be sensitive against outliers (Kong et al.,
2011). However, at the era of big data outliers are
inevitable due to the ever increasing data sizes. As
a result, developing a more robust NMF model for
multi-relational data clustering has become more
and more important. In this paper, we further de-
velop the symmetric NMF clustering model pro-
posed in (Wang et al., 2011c) by using the `1-norm
distances, such that our new clustering model is
more robust against outliers, which is of particular
importance in multi-relational data.
</bodyText>
<sectionHeader confidence="0.6774245" genericHeader="method">
2 Robust Multi-Relational Clustering via
`1-Norm Symmetric NMTF (S-NMTF)
</sectionHeader>
<bodyText confidence="0.9990985">
In this section, we first introduce the backgrounds
to use symmetric NMF to cluster multi-relational
data. Then we develop our new `1-norm symmet-
ric NMF model for better robustness against outly-
ing data. The solution algorithm to our new model
will be proposed and analyzed in the next section.
Notations. In this paper, we use upper case let-
ters to denote matrices. Given a matrix M, its en-
</bodyText>
<page confidence="0.931174">
397
</page>
<bodyText confidence="0.7303788">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 397–401,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
try at the i-th row and j-th column is denoted as
M(ij). The Frobenius norm of a matrix M is de-
</bodyText>
<page confidence="0.485126">
1/2
</page>
<bodyText confidence="0.9632595">
noted as kMkF = (Ei Ej MSW) and its `1-
norm is denoted as k M k 1 = Ei Ej  |M(ij)
</bodyText>
<subsectionHeader confidence="0.985314">
2.1 Problem Formalization
</subsectionHeader>
<bodyText confidence="0.9680077">
K-type relational data set can be denoted
as χ = {χ1, χ2, . . . , χK} , where χk =
{xk1, xk2, ... , xk � represents the data set of k-th
nk
type. Suppose we are given a set of relationship
matrices {Rkl ∈ &lt;nk×nl}(1≤k≤K,1≤l≤K) between
different types of data objects, then we have Rkl =
RTlk. Our goal is to simultaneously partition the
data objects in χ1, χ2, . . . , χK into c1, c2,. . . , cK
disjoint clusters respectively.
</bodyText>
<subsectionHeader confidence="0.999153">
2.2 Our objective
</subsectionHeader>
<bodyText confidence="0.999875">
To cluster multi-relation data, symmetric NMF has
been taken advantage that solves the following op-
timization problem (Wang et al., 2008):
</bodyText>
<equation confidence="0.991834">
�min J = kRkl − GkSklGTl k2F,
1≤k&lt;l≤K (1)
s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K .
</equation>
<bodyText confidence="0.88565175">
It has also been shown that solving the above
equation is equivalent to solve (Long et al., 2006):
min J = kR − GSGT k2F, s.t. G ≥ 0, (2)
in which
</bodyText>
<equation confidence="0.994643307692308">
⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK
12 1K
⎢ Rn2×n1 0n2×n2 ··· Rn2×nK
⎢ 21 2K
R = ⎢ ⎢ ...
⎣ . .. ....
. .
RnK×n1 RnK×n2 ··· 0nK×nK
K1 K2
Gn1×c1 0n1×c2 ··· 0n1×cK
1
0n2×c1 Gn2×c2 ··· 0n2×cK
2
</equation>
<bodyText confidence="0.999923642857143">
Despite its successfulness of the method pro-
posed in (Wang et al., 2011c) in multi-relational
data clustering, the objectives in Equations (1—2)
use the squared `2-norm distances to measure the
matrix approximation errors, which, though, are
prone to outliers. As a result, the clustering re-
sults could be heavily dominated by outlying data
points with large approximation errors (Kong et
al., 2011; Nie et al., 2011; Wang et al., 2014). To
improve the robustness of the clustering model,
following prior works (Kong et al., 2011; Nie et
al., 2011; Wang et al., 2014) we propose to use
the following `1-norm symmetric NMTF model
for multi-relational data clustering:
</bodyText>
<equation confidence="0.617637">
min J = kR − GSGT k1 s.t. G ≥ 0, (4)
</equation>
<bodyText confidence="0.999929375">
In this new formulation, the approximation errors
are measured by the `1-norm distances, which are
expected to be more insensitive to outlying data
points. As shown in Figure 1, when there ex-
ist outliers in the input data, traditional squared
Frobenius-norm NMF are inclined to cluster in-
correctly, while the `1-norm NMF are more robust
and can cluster more accurately.
</bodyText>
<listItem confidence="0.961962">
Algorithm 1: Algorithm to solve `1-norm S-
NMTF
Data: Relationship matrices: {Rij}1≤i&lt;j≤K
Result: Factor matrices: {Gk}1≤k≤K
1. Construct R, G, S
2. Initialize G as in (Ding et al., 2006).
repeat
3. Construct diagonal matrix D, where
</listItem>
<equation confidence="0.969200125">
� |R−GSGT |i
D(i, i) =
kR−GSGT k2i .
4. Compute
S = (GTG)−1GTRG(GTG)−1.
5. Update �
(RDGS)(ij) �
G(ij) ← G(ij) (GSGT DGS)(ij)
</equation>
<figure confidence="0.947447909090909">
until Converges
⎤
⎦⎥⎥⎥⎥
⎤ ,
⎦⎥⎥⎥⎥
,
1
4
.
,
(3)
</figure>
<sectionHeader confidence="0.576057" genericHeader="method">
3 Algorithm to Solve i1-Norm S-NMTF
</sectionHeader>
<subsectionHeader confidence="0.952345">
and its analysis
</subsectionHeader>
<bodyText confidence="0.996557">
The computational algorithm for the proposed `1-
norm S-NMTF approach is summarized in Algo-
rithm 1 (Due to space limit, the derivation of the
algorithm is skipped and will be provided in our
</bodyText>
<figure confidence="0.998307894736842">
... ... ..
.. ..
0nK×c1 0nK×c2 ··· GnK×cK
K
0c1×c1 Sc1×c2 ··· Sc1×cK
12 1K
Sc2×c1 0c2×c2 ··· Sc2×cK
21 2K
..
.. ..
ScKK1 ×c1 ScK2 ×c2 ··· 0cK ×cK
K
⎤
⎦⎥⎥⎥⎥
... ...
G = ⎡
S = ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎢ ⎢ ⎢ ⎢ ⎣
</figure>
<bodyText confidence="0.782199">
where Rji = RT ij and Sij = STji.
journal version of the paper). Upon solution, the
</bodyText>
<page confidence="0.86521">
398
</page>
<figure confidence="0.9985377">
80
70
60
50
40
30
20
10
0
80
70
60
50
40
30
20
10
0
0 20 40 60 80 100 120
0 20 40 60 80 100 120
</figure>
<figureCaption confidence="0.998871333333333">
Figure 1: Clustering data in two clusters with some outliers (represented as triangle). Left: Clustering
performance by using traditional squared Frobenius-norm NMF algorithm. Right: Clustering perfor-
mance by using the proposed `1-norm NMF algorithm.
</figureCaption>
<bodyText confidence="0.9158542">
final cluster labels are obtained from the resulted
Gk.
The following theorems guarantee the correct-
ness of Algorithm 1 (Due to space limit, the
derivation of the algorithm is skipped and will be
provided in our journal version of the paper).
Theorem 3.1 If the updating rules of G and S in
Algorithm 1 converges, the final solution satisfies
the KKT optimal condition.
This is the fixed point relationships that the so-
lution must satisfy.
The following lemmas and theorem guarantee
the convergence of Algorithm 1 (Due to space
limit, the derivation of the algorithm is skipped
and will be provided in our journal version of the
paper).
Lemma 3.2 (Lee and Seung, 1999) Z(h, h0) is
an auxiliary function of F(h) if the conditions
Z(h, h0) ≥ F(h) and Z(h, h0) = F(h) are sat-
isfied.
</bodyText>
<equation confidence="0.75482">
then the following function
Z(G, G0) =
G(ji)G(kl)
G0 (ji)S(jk)G0 (kl)D(ll)R(li)(1 + log )
G0(ij)G0(kl)
G4
(ij)
(G0SG0T DG0S)(ij) G03
(ij)
</equation>
<bodyText confidence="0.624067666666667">
is an auxiliary function of J(G). Furthermore, it
is a convex function in G and its global minimum
is
</bodyText>
<equation confidence="0.8946645">
(RDGS)(ik) l
G (ik) = G(ik) l(GSGTDGS)(ik)]
</equation>
<bodyText confidence="0.999588">
Based on the property of auxiliary function and
convex function, by updating G, we can always
get the optimal solution to the object function, thus
determining the final cluster label.
</bodyText>
<sectionHeader confidence="0.94487" genericHeader="evaluation">
4 Experiments Result
</sectionHeader>
<bodyText confidence="0.954293666666667">
In this section, We test our proposed algorithm on
IMDB dataset by using its inter-type relationship
information.
</bodyText>
<equation confidence="0.8853102">
�
− 2
ijkl
+�
ij
</equation>
<bodyText confidence="0.961210666666667">
Lemma 3.3 (Lee and Seung, 1999) If Z is an aux-
iliary function for F, then F is non-increasing un-
der the update h(t+1) = arg minh Z(h, h0).
</bodyText>
<equation confidence="0.771672666666667">
Theorem 3.4 Let
J(G) = tr(−2RDGSGT + GSGT DGSGT ),
(5)
</equation>
<subsectionHeader confidence="0.60539">
4.1 Data set
</subsectionHeader>
<bodyText confidence="0.998148142857143">
We use the dataset from ACL-IMDB provided by
(Maas et al., 2011). In this dataset, there is a sub-
training set of 25000 highly polar movie reviews,
in which positive and negative comments come up
with one half(12500) each. The dataset also in-
cludes the following two important files: the con-
tent of each comment and the corresponding URL
</bodyText>
<page confidence="0.963402">
399
</page>
<figure confidence="0.6899495">
1
4
</figure>
<bodyText confidence="0.999967666666667">
where each comment comes from. There are also
some other files but not related with the experi-
ment we conduct, thus we skip them.
</bodyText>
<subsectionHeader confidence="0.995954">
4.2 Experiments settings
</subsectionHeader>
<bodyText confidence="0.999985592592593">
In our experiment, we set the multi-type data as
3 types: author, comment and word. As it is
discussed in the 3rd part, there are three rela-
tionships we need to find, which correspond to
three matrices we need to construct the multi-type
data matrix:comment-author, comment-word and
author-word. By making use of the URL of ev-
ery comment, we can find the author who posts
the corresponding comment, thus we can build the
author-comment matrix.Since each comment with
content is given by the dataset file, we could there-
fore construct the matrix of comment-word, and
the author-word matrix is the product of author-
comment matrix and comment-word matrix.
We could find the first 1500 authors who post
comments most, since the comments from the
same person are more likely to have some corre-
lations, such as similar sentence structures, same
words and etc. We also rule out the stop-words
since they may disturb the clustering and they are
meaningless to the property of comments. To
make our experiments to be more persuasive, we
also add some noise to the three relationship matri-
ces with a ratio of 25 percentage(1/5 in amplitude).
By randomly choosing 500 authors from 1500, we
could generate many sub-datasets to conduct our
experiments.
</bodyText>
<subsectionHeader confidence="0.999284">
4.3 Experiments Results
</subsectionHeader>
<bodyText confidence="0.999702333333333">
We compare the performance of our proposed f1-
norm S-NMTF algorithm with other methods such
as P-NMF, Frobenius norm S-NMTF, traditional
NMF and K-means clustering. For simplicity, we
only compare the clustering accuracy of comment-
word matrix since its label (positive or negative)
is fixed(the grounding label), thus could be com-
pared with the clustering results by using the clus-
tering algorithms.
Table 1 shows that when the data is pure, in
many cases(more than the listed), `1-norm S-
NMTF approach has better performance than oth-
ers
Table 2 illustrates the situation when some noise
is added to the data, it is easy to find that `1-norm
S-NMTF algorithm is the best in terms of cluster-
ing accuracy. This meets our analysis in our Moti-
vation part.
</bodyText>
<table confidence="0.920772">
Alg L11 L22 PMF NMF Kms
set 1 0.578 0.528 0.554 0.510 0.504
set 2 0.583 0.556 0.551 0.521 0.521
set 3 0.584 0.559 0.555 0.501 0.501
set 4 0.551 0.522 0.502 0.527 0.506
set 5 0.566 0.534 0.506 0.529 0.531
set 6 0.558 0.517 0.510 0.535 0.526
</table>
<tableCaption confidence="0.996748">
Table 1: Clustering Accuracy with Pure Data.
</tableCaption>
<table confidence="0.945885142857143">
Alg L11 L22 PMF NMF Kms
sub 1 0.586 0.545 0.508 0.530 0.530
sub 2 0.575 0.535 0.540 0.518 0.532
sub 3 0.567 0.528 0.520 0.500 0.500
sub 4 0.574 0.533 0.525 0.500 0.500
sub 5 0.574 0.537 0.530 0.519 0.518
sub 6 0.556 0.525 0.524 0.504 0.505
</table>
<tableCaption confidence="0.997638">
Table 2: Clustering Accuracy with Noise.
</tableCaption>
<bodyText confidence="0.983244125">
Careful examination in Table 3 reveals the fact
that `1-norm S-NMTF algorithm performs more
robust than any other algorithm. Though the clus-
tering accuracy of `1-norm S-NMTF decreases
when noise exists, still it reduces the least among
the five algorithms. This result convincingly
demonstrates the robustness of our proposed f1-
norm S-NMTF method.
</bodyText>
<table confidence="0.986567857142857">
Alg L11 L22 PMF NMF Kms
s.1(P) 0.547 0.546 0.521 0.546 0.546
s.1(N) 0.546 0.525 0.516 0.540 0.545
s.2(P) 0.543 0.543 0.534 0.543 0.543
s.2(N) 0.543 0.539 0.531 0.513 0.531
s.3(P) 0.536 0.536 0.524 0.536 0.536
s.3(N) 0.536 0.534 0.522 0.517 0.508
</table>
<tableCaption confidence="0.998549">
Table 3: Clustering Accuracy Contrast.
</tableCaption>
<sectionHeader confidence="0.997853" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999035">
In this paper, we presented an `1-norm Symmet-
ric Nonnegative Matrix Tri-Factorization Frame-
work to cluster multi-type relational data simulta-
neously. Our proposed approach clusters different
types of data, using its inter-type relationship by
transforming the original problem into a symmet-
ric NMTF problem. We also presented an auxil-
iary function and high order matrix inequality to
derive the solution algorithm. The proposed algo-
rithm not only makes use of the rich data struc-
</bodyText>
<page confidence="0.976846">
400
</page>
<bodyText confidence="0.999992666666667">
ture to improve the clustering accuracy, but also
remains robust when there is noise and outliers.
Experimental results demonstrate the potential us-
age and advantage of f,-norm S-NMTF in cluster-
ing especially when there are outliers, which is in
accordance with our theory analysis.
</bodyText>
<sectionHeader confidence="0.997879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999168621212121">
C. Ding, X. He, and H.D. Simon. 2005. On the equiv-
alence of nonnegative matrix factorization and spec-
tral clustering. In SDM.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthog-
onal nonnegative matrix t-factorizations for cluster-
ing. In SIGKDD.
C. Ding, T. Li, and M.I. Jordan. 2010. Convex
and semi-nonnegative matrix factorizations. IEEE
TPAMI, 32(1):45–55.
Deguang Kong, Chris Ding, and Heng Huang. 2011.
Robust nonnegative matrix factorization using l21-
norm. In Proceedings of the 20th ACM international
conference on Information and knowledge manage-
ment, pages 673–682. ACM.
D.D. Lee and H.S. Seung. 1999. Learning the parts
of objects by non-negative matrix factorization. Na-
ture, 401(6755):788–791.
Bo Long, Zhongfei Mark Zhang, Xiaoyun Wu, and
Philip S Yu. 2006. Spectral clustering for multi-
type relational data. In Proceedings of the 23rd in-
ternational conference on Machine learning, pages
585–592. ACM.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Feiping Nie, Heng Huang, Chris Ding, Dijun Luo, and
Hua Wang. 2011. Robust principal component anal-
ysis with non-greedy l1-norm maximization. In IJ-
CAI Proceedings-International Joint Conference on
Artificial Intelligence, volume 22, page 1433. Cite-
seer.
F. Wang, T. Li, and C. Zhang. 2008. Semi-supervised
clustering via matrix factorization. In SDM.
H. Wang, H. Huang, F. Nie, and C. Ding. 2011a.
Cross-language web page classification via dual
knowledge transfer using nonnegative matrix tri-
factorization. In SIGIR.
H. Wang, F. Nie, H. Huang, and C. Ding. 2011b.
Dyadic transfer learning for cross-domain image
classification. In ICCV.
Hua Wang, Heng Huang, and Chris Ding. 2011c. Si-
multaneous clustering of multi-type relational data
via symmetric nonnegative matrix tri-factorization.
In Proceedings of the 20th ACM international con-
ference on Information and knowledge management,
pages 279–284. ACM.
Hua Wang, Feiping Nie, Heng Huang, and Chris Ding.
2011d. Nonnegative matrix tri-factorization based
high-order co-clustering and its fast implementation.
In Data Mining (ICDM), 2011 IEEE 11th Interna-
tional Conference on, pages 774–783. IEEE.
Hua Wang, Heng Huang, Chris Ding, and Feiping Nie.
2013. Predicting protein–protein interactions from
multimodal biological data sources via nonnegative
matrix tri-factorization. Journal of Computational
Biology, 20(4):344–358.
Hua Wang, Feiping Nie, and Heng Huang. 2014. Ro-
bust distance metric learning via simultaneous l1-
norm minimization and maximization. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1836–1844.
</reference>
<page confidence="0.998623">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.750682">
<title confidence="0.997102">Multi-Relational Clustering via Symmetric Matrix Factorization</title>
<author confidence="0.999747">Kai Liu</author>
<affiliation confidence="0.9527025">Colorado school of Mines Department of EECS</affiliation>
<address confidence="0.996042">Golden, Colorado 80401</address>
<email confidence="0.999798">kaliu@mines.edu</email>
<abstract confidence="0.9855096875">this paper, we propose an Symmetric Nonnegative Matrix Triframework to cluster multi-type relational data by utilizing their interrelatedness. Due to the distances in our new objective function, the proposed approach is robust against noise and outliers, which are inherent in multi-relational data. We also derive the solution algorithm and rigorously analyze its correctness and convergence. The promising experimental results of the algorithm applied to text clustering on IMDB dataset validate the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Ding</author>
<author>X He</author>
<author>H D Simon</author>
</authors>
<title>On the equivalence of nonnegative matrix factorization and spectral clustering.</title>
<date>2005</date>
<booktitle>In SDM.</booktitle>
<contexts>
<context position="1682" citStr="Ding et al., 2005" startWordPosition="243" endWordPosition="246">arted to appear in much richer structures. To be more specific, in many real-world problems a pair of object can be related in several different ways, which inevitably complicates the problem and calls for new clustering algorithms for better understanding to the data. To address this new challenge, Wang et. al. (Wang et al., 2011c; Wang et al., 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), bioinformatics (Wang et al., 2013), natural language understanding (Wang et al., 2011a), to name a few. Compared to many traditional clustering methods, such as K-means clustering, NMF has better mathematical interpretation, which usually lead to improved accuracy on clustering (Ding et</context>
</contexts>
<marker>Ding, He, Simon, 2005</marker>
<rawString>C. Ding, X. He, and H.D. Simon. 2005. On the equivalence of nonnegative matrix factorization and spectral clustering. In SDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ding</author>
<author>T Li</author>
<author>W Peng</author>
<author>H Park</author>
</authors>
<title>Orthogonal nonnegative matrix t-factorizations for clustering.</title>
<date>2006</date>
<booktitle>In SIGKDD.</booktitle>
<contexts>
<context position="6572" citStr="Ding et al., 2006" startWordPosition="1080" endWordPosition="1083">lational data clustering: min J = kR − GSGT k1 s.t. G ≥ 0, (4) In this new formulation, the approximation errors are measured by the `1-norm distances, which are expected to be more insensitive to outlying data points. As shown in Figure 1, when there exist outliers in the input data, traditional squared Frobenius-norm NMF are inclined to cluster incorrectly, while the `1-norm NMF are more robust and can cluster more accurately. Algorithm 1: Algorithm to solve `1-norm SNMTF Data: Relationship matrices: {Rij}1≤i&lt;j≤K Result: Factor matrices: {Gk}1≤k≤K 1. Construct R, G, S 2. Initialize G as in (Ding et al., 2006). repeat 3. Construct diagonal matrix D, where � |R−GSGT |i D(i, i) = kR−GSGT k2i . 4. Compute S = (GTG)−1GTRG(GTG)−1. 5. Update � (RDGS)(ij) � G(ij) ← G(ij) (GSGT DGS)(ij) until Converges ⎤ ⎦⎥⎥⎥⎥ ⎤ , ⎦⎥⎥⎥⎥ , 1 4 . , (3) 3 Algorithm to Solve i1-Norm S-NMTF and its analysis The computational algorithm for the proposed `1- norm S-NMTF approach is summarized in Algorithm 1 (Due to space limit, the derivation of the algorithm is skipped and will be provided in our ... ... .. .. .. 0nK×c1 0nK×c2 ··· GnK×cK K 0c1×c1 Sc1×c2 ··· Sc1×cK 12 1K Sc2×c1 0c2×c2 ··· Sc2×cK 21 2K .. .. .. ScKK1 ×c1 ScK2 ×c2 ·</context>
</contexts>
<marker>Ding, Li, Peng, Park, 2006</marker>
<rawString>C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogonal nonnegative matrix t-factorizations for clustering. In SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ding</author>
<author>T Li</author>
<author>M I Jordan</author>
</authors>
<title>Convex and semi-nonnegative matrix factorizations.</title>
<date>2010</date>
<journal>IEEE TPAMI,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="2293" citStr="Ding et al., 2010" startWordPosition="338" endWordPosition="341">, 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), bioinformatics (Wang et al., 2013), natural language understanding (Wang et al., 2011a), to name a few. Compared to many traditional clustering methods, such as K-means clustering, NMF has better mathematical interpretation, which usually lead to improved accuracy on clustering (Ding et al., 2010). Traditional clustering algorithms concentrate on dealing with homogeneous data, in which all the data belong to one single type (Wang et al., 2011d). To deal with the richer data structures in modern real-world applications, symmetric Nonnegative Matrix Tri-Factorization (NMTF)(Wang et al., 2011c) have demonstrated its effectiveness on simultaneous clustering of multi-type relational data by utilizing the interrelatedness among different data types. Traditional NMF algorithms routinely use the least square error functions, which are notably known to be sensitive against outliers (Kong et al.</context>
</contexts>
<marker>Ding, Li, Jordan, 2010</marker>
<rawString>C. Ding, T. Li, and M.I. Jordan. 2010. Convex and semi-nonnegative matrix factorizations. IEEE TPAMI, 32(1):45–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deguang Kong</author>
<author>Chris Ding</author>
<author>Heng Huang</author>
</authors>
<title>Robust nonnegative matrix factorization using l21-norm.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management,</booktitle>
<pages>673--682</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2900" citStr="Kong et al., 2011" startWordPosition="425" endWordPosition="428"> al., 2010). Traditional clustering algorithms concentrate on dealing with homogeneous data, in which all the data belong to one single type (Wang et al., 2011d). To deal with the richer data structures in modern real-world applications, symmetric Nonnegative Matrix Tri-Factorization (NMTF)(Wang et al., 2011c) have demonstrated its effectiveness on simultaneous clustering of multi-type relational data by utilizing the interrelatedness among different data types. Traditional NMF algorithms routinely use the least square error functions, which are notably known to be sensitive against outliers (Kong et al., 2011). However, at the era of big data outliers are inevitable due to the ever increasing data sizes. As a result, developing a more robust NMF model for multi-relational data clustering has become more and more important. In this paper, we further develop the symmetric NMF clustering model proposed in (Wang et al., 2011c) by using the `1-norm distances, such that our new clustering model is more robust against outliers, which is of particular importance in multi-relational data. 2 Robust Multi-Relational Clustering via `1-Norm Symmetric NMTF (S-NMTF) In this section, we first introduce the backgro</context>
<context position="5711" citStr="Kong et al., 2011" startWordPosition="933" endWordPosition="936"> GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min J = kR − GSGT k1 s.t. G ≥ 0, (4) In this new formulation, the approximation errors are measured by the `1-norm distances, which are expected to be more insensitive to outlying data points. As shown in Figure 1, when there exist outliers in the input data, traditional squared Frobenius-norm NMF are inclined to cluster incorrec</context>
</contexts>
<marker>Kong, Ding, Huang, 2011</marker>
<rawString>Deguang Kong, Chris Ding, and Heng Huang. 2011. Robust nonnegative matrix factorization using l21-norm. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 673–682. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lee</author>
<author>H S Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<issue>6755</issue>
<contexts>
<context position="1488" citStr="Lee and Seung, 1999" startWordPosition="214" endWordPosition="217">he data points in the same group can share some commonalities whilst those from different groups are dissimilar. With the recent progresses of Internet and computational technologies, data have started to appear in much richer structures. To be more specific, in many real-world problems a pair of object can be related in several different ways, which inevitably complicates the problem and calls for new clustering algorithms for better understanding to the data. To address this new challenge, Wang et. al. (Wang et al., 2011c; Wang et al., 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), bioinformatics (Wang et al., 2013), natural language understanding (Wang et al., 2011a), to n</context>
<context position="8336" citStr="Lee and Seung, 1999" startWordPosition="1417" endWordPosition="1420">ted Gk. The following theorems guarantee the correctness of Algorithm 1 (Due to space limit, the derivation of the algorithm is skipped and will be provided in our journal version of the paper). Theorem 3.1 If the updating rules of G and S in Algorithm 1 converges, the final solution satisfies the KKT optimal condition. This is the fixed point relationships that the solution must satisfy. The following lemmas and theorem guarantee the convergence of Algorithm 1 (Due to space limit, the derivation of the algorithm is skipped and will be provided in our journal version of the paper). Lemma 3.2 (Lee and Seung, 1999) Z(h, h0) is an auxiliary function of F(h) if the conditions Z(h, h0) ≥ F(h) and Z(h, h0) = F(h) are satisfied. then the following function Z(G, G0) = G(ji)G(kl) G0 (ji)S(jk)G0 (kl)D(ll)R(li)(1 + log ) G0(ij)G0(kl) G4 (ij) (G0SG0T DG0S)(ij) G03 (ij) is an auxiliary function of J(G). Furthermore, it is a convex function in G and its global minimum is (RDGS)(ik) l G (ik) = G(ik) l(GSGTDGS)(ik)] Based on the property of auxiliary function and convex function, by updating G, we can always get the optimal solution to the object function, thus determining the final cluster label. 4 Experiments Resul</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>D.D. Lee and H.S. Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Long</author>
<author>Zhongfei Mark Zhang</author>
<author>Xiaoyun Wu</author>
<author>Philip S Yu</author>
</authors>
<title>Spectral clustering for multitype relational data.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning,</booktitle>
<pages>585--592</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5080" citStr="Long et al., 2006" startWordPosition="813" endWordPosition="816"> k-th nk type. Suppose we are given a set of relationship matrices {Rkl ∈ &lt;nk×nl}(1≤k≤K,1≤l≤K) between different types of data objects, then we have Rkl = RTlk. Our goal is to simultaneously partition the data objects in χ1, χ2, . . . , χK into c1, c2,. . . , cK disjoint clusters respectively. 2.2 Our objective To cluster multi-relation data, symmetric NMF has been taken advantage that solves the following optimization problem (Wang et al., 2008): �min J = kRkl − GkSklGTl k2F, 1≤k&lt;l≤K (1) s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K . It has also been shown that solving the above equation is equivalent to solve (Long et al., 2006): min J = kR − GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approxim</context>
</contexts>
<marker>Long, Zhang, Wu, Yu, 2006</marker>
<rawString>Bo Long, Zhongfei Mark Zhang, Xiaoyun Wu, and Philip S Yu. 2006. Spectral clustering for multitype relational data. In Proceedings of the 23rd international conference on Machine learning, pages 585–592. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="9340" citStr="Maas et al., 2011" startWordPosition="1598" endWordPosition="1601">DGS)(ik)] Based on the property of auxiliary function and convex function, by updating G, we can always get the optimal solution to the object function, thus determining the final cluster label. 4 Experiments Result In this section, We test our proposed algorithm on IMDB dataset by using its inter-type relationship information. � − 2 ijkl +� ij Lemma 3.3 (Lee and Seung, 1999) If Z is an auxiliary function for F, then F is non-increasing under the update h(t+1) = arg minh Z(h, h0). Theorem 3.4 Let J(G) = tr(−2RDGSGT + GSGT DGSGT ), (5) 4.1 Data set We use the dataset from ACL-IMDB provided by (Maas et al., 2011). In this dataset, there is a subtraining set of 25000 highly polar movie reviews, in which positive and negative comments come up with one half(12500) each. The dataset also includes the following two important files: the content of each comment and the corresponding URL 399 1 4 where each comment comes from. There are also some other files but not related with the experiment we conduct, thus we skip them. 4.2 Experiments settings In our experiment, we set the multi-type data as 3 types: author, comment and word. As it is discussed in the 3rd part, there are three relationships we need to fin</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feiping Nie</author>
<author>Heng Huang</author>
<author>Chris Ding</author>
<author>Dijun Luo</author>
<author>Hua Wang</author>
</authors>
<title>Robust principal component analysis with non-greedy l1-norm maximization.</title>
<date>2011</date>
<booktitle>In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,</booktitle>
<volume>22</volume>
<pages>1433</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="5729" citStr="Nie et al., 2011" startWordPosition="937" endWordPosition="940"> 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min J = kR − GSGT k1 s.t. G ≥ 0, (4) In this new formulation, the approximation errors are measured by the `1-norm distances, which are expected to be more insensitive to outlying data points. As shown in Figure 1, when there exist outliers in the input data, traditional squared Frobenius-norm NMF are inclined to cluster incorrectly, while the `1-</context>
</contexts>
<marker>Nie, Huang, Ding, Luo, Wang, 2011</marker>
<rawString>Feiping Nie, Heng Huang, Chris Ding, Dijun Luo, and Hua Wang. 2011. Robust principal component analysis with non-greedy l1-norm maximization. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22, page 1433. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wang</author>
<author>T Li</author>
<author>C Zhang</author>
</authors>
<title>Semi-supervised clustering via matrix factorization.</title>
<date>2008</date>
<booktitle>In SDM.</booktitle>
<contexts>
<context position="4912" citStr="Wang et al., 2008" startWordPosition="775" endWordPosition="778"> Ej |M(ij) 2.1 Problem Formalization K-type relational data set can be denoted as χ = {χ1, χ2, . . . , χK} , where χk = {xk1, xk2, ... , xk � represents the data set of k-th nk type. Suppose we are given a set of relationship matrices {Rkl ∈ &lt;nk×nl}(1≤k≤K,1≤l≤K) between different types of data objects, then we have Rkl = RTlk. Our goal is to simultaneously partition the data objects in χ1, χ2, . . . , χK into c1, c2,. . . , cK disjoint clusters respectively. 2.2 Our objective To cluster multi-relation data, symmetric NMF has been taken advantage that solves the following optimization problem (Wang et al., 2008): �min J = kRkl − GkSklGTl k2F, 1≤k&lt;l≤K (1) s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K . It has also been shown that solving the above equation is equivalent to solve (Long et al., 2006): min J = kR − GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matri</context>
</contexts>
<marker>Wang, Li, Zhang, 2008</marker>
<rawString>F. Wang, T. Li, and C. Zhang. 2008. Semi-supervised clustering via matrix factorization. In SDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>H Huang</author>
<author>F Nie</author>
<author>C Ding</author>
</authors>
<title>Cross-language web page classification via dual knowledge transfer using nonnegative matrix trifactorization.</title>
<date>2011</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="1396" citStr="Wang et al., 2011" startWordPosition="201" endWordPosition="204">ion Traditional clustering aims to partition data points into several groups, such that the data points in the same group can share some commonalities whilst those from different groups are dissimilar. With the recent progresses of Internet and computational technologies, data have started to appear in much richer structures. To be more specific, in many real-world problems a pair of object can be related in several different ways, which inevitably complicates the problem and calls for new clustering algorithms for better understanding to the data. To address this new challenge, Wang et. al. (Wang et al., 2011c; Wang et al., 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), b</context>
<context position="3217" citStr="Wang et al., 2011" startWordPosition="480" endWordPosition="483">emonstrated its effectiveness on simultaneous clustering of multi-type relational data by utilizing the interrelatedness among different data types. Traditional NMF algorithms routinely use the least square error functions, which are notably known to be sensitive against outliers (Kong et al., 2011). However, at the era of big data outliers are inevitable due to the ever increasing data sizes. As a result, developing a more robust NMF model for multi-relational data clustering has become more and more important. In this paper, we further develop the symmetric NMF clustering model proposed in (Wang et al., 2011c) by using the `1-norm distances, such that our new clustering model is more robust against outliers, which is of particular importance in multi-relational data. 2 Robust Multi-Relational Clustering via `1-Norm Symmetric NMTF (S-NMTF) In this section, we first introduce the backgrounds to use symmetric NMF to cluster multi-relational data. Then we develop our new `1-norm symmetric NMF model for better robustness against outlying data. The solution algorithm to our new model will be proposed and analyzed in the next section. Notations. In this paper, we use upper case letters to denote matrice</context>
<context position="5384" citStr="Wang et al., 2011" startWordPosition="884" endWordPosition="887">jective To cluster multi-relation data, symmetric NMF has been taken advantage that solves the following optimization problem (Wang et al., 2008): �min J = kRkl − GkSklGTl k2F, 1≤k&lt;l≤K (1) s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K . It has also been shown that solving the above equation is equivalent to solve (Long et al., 2006): min J = kR − GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min </context>
</contexts>
<marker>Wang, Huang, Nie, Ding, 2011</marker>
<rawString>H. Wang, H. Huang, F. Nie, and C. Ding. 2011a. Cross-language web page classification via dual knowledge transfer using nonnegative matrix trifactorization. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>F Nie</author>
<author>H Huang</author>
<author>C Ding</author>
</authors>
<title>Dyadic transfer learning for cross-domain image classification.</title>
<date>2011</date>
<booktitle>In ICCV.</booktitle>
<contexts>
<context position="1396" citStr="Wang et al., 2011" startWordPosition="201" endWordPosition="204">ion Traditional clustering aims to partition data points into several groups, such that the data points in the same group can share some commonalities whilst those from different groups are dissimilar. With the recent progresses of Internet and computational technologies, data have started to appear in much richer structures. To be more specific, in many real-world problems a pair of object can be related in several different ways, which inevitably complicates the problem and calls for new clustering algorithms for better understanding to the data. To address this new challenge, Wang et. al. (Wang et al., 2011c; Wang et al., 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), b</context>
<context position="3217" citStr="Wang et al., 2011" startWordPosition="480" endWordPosition="483">emonstrated its effectiveness on simultaneous clustering of multi-type relational data by utilizing the interrelatedness among different data types. Traditional NMF algorithms routinely use the least square error functions, which are notably known to be sensitive against outliers (Kong et al., 2011). However, at the era of big data outliers are inevitable due to the ever increasing data sizes. As a result, developing a more robust NMF model for multi-relational data clustering has become more and more important. In this paper, we further develop the symmetric NMF clustering model proposed in (Wang et al., 2011c) by using the `1-norm distances, such that our new clustering model is more robust against outliers, which is of particular importance in multi-relational data. 2 Robust Multi-Relational Clustering via `1-Norm Symmetric NMTF (S-NMTF) In this section, we first introduce the backgrounds to use symmetric NMF to cluster multi-relational data. Then we develop our new `1-norm symmetric NMF model for better robustness against outlying data. The solution algorithm to our new model will be proposed and analyzed in the next section. Notations. In this paper, we use upper case letters to denote matrice</context>
<context position="5384" citStr="Wang et al., 2011" startWordPosition="884" endWordPosition="887">jective To cluster multi-relation data, symmetric NMF has been taken advantage that solves the following optimization problem (Wang et al., 2008): �min J = kRkl − GkSklGTl k2F, 1≤k&lt;l≤K (1) s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K . It has also been shown that solving the above equation is equivalent to solve (Long et al., 2006): min J = kR − GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min </context>
</contexts>
<marker>Wang, Nie, Huang, Ding, 2011</marker>
<rawString>H. Wang, F. Nie, H. Huang, and C. Ding. 2011b. Dyadic transfer learning for cross-domain image classification. In ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Heng Huang</author>
<author>Chris Ding</author>
</authors>
<title>Simultaneous clustering of multi-type relational data via symmetric nonnegative matrix tri-factorization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management,</booktitle>
<pages>279--284</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1396" citStr="Wang et al., 2011" startWordPosition="201" endWordPosition="204">ion Traditional clustering aims to partition data points into several groups, such that the data points in the same group can share some commonalities whilst those from different groups are dissimilar. With the recent progresses of Internet and computational technologies, data have started to appear in much richer structures. To be more specific, in many real-world problems a pair of object can be related in several different ways, which inevitably complicates the problem and calls for new clustering algorithms for better understanding to the data. To address this new challenge, Wang et. al. (Wang et al., 2011c; Wang et al., 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), b</context>
<context position="3217" citStr="Wang et al., 2011" startWordPosition="480" endWordPosition="483">emonstrated its effectiveness on simultaneous clustering of multi-type relational data by utilizing the interrelatedness among different data types. Traditional NMF algorithms routinely use the least square error functions, which are notably known to be sensitive against outliers (Kong et al., 2011). However, at the era of big data outliers are inevitable due to the ever increasing data sizes. As a result, developing a more robust NMF model for multi-relational data clustering has become more and more important. In this paper, we further develop the symmetric NMF clustering model proposed in (Wang et al., 2011c) by using the `1-norm distances, such that our new clustering model is more robust against outliers, which is of particular importance in multi-relational data. 2 Robust Multi-Relational Clustering via `1-Norm Symmetric NMTF (S-NMTF) In this section, we first introduce the backgrounds to use symmetric NMF to cluster multi-relational data. Then we develop our new `1-norm symmetric NMF model for better robustness against outlying data. The solution algorithm to our new model will be proposed and analyzed in the next section. Notations. In this paper, we use upper case letters to denote matrice</context>
<context position="5384" citStr="Wang et al., 2011" startWordPosition="884" endWordPosition="887">jective To cluster multi-relation data, symmetric NMF has been taken advantage that solves the following optimization problem (Wang et al., 2008): �min J = kRkl − GkSklGTl k2F, 1≤k&lt;l≤K (1) s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K . It has also been shown that solving the above equation is equivalent to solve (Long et al., 2006): min J = kR − GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min </context>
</contexts>
<marker>Wang, Huang, Ding, 2011</marker>
<rawString>Hua Wang, Heng Huang, and Chris Ding. 2011c. Simultaneous clustering of multi-type relational data via symmetric nonnegative matrix tri-factorization. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 279–284. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Feiping Nie</author>
<author>Heng Huang</author>
<author>Chris Ding</author>
</authors>
<title>Nonnegative matrix tri-factorization based high-order co-clustering and its fast implementation.</title>
<date>2011</date>
<booktitle>In Data Mining (ICDM), 2011 IEEE 11th International Conference on,</booktitle>
<pages>774--783</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1396" citStr="Wang et al., 2011" startWordPosition="201" endWordPosition="204">ion Traditional clustering aims to partition data points into several groups, such that the data points in the same group can share some commonalities whilst those from different groups are dissimilar. With the recent progresses of Internet and computational technologies, data have started to appear in much richer structures. To be more specific, in many real-world problems a pair of object can be related in several different ways, which inevitably complicates the problem and calls for new clustering algorithms for better understanding to the data. To address this new challenge, Wang et. al. (Wang et al., 2011c; Wang et al., 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), b</context>
<context position="3217" citStr="Wang et al., 2011" startWordPosition="480" endWordPosition="483">emonstrated its effectiveness on simultaneous clustering of multi-type relational data by utilizing the interrelatedness among different data types. Traditional NMF algorithms routinely use the least square error functions, which are notably known to be sensitive against outliers (Kong et al., 2011). However, at the era of big data outliers are inevitable due to the ever increasing data sizes. As a result, developing a more robust NMF model for multi-relational data clustering has become more and more important. In this paper, we further develop the symmetric NMF clustering model proposed in (Wang et al., 2011c) by using the `1-norm distances, such that our new clustering model is more robust against outliers, which is of particular importance in multi-relational data. 2 Robust Multi-Relational Clustering via `1-Norm Symmetric NMTF (S-NMTF) In this section, we first introduce the backgrounds to use symmetric NMF to cluster multi-relational data. Then we develop our new `1-norm symmetric NMF model for better robustness against outlying data. The solution algorithm to our new model will be proposed and analyzed in the next section. Notations. In this paper, we use upper case letters to denote matrice</context>
<context position="5384" citStr="Wang et al., 2011" startWordPosition="884" endWordPosition="887">jective To cluster multi-relation data, symmetric NMF has been taken advantage that solves the following optimization problem (Wang et al., 2008): �min J = kRkl − GkSklGTl k2F, 1≤k&lt;l≤K (1) s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K . It has also been shown that solving the above equation is equivalent to solve (Long et al., 2006): min J = kR − GSGT k2F, s.t. G ≥ 0, (2) in which ⎡ 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min </context>
</contexts>
<marker>Wang, Nie, Huang, Ding, 2011</marker>
<rawString>Hua Wang, Feiping Nie, Heng Huang, and Chris Ding. 2011d. Nonnegative matrix tri-factorization based high-order co-clustering and its fast implementation. In Data Mining (ICDM), 2011 IEEE 11th International Conference on, pages 774–783. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Heng Huang</author>
<author>Chris Ding</author>
<author>Feiping Nie</author>
</authors>
<title>Predicting protein–protein interactions from multimodal biological data sources via nonnegative matrix tri-factorization.</title>
<date>2013</date>
<journal>Journal of Computational Biology,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="2029" citStr="Wang et al., 2013" startWordPosition="298" endWordPosition="301"> 2011d) proposed nonnegative matrix factorization (NMF) (Lee and Seung, 1999) based computational algorithms that have successfully solved the problems. Due to its mathematical elegance and its equivalence to K-means clustering and spectral clustering (Ding et al., 2005), NMF (Lee and Seung, 1999) has been broadly studied in recent years and successfully solved a variety of practical problems in data mining and machine learning, such as those Hua Wang Colorado school of Mines Department of EECS Golden, Colorado 80401 HUAWANGCS@gmail.com in computer vision (Wang et al., 2011b), bioinformatics (Wang et al., 2013), natural language understanding (Wang et al., 2011a), to name a few. Compared to many traditional clustering methods, such as K-means clustering, NMF has better mathematical interpretation, which usually lead to improved accuracy on clustering (Ding et al., 2010). Traditional clustering algorithms concentrate on dealing with homogeneous data, in which all the data belong to one single type (Wang et al., 2011d). To deal with the richer data structures in modern real-world applications, symmetric Nonnegative Matrix Tri-Factorization (NMTF)(Wang et al., 2011c) have demonstrated its effectiveness</context>
</contexts>
<marker>Wang, Huang, Ding, Nie, 2013</marker>
<rawString>Hua Wang, Heng Huang, Chris Ding, and Feiping Nie. 2013. Predicting protein–protein interactions from multimodal biological data sources via nonnegative matrix tri-factorization. Journal of Computational Biology, 20(4):344–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Feiping Nie</author>
<author>Heng Huang</author>
</authors>
<title>Robust distance metric learning via simultaneous l1-norm minimization and maximization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1836--1844</pages>
<contexts>
<context position="5749" citStr="Wang et al., 2014" startWordPosition="941" endWordPosition="944"> 0n1×n1 Rn1×n2 · · · Rn1×nK 12 1K ⎢ Rn2×n1 0n2×n2 ··· Rn2×nK ⎢ 21 2K R = ⎢ ⎢ ... ⎣ . .. .... . . RnK×n1 RnK×n2 ··· 0nK×nK K1 K2 Gn1×c1 0n1×c2 ··· 0n1×cK 1 0n2×c1 Gn2×c2 ··· 0n2×cK 2 Despite its successfulness of the method proposed in (Wang et al., 2011c) in multi-relational data clustering, the objectives in Equations (1—2) use the squared `2-norm distances to measure the matrix approximation errors, which, though, are prone to outliers. As a result, the clustering results could be heavily dominated by outlying data points with large approximation errors (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014). To improve the robustness of the clustering model, following prior works (Kong et al., 2011; Nie et al., 2011; Wang et al., 2014) we propose to use the following `1-norm symmetric NMTF model for multi-relational data clustering: min J = kR − GSGT k1 s.t. G ≥ 0, (4) In this new formulation, the approximation errors are measured by the `1-norm distances, which are expected to be more insensitive to outlying data points. As shown in Figure 1, when there exist outliers in the input data, traditional squared Frobenius-norm NMF are inclined to cluster incorrectly, while the `1-norm NMF are more ro</context>
</contexts>
<marker>Wang, Nie, Huang, 2014</marker>
<rawString>Hua Wang, Feiping Nie, and Heng Huang. 2014. Robust distance metric learning via simultaneous l1-norm minimization and maximization. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1836–1844.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>