<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9955035">
Deriving Generalized Knowledge from Corpora using WordNet
Abstraction
</title>
<author confidence="0.999313">
Benjamin Van Durme, Phillip Michalak and Lenhart K. Schubert
</author>
<affiliation confidence="0.933925666666667">
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
</affiliation>
<sectionHeader confidence="0.985272" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965285714286">
Existing work in the extraction of com-
monsense knowledge from text has been
primarily restricted to factoids that serve
as statements about what may possibly ob-
tain in the world. We present an ap-
proach to deriving stronger, more general
claims by abstracting over large sets of
factoids. Our goal is to coalesce the ob-
served nominals for a given predicate ar-
gument into a few predominant types, ob-
tained as WordNet synsets. The results can
be construed as generically quantified sen-
tences restricting the semantic type of an
argument position of a predicate.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996775862069">
Our interest is ultimately in building systems
with commonsense reasoning and language un-
derstanding abilities. As is widely appreciated,
such systems will require large amounts of gen-
eral world knowledge. Large text corpora are
an attractive potential source of such knowledge.
However, current natural language understand-
ing (NLU) methods are not general and reliable
enough to enable broad assimilation, in a formal-
ized representation, of explicitly stated knowledge
in encyclopedias or similar sources. As well, such
sources typically do not cover the most obvious
facts of the world, such as that ice cream may be
delicious and may be coated with chocolate, or
that children may play in parks.
Methods currently exist for extracting simple
“factoids” like those about ice cream and children
just mentioned (see in particular (Schubert, 2002;
Schubert and Tong, 2003)), but these are quite
weak as general claims, and – being unconditional
– are unsuitable for inference chaining. Consider
however the fact that when something is said, it
is generally said by a person, organization or text
source; this a conditional statement dealing with
the potential agents of saying, and could enable
useful inferences. For example, in the sentence,
“The tires were worn and they said I had to re-
place them”, they might be mistakenly identified
with the tires, without the knowledge that saying
is something done primarily by persons, organiza-
tions or text sources. Similarly, looking into the
future one can imagine telling a household robot,
“The cat needs to drink something”, with the ex-
pectation that the robot will take into account that
if a cat drinks something, it is usually water or
milk (whereas people would often have broader
options).
The work reported here is aimed at deriving
generalizations of the latter sort from large sets of
weaker propositions, by examining the hierarchi-
cal relations among sets of types that occur in the
argument positions of verbal or other predicates.
The generalizations we are aiming at are certainly
not the only kinds derivable from text corpora (as
the extensive literature on finding isa-relations,
partonomic relations, paraphrase relations, etc. at-
tests), but as just indicated they do seem poten-
tially useful. Also, thanks to their grounding in
factoids obtained by open knowledge extraction
from large corpora, the propositions obtained are
very broad in scope, unlike knowledge extracted
in a more targeted way.
In the following we first briefly review the
method developed by Schubert and collaborators
to abstract factoids from text; we then outline our
approach to obtaining strengthened propositions
from such sets of factoids. We report positive re-
sults, while making only limited use of standard
</bodyText>
<note confidence="0.9229395">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 808–816,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997658">
808
</page>
<bodyText confidence="0.999841">
corpus statistics, concluding that future endeav-
ors exploring knowledge extraction and WordNet
should go beyond the heuristics employed in re-
cent work.
</bodyText>
<sectionHeader confidence="0.991543" genericHeader="introduction">
2 KNEXT
</sectionHeader>
<bodyText confidence="0.982934611111111">
Schubert (2002) presented an approach to ac-
quiring general world knowledge from text
corpora based on parsing sentences and mapping
syntactic forms into logical forms (LFs), then
gleaning simple propositional factoids from these
LFs through abstraction. Logical forms were
based on Episodic Logic (Schubert and Hwang,
2000), a formalism designed to accommodate in
a straightforward way the semantic phenomena
observed in all languages, such as predication,
logical compounding, generalized quantification,
modification and reification of predicates and
propositions, and event reference. An example
from Schubert and Tong (2003) of factoids
obtained from a sentence in the Brown corpus by
their KNEXT system is the following:
Rilly or Glendora had entered her room while
she slept, bringing back her washed clothes.
</bodyText>
<figure confidence="0.6081736">
A NAMED-ENTITY MAY ENTER A ROOM.
A FEMALE-INDIVIDUAL MAY HAVE A ROOM.
A FEMALE-INDIVIDUAL MAY SLEEP.
A FEMALE-INDIVIDUAL MAY HAVE CLOTHES.
CLOTHES CAN BE WASHED.
</figure>
<equation confidence="0.979460625">
((:I (:Q DET NAMED-ENTITY) ENTER[V]
(:Q THE ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL)SLEEP[V])
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET (:F PLUR CLOTHE[N])))
(:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A]))
</equation>
<bodyText confidence="0.9985144">
Here the upper-case sentences are automatically
generated verbalizations of the abstracted LFs
shown beneath them.1
The initial development of KNEXT was based
on the hand-constructed parse trees in the Penn
Treebank version of the Brown corpus, but sub-
sequently Schubert and collaborators refined and
extended the system to work with parse trees ob-
tained with statistical parsers (e.g., that of Collins
(1997) or Charniak (2000)) applied to larger cor-
pora, such as the British National Corpus (BNC),
a 100 million-word, mixed genre collection, along
with Web corpora of comparable size (see work of
Van Durme et al. (2008) and Van Durme and Schu-
bert (2008) for details). The BNC yielded over 2
</bodyText>
<footnote confidence="0.967457">
1Keywords like :i, :q, and :f are used to indicate in-
fix predication, unscoped quantification, and function appli-
cation, but these details need not concern us here.
</footnote>
<bodyText confidence="0.998287">
factoids per sentence on average, resulting in a to-
tal collection of several million. Human judging of
the factoids indicates that about 2 out of 3 factoids
are perceived as reasonable claims.
The goal in this work, with respect to the ex-
ample given, would be to derive with the use of a
large collection of KNEXT outputs, a general state-
ment such as If something may sleep, it is probably
either an animal or a person.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
3 Resources
</sectionHeader>
<subsectionHeader confidence="0.999981">
3.1 WordNet and Senses
</subsectionHeader>
<bodyText confidence="0.999971617647059">
While the community continues to make gains
in the automatic construction of reliable, general
ontologies, the WordNet sense hierarchy (Fell-
baum, 1998) continues to be the resource of
choice for many computational linguists requiring
an ontology-like structure. In the work discussed
here we explore the potential of WordNet as an un-
derlying concept hierarchy on which to base gen-
eralization decisions.
The use of WordNet raises the challenge of
dealing with multiple semantic concepts associ-
ated with the same word, i.e., employing Word-
Net requires word sense disambiguation in order
to associate terms observed in text with concepts
(synsets) within the hierarchy.
In their work on determining selectional prefer-
ences, both Resnik (1997) and Li and Abe (1998)
relied on uniformly distributing observed frequen-
cies for a given word across all its senses, an ap-
proach later followed by Pantel et al. (2007).2 Oth-
ers within the knowledge acquisition community
have favored taking the first, most dominant sense
of each word (e.g., see Suchanek et al. (2007) and
Pas¸ca (2008)).
As will be seen, our algorithm does not select
word senses prior to generalizing them, but rather
as a byproduct of the abstraction process. More-
over, it potentially selects multiple senses of a
word deemed equally appropriate in a given con-
text, and in that sense provides coarse-grained dis-
ambiguation. This also prevents exaggeration of
the contribution of a term to the abstraction, as a
result of being lexicalized in a particularly fine-
grained way.
</bodyText>
<subsectionHeader confidence="0.999609">
3.2 Propositional Templates
</subsectionHeader>
<footnote confidence="0.946652333333333">
While the procedure given here is not tied to a
particular formalism in representing semantic con-
2Personal communication
</footnote>
<page confidence="0.998606">
809
</page>
<bodyText confidence="0.994281967741935">
text, in our experiments we make use of proposi-
tional templates, based on the verbalizations aris-
ing from KNEXT logical forms. Specifically, a
proposition F with m argument positions gener-
ates m templates, each with one of the arguments
replaced by an empty slot. Hence, the statement,
A MAN MAY GIVE A SPEECH, gives rise to two
templates, A MAN MAY GIVE A , and A MAY
GIVE A SPEECH. Such templates match statements
with identical structure except at the template’s
slots. Thus, the factoid A POLITICIAN MAY GIVE
A SPEECH would match the second template. The
slot-fillers from matching factoids (e.g., MAN and
POLITICIAN form the input lemmas to our abstrac-
tion algorithm described below.
Additional templates are generated by further
weakening predicate argument restrictions. Nouns
in a template that have not been replaced by a free
slot can be replaced with an wild-card, indicating
that anything may fill its position. While slots
accumulate their arguments, these do not, serv-
ing simply as relaxed interpretive constraints on
the original proposition. For the running exam-
ple we would have; A MAY GIVE A ?, and, A ?
MAY GIVE A , yielding observation sets pertain-
ing to things that may give, and things that may be
given.3
We have not restricted our focus to two-
argument verbal predicates; examples such as A
PERSON CAN BE HAPPY WITH A , and, A CAN
BE MAGICAL, can be seen in Section 5.
</bodyText>
<sectionHeader confidence="0.981296" genericHeader="method">
4 Deriving Types
</sectionHeader>
<bodyText confidence="0.9993485">
Our method for type derivation assumes access to
a word sense taxonomy, providing:
</bodyText>
<equation confidence="0.9799822">
W : set of words, potentially multi-token
N : set of nodes, e.g., word senses, or synsets
P : N → {N*} : parent function
S : W → (N+) : sense function
L: N × N→ Q&gt;0 : path length function
</equation>
<bodyText confidence="0.944467555555556">
L is a distance function based on P that gives
the length of the shortest path from a node to a
dominating node, with base case: L(n, n) = 1.
When appropriate, we write L(w, n) to stand for
the arithmetic mean over L(n&apos;, n) for all senses n&apos;
3It is these most general templates that best correlate with
existing work in verb argument preference selection; how-
ever, a given KNEXT logical form may arise from multiple
distinct syntactic constructs.
</bodyText>
<equation confidence="0.855250875">
function SCORE (n ∈ N, α ∈ IlR+, C ⊆ W ⊆ W) :
C&apos; ← D(n) \ C
return E.EC&apos; (w n)
|C1|.
function DERIVETYPES (W ⊆ W, m ∈ ICY+, p ∈ (0, 1]) :
α ← 1, C ← {}, R ← {}
&gt; while too few words covered
while |C |&lt; p × |W |:
n&apos; ← argmin
nEAr\R
R ← R ∪ {n&apos;}
C ←C ∪ D(n&apos;)
if |R |&gt; m :
&gt; cardinality bound exceeded – restart
α ← α + S, C ← {}, R ← {}
return R
</equation>
<figureCaption confidence="0.961612">
Figure 1: Algorithm for deriving slot type restrictions, with
S representing a fixed step size.
</figureCaption>
<bodyText confidence="0.997424257142857">
of w that are dominated by n.4 In the definition of
S, (N+) stands for an ordered list of nodes.
We refer to a given predicate argument position
for a specified propositional template simply as a
slot. W ⊆ W will stand for the set of words found
to occupy a given slot (in the corpus employed),
and D : N→W* is a function mapping a node to
the words it (partially) sense dominates. That is,
for all n ∈ N and w ∈ W, if w ∈ D(n) then
there is at least one sense n&apos; ∈ S(w) such that n is
an ancestor of n&apos; as determined through use of P.
For example, we would expect the word bank to be
dominated by a node standing for a class such as
company as well as a separate node standing for,
e.g., location.
Based on this model we give a greedy search al-
gorithm in Figure 1 for deriving slot type restric-
tions. The algorithm attempts to find a set of dom-
inating word senses that cover at least one of each
of a majority of the words in the given set of obser-
vations. The idea is to keep the number of nodes in
the dominating set small, while maintaining high
coverage and not abstracting too far upward.
For a given slot we start with a set of observed
words W, an upper bound m on the number of
types allowed in the result R, and a parameter p
setting a lower bound on the fraction of items in W
that a valid solution must dominate. For example,
when m = 3 and p = 0.9, this says we require the
solution to consist of no more than 3 nodes, which
together must dominate at least 90% of W.
The search begins with initializing the cover set
C, and the result set R as empty, with the variable
4E.g., both senses of female in WN are dominated by the
node for (organism, being), but have different path lengths.
</bodyText>
<equation confidence="0.743185">
SCORE(n, α, C)
</equation>
<page confidence="0.920508">
810
</page>
<bodyText confidence="0.999981689655172">
α set to 1. Observe that at any point in the exe-
cution of DERIVETYPES, C represents the set of
all words from W with at least one sense having
as an ancestor a node in R. While C continues to
be smaller than the percentage required for a so-
lution, nodes are added to R based on whichever
element of N has the smallest score.
The SCORE function first computes the modi-
fied coverage of n, setting C&apos; to be all words in W
that are dominated by n that haven’t yet been “spo-
ken for” by a previously selected (and thus lower
scoring) node. SCORE returns the sum of the path
lengths between the elements of the modified set
of dominated nodes and n, divided by that set’s
size, scaled by the exponent α. Note when α = 1,
SCORE simply returns the average path length of
the words dominated by n.
If the size of the result grows beyond the speci-
fied threshold, R and C are reset, α is incremented
by some step size S, and the search starts again.
As α grows, the function increasingly favors the
coverage of a node over the summed path length.
Each iteration of DERIVETYPES thus represents a
further relaxation of the desire to have the returned
nodes be as specific as possible. Eventually, α
will be such that the minimum scoring nodes will
be found high enough in the tree to cover enough
of the observations to satisfy the threshold p, at
which point R is returned.
</bodyText>
<subsectionHeader confidence="0.998672">
4.1 Non-reliance on Frequency
</subsectionHeader>
<bodyText confidence="0.941913">
As can be observed, our approach makes no use of
the relative or absolute frequencies of the words in
W, even though such frequencies could be added
as, e.g., relative weights on length in SCORE. This
is a purposeful decision motivated both by practi-
cal and theoretical concerns.
Practically, a large portion of the knowledge ob-
served in KNEXT output is infrequently expressed,
and yet many tend to be reasonable claims about
the world (despite their textual rarity). For ex-
ample, a template shown in Section 5, A MAY
WEAR A CRASH HELMET, was supported by just
two sentences in the BNC. However, based on
those two observations we were able to conclude
that usually If something wears a crash helmet, it
is probably a male person.
Initially our project began as an application of
the closely related MDL approach of Li and Abe
(1998), but was hindered by sparse data. We ob-
served that our absolute frequencies were often too
low to perform meaningful comparisons of rela-
tive frequency, and that different examples in de-
velopment tended to call for different trade-offs
between model cost and coverage. This was due
as much to the sometimes idiosyncratic structure
of WordNet as it was to lack of evidence.5
Theoretically, our goal is distinct from related
efforts in acquiring, e.g., verb argument selec-
tional preferences. That work is based on the de-
sire to reproduce distributional statistics underly-
ing the text, and thus relative differences in fre-
quency are the essential characteristic. In this
work we aim for general statements about the real
world, which in order to gather we rely on text as
a limited proxy view. E.g., given 40 hypothetical
sentences supporting A MAN MAY EAT A TACO,
and just 2 sentences supporting A WOMAN MAY
EAT A TACO, we would like to conclude simply
that A PERSON MAY EAT A TACO, remaining ag-
nostic as to relative frequency, as we’ve no reason
to view corpus-derived counts as (strongly) tied to
the likelihood of corresponding situations in the
world; they simply tell us what is generally possi-
ble and worth mentioning.
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999791">
5.1 Tuning to WordNet
</subsectionHeader>
<bodyText confidence="0.999938222222222">
Our method as described thus far is not tied to a
particular word sense taxonomy. Experiments re-
ported here relied on the following model adjust-
ments in order to make use of WordNet (version
3.0).
The function P was set to return the union of
a synset’s hypernym and instance hypernym rela-
tions.
Regarding the function L, WordNet is con-
structed such that always picking the first sense
of a given nominal tends to be correct more of-
ten than not (see discussion by McCarthy et al.
(2004)). To exploit this structural bias, we em-
ployed a modified version of L that results in
a preference for nodes corresponding to the first
sense of words to be covered, especially when the
number of distinct observations were low (such as
earlier, with crash helmet):
</bodyText>
<equation confidence="0.73085675">
� 1 111,1 ]w E W : S(w) = (n,
L(n, n) =
...)
1 otherwise
</equation>
<bodyText confidence="0.451947">
5For the given example, this method (along with the con-
straints of Table 1) led to the overly general type, living thing.
</bodyText>
<page confidence="0.963463">
811
</page>
<table confidence="0.930029571428571">
word #
abstraction 6
attribute 2
matter 3
physical entity 1
whole 2
gloss
</table>
<tableCaption confidence="0.676324625">
a general concept formed by extracting common features from specific examples
an abstraction belonging to or characteristic of an entity
that which has mass and occupies space
an entity that has physical existence
an assemblage of parts that is regarded as a single entity
Table 1: (word, sense #) pairs in WordNet 3.0 considered overly general for our purposes.
Table 2: Development templates, paired with the number of
distinct words observed to appear in the given slot.
</tableCaption>
<bodyText confidence="0.999391333333333">
Note that when |W  |= 1, then G returns 0 for
the term’s first sense, resulting in a score of 0 for
that synset. This will be the unique minimum,
leading DERIVETYPES to act as the first-sense
heuristic when used with single observations.
Parameters were set for our data based on man-
ual experimentation using the templates seen in
Table 2. We found acceptable results when us-
ing a threshold of p = 70%, and a step size of
S = 0.1. The cardinality bound m was set to 4
when |W  |&gt; 4, and otherwise m = 2.
In addition, we found it desirable to add a few
hard restrictions on the maximum level of general-
ity. Nodes corresponding to the word sense pairs
given in Table 1 were not allowed as abstraction
candidates, nor their ancestors, implemented by
giving infinite length to any path that crossed one
of these synsets.
</bodyText>
<subsectionHeader confidence="0.988063">
5.2 Observations during Development
</subsectionHeader>
<bodyText confidence="0.999944761904762">
Our method assumes that if multiple words occur-
ring in the same slot can be subsumed under the
same abstract class, then this information should
be used to bias sense interpretation of these ob-
served words, even when it means not picking the
first sense. In general this bias is crucial to our ap-
proach, and tends to select correct senses of the
words in an argument set W. But an example
where this strategy errs was observed for the tem-
plate A MAY BARK, which yielded the general-
ization that If something barks, then it is proba-
bly a person. This was because there were numer-
ous textual occurrences of various types of people
“barking” (speaking loudly and aggressively), and
so the occurrences of dogs barking, which showed
no type variability, were interpreted as involving
the unusual sense of dog as a slur applied to cer-
tain people.
The template, A CAN BE WHISKERED, had
observations including both face and head. This
prompted experiments in allowing part holonym
relations (e.g., a face is part of a head) as part
of the definition of P , with the final decision be-
ing that such relations lead to less intuitive gen-
eralizations rather than more, and thus these re-
lation types were not included. The remaining
relation types within WordNet were individually
examined via inspection of randomly selected ex-
amples from the hierarchy. As with holonyms we
decided that using any of these additional relation
types would degrade performance.
A shortcoming was noted in WordNet, regard-
ing its ability to represent binary valued attributes,
based on the template, A CAN BE PREGNANT.
While we were able to successfully generalize to
female person, there were a number of words ob-
served which unexpectedly fell outside that asso-
ciated synset. For example, a queen and a duchess
may each be a female aristocrat, a mum may be a
female parent,6 and a fiancee has the exclusive in-
terpretation as being synonymous with the gender
entailing bride-to-be.
</bodyText>
<sectionHeader confidence="0.999751" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.993108666666667">
From the entire set of BNC-derived KNEXT
propositional templates, evaluations were per-
formed on a set of 21 manually selected examples,
</bodyText>
<footnote confidence="0.980456">
6Serving as a good example of distributional preferencing,
the primary sense of mum is as a flower.
</footnote>
<figure confidence="0.918103114285714">
Propositional Template Num.
A CAN BE WHISKERED
GOVERNORS MAY HAVE -S
A CAN BE PREGNANT
A PERSON MAY BUY A
A MAY BARK
A COMPANY MAY HAVE A
A MAY SMOKE
A CAN BE TASTY
A SONG MAY HAVE A
A CAN BE SUCCESSFUL
CAN BE AT A ROAD
A CAN BE MAGICAL
CAN BE FOR A DICTATOR
MAY FLOAT
GUIDELINES CAN BE FOR -S
A MAY WEAR A CRASH HELMET
A MAY CRASH
4
4
28
105
6
713
8
33
31
664
20
96
5
5
4
2
12
</figure>
<page confidence="0.993698">
812
</page>
<tableCaption confidence="0.999568">
Table 3: Templates chosen for evaluation.
</tableCaption>
<bodyText confidence="0.998861279411765">
together representing the sorts of knowledge for
which we are most interested in deriving strength-
ened argument type restrictions. All modification
of the system ceased prior to the selection of these
templates, and the authors had no knowledge of
the underlying words observed for any particular
slot. Further, some of the templates were purpose-
fully chosen as potentially problematic, such as, A
? MAY OBSERVE A , or A PERSON MAY PAINT
A . Without additional context, templates such
as these were expected to allow for exceptionally
broad sorts of arguments.
For these 21 templates, 65 types were derived,
giving an average of 3.1 types per slot, and allow-
ing for statements such as seen in Table 4.
One way in which to measure the quality of an
argument abstraction is to go back to the under-
lying observed words, and evaluate the resultant
sense(s) implied by the chosen abstraction. We say
senses plural, as the majority of KNEXT propo-
sitions select senses that are more coarse-grained
than WordNet synsets. Thus, we wish to evaluate
these more coarse-grained sense disambiguation
results entailed by our type abstractions.7 We per-
formed this evaluation using as comparisons the
first-sense, and all-senses heuristics.
The first-sense heuristic can be thought of as
striving for maximal specificity at the risk of pre-
cluding some admissible senses (reduced recall),
7Allowing for multiple fine-grained senses to be judged
as appropriate in a given context goes back at least to Sussna
(1993); discussed more recently by, e.g., Navigli (2006).
while the all-senses heuristic insists on including
all admissible senses (perfect recall) at the risk of
including inadmissible ones.
Table 5 gives the results of two judges evaluat-
ing 314 (word, sense) pairs across the 21 selected
templates. These sense pairs correspond to pick-
ing one word at random for each abstracted type
selected for each template slot. Judges were pre-
sented with a sampled word, the originating tem-
plate, and the glosses for each possible word sense
(see Figure 2). Judges did not know ahead of time
the subset of senses selected by the system (as en-
tailed by the derived type abstraction). Taking the
judges’ annotations as the gold standard, we report
precision, recall and F-score with a Q of 0.5 (favor-
ing precision over recall, owing to our preference
for reliable knowledge over more).
In all cases our method gives precision results
comparable or superior to the first-sense heuristic,
while at all times giving higher recall. In partic-
ular, for the case of Primary type, corresponding
to the derived type that accounted for the largest
number of observations for the given argument
slot, our method shows strong performance across
the board, suggesting that our derived abstractions
are general enough to pick up multiple acceptable
senses for observed words, but not so general as to
allow unrelated senses.
We designed an additional test of our method’s
performance, aimed at determining whether the
distinction between admissible senses and inad-
missible ones entailed by our type abstractions
were in accord with human judgement. To this
end, we automatically chose for each template
the observed word that had the greatest num-
ber of senses not dominated by a derived type
</bodyText>
<note confidence="0.333478">
A MAY HAVE A BROTHER
</note>
<footnote confidence="0.710223181818182">
1 WOMAN : an adult female person (as opposed to a
man); ”the woman kept house while the man hunted”
2 WOMAN : a female person who plays a significant
role (wife or mistress or girlfriend) in the life of a partic-
ular man; ”he was faithful to his woman”
3 WOMAN : a human female employed to do house-
work; ”the char will clean the carpet”; ”I have a woman
who comes in four hours a day while I write”
*4 WOMAN: women as a class; ”it’s an insult to Amer-
ican womanhood”; ”woman is the glory of creation”;
”the fair sex gathered on the veranda”
</footnote>
<figureCaption confidence="0.995506666666667">
Figure 2: Example of a context and senses provided for
evaluation, with the fourth sense being judged as inappropri-
ate.
</figureCaption>
<figure confidence="0.795711906976744">
Propositional Template Num.
A MAY HAVE A BROTHER
A ? MAY ATTACK A
A FISH MAY HAVE A
A CAN BE FAMOUS
A ? MAY ENTERTAIN A
A MAY HAVE A CURRENCY
A MALE MAY BUILD A
A CAN BE FAST-GROWING
A PERSON MAY WRITE A
A ? MAY WRITE A
A PERSON MAY TRY TO GET A
A?MAY TRY TO GET A
A MAY FALL DOWN
A PERSON CAN BE HAPPY WITH A
A ? MAY OBSERVE A
A MESSAGE MAY UNDERGO A
A ? MAY WASH A
A PERSON MAY PAINT A
A MAY FLY TOA?
A ? MAY FLY TOA
A CAN BE NERVOUS
28
23
38
665
8
18
42
15
47
99
11
17
5
36
38
14
5
8
9
4
131
</figure>
<page confidence="0.767464">
813
</page>
<construct confidence="0.401020571428571">
If something is famous, it is probably a person1, an artifact1, or a communication2
If ? writes something, it is probably a communication2
If a person is happy with something, it is probably a communication2, a work1, a final result1, or a state of affairs1
If a fish has something, it is probably a cognition1, a torso1, an interior2, or a state2
If something is fast growing, it is probably a group1 or a business3
If a message undergoes something, it is probably a message2, a transmission2, a happening1, or a creation1
If a male builds something, it is probably a structure1, a business3, or a group1
</construct>
<tableCaption confidence="0.939910666666667">
Table 4: Examples, both good and bad, of resultant statements able to be made post-derivation. Authors manually selected
one word from each derived synset, with subscripts referring to sense number. Types are given in order of support, and thus the
first are examples of “Primary” in Table 5.
</tableCaption>
<table confidence="0.999932625">
Method Prec u, F.5 Prec n, F.5 Type
Recall Recall
derived 80.2 39.2 66.4 61.5 47.5 58.1
first 81.5 28.5 59.4 63.1 34.7 54.2 All
all 59.2 100.0 64.5 37.6 100.0 42.9
derived 90.0 50.0 77.6 73.3 71.0 72.8
first 85.7 33.3 65.2 66.7 45.2 60.9 Primary
all 69.2 100.0 73.8 39.7 100.0 45.2
</table>
<tableCaption confidence="0.801362857142857">
Table 5: Precision, Recall and F-score (Q = 0.5) for coarse grained WSD labels using the methods: derive from corpus data,
first-sense heuristic and all-sense heuristic. Results are calculated against both the union Uj and intersection nj of manual
judgements, calculated for all derived argument types, as well as Primary derived types exclusively.
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
</tableCaption>
<listItem confidence="0.5802202">
1. I agree.
2. I lean towards agreement.
3. I’m not sure.
4. I lean towards disagreement.
5. I disagree.
</listItem>
<figureCaption confidence="0.998601">
Figure 3: Instructions for evaluating KNEXT propositions.
</figureCaption>
<bodyText confidence="0.974243409090909">
restriction. For each of these alternative (non-
dominated) senses, we selected the ancestor ly-
ing at the same distance towards the root from the
given sense as the average distance from the dom-
inated senses to the derived type restriction. In
the case where going this far from an alternative
sense towards the root would reach a path passing
through the derived type and one of its subsumed
senses, the distance was cut back until this was no
longer the case.
These alternative senses, guaranteed to not be
dominated by derived type restrictions, were then
presented along with the derived type and the
original template to two judges, who were given
the same instructions as used by Van Durme and
Schubert (2008), which can be found in Figure 3.
Results for this evaluation are found in Table 6,
where we see that the automatically derived type
restrictions are strongly favored over alternative
judge 1 judge 2 corr
derived 1.76 2.10 0.60
alternative 3.63 3.54 0.58
</bodyText>
<tableCaption confidence="0.9870975">
Table 6: Average assessed quality for derived and alterna-
tive synsets, paired with Pearson correlation values.
</tableCaption>
<bodyText confidence="0.999337375">
abstracted types that were possible based on the
given word. Achieving even stronger rejection of
alternative types would be difficult, since KNEXT
templates often provide insufficient context for
full disambiguation of all their constituents, and
judges were allowed to base their assessments on
any interpretation of the verbalization that they
could reasonably come up with.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.994687636363636">
There is a wealth of existing research focused on
learning probabilistic models for selectional re-
strictions on syntactic arguments. Resnik (1993)
used a measure he referred to as selectional pref-
erence strength, based on the KL-divergence be-
tween the probability of a class and that class
given a predicate, with variants explored by Ribas
(1995). Li and Abe (1998) used a tree cut model
over WordNet, based on the principle of Minimum
Description Length (MDL). McCarthy has per-
formed extensive work in the areas of selectional
</bodyText>
<page confidence="0.99365">
814
</page>
<bodyText confidence="0.999199204081633">
preference and WSD, e.g., (McCarthy, 1997; Mc-
Carthy, 2001). Calling the generalization problem
a case of engineering in the face of sparse data,
Clark and Weir (2002) looked at a number of pre-
vious methods, one conclusion being that the ap-
proach of Li and Abe appears to over-generalize.
Cao et al. (2008) gave a distributional method
for deriving semantic restrictions for FrameNet
frames, with the aim of building an Italian
FrameNet. While our goals are related, their work
can be summarized as taking a pre-existing gold
standard, and extending it via distributional simi-
larity measures based on shallow contexts (in this
case, n-gram contexts up to length 5). We have
presented results on strengthening type restrictions
on arbitrary predicate argument structures derived
directly from text.
In describing ALICE, a system for lifelong
learning, Banko and Etzioni (2007) gave a sum-
mary of a proposition abstraction algorithm devel-
oped independently that is in some ways similar
to DERIVETYPES. Beyond differences in node
scoring and their use of the first sense heuristic,
the approach taken here differs in that it makes no
use of relative term frequency, nor contextual in-
formation outside a particular propositional tem-
plate.8 Further, while we are concerned with gen-
eral knowledge acquired over diverse texts, AL-
ICE was built as an agent meant for construct-
ing domain-specific theories, evaluated on a 2.5-
million-page collection of Web documents per-
taining specifically to nutrition.
Minimizing word sense ambiguity by focus-
ing on a specific domain was later seen in the
work of Liakata and Pulman (2008), who per-
formed hierarchical clustering using output from
their KNEXT-like system first described in (Li-
akata and Pulman, 2002). Terminal nodes of the
resultant structure were used as the basis for in-
ferring semantic type restrictions, reminiscent of
the use of CBC clusters (Pantel and Lin, 2002) by
Pantel et al. (2007), for typing the arguments of
paraphrase rules.
Assigning pre-compiled instances to their first-
sense reading in WordNet, Pas¸ca (2008) then gen-
eralized class attributes extracted for these terms,
using as a resource Google search engine query
logs.
Katrenko and Adriaans (2008) explored a con-
</bodyText>
<footnote confidence="0.954593">
8Banko and Etzioni abstracted over subsets of pre-
clustered terms, built using corpus-wide distributional fre-
quencies
</footnote>
<bodyText confidence="0.999454307692308">
strained version of the task considered here. Using
manually annotated semantic relation data from
SemEval-2007, pre-tagged with correct argument
senses, the authors chose the least common sub-
sumer for each argument of each relation consid-
ered. Our approach keeps with the intuition of
preferring specific over general concepts in Word-
Net, but allows for the handling of relations au-
tomatically discovered, whose arguments are not
pre-tagged for sense and tend to be more wide-
ranging. We note that the least common sub-
sumer for many of our predicate arguments would
in most cases be far too abstract.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998477575757576">
As the volume of automatically acquired knowl-
edge grows, it becomes more feasible to abstract
from existential statements to stronger, more gen-
eral claims on what usually obtains in the real
world. Using a method motivated by that used
in deriving selectional preferences for verb argu-
ments, we’ve shown progress in deriving semantic
type restrictions for arbitrary predicate argument
positions, with no prior knowledge of sense in-
formation, and with no training data other than a
handful of examples used to tune a few simple pa-
rameters.
In this work we have made no use of rela-
tive term counts, nor corpus-wide, distributional
frequencies. Despite foregoing these often-used
statistics, our methods outperform abstraction
based on a strict first-sense heuristic, employed in
many related studies.
Future work may include a return to the MDL
approach of Li and Abe (1998), but using a fre-
quency model that “corrects” for the biases in texts
relative to world knowledge – for example, cor-
recting for the preponderance of people as sub-
jects of textual assertions, even for verbs like bark,
glow, or fall, which we know to be applicable to
numerous non-human entities.
Acknowledgements Our thanks to Matthew
Post and Mary Swift for their assistance in eval-
uation, and Daniel Gildea for regular advice. This
research was supported in part by NSF grants IIS-
0328849 and IIS-0535105, as well as a University
of Rochester Provost’s Multidisciplinary Award
(2008).
</bodyText>
<page confidence="0.994491">
815
</page>
<note confidence="0.34431625">
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. ISP: Learning Infer-
ential Selectional Preferences. In Proceedings of NAACL-
HLT.
</note>
<sectionHeader confidence="0.523621" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985081383720931">
Michele Banko and Oren Etzioni. 2007. Strategies for Life-
long Knowledge Extraction from the Web. In Proceedings
of K-CAP.
BNC Consortium. 2001. The British National Corpus, ver-
sion 2 (BNC World). Distributed by Oxford University
Computing Services.
Diego De Cao, Danilo Croce, Marco Pennacchiotti, and
Roberto Basili. 2008. Combining Word Sense and Us-
age for Modeling Frame Semantics. In Proceedings of
Semantics in Text Processing (STEP).
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL.
Stephen Clark and David Weir. 2002. Class-based probabil-
ity estimation using a semantic hierarchy. Computational
Linguistics, 28(2).
Michael Collins. 1997. Three Generative, Lexicalised Mod-
els for Statistical Parsing. In Proceedings of ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
Types of Some Generic Relation Arguments: Detection
and Evaluation. In Proceedings of ACL-HLT.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computational
Linguistics, 24(2).
Maria Liakata and Stephen Pulman. 2002. From Trees to
Predicate Argument Structures. In Proceedings of COL-
ING.
Maria Liakata and Stephen Pulman. 2008. Automatic Fine-
Grained Semantic Classification for Domain Adaption. In
Proceedings of Semantics in Text Processing (STEP).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Using automatically acquired predominant
senses for Word Sense Disambiguation. In Proceedings
of Senseval-3: Third International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text.
Diana McCarthy. 1997. Estimation of a probability distribu-
tion over a hierarchical classification. In The Tenth White
House Papers COGS - CSRP 440.
Diana McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Subcatego-
rization Frames and Selectional Preferences. Ph.D. the-
sis, University of Sussex.
Roberto Navigli. 2006. Meaningful Clustering of Senses
Helps Boost Word Sense Disambiguation Performance. In
Proceedings of COLING-ACL.
Marius Pas¸ca. 2008. Turning Web Text and Search Queries
into Factual Knowledge: Hierarchical Class Attribute Ex-
traction. In Proceedings of AAAI.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses from Text. In Proceedings of KDD.
Philip Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Philip Resnik. 1997. Selectional preference and sense dis-
ambiguation. In Proceedings of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why, What,
and How?
Francesc Ribas. 1995. On learning more appropriate Selec-
tional Restrictions. In Proceedings of EACL.
Lenhart K. Schubert and Chung Hee Hwang. 2000. Episodic
Logic meets Little Red Riding Hood: A comprehensive,
natural representation for language understanding. In
L. Iwanska and S.C. Shapiro, editors, Natural Language
Processing and Knowledge Representation: Language
for Knowledge and Knowledge for Language. MIT/AAAI
Press.
Lenhart K. Schubert and Matthew H. Tong. 2003. Extracting
and evaluating general world knowledge from the brown
corpus. In Proceedings of HLT/NAACL Workshop on Text
Meaning, May 31.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proceedings of HLT.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A Core of Semantic Knowledge Unifying
WordNet and Wikipedia. In Proceedings of WWW.
Michael Sussna. 1993. Word sense disambiguation for free-
text indexing using a massive semantic network. In Pro-
ceedings of CIKM.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
Knowledge Extraction through Compositional Language
Processing. In Proceedings of Semantics in Text Process-
ing (STEP).
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In Proceedings
of COLING.
</reference>
<page confidence="0.998826">
816
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990676">
<title confidence="0.9988045">Deriving Generalized Knowledge from Corpora using WordNet Abstraction</title>
<author confidence="0.999993">Van_Michalak K Schubert</author>
<affiliation confidence="0.999952">Department of Computer Science University of Rochester</affiliation>
<address confidence="0.999949">Rochester, NY 14627, USA</address>
<abstract confidence="0.999523066666667">Existing work in the extraction of commonsense knowledge from text has been primarily restricted to factoids that serve statements about what possibly obtain in the world. We present an approach to deriving stronger, more general claims by abstracting over large sets of factoids. Our goal is to coalesce the observed nominals for a given predicate argument into a few predominant types, obtained as WordNet synsets. The results can be construed as generically quantified sentences restricting the semantic type of an argument position of a predicate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>Strategies for Lifelong Knowledge Extraction from the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of K-CAP.</booktitle>
<contexts>
<context position="30006" citStr="Banko and Etzioni (2007)" startWordPosition="5186" endWordPosition="5189">i and Abe appears to over-generalize. Cao et al. (2008) gave a distributional method for deriving semantic restrictions for FrameNet frames, with the aim of building an Italian FrameNet. While our goals are related, their work can be summarized as taking a pre-existing gold standard, and extending it via distributional similarity measures based on shallow contexts (in this case, n-gram contexts up to length 5). We have presented results on strengthening type restrictions on arbitrary predicate argument structures derived directly from text. In describing ALICE, a system for lifelong learning, Banko and Etzioni (2007) gave a summary of a proposition abstraction algorithm developed independently that is in some ways similar to DERIVETYPES. Beyond differences in node scoring and their use of the first sense heuristic, the approach taken here differs in that it makes no use of relative term frequency, nor contextual information outside a particular propositional template.8 Further, while we are concerned with general knowledge acquired over diverse texts, ALICE was built as an agent meant for constructing domain-specific theories, evaluated on a 2.5- million-page collection of Web documents pertaining specifi</context>
</contexts>
<marker>Banko, Etzioni, 2007</marker>
<rawString>Michele Banko and Oren Etzioni. 2007. Strategies for Lifelong Knowledge Extraction from the Web. In Proceedings of K-CAP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BNC Consortium</author>
</authors>
<date>2001</date>
<journal>The British National Corpus, version</journal>
<volume>2</volume>
<institution>(BNC World). Distributed by Oxford University Computing Services.</institution>
<marker>Consortium, 2001</marker>
<rawString>BNC Consortium. 2001. The British National Corpus, version 2 (BNC World). Distributed by Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego De Cao</author>
<author>Danilo Croce</author>
<author>Marco Pennacchiotti</author>
<author>Roberto Basili</author>
</authors>
<title>Combining Word Sense and Usage for Modeling Frame Semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of Semantics in Text Processing (STEP).</booktitle>
<marker>De Cao, Croce, Pennacchiotti, Basili, 2008</marker>
<rawString>Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili. 2008. Combining Word Sense and Usage for Modeling Frame Semantics. In Proceedings of Semantics in Text Processing (STEP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="5556" citStr="Charniak (2000)" startWordPosition="855" endWordPosition="856">IDUAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL)SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of KNEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al. (2008) and Van Durme and Schubert (2008) for details). The BNC yielded over 2 1Keywords like :i, :q, and :f are used to indicate infix predication, unscoped quantification, and function application, but these details need not concern us here. factoids per sentence on average, resulting in a total collection of several million. Human judging of the factoids indicates that about 2 out of 3 factoids are perceive</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="29298" citStr="Clark and Weir (2002)" startWordPosition="5075" endWordPosition="5078">s for selectional restrictions on syntactic arguments. Resnik (1993) used a measure he referred to as selectional preference strength, based on the KL-divergence between the probability of a class and that class given a predicate, with variants explored by Ribas (1995). Li and Abe (1998) used a tree cut model over WordNet, based on the principle of Minimum Description Length (MDL). McCarthy has performed extensive work in the areas of selectional 814 preference and WSD, e.g., (McCarthy, 1997; McCarthy, 2001). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Cao et al. (2008) gave a distributional method for deriving semantic restrictions for FrameNet frames, with the aim of building an Italian FrameNet. While our goals are related, their work can be summarized as taking a pre-existing gold standard, and extending it via distributional similarity measures based on shallow contexts (in this case, n-gram contexts up to length 5). We have presented results on strengthening type restrictions on arbitrary predicate argument structur</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5537" citStr="Collins (1997)" startWordPosition="852" endWordPosition="853">Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL)SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of KNEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al. (2008) and Van Durme and Schubert (2008) for details). The BNC yielded over 2 1Keywords like :i, :q, and :f are used to indicate infix predication, unscoped quantification, and function application, but these details need not concern us here. factoids per sentence on average, resulting in a total collection of several million. Human judging of the factoids indicates that about 2 out of 3 fa</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6593" citStr="Fellbaum, 1998" startWordPosition="1033" endWordPosition="1035">us here. factoids per sentence on average, resulting in a total collection of several million. Human judging of the factoids indicates that about 2 out of 3 factoids are perceived as reasonable claims. The goal in this work, with respect to the example given, would be to derive with the use of a large collection of KNEXT outputs, a general statement such as If something may sleep, it is probably either an animal or a person. 3 Resources 3.1 WordNet and Senses While the community continues to make gains in the automatic construction of reliable, general ontologies, the WordNet sense hierarchy (Fellbaum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Katrenko</author>
<author>Pieter Adriaans</author>
</authors>
<title>Semantic Types of Some Generic Relation Arguments: Detection and Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="31344" citStr="Katrenko and Adriaans (2008)" startWordPosition="5399" endWordPosition="5402">k of Liakata and Pulman (2008), who performed hierarchical clustering using output from their KNEXT-like system first described in (Liakata and Pulman, 2002). Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al. (2007), for typing the arguments of paraphrase rules. Assigning pre-compiled instances to their firstsense reading in WordNet, Pas¸ca (2008) then generalized class attributes extracted for these terms, using as a resource Google search engine query logs. Katrenko and Adriaans (2008) explored a con8Banko and Etzioni abstracted over subsets of preclustered terms, built using corpus-wide distributional frequencies strained version of the task considered here. Using manually annotated semantic relation data from SemEval-2007, pre-tagged with correct argument senses, the authors chose the least common subsumer for each argument of each relation considered. Our approach keeps with the intuition of preferring specific over general concepts in WordNet, but allows for the handling of relations automatically discovered, whose arguments are not pre-tagged for sense and tend to be m</context>
</contexts>
<marker>Katrenko, Adriaans, 2008</marker>
<rawString>Sophia Katrenko and Pieter Adriaans. 2008. Semantic Types of Some Generic Relation Arguments: Detection and Evaluation. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="7203" citStr="Li and Abe (1998)" startWordPosition="1128" endWordPosition="1131">, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sense provides coarse-gra</context>
<context position="14636" citStr="Li and Abe (1998)" startWordPosition="2517" endWordPosition="2520"> decision motivated both by practical and theoretical concerns. Practically, a large portion of the knowledge observed in KNEXT output is infrequently expressed, and yet many tend to be reasonable claims about the world (despite their textual rarity). For example, a template shown in Section 5, A MAY WEAR A CRASH HELMET, was supported by just two sentences in the BNC. However, based on those two observations we were able to conclude that usually If something wears a crash helmet, it is probably a male person. Initially our project began as an application of the closely related MDL approach of Li and Abe (1998), but was hindered by sparse data. We observed that our absolute frequencies were often too low to perform meaningful comparisons of relative frequency, and that different examples in development tended to call for different trade-offs between model cost and coverage. This was due as much to the sometimes idiosyncratic structure of WordNet as it was to lack of evidence.5 Theoretically, our goal is distinct from related efforts in acquiring, e.g., verb argument selectional preferences. That work is based on the desire to reproduce distributional statistics underlying the text, and thus relative</context>
<context position="28965" citStr="Li and Abe (1998)" startWordPosition="5020" endWordPosition="5023">KNEXT templates often provide insufficient context for full disambiguation of all their constituents, and judges were allowed to base their assessments on any interpretation of the verbalization that they could reasonably come up with. 7 Related Work There is a wealth of existing research focused on learning probabilistic models for selectional restrictions on syntactic arguments. Resnik (1993) used a measure he referred to as selectional preference strength, based on the KL-divergence between the probability of a class and that class given a predicate, with variants explored by Ribas (1995). Li and Abe (1998) used a tree cut model over WordNet, based on the principle of Minimum Description Length (MDL). McCarthy has performed extensive work in the areas of selectional 814 preference and WSD, e.g., (McCarthy, 1997; McCarthy, 2001). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Cao et al. (2008) gave a distributional method for deriving semantic restrictions for FrameNet frames, with the aim of building an Italian FrameN</context>
<context position="32962" citStr="Li and Abe (1998)" startWordPosition="5656" endWordPosition="5659">onal preferences for verb arguments, we’ve shown progress in deriving semantic type restrictions for arbitrary predicate argument positions, with no prior knowledge of sense information, and with no training data other than a handful of examples used to tune a few simple parameters. In this work we have made no use of relative term counts, nor corpus-wide, distributional frequencies. Despite foregoing these often-used statistics, our methods outperform abstraction based on a strict first-sense heuristic, employed in many related studies. Future work may include a return to the MDL approach of Li and Abe (1998), but using a frequency model that “corrects” for the biases in texts relative to world knowledge – for example, correcting for the preponderance of people as subjects of textual assertions, even for verbs like bark, glow, or fall, which we know to be applicable to numerous non-human entities. Acknowledgements Our thanks to Matthew Post and Mary Swift for their assistance in evaluation, and Daniel Gildea for regular advice. This research was supported in part by NSF grants IIS0328849 and IIS-0535105, as well as a University of Rochester Provost’s Multidisciplinary Award (2008). 815 Patrick Pan</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Liakata</author>
<author>Stephen Pulman</author>
</authors>
<title>From Trees to Predicate Argument Structures.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="30873" citStr="Liakata and Pulman, 2002" startWordPosition="5324" endWordPosition="5328"> it makes no use of relative term frequency, nor contextual information outside a particular propositional template.8 Further, while we are concerned with general knowledge acquired over diverse texts, ALICE was built as an agent meant for constructing domain-specific theories, evaluated on a 2.5- million-page collection of Web documents pertaining specifically to nutrition. Minimizing word sense ambiguity by focusing on a specific domain was later seen in the work of Liakata and Pulman (2008), who performed hierarchical clustering using output from their KNEXT-like system first described in (Liakata and Pulman, 2002). Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al. (2007), for typing the arguments of paraphrase rules. Assigning pre-compiled instances to their firstsense reading in WordNet, Pas¸ca (2008) then generalized class attributes extracted for these terms, using as a resource Google search engine query logs. Katrenko and Adriaans (2008) explored a con8Banko and Etzioni abstracted over subsets of preclustered terms, built using corpus-wide distributional frequenci</context>
</contexts>
<marker>Liakata, Pulman, 2002</marker>
<rawString>Maria Liakata and Stephen Pulman. 2002. From Trees to Predicate Argument Structures. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Liakata</author>
<author>Stephen Pulman</author>
</authors>
<title>Automatic FineGrained Semantic Classification for Domain Adaption.</title>
<date>2008</date>
<booktitle>In Proceedings of Semantics in Text Processing (STEP).</booktitle>
<contexts>
<context position="30746" citStr="Liakata and Pulman (2008)" startWordPosition="5306" endWordPosition="5309">VETYPES. Beyond differences in node scoring and their use of the first sense heuristic, the approach taken here differs in that it makes no use of relative term frequency, nor contextual information outside a particular propositional template.8 Further, while we are concerned with general knowledge acquired over diverse texts, ALICE was built as an agent meant for constructing domain-specific theories, evaluated on a 2.5- million-page collection of Web documents pertaining specifically to nutrition. Minimizing word sense ambiguity by focusing on a specific domain was later seen in the work of Liakata and Pulman (2008), who performed hierarchical clustering using output from their KNEXT-like system first described in (Liakata and Pulman, 2002). Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al. (2007), for typing the arguments of paraphrase rules. Assigning pre-compiled instances to their firstsense reading in WordNet, Pas¸ca (2008) then generalized class attributes extracted for these terms, using as a resource Google search engine query logs. Katrenko and Adriaans (2008) e</context>
</contexts>
<marker>Liakata, Pulman, 2008</marker>
<rawString>Maria Liakata and Stephen Pulman. 2008. Automatic FineGrained Semantic Classification for Domain Adaption. In Proceedings of Semantics in Text Processing (STEP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Using automatically acquired predominant senses for Word Sense Disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<contexts>
<context position="16381" citStr="McCarthy et al. (2004)" startWordPosition="2817" endWordPosition="2820">nding situations in the world; they simply tell us what is generally possible and worth mentioning. 5 Experiments 5.1 Tuning to WordNet Our method as described thus far is not tied to a particular word sense taxonomy. Experiments reported here relied on the following model adjustments in order to make use of WordNet (version 3.0). The function P was set to return the union of a synset’s hypernym and instance hypernym relations. Regarding the function L, WordNet is constructed such that always picking the first sense of a given nominal tends to be correct more often than not (see discussion by McCarthy et al. (2004)). To exploit this structural bias, we employed a modified version of L that results in a preference for nodes corresponding to the first sense of words to be covered, especially when the number of distinct observations were low (such as earlier, with crash helmet): � 1 111,1 ]w E W : S(w) = (n, L(n, n) = ...) 1 otherwise 5For the given example, this method (along with the constraints of Table 1) led to the overly general type, living thing. 811 word # abstraction 6 attribute 2 matter 3 physical entity 1 whole 2 gloss a general concept formed by extracting common features from specific example</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Using automatically acquired predominant senses for Word Sense Disambiguation. In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Estimation of a probability distribution over a hierarchical classification.</title>
<date>1997</date>
<booktitle>In The Tenth White House Papers COGS - CSRP 440.</booktitle>
<contexts>
<context position="29173" citStr="McCarthy, 1997" startWordPosition="5056" endWordPosition="5057">reasonably come up with. 7 Related Work There is a wealth of existing research focused on learning probabilistic models for selectional restrictions on syntactic arguments. Resnik (1993) used a measure he referred to as selectional preference strength, based on the KL-divergence between the probability of a class and that class given a predicate, with variants explored by Ribas (1995). Li and Abe (1998) used a tree cut model over WordNet, based on the principle of Minimum Description Length (MDL). McCarthy has performed extensive work in the areas of selectional 814 preference and WSD, e.g., (McCarthy, 1997; McCarthy, 2001). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Cao et al. (2008) gave a distributional method for deriving semantic restrictions for FrameNet frames, with the aim of building an Italian FrameNet. While our goals are related, their work can be summarized as taking a pre-existing gold standard, and extending it via distributional similarity measures based on shallow contexts (in this case, n-gram co</context>
</contexts>
<marker>McCarthy, 1997</marker>
<rawString>Diana McCarthy. 1997. Estimation of a probability distribution over a hierarchical classification. In The Tenth White House Papers COGS - CSRP 440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical Acquisition at the SyntaxSemantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="29190" citStr="McCarthy, 2001" startWordPosition="5058" endWordPosition="5060">up with. 7 Related Work There is a wealth of existing research focused on learning probabilistic models for selectional restrictions on syntactic arguments. Resnik (1993) used a measure he referred to as selectional preference strength, based on the KL-divergence between the probability of a class and that class given a predicate, with variants explored by Ribas (1995). Li and Abe (1998) used a tree cut model over WordNet, based on the principle of Minimum Description Length (MDL). McCarthy has performed extensive work in the areas of selectional 814 preference and WSD, e.g., (McCarthy, 1997; McCarthy, 2001). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Cao et al. (2008) gave a distributional method for deriving semantic restrictions for FrameNet frames, with the aim of building an Italian FrameNet. While our goals are related, their work can be summarized as taking a pre-existing gold standard, and extending it via distributional similarity measures based on shallow contexts (in this case, n-gram contexts up to leng</context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>Diana McCarthy. 2001. Lexical Acquisition at the SyntaxSemantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="22397" citStr="Navigli (2006)" startWordPosition="3871" endWordPosition="3872">XT propositions select senses that are more coarse-grained than WordNet synsets. Thus, we wish to evaluate these more coarse-grained sense disambiguation results entailed by our type abstractions.7 We performed this evaluation using as comparisons the first-sense, and all-senses heuristics. The first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses (reduced recall), 7Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna (1993); discussed more recently by, e.g., Navigli (2006). while the all-senses heuristic insists on including all admissible senses (perfect recall) at the risk of including inadmissible ones. Table 5 gives the results of two judges evaluating 314 (word, sense) pairs across the 21 selected templates. These sense pairs correspond to picking one word at random for each abstracted type selected for each template slot. Judges were presented with a sampled word, the originating template, and the glosses for each possible word sense (see Figure 2). Judges did not know ahead of time the subset of senses selected by the system (as entailed by the derived t</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Turning Web Text and Search Queries into Factual Knowledge: Hierarchical Class Attribute Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<marker>Pas¸ca, 2008</marker>
<rawString>Marius Pas¸ca. 2008. Turning Web Text and Search Queries into Factual Knowledge: Hierarchical Class Attribute Extraction. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="31043" citStr="Pantel and Lin, 2002" startWordPosition="5353" endWordPosition="5356">cquired over diverse texts, ALICE was built as an agent meant for constructing domain-specific theories, evaluated on a 2.5- million-page collection of Web documents pertaining specifically to nutrition. Minimizing word sense ambiguity by focusing on a specific domain was later seen in the work of Liakata and Pulman (2008), who performed hierarchical clustering using output from their KNEXT-like system first described in (Liakata and Pulman, 2002). Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al. (2007), for typing the arguments of paraphrase rules. Assigning pre-compiled instances to their firstsense reading in WordNet, Pas¸ca (2008) then generalized class attributes extracted for these terms, using as a resource Google search engine query logs. Katrenko and Adriaans (2008) explored a con8Banko and Etzioni abstracted over subsets of preclustered terms, built using corpus-wide distributional frequencies strained version of the task considered here. Using manually annotated semantic relation data from SemEval-2007, pre-tagged with correct argument senses, the authors c</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering Word Senses from Text. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A ClassBased Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="28745" citStr="Resnik (1993)" startWordPosition="4984" endWordPosition="4985"> derived and alternative synsets, paired with Pearson correlation values. abstracted types that were possible based on the given word. Achieving even stronger rejection of alternative types would be difficult, since KNEXT templates often provide insufficient context for full disambiguation of all their constituents, and judges were allowed to base their assessments on any interpretation of the verbalization that they could reasonably come up with. 7 Related Work There is a wealth of existing research focused on learning probabilistic models for selectional restrictions on syntactic arguments. Resnik (1993) used a measure he referred to as selectional preference strength, based on the KL-divergence between the probability of a class and that class given a predicate, with variants explored by Ribas (1995). Li and Abe (1998) used a tree cut model over WordNet, based on the principle of Minimum Description Length (MDL). McCarthy has performed extensive work in the areas of selectional 814 preference and WSD, e.g., (McCarthy, 1997; McCarthy, 2001). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one co</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A ClassBased Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why,</booktitle>
<location>What, and How?</location>
<contexts>
<context position="7181" citStr="Resnik (1997)" startWordPosition="1125" endWordPosition="1126">ierarchy (Fellbaum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sen</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas</author>
</authors>
<title>On learning more appropriate Selectional Restrictions.</title>
<date>1995</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="28946" citStr="Ribas (1995)" startWordPosition="5018" endWordPosition="5019">ficult, since KNEXT templates often provide insufficient context for full disambiguation of all their constituents, and judges were allowed to base their assessments on any interpretation of the verbalization that they could reasonably come up with. 7 Related Work There is a wealth of existing research focused on learning probabilistic models for selectional restrictions on syntactic arguments. Resnik (1993) used a measure he referred to as selectional preference strength, based on the KL-divergence between the probability of a class and that class given a predicate, with variants explored by Ribas (1995). Li and Abe (1998) used a tree cut model over WordNet, based on the principle of Minimum Description Length (MDL). McCarthy has performed extensive work in the areas of selectional 814 preference and WSD, e.g., (McCarthy, 1997; McCarthy, 2001). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Cao et al. (2008) gave a distributional method for deriving semantic restrictions for FrameNet frames, with the aim of buildin</context>
</contexts>
<marker>Ribas, 1995</marker>
<rawString>Francesc Ribas. 1995. On learning more appropriate Selectional Restrictions. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
<author>Chung Hee Hwang</author>
</authors>
<title>Episodic Logic meets Little Red Riding Hood: A comprehensive, natural representation for language understanding.</title>
<date>2000</date>
<booktitle>Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language.</booktitle>
<editor>In L. Iwanska and S.C. Shapiro, editors,</editor>
<publisher>MIT/AAAI Press.</publisher>
<contexts>
<context position="4208" citStr="Schubert and Hwang, 2000" startWordPosition="654" endWordPosition="657">the European Chapter of the ACL, pages 808–816, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 808 corpus statistics, concluding that future endeavors exploring knowledge extraction and WordNet should go beyond the heuristics employed in recent work. 2 KNEXT Schubert (2002) presented an approach to acquiring general world knowledge from text corpora based on parsing sentences and mapping syntactic forms into logical forms (LFs), then gleaning simple propositional factoids from these LFs through abstraction. Logical forms were based on Episodic Logic (Schubert and Hwang, 2000), a formalism designed to accommodate in a straightforward way the semantic phenomena observed in all languages, such as predication, logical compounding, generalized quantification, modification and reification of predicates and propositions, and event reference. An example from Schubert and Tong (2003) of factoids obtained from a sentence in the Brown corpus by their KNEXT system is the following: Rilly or Glendora had entered her room while she slept, bringing back her washed clothes. A NAMED-ENTITY MAY ENTER A ROOM. A FEMALE-INDIVIDUAL MAY HAVE A ROOM. A FEMALE-INDIVIDUAL MAY SLEEP. A FEMA</context>
</contexts>
<marker>Schubert, Hwang, 2000</marker>
<rawString>Lenhart K. Schubert and Chung Hee Hwang. 2000. Episodic Logic meets Little Red Riding Hood: A comprehensive, natural representation for language understanding. In L. Iwanska and S.C. Shapiro, editors, Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language. MIT/AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
<author>Matthew H Tong</author>
</authors>
<title>Extracting and evaluating general world knowledge from the brown corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL Workshop on Text Meaning,</booktitle>
<contexts>
<context position="1665" citStr="Schubert and Tong, 2003" startWordPosition="255" endWordPosition="258">ntial source of such knowledge. However, current natural language understanding (NLU) methods are not general and reliable enough to enable broad assimilation, in a formalized representation, of explicitly stated knowledge in encyclopedias or similar sources. As well, such sources typically do not cover the most obvious facts of the world, such as that ice cream may be delicious and may be coated with chocolate, or that children may play in parks. Methods currently exist for extracting simple “factoids” like those about ice cream and children just mentioned (see in particular (Schubert, 2002; Schubert and Tong, 2003)), but these are quite weak as general claims, and – being unconditional – are unsuitable for inference chaining. Consider however the fact that when something is said, it is generally said by a person, organization or text source; this a conditional statement dealing with the potential agents of saying, and could enable useful inferences. For example, in the sentence, “The tires were worn and they said I had to replace them”, they might be mistakenly identified with the tires, without the knowledge that saying is something done primarily by persons, organizations or text sources. Similarly, l</context>
<context position="4513" citStr="Schubert and Tong (2003)" startWordPosition="694" endWordPosition="697">bert (2002) presented an approach to acquiring general world knowledge from text corpora based on parsing sentences and mapping syntactic forms into logical forms (LFs), then gleaning simple propositional factoids from these LFs through abstraction. Logical forms were based on Episodic Logic (Schubert and Hwang, 2000), a formalism designed to accommodate in a straightforward way the semantic phenomena observed in all languages, such as predication, logical compounding, generalized quantification, modification and reification of predicates and propositions, and event reference. An example from Schubert and Tong (2003) of factoids obtained from a sentence in the Brown corpus by their KNEXT system is the following: Rilly or Glendora had entered her room while she slept, bringing back her washed clothes. A NAMED-ENTITY MAY ENTER A ROOM. A FEMALE-INDIVIDUAL MAY HAVE A ROOM. A FEMALE-INDIVIDUAL MAY SLEEP. A FEMALE-INDIVIDUAL MAY HAVE CLOTHES. CLOTHES CAN BE WASHED. ((:I (:Q DET NAMED-ENTITY) ENTER[V] (:Q THE ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL)SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N]</context>
</contexts>
<marker>Schubert, Tong, 2003</marker>
<rawString>Lenhart K. Schubert and Matthew H. Tong. 2003. Extracting and evaluating general world knowledge from the brown corpus. In Proceedings of HLT/NAACL Workshop on Text Meaning, May 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
</authors>
<title>Can we derive general world knowledge from texts?</title>
<date>2002</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="1639" citStr="Schubert, 2002" startWordPosition="253" endWordPosition="254"> attractive potential source of such knowledge. However, current natural language understanding (NLU) methods are not general and reliable enough to enable broad assimilation, in a formalized representation, of explicitly stated knowledge in encyclopedias or similar sources. As well, such sources typically do not cover the most obvious facts of the world, such as that ice cream may be delicious and may be coated with chocolate, or that children may play in parks. Methods currently exist for extracting simple “factoids” like those about ice cream and children just mentioned (see in particular (Schubert, 2002; Schubert and Tong, 2003)), but these are quite weak as general claims, and – being unconditional – are unsuitable for inference chaining. Consider however the fact that when something is said, it is generally said by a person, organization or text source; this a conditional statement dealing with the potential agents of saying, and could enable useful inferences. For example, in the sentence, “The tires were worn and they said I had to replace them”, they might be mistakenly identified with the tires, without the knowledge that saying is something done primarily by persons, organizations or </context>
<context position="3900" citStr="Schubert (2002)" startWordPosition="611" endWordPosition="612">fly review the method developed by Schubert and collaborators to abstract factoids from text; we then outline our approach to obtaining strengthened propositions from such sets of factoids. We report positive results, while making only limited use of standard Proceedings of the 12th Conference of the European Chapter of the ACL, pages 808–816, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 808 corpus statistics, concluding that future endeavors exploring knowledge extraction and WordNet should go beyond the heuristics employed in recent work. 2 KNEXT Schubert (2002) presented an approach to acquiring general world knowledge from text corpora based on parsing sentences and mapping syntactic forms into logical forms (LFs), then gleaning simple propositional factoids from these LFs through abstraction. Logical forms were based on Episodic Logic (Schubert and Hwang, 2000), a formalism designed to accommodate in a straightforward way the semantic phenomena observed in all languages, such as predication, logical compounding, generalized quantification, modification and reification of predicates and propositions, and event reference. An example from Schubert an</context>
</contexts>
<marker>Schubert, 2002</marker>
<rawString>Lenhart K. Schubert. 2002. Can we derive general world knowledge from texts? In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="7498" citStr="Suchanek et al. (2007)" startWordPosition="1177" endWordPosition="1180">hallenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sense provides coarse-grained disambiguation. This also prevents exaggeration of the contribution of a term to the abstraction, as a result of being lexicalized in a particularly finegrained way. 3.2 Propositional Templates While the procedure given here is not tied to a particular formalism in representing semantic co</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Sussna</author>
</authors>
<title>Word sense disambiguation for freetext indexing using a massive semantic network.</title>
<date>1993</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="22347" citStr="Sussna (1993)" startWordPosition="3864" endWordPosition="3865">ion. We say senses plural, as the majority of KNEXT propositions select senses that are more coarse-grained than WordNet synsets. Thus, we wish to evaluate these more coarse-grained sense disambiguation results entailed by our type abstractions.7 We performed this evaluation using as comparisons the first-sense, and all-senses heuristics. The first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses (reduced recall), 7Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna (1993); discussed more recently by, e.g., Navigli (2006). while the all-senses heuristic insists on including all admissible senses (perfect recall) at the risk of including inadmissible ones. Table 5 gives the results of two judges evaluating 314 (word, sense) pairs across the 21 selected templates. These sense pairs correspond to picking one word at random for each abstracted type selected for each template slot. Judges were presented with a sampled word, the originating template, and the glosses for each possible word sense (see Figure 2). Judges did not know ahead of time the subset of senses se</context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>Michael Sussna. 1993. Word sense disambiguation for freetext indexing using a massive semantic network. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Lenhart Schubert</author>
</authors>
<title>Open Knowledge Extraction through Compositional Language Processing.</title>
<date>2008</date>
<booktitle>In Proceedings of Semantics in Text Processing (STEP).</booktitle>
<marker>Van Durme, Schubert, 2008</marker>
<rawString>Benjamin Van Durme and Lenhart Schubert. 2008. Open Knowledge Extraction through Compositional Language Processing. In Proceedings of Semantics in Text Processing (STEP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ting Qian</author>
<author>Lenhart Schubert</author>
</authors>
<title>Class-driven Attribute Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Van Durme, Qian, Schubert, 2008</marker>
<rawString>Benjamin Van Durme, Ting Qian, and Lenhart Schubert. 2008. Class-driven Attribute Extraction. In Proceedings of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>