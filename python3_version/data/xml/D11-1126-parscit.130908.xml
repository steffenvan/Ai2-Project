<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007734">
<title confidence="0.996352">
Watermarking the Outputs of Structured Prediction with an
application in Statistical Machine Translation.
</title>
<author confidence="0.993901">
Ashish Venugopal1 Jakob Uszkoreit1 David Talbot1 Franz J. Och1 Juri Ganitkevitch2
</author>
<affiliation confidence="0.921587">
1Google, Inc. 2Center for Language and Speech Processing
</affiliation>
<address confidence="0.82525">
1600 Amphitheatre Parkway Johns Hopkins University
Mountain View, 94303, CA Baltimore, MD 21218, USA
</address>
<email confidence="0.998727">
{avenugopal,uszkoreit,talbot,ochI@google.com juri@cs.jhu.edu
</email>
<sectionHeader confidence="0.983155" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917222222222">
We propose a general method to water-
mark and probabilistically identify the
structured outputs of machine learning al-
gorithms. Our method is robust to lo-
cal editing operations and provides well
defined trade-offs between the ability to
identify algorithm outputs and the qual-
ity of the watermarked output. Unlike
previous work in the field, our approach
does not rely on controlling the inputs to
the algorithm and provides probabilistic
guarantees on the ability to identify col-
lections of results from one’s own algo-
rithm. We present an application in statis-
tical machine translation, where machine
translated output is watermarked at mini-
mal loss in translation quality and detected
with high recall.
</bodyText>
<sectionHeader confidence="0.981544" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.989556844444444">
Machine learning algorithms provide structured
results to input queries by simulating human be-
havior. Examples include automatic machine
translation (Brown et al., 1993) or automatic
text and rich media summarization (Goldstein
et al., 1999). These algorithms often estimate
some portion of their models from publicly avail-
able human generated data. As new services
that output structured results are made avail-
able to the public and the results disseminated
on the web, we face a daunting new challenge:
Machine generated structured results contam-
inate the pool of naturally generated human
data. For example, machine translated output
and human generated translations are currently
both found extensively on the web, with no auto-
matic way of distinguishing between them. Al-
gorithms that mine data from the web (Uszko-
reit et al., 2010), with the goal of learning to
simulate human behavior, will now learn mod-
els from this contaminated and potentially self-
generated data, reinforcing the errors commit-
ted by earlier versions of the algorithm.
It is beneficial to be able to identify a set of
encountered structured results as having been
generated by one’s own algorithm, with the pur-
pose of filtering such results when building new
models.
Problem Statement: We define a struc-
tured result of a query q as r = {z1 • • • zLI where
the order and identity of elements zi are impor-
tant to the quality of the result r. The structural
aspect of the result implies the existence of alter-
native results (across both the order of elements
and the elements themselves) that might vary in
their quality.
Given a collection of N results, CN =
r1 • • • rN, where each result ri has k ranked alter-
natives Dk(qi) of relatively similar quality and
queries q1 • • • qN are arbitrary and not controlled
by the watermarking algorithm, we define the
watermarking task as:
Task. Replace ri with rz E Dk(qi) for some sub-
set of results in CN to produce a watermarked
collection CN
</bodyText>
<listItem confidence="0.863245">
such that:
• C�N is probabilistically identifiable as having
been generated by one’s own algorithm.
</listItem>
<page confidence="0.886619">
1363
</page>
<note confidence="0.956173">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1363–1372,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99667598">
• the degradation in quality from CN to the enough to affect machine translations systems
watermarked C0N should be analytically con- that train on mined parallel data. In this work,
trollable, trading quality for detection per- we develop a general approach to watermark
formance. structured outputs and apply it to the outputs
• C0N should not be detectable as water- of a statistical machine translation system with
marked content without access to the gen- the goal of identifying these same outputs on the
erating algorithms. web. In the context of the watermarking task
• the detection of C0N should be robust to sim- defined above, we output selecting alternative
ple edit operations performed on individual translations for input source sentences. These
results r E C0N. translations often undergo simple edit and for-
2 Impact on Statistical Machine matting operations such as case changes, sen-
Translation tence and word deletion or post editing, prior to
Recent work(Resnik and Smith, 2003; Munteanu publishing on the web. We want to ensure that
and Marcu, 2005; Uszkoreit et al., 2010) has we can still detect watermarked translations de-
shown that multilingual parallel documents can spite these edit operations. Given the rapid pace
be efficiently identified on the web and used as of development within machine translation, it
training data to improve the quality of statisti- is also important that the watermark be robust
cal machine translation. to improvements in underlying translation qual-
The availability of free translation services ity. Results from several iterations of the system
(Google Translate, Bing Translate) and tools within a single collection of documents should be
(Moses, Joshua), increase the risk that the con- identifiable under probabilistic bounds.
tent found by parallel data mining is in fact gen- While we present evaluation results for sta-
erated by a machine, rather than by humans. In tistical machine translation, our proposed ap-
this work, we focus on statistical machine trans- proach and associated requirements are applica-
lation as an application for watermarking, with ble to any algorithm that produces structured
the goal of discarding documents from training results with several plausible alternatives. The
if they have been generated by one’s own algo- alternative results can arise as a result of inher-
rithms. ent task ambiguity (for example, there are mul-
To estimate the magnitude of the problem, tiple correct translations for a given input source
we used parallel document mining (Uszkoreit et sentence) or modeling uncertainty (for example,
al., 2010) to generate a collection of bilingual a model assigning equal probability to two com-
document pairs across several languages. For peting results).
each document, we inspected the page content 3 Watermark Structured Results
for source code that indicates the use of trans- Selecting an alternative r0 from the space of al-
lation modules/plug-ins that translate and pub- ternatives Dk(q) can be stated as:
lish the translated content. r0 = arg max w(r, Dk(q), h) (1)
We computed the proportion of the content r∈Dk(q)
within our corpus that uses these modules. We where w ranks r E Dk(q) based on r’s presen-
find that a significant proportion of the mined tation of a watermarking signal computed by a
parallel data for some language pairs is gener- hashing operation h. In this approach, w and
ated via one of these translation modules. The its component operation h are the only secrets
top 3 languages pairs, each with parallel trans- held by the watermarker. This selection crite-
lations into English, are Tagalog (50.6%), Hindi rion is applied to all system outputs, ensuring
(44.5%) and Galician (41.9%). While these that watermarked and non-watermarked version
proportions do not reflect impact on each lan- of a collection will never be available for compar-
guage’s monolingual web, they are certainly high ison.
1364
A specific implementation of w within our wa-
termarking approach can be evaluated by the
following metrics:
</bodyText>
<listItem confidence="0.994322333333333">
• False Positive Rate: how often non-
watermarked collections are falsely identi-
fied as watermarked.
• Recall Rate: how often watermarked col-
lections are correctly identified as water-
marked.
• Quality Degradation: how significantly
does C&apos;N differ from CN when evaluated by
task specific quality metrics.
</listItem>
<bodyText confidence="0.999929941176471">
While identification is performed at the col-
lection level, we can scale these metrics based
on the size of each collection to provide more
task sensitive metrics. For example, in machine
translation, we count the number of words in
the collection towards the false positive and re-
call rates.
In Section 3.1, we define a random hashing
operation h and a task independent implemen-
tation of the selector function w. Section 3.2
describes how to classify a collection of water-
marked results. Section 3.3 and 3.4 describes re-
finements to the selection and classification cri-
teria that mitigate quality degradation. Follow-
ing a comparison to related work in Section 4,
we present experimental results for several lan-
guages in Section 5.
</bodyText>
<subsectionHeader confidence="0.974621">
3.1 Watermarking: CN → C&apos;N
</subsectionHeader>
<bodyText confidence="0.978226666666667">
We define a random hashing operation h that is
applied to result r. It consists of two compo-
nents:
</bodyText>
<listItem confidence="0.979755285714286">
• A hash function applied to a structured re-
sult r to generate a bit sequence of a fixed
length.
• An optional mapping that maps a single
candidate result r to a set of sub-results.
Each sub-result is then hashed to generate
a concatenated bit sequence for r.
</listItem>
<bodyText confidence="0.999830961538462">
A good hash function produces outputs whose
bits are independent. This implies that we can
treat the bits for any input structured results
as having been generated by a binomial distri-
bution with equal probability of generating 1s
vs 0s. This condition also holds when accu-
mulating the bit sequences over a collection of
results as long as its elements are selected uni-
formly from the space of possible results. There-
fore, the bits generated from a collection of un-
watermarked results will follow a binomial dis-
tribution with parameter p = 0.5. This result
provides a null hypothesis for a statistical test
on a given bit sequence, testing whether it is
likely to have been generated from a binomial
distribution binomial(n, p) where p = 0.5 and n
is the length of the bit sequence.
For a collection CN = ri · · · rN, we can define
a watermark ranking function w to systemati-
cally select alternatives r&apos;i E Dk(q), such that
the resulting C&apos;N is unlikely to produce bit se-
quences that follow the p = 0.5 binomial distri-
bution. A straightforward biasing criteria would
be to select the candidate whose bit sequence ex-
hibits the highest ratio of 1s. w can be defined
as:
</bodyText>
<equation confidence="0.985868">
w(r, Dk(q), h) = #(1, r(r)) (2)
()|
</equation>
<bodyText confidence="0.999963333333333">
where h(r) returns the randomized bit sequence
for result r, and #(x, ~y) counts the number of
occurrences of x in sequence ~y. Selecting alter-
natives results to exhibit this bias will result in
watermarked collections that exhibit this same
bias.
</bodyText>
<subsectionHeader confidence="0.99972">
3.2 Detecting the Watermark
</subsectionHeader>
<bodyText confidence="0.9999878">
To classify a collection CN as watermarked or
non-watermarked, we apply the hashing opera-
tion h on each element in CN and concatenate
the sequences. This sequence is tested against
the null hypothesis that it was generated by a
binomial distribution with parameter p = 0.5.
We can apply a Fisherian test of statistical sig-
nificance to determine whether the observed dis-
tribution of bits is unlikely to have occurred by
chance under the null hypothesis (binomial with
p = 0.5).
We consider a collection of results that rejects
the null hypothesis to be watermarked results
generated by our own algorithms. The p-value
under the null hypothesis is efficiently computed
</bodyText>
<page confidence="0.946946">
1365
</page>
<bodyText confidence="0.996602285714286">
by:
where x is the number of 1s observed in the col-
lection, and n is the total number of bits in the
sequence. Comparing this p-value against a de-
sired significance level α, we reject the null hy-
pothesis for collections that have Pn(X &gt; x) &lt;
α, thus deciding that such collections were gen-
erated by our own system.
This classification criteria has a fixed false
positive rate. Setting α = 0.05, we know that
5% of non-watermarked bit sequences will be
falsely labeled as watermarked. This parameter
α can be controlled on an application specific ba-
sis. By biasing the selection of candidate results
to produce more 1s than 0s, we have defined
a watermarking approach that exhibits a fixed
false positive rate, a probabilistically bounded
detection rate and a task independent hashing
and selection criteria. In the next sections, we
will deal with the question of robustness to edit
operations and quality degradation.
</bodyText>
<subsectionHeader confidence="0.9952">
3.3 Robustness and Inherent Bias
</subsectionHeader>
<bodyText confidence="0.9999928">
We would like the ability to identify water-
marked collections to be robust to simple edit
operations. Even slight modifications to the ele-
ments within an item r would yield (by construc-
tion of the hash function), completely different
bit sequences that no longer preserve the biases
introduced by the watermark selection function.
To ensure that the distributional biases intro-
duced by the watermark selector are preserved,
we can optionally map individual results into a
set of sub-results, each one representing some lo-
cal structure of r. h is then applied to each sub-
result and the results concatenated to represent
r. This mapping is defined as a component of
the h operation.
While a particular edit operation might af-
fect a small number of sub-results, the majority
of the bits in the concatenated bit sequence for
r would remain untouched, thereby limiting the
damage to the biases selected during watermark-
</bodyText>
<page confidence="0.924425">
1366
</page>
<bodyText confidence="0.998062142857143">
ing. This is of course no defense to edit opera-
tions that are applied globally across the result;
our expectation is that such edits would either
significantly degrade the quality of the result or
be straightforward to identify directly.
For example, a sequence of words r = z1 · · · zL
can be mapped into a set of consecutive n-gram
sequences. Operations to edit a word zi in r will
only affect events that consider the word zi. To
account for the fact that alternatives in Dk(q)
might now result in bit sequences of different
lengths, we can generalize the biasing criteria to
directly reflect the expected contribution to the
watermark by defining:
</bodyText>
<equation confidence="0.990448">
w(r, Dk(q), h) = Pn(X &gt; #(1, h(r))) (5)
</equation>
<bodyText confidence="0.999726125">
where Pn gives probabilities from binomial(n =
Jh(r)J, p = 0.5).
Inherent collection level biases: Our null
hypothesis is based on the assumption that col-
lections of results draw uniformly from the space
of possible results. This assumption might not
always hold and depends on the type of the re-
sults and collection. For example, considering
a text document as a collection of sentences,
we can expect that some sentences might repeat
more frequently than others.
This scenario is even more likely when ap-
plying a mapping into sub-results. n-gram se-
quences follow long-tailed or Zipfian distribu-
tions, with a small number of n-grams contribut-
ing heavily toward the total number of n-grams
in a document.
A random hash function guarantees that in-
puts are distributed uniformly at random over
the output range. However, the same input will
be assigned the same output deterministically.
Therefore, if the distribution of inputs is heav-
ily skewed to certain elements of the input space,
the output distribution will not be uniformly
distributed. The bit sequences resulting from
the high frequency sub-results have the potential
to generate inherently biased distributions when
accumulated at the collection level. We want to
choose a mapping that tends towards generating
uniformly from the space of sub-results. We can
empirically measure the quality of a sub-result
mapping for a specific task by computing the
</bodyText>
<equation confidence="0.975364666666667">
p _ value = Pn(X &gt; x) (3)
n
(n)
pi(1 _ p)n−i (4)
i
i=x
</equation>
<bodyText confidence="0.999576037037037">
false positive rate on non-watermarked collec-
tions. For a given significance level α, an ideal
mapping would result in false positive rates close
to α as well.
Figure 1 shows false positive rates from 4 al-
ternative mappings, computed on a large corpus
of French documents (see Table 1 for statistics).
Classification decisions are made at the collec-
tion level (documents) but the contribution to
the false positive rate is based on the number
of words in the classified document. We con-
sider mappings from a result (sentence) into its
1-grams, 1 − 5-grams and 3 − 5 grams as well
as the non-mapping case, where the full result
is hashed.
Figure 1 shows that the 1-grams and 1 − 5-
gram generate sub-results that result in heav-
ily biased false positive rates. The 3 − 5 gram
mapping yields false positive rates close to their
theoretically expected values. 1 Small devia-
tions are expected since documents make differ-
ent contributions to the false positive rate as a
function of the number of words that they repre-
sent. For the remainder of this work, we use the
3-5 gram mapping and the full sentence map-
ping, since the alternatives generate inherently
distributions with very high false positive rates.
</bodyText>
<subsectionHeader confidence="0.995559">
3.4 Considering Quality
</subsectionHeader>
<bodyText confidence="0.9993235">
The watermarking described in Equation 3
chooses alternative results on a per result basis,
with the goal of influencing collection level bit
sequences. The selection criteria as described
will choose the most biased candidates available
in Dk(q). The parameter k determines the ex-
tent to which lesser quality alternatives can be
chosen. If all the alternatives in each Dk(q) are
of relatively similar quality, we expect minimal
degradation due to watermarking.
Specific tasks however can be particularly sen-
sitive to choosing alternative results. Discrimi-
native approaches that optimize for arg max se-
lection like (Och, 2003; Liang et al., 2006; Chi-
ang et al., 2009) train model parameters such
&apos;In the final version of this paper we will perform sam-
pling to create a more reliable estimate of the false posi-
tive rate that is not overly influenced by document length
distributions.
that the top-ranked result is well separated from
its competing alternatives. Different queries also
differ in the inherent ambiguity expected from
their results; sometimes there really is just one
correct result for a query, while for other queries,
several alternatives might be equally good.
By generalizing the definition of the w func-
tion to interpolate the estimated loss in quality
and the gain in the watermarking signal, we can
trade-off the ability to identify the watermarked
collections against quality degradation:
</bodyText>
<equation confidence="0.9946185">
w(r, Dk(q), fw) = A * gain(r, Dk(q), fw) (6)
−(1 − A) * loss(r, Dk(q))
</equation>
<bodyText confidence="0.9892774">
Loss: The loss(r, Dk(q)) function reflects the
quality degradation that results from selecting
alternative r as opposed to the best ranked can-
didate in Dk(q)). We will experiment with two
variants:
</bodyText>
<equation confidence="0.750538666666667">
lossrank(r, Dk(q)) = (rank(r) − k)/k
losscost(r, Dk(q)) = (cost(r)−cost(r1))/ cost(r1)
where:
</equation>
<listItem confidence="0.999764166666667">
• rank(r): returns the rank of r within Dk(q).
• cost(r): a weighted sum of features (not
normalized over the search space) in a log-
linear model such as those mentioned in
(Och, 2003).
• r1: the highest ranked alternative in Dk(q).
</listItem>
<bodyText confidence="0.994662666666667">
lossrank provides a generally applicable criteria
to select alternatives, penalizing selection from
deep within Dk(q). This estimate of the qual-
ity degradation does not reflect the generating
model’s opinion on relative quality. losscost con-
siders the relative increase in the generating
model’s cost assigned to the alternative trans-
lation.
Gain: The gain(r, Dk(q), fw) function reflects
the gain in the watermarking signal by selecting
candidate r. We simply define the gain as the
Pn(X &gt; #(1, h(r))) from Equation 5.
</bodyText>
<page confidence="0.983024">
1367
</page>
<figure confidence="0.997991395833333">
portion of documents
(a) 1-grams mapping
(c) 3 − 5-grams mapping
(b) 1 − 5-grams mapping
(d) Full result hashing
0.05 0.10 0.25 0.5
p-value threshold
portion of documents
0.8
0.6
0.4
0.2
0
1
expected
observed
0.05 0.10 0.25 0.5
p-value threshold
portion of documents
0.8
0.6
0.4
0.2
0
1
expected
observed
1
0.8
0.6
0.4
0.2
0
0.05 0.10 0.25 0.5
p-value threshold
expected
observed
0.05 0.10 0.25 0.5
p-value threshold
portion of documents
0.8
0.6
0.4
0.2
0
1
expected
observed
</figure>
<figureCaption confidence="0.995645">
Figure 1: Comparison of expected false positive rates against observed false positive rates for different
sub-result mappings.
</figureCaption>
<sectionHeader confidence="0.998897" genericHeader="introduction">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999990342105263">
Using watermarks with the goal of transmitting
a hidden message within images, video, audio
and monolingual text media is common. For
structured text content, linguistic approaches
like (Chapman et al., 2001; Gupta et al., 2006)
use language specific linguistic and semantic
expansions to introduce hidden watermarks.
These expansions provide alternative candidates
within which messages can be encoded. Re-
cent publications have extended this idea to ma-
chine translation, using multiple systems and
expansions to generate alternative translations.
(Stutsman et al., 2006) uses a hashing function
to select alternatives that encode the hidden
message in the lower order bits of the transla-
tion. In each of these approaches, the water-
marker has control over the collection of results
into which the watermark is to be embedded.
These approaches seek to embed a hidden
message into a collection of results that is se-
lected by the watermarker. In contrast, we ad-
dress the condition where the input queries are
not in the watermarker’s control.
The goal is therefore to introduce the water-
mark into all generated results, with the goal of
probabilistically identifying such outputs. Our
approach is also task independent, avoiding the
need for templates to generate additional al-
ternatives. By addressing the problem directly
within the search space of a dynamic program-
ming algorithm, we have access to high quality
alternatives with well defined models of qual-
ity loss. Finally, our approach is robust to local
word editing. By using a sub-result mapping, we
increase the level of editing required to obscure
the watermark signal; at high levels of editing,
the quality of the results themselves would be
significantly degraded.
</bodyText>
<sectionHeader confidence="0.998748" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999704428571429">
We evaluate our watermarking approach applied
to the outputs of statistical machine translation
under the following experimental setup.
A repository of parallel (aligned source and
target language) web documents is sampled to
produce a large corpus on which to evaluate the
watermarking classification performance. The
</bodyText>
<page confidence="0.984571">
1368
</page>
<bodyText confidence="0.999951770833334">
corpora represent translations into 4 diverse tar-
get languages, using English as the source lan-
guage. Each document in this corpus can be
considered a collection of un-watermarked struc-
tured results, where source sentences are queries
and each target sentence represents a structured
result.
Using a state-of-the-art phrase-based statisti-
cal machine translation system (Och and Ney,
2004) trained on parallel documents identified
by (Uszkoreit et al., 2010), we generate a set
of 100 alternative translations for each source
sentence. We apply the proposed watermarking
approach, along with the proposed refinements
that address task specific loss (Section 3.4) and
robustness to edit operations (Section 3.3) to
generate watermarked corpora.
Each method is controlled via a single param-
eter (like k or A) which is varied to generate
alternative watermarked collections. For each
parameter value, we evaluate the Recall Rate
and Quality Degradation with the goal of find-
ing a setting that yields a high recall rate, min-
imal quality degradation. False positive rates
are evaluated based on a fixed classification sig-
nificance level of α = 0.05. The false posi-
tive and recall rates are evaluated on the word
level; a document that is misclassified or cor-
rectly identified contributes its length in words
towards the error calculation. In this work, we
use α = 0.05 during classification corresponding
to an expected 5% false positive rate. The false
positive rate is a function of h and the signifi-
cance level α and therefore constant across the
parameter values k and A.
We evaluate quality degradation on human
translated test corpora that are more typical for
machine translation evaluation. Each test cor-
pus consists of 5000 source sentences randomly
selected from the web and translated into each
respective language.
We chose to evaluate quality on test corpora
to ensure that degradations are not hidden by
imperfectly matched web corpora and are con-
sistent with the kind of results often reported for
machine translation systems. As with the clas-
sification corpora, we create watermarked ver-
sions at each parameter value. For a given pa-
</bodyText>
<figure confidence="0.8541495">
0.5 0.6 0.7 0.8 0.9 1
recall
</figure>
<figureCaption confidence="0.942556666666667">
Figure 2: BLEU loss against recall of watermarked
content for the baseline approach (max K-best),
rank and cost interpolation.
</figureCaption>
<bodyText confidence="0.99784">
rameter value, we measure false positive and re-
call rates on the classification corpora and qual-
ity degradation on the evaluation corpora.
Table 1 shows corpus statistics for the classi-
fication and test corpora and non-watermarked
BLEU scores for each target language. All
source texts are in English.
</bodyText>
<subsectionHeader confidence="0.988421">
5.1 Loss Interpolated Experiments
</subsectionHeader>
<bodyText confidence="0.999918714285714">
Our first set of experiments demonstrates base-
line performance using the watermarking crite-
ria in Equation 5 versus the refinements sug-
gested in Section 3.4 to mitigate quality degra-
dation. The h function is computed on the full
sentence result r with no sub-event mapping.
The following methods are evaluated in Figure 2.
</bodyText>
<listItem confidence="0.991426222222222">
• Baseline method (labeled “max K-best”):
selects r&apos; purely based on gain in water-
marking signal (Equation 5) and is param-
eterized by k: the number of alternatives
considered for each result.
• Rank interpolation: incorporates rank into
w, varying the interpolation parameter A.
• Cost interpolation: incorporates cost into
w, varying the interpolation parameter A.
</listItem>
<bodyText confidence="0.9977955">
The observed false positive rate on the French
classification corpora is 1.9%.
</bodyText>
<figure confidence="0.956569272727273">
FRENCH max K-Best
FRENCH model cost gain
FRENCH rank gain
BLEU loss 0
-0.2
-0.4
-0.6
-0.8
-1
-1.2
-1.4
</figure>
<page confidence="0.94462">
1369
</page>
<table confidence="0.999552">
Classification Quality
Target # words # sentences # documents # words # sentences BLEU %
Arabic 200107 15820 896 73592 5503 12.29
French 209540 18024 600 73592 5503 26.45
Hindi 183676 13244 1300 73409 5489 20.57
Turkish 171671 17155 1697 73347 5486 13.67
</table>
<tableCaption confidence="0.9777435">
Table 1: Content statistics for classification and quality degradation corpora. Non-watermarked BLEU
scores are reported for the quality corpora.
</tableCaption>
<bodyText confidence="0.999975538461538">
We consider 0.2% BLEU loss as a thresh-
old for acceptable quality degradation. Each
method is judged by its ability to achieve high
recall below this quality degradation threshold.
Applying cost interpolation yields the best
results in Figure 2, achieving a recall of 85%
at 0.2% BLEU loss, while rank interpolation
achieves a recall of 76%. The baseline approach
of selecting the highest gain candidate within a
depth of k candidates does not provide sufficient
parameterization to yield low quality degrada-
tion. At k = 2, this method yields almost 90%
recall, but with approximately 0.4% BLEU loss.
</bodyText>
<subsectionHeader confidence="0.993932">
5.2 Robustness Experiments
</subsectionHeader>
<bodyText confidence="0.999911454545455">
In Section 5.2, we proposed mapping results into
sub-events or features. We considered alterna-
tive feature mappings in Figure 1, finding that
mapping sentence results into a collection of 3-
5 grams yields acceptable false positive rates at
varied levels of α.
Figure 3 presents results that compare mov-
ing from the result level hashing to the 3-5 gram
sub-result mapping. We show the impact of the
mapping on the baseline max K-best method as
well as for cost interpolation. There are sub-
stantial reductions in recall rate at the 0.2%
BLEU loss level when applying sub-result map-
pings in cases. The cost interpolation method
recall drops from 85% to 77% when using the
3-5 grams event mapping. The observed false
positive rate of the 3-5 gram mapping is 4.7%.
By using the 3-5 gram mapping, we expect
to increase robustness against local word edit
operations, but we have sacrificed recall rate due
to the inherent distributional bias discussed in
Section 3.3.
</bodyText>
<figure confidence="0.85274">
0.5 0.6 0.7 0.8 0.9 1
recall
</figure>
<figureCaption confidence="0.97021">
Figure 3: BLEU loss against recall of watermarked
</figureCaption>
<bodyText confidence="0.702708333333333">
content for the baseline and cost interpolation meth-
ods using both result level and 3-5 gram mapped
events.
</bodyText>
<subsectionHeader confidence="0.97828">
5.3 Multilingual Experiments
</subsectionHeader>
<bodyText confidence="0.997784888888889">
The watermarking approach proposed here in-
troduces no language specific watermarking op-
erations and it is thus broadly applicable to
translating into all languages. In Figure 4, we
report results for the baseline and cost interpola-
tion methods, considering both the result level
and 3-5 gram mapping. We set α = 0.05 and
measure recall at 0.2% BLEU degradation for
translation from English into Arabic, French,
Hindi and Turkish. The observed false posi-
tive rates for full sentence hashing are: Arabic:
2.4%, French: 1.8%, Hindi: 5.6% and Turkish:
5.5%, while for the 3-5 gram mapping, they are:
Arabic: 5.8%, French: 7.5%, Hindi:3.5% and
Turkish: 6.2%. Underlying translation qual-
ity plays an important role in translation qual-
ity degradation when watermarking. Without
a sub-result mapping, French (BLEU: 26.45%)
</bodyText>
<figure confidence="0.9926265">
BLEU loss
-0.2
-0.4
-0.6
-0.8
-1.2
-1.4
-1
0
FRENCH nbest max K-best full sentence
FRENCH max K-best 3-5 grams
FRENCH cost interp. full sentence
FRENCH cost interp. 3-5 grams
1370
recall
Arabic French Hindi Turkish
</figure>
<figureCaption confidence="0.995831">
Figure 4: Loss of recall when using 3-5 gram mapping
vs sentence level mapping for Arabic, French, Hindi
and Turkish translations.
</figureCaption>
<bodyText confidence="0.99993">
achieves recall of 85% at 0.2% BLEU loss, while
the other languages achieve over 90% recall at
the same BLEU loss threshold. Using a sub-
result mapping degrades quality for each lan-
guage pair, but changes the relative perfor-
mance. Turkish experiences the highest rela-
tive drop in recall, unlike French and Arabic,
where results are relatively more robust to using
sub-sentence mappings. This is likely a result of
differences in n-gram distributions across these
languages. The languages considered here all
use space separated words. For languages that
do not, like Chinese or Thai, our approach can
be applied at the character level.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999947428571428">
In this work we proposed a general method
to watermark and probabilistically identify the
structured outputs of machine learning algo-
rithms. Our method provides probabilistic
bounds on detection ability, analytic control on
quality degradation and is robust to local edit-
ing operations. Our method is applicable to
any task where structured outputs are generated
with ambiguities or ties in the results. We ap-
plied this method to the outputs of statistical
machine translation, evaluating each refinement
to our approach with false positive and recall
rates against BLEU score quality degradation.
Our results show that it is possible, across sev-
eral language pairs, to achieve high recall rates
(over 80%) with low false positive rates (between
5 and 8%) at minimal quality degradation (0.2%
BLEU), while still allowing for local edit opera-
tions on the translated output. In future work
we will continue to investigate methods to mit-
igate quality loss.
</bodyText>
<sectionHeader confidence="0.996924" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997361">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Peter F. Brown, Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19:263–311.
Mark Chapman, George Davida, and Marc
Rennhardway. 2001. A practical and effec-
tive approach to large-scale automated linguistic
steganography. In Proceedings of the Information
Security Conference.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Lan-
guage Technologies (NAACL-HLT).
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics.
In Research and Development in Information Re-
trieval, pages 121–128.
Gaurav Gupta, Josef Pieprzyk, and Hua Xiong
Wang. 2006. An attack-localizing watermarking
scheme for natural language documents. In Pro-
ceedings of the 2006 ACM Symposium on Informa-
tion, computer and communications security, ASI-
ACCS ’06, pages 157–165, New York, NY, USA.
ACM.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end dis-
criminative approach to machine translation. In
Proceedings of the Joint International Conference
on Computational Linguistics and Association of
Computational Linguistics (COLING/ACL, pages
761–768.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2004. The
</reference>
<figure confidence="0.998563636363636">
0.95
0.85
0.75
0.65
0.9
0.8
0.7
0.6
1
sentence-level
3-to-5 grams
</figure>
<page confidence="0.940919">
1371
</page>
<reference confidence="0.990279823529412">
alignment template approach to statistical ma-
chine translation. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 2003 Meeting of the Asssociation of Com-
putational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as
a parallel corpus. computational linguistics. Com-
putational Linguistics.
Ryan Stutsman, Mikhail Atallah, Christian
Grothoff, and Krista Grothoff. 2006. Lost
in just the translation. In Proceedings of the 2006
ACM Symposium on Applied Computing.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 2010 COLING.
</reference>
<page confidence="0.994006">
1372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9969315">Watermarking the Outputs of Structured Prediction with application in Statistical Machine Translation.</title>
<author confidence="0.999974">Jakob David Franz J Juri</author>
<affiliation confidence="0.993594">Inc. for Language and Speech Processing</affiliation>
<address confidence="0.8201515">1600 Amphitheatre Parkway Johns Hopkins University Mountain View, 94303, CA Baltimore, MD 21218, USA</address>
<email confidence="0.999859">juri@cs.jhu.edu</email>
<abstract confidence="0.998566880597015">We propose a general method to watermark and probabilistically identify the structured outputs of machine learning algorithms. Our method is robust to local editing operations and provides well defined trade-offs between the ability to identify algorithm outputs and the quality of the watermarked output. Unlike previous work in the field, our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one’s own algorithm. We present an application in statistical machine translation, where machine translated output is watermarked at minimal loss in translation quality and detected with high recall. 1 Motivation Machine learning algorithms provide structured results to input queries by simulating human behavior. Examples include automatic machine translation (Brown et al., 1993) or automatic text and rich media summarization (Goldstein et al., 1999). These algorithms often estimate some portion of their models from publicly available human generated data. As new services that output structured results are made available to the public and the results disseminated on the web, we face a daunting new challenge: Machine generated structured results contaminate the pool of naturally generated human data. For example, machine translated output and human generated translations are currently both found extensively on the web, with no automatic way of distinguishing between them. Algorithms that mine data from the web (Uszkoreit et al., 2010), with the goal of learning to simulate human behavior, will now learn models from this contaminated and potentially selfgenerated data, reinforcing the errors committed by earlier versions of the algorithm. It is beneficial to be able to identify a set of encountered structured results as having been generated by one’s own algorithm, with the purpose of filtering such results when building new models. Statement: define a strucresult of a query • • order and identity of elements importo the quality of the result The structural aspect of the result implies the existence of alternative results (across both the order of elements and the elements themselves) that might vary in their quality. a collection of • • where each result alterof relatively similar quality and • • arbitrary and not controlled by the watermarking algorithm, we define the watermarking task as: some subof results in produce a watermarked such that: • is probabilistically identifiable as having been generated by one’s own algorithm.</abstract>
<date confidence="0.548537">1363</date>
<note confidence="0.8969344">of the 2011 Conference on Empirical Methods in Natural Language pages 1363–1372, Scotland, UK, July 27–31, 2011. Association for Computational Linguistics the degradation in quality from the should be analytically con-trollable, trading quality for detection per-formance. enough to affect machine translations systems that train on mined parallel data. In this work, we develop a general approach to watermark structured outputs and apply it to the outputs of a statistical machine translation system with the goal of identifying these same outputs on the web. In the context of the watermarking task defined above, we output selecting alternative translations for input source sentences. These translations often undergo simple edit and for-matting operations such as case changes, sen-tence and word deletion or post editing, prior to publishing on the web. We want to ensure that we can still detect watermarked translations de-spite these edit operations. Given the rapid pace of development within machine translation, it is also important that the watermark be robust to improvements in underlying translation qual-ity. Results from several iterations of the system within a single collection of documents should be identifiable under probabilistic bounds. • should not be detectable as water-marked content without access to the gen-erating algorithms. While we present evaluation results for sta-tistical machine translation, our proposed ap-proach and associated requirements are applica-ble to any algorithm that produces structured results with several plausible alternatives. The alternative results can arise as a result of inher-ent task ambiguity (for example, there are mul-tiple correct translations for a given input source sentence) or modeling uncertainty (for example, a model assigning equal probability to two com-peting results). the detection of should be robust to sim-ple edit operations performed on individual 3 Watermark Structured Results 2 Impact on Statistical Machine Translation an alternative from the space of alcan be stated as: Recent work(Resnik and Smith, 2003; Munteanu and Marcu, 2005; Uszkoreit et al., 2010) has shown that multilingual parallel documents can be efficiently identified on the web and used as training data to improve the quality of statisti-cal machine translation. arg max (1) The availability of free translation services (Google Translate, Bing Translate) and tools (Moses, Joshua), increase the risk that the con-tent found by parallel data mining is in fact gen-erated by a machine, rather than by humans. In this work, we focus on statistical machine trans-lation as an application for watermarking, with the goal of discarding documents from training if they have been generated by one’s own algo-rithms. based on presen-tation of a watermarking signal computed by a operation In this approach, component operation the only held by the watermarker. This selection crite-rion is applied to all system outputs, ensuring that watermarked and non-watermarked version of a collection will never be available for compar-ison. To estimate the magnitude of the problem, we used parallel document mining (Uszkoreit et al., 2010) to generate a collection of bilingual document pairs across several languages. For each document, we inspected the page content for source code that indicates the use of trans-lation modules/plug-ins that translate and pub-lish the translated content. We computed the proportion of the content within our corpus that uses these modules. We find that a significant proportion of the mined parallel data for some language pairs is gener-ated via one of these translation modules. The top 3 languages pairs, each with parallel trans-lations into English, are Tagalog (50.6%), Hindi (44.5%) and Galician (41.9%). While these proportions do not reflect impact on each lan-guage’s monolingual web, they are certainly high</note>
<date confidence="0.486103">1364</date>
<abstract confidence="0.997684081677705">specific implementation of our watermarking approach can be evaluated by the following metrics: • False Positive Rate: how often noncollections are identified as watermarked. • Recall Rate: how often watermarked colare as watermarked. • Quality Degradation: how significantly from evaluated by task specific quality metrics. While identification is performed at the collection level, we can scale these metrics based on the size of each collection to provide more task sensitive metrics. For example, in machine translation, we count the number of words in the collection towards the false positive and recall rates. In Section 3.1, we define a random hashing a task independent implemenof the selector function Section 3.2 describes how to classify a collection of watermarked results. Section 3.3 and 3.4 describes refinements to the selection and classification criteria that mitigate quality degradation. Following a comparison to related work in Section 4, we present experimental results for several languages in Section 5. Watermarking: define a random hashing operation is to result It consists of two components: • A hash function applied to a structured regenerate a bit sequence of a fixed length. • An optional mapping that maps a single result a set of sub-results. Each sub-result is then hashed to generate concatenated bit sequence for A good hash function produces outputs whose bits are independent. This implies that we can treat the bits for any input structured results as having been generated by a binomial distribution with equal probability of generating 1s vs 0s. This condition also holds when accumulating the bit sequences over a collection of results as long as its elements are selected uniformly from the space of possible results. Therebits generated from a collection of unwatermarked results will follow a binomial diswith parameter This result provides a null hypothesis for a statistical test on a given bit sequence, testing whether it is likely to have been generated from a binomial where and is the length of the bit sequence. a collection · · we can define watermark ranking function systematiselect alternatives such that resulting produce bit sethat follow the binomial distribution. A straightforward biasing criteria would be to select the candidate whose bit sequence exthe highest ratio of 1s. be defined as: = returns the randomized bit sequence result and counts the number of of sequence Selecting alternatives results to exhibit this bias will result in watermarked collections that exhibit this same bias. 3.2 Detecting the Watermark classify a collection watermarked or non-watermarked, we apply the hashing operaeach element in concatenate the sequences. This sequence is tested against the null hypothesis that it was generated by a distribution with parameter We can apply a Fisherian test of statistical significance to determine whether the observed distribution of bits is unlikely to have occurred by chance under the null hypothesis (binomial with consider a collection of results that the null hypothesis to be watermarked results by our own algorithms. The under the null hypothesis is efficiently computed 1365 by: the number of 1s observed in the coland the total number of bits in the Comparing this against a designificance level we reject the null hyfor collections that have thus deciding that such collections were generated by our own system. This classification criteria has a fixed false rate. Setting we know that of sequences will be as watermarked. This parameter be controlled on an application specific basis. By biasing the selection of candidate results to produce more 1s than 0s, we have defined a watermarking approach that exhibits a fixed false positive rate, a probabilistically bounded detection rate and a task independent hashing and selection criteria. In the next sections, we will deal with the question of robustness to edit operations and quality degradation. 3.3 Robustness and Inherent Bias We would like the ability to identify watermarked collections to be robust to simple edit operations. Even slight modifications to the elewithin an item yield (by construction of the hash function), completely different bit sequences that no longer preserve the biases introduced by the watermark selection function. To ensure that the distributional biases introduced by the watermark selector are preserved, we can optionally map individual results into a set of sub-results, each one representing some lostructure of then applied to each subresult and the results concatenated to represent This mapping is defined as a component of While a particular edit operation might affect a small number of sub-results, the majority of the bits in the concatenated bit sequence for remain untouched, thereby limiting the to the biases selected during watermark- 1366 ing. This is of course no defense to edit operations that are applied globally across the result; our expectation is that such edits would either significantly degrade the quality of the result or be straightforward to identify directly. example, a sequence of words · · be mapped into a set of consecutive Operations to edit a word affect events that consider the word To for the fact that alternatives in might now result in bit sequences of different lengths, we can generalize the biasing criteria to directly reflect the expected contribution to the watermark by defining: = (5) gives probabilities from p collection level biases: null hypothesis is based on the assumption that collections of results draw uniformly from the space of possible results. This assumption might not always hold and depends on the type of the results and collection. For example, considering a text document as a collection of sentences, we can expect that some sentences might repeat more frequently than others. This scenario is even more likely when apa mapping into sub-results. sequences follow long-tailed or Zipfian distribuwith a small number of contributheavily toward the total number of in a document. A random hash function guarantees that inputs are distributed uniformly at random over the output range. However, the same input will be assigned the same output deterministically. Therefore, if the distribution of inputs is heavily skewed to certain elements of the input space, the output distribution will not be uniformly distributed. The bit sequences resulting from the high frequency sub-results have the potential to generate inherently biased distributions when accumulated at the collection level. We want to choose a mapping that tends towards generating uniformly from the space of sub-results. We can empirically measure the quality of a sub-result mapping for a specific task by computing the (3) n (4) i i=x false positive rate on non-watermarked collec- For a given significance level an ideal mapping would result in false positive rates close well. Figure 1 shows false positive rates from 4 alternative mappings, computed on a large corpus of French documents (see Table 1 for statistics). Classification decisions are made at the collection level (documents) but the contribution to the false positive rate is based on the number of words in the classified document. We consider mappings from a result (sentence) into its 1-grams, 1 − 5-grams and 3 − 5 grams as well as the non-mapping case, where the full result is hashed. Figure 1 shows that the 1-grams and 1 − 5gram generate sub-results that result in heavily biased false positive rates. The 3 − 5 gram mapping yields false positive rates close to their expected values. deviations are expected since documents make different contributions to the false positive rate as a function of the number of words that they represent. For the remainder of this work, we use the 3-5 gram mapping and the full sentence mapping, since the alternatives generate inherently distributions with very high false positive rates. 3.4 Considering Quality The watermarking described in Equation 3 chooses alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available The parameter the extent to which lesser quality alternatives can be If all the alternatives in each are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. the top-ranked result is well its competing alternatives. Different queries also differ in the inherent ambiguity expected from their results; sometimes there really is just one correct result for a query, while for other queries, several alternatives might be equally good. generalizing the definition of the function to interpolate the estimated loss in quality and the gain in the watermarking signal, we can trade-off the ability to identify the watermarked collections against quality degradation: = − * (6) function reflects the quality degradation that results from selecting opposed to the best ranked canin We will experiment with two variants: = − = where: returns the rank of • a weighted sum of features (not normalized over the search space) in a loglinear model such as those mentioned in (Och, 2003). • the highest ranked alternative in a generally applicable criteria to select alternatives, penalizing selection from within This estimate of the quality degradation does not reflect the generating opinion on relative quality. considers the relative increase in the generating model’s cost assigned to the alternative translation. function reflects the gain in the watermarking signal by selecting We simply define the gain as the from Equation 5. 1367 portion of documents (a) 1-grams mapping 3 mapping 1 mapping (d) Full result hashing 0.05 0.10 0.25 0.5 p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed 0.05 0.10 0.25 0.5 p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed 1 0.8 0.6 0.4 0.2 0 0.05 0.10 0.25 0.5 p-value threshold expected observed 0.05 0.10 0.25 0.5 p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed Figure 1: Comparison of expected false positive rates against observed false positive rates for different sub-result mappings. 4 Related Work Using watermarks with the goal of transmitting a hidden message within images, video, audio and monolingual text media is common. For structured text content, linguistic approaches like (Chapman et al., 2001; Gupta et al., 2006) use language specific linguistic and semantic expansions to introduce hidden watermarks. These expansions provide alternative candidates within which messages can be encoded. Recent publications have extended this idea to machine translation, using multiple systems and expansions to generate alternative translations. (Stutsman et al., 2006) uses a hashing function to select alternatives that encode the hidden message in the lower order bits of the translation. In each of these approaches, the watermarker has control over the collection of results into which the watermark is to be embedded. These approaches seek to embed a hidden into a collection of results that is sethe watermarker. In contrast, we address the condition where the input queries are not in the watermarker’s control. The goal is therefore to introduce the watermark into all generated results, with the goal of probabilistically identifying such outputs. Our approach is also task independent, avoiding the need for templates to generate additional alternatives. By addressing the problem directly within the search space of a dynamic programming algorithm, we have access to high quality alternatives with well defined models of quality loss. Finally, our approach is robust to local word editing. By using a sub-result mapping, we increase the level of editing required to obscure the watermark signal; at high levels of editing, the quality of the results themselves would be significantly degraded. 5 Experiments We evaluate our watermarking approach applied to the outputs of statistical machine translation under the following experimental setup. A repository of parallel (aligned source and target language) web documents is sampled to produce a large corpus on which to evaluate the The 1368 corpora represent translations into 4 diverse target languages, using English as the source language. Each document in this corpus can be considered a collection of un-watermarked structured results, where source sentences are queries and each target sentence represents a structured result. Using a state-of-the-art phrase-based statistical machine translation system (Och and Ney, 2004) trained on parallel documents identified by (Uszkoreit et al., 2010), we generate a set of 100 alternative translations for each source sentence. We apply the proposed watermarking approach, along with the proposed refinements that address task specific loss (Section 3.4) and robustness to edit operations (Section 3.3) to generate watermarked corpora. Each method is controlled via a single param- (like which is varied to generate alternative watermarked collections. For each parameter value, we evaluate the Recall Rate and Quality Degradation with the goal of finding a setting that yields a high recall rate, minimal quality degradation. False positive rates are evaluated based on a fixed classification siglevel of The false positive and recall rates are evaluated on the word level; a document that is misclassified or correctly identified contributes its length in words towards the error calculation. In this work, we during classification corresponding to an expected 5% false positive rate. The false rate is a function of the signifilevel therefore constant across the values We evaluate quality degradation on human translated test corpora that are more typical for machine translation evaluation. Each test corpus consists of 5000 source sentences randomly selected from the web and translated into each respective language. We chose to evaluate quality on test corpora to ensure that degradations are not hidden by imperfectly matched web corpora and are consistent with the kind of results often reported for machine translation systems. As with the classification corpora, we create watermarked verat each parameter value. For a given pa- 0.5 0.6 0.7 0.8 0.9 1 recall Figure 2: BLEU loss against recall of watermarked for the baseline approach (max rank and cost interpolation. rameter value, we measure false positive and recall rates on the classification corpora and quality degradation on the evaluation corpora. Table 1 shows corpus statistics for the classification and test corpora and non-watermarked BLEU scores for each target language. All source texts are in English. 5.1 Loss Interpolated Experiments Our first set of experiments demonstrates baseline performance using the watermarking criteria in Equation 5 versus the refinements suggested in Section 3.4 to mitigate quality degra- The is computed on the full result no sub-event mapping. The following methods are evaluated in Figure 2. • Baseline method (labeled “max K-best”): based on gain in watermarking signal (Equation 5) and is paramby the number of alternatives considered for each result. • Rank interpolation: incorporates rank into varying the interpolation parameter • Cost interpolation: incorporates cost into varying the interpolation parameter The observed false positive rate on the French corpora is FRENCH max K-Best FRENCH model cost gain FRENCH rank gain BLEU loss 0 -0.2 -0.4 -0.6 -0.8 -1 -1.2 -1.4</abstract>
<date confidence="0.658309">1369</date>
<title confidence="0.952635">Classification Quality</title>
<author confidence="0.528314">Target BLEU</author>
<note confidence="0.7038584">Arabic 200107 15820 896 73592 5503 12.29 French 209540 18024 600 73592 5503 26.45 Hindi 183676 13244 1300 73409 5489 20.57 Turkish 171671 17155 1697 73347 5486 13.67 Table 1: Content statistics for classification and quality degradation corpora. Non-watermarked BLEU</note>
<abstract confidence="0.972752368852459">scores are reported for the quality corpora. We consider 0.2% BLEU loss as a threshold for acceptable quality degradation. Each method is judged by its ability to achieve high recall below this quality degradation threshold. Applying cost interpolation yields the best results in Figure 2, achieving a recall of 85% at 0.2% BLEU loss, while rank interpolation achieves a recall of 76%. The baseline approach of selecting the highest gain candidate within a of does not provide sufficient parameterization to yield low quality degrada- At 2, this method yields almost 90% recall, but with approximately 0.4% BLEU loss. 5.2 Robustness Experiments In Section 5.2, we proposed mapping results into sub-events or features. We considered alternative feature mappings in Figure 1, finding that mapping sentence results into a collection of 3- 5 grams yields acceptable false positive rates at levels of Figure 3 presents results that compare moving from the result level hashing to the 3-5 gram sub-result mapping. We show the impact of the on the baseline max method as well as for cost interpolation. There are subreductions in recall rate at the BLEU loss level when applying sub-result mappings in cases. The cost interpolation method recall drops from 85% to 77% when using the 3-5 grams event mapping. The observed false positive rate of the 3-5 gram mapping is 4.7%. By using the 3-5 gram mapping, we expect to increase robustness against local word edit operations, but we have sacrificed recall rate due to the inherent distributional bias discussed in Section 3.3. 0.5 0.6 0.7 0.8 0.9 1 recall Figure 3: BLEU loss against recall of watermarked content for the baseline and cost interpolation methods using both result level and 3-5 gram mapped events. 5.3 Multilingual Experiments The watermarking approach proposed here introduces no language specific watermarking operations and it is thus broadly applicable to translating into all languages. In Figure 4, we report results for the baseline and cost interpolation methods, considering both the result level 3-5 gram mapping. We set and measure recall at 0.2% BLEU degradation for translation from English into Arabic, French, Hindi and Turkish. The observed false positive rates for full sentence hashing are: Arabic: 2.4%, French: 1.8%, Hindi: 5.6% and Turkish: 5.5%, while for the 3-5 gram mapping, they are: Arabic: 5.8%, French: 7.5%, Hindi:3.5% and Turkish: 6.2%. Underlying translation quality plays an important role in translation quality degradation when watermarking. Without a sub-result mapping, French (BLEU: 26.45%) BLEU loss -0.2 -0.4 -0.6 -0.8 -1.2 -1.4 -1 0 FRENCH nbest max K-best full sentence FRENCH max K-best 3-5 grams FRENCH cost interp. full sentence FRENCH cost interp. 3-5 grams 1370 recall Arabic French Hindi Turkish Figure 4: Loss of recall when using 3-5 gram mapping vs sentence level mapping for Arabic, French, Hindi and Turkish translations. achieves recall of 85% at 0.2% BLEU loss, while the other languages achieve over 90% recall at the same BLEU loss threshold. Using a subresult mapping degrades quality for each language pair, but changes the relative performance. Turkish experiences the highest relative drop in recall, unlike French and Arabic, where results are relatively more robust to using sub-sentence mappings. This is likely a result of in distributions across these languages. The languages considered here all use space separated words. For languages that do not, like Chinese or Thai, our approach can be applied at the character level. 6 Conclusions In this work we proposed a general method to watermark and probabilistically identify the outputs of machine learning algorithms. Our method provides bounds on detection ability, analytic control on quality degradation and is robust to local editing operations. Our method is applicable to any task where structured outputs are generated with ambiguities or ties in the results. We applied this method to the outputs of statistical machine translation, evaluating each refinement to our approach with false positive and recall rates against BLEU score quality degradation. Our results show that it is possible, across several language pairs, to achieve high recall rates (over 80%) with low false positive rates (between 5 and 8%) at minimal quality degradation (0.2% BLEU), while still allowing for local edit operations on the translated output. In future work we will continue to investigate methods to mitigate quality loss. References Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Minimum error rate in statistical machine translation. In Proceedings of the 2007 Joint Conference on Empir-</abstract>
<title confidence="0.894755">ical Methods in Natural Language Processing and Natural Language</title>
<author confidence="0.997898">Peter F Brown</author>
<author confidence="0.997898">Vincent J Della Pietra</author>
<author confidence="0.997898">Stephen</author>
<note confidence="0.754293090909091">A. Della Pietra, and Robert. L. Mercer. 1993. The mathematics of statistical machine transla- Parameter estimation. Lin- 19:263–311. Mark Chapman, George Davida, and Marc Rennhardway. 2001. A practical and effective approach to large-scale automated linguistic In of the Information David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine trans- In American Chapter of the Associa-</note>
<title confidence="0.8238665">tion for Computational Linguistics - Human Lan- Technologies</title>
<author confidence="0.6661635">Summarizing text docu-</author>
<abstract confidence="0.907872625">ments: Sentence selection and evaluation metrics. and Development in Information Repages 121–128. Gaurav Gupta, Josef Pieprzyk, and Hua Xiong Wang. 2006. An attack-localizing watermarking for natural language documents. In Proceedings of the 2006 ACM Symposium on Informacomputer and communications ASI-</abstract>
<address confidence="0.770854">ACCS ’06, pages 157–165, New York, NY, USA.</address>
<email confidence="0.656441">ACM.</email>
<author confidence="0.71672">An end-to-end dis-</author>
<note confidence="0.724865615384615">criminative approach to machine translation. In Proceedings of the Joint International Conference on Computational Linguistics and Association of Linguistics pages 761–768. Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exnon-parallel corpora. Lin- Franz Josef Och and Hermann Ney. 2004. The 0.95 0.85 0.75 0.65</note>
<abstract confidence="0.948672166666667">0.9 0.8 0.7 0.6 1 sentence-level 3-to-5 grams 1371 alignment template approach to statistical matranslation. Franz Josef Och. 2003. Minimum error rate training statistical machine translation. In</abstract>
<note confidence="0.666238909090909">of the 2003 Meeting of the Asssociation of Com- Philip Resnik and Noah A. Smith. 2003. The web as parallel corpus. computational linguistics. Com- Ryan Stutsman, Mikhail Atallah, Christian Grothoff, and Krista Grothoff. 2006. just the translation. In of the 2006 Symposium on Applied Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. 2010. Large scale parallel docmining for machine translation. In Proof the 2010</note>
<date confidence="0.571557">1372</date>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Minimum error rate training in statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Chapman</author>
<author>George Davida</author>
<author>Marc Rennhardway</author>
</authors>
<title>A practical and effective approach to large-scale automated linguistic steganography.</title>
<date>2001</date>
<booktitle>In Proceedings of the Information Security Conference.</booktitle>
<contexts>
<context position="19615" citStr="Chapman et al., 2001" startWordPosition="3214" endWordPosition="3217">ved 0.05 0.10 0.25 0.5 p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed 1 0.8 0.6 0.4 0.2 0 0.05 0.10 0.25 0.5 p-value threshold expected observed 0.05 0.10 0.25 0.5 p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed Figure 1: Comparison of expected false positive rates against observed false positive rates for different sub-result mappings. 4 Related Work Using watermarks with the goal of transmitting a hidden message within images, video, audio and monolingual text media is common. For structured text content, linguistic approaches like (Chapman et al., 2001; Gupta et al., 2006) use language specific linguistic and semantic expansions to introduce hidden watermarks. These expansions provide alternative candidates within which messages can be encoded. Recent publications have extended this idea to machine translation, using multiple systems and expansions to generate alternative translations. (Stutsman et al., 2006) uses a hashing function to select alternatives that encode the hidden message in the lower order bits of the translation. In each of these approaches, the watermarker has control over the collection of results into which the watermark </context>
</contexts>
<marker>Chapman, Davida, Rennhardway, 2001</marker>
<rawString>Mark Chapman, George Davida, and Marc Rennhardway. 2001. A practical and effective approach to large-scale automated linguistic steganography. In Proceedings of the Information Security Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT).</booktitle>
<contexts>
<context position="16926" citStr="Chiang et al., 2009" startWordPosition="2778" endWordPosition="2782">s alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available in Dk(q). The parameter k determines the extent to which lesser quality alternatives can be chosen. If all the alternatives in each Dk(q) are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such &apos;In the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. that the top-ranked result is well separated from its competing alternatives. Different queries also differ in the inherent ambiguity expected from their results; sometimes there really is just one correct result for a query, while for other queries, several alternatives might be equally good. By generalizing the definition of the w function to interpolate the estimated loss in qua</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="1374" citStr="Goldstein et al., 1999" startWordPosition="192" endWordPosition="195">ield, our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one’s own algorithm. We present an application in statistical machine translation, where machine translated output is watermarked at minimal loss in translation quality and detected with high recall. 1 Motivation Machine learning algorithms provide structured results to input queries by simulating human behavior. Examples include automatic machine translation (Brown et al., 1993) or automatic text and rich media summarization (Goldstein et al., 1999). These algorithms often estimate some portion of their models from publicly available human generated data. As new services that output structured results are made available to the public and the results disseminated on the web, we face a daunting new challenge: Machine generated structured results contaminate the pool of naturally generated human data. For example, machine translated output and human generated translations are currently both found extensively on the web, with no automatic way of distinguishing between them. Algorithms that mine data from the web (Uszkoreit et al., 2010), wit</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Research and Development in Information Retrieval, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaurav Gupta</author>
<author>Josef Pieprzyk</author>
<author>Hua Xiong Wang</author>
</authors>
<title>An attack-localizing watermarking scheme for natural language documents.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, ASIACCS ’06,</booktitle>
<pages>157--165</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="19636" citStr="Gupta et al., 2006" startWordPosition="3218" endWordPosition="3221"> p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed 1 0.8 0.6 0.4 0.2 0 0.05 0.10 0.25 0.5 p-value threshold expected observed 0.05 0.10 0.25 0.5 p-value threshold portion of documents 0.8 0.6 0.4 0.2 0 1 expected observed Figure 1: Comparison of expected false positive rates against observed false positive rates for different sub-result mappings. 4 Related Work Using watermarks with the goal of transmitting a hidden message within images, video, audio and monolingual text media is common. For structured text content, linguistic approaches like (Chapman et al., 2001; Gupta et al., 2006) use language specific linguistic and semantic expansions to introduce hidden watermarks. These expansions provide alternative candidates within which messages can be encoded. Recent publications have extended this idea to machine translation, using multiple systems and expansions to generate alternative translations. (Stutsman et al., 2006) uses a hashing function to select alternatives that encode the hidden message in the lower order bits of the translation. In each of these approaches, the watermarker has control over the collection of results into which the watermark is to be embedded. Th</context>
</contexts>
<marker>Gupta, Pieprzyk, Wang, 2006</marker>
<rawString>Gaurav Gupta, Josef Pieprzyk, and Hua Xiong Wang. 2006. An attack-localizing watermarking scheme for natural language documents. In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, ASIACCS ’06, pages 157–165, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cote</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint International Conference on Computational Linguistics and Association of Computational Linguistics (COLING/ACL,</booktitle>
<pages>761--768</pages>
<contexts>
<context position="16904" citStr="Liang et al., 2006" startWordPosition="2774" endWordPosition="2777">in Equation 3 chooses alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available in Dk(q). The parameter k determines the extent to which lesser quality alternatives can be chosen. If all the alternatives in each Dk(q) are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such &apos;In the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. that the top-ranked result is well separated from its competing alternatives. Different queries also differ in the inherent ambiguity expected from their results; sometimes there really is just one correct result for a query, while for other queries, several alternatives might be equally good. By generalizing the definition of the w function to interpolate the</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the Joint International Conference on Computational Linguistics and Association of Computational Linguistics (COLING/ACL, pages 761–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics.</title>
<date>2005</date>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="21860" citStr="Och and Ney, 2004" startWordPosition="3558" endWordPosition="3561">n under the following experimental setup. A repository of parallel (aligned source and target language) web documents is sampled to produce a large corpus on which to evaluate the watermarking classification performance. The 1368 corpora represent translations into 4 diverse target languages, using English as the source language. Each document in this corpus can be considered a collection of un-watermarked structured results, where source sentences are queries and each target sentence represents a structured result. Using a state-of-the-art phrase-based statistical machine translation system (Och and Ney, 2004) trained on parallel documents identified by (Uszkoreit et al., 2010), we generate a set of 100 alternative translations for each source sentence. We apply the proposed watermarking approach, along with the proposed refinements that address task specific loss (Section 3.4) and robustness to edit operations (Section 3.3) to generate watermarked corpora. Each method is controlled via a single parameter (like k or A) which is varied to generate alternative watermarked collections. For each parameter value, we evaluate the Recall Rate and Quality Degradation with the goal of finding a setting that</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the Asssociation of Computational Linguistics.</booktitle>
<contexts>
<context position="16884" citStr="Och, 2003" startWordPosition="2772" endWordPosition="2773"> described in Equation 3 chooses alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available in Dk(q). The parameter k determines the extent to which lesser quality alternatives can be chosen. If all the alternatives in each Dk(q) are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such &apos;In the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. that the top-ranked result is well separated from its competing alternatives. Different queries also differ in the inherent ambiguity expected from their results; sometimes there really is just one correct result for a query, while for other queries, several alternatives might be equally good. By generalizing the definition of the w functio</context>
<context position="18216" citStr="Och, 2003" startWordPosition="2991" endWordPosition="2992">entify the watermarked collections against quality degradation: w(r, Dk(q), fw) = A * gain(r, Dk(q), fw) (6) −(1 − A) * loss(r, Dk(q)) Loss: The loss(r, Dk(q)) function reflects the quality degradation that results from selecting alternative r as opposed to the best ranked candidate in Dk(q)). We will experiment with two variants: lossrank(r, Dk(q)) = (rank(r) − k)/k losscost(r, Dk(q)) = (cost(r)−cost(r1))/ cost(r1) where: • rank(r): returns the rank of r within Dk(q). • cost(r): a weighted sum of features (not normalized over the search space) in a loglinear model such as those mentioned in (Och, 2003). • r1: the highest ranked alternative in Dk(q). lossrank provides a generally applicable criteria to select alternatives, penalizing selection from deep within Dk(q). This estimate of the quality degradation does not reflect the generating model’s opinion on relative quality. losscost considers the relative increase in the generating model’s cost assigned to the alternative translation. Gain: The gain(r, Dk(q), fw) function reflects the gain in the watermarking signal by selecting candidate r. We simply define the gain as the Pn(X &gt; #(1, h(r))) from Equation 5. 1367 portion of documents (a) 1</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 2003 Meeting of the Asssociation of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus. computational linguistics. Computational Linguistics.</title>
<date>2003</date>
<contexts>
<context position="4391" citStr="Resnik and Smith, 2003" startWordPosition="691" endWordPosition="694">statistical machine translation system with marked content without access to the gen- the goal of identifying these same outputs on the erating algorithms. web. In the context of the watermarking task • the detection of C0N should be robust to sim- defined above, we output selecting alternative ple edit operations performed on individual translations for input source sentences. These results r E C0N. translations often undergo simple edit and for2 Impact on Statistical Machine matting operations such as case changes, senTranslation tence and word deletion or post editing, prior to Recent work(Resnik and Smith, 2003; Munteanu publishing on the web. We want to ensure that and Marcu, 2005; Uszkoreit et al., 2010) has we can still detect watermarked translations deshown that multilingual parallel documents can spite these edit operations. Given the rapid pace be efficiently identified on the web and used as of development within machine translation, it training data to improve the quality of statisti- is also important that the watermark be robust cal machine translation. to improvements in underlying translation qualThe availability of free translation services ity. Results from several iterations of the s</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. computational linguistics. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Stutsman</author>
<author>Mikhail Atallah</author>
<author>Christian Grothoff</author>
<author>Krista Grothoff</author>
</authors>
<title>Lost in just the translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 ACM Symposium on Applied Computing.</booktitle>
<contexts>
<context position="19979" citStr="Stutsman et al., 2006" startWordPosition="3264" endWordPosition="3267"> for different sub-result mappings. 4 Related Work Using watermarks with the goal of transmitting a hidden message within images, video, audio and monolingual text media is common. For structured text content, linguistic approaches like (Chapman et al., 2001; Gupta et al., 2006) use language specific linguistic and semantic expansions to introduce hidden watermarks. These expansions provide alternative candidates within which messages can be encoded. Recent publications have extended this idea to machine translation, using multiple systems and expansions to generate alternative translations. (Stutsman et al., 2006) uses a hashing function to select alternatives that encode the hidden message in the lower order bits of the translation. In each of these approaches, the watermarker has control over the collection of results into which the watermark is to be embedded. These approaches seek to embed a hidden message into a collection of results that is selected by the watermarker. In contrast, we address the condition where the input queries are not in the watermarker’s control. The goal is therefore to introduce the watermark into all generated results, with the goal of probabilistically identifying such ou</context>
</contexts>
<marker>Stutsman, Atallah, Grothoff, Grothoff, 2006</marker>
<rawString>Ryan Stutsman, Mikhail Atallah, Christian Grothoff, and Krista Grothoff. 2006. Lost in just the translation. In Proceedings of the 2006 ACM Symposium on Applied Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay Ponte</author>
<author>Ashok Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 COLING.</booktitle>
<contexts>
<context position="1969" citStr="Uszkoreit et al., 2010" startWordPosition="286" endWordPosition="290">ion (Goldstein et al., 1999). These algorithms often estimate some portion of their models from publicly available human generated data. As new services that output structured results are made available to the public and the results disseminated on the web, we face a daunting new challenge: Machine generated structured results contaminate the pool of naturally generated human data. For example, machine translated output and human generated translations are currently both found extensively on the web, with no automatic way of distinguishing between them. Algorithms that mine data from the web (Uszkoreit et al., 2010), with the goal of learning to simulate human behavior, will now learn models from this contaminated and potentially selfgenerated data, reinforcing the errors committed by earlier versions of the algorithm. It is beneficial to be able to identify a set of encountered structured results as having been generated by one’s own algorithm, with the purpose of filtering such results when building new models. Problem Statement: We define a structured result of a query q as r = {z1 • • • zLI where the order and identity of elements zi are important to the quality of the result r. The structural aspect</context>
<context position="4488" citStr="Uszkoreit et al., 2010" startWordPosition="708" endWordPosition="711">f identifying these same outputs on the erating algorithms. web. In the context of the watermarking task • the detection of C0N should be robust to sim- defined above, we output selecting alternative ple edit operations performed on individual translations for input source sentences. These results r E C0N. translations often undergo simple edit and for2 Impact on Statistical Machine matting operations such as case changes, senTranslation tence and word deletion or post editing, prior to Recent work(Resnik and Smith, 2003; Munteanu publishing on the web. We want to ensure that and Marcu, 2005; Uszkoreit et al., 2010) has we can still detect watermarked translations deshown that multilingual parallel documents can spite these edit operations. Given the rapid pace be efficiently identified on the web and used as of development within machine translation, it training data to improve the quality of statisti- is also important that the watermark be robust cal machine translation. to improvements in underlying translation qualThe availability of free translation services ity. Results from several iterations of the system (Google Translate, Bing Translate) and tools within a single collection of documents should</context>
<context position="21929" citStr="Uszkoreit et al., 2010" startWordPosition="3568" endWordPosition="3571">el (aligned source and target language) web documents is sampled to produce a large corpus on which to evaluate the watermarking classification performance. The 1368 corpora represent translations into 4 diverse target languages, using English as the source language. Each document in this corpus can be considered a collection of un-watermarked structured results, where source sentences are queries and each target sentence represents a structured result. Using a state-of-the-art phrase-based statistical machine translation system (Och and Ney, 2004) trained on parallel documents identified by (Uszkoreit et al., 2010), we generate a set of 100 alternative translations for each source sentence. We apply the proposed watermarking approach, along with the proposed refinements that address task specific loss (Section 3.4) and robustness to edit operations (Section 3.3) to generate watermarked corpora. Each method is controlled via a single parameter (like k or A) which is varied to generate alternative watermarked collections. For each parameter value, we evaluate the Recall Rate and Quality Degradation with the goal of finding a setting that yields a high recall rate, minimal quality degradation. False positi</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of the 2010 COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>