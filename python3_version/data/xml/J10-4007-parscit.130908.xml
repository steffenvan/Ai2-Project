<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9991015">
A Flexible, Corpus-Driven Model of Regular
and Inverse Selectional Preferences
</title>
<author confidence="0.998565">
Katrin Erk*
</author>
<affiliation confidence="0.995849">
University of Texas at Austin
</affiliation>
<author confidence="0.970217">
Sebastian Padó**
</author>
<affiliation confidence="0.959836">
Heidelberg University
</affiliation>
<author confidence="0.73855">
Ulrike Padó†
</author>
<affiliation confidence="0.615093">
Vico Research and Consulting GmbH
</affiliation>
<bodyText confidence="0.996189571428571">
We present a vector space–based model for selectional preferences that predicts plausibility
scores for argument headwords. It does not require any lexical resources (such as WordNet). It
can be trained either on one corpus with syntactic annotation, or on a combination of a small
semantically annotated primary corpus and a large, syntactically analyzed generalization cor-
pus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for
predicates given argument heads.
We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task
(prediction of human plausibility judgments), gauging the influence of different parameters and
comparing our model against other model classes. We obtain consistent benefits from using the
disambiguation and semantic role information provided by a semantically tagged primary cor-
pus. As for parameters, we identify settings that yield good performance across a range of experi-
mental conditions. However, frequency remains a major influence of prediction quality, and
we also identify more robust parameter settings suitable for applications with many infrequent
items.
</bodyText>
<sectionHeader confidence="0.996955" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999052166666667">
Selectional preferences or selectional constraints describe knowledge about possible
and plausible fillers for a predicate’s argument positions. They model the fact that there
is often a semantically coherent set of concepts that can fill a given argument posi-
tion. Selectional preferences can help for many text analysis tasks which involve com-
paring different attachment decisions. Examples include syntactic disambiguation
(Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD,
</bodyText>
<note confidence="0.514987">
* Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712.
</note>
<email confidence="0.7048415">
E-mail: katrin.erk@mail.utexas.edu.
** E-mail: pado@cl.uni-heidelberg.de.
</email>
<note confidence="0.85817">
† E-mail:ulrike.pado@vico-research.com.
Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication:
29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P.
a visiting scholar at Stanford University.
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.993663304347826">
McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and
characterizing the conditions under which entailment holds between two predicates
(Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selec-
tional preferences are also helpful for determining linguistic properties of predicates
and predicate–argument combinations, for example in compositionality assessment
(McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations
(McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibil-
ity judgments for predicate–argument combinations (Resnik 1996) and effects in human
sentence reading times (Padó, Crocker, and Keller 2009).
All these applications rely on the availability of broad-coverage, reliable selectional
preferences for predicates and their argument positions. Given the immense effort nec-
essary for manual semantic lexicon building and its associated reliability problems (see,
e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences
acquire selectional preferences automatically from large corpora.
The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument
headword (or filler) from a corpus, and then to compute selectional preference as
relative frequencies. However, due to the Zipfian nature of word frequencies, the first
step on its own results in a very sparse list of headwords, in particular for less frequent
predicates. As an example, the verb anglicize only appears with nine direct objects in
the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them,
name, appears more than once. Many highly plausible fillers are missing from the list,
such as word or spelling.
In order to make sensible predictions for triples that are unseen at training time,
it is crucial to add a generalization step that infers a degree of preference for new,
unseen headwords for a given predicate and role.1 The result is, in the ideal case, an
assignment to every possible headword of some degree of compatibility (or plausibil-
ity) with the predicate’s preferences. In the case of anglicize, the desired result would be
a high plausibility for words like the (previously seen) wordlist and surname as well as
the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like
cow and machine.
The predominant approach to generalizing over headwords, first introduced by
Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The
idea is to map all observed headwords onto synsets, and then generalize to a characteri-
zation of the selectional preference in terms of the WordNet noun hierarchy. This can be
achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson
2000; Clark and Weir 2001). The performance of these models relies on the coverage
of the lexical resources, which can be a problem even for English (Gildea and Jurafsky
2002). An alternative approach to generalization uses co-occurrence information, either
in the form of distributional models or through a clustering approach. These models,
which avoid dependence on lexical resources, use corpus data for generalization
(Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008).
In this article, we present a lightweight model for the acquisition and representa-
tion of selectional preferences. Our model is fully distributional and does not require
any knowledge sources beyond a large corpus where subjects and objects can be iden-
tified with reasonable accuracy. Its key point is to use vector space similarity (Lund
and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen
</bodyText>
<footnote confidence="0.9649735">
1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similar
predicates.
</footnote>
<page confidence="0.984101">
724
</page>
<note confidence="0.77701">
Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.999909888888889">
headwords. The vector space representations which serve as a basis for computing
similarity can in principle be computed from any arbitrary corpus, given that it is large
enough. In particular, this need not be the same corpus as the one on which we observe
predicate–headword co-occurrences. Our model thus distinguishes between a primary
corpus, from which the predicate–role–headword triples are extracted, and a generali-
zation corpus for computing the vector space representations. This distinction makes
it possible to apply our model to primary corpora with rich information that are too
small for efficient generalization, such as domain-specific corpora or corpora with
deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization
corpus is available. We empirically demonstrate the benefit of this distinction. We use
FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as
generalization corpus, modeling selectional preferences for semantic roles with near-
perfect coverage and low error rate.2
We evaluate our model on two tasks. The first task is pseudo-disambiguation
(Yarowsky 1993), where the model decides which of two randomly chosen words is a
better filler for the given argument position. This task tests model properties that are
needed for concrete semantic analysis tasks, most notably word sense disambiguation,
but also for semantic role labeling. The second task is the prediction of human
plausibility ratings, which is a standard task-independent benchmark for the quality
of selectional preferences. We test our model across a range of parameter settings to
identify best-practice values and show that it robustly outperforms both WordNet-
based and other distributional models on both tasks.
Finally, we investigate inverse preferences, that is, preferences that arguments
have for their predicates. Although there is ample cognitive evidence for the existence
of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been in-
vestigated systematically in linguistics. However, statistics about inverse preferences
have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al.
1999). We investigate the properties of inverse selectional preferences in comparison to
regular selectional preferences, and show that it is possible to predict inverse prefer-
ences with our selectional preference model as well.
The model that we discuss in this article, EPP, was first introduced in Erk (2007)
(using a pseudo-disambiguation task for evaluation) and further studied by Pad6, Pad6,
and Erk (2007) (evaluating against human plausibility judgments). In the current text,
we perform a more extensive evaluation and analysis, including the new evaluation on
inverse preferences, and we introduce a new similarity measure, nGCM, which achieves
excellent performance in many settings.
</bodyText>
<sectionHeader confidence="0.986661" genericHeader="method">
2. Computational Models of Selectional Preferences
</sectionHeader>
<bodyText confidence="0.767792625">
In this section, we provide an overview of corpus-based models of selectional prefer-
ences. See Table 1 for a summary of the notation that we use.
2 As descriptions of semantic classes of participants in events, selectional preferences are most naturally
applied to semantic argument positions, that is, semantic roles (such as agent or patient). In contrast,
syntactic argument positions (like subject and object) can comprise several semantic argument positions,
due to the presence of diathesis alternations, and thus show less consistent selectional preferences.
Nevertheless, work in computational linguistics also makes use of selectional preferences for syntactic
argument positions, considering them noisy approximations of semantic argument positions.
</bodyText>
<page confidence="0.992463">
725
</page>
<note confidence="0.740111">
Computational Linguistics Volume 36, Number 4
</note>
<tableCaption confidence="0.95129">
Table 1
</tableCaption>
<table confidence="0.437964875">
Notation used throughout the article.
w E Lemmas Word. We assume lemmatization throughout.
v E Preds Predicate. Preds may be a subset of Lemmas, or a set of
semantic classes.
r E Roles Role/Argument slot. Roles may be a set of grammatical
functions, or of semantic roles.
a E Args C_ Lemmas (Potential) argument headword.
c E C Semantic class on which selectional preferences are
</table>
<tableCaption confidence="0.624468">
conditioned, for example, WordNet sense, FrameNet frame,
or latent semantic class.
</tableCaption>
<bodyText confidence="0.601334833333333">
VS = (DTrans, Vector space. Basis is a set of basis elements, sima similarity
Basis, sim, STrans) measure, DTrans a transformation of raw counts, and STrans
a transformation of the space.
We write w� = (wb1, ... , wbn) for the representation of w E
Lemmas in a vector space with Basis = {b1,...,bn}.
wtr,v(a) Weight of argument headword a for predicate v and role r.
</bodyText>
<subsectionHeader confidence="0.888477">
2.1 Historical Models
</subsectionHeader>
<bodyText confidence="0.999972764705882">
In formal linguistics, selectional restrictions were employed as strict Boolean restrictions
by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual dis-
ambiguation process between predicates and their modifiers. Sentences are semantically
anomalous if there are no mutually consistent readings for the two words. Semantically
anomalous sentences would receive no reading, whereas ambiguous sentences would
receive several readings.
The strict dismissal as meaningless of sentences that violate selectional restrictions
was later criticized. A case in point is metaphors, which often combine predicates and
arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated
that “rejecting utterances is just what humans do not. They try to understand them.”
He proposes to reconceptualize selectional restrictions as preferences whose violation
is dispreferred, but not fatal. His proposal for a semantic interpretation mechanism still
uses semantic primitives, but always produces a single most plausible interpretation by
choosing the senses of each word that maximize the compatibility between selectional
preferences and semantic types. In this manner, he is able to compute semantic repre-
sentations for sentences that violate selectional restrictions, including metaphors such
as “my car drinks gasoline.”
</bodyText>
<subsectionHeader confidence="0.9981">
2.2 Semantic Hierarchy–Based Models
</subsectionHeader>
<bodyText confidence="0.999291">
The first broad-coverage computational model of selectional preferences, and still one
of the best-known ones, namely that of Resnik (1996), belongs to the class of semantic
hierarchy–based models. These models generalize over observed headwords using a
semantic hierarchy or ontology for nouns. The two main advantages of such models are
that (a) they can make predictions for all words covered by the hierarchy, even for very
infrequent ones for which distributional representations tend to be unreliable; and (b)
the hierarchy robustly guides generalization even for few observed headwords.
Resnik’s model instantiates the set of relations Roles with grammatical functions
which can be observed in syntactically analyzed corpora. More specifically, it concen-
</bodyText>
<page confidence="0.991049">
726
</page>
<note confidence="0.847333">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.921137571428571">
trates on selectional preferences for subjects and objects. For the generalization step,
Resnik’s model maps all headwords onto WordNet synsets (or classes) c. Resnik first
computes the overall selectional preference strength for each verb–relation pair (v, r),
that is, the degree to which the pair constrains possible fillers. To estimate this quantity,
the distribution of WordNet synsets for this particular verb–relation pair is compared
to the distribution of synsets over all verbs, given the relation r. Technically, this is
achieved using Kullback–Leibler divergence:
</bodyText>
<equation confidence="0.700523666666667">
� P(c|v,r)log (P(c|v,r)) (1)
SelStr(v, r) = D(P(c|v,r)||P(c|r)) = P(c|r)
c∈C
</equation>
<bodyText confidence="0.999580666666667">
The parameters P(c|v, r) and P(c|r) are estimated from the corpus frequencies of tuples
(v, r, a) and the membership of nouns a in WordNet classes c: The observed frequency
of (v, r, a) is split equally among all WordNet classes for a. This avoids word sense
disambiguration, but incurs a certain share of wrong attributions. The intuition of
SelStr(v, r) is that a verb–relation pair that only allows a limited range of argument heads
will have a posterior distribution over classes that strongly diverges from the prior.
Next, the selectional association of the triple, SelAssoc(v, r, c), is computed as the
ratio of the selectional preference strength for this particular class c to the overall selec-
tional preference strength of the verb–relation pair (v, r). This is shown in Equation (2).
</bodyText>
<equation confidence="0.991149">
P(c|v, r)logP(c|v,r)
P(c|r)
SelAssoc(v, r, c) = (2)
SelStr(v, r)
</equation>
<bodyText confidence="0.999925933333333">
Finally, the selectional preference between a verb, a relation, and an argument head
is defined as the maximal selectional association of the verb, the relation, and any
WordNet class c that the argument can instantiate. We will refer to this model as
RESNIK herein.
In subsequent years, a number of WordNet-based models were developed that
differ from Resnik’s model in the details of how the generalization in the WordNet
hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a
tree cut through the WordNet noun hierarchy that minimizes tree cut length while
maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by
ascending the WordNet noun hierarchy as long as the degree of selectional preference
among siblings is not significantly different. Ciaramita and Johnson (2000) encode
WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “ex-
plain away” ambiguity. Grishman and Sterling (1992) perform generalization on the
basis of a manually constructed semantic hierarchy specifically developed on the same
corpus.
</bodyText>
<subsectionHeader confidence="0.955106">
2.3 Distributional Models
</subsectionHeader>
<bodyText confidence="0.9962094">
Distributional models do not make use of any lexicon resource for the generalization
step. Instead, they use word co-occurrence—typically obtained from the same corpus
as the observed headwords—for generalization. This independence from manually
constructed resources gives distributional models a good cost–benefit ratio and makes
them especially attractive for domain-specific applications. These models, like the
</bodyText>
<page confidence="0.984953">
727
</page>
<note confidence="0.797615">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.999674">
semantic hierarchy–based models, usually use grammatical functions as the set Roles
for which selectional preferences are predicted.
Pereira, Tishby, and Lee (1993) and Rooth et al. (1999) generalize by discovering
latent classes of noun–verb pairs with soft clustering. They model the probability of
a word a as the argument of a predicate v as the probability of generating v and a
independently from the latent classes c:
</bodyText>
<equation confidence="0.9939635">
P(v,a) _ � �P(c,v,a) _ P(c)P(v|c)P(a|c) (3)
cEC cEC
</equation>
<bodyText confidence="0.9992774">
Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P(c),
P(v|c), and P(a|c). Their procedure supports hierarchical clustering and can optimize
the number of clusters. Rooth et al. (1999) present a simpler Expectation Maximization–
based estimation procedure which takes the number of clusters as input parameter. We
refer to this model as ROOTH ET AL. herein.
Dagan, Lee, and Pereira (1999) introduce a general model for computing co-
occurrence probabilities with similarity-based smoothing. Although not intended as a
model of selectional preferences, it can also be interpreted as such. Given a similarity
measure sim defined on word pairs, they compute the smoothed occurrence probability
of a word w2 given w1 as
</bodyText>
<equation confidence="0.922235">
�Psim(w2|w1) _
wESimset(w1)
where Simset(w) is the set of words most similar to w according to sim, and Z(w1) _
E
wESimset(w1) sim(w1,w) is a normalizing factor. This model predicts w2 given w1 by
</equation>
<bodyText confidence="0.999552615384615">
backing off from w1 to words w similar to w1. The contribution of each w in predicting
P(w2|w1) is weighted by sim(w1,w). The similarity sim(w1,w) is computed on vector
space representations.
Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative ap-
proach to the prediction of selectional preferences. The features they use are mainly co-
occurrence statistics, enriched with morphological context features to alleviate sparse
data problems for low-frequency argument heads. They train one SVM per verb–
argument position pair, using unobserved verb–argument combinations as negative
examples, which makes their approach independent of manually annotated training
data. Schulte im Walde et al. (2008) present a model that combines features of the
semantic hierarchy–based and the distributional approaches by integrating WordNet
into an EM-based clustering model; Schulte im Walde (2010) shows that integrating
noun–modifier relations improves the prediction of human plausibility judgments.
</bodyText>
<subsectionHeader confidence="0.999102">
2.4 Semantic Role–Based Models
</subsectionHeader>
<bodyText confidence="0.999979833333333">
The third class of models takes advantage of semantic resources beyond simple seman-
tic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the
prediction of selectional preferences for semantic roles rather than grammatical func-
tions. From a linguistic perspective, semantic roles represent a more appropriate level
for defining selectional preferences. For that reason, the role annotation provides cleaner
and more specific training data than even a manually syntactically annotated corpus
</bodyText>
<equation confidence="0.999084">
sim(w1, w)
Z(w 1)
P(w2|w) (4)
</equation>
<page confidence="0.972625">
728
</page>
<note confidence="0.42358">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.998359375">
would. These advantages, however, come at the cost of considerably greater sparsity
issues.
Padó, Crocker, and Keller (2009) present a model based on FrameNet (Fillmore,
Johnson, and Petruck 2003). This model estimates selectional preferences with a gen-
erative probability model that equates the plausibility of a (v, r, a) triple with the joint
probability of observing the thematic role r, the verb v, and the argument a, plus the
verb’s FrameNet sense c and the grammatical function gf of the argument. This joint
probability can be decomposed using the chain rule:
</bodyText>
<equation confidence="0.996126">
P(v,c,r,gf,a)=P(v)P(civ)P(rIv,c)P(gfIr,v,c)P(algf,r,v,c) (5)
</equation>
<bodyText confidence="0.999890857142857">
The model does not make any independence assumptions. To counteract sparse data
issues for the more complex terms, the model applies WordNet-based generalization
(for nouns), distributional clustering (for verbs), and Good–Turing smoothing. We refer
to this model as PADO ET AL. Another semantic role–based model was proposed by
Vandekerckhove, Sandra, and Daelemans (2009). It acquires selectional preferences for
PropBank roles from a PropBank-labeled corpus, generalizing to unseen headwords
with memory-based learning.
</bodyText>
<sectionHeader confidence="0.893065" genericHeader="method">
3. A Distributional Exemplar-Based Model of Selectional Preferences: EPP
</sectionHeader>
<bodyText confidence="0.999968076923077">
We now present the EPP model of selectional preferences. It falls into the category of
distributional models. More specifically, it is an exemplar model that remembers all
seen headwords for a given argument position and computes the degree of plausibility
for a new headword candidate through its similarity to the stored exemplars. Exemplars
are modeled as vectors in a semantic space.
Exemplar models are a well-known modeling framework that is used in psychol-
ogy (Nosofsky 1986), in computational linguistics (under the name of memory-based
learning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonet-
ics (Hay, Nolan, and Drager 2006). The appeal of exemplar models is that they provide
a cognitively plausible process of learning as storing exemplars, and categorization as
similarity computation that is grounded in features of the exemplars (e.g., formants in
phonetics, and contexts in lexical semantics).
The representation of selectional preferences through feature vectors also fits in well
with work in psycholinguistics by McRae, Ferretti, and Amyote (1997), who studied the
characterization of verb selectional preferences through features elicited from human
subjects. They found high overlap between features used to characterize the selectional
preferences on the one hand, and features listed for typical role fillers on the other hand.
For example, features generated for the agent role of frighten include mean, scary, and
ugly, features that were also highly relevant for the typical filler noun monster.
As briefly mentioned in Section 1, we consider selectional preferences to be charac-
terizations of typical fillers for the semantic roles of a predicate. Still, we keep our model
modular to different notions of argumenthood, such that it is also applicable to the
computation of selectional preferences for syntactic dependents of a predicate, as this is
an important case for computational applications. When we compute selectional prefer-
ences for syntactic dependents rather than semantic roles, we view syntactic argument
positions as noisy approximations of semantic roles.
</bodyText>
<page confidence="0.992921">
729
</page>
<figure confidence="0.340515">
Computational Linguistics Volume 36, Number 4
</figure>
<subsectionHeader confidence="0.980166">
3.1 The Model
</subsectionHeader>
<bodyText confidence="0.998907142857143">
As stated previously, we assume that we have two corpora which assume different func-
tions in the model: the primary corpus, which provides information about predicate–
argument co-occurrences but may be too sparse for generalization; and the large, but
potentially noisy, generalization corpus, from which we obtain reliable semantic simi-
larity estimates.
Thus, the first step is the extraction of triples (v,r,a) of a predicate v E Preds, a
relation r E Roles, and a headword a E Args from the primary corpus. Let Seenargs(r,v)
be the set of argument headwords seen with an argument position r of a predicate v
in the primary corpus. Given these triples, we predict the plausibility for an arbitrary
noun a0 in position (v, r) through the semantic similarity of a0 to all the members
of Seenargs(r,v). We obtain these similarity ratings by first computing vector space
representations for both and the members of seen(r,v) from the generalization corpus,
and then using a standard vector space similarity measure. We compute the plausibility
for a0 as
</bodyText>
<equation confidence="0.8935616">
SelprefEPPr,v(a0) = � wtr,v(a) · sim(a0,a) (6)
a∈Seenargs(r,v) Zr,v
where sim(a0,a) is the similarity between the vector space representations of a0 and
a, wtr,v(a) a weight for the seen headword a, and Zr,v a normalization constant, Zr,v =
E
</equation>
<bodyText confidence="0.920860833333333">
a∈Seenargs(r,v) wtr,v(a), so that the number of observed exemplars for each (v, r) pair does
not matter. Because SelprefEPP is basically a weighted average over similarity values, the
range of SelprefEPP is identical to the range of the employed similarity function sim. For
example, the range is [−1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf.
Section 3.3). We discuss possible choices of both the similarity sim and the weight wtr,v
in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.997803">
3.2 Vector Space Representations
</subsectionHeader>
<bodyText confidence="0.999843166666667">
We use vector space representations for generalization. In a vector space model, each
target word is represented as a vector, typically constructed from co-occurrence counts
with context words in a large corpus (the so-called basis elements). The underlying
assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar
meanings occur in similar contexts and will be assigned similar vectors. Thus, the
distance between the vectors of two target words, as given by some distance measure
(e.g., Cosine or Jaccard), reflects their semantic similarity.
Vector space models are simple to construct, and the semantic similarity they pro-
vide has found a wide range of applications. Examples in NLP include information
retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette
1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources
based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range
of applications that profit from knowledge about word similarity. In cognitive science,
they have been used, for example, to account for the influence of context on human
lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald
2000).
An idealized example for a semantic space representation of selectional preferences
is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the
</bodyText>
<page confidence="0.994226">
730
</page>
<note confidence="0.706448">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<figureCaption confidence="0.978885">
Figure 1
</figureCaption>
<bodyText confidence="0.986525666666667">
An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter).
fillers of the agent and patient position of shoot, respectively. In order to judge whether a
hunter is a plausible agent of shoot, the vector space representation of hunter is compared
to the members of the exemplar cloud for the agent position—namely, poacher, policeman,
and director. Due to the high average similarity of the hunter vector to these vectors,
hunter will be judged a fairly good agent of shoot. Compare this with the result for the
patient role: hunter is rather distant from roe, deer, and buck, and is therefore predicted
to be a bad patient of shoot. However, note that hunter is still more plausible as a patient
of shoot than, for example, director.
</bodyText>
<subsectionHeader confidence="0.998437">
3.3 Formalization and Parameter Choice
</subsectionHeader>
<bodyText confidence="0.99993419047619">
Vector space models have been formalized by Lowe (2001) as tuples VS = (DTrans,
Basis, sim, STrans), where Basis is a set of basis elements or dimensions, DTrans is a
transformation of raw co-occurrence counts, sim is a similarity measure, and STrans is
a transformation of the whole space, typically dimensionality reduction. An additional
parameter that becomes relevant for our use of vector spaces (cf. Equation [6]) is the
weighting function wt that determines the contribution of each exemplar to the overall
similarity. We discuss the parameters in turn and discuss our reasons for either explor-
ing them or fixing them.
Basis elements Basis. Traditionally, context words are used as basis elements, and co-
occurrence is defined in terms of a surface window. Such bag-of-words spaces tend to
group words by topics. They ignore the syntactic relation between context items and
the target, which is a problem for selectional preference modeling. The top table in
Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though
they show complementary plausibility ratings. The reason is that deer and hunter often
co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities).
The bottom table in Figure 1(b) indicates a way out of this problem, namely the use
of word-relation pairs as basis elements (Grefenstette 1994; Padó and Lapata 2007).
This space splits the co-occurrences with context words such as shoot based on the
grammatical relation between target and context word, and this split looks different
for different words: whereas deer occurs exclusively as the object of shoot, hunter pre-
dominantly occurs as the subject. We find the reverse pattern for escape. In consequence,
</bodyText>
<page confidence="0.995943">
731
</page>
<table confidence="0.438505">
Computational Linguistics Volume 36, Number 4
</table>
<tableCaption confidence="0.987843">
Table 2
</tableCaption>
<bodyText confidence="0.788928">
Similarity measures explored in this article. Notation: We assume Basis = {b1, ... , bn}. We write I
for mutual information, and BE(a) for the set of basis elements that co-occur at least once with a.
</bodyText>
<equation confidence="0.990210357142857">
Lin(
E
sim R R&apos;) = E( )EBE(a)n BE(a&apos;) I(a,r,v)+I(a&apos;,r,v) El= ab ·a&apos;b
&apos;&apos;° sim (a a&apos;) i
(r,v)EBE(a) I(a,r,v)E(r,v)EBE(a&apos;) I(a&apos;,r,v) cosine = I1a||·||a�11
i |BE(a)∩BE(a&apos;)|
simJaccard(a,a) = |BE(a)∪BE(a&apos;)|
simnGCM(a,a&apos;) = exp( − /En 1 ( ||a   |a&apos; ||)2) where ||�a ||= /fin 1 abi
simHindle(a,a&apos;) = Eni= 1 simHindle(a,a&apos;,i) where v
I( min(I(a,bi),I(a&apos;,bi)) if I(a, bi) &gt; 0 and I(a&apos;, bi) &gt; 0
simHindle(a,d,i) = ( abs(max(I(a,bi),I(a&apos;,bi))) if I(a,bi) G 0 and I(a&apos;, bi) G 0
&apos;
i0
abi ab
</equation>
<bodyText confidence="0.999912785714286">
the resulting spaces gain the ability to distinguish between words like hunter and deer,
based on differences in typical occurrences in argument positions.
On the downside, dependency-based spaces are more expensive to compute than
word-based spaces because they require a corpus with syntactic analysis. Thus, we
explore both options. The word-based space records co-occurrences within a surface
window of 10 (lemmatized) words.3 We refer to it as WORDSPACE. The dependency-
based space, called DEPSPACE, has basis elements consisting of a grammatical function
concatenated with a word, as in the bottom example in Figure 1(b) (Padó and Lapata
2007). Following earlier experiments on the representation of selectional preferences
in word-dependency-relation spaces (Padó, Padó, and Erk 2007), we use asubject–
object context specification that only considers co-occurrences between verbs and their
subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items
as basis elements.
Similarity measure sim. In principle, any similarity measure for vectors can be plugged
into our model. Previous studies that compared similarity measures came to various
conclusions about the usefulness of different measures. Cosine similarity is very popu-
lar in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient
in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice
emerged in first place. Padó and Lapata (2007) found good results with Lin’s measure
for predominant word sense identification.
Because it is unclear whether the findings about best similarity measures general-
ize to new tasks, we will investigate a range of similarity measures shown in Table 2:
Cosine, the Dice and Jaccard coefficients, Hindle’s (1990) and Lin’s (1998) mutual
information-based metrics, and an adaptation of Nosofsky’s (1986) Generalized Context
Model (GCM), a model for exemplar-based similarity from psychology. The original
GCM includes normalization by summed similarity over all classes of exemplars, which
introduces competition between categories. Our version, which we call nGCM, instead
normalizes by vector length to alleviate the influence of overall target frequency, but
</bodyText>
<footnote confidence="0.99352975">
3 We do not remove stop words for reasons of simplicity, as there is no unequivocal definition of this set,
and we do not wish to remove potentially informative contexts.
4 This context specification is available as soonly in the DependencyVectors software package
(http://www.nlpado.de/∼sebastian/dv.html) starting from Release 2.5.
</footnote>
<equation confidence="0.85688875">
=
i
simDice(a,a)
2·|BE(a)∩BE(a&apos;)|
|
|
|
|
BE(a)
+
BE(a&apos;)
else
</equation>
<page confidence="0.972363">
732
</page>
<note confidence="0.423611">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.999874366666667">
preserves the central idea that similarity decreases exponentially with distance (Shepard
1987).
All similarity measures from Table 2 are applicable to semantic spaces with arbitrary
basis elements, with the exception of the Lin measure, whose definition applies only
to dependency-based spaces. The reason is that it decomposes the basis elements into
relation–word pairs (r,v). For semantic spaces with words as basis elements, the Lin
measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007).
Transformations DTrans and STrans. Next, we come to transformations on counts and vec-
tor spaces. Concerning the count transformations DTrans, all counts are log-likelihood
transformed (Dunning 1993), a standard procedure for word-based semantic space
models which alleviates the problematic effects of the Zipfian distribution of lexical
items, as proposed by Lowe (2001). As for transformations on the complete space STrans,
many studies do not perform dimensionality reduction at all. Others, like the LSA fam-
ily of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To
gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500-
dimensional spaces created using Principal Component Analysis (PCA), a standard
method for dimensionality reduction that identifies the directions of highest variance
in a high-dimensional space.
Weight functions wt. Exemplar-based models are usually applied in conjunction with a
function that can assign each exemplar an individual weight, which can be interpreted
cognitively as degree of activation (Nosofsky 1986). We assess a small number of
weight functions to investigate their importance within the EPP model. The first one,
UNI, assumes a uniform distribution, wtr,v(a) = 1. The second one, FREQ, uses the co-
occurrence frequency as weight, wtr,v(a) = freq(a,r,v), with the intuition that more fre-
quent exemplars should be both more activated and more reliable. Finally, we consider
a weight function that is an analogue of inverse document frequency in Information
Retrieval. It we�i.ghts words higher that occur with a smaller number of verb–role pairs:
wtr,v(a) = log senrv(a)|&apos;)|, where we write Seenrv(a) for the set of verb–role pairs (r,v)
for which a occurs as a headword.5 We abbreviate this weight function by DISCR for
‘discrimination’.
</bodyText>
<subsectionHeader confidence="0.872419">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999896083333333">
Our EPP model can be seen as a straightforward implementation of the intuition to
model selectional preference by generalizing from seen headwords to other, similar,
words. We use vector space representations to judge the similarity of words, obtaining
a completely corpus-driven model that does not require any additional resources and is
very flexible. A complementary view on this model is as a generalization of traditional
vector space models that represent semantic similarities between pairs of words. The
EPP model goes beyond this by computing similarity between a vector and a set of other
vectors. By instantiating the set with the vectors for seen headwords of some relation r,
the similarity turns into a plausibility prediction that is specific to this relation.
Like other distributional models, the EPP model is applicable whenever corpus
data are available; no lexical resource is required. Additionally, it does not require the
headword observation step and the generalization step (cf. Section 1) to use the same
</bodyText>
<footnote confidence="0.825625">
5 By keeping the constant l Uar Seenrv(a&apos;)|, we guarantee that the fraction remains larger than one, and
wtr,v(a) remains positive. This is to ensure that the weighted average in Equation (6) yields correct results.
</footnote>
<page confidence="0.985924">
733
</page>
<note confidence="0.310588">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.991103710526316">
corpus.6 This allows us to work with a relatively small and deeply linguistically ana-
lyzed corpus of seen headwords, the FrameNet corpus, while using a much bigger data
set to generalize over seen headwords. It also allows us to make predictions for the
potentially deeper relations annotated in the primary corpus, for example, semantic
roles. We will investigate the potential of this setup in our Experiments 1 and 2.
As a distributional model, EPP avoids the two pitfalls of resource-based models.
One is a coverage problem due to the limited size of the resource (see the task-based
evaluation in Gildea and Jurafsky [2002]). For example, the semantic role–based PADO
ET AL. model resorts to class-based smoothing methods to improve coverage, which
EPP does not need. The other problem of resource-based models is that the shape of the
WordNet hierarchy determines the generalizations that the models make. These are not
always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives
a high preference because tragedy in WordNet is a type of written communication, which
is a preferred argument class of answer.
The ROOTH ET AL. model (Rooth et al. 1999) shares the resource independence of
EPP, but has complementary benefits and problems. Querying the probabilistic ROOTH
ET AL. model takes only constant time, whereas querying the exemplar-based EPP
model takes time linear in the number of seen arguments for the argument position.
However, the ROOTH ET AL. model requires a dedicated training phase with a space
complexity linear in the total number of verbs and nouns, which can lead to practical
problems for large corpora (cf. Section 5.1). The separation of similarity computation
and headword observation in EPP also gives the experimenter more fine-grained control
over the types and sources of information in the model.
The EPP model looks superficially similar to the model of Dagan, Lee, and Pereira
(1999). However, they differ in the role of the similarity measure: The Dagan, Lee, and
Pereira model computes a co-occurrence probability, and it uses similarity as a weight-
ing scheme. The EPP model computes similarity (of a word to the typical fillers of an
argument position), and its weighting schemes are separate from the similarity measure.
The two models also differ in the kinds of items they consider as a basis for generaliza-
tion (or smoothing): In computing the probability of seeing a word w2 after w1, the sum
in the Dagan, Lee, and Pereira model runs over all words that are similar to w1, whereas
the sum in the EPP model runs over all words that have been seen as headwords in the
argument position in question. Given that occurrence in an argument position is a form
of co-occurrence, and similarity (in both models) is computed on the basis of vectors
derived from co-occurrence counts, one could say that the sum in the EPP model runs
over words determined by first-order co-occurrence, whereas the sum in Dagan, Lee,
and Pereira runs over words chosen through second-order co-occurrence (where w1 and
w2 are second-order co-occurring if they both tend to occur with the same words w3).
</bodyText>
<sectionHeader confidence="0.487307" genericHeader="method">
4. Design of the Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999656833333333">
In this section, we give a high-level overview over the experiments and experimental
settings we will use subsequently. Details will be provided in the following sections.
We evaluate the EPP model in three ways: We test the prediction of verbal
selectional preference models with a pseudo-disambiguation task (Experiment 1).
Then, we address the task of predicting human verb–argument plausibility ratings
(Experiment 2). Finally, we investigate inverse selectional preferences—preferences of
</bodyText>
<page confidence="0.907044">
6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option.
734
</page>
<bodyText confidence="0.985910306122449">
Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences
nouns for the predicates that they co-occur with—again using pseudo-disambiguation
(Experiment 3).
We compare the EPP model to models from the three model categories presented in
Section 2: RESNIK as a hierarchical model; ROOTH ET AL. as a distributional model; and
PADO ET AL. as a semantic role–based model. As both Brockmann and Lapata (2003)
and Pad6 (2007) have argued, no WordNet-based model systematically outperforms
the others, and the RESNIK model shows the most consistent behavior across different
scenarios. Among the distributional models, we choose ROOTH ET AL. as a model that
performs soft clustering and thus shows a marked difference to the EPP model. To our
knowledge, this is the first comparison of all three generalization paradigms: semantic
hierarchy–based, distributional, and semantic role–based.7
As mentioned earlier, we employ two tasks to evaluate the four models: pseudo-
disambiguation and the prediction of human plausibility ratings. The pseudo-
disambiguation task (Yarowsky 1993) has become a standard evaluation measure for
selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al. 1999). Given
a choice of two potential headwords, the task of a selectional preference model is to
pick the more plausible one to fill a particular argument position of a given predicate.
Pseudo-disambiguation can be viewed as a word sense disambiguation task in which
the two potential headwords together form a “pseudo-word,” for example herb/struggle
from the original words herb and struggle. The task is to “disambiguate” the pseudo-
word to the word that fits better in the given context. It can also be viewed as an in vitro
version of semantic role labeling and dependency parsing (depending on whether the
relations are semantic roles or grammatical functions) (Zapirain, Agirre, and Màrquez
2009). In this case, the scenario is that of a sentence containing a predicate and two
words that could potentially fill an argument position of that predicate, for example, the
predicate recommend with the potential headwords herb and struggle for the grammatical
relation of direct object. The task is to decide which of the two potential headwords is
better suited to fill the argument position.
Human plausibility ratings, on the other hand, make considerably more fine-
grained distinctions than those occurring in pseudo-disambiguation tasks. Here, mod-
els predict the exact human ratings for verb–argument–role triples. Ratings are collected
to further control carefully selected experimental items for psycholinguistic studies
(Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus
1998), or are solicited for corpus-derived triples specifically to create evaluation data for
plausibility models (Brockmann and Lapata 2003; Pad6 2007).
We contrast two different levels of semantic analysis for the predicates and argu-
ment positions. In the SEM PRIMARY setting, the predicates are FrameNet frames, each
of them potentially instantiated by multiple different verbs. The argument positions in
these settings are frame-semantic roles. This setting most closely matches the notion
of selectional preferences as characterizations of semantic arguments of an event. In
addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument
positions are grammatical functions (subject and direct object). Viewing grammatical
functions as shallow approximations of semantic roles, we can expect the selectional
preference models for this setting to yield noisier estimates than in the SEM PRIMARY
setting. The two settings will differ only in the choice of primary corpus, but will use
the same generalization corpus.
7 Erk (2007) has a comparison between hierarchy-based and distributional models, but does not include a
semantic role–based model.
</bodyText>
<page confidence="0.975754">
735
</page>
<note confidence="0.279723">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.999804666666667">
Table 3 illustrates the difference between the SEM PRIMARY setting and the SYN
PRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARY
setting has predicates like the FrameNet frame (predicate sense) ADORNING, with the
semantic role THEME as argument position. In contrast, the SYN PRIMARY setting has
predicates that are verb lemmas, such as cause, and argument positions that are gram-
matical functions (subj). In both settings, the two potential headwords (here called
headword and confounder, to be explained in more detail in the next section) to be
distinguished in the pseudo-disambiguation task are noun lemmas.
The verb–dependency–headword tuples of the SYN PRIMARY setting yield much
more coarse-grained and noisy characterizations of selectional preferences; however,
they can be extracted from corpora with only syntactic annotation. We are therefore
able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this
setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could
parse almost all of the corpus, resulting in 6,005,130 parsed sentences.
For the SEM PRIMARY setting, we require a primary corpus with role-semantic
annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck
2003). FrameNet is a semantic lexicon for English that groups words in semantic classes
called frames and lists fine-grained semantic argument roles for each frame. Ambiguity
is expressed by membership of a word in multiple frames. Each frame is exemplified
with annotated example sentences extracted from the BNC. The FrameNet release 1.2
comprises 131,582 annotated sentences (roughly three million words). To determine
headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.
As generalization corpus, we use the Minipar-parsed BNC in both settings. The ex-
perimentation with two different primary corpora allows us to directly study the influ-
ence of the disambiguation of predicates and the semantic characterization of argument
positions on the performance of selectional preference models. Note, however, that the
comparison is complicated by differences between the two corpora: The primary corpus
for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the
determination of predicates, grammatical functions, and headwords. The primary cor-
pus for the SEM PRIMARY setting is manually annotated for semantics but is parsed
automatically to determine headwords. This can introduce noise in the headwords, but
not in the determination of predicates and semantic roles. Also, the primary corpus for
the SYN PRIMARY setting is much larger than the one used in the SEM PRIMARY setting.
</bodyText>
<sectionHeader confidence="0.853377" genericHeader="method">
5. Experiment 1: Pseudo-Disambiguation
</sectionHeader>
<bodyText confidence="0.997691">
The first experiment uses a pseudo-disambiguation task to evaluate the models’ perfor-
mance on modeling the plausibility of nouns as headwords of argument positions of
verbal predicates.
</bodyText>
<tableCaption confidence="0.948988">
Table 3
</tableCaption>
<footnote confidence="0.6134705">
Pseudo-disambiguation items for the SYN PRIMARY setting and the SEM PRIMARY setting.
Setting Predicate (v) Arg. pos. (r) Headword (a) Confounder (a�)
SYN cause subj succession island
appear subj feasibility desire
SEM ADORNING THEME illustration axe
ROPE_MANIPULATION ROPE cord literature
</footnote>
<page confidence="0.988027">
736
</page>
<bodyText confidence="0.304583">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</bodyText>
<listItem confidence="0.939752857142857">
Require: Some corpus T: a list of triples (v, r, a) of seen predicates, roles, and arguments.
Require: Some corpus N: a list of noun lemmas, along with a function freqN : N -4 1N
that associates each noun n E N with its corpus frequency.
1: Nmid = In E N I freqN(n) &gt; 30 and freqN(n) &lt; 3,000}
2: We define a probability distribution over the n E N b n freqN(n)
p ty PN mid y PN( n) = E
m freqN(m)
3: conf = { } # set of headword/confounder mappings, starts empty
4: AT = {a I (v, r, a) E T} # set of seen headwords
5: for every a in AT do
6: choose a confounder a&apos; E Nmid according to pN
7: conf = conf U I a H a&apos; }
8: end for
9: Return: conf
</listItem>
<figureCaption confidence="0.914469">
Figure 2
</figureCaption>
<bodyText confidence="0.267493">
Algorithm for choosing confounders.
</bodyText>
<subsectionHeader confidence="0.967258">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.997674258064516">
Task and data. In a data set of tuples (v, r, a) of a predicate v, argument position r, and
headword a, each tuple is paired with a confounder a&apos;. The task is to pick the original
headword by comparing the tuples (v, r, a) and (v, r, a&apos;). Table 3 shows some examples.
We begin by collecting all triples (v, r, a) observed in the respective primary corpus.
In the SYN PRIMARY setting, this corresponds to all headwords observed in subject or
direct object position of a verbal predicate in the BNC, and in the SEM PRIMARY setting,
to all nouns observed as headword of some semantic role in a frame introduced by a
verb. From this set of triples (v, r, a) for a given primary corpus, we draw an evaluation
sample that is balanced by the corpus frequency of predicates and argument position.
As test set, we choose 100 (v, r) pairs at random, drawing 20 pairs each from five fre-
quency bands: 50–100 occurrences; 100–200 occurrences; 200–500; 500–1,000; and more
than 1,000 occurrences. For any chosen predicate–relation pair, we sample triples (v, r, a)
equally from six frequency bands of arguments a: 1–50 occurrences; 50–100; 100–200;
200–500; 500-1,000; and more than 1,000 occurrences. These evaluation samples contain
a total of 213,929 (SYN) and 65,902 (SEM) tuples.
Next, we pair each headword with a confounder sampled from the primary corpus
as described in Figure 2.8 In the literature, there have been two different approaches to
choosing confounders for pseudo-disambiguation tasks: The first approach, used by
Dagan, Lee, and Pereira (1999), chooses confounders to match the headword a in
frequency. The second approach, used in Rooth et al. (1999), sets the probability that
a word is drawn as a confounder to its relative frequency. The advantage and dis-
advantage of the first approach is that it largely eliminates the frequency bias that is
a general problem of vector space-based approaches. This is an advantage in that it
allows the generalization achieved by the model to be evaluated without any distortion
from frequency bias. It is a disadvantage in that in any practical application making
use of selectional preferences, the data will not be frequency-balanced. For example,
selectional preferences could be used by a dependency parser to decide which word in
the sentence to link to a given verb via a subject edge, or selectional preferences could
8 The confounder is the same for all instances of the headword a in the evaluation sample, regardless of the
values for r and v. As confounder candidates, we only use words with between 30 and 3,000 occurrences
in the BNC, following Rooth et al. (1999).
</bodyText>
<page confidence="0.95016">
737
</page>
<note confidence="0.272913">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.999867666666667">
be used by a semantic role labeler to decide which constituent is the overall best filler
for the AGENT role for a given predicate. In such cases, it does not appear warranted to
assume that the frequencies of different headword candidates are balanced. We choose
the second option for our experiments, using relative corpus frequency to approximate
the probability of encountering different headword candidates.
Training of models. As stated earlier, we evaluate all models in the SYN PRIMARY setting
and the SEM PRIMARY setting. In all experiments herein, we perform two 2-fold cross-
validations runs. In each run, we randomly split the respective (SYN or SEM) evaluation
sample into a training and a test set at the token level. Figure 3 describes the experimen-
tal procedure in pseudo-code.
The EPP, RESNIK, and PADO ET AL. models are trained on the training split of the
evaluation sample. The EPP model additionally uses the BNC as generalization corpus
in both the SYN PRIMARY setting and the SEM PRIMARY setting. This generalization
corpus is used to compute either a WORDSPACE or a DEPSPACE vector space, as
discussed in Section 3.3. For the ROOTH ET AL. model, we had to employ a frequency
</bodyText>
<listItem confidence="0.981851870967742">
Require: A set Formalisms of formalisms to test
Require: A primary corpus T: a list of triples (v, r, a) of seen predicates, argument
positions, and arguments, along with a function freqT : T → 1N that associates each
triple (v, r, a) E T with its corpus frequency
Require: A mapping conf : Lemmas → Lemmas of headwords to confounders such that
{a  |(v, r, a) E T} C Domain(conf )
1: eval_results = { }
2: for splitno in 1:2 do
3: # prepare two independent splits
4: half1 = { }, half2 = { } # mappings from headwords to counts
5: for each tuple t in T do
6: # decide how many occurrences of t to put in half1, half2 by drawing from the binomial
distribution
7: Sample k ∼ B( freqT(t), 0.5)
8: half1 = half1 U { t H k }, half2 = half2 U { t H freqT(t) − k }
9: end for
10: splits = { (half1, half2), (half2, half1) }
11: for (ftrain,ftest) in splits do
12: for each formalism F in Formalisms do
13: train a model mF according to formalism F using the training set defined by
the frequency function ftrain.
14: for each tuple (v, r, a) in T do
15: for i in 1:ftest(v, r, a) do
16: Evaluate the performance of mF on the tuple (v, r, a, conf (a)) and add the
result to eval_results
17: end for
18: end for
19: end for
20: end for
21: end for
22: Return: eval_results
</listItem>
<figureCaption confidence="0.628817">
Figure 3
</figureCaption>
<bodyText confidence="0.3405">
Algorithm for running a pseudo-disambiguation experiment
</bodyText>
<page confidence="0.961281">
738
</page>
<bodyText confidence="0.983608387096774">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
cutoff of five in the SYN PRIMARY setting to reduce the amount of training data due to
memory limitations. The PADO ET AL. model is only used in the SEM PRIMARY setting:
FrameNet is an integral part of this model, and it cannot be used in a syntax-only
setting without major changes. For details on training, see Section 2.4. Note that no
verb classes had to be induced from the data, because the predicates v are already
instantiated by verb classes, namely, FrameNet frames (see Table 3).
Finally, we report three baselines. The first baseline, headword frequency (HW), is
very simple. It decides between the headword a and the confounder a&apos; by comparing
the frequencies f (a) and f (a&apos;). The second, more informed, baseline is triple frequency
(TRIPLE). It votes for a if f (v, r, a) &gt; f (v, r, a&apos;), and vice versa. The third baseline, a bigram
language model (LM), was constructed by training a 2-gram language model from the
large English ukWAC Web corpus (Baroni et al. 2009) using the SRILM toolkit (Stolcke
2002) with default Good–Turing smoothing. We retained only verbs, nouns, adjectives,
and adverbs in order to maximize the proximity between verbs and their subjects and
objects. We defined the preference score for verb–subject triples as the probability of the
sequence av, that is, Pref(v, subj, a) = P(vIa). Conversely, the preference score for verb–
object triples was defined as the probability of the sequence va, that is, Pref(v,obj,a) =
P(a|v). Again, the model compares Pref (v, r, a) and Pref (v, r, a&apos;) to make its decision.
Evaluation. For all models, we report two evaluation figures. One is coverage: A tuple
is covered if the model assigns some preference to both a and a&apos;, and the preferences are
not equal. The second is error rate, which is the relative frequency, among all covered
tuples, of instances where the confounder was at least equally preferred. Both coverage
and error rate are averages over the 2 x 2 cross-validation runs in each setting.
We determine the statistical significance of differences between error rates using
bootstrap resampling (Efron and Tibshirani 1994). This procedure samples correspond-
ing model predictions with replacement from the set of predictions made by the models
to be compared and computes the difference in error rates. On the basis of n such
samples (n = 1,000), the empirical 95% confidence interval for the difference in strength
on the basis of all observed differences is computed. If the interval includes 0, the
difference is not statistically significant.
</bodyText>
<subsectionHeader confidence="0.983299">
5.2 SYN PRIMARY Setting: Results
</subsectionHeader>
<bodyText confidence="0.999937533333333">
Table 4 shows the results for the SYN PRIMARY setting. The overall best error rate is
achieved by a variant of the EPP model, with the RESNIK model coming in second
(the performance difference is significant at the 0.05 level). The EPP variants also show
near-perfect coverage, whereas the RESNIK model delivers results only for 63% of the
data points. We found a very high error rate and a comparatively low coverage for
ROOTH ET AL., which most likely stems from the data pruning necessary to reduce the
training data (compare the subsequent results in the SEM PRIMARY setting). The PADO
ET AL. model was not tested in the SEM PRIMARY setting, because it requires semantic
role annotation. The HW baseline is somewhat below chance (50%), which is an effect of
our by-token sampling procedure, according to which confounders often have higher
corpus frequencies than the real arguments. The TRIPLE baseline has a better error rate
than the LM baseline, but has very low coverage. Both the RESNIK and the EPP models
outperform the baselines in terms of error rate. That they outperform the TRIPLE
baseline in terms of error rate indicates that we sometimes have confounders that have
actually been seen more often with the verb–argument pair than the headword, but that
</bodyText>
<page confidence="0.989155">
739
</page>
<table confidence="0.568967">
Computational Linguistics Volume 36, Number 4
</table>
<tableCaption confidence="0.996803">
Table 4
</tableCaption>
<table confidence="0.998982454545454">
SYN PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
Cosine 32.8 30.3 31.2 98.5
Dice 49.4 48.2 47.5 97.1
nGCM 27.6 27.5 25.7 98.5
EPP:DEPSPACE Hindle 53.7 52.3 52.8 96.6
Jaccard 49.5 48.2 47.6 97.1
Lin 35.5 34.3 33.2 98.8
Cosine 30.2 28.7 28.8 98.1
Dice 29.9 30.8 28.6 98.2
nGCM 26.4 26.4 25.6 98.1
EPP:DEPSPACE, PCA Hindle 45.0 44.4 44.2 95.7
Jaccard 29.7 30.7 28.5 98.2
Lin 28.7 29.1 26.7 97.7
Cosine 35.3 35.8 34.0 97.4
Dice 51.0 50.7 50.3 96.0
nGCM 33.2 34.7 31.8 97.4
EPP:WORDSPACE Hindle 52.7 52.8 52.4 96.0
Jaccard 51.8 52.0 51.3 96.0
Lin 32.0 31.8 31.4 98.2
Cosine 30.3 31.3 29.4 97.1
Dice 31.3 32.4 30.5 97.8
nGCM 30.0 30.9 29.0 97.1
EPP:WORDSPACE, PCA Hindle 40.2 41.0 40.4 95.3
Jaccard 31.0 32.1 30.2 97.8
Lin 27.8 29.8 26.9 97.3
RESNIK 28.1 63.4
ROOTH ET AL. 58.1 61.5
PADO ET AL. – –
HW 60.0 100.0
TRIPLE 32.0 4.0
LM 37.0 86.0
</table>
<bodyText confidence="0.996118636363636">
are dissimilar from other seen headwords, which allows RESNIK and EPP to identify
them as confounders in spite of their higher co-occurrence frequency.
We now turn to a comparison of the EPP variants. The coverage of all EPP models is
very high (0.95 or higher), independent of space, similarity measure, and dimensionality
reduction. We generally observe that error rates are lower when word meaning is
represented in DEPSPACE, and when discrimination weighting is used. In DEPSPACE,
nGCM works best, yielding the overall best result with an error rate of 25.6–25.7%.
In WORDSPACE, the Lin measure shows the best error rates with an error rate of just
below 27%. These results hold both for the unreduced and the reduced spaces and are
highly significant (p ≤ 0.01). Hindle is clearly the worst measure at around random
performance.
</bodyText>
<page confidence="0.939216">
740
</page>
<note confidence="0.612076">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.999801833333333">
The difference between UNI and DISCR is significant throughout; the difference
between FREQ and DISCR is less uniform. In DEPSPACE, the difference between the best
measure with and without PCA (nGCM in both cases) is not significant; in WORDSPACE,
the difference between the best measure with and without PCA (Lin in both cases) is
significant (p &lt; 0.01).
For both WORDSPACEs and DEPSPACEs without PCA, the similarity measures divide
into two distinct groups: Lin, nGCM, and Cosine on the one hand and Jaccard, Dice, and
Hindle on the other, with a significant difference in performance between the groups
(p &lt; 0.01). The use of dimensionality reduction through PCA improves performance
for all similarity measures, in WORDSPACE as well as DEPSPACE. The improvement is
especially marked for the Dice and Jaccard measures, which perform at the level of
a random baseline for unreduced spaces. We assume that these set intersection-based
measures benefit from the independent dimensions that PCA produces. For the simi-
larity measures with best performance, the improvement through PCA is less marked.
Thus, PCA-reduced spaces show more similar error rates across similarity measures.
After PCA, only nGCM and Lin still significantly (p &lt; 0.01) outperform the others
in DEPSPACE, and in WORDSPACE, Lin is the only measure that performs significantly
differently from the rest (p &lt; 0.01).
As arguments are sampled from six frequency bins, we can inspect the effect of
argument frequency on error rate. Figure 4 examines the performance of the EPP model
with different similarity measures and weighting schemes by argument frequency bins
(cf. the subsection Task and Data in Section 5). We find that the overall best weighting
scheme, DISCR, also works best for all except the highest argument frequency bin. In
the DEPSPACE setting (upper row), all similarity measures show a frequency bias in that
</bodyText>
<figureCaption confidence="0.677886">
Figure 4
</figureCaption>
<equation confidence="0.314022">
SYN PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1–50; 2 = 50–100;
3 = 100–200; 4 = 200–500; 5 = 500–1,000; 6 &gt; 1,000.
</equation>
<page confidence="0.843129">
741
</page>
<figure confidence="0.830897">
Computational Linguistics Volume 36, Number 4
</figure>
<figureCaption confidence="0.968636">
Figure 5
</figureCaption>
<equation confidence="0.679388">
SYN PRIMARY setting: Error rate by predicate frequency bin: DISCR weighting. Bins: 1 = 50–100;
2 = 100–200; 3 = 200–500; 4 = 500–1,000; 5 &gt; 1,000.
</equation>
<bodyText confidence="0.9994786875">
error rate is lower for more frequent arguments, but this bias is much less pronounced
in Cosine and nGCM than in the other measures, with error rates varying between 45%
and 25% rather than 80% and 20%. (Dice and Hindle, not shown here, exhibit similar
behavior to Jaccard.) In PCA-transformed DEPSPACE (middle row), this frequency bias
largely disappears for all similarity measures. In WORDSPACE (bottom row), although
there is again a frequency bias in all similarity measures, Lin now joins Cosine and
nGCM in being much less biased than Jaccard, Dice, and Hindle. For WORDSPACE
with PCA-transformation, not shown here, the curves resemble those of DEPSPACE with
PCA-transformation.
Figure 5 examines the effect of (predicate, argument position) pair frequency
on error rate. Predicate–argument position pairs were sampled from five frequency
bins. The figure shows DISCR weighting only. In the spaces without dimensionality
reduction, there is a clear division between Cosine, nGCM, and Lin on the one hand,
and Jaccard, Dice, and Hindle on the other. In PCA spaces, all measures except for
Hindle are similar in their performance. In both DEPSPACE conditions, error rate
decreases towards the higher frequency predicate bins, although this is not so in
</bodyText>
<page confidence="0.962913">
742
</page>
<note confidence="0.632257">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.9992876">
WORDSPACE. It seems that in the sparser DEPSPACE, models can still profit from the
additional seen headwords in the highest predicate frequency bins, whereas in the less
sparse but noisier WORDSPACE, the added noise is stronger than the added signal in
the highest predicate frequency bins. For the lowest predicate frequency bins, the best
results in WORDSPACE are better than those in DEPSPACE.
</bodyText>
<subsectionHeader confidence="0.996511">
5.3 SEM PRIMARY Setting: Results
</subsectionHeader>
<bodyText confidence="0.992649">
Table 5 shows the results for the SEM PRIMARY setting, where we predict head words for
pairs of a frame (predicate sense) and semantic role. In comparison to the SYN PRIMARY
setting (Table 4), error rates are lower across the board. The difference for the EPP models
is on average around 10%.
</bodyText>
<tableCaption confidence="0.997122">
Table 5
</tableCaption>
<table confidence="0.9632006">
SEM PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
Cosine 19.8 16.9 19.0 97.1
Dice 42.3 32.4 39.5 96.3
nGCM 20.2 16.3 20.8 97.1
EPP:DEPSPACE Hindle 48.3 46.8 47.8 93.4
Jaccard 41.5 31.5 38.5 96.3
Lin 31.1 20.2 29.0 97.7
Cosine 18.5 17.0 17.8 96.9
Dice 19.3 19.2 18.0 97.6
nGCM 16.9 15.7 16.4 96.9
EPP:DEPSPACE, PCA Hindle 44.9 45.6 44.7 89.3
Jaccard 18.2 18.5 17.5 97.6
Lin 18.8 19.5 18.3 98.9
Cosine 24.1 20.4 23.4 93.1
Dice 23.7 24.5 22.5 89.6
nGCM 21.1 17.8 19.4 93.1
EPP:WORDSPACE Hindle 31.8 33.1 31.8 83.1
Jaccard 24.8 26.5 24.2 89.6
Lin 22.3 18.4 21.9 92.8
Cosine 21.0 17.6 20.5 93.1
Dice 18.5 16.4 17.8 96.8
nGCM 19.7 16.4 19.3 93.1
EPP:WORDSPACE, PCA Hindle 41.0 39.8 40.7 90.6
Jaccard 18.1 16.2 17.6 96.8
Lin 21.3 17.1 20.7 98.3
RESNIK 16.5 62.8
ROOTH ET AL. 24.9 100.0
PADO ET AL. 7.1 59.0
HW 65.0 100.0
TRIPLE 44.0 2.0
LM NA NA
743
Computational Linguistics Volume 36, Number 4
</table>
<bodyText confidence="0.995699152173913">
The error rate of the PADO ET AL. model, at 7%, is the best by a large margin. We
attribute this to the extensive generalization mechanisms that the model uses, which
draw on an array of lexical–semantic resources. However, with a coverage of 59%,
the model is still unable to make predictions for many of the test items. Error rates
for the RESNIK and the EPP models are comparable, at 16.5% for RESNIK and 15.7% for
the best EPP variant. The two models differ sharply in coverage, however: 62.8% for
RESNIK, consistent with the findings of Gildea and Jurafsky (2002), and between 90%
and 98% for EPP variants. The RESNIK model also profits from the presence of semantic
disambiguation in the SEM PRIMARY setting (in the SYN PRIMARY setting its error
rate was 28%), which underlines the substantial impact that properties of the training
data have on semantic hierarchy–based models of selectional preferences. ROOTH
ET AL. now has perfect coverage, affirming our assumption that the very bad results
of the ROOTH ET AL. model in the SYN PRIMARY setting were an artifact of the data
sampling necessary for that data set. Although its error rate of 24.9% is a substantial
improvement over all baselines, the EPP model achieves error rates that are up to
9 points lower at a comparable coverage. Among the baselines, HW shows that here, as
in the SYN PRIMARY setting, arguments have some tendency of having lower frequency
than the confounders. The TRIPLE baseline shows near-random performance, at very
low coverage, a result of the very small size of the corpus. Because there is no large
corpus with frame-semantic roles, nor is the annotation easily linearizable, we could
not compute a LM baseline in the SEM PRIMARY setting.
Among EPP models, the DEPSPACEs and WORDSPACEs perform comparably, with a
non-significant advantage for DEPSPACE among the best models. Overall error rates
show the same clear divide between the three high-performing similarity measures
(Cosine, nGCM, and Lin) and the three weaker ones (Dice, Jaccard, and Hindle). Di-
mensionality reduction again dramatically improves the weaker models, with Jaccard
yielding the best result for the PCA-reduced WORDSPACE.9 Whereas all best parame-
trizations in the SYN PRIMARY setting used DISCR weighting, it is now FREQ weighting
that yields the best results.
Figure 6 again analyzes the influence of argument frequency on performance by
showing the performance of different variants of the EPP model over six argument
frequency bins. The upper row shows DEPSPACE without dimensionality reduction.
Note that FREQ weighting now works especially well for the lowest argument frequency
bin, much better than DISCR and PLAIN. This is the opposite of what we saw for the
SYN PRIMARY setting in Figure 4. With DISCR and PLAIN weighting, Jaccard and Lin
again have noticeable problems with the lowest argument frequency bins—as in the SYN
PRIMARY setting—but not with FREQ weighting. With DEPSPACE and dimensionality
reduction (middle row), we get error rates of ≤ 26% for all settings and all frequency
bins. On the lowest frequency bin, we again see a large advantage of FREQ weighting
over the two other weighting schemes. The bottom row shows WORDSPACE without
dimensionality reduction. Note that there is much less variation in error rates across
frequency bins here than in unreduced DEPSPACE.
Figure 7 charts error rate by predicate frequency bin, showing FREQ weighting
only, as this showed the best results on this data set. The figure clearly illustrates the
divide between the top and the bottom three similarity measures in DEPSPACE, as well
as the disappearance of this divide for both PCA settings. In unreduced WORDSPACE,
</bodyText>
<footnote confidence="0.557761">
9 The differences to other similarity metrics in the FREQ setting are insignificant, with the exception
of Hindle.
</footnote>
<page confidence="0.981925">
744
</page>
<note confidence="0.641469">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.970236666666667">
the divide is not as clearly visible. The figure also indicates a slight tendency for error
rates to rise for the lowest-frequency as well as the highest-frequency predicates, across
all spaces.
</bodyText>
<subsectionHeader confidence="0.805483">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99991894117647">
The resource-based approaches that we tested, RESNIK and PADO ET AL., show superior
performance when they have coverage (which coincides with findings in other lexical
semantics tasks that supervised data, when available, always increases performance),
but showed low coverage, at most 63% (RESNIK, SYN PRIMARY setting). The EPP model
achieves near-perfect coverage at good error rates: In the SYN PRIMARY setting, the
RESNIK model achieved an error rate of 28%, and the best EPP variant was at 26%. In
the SEM PRIMARY setting, error rates were 7% for the PADO ET AL. model, 16.5% for
the RESNIK model, and 16% for the best EPP variant. Comparing the EPP and ROOTH
ET AL. models in the SEM PRIMARY setting, we find that the use of an additional gen-
eralization corpus in the EPP model seems to offset any advantages introduced by the
joint clustering of predicates and arguments.
The difference in model performance on the two primary corpora (SYN and SEM)
is striking. Even though the FrameNet corpus is smaller and a sparse data prob-
lem might be expected, models perform at considerably lower error rates in the SEM
PRIMARY setting than when the primary corpus is the larger BNC. This underscores
the point that selectional preferences belong to a predicate sense rather than a predicate
lemma, and that they describe the semantics of fillers of semantic roles rather than of
</bodyText>
<footnote confidence="0.580252666666667">
Figure 6
SEM PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1–50; 2 = 50–100;
3 = 100–200; 4 = 200–500; 5 = 500–1,000; 6 &gt; 1,000.
</footnote>
<page confidence="0.944528">
745
</page>
<figure confidence="0.825276">
Computational Linguistics Volume 36, Number 4
</figure>
<figureCaption confidence="0.999371">
Figure 7
</figureCaption>
<equation confidence="0.5791425">
SEM PRIMARY setting: Error rate by predicate frequency bin: FREQ weighting. Bins: 1 = 50–100;
2 = 100–200; 3 = 200–500; 4 = 500–1,000; 5 &gt; 1,000.
</equation>
<bodyText confidence="0.9999779">
syntactic dependents (recall that in this setting, we predict head words for pairs of a
predicate sense and semantic role). In the SEM PRIMARY setting, the data is cleaner, so
it is expected that seen headwords of an argument position will be more semantically
uniform. This has a strong influence on model performance. Another factor contributing
to the difference in performance between the two data sets may be that the primary
corpus in the SYN PRIMARY setting is parsed automatically, whereas manual annotation
is available in the FrameNet corpus. However, although this manual annotation iden-
tifies predicate senses, role headwords are still determined through automatic parsing.
The division of the training data into a primary and a secondary corpus allows us to
successfully use FrameNet as the basis for semantic space–based similarity estimates
despite the fact that this corpus alone would be too small to sustain the construction of a
robust space.
In terms of model parameters for EPP, the following patterns stand out. Cosine,
Lin, and nGCM show good performance across all spaces and parameter settings; Dice
and Jaccard work comparably only on spaces that use dimensionality reduction. The
Hindle measure is an underperformer in all conditions. With Lin, Jaccard, Dice, and
Hindle, error rates rise sharply for less frequent arguments in many spaces. Although
Cosine and nGCM also have some frequency bias, it is much less pronounced. nGCM
seems to work well with sparse data sets that are not too noisy, as evidenced by the
fact that it has the best performance among all EPP variants on both DEPSPACEs in
</bodyText>
<page confidence="0.997456">
746
</page>
<note confidence="0.843799">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<tableCaption confidence="0.98493">
Table 6
</tableCaption>
<bodyText confidence="0.958627076923077">
Verb–argument position–noun triples with plausibility judgments on a 7-point scale (McRae
et al., 1998).
Verb Argument position Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
the SYN PRIMARY setting, as well as in all SEM conditions except reduced WORDSPACE.
The Lin measure seems to work well with noisier data: It is the best EPP model when
using WORDSPACE in the SYN PRIMARY setting. Cosine, although never showing the
top performance, is among the best models in any setting. Although dimensionality
reduction only improves the overall error rates of the best models by a few points, it
has two important consequences: First, dimensionality reduction reduces dependence
of the results on the exact similarity measure chosen, as all measures except Hindle
show nearly indistinguishable error rates on reduced spaces (Figures 5 and 7). Second,
low-frequency arguments profit by a huge margin when PCA is used (Figures 4 and 6).
Among weighting schemes, DISCR weighting seems to be most useful when the data
is sparse but somewhat noisy (as is the case in the lower argument frequency bins in
the SYN PRIMARY setting). Frequency weighting seems to work best when the data is
either not sparse (as in the highest argument frequency bin in the SYN PRIMARY setting)
or very clean but sparse (as in the lowest argument frequency bin in the SEM PRIMARY
setting). A comparison of the two vector spaces, DEPSPACE and WORDSPACE, shows
no clear winner. When the collections of seen headwords are noisier, as they are in the
SYN PRIMARY setting, DEPSPACE, with its more aggressive filtering, yields the better
results. Sets of headwords collected by predicate sense, as in the SEM PRIMARY setting,
are sparser but cleaner, and WORDSPACE shows lower error rates.
</bodyText>
<sectionHeader confidence="0.967082" genericHeader="method">
6. Experiment 2: Human Plausibility Judgments
</sectionHeader>
<bodyText confidence="0.999921222222222">
Experimental psycholinguistics affords a second perspective on selectional preferences:
The plausibility of verb–argument pairs has been shown to have an important effect
on human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnsey
et al. 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998). In these studies, plausibility
was operationalized as the thematic fit or selectional preference between a verb and its
argument in a specific argument position. Models of human sentence processing there-
fore need selectional preference models (Padó, Crocker, and Keller 2009). Conversely,
psycholinguistic plausibility judgments can be used to evaluate computational models
of selectional preferences.
</bodyText>
<subsectionHeader confidence="0.965814">
6.1 Experimental Materials
</subsectionHeader>
<bodyText confidence="0.997333">
We present evaluations on two plausibility judgment data sets used in recent studies.
</bodyText>
<page confidence="0.985019">
747
</page>
<note confidence="0.484689">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.999903956521739">
The first data set consists of 100 data points10 from McRae, Spivey-Knowlton, and
Tanenhaus (1998). Our example in Table 6, which is taken from this data set, was
elicited by asking study participants to rate the plausibility of, for example, a hunter
shooting (AGENT) or being shot (PATIENT). The data point demonstrates the McRae set’s
balanced structure: 25 verbs are paired with two argument headwords in two argument
positions each, such that each argument is highly plausible in one argument position
but implausible in the other (hunters shoot, but are seldom shot, and vice versa for deer).
The resulting distribution of ratings is thus highly bimodal. Models can only reliably
predict the human ratings in this data set if they can capture the difference between
verb argument positions as well as between individual fillers. However, because the
verb–argument pairs were created by hand and with strict requirements, many of the
arguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight). When
FrameNet is used to annotate senses for the verbs, no appropriate senses are available
for 28 of the 100 verb–argument pairs, reducing the test set to 72 data points.
The second, larger data set addresses this sparseness issue. Its triples are con-
structed on the basis of corpus co-occurrences (Padó 2007). Eighteen verbs are combined
with their three most frequent subjects and objects found in the Penn Treebank and
FrameNet corpora, respectively, up to a total of 12 arguments. Each verb–argument pair
was rated both as an agent and as a patient (i.e., both in the observed and an unobserved
argument position), which leads to a total of 24 rated triples per verb. The data set
contains ratings for 414 triples. The resulting judgments show a more even distribution
of data. With FrameNet annotation for the verbs, appropriate senses are not attested for
six verb–argument pairs, reducing the test set to 408 data points.
</bodyText>
<subsectionHeader confidence="0.996196">
6.2 Setup
</subsectionHeader>
<bodyText confidence="0.999963533333333">
We evaluate the same four models as in Experiment 1: EPP, the WordNet-based RESNIK
model, the distributional ROOTH ET AL. model, and the semantic role–based PADO
ET AL. model. We again compare a SYN PRIMARY setting, where the models make pre-
dictions for pairs of a verb and a grammatical function, with a SEM PRIMARY setting,
for which the two test data sets were annotated with verb sense and semantic roles in
the FrameNet paradigm (Padó 2007) and where models make predictions for pairs of a
frame and a semantic role. As before, the PADO ET AL. model is only tested in the SEM
PRIMARY setting.11 For the EPP model, we focus on parsed, dimensionally unreduced
spaces and DISCR weighting, following earlier results (Padó, Padó, and Erk 2007). We
provide results for the best WORDSPACE models from Experiment 1 for comparison.
The primary corpora for training selectional preference models were prepared as in
Experiment 1 (cf. Section 5.1). The generalization corpus for EPP was again the BNC.
For the ROOTH ET AL. model in the SYN PRIMARY setting, we again used a frequency
cutoff. We found the RESNIK model to perform better when using just a subset of the
BNC (namely, all the triples for verbs present in the test set).
</bodyText>
<subsectionHeader confidence="0.99239">
6.3 Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.999948">
We evaluate our models by correlating the predicted plausibility values with the human
judgments, which range between 1 and 7. Because we do not assume a priori that there
</bodyText>
<page confidence="0.73462825">
10 The original data set has 60 data points more, which were used as the development set for the PADO
ET AL. model.
11 The PADO ET AL. model now uses automatically induced verb clusters instead of FrameNet frames.
748
</page>
<note confidence="0.843699">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<tableCaption confidence="0.997747">
Table 7
</tableCaption>
<table confidence="0.984883">
Comparison of EPP DEPSPACE models on McRae data. Unreduced spaces, DISCR weighting.
***p &lt; 0.001.
SEM SYN
Sim Coverage Spearman’s p Coverage Spearman’s p
Dice 100% 0.038 ns 98% 0.148 ns
Jaccard 100% 0.045 ns 98% 0.153 ns
Cosine 100% 0.162 ns 98% 0.197 ns
Hindle 100% 0.060 ns 98% 0.108 ns
Lin 100% 0.085 ns 98% 0.094 ns
nGCM 100% 0.154 ns 98% 0.325 ***
</table>
<bodyText confidence="0.996787555555556">
is a linear correlation between the two variables, we do not use Pearson’s product-
moment correlation, but instead Spearman’s p, a non-parametric rank-order correlation
coefficient.12 Note that significance is harder to reach the smaller the number of data
points is.
In line with Experiment 1, we include a simple frequency baseline FREQ, which
predicts the plausibility of each item as its frequency in the BNC (SYN) and in FrameNet
(SEM), respectively. With regard to an upper bound, we assume that automatic models
of plausibility should not be expected to surpass the typical human agreement on the
plausibility judgment. This is roughly p ≈ 0.7 for the Pado data set.
</bodyText>
<subsectionHeader confidence="0.992072">
6.4 McRae Data Set: Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999182823529412">
Table 7 focuses on EPP variants with unreduced DEPSPACE for the McRae data set. We
see that this data set is rather difficult to model. None of the models trained in the SEM
PRIMARY setting achieves a significant correlation.13 Apparently, the FrameNet corpus
is too small to acquire selectional preferences that generalize well to the infrequent
items that make up the McRae data set. In the SYN PRIMARY setting, the nGCM model’s
predictions reach significance.
Table 8 shows results on the McRae data set for all selectional preference models
that we are considering. For EPP, we only show nGCM as the best-performing similarity
measure from the pseudo-disambiguation task, and Cosine as a widely used vanilla
measure. The results for the SEM PRIMARY setting (left-hand side) mirror the results
for the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL. model shows the
best correlation (it is the only model to predict human judgments significantly). It
overcomes the sparseness in the FrameNet corpus by using semantic verb classes that
are particularly geared towards grouping the existing verb occurrences in the way
that is most meaningful for this task. It covers about 80% of the test data. EPP has
full coverage, and although it does not make statistically significant predictions, it
shows substantially higher correlation coefficients than ROOTH ET AL. and RESNIK.
</bodyText>
<footnote confidence="0.746877333333333">
12 A second concern is the computation of significance values: The methods most widely used for the
Pearson coefficient (Student’s t-distribution, Fisher transformation) assume that the variables are
normally distributed, which is not the case in our data set. For Spearman’s p, we use the algorithm by
Best and Roberts (1975), which does not make this assumption.
13 Significance here refers to significance of correlation with the human data, not significance of differences
between models.
</footnote>
<page confidence="0.994404">
749
</page>
<table confidence="0.414027">
Computational Linguistics Volume 36, Number 4
</table>
<tableCaption confidence="0.995503">
Table 8
</tableCaption>
<table confidence="0.966716181818182">
Comparison across models on McRae data. **p &lt; 0.01, ***p &lt; 0.001.
SEM SYN
Model Coverage Spearman’s p Coverage Spearman’s p
EPP (DEPSPACE nGCM) 100% 0.154 ns 98% 0.325 ***
EPP (DEPSPACE Cosine) 100% 0.162 ns 98% 0.197 ns
RESNIK 100% –0.041 ns 100% 0.123 ns
ROOTH ET AL. 67% 0.078 ns 48% 0.465 ***
PADO ET AL. 78% 0.415 ** – –
EPP (WORDSPACE Lin) 100% 0.138 ns 98% 0.062 ns
EPP (WORDSPACE nGCM) 100% 0.167 ns 98% 0.110 ns
FREQ 18% 0.087 ns 36% 0.103 ns
</table>
<bodyText confidence="0.9994953">
The DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simple
frequency baseline has very low coverage and correlation.
As the right-hand side of Table 8 shows, both ROOTH ET AL. and EPP achieve
better results in the SYN PRIMARY setting than in the SEM PRIMARY setting. The ROOTH
ET AL. model obtains a highly significant correlation. The combination of infrequent
headwords in the McRae data set and the large primary corpus brings out the benefits
that the ROOTH ET AL. model can derive from generalizing from verbs and nouns to
the latent classes via soft clustering. Unfortunately, its coverage is still quite low (48%),
and for this reason, the difference from the best EPP model is not significant.14 In the
SYN PRIMARY setting, the EPP DEPSPACE models clearly outperform the WORDSPACE
because of the DEPSPACE models’ more aggressive filtering. Interestingly, RESNIK
still performs poorly in the SYN PRIMARY setting: WordNet does not make the right
generalizations to capture the selectional preferences at play in the McRae data, no
matter how much training data is available. This is underscored by an analysis of which
WordNet classes were most frequently determined as the strongest association with
the target verbs: The classes entity, person, and physical object are assigned in 60 out of
100 test cases for the McRae data (SYN PRIMARY setting), a data set where plausibility
is determined by factors much more fine-grained than animacy. (In the SEM PRIMARY
setting, the picture is similar with classes person, organism, and entity assigned in 48 out
of 72 test cases.) The frequency baseline again performs badly.
</bodyText>
<subsectionHeader confidence="0.991543">
6.5 Pado Data Set: Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999949166666667">
We now turn to the Pado data set. Again, we first focus on the performance of differ-
ent similarity measures in EPP using unreduced DEPSPACE (Table 9). Correlation with
human judgments is much better than for the McRae data set, and highly significant
for all SEM PRIMARY setting models and three of the SYN PRIMARY setting models. In
both settings, Cosine and Lin are the best measures (difference not significant), followed
by nGCM. Hindle comes out worst once more. The difference between the strong and
</bodyText>
<footnote confidence="0.947283666666667">
14 As in Experiment 1, we apply bootstrap resampling to determine the significance of differences between
models. This procedure also takes differences in coverage into account—specifically, a significant
difference becomes harder to achieve as the number of data points shared between the models shrinks.
</footnote>
<page confidence="0.994039">
750
</page>
<note confidence="0.844192">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<tableCaption confidence="0.998209">
Table 9
</tableCaption>
<table confidence="0.9876719">
Comparison of EPP DEPSPACE parametrizations on Padó data. Unreduced spaces, DISCR
weighting. **p &lt; 0.01; ***p &lt; 0.001.
SEM SYN
Sim Coverage Spearman’s p Coverage Spearman’s p
Dice 100% 0.289 *** 100% 0.026 ns
Jaccard 100% 0.285 *** 100% 0.023 ns
Cosine 100% 0.508 *** 100% 0.403 ***
Hindle 100% 0.160 ** 100% –0.004 ns
Lin 100% 0.498 *** 100% 0.229 ***
nGCM 100% 0.384 *** 100% 0.156 **
</table>
<bodyText confidence="0.995669944444445">
weak measures is more pronounced for the SYN PRIMARY setting, compared with the
SEM PRIMARY setting. Coverage is at or close to 100% throughout.
Table 10 shows results on the Pado data set for all selectional preference models that
we consider. In the SEM PRIMARY setting (where both the data and the primary corpus
have FrameNet annotation), EPP and the deep PADO ET AL. model predict the human
judgments similarly well (difference not significant). Because all verbs in this data set
are covered by FrameNet, the PADO ET AL. model also shows a nearly perfect cover-
age. EPP and PADO ET AL. do much better than ROOTH ET AL. (differences significant
at p ≤ 0.01). ROOTH ET AL. has the lowest coverage at 88%, but this is still higher than
its coverage of the McRae data. As with the McRae data, ROOTH ET AL. achieves better
correlation in the SYN PRIMARY setting than the SEM PRIMARY setting, indicating that
the frequency cutoff does not harm performance as much in Experiment 2 as it did in
Experiment 1. However, the coverage of ROOTH ET AL. is lower in the SYN PRIMARY
setting, perhaps because the SEM PRIMARY setting smoothes rare verbs by grouping
them in frames with other verbs. RESNIK also achieves better correlation in the SYN
PRIMARY setting, but recall that it was trained on a subset of the BNC only to reduce
noise in the training data—when trained on the whole BNC set, performance degrades
to p = 0.060. The difference from the best EPP model remains numerically large. As for
</bodyText>
<tableCaption confidence="0.992575">
Table 10
</tableCaption>
<table confidence="0.962453454545455">
Comparison across models on Padó data. ***p &lt; 0.001.
SEM SYN
Model Coverage Spearman’s p Coverage Spearman’s p
EPP (DEPSPACE Cosine) 100% 0.489 *** 98% 0.470 ***
EPP (DEPSPACE nGCM) 100% 0.393 *** 98% 0.328 ***
RESNIK 98% 0.230 *** 97% 0.317 ***
ROOTH ET AL. 88% 0.060 ns 58% 0.200 ***
PADO ET AL. 97% 0.515 *** – –
EPP (WORDSPACE Lin) 100% 0.254 *** 100% 0.056 ns
EPP (WORDSPACE nGCM) 100% 0.192 *** 100% 0.078 ns
FREQ 32% –0.041 ns 69% 0.090 ns
</table>
<page confidence="0.889094">
751
</page>
<note confidence="0.420927">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.982424142857143">
the McRae data set, the EPP WORDSPACE models show much worse performance than
the DEPSPACE models, and do not significantly predict the human plausibility ratings.
The frequency baseline shows a considerably better coverage for this data set, but
its correlations hover around zero, which underlines our intuition that verb–argument
combinations can be plausible without being frequent in corpora. An example is the
combination (to) embarrass (an) official, which is rated as highly plausible, but occurs
only once each in the BNC and FrameNet.
</bodyText>
<subsectionHeader confidence="0.892273">
6.6 Discussion
</subsectionHeader>
<bodyText confidence="0.999706897435897">
The McRae data set seems in general more difficult to account for than the Pado data
set, as noted by Padó, Padó, and Erk (2007). They explain it by a general frequency effect
in the BNC data (which are a superset of the FrameNet data): The median frequency of
the hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpus-
derived Pado nouns.
Comparing all selectional preference models, we find that the RESNIK and the
ROOTH ET AL. models generally do worse than EPP both in terms of coverage and
quality of predictions. One notable exception is the excellent performance of the ROOTH
ET AL. model on the McRae data in the SYN PRIMARY setting, which comes, however,
with a low coverage of less than 50%. A closer inspection of the predictions showed
that ROOTH ET AL. makes many predictions for verb–object pairs but abstains from
subjects, thus reducing the complexity of the task. For only 20% of verbs, predictions
are made for subjects and objects. As noted in Padó, Padó, and Erk (2007), the relatively
poor performance of the RESNIK model may be explained by the fact that its ability to
generalize is limited to the structure of WordNet, where some semantic distinctions are
easier to make than others. For example, a fairly easy distinction to make for WordNet-
based models is animate vs. inanimate. Because the Pado set contains a portion of
inanimate arguments with animate counterparts, the RESNIK model does well on those.
In contrast, in the McRae test set, all arguments are animates, and thus similar to one
another in terms of WordNet.
The deep PADO ET AL. model achieves the best correlation with the human judg-
ments on both data sets, but it is limited to the SEM PRIMARY setting. Although the
best model is not always among the EPP DEPSPACE models, they consistently show a
coverage of close to 100%, and are generally statistically indistinguishable from the best
model. Unlike ROOTH ET AL. and RESNIK, whose performance varies widely between
the SEM PRIMARY setting and the SYN PRIMARY setting, the correlation coefficients for
the EPP models are generally similar across settings. We take this as evidence that EPP
models can extract relevant information from deeper annotation on small corpora as
well as from large, but noisy and shallow, training data.
Finally, we consider the different similarity measures for the EPP model evaluated
on unreduced DEPSPACE. The picture differs somewhat between the two data sets, but
the Cosine measure performs well overall, with Lin and nGCM generally in second
and third place. So, the group of the three best similarity measures is the same as in
Experiment 1, but Cosine shows better performance. One possible reason for this lies
in the verb frequency, which is relatively high in both data sets: 68% of the McRae
verbs and 83% of the Pado verbs have BNC frequencies of 1,000 and more, whereas
Experiment 1 used an equal number of predicates from five frequency bins, the highest
being 1,000 and more occurrences. In that highest predicate frequency bin, Cosine
consistently performed as well as Lin or better in Experiment 1 (Figures 5 and 7).
</bodyText>
<page confidence="0.987103">
752
</page>
<note confidence="0.731384">
Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<sectionHeader confidence="0.251097" genericHeader="method">
7. Experiment 3: Inverse Selectional Preferences
</sectionHeader>
<bodyText confidence="0.99996852631579">
The term selectional preference is typically used to describe the semantic constraints
that predicates place on their arguments. In this section, we will investigate how nominal
arguments place semantic constraints or expectations on the predicates with which they
occur. Such expectations can be thought of as typical events that involve the given
object. For example, a noun like apple could be said to have preferences about its inverse
subject position, that is, the verbs that can take it as a plausible subject. Examples might
be verbs like grow or fall; for its inverse object position, apple probably prefers verbs
like eat, cut, or plant. We will use the term inverse selectional preference to refer to
preferences of nouns for their predicates, distinguishing them from regular selectional
preferences.
It is clear that not all verbs will be equally likely to occur with a given noun–
role pair. Still, inverse selectional preferences warrant a closer look: To what extent do
inverse selectional preferences differ from regular ones? And are the tasks of predicting
regular and inverse selectional preferences equally difficult? We start in Section 7.2 with
an exploratory data analysis of inverse selectional preferences, which shows that inverse
selectional preferences show semantically coherent patterns like regular selectional
preferences, but that, in contrast to most verbs, nouns tend to occur with multiple
semantic groups of verbs. In Sections 7.3–7.5, we test the EPP model on a pseudo-
disambiguation task for inverse selectional preferences.
</bodyText>
<subsectionHeader confidence="0.564653">
7.1 Related Work
</subsectionHeader>
<bodyText confidence="0.99997508">
In computational linguistics, some approaches to characterizing selectional preferences
have used the symmetric nature of their models to characterize nouns in terms of the
verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly
compare the two types of preferences. Also, there are approaches using selectional
preference information, in particular for word sense disambiguation and related tasks,
that could be characterized as using regular along with inverse selectional preferences
(Dligach and Palmer 2008; Erk and Pad6 2008; Nastase 2008). By comparing selectional
preference model performance on the tasks of predicting inverse and regular selectional
preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can
be achieved by using inverse preferences in word sense analysis tasks.
At the same time, inverse selectional preferences have been the object of fruitful
research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a
particularly plausible argument for the existence of expectations of nouns for their
predicates in human language processing is head-final word order (as in Japanese or
in German subordinate clauses), where hearers may encounter all objects before the
head. It is likely that these objects are immediately integrated into a preliminary event
structure with an assumed predicate instead of being stored in short-term memory until
the predicate is encountered (Konieczny and Döring 2003; Nakatani and Gibson 2009).
Another strand of work is McRae et al. (2001, 2005), who have studied priming of verbs
from nouns. They found that a noun engenders priming of verbs for which it is a typical
agent, patient, instrument, or location.
In theoretical linguistics, the idea of event knowledge being encoded in the lex-
ical entries of nouns has been formulated in the context of Pustejovsky’s generative
lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide infor-
mation about the typical use of an object (book: read) and its construction (book: write),
</bodyText>
<page confidence="0.995603">
753
</page>
<note confidence="0.592164">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.999839666666667">
respectively. Pustejovsky uses this knowledge to account, for example, for the interpre-
tation of logical metonymy (begin a book). Although qualia roles are instantiated with
individual predicates rather than characterizations of all possible events, construction
and use are arguably two very salient events for an object. Through the data exploration
in Section 7.2, we hope to contribute to a linguistic characterization of inverse selectional
preferences.
</bodyText>
<subsectionHeader confidence="0.999327">
7.2 Empirical Analysis of Inverse Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.999800454545455">
The first question we ask concerns the selectional preference strength of regular and
inverse selectional preferences, using the measure introduced by Resnik (1996) to de-
termine the degree to which verbs select for nouns, and vice versa. As verb–role pairs,
we re-use the same 100 pairs that were used for the pseudo-disambiguation task in
Experiment 1. For the comparison, we randomly sample a total of 100 noun/inverse-
role pairs from the BNC, using the same five frequency bands as for the verbs (50–
100, 100–200, 200–500, 500–1,000, &gt;1,000). The sample contains approximately the same
number of (inverse) subject and object roles.
We adapt the selectional preference strength measure from Equation (1) to our
case: Unlike Resnik, we compute KL divergence not on a distribution across WordNet
synsets, but on a distribution across lemmas.
</bodyText>
<equation confidence="0.999301">
SelStr(w1,r) = D(P(w2|w1,r)||P(w2|r)) (7)
</equation>
<bodyText confidence="0.999960615384615">
For regular selectional preferences, w1 is a verb lemma, w2 a noun lemma, and r a role.
For inverse preferences, w1 is a noun lemma, w2 a verb lemma, and r an inverse role.
SelStr(w1, r) can be interpreted as a measure of the degree to which w1 has selectional
preferences concerning the role r. We induce the probability distributions through
maximum likelihood estimation on the BNC.
We can expect to see the same overall tendency in regular and inverse selec-
tional preference strength. It is not possible that inverse selectional preference strength
would be uniform throughout if regular selectional preference strength varied between
verbs. After all, if we fix the relation r for the time being, P(v|n) and P(n|v) are re-
lated through Bayes’ formula. Instead, the questions we will ask are more specific.
Are regular and inverse preference strengths similar in size? Are regular and inverse
preference strengths similar by frequency band—that is, do frequent nouns behave
similarly to frequent verbs? And what effects do we see of the prior distributions P(n|r)
and P(v|r)?
Table 11 shows the range of selectional preference strengths found in each frequency
band for verbs and nouns. As expected, we see substantial strengths in both regular
and inverse preferences. Both parts of speech show the same pattern of decreasing KL
divergences for higher-frequency words, presumably because frequent words tend to be
polysemous, and can combine with many different words. However, the strengths for
inverse selectional preferences are in general lower than those for regular preferences.
One possible reason for this is that the number of nouns seen with a verb–role
pair might differ, in general, from the number of verbs seen with each noun–role
pair. However, we find that verbs and nouns occur with roughly the same number of
associates in the frequency bands up to the 200–500 band. In the band 500–1,000, verbs
appear with roughly one third more nouns than nouns appear with verbs, and in the
band of 1,000 occurrences or more, verbs appear with twice as many nouns (on average)
</bodyText>
<page confidence="0.997392">
754
</page>
<note confidence="0.88611">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<tableCaption confidence="0.720347">
Table 11
Minimal, median, and maximal selectional preference strength (measured in terms of KL
divergence) in a sample of 100 verbs and 100 nouns (20 lemmas each per frequency band).
Band Verbs Nouns
min median max min median max
</tableCaption>
<table confidence="0.9995654">
50–100 4.5 7.4 8.8 3.7 4.8 6.3
100–200 3.9 5.8 7.6 2.7 3.8 5.0
200–500 3.3 5.2 6.9 2.4 3.3 4.7
500–1,000 2.4 4.4 5.9 1.9 2.9 4.1
1,000– 1.8 3.6 6.2 1.4 2.3 3.7
</table>
<bodyText confidence="0.994078555555556">
as nouns appear with verbs in this band (1,189 vs. 636). Incidentally, the fact that the
highest-frequency verbs (which also tend to be the most ambiguous) appear in a much
larger number of contexts than the highest-frequency nouns could be a contributing
factor to the well-known problem that verbs are harder to disambiguate than nouns.
For the lower frequency bands, number of associates is unlikely to be the reason for
the weaker inverse preferences. Instead, a more likely reason for the overall weaker
inverse preferences lies in the overall distributions of nouns and verbs in the BNC.
Both show a Zipfian distribution, but there are 15,570 verbs as opposed to 455,173
nouns. Recall that KL divergence will be high whenever the individual terms p(w2|w1,r)
</bodyText>
<equation confidence="0.6199085">
p(w2|r)
to be summed are large. This, in turn, is the case when p(w2|r) is small. And p(w2|r) may
</equation>
<bodyText confidence="0.999596375">
be small when the distribution p(w2|r) ranges over a larger number of words w2. For
regular selectional preferences, the w2 are nouns, and for inverse preferences the w2 are
verbs. Because there are many more nouns than verbs, the denominator p(w2|r) tends to
be smaller for regular preferences.
To get a clearer understanding of how inverse selectional preferences compare to
regular selectional preferences, we next do a qualitative analysis, looking at association
strength SelAssoc for individual triples verb–role–noun and noun–inverse-role–verb.
We adapt Equation (2) to the lexicon-free case and obtain
</bodyText>
<equation confidence="0.970021">
1 SelAssoc(w1, r, w2) = SelSt r(w1, r) P(w2|w1, r) log P P(zv2|r)r) (8)
</equation>
<bodyText confidence="0.999683933333333">
Table 12 shows the five strongest associates for one verb–role pair and one noun–role
pair from each frequency band. The associates on both sides of the table generally
are semantically coherent and make intuitive sense. However, there is an interesting
difference between the verbs and nouns: We find that the nouns’ preferred verbs can
often be grouped loosely into several meaning clusters, whereas the verbs’ associates
tend to group into one cluster per grammatical function. For example, predicates taking
wheat as objects fall into those describing production (grow, sow) and those describing
processing (shred, grind, mill). Similarly, the predicates found for pill either concern
ingestion (take, swallow, pop), prescription, or idiomatic usage. In contrast, the objects of
rebut describe different kinds of statements, and the objects of celebrate are anniversaries
and other special events. Another observation that we can make in Table 12 is that
the nouns’ most preferred associates have a similarly large share in the nouns’ overall
selectional preference strength as the verbs’ most preferred associates have in the verbs’
selectional preference strength. This indicates that the distribution of selectional prefer-
ences is similarly skewed towards the most preferred associate for verbs and nouns.
</bodyText>
<page confidence="0.996311">
755
</page>
<note confidence="0.454494">
Computational Linguistics Volume 36, Number 4
</note>
<tableCaption confidence="0.83256925">
Table 12
Examples of regular and inverse selectional preferences from different frequency bands for
argument positions of nouns and verbs: overall selectional preference strength SelStr and most
highly associated fillers with association strengths SelAssoc.
</tableCaption>
<table confidence="0.95974975">
Band Verbs Nouns
rebut–obj, SelStr(w)= 7.43 wreckage–obj−1, SelStr(w)= 5.91
50–100
presumption 0.283
allegation 0.088
charge 0.082
criticism 0.049
claim 0.041
</table>
<figure confidence="0.847274285714286">
survey 0.126
examine 0.089
sift 0.075
clear 0.056
sight 0.051
100–200
200–500
500–1,000
1,000–
enunciate–obj, SelStr(w)= 6.89 wheat–obj−1, SelStr(w)= 5.00
grow 0.184
shred 0.049
grind 0.049
mill 0.042
sow 0.040
break_with–obj, SelStr(w)= 6.92 pill–obj−1, SelStr(w)= 4.15
take 0.290
swallow 0.165
sweeten 0.070
prescribe 0.049
pop 0.028
</figure>
<table confidence="0.994452045454545">
commence–obj, SelStr(w)= 5.92 dividend–obj−1, SelStr(w)= 4.10
proceedings 0.185 pay 0.508
action 0.051 declare 0.064
work 0.043 receive 0.064
proceeding 0.041 recommend 0.054
operation 0.033 raise 0.023
celebrate–obj, SelStr(w)= 6.23 requirement–obj −1, SelStr(w)= 3.24
anniversary 0.177 meet 0.332
birthday 0.170 satisfy 0.015
centenary 0.046 comply_with 0.093
victory 0.033 fulfill 0.061
mass 0.028 impose 0.028
principle 0.242
word 0.085
theory 0.034
philosophy 0.034
policy 0.029
tradition 0.237
past 0.054
precedent 0.035
convention 0.035
Rome 0.022
</table>
<bodyText confidence="0.999504666666667">
In sum, we find that inverse selectional preferences have weaker overall selectional
preference strength than regular preferences, but that may be due more to specifics of
the formula used rather than the skewness towards preferred role fillers. Two differ-
ences do emerge, though. First, noun selectional preferences show more semantic filler
sets than verb preferences. Second, the highest frequency verbs appear with many more
different associates than the highest frequency nouns.
</bodyText>
<subsectionHeader confidence="0.999411">
7.3 Modeling Inverse Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.99926575">
In the rest of this section, we test selectional preference models on the task of pre-
dicting inverse selectional preferences in a pseudo-disambiguation task, and compare
the results to the performance on predicting regular preferences in Experiment 1. We
do not repeat Experiment 2 even though it would have been technically possible to
</bodyText>
<page confidence="0.991495">
756
</page>
<note confidence="0.716188">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.99991225">
re-use the McRae and Pado data sets and predict plausibility judgments through inverse
preferences. However, the data sets combine each verb with both plausible and im-
plausible nouns, but they do not combine each noun with different verbs in a balanced
fashion, so a repetition of Experiment 2 with inverse preferences would not be very
informative.
For the pseudo-disambiguation experiment, we focus on the EPP model. Distri-
butional models can, in general, be used straightforwardly to model both regular
and inverse selectional preferences. This is different for models like RESNIK that use
the WordNet noun hierarchy to represent regular selectional preferences. To model
inverse preferences, it would be necessary to use the WordNet verb hierarchy. However,
WordNet organizes verbs in a comparatively flat, unconnected hierarchy with a high
branching factor formed by the hypernymy/troponymy (“type of”) relation. This makes
effective generalization difficult, in particular in conjunction with the marked variation
in the set of preferred predicates that we observed for inverse selectional preferences in
Section 7.2.
We adapt the formulation of the EPP model to the inverse selectional preference
case as follows. Let a stand for a noun, r for an inverse argument position of this
noun, and Seenpreds(r,a) for the set of predicates seen with noun a and role r. Then the
selectional preference SelprefEPP of (r, a) for a verb v0 is defined in parallel to Equation (6)
as weighted average similarity to seen verbs:
</bodyText>
<equation confidence="0.9892255">
SelprefEPPr,a(v0) = � wtr,a(v) sim(v, v0) (9)
v∈Seenpreds(r,a) Zr,a
</equation>
<bodyText confidence="0.813582">
with Zr,a = Ev∈Seenpreds(r,a) wtr,a(v) as the normalization constant.
</bodyText>
<subsectionHeader confidence="0.977249">
7.4 Pseudo-Disambiguation: Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999942533333333">
We evaluate inverse selectional preferences on a pseudo-disambiguation task that is
set up completely analogously to our experiments on regular preferences in Section 5:
given a noun, an inverse argument position, one verb observed in this position, and a
confounder verb, distinguish between the two verbs. We use the 100 nouns sampled
across five frequency bands that we already used in Section 7.2. We experiment with
both WORDSPACE and DEPSPACE models, but restrict our attention to DISCR weighting,
which showed good results in Experiment 1.
In Section 5, we experimented on two different primary corpora, the BNC (SYN
PRIMARY setting) and FrameNet (SEM PRIMARY setting). Subsequently, we will use the
SYN PRIMARY setting again, but not the SEM PRIMARY setting. In the SEM PRIMARY
setting, the roles are FrameNet frame elements (semantic roles). However, frame ele-
ments are specific to a single frame, for example, the frame element ROPE belongs to
the frame ROPE_MANIPULATION.15 It would thus be pointless to predict a verb frame
given a noun and a frame element name, as the frame element already gives away the
frame.
</bodyText>
<footnote confidence="0.883100666666667">
15 It is possible for multiple frame elements to share a name, for example there are multiple frames with a
frame element named THEME. However, conceptually, this is only a shared name, not a shared role across
frames.
</footnote>
<page confidence="0.976828">
757
</page>
<note confidence="0.548067">
Computational Linguistics Volume 36, Number 4
</note>
<subsectionHeader confidence="0.962504">
7.5 Pseudo-Disambiguation: Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999941809523809">
Table 13 shows the results of testing the EPP model for inverse selectional preferences
on pseudo-disambiguation. Coverage is very good for all model variants, similarly to
Experiment 1. The error rates, as well, are close to those for the regular preferences in
the SYN PRIMARY setting (cf. Table 4). The best model there (DEPSPACE, PCA, nGCM
with DISCR weighting) achieved an error rate of 25.6%, and the best model for inverse
preferences (WORDSPACE, Lin with DISCR weighting) reaches an error rate of 27.2%
here. Lin shows the best error rates in all conditions, closely followed by nGCM (the
difference is significant in WORDSPACE and the reduced DEPSPACE, but not significant
in the unreduced DEPSPACE). The Hindle similarity measure again brings up the rear.
In PCA-transformed spaces, the error rates are similar across all similarity measures
except for Hindle, as in Experiment 1.
WORDSPACEs yield better results than DEPSPACEs here, in contrast to Experiment 1.
The best WORDSPACE model (Lin without PCA) reaches significantly better error rates
(p ≤ 0.01) than the best DEPSPACE model (Lin with PCA). We think that the reason for
this lies in the fact that for inverse selectional preferences, the true associate and the
confounder that need to be distinguished in the pseudo-disambiguation task are verbs
rather than nouns. A noun will probably have more other nouns in a bag-of-words
context window than a verb would other verbs, which will make it easier to distinguish
verbs in a WORDSPACE than to distinguish nouns. A DEPSPACE, in contrast, will bring
out differences in the immediate syntactic neighborhood of nouns even if they occur in
the same sentence.
</bodyText>
<sectionHeader confidence="0.848649" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.999739333333333">
In this article, we have presented a similarity-based model of selectional preferences,
EPP. It computes the selectional fit of a candidate role filler as a weighted sum of seman-
tic similarities to headwords observed in a corpus, in a straightforward implementation
</bodyText>
<tableCaption confidence="0.994147">
Table 13
</tableCaption>
<table confidence="0.967968375">
Pseudo-disambiguation results for inverse selectional preferences (BNC as primary and
secondary corpus, DISCR weighting). ER = Error rate; Cov = Coverage.
Dimensions Similarity DEPSPACE WORDSPACE
ER (%) Cov (%) ER (%) Cov (%)
Cosine 37.4 99.0 34.0 99.1
Dice 42.4 98.8 43.4 98.7
Original nGCM 33.7 99.3 31.5 99.3
2,000 dimensions Hindle 48.8 96.0 52.2 94.6
Jaccard 36.7 99.4 44.9 98.7
Lin 32.8 98.9 27.2 98.9
Cosine 35.2 99.0 31.3 99.4
Dice 35.0 99.6 32.9 99.8
PCA nGCM 32.4 99.2 30.3 99.6
500 dimensions Hindle 44.2 99.0 48.7 99.1
Jaccard 34.8 99.6 32.6 99.8
Lin 30.6 99.8 28.8 99.8
</table>
<page confidence="0.985534">
758
</page>
<note confidence="0.590087">
Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<bodyText confidence="0.9999728">
of the intuition that plausibility judgments should generalize to fillers with similar
meaning. Our model is simple and easy to compute. In common with other distri-
butional models like Rooth et al. (1999), it does not depend on lexical resources. Our
model derives additional flexibility from distinguishing between a primary corpus (for
observing headwords) and a generalization corpus (for inducing semantic similarities).
This allows it to use primary corpora with deeper semantic annotation that are too small
as a basis for computing vector space representations.
We have evaluated the EPP model on two tasks, a pseudo-disambiguation task
that can be viewed as an abstraction of both word sense disambiguation and semantic
role labeling, as well as on the prediction of human plausibility judgments. The model
achieves similar error rates to the semantic hierarchy–based RESNIK model, at consid-
erably higher coverage, and it achieves lower error rates than the ROOTH ET AL. soft
clustering model. The semantic role–based PADO ET AL. model, although highly accu-
rate in its predictions, has much lower coverage and needs a semantically annotated
corpus as a basis. We have also demonstrated that our model is able to meaningfully
model inverse selectional preferences, that is, expectations of nouns about verbs for
which they appear as arguments.
With respect to parameter settings of the EPP model, we find consistent patterns
across the three tasks we have considered. nGCM, Lin, and Cosine are the best-
performing similarity measures throughout. The good performance of the nGCM mea-
sure, an exponential similarity measure, is particularly noteworthy. We found it to work
well on data sets that are sparse and not too noisy, whereas the Lin similarity measure
achieved better performance when the data was noisy (see Section 5.4 for details). Di-
mensionality reduction (PCA) on the vector space raises the performance of the Jaccard
and Dice similarity measures to a similar level as the best three. More importantly, PCA
neutralizes a strong frequency bias that otherwise leads to a large performance drop
on rare arguments. Concerning weighting schemes, we found that frequency-based
weighting works well when the data is either clean or not too sparse. In the face of sparse
noisy data, DISCR weighting (a variant of tf/idf) is helpful. Comparing bag-of-words–
based and dependency-based vector spaces, DEPSPACEs are sparser but cleaner than
WORDSPACEs. Accordingly, DEPSPACEs are at an advantage when many headwords are
available, making efficient use of this information, whereas WORDSPACEs work better
for predicates with few seen headwords because they are less affected by sparseness.
We conclude with two open questions. The first question concerns the appropriate
representation of selectional preferences for polysemous verbs such as address, whose
direct object can either be a person, or a problem. Polysemy leads to headwords with
lower similarity among them than for non-polysemous verbs, which in turn can lead
to artificially low plausibilities for all fillers. In the SEM PRIMARY setting, occurrences
of polysemous verbs are separated into different frames. In future work, we hope to
improve our SYN PRIMARY setting models by clustering the seen headwords, and then
computing plausibility of new headwords relative to the nearest cluster.
A second question is the usefulness of inverse selectional preferences for the ac-
quisition of fine-grained information about nouns. As we discussed in Section 7, the
preferred verbs for a noun can often be grouped into meaning clusters. In future work,
we plan to investigate whether there are groups of predicates that recur across similar
nouns, and how they can be characterized. We expect some groups to correspond to
Pustejovsky’s qualia (Pustejovsky 1995), which constitute particularly salient events
for an object, namely, their creation and typical use. However, we expect corpus data
to yield a more complex picture of the events connected to a noun, which manifest
themselves in the form of additional, more specific meaning clusters.
</bodyText>
<page confidence="0.994066">
759
</page>
<note confidence="0.649713">
Computational Linguistics Volume 36, Number 4
</note>
<sectionHeader confidence="0.99505" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99746975">
We would like to thank Detlef Prescher
and Carsten Brockmann for their
implementations of the ROOTH ET AL.
and RESNIK models, respectively. We
are also grateful to Nate Chambers and
Yves Peirsman, as well as to the
anonymous reviewers, for their
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999030327102804">
Abe, Naoki and Hang Li. 1996. Learning
word association norms using tree cut pair
models. In Proceedings of the 10th
International Conference on Machine
Learning, pages 3–11, Bari.
Baroni, Marco, Silvia Bernardini, Adriano
Ferraresi, and Eros Zanchetta. 2009. The
wacky wide Web: A collection of very
large linguistically processed Web-crawled
corpora. Language Resources and Evaluation,
43(3):209–222.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the 13th Conference on
Empirical Methods in Natural Language
Processing, pages 59–68, Honolulu, HI.
Best, John and D. E. Roberts. 1975. Algorithm
AS 89: The upper tail probabilities of
Spearman’s Rho. Applied Statistics,
24:377–379.
Briscoe, Ted and Bran Boguraev, editors.
1989. Computational Lexicography for
Natural Language Processing. Longman
Publishing Group, New York.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 16th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 27–34, Budapest.
Burnard, Lou, 1995. User’s Guide for the
British National Corpus. British National
Corpus Consortium, Oxford University
Computing Services.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference
with Bayesian networks. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 187–193,
Saarbrücken.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
2nd Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 95–102, Pittsburgh, PA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16–23, Madrid.
Curran, James. 2004. From Distributional to
Semantic Similarity. Ph.D. thesis, University
of Edinburgh.
Daelemans, Walter and Antal van den
Bosch. 2005. Memory-Based Language
Processing. Cambridge University Press,
Cambridge, UK.
Dagan, Ido, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models
of word cooccurrence probabilities.
Machine Learning, 34(1):34–69.
Dligach, Dmitriy and Martha Palmer. 2008.
Novel semantic features for verb sense
disambiguation. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics:Human Language
Technologies, Short Papers, pages 29–32,
Columbus, OH.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61–74.
Efron, Bradley and Robert Tibshirani.1994.
An Introduction to the Bootstrap.
Monographs on Statistics and Applied
Probability 57. Chapman &amp; Hall, London.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 216–223, Prague.
Erk, Katrin and Sebastian Padó. 2008.
A structured vector space model for
word meaning in context. In Proceedings
of the 13th Conference on Empirical
Methods in Natural Language Processing,
pages 897–906, Honolulu, HI.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16:235–250.
Firth, John Rupert. 1957. A synopsis of
linguistic theory, 1930–1955. In Philological
Society, editor, Studies in Linguistic
Analysis. Blackwell, Oxford, pages 1–32.
Garnsey, Susan, Neal Pearlmutter, Elizabeth
Myers, and Melanie Lotocky.1997. The
contributions of verb bias and plausibility
to the comprehension of temporarily
ambiguous sentences. Journal of Memory
and Language, 37:58–93.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245–288.
</reference>
<page confidence="0.988174">
760
</page>
<note confidence="0.817136">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<reference confidence="0.999094991525424">
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht.
Grishman, Ralph and John Sterling. 1992.
Acquisition of selectional patterns. In
Proceedings of the 14th International
Conference on Computational Linguistics,
pages 658–664, Nantes.
Harris, Zellig. 1968. Mathematical Structure of
Language. Wiley, New York.
Hay, Jennifer, Aaron Nolan, and Katie
Drager. 2006. From fush to feesh: Exemplar
priming in speech perception. The
Linguistic Review, 23(3):351–379.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics,
pages 268–275, Pittsburgh, PA.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103–120.
Katz, Jerrold J. and Jerry A. Fodor. 1963. The
structure of a semantic theory. Language,
39(2):170–210.
Katz, Jerrold J. and Paul M. Postal. 1964. An
Integrated Theory of Linguistic Descriptions.
Research Monograph No. 26. MIT Press,
Cambridge, MA.
Konieczny, Lars and Philipp Döring. 2003.
Anticipation of clause-final heads.
Evidence from eye-tracking and SRNs.
In Proceedings of the 4th International
Conference on Cognitive Science,
pages 330–335, Sydney.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato’s problem: The latent
semantic analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104(2):211–240.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, pages 25–32, College Park, MA.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112–120,
Columbus, OH.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the Joint Annual Meeting of the Association
for Computational Linguistics and
International Conference on Computational
Linguistics, pages 768–774, Montreal.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of the 23rd
Annual Conference of the Cognitive Science
Society, pages 576–581, Edinburgh.
Lowe, Will and Scott McDonald. 2000. The
direct route: Mediated priming in semantic
space. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society,
pages 675–680, Philadelphia, PA.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and
Computers, 28:203–208.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 256–263,
Seattle, WA.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639–654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42th Annual
Meeting of the Association for Computational
Linguistics, pages 279–286, Barcelona.
McCarthy, Diana, Sriram Venkatapathy, and
Aravind K. Joshi. 2007. Detecting
compositionality of verb-object
combinations using selectional
preferences. In Proceedings of the 12th Joint
Conference on Empirical Methods in Natural
Language Processing and Conference on
Natural Language Learning, pages 369–379,
Prague.
McDonald, Scott and Chris Brew. 2004. A
distributional model of semantic context
effects in lexical processing. In Proceedings
of the 42th Annual Meeting of the Association
for Computational Linguistics, pages 17–24,
Barcelona.
McRae, Ken, Todd Ferretti, and Liane
Amyote. 1997. Thematic roles as
verb-specific concepts. Language and
Cognitive Processes, 12(2/3):137–176.
McRae, Ken, Mary Hare, Jeffrey Elman, and
Todd Ferretti. 2005. A basis for generating
expectancies for verbs from nouns.
Memory and Cognition, 33(7):1174–1184.
McRae, Ken, Mary Hare, Todd Ferretti, and
Jeffrey Elman. 2001. Activating verbs
typical agents, patients, instruments, and
locations via event schemas. In Proceedings
</reference>
<page confidence="0.920621">
761
</page>
<reference confidence="0.994558369747899">
Computational Linguistics Volume 36, Number 4
of the Twenty-Third Annual Conference of the
Cognitive Science Society, pages 617–622,
Mahwah, NJ.
McRae, Ken, Michael Spivey-Knowlton, and
Michael Tanenhaus. 1998. Modeling the
influence of thematic fit (and other
constraints) in on-line sentence
comprehension. Journal of Memory and
Language, 38:283–312.
Miller, George, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine
Miller. 1990. Five papers on WordNet.
International Journal of Lexicography,
3(4):235–312.
Nakatani, Kentaro and Edward Gibson. 2009.
An on-line study of Japanese nesting
complexity. Cognitive Science, 1(34):94–112.
Nastase, Vivi. 2008. Unsupervised all-words
word sense disambiguation with
grammatical dependencies. In Proceedings
of the 3rd International Joint Conference
on Natural Language Processing,
pages 757–762, Honolulu, HI.
Nosofsky, Robert. 1986. Attention, similarity,
and the identification-categorization
relationship. Journal of Experimental
Psychology: General, 115(1):39–57.
Padó, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161–199.
Padó, Sebastian, Ulrike Padó, and Katrin Erk.
2007. Flexible, corpus-based modelling of
human plausibility judgements. In
Proceedings of the 12th Joint Conference on
Empirical Methods in Natural Language
Processing and Conference on Natural
Language Learning, pages 400–409, Prague.
Padó, Ulrike. 2007. The Integration of Syntax
and Semantic Plausibility in a Wide-Coverage
Model of Human Sentence Processing. Ph.D.
thesis, Saarland University, Saarbrücken,
Germany.
Padó, Ulrike, Matthew W. Crocker, and
Frank Keller. 2009. A probabilistic model
of semantic plausibility in sentence
processing. Cognitive Science, 33:794–838.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences. In
Proceedings of the Joint Human Language
Technology Conference and Annual Meeting
of the North American Chapter of the
Association for Computational Linguistics,
pages 564–571, Rochester, NY.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183–190,
Columbus, OH.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127–159.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104–111,
College Park, MA.
Salton, Gerard, Anita Wong, and Chung-Shu
Yang. 1975. A vector-space model for
information retrieval. Journal of the
American Society for Information Science,
18:613–620.
Schulte im Walde, Sabine. 2010. Comparing
computational approaches to selectional
preferences: Second-order co-occurrence
vs. latent semantic clusters. In Proceedings
of the 7th International Conference on
Language Resources and Evaluation,
pages 1381–1388, Valleta.
Schulte im Walde, Sabine, Christian Hying,
Christian Scheible, and Helmut Schmid.
2008. Combining EM training and the
MDL principle for an automatic verb
classification incorporating selectional
preferences. In Proceedings of the 46th
Annual Meeting of the Association for
Computational Linguistics, pages 496–504,
Columbus, OH.
Shepard, Roger. 1987. Towards a universal
law of generalization for psychological
science. Science, 237(4820):1317–1323.
Stolcke, Andreas. 2002. SRILM—an
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, pages 901–904,
Denver, CO.
Toutanova, Kristina, Christoper D. Manning,
Dan Flickinger, and Stephan Oepen. 2005.
Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83–105.
Trueswell, John, Michael Tanenhaus, and
Susan Garnsey. 1994. Semantic influences
on parsing: Use of thematic role
information in syntactic ambiguity
resolution. Journal of Memory and Language,
33:285–318.
Vandekerckhove, Bram, Dominiek Sandra,
and Walter Daelemans. 2009. A robust and
extensible exemplar-based model of
</reference>
<page confidence="0.922951">
762
</page>
<note confidence="0.469544">
Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences
</note>
<reference confidence="0.999478821428571">
thematic fit. In Proceedings of the 12th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 826–834, Athens.
Wilks, Yorick.1975. Preference semantics.
In E. Keenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, UK, pages 329–350.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 266–271, Princeton, NJ.
Zanzotto, Fabio Massimo, Marco
Pennacchiotti, and Maria Teresa Pazienza.
2006. Discovering asymmetric entailment
relations between verbs using selectional
preferences. In Proceedings of the Joint
Annual Meeting of the Association for
Computational Linguistics and International
Conference on Computational Linguistics,
pages 849–856, Sydney.
Zapirain, Beñat, Eneko Agirre, and Lluís
Màrquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the 47th Annual Meeting of the Association
for Computational Linguistics, pages 73–76,
Singapore.
</reference>
<page confidence="0.998718">
763
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502004">
<title confidence="0.853291">A Flexible, Corpus-Driven Model of Regular</title>
<affiliation confidence="0.91019325">and Inverse Selectional Preferences University of Texas at Austin Heidelberg University Vico Research and Consulting GmbH</affiliation>
<abstract confidence="0.995497142857143">We present a vector space–based model for selectional preferences that predicts plausibility scores for argument headwords. It does not require any lexical resources (such as WordNet). It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads. We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the influence of different parameters and comparing our model against other model classes. We obtain consistent benefits from using the disambiguation and semantic role information provided by a semantically tagged primary corpus. As for parameters, we identify settings that yield good performance across a range of experimental conditions. However, frequency remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
<author>Hang Li</author>
</authors>
<title>Learning word association norms using tree cut pair models.</title>
<date>1996</date>
<booktitle>In Proceedings of the 10th International Conference on Machine Learning,</booktitle>
<pages>3--11</pages>
<location>Bari.</location>
<contexts>
<context position="5259" citStr="Abe and Li 1996" startWordPosition="760" endWordPosition="763">, the desired result would be a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, first introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the ac</context>
<context position="15259" citStr="Abe and Li (1996)" startWordPosition="2265" endWordPosition="2268">l preference strength of the verb–relation pair (v, r). This is shown in Equation (2). P(c|v, r)logP(c|v,r) P(c|r) SelAssoc(v, r, c) = (2) SelStr(v, r) Finally, the selectional preference between a verb, a relation, and an argument head is defined as the maximal selectional association of the verb, the relation, and any WordNet class c that the argument can instantiate. We will refer to this model as RESNIK herein. In subsequent years, a number of WordNet-based models were developed that differ from Resnik’s model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not significantly different. Ciaramita and Johnson (2000) encode WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “explain away” ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy specificall</context>
</contexts>
<marker>Abe, Li, 1996</marker>
<rawString>Abe, Naoki and Hang Li. 1996. Learning word association norms using tree cut pair models. In Proceedings of the 10th International Conference on Machine Learning, pages 3–11, Bari.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="53948" citStr="Baroni et al. 2009" startWordPosition="8342" endWordPosition="8345">induced from the data, because the predicates v are already instantiated by verb classes, namely, FrameNet frames (see Table 3). Finally, we report three baselines. The first baseline, headword frequency (HW), is very simple. It decides between the headword a and the confounder a&apos; by comparing the frequencies f (a) and f (a&apos;). The second, more informed, baseline is triple frequency (TRIPLE). It votes for a if f (v, r, a) &gt; f (v, r, a&apos;), and vice versa. The third baseline, a bigram language model (LM), was constructed by training a 2-gram language model from the large English ukWAC Web corpus (Baroni et al. 2009) using the SRILM toolkit (Stolcke 2002) with default Good–Turing smoothing. We retained only verbs, nouns, adjectives, and adverbs in order to maximize the proximity between verbs and their subjects and objects. We defined the preference score for verb–subject triples as the probability of the sequence av, that is, Pref(v, subj, a) = P(vIa). Conversely, the preference score for verb– object triples was defined as the probability of the sequence va, that is, Pref(v,obj,a) = P(a|v). Again, the model compares Pref (v, r, a) and Pref (v, r, a&apos;) to make its decision. Evaluation. For all models, we </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Baroni, Marco, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation, 43(3):209–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>59--68</pages>
<location>Honolulu, HI.</location>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Bergsma, Shane, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of the 13th Conference on Empirical Methods in Natural Language Processing, pages 59–68, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Best</author>
<author>D E Roberts</author>
</authors>
<title>Algorithm AS 89: The upper tail probabilities of Spearman’s Rho.</title>
<date>1975</date>
<journal>Applied Statistics,</journal>
<pages>24--377</pages>
<contexts>
<context position="80776" citStr="Best and Roberts (1975)" startWordPosition="12730" endWordPosition="12733">g the existing verb occurrences in the way that is most meaningful for this task. It covers about 80% of the test data. EPP has full coverage, and although it does not make statistically significant predictions, it shows substantially higher correlation coefficients than ROOTH ET AL. and RESNIK. 12 A second concern is the computation of significance values: The methods most widely used for the Pearson coefficient (Student’s t-distribution, Fisher transformation) assume that the variables are normally distributed, which is not the case in our data set. For Spearman’s p, we use the algorithm by Best and Roberts (1975), which does not make this assumption. 13 Significance here refers to significance of correlation with the human data, not significance of differences between models. 749 Computational Linguistics Volume 36, Number 4 Table 8 Comparison across models on McRae data. **p &lt; 0.01, ***p &lt; 0.001. SEM SYN Model Coverage Spearman’s p Coverage Spearman’s p EPP (DEPSPACE nGCM) 100% 0.154 ns 98% 0.325 *** EPP (DEPSPACE Cosine) 100% 0.162 ns 98% 0.197 ns RESNIK 100% –0.041 ns 100% 0.123 ns ROOTH ET AL. 67% 0.078 ns 48% 0.465 *** PADO ET AL. 78% 0.415 ** – – EPP (WORDSPACE Lin) 100% 0.138 ns 98% 0.062 ns EP</context>
</contexts>
<marker>Best, Roberts, 1975</marker>
<rawString>Best, John and D. E. Roberts. 1975. Algorithm AS 89: The upper tail probabilities of Spearman’s Rho. Applied Statistics, 24:377–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Bran Boguraev</author>
<author>editors</author>
</authors>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<publisher>Longman Publishing Group,</publisher>
<location>New York.</location>
<marker>Briscoe, Boguraev, editors, 1989</marker>
<rawString>Briscoe, Ted and Bran Boguraev, editors. 1989. Computational Lexicography for Natural Language Processing. Longman Publishing Group, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Brockmann</author>
<author>Mirella Lapata</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 16th Meeting of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>27--34</pages>
<location>Budapest.</location>
<contexts>
<context position="40127" citStr="Brockmann and Lapata (2003)" startWordPosition="6068" endWordPosition="6071">bility ratings (Experiment 2). Finally, we investigate inverse selectional preferences—preferences of 6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option. 734 Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences nouns for the predicates that they co-occur with—again using pseudo-disambiguation (Experiment 3). We compare the EPP model to models from the three model categories presented in Section 2: RESNIK as a hierarchical model; ROOTH ET AL. as a distributional model; and PADO ET AL. as a semantic role–based model. As both Brockmann and Lapata (2003) and Pad6 (2007) have argued, no WordNet-based model systematically outperforms the others, and the RESNIK model shows the most consistent behavior across different scenarios. Among the distributional models, we choose ROOTH ET AL. as a model that performs soft clustering and thus shows a marked difference to the EPP model. To our knowledge, this is the first comparison of all three generalization paradigms: semantic hierarchy–based, distributional, and semantic role–based.7 As mentioned earlier, we employ two tasks to evaluate the four models: pseudodisambiguation and the prediction of human </context>
<context position="42564" citStr="Brockmann and Lapata 2003" startWordPosition="6430" endWordPosition="6433"> two potential headwords is better suited to fill the argument position. Human plausibility ratings, on the other hand, make considerably more finegrained distinctions than those occurring in pseudo-disambiguation tasks. Here, models predict the exact human ratings for verb–argument–role triples. Ratings are collected to further control carefully selected experimental items for psycholinguistic studies (Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus 1998), or are solicited for corpus-derived triples specifically to create evaluation data for plausibility models (Brockmann and Lapata 2003; Pad6 2007). We contrast two different levels of semantic analysis for the predicates and argument positions. In the SEM PRIMARY setting, the predicates are FrameNet frames, each of them potentially instantiated by multiple different verbs. The argument positions in these settings are frame-semantic roles. This setting most closely matches the notion of selectional preferences as characterizations of semantic arguments of an event. In addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument positions are grammatical functions (subject and direct object). Viewing gr</context>
</contexts>
<marker>Brockmann, Lapata, 2003</marker>
<rawString>Brockmann, Carsten and Mirella Lapata. 2003. Evaluating and combining approaches to selectional preference acquisition. In Proceedings of the 16th Meeting of the European Chapter of the Association for Computational Linguistics, pages 27–34, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User’s Guide for the British National Corpus.</title>
<date>1995</date>
<institution>British National Corpus Consortium, Oxford University Computing Services.</institution>
<contexts>
<context position="4107" citStr="Burnard 1995" startWordPosition="570" endWordPosition="571">ntemporary models of selectional preferences acquire selectional preferences automatically from large corpora. The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument headword (or filler) from a corpus, and then to compute selectional preference as relative frequencies. However, due to the Zipfian nature of word frequencies, the first step on its own results in a very sparse list of headwords, in particular for less frequent predicates. As an example, the verb anglicize only appears with nine direct objects in the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them, name, appears more than once. Many highly plausible fillers are missing from the list, such as word or spelling. In order to make sensible predictions for triples that are unseen at training time, it is crucial to add a generalization step that infers a degree of preference for new, unseen headwords for a given predicate and role.1 The result is, in the ideal case, an assignment to every possible headword of some degree of compatibility (or plausibility) with the predicate’s preferences. In the case of anglicize, the desired result would be a high plausibility for words lik</context>
<context position="44585" citStr="Burnard 1995" startWordPosition="6731" endWordPosition="6732">s predicates that are verb lemmas, such as cause, and argument positions that are grammatical functions (subj). In both settings, the two potential headwords (here called headword and confounder, to be explained in more detail in the next section) to be distinguished in the pseudo-disambiguation task are noun lemmas. The verb–dependency–headword tuples of the SYN PRIMARY setting yield much more coarse-grained and noisy characterizations of selectional preferences; however, they can be extracted from corpora with only syntactic annotation. We are therefore able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could parse almost all of the corpus, resulting in 6,005,130 parsed sentences. For the SEM PRIMARY setting, we require a primary corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists fine-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exempl</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Burnard, Lou, 1995. User’s Guide for the British National Corpus. British National Corpus Consortium, Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional preference with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>187--193</pages>
<location>Saarbrücken.</location>
<contexts>
<context position="5300" citStr="Ciaramita and Johnson 2000" startWordPosition="766" endWordPosition="769">a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, first introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selection</context>
<context position="15626" citStr="Ciaramita and Johnson (2000)" startWordPosition="2317" endWordPosition="2320">can instantiate. We will refer to this model as RESNIK herein. In subsequent years, a number of WordNet-based models were developed that differ from Resnik’s model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not significantly different. Ciaramita and Johnson (2000) encode WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “explain away” ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy specifically developed on the same corpus. 2.3 Distributional Models Distributional models do not make use of any lexicon resource for the generalization step. Instead, they use word co-occurrence—typically obtained from the same corpus as the observed headwords—for generalization. This independence from manually constructed resources gives distributional models a good cost–b</context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Ciaramita, Massimiliano and Mark Johnson. 2000. Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. In Proceedings of the 18th International Conference on Computational Linguistics, pages 187–193, Saarbrücken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>95--102</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="5322" citStr="Clark and Weir 2001" startWordPosition="770" endWordPosition="773">s like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, first introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our mo</context>
<context position="15439" citStr="Clark and Weir (2001)" startWordPosition="2291" endWordPosition="2294"> preference between a verb, a relation, and an argument head is defined as the maximal selectional association of the verb, the relation, and any WordNet class c that the argument can instantiate. We will refer to this model as RESNIK herein. In subsequent years, a number of WordNet-based models were developed that differ from Resnik’s model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not significantly different. Ciaramita and Johnson (2000) encode WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “explain away” ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy specifically developed on the same corpus. 2.3 Distributional Models Distributional models do not make use of any lexicon resource for the generalization step. Instead, they use word co-occur</context>
</contexts>
<marker>Clark, Weir, 2001</marker>
<rawString>Clark, Stephen and David Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 95–102, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="45434" citStr="Collins (1997)" startWordPosition="6860" endWordPosition="6861">ry corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists fine-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exemplified with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 131,582 annotated sentences (roughly three million words). To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. As generalization corpus, we use the Minipar-parsed BNC in both settings. The experimentation with two different primary corpora allows us to directly study the influence of the disambiguation of predicates and the semantic characterization of argument positions on the performance of selectional preference models. Note, however, that the comparison is complicated by differences between the two corpora: The primary corpus for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the determination of predicates, grammatical functions, and headwords. The primary c</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="30950" citStr="Curran (2004)" startWordPosition="4645" endWordPosition="4646"> context specification that only considers co-occurrences between verbs and their subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items as basis elements. Similarity measure sim. In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similarity is very popular in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice emerged in first place. Padó and Lapata (2007) found good results with Lin’s measure for predominant word sense identification. Because it is unclear whether the findings about best similarity measures generalize to new tasks, we will investigate a range of similarity measures shown in Table 2: Cosine, the Dice and Jaccard coefficients, Hindle’s (1990) and Lin’s (1998) mutual information-based metrics, and an adaptation of Nosofsky’s (1986) Generalized Context Model (GCM), a model for exemplar-based similarity from psychology. The original GCM includes normalization by summed similarity</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>Curran, James. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>Memory-Based Language Processing.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Daelemans, Walter and Antal van den Bosch. 2005. Memory-Based Language Processing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Dagan, Ido, Lillian Lee, and Fernando C. N. Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1):34–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Novel semantic features for verb sense disambiguation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies, Short Papers,</booktitle>
<pages>29--32</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="92380" citStr="Dligach and Palmer 2008" startWordPosition="14647" endWordPosition="14650">a pseudodisambiguation task for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Pad6 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processi</context>
</contexts>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dligach, Dmitriy and Martha Palmer. 2008. Novel semantic features for verb sense disambiguation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies, Short Papers, pages 29–32, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="32960" citStr="Dunning 1993" startWordPosition="4940" endWordPosition="4941">ity measures from Table 2 are applicable to semantic spaces with arbitrary basis elements, with the exception of the Lin measure, whose definition applies only to dependency-based spaces. The reason is that it decomposes the basis elements into relation–word pairs (r,v). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipfian distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans, many studies do not perform dimensionality reduction at all. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500- dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction t</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bradley Efron</author>
<author>Robert Tibshirani 1994</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<booktitle>Monographs on Statistics and Applied Probability 57.</booktitle>
<publisher>Chapman &amp; Hall,</publisher>
<location>London.</location>
<marker>Efron, 1994, </marker>
<rawString>Efron, Bradley and Robert Tibshirani.1994. An Introduction to the Bootstrap. Monographs on Statistics and Applied Probability 57. Chapman &amp; Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--223</pages>
<location>Prague.</location>
<contexts>
<context position="8938" citStr="Erk (2007)" startWordPosition="1316" endWordPosition="1317">cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was first introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Pad6, Pad6, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse preferences, and we introduce a new similarity measure, nGCM, which achieves excellent performance in many settings. 2. Computational Models of Selectional Preferences In this section, we provide an overview of corpus-based models of selectional preferences. See Table 1 for a summary of the notation that we use. 2 As descriptions </context>
<context position="30312" citStr="Erk 2007" startWordPosition="4553" endWordPosition="4554">cy-based spaces are more expensive to compute than word-based spaces because they require a corpus with syntactic analysis. Thus, we explore both options. The word-based space records co-occurrences within a surface window of 10 (lemmatized) words.3 We refer to it as WORDSPACE. The dependencybased space, called DEPSPACE, has basis elements consisting of a grammatical function concatenated with a word, as in the bottom example in Figure 1(b) (Padó and Lapata 2007). Following earlier experiments on the representation of selectional preferences in word-dependency-relation spaces (Padó, Padó, and Erk 2007), we use asubject– object context specification that only considers co-occurrences between verbs and their subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items as basis elements. Similarity measure sim. In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similarity is very popular in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient in pseudo-disambiguation. In the syn</context>
<context position="43478" citStr="Erk (2007)" startWordPosition="6568" endWordPosition="6569">les. This setting most closely matches the notion of selectional preferences as characterizations of semantic arguments of an event. In addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument positions are grammatical functions (subject and direct object). Viewing grammatical functions as shallow approximations of semantic roles, we can expect the selectional preference models for this setting to yield noisier estimates than in the SEM PRIMARY setting. The two settings will differ only in the choice of primary corpus, but will use the same generalization corpus. 7 Erk (2007) has a comparison between hierarchy-based and distributional models, but does not include a semantic role–based model. 735 Computational Linguistics Volume 36, Number 4 Table 3 illustrates the difference between the SEM PRIMARY setting and the SYN PRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARY setting has predicates like the FrameNet frame (predicate sense) ADORNING, with the semantic role THEME as argument position. In contrast, the SYN PRIMARY setting has predicates that are verb lemmas, such as cause, and argument positions that are grammatical functions (s</context>
<context position="77012" citStr="Erk 2007" startWordPosition="12109" endWordPosition="12110">he semantic role–based PADO ET AL. model. We again compare a SYN PRIMARY setting, where the models make predictions for pairs of a verb and a grammatical function, with a SEM PRIMARY setting, for which the two test data sets were annotated with verb sense and semantic roles in the FrameNet paradigm (Padó 2007) and where models make predictions for pairs of a frame and a semantic role. As before, the PADO ET AL. model is only tested in the SEM PRIMARY setting.11 For the EPP model, we focus on parsed, dimensionally unreduced spaces and DISCR weighting, following earlier results (Padó, Padó, and Erk 2007). We provide results for the best WORDSPACE models from Experiment 1 for comparison. The primary corpora for training selectional preference models were prepared as in Experiment 1 (cf. Section 5.1). The generalization corpus for EPP was again the BNC. For the ROOTH ET AL. model in the SYN PRIMARY setting, we again used a frequency cutoff. We found the RESNIK model to perform better when using just a subset of the BNC (namely, all the triples for verbs present in the test set). 6.3 Evaluation Procedure We evaluate our models by correlating the predicted plausibility values with the human judgm</context>
<context position="87120" citStr="Erk (2007)" startWordPosition="13804" endWordPosition="13805">the DEPSPACE models, and do not significantly predict the human plausibility ratings. The frequency baseline shows a considerably better coverage for this data set, but its correlations hover around zero, which underlines our intuition that verb–argument combinations can be plausible without being frequent in corpora. An example is the combination (to) embarrass (an) official, which is rated as highly plausible, but occurs only once each in the BNC and FrameNet. 6.6 Discussion The McRae data set seems in general more difficult to account for than the Pado data set, as noted by Padó, Padó, and Erk (2007). They explain it by a general frequency effect in the BNC data (which are a superset of the FrameNet data): The median frequency of the hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpusderived Pado nouns. Comparing all selectional preference models, we find that the RESNIK and the ROOTH ET AL. models generally do worse than EPP both in terms of coverage and quality of predictions. One notable exception is the excellent performance of the ROOTH ET AL. model on the McRae data in the SYN PRIMARY setting, which comes, however, with a low coverage of less than 50%. </context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Erk, Katrin. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 216–223, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Honolulu, HI.</location>
<marker>Erk, Padó, 2008</marker>
<rawString>Erk, Katrin and Sebastian Padó. 2008. A structured vector space model for word meaning in context. In Proceedings of the 13th Conference on Empirical Methods in Natural Language Processing, pages 897–906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--235</pages>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Fillmore, Charles J., Christopher R. Johnson, and Miriam R. L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rupert Firth</author>
</authors>
<title>A synopsis of linguistic theory, 1930–1955.</title>
<date>1957</date>
<booktitle>In Philological Society, editor, Studies in Linguistic Analysis.</booktitle>
<pages>1--32</pages>
<publisher>Blackwell,</publisher>
<location>Oxford,</location>
<contexts>
<context position="24998" citStr="Firth (1957)" startWordPosition="3721" endWordPosition="3722">fEPP is identical to the range of the employed similarity function sim. For example, the range is [−1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf. Section 3.3). We discuss possible choices of both the similarity sim and the weight wtr,v in Section 3.3. 3.2 Vector Space Representations We use vector space representations for generalization. In a vector space model, each target word is represented as a vector, typically constructed from co-occurrence counts with context words in a large corpus (the so-called basis elements). The underlying assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reflects their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, John Rupert. 1957. A synopsis of linguistic theory, 1930–1955. In Philological Society, editor, Studies in Linguistic Analysis. Blackwell, Oxford, pages 1–32.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Susan Garnsey</author>
<author>Neal Pearlmutter</author>
<author>Elizabeth Myers</author>
<author>Melanie Lotocky 1997</author>
</authors>
<title>The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences.</title>
<journal>Journal of Memory and Language,</journal>
<pages>37--58</pages>
<contexts>
<context position="73699" citStr="Garnsey et al. 1997" startWordPosition="11579" endWordPosition="11582">hen the collections of seen headwords are noisier, as they are in the SYN PRIMARY setting, DEPSPACE, with its more aggressive filtering, yields the better results. Sets of headwords collected by predicate sense, as in the SEM PRIMARY setting, are sparser but cleaner, and WORDSPACE shows lower error rates. 6. Experiment 2: Human Plausibility Judgments Experimental psycholinguistics affords a second perspective on selectional preferences: The plausibility of verb–argument pairs has been shown to have an important effect on human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnsey et al. 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998). In these studies, plausibility was operationalized as the thematic fit or selectional preference between a verb and its argument in a specific argument position. Models of human sentence processing therefore need selectional preference models (Padó, Crocker, and Keller 2009). Conversely, psycholinguistic plausibility judgments can be used to evaluate computational models of selectional preferences. 6.1 Experimental Materials We present evaluations on two plausibility judgment data sets used in recent studies. 747 Computational Linguistics Volume 3</context>
</contexts>
<marker>Garnsey, Pearlmutter, Myers, 1997, </marker>
<rawString>Garnsey, Susan, Neal Pearlmutter, Elizabeth Myers, and Melanie Lotocky.1997. The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences. Journal of Memory and Language, 37:58–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2539" citStr="Gildea and Jurafsky 2002" startWordPosition="353" endWordPosition="356">tics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ** E-mail: pado@cl.uni-heidelberg.de. † E-mail:ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sente</context>
<context position="5471" citStr="Gildea and Jurafsky 2002" startWordPosition="795" endWordPosition="798">like cow and machine. The predominant approach to generalizing over headwords, first introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identified with reason</context>
<context position="64590" citStr="Gildea and Jurafsky (2002)" startWordPosition="10107" endWordPosition="10110">M NA NA 743 Computational Linguistics Volume 36, Number 4 The error rate of the PADO ET AL. model, at 7%, is the best by a large margin. We attribute this to the extensive generalization mechanisms that the model uses, which draw on an array of lexical–semantic resources. However, with a coverage of 59%, the model is still unable to make predictions for many of the test items. Error rates for the RESNIK and the EPP models are comparable, at 16.5% for RESNIK and 15.7% for the best EPP variant. The two models differ sharply in coverage, however: 62.8% for RESNIK, consistent with the findings of Gildea and Jurafsky (2002), and between 90% and 98% for EPP variants. The RESNIK model also profits from the presence of semantic disambiguation in the SEM PRIMARY setting (in the SYN PRIMARY setting its error rate was 28%), which underlines the substantial impact that properties of the training data have on semantic hierarchy–based models of selectional preferences. ROOTH ET AL. now has perfect coverage, affirming our assumption that the very bad results of the ROOTH ET AL. model in the SYN PRIMARY setting were an artifact of the data sampling necessary for that data set. Although its error rate of 24.9% is a substant</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="25528" citStr="Grefenstette 1994" startWordPosition="3801" endWordPosition="3802">he so-called basis elements). The underlying assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reflects their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that profit from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the influence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by t</context>
<context position="28417" citStr="Grefenstette 1994" startWordPosition="4254" endWordPosition="4255">dow. Such bag-of-words spaces tend to group words by topics. They ignore the syntactic relation between context items and the target, which is a problem for selectional preference modeling. The top table in Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though they show complementary plausibility ratings. The reason is that deer and hunter often co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities). The bottom table in Figure 1(b) indicates a way out of this problem, namely the use of word-relation pairs as basis elements (Grefenstette 1994; Padó and Lapata 2007). This space splits the co-occurrences with context words such as shoot based on the grammatical relation between target and context word, and this split looks different for different words: whereas deer occurs exclusively as the object of shoot, hunter predominantly occurs as the subject. We find the reverse pattern for escape. In consequence, 731 Computational Linguistics Volume 36, Number 4 Table 2 Similarity measures explored in this article. Notation: We assume Basis = {b1, ... , bn}. We write I for mutual information, and BE(a) for the set of basis elements that co</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Acquisition of selectional patterns.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>658--664</pages>
<location>Nantes.</location>
<contexts>
<context position="15766" citStr="Grishman and Sterling (1992)" startWordPosition="2340" endWordPosition="2343">fer from Resnik’s model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not significantly different. Ciaramita and Johnson (2000) encode WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “explain away” ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy specifically developed on the same corpus. 2.3 Distributional Models Distributional models do not make use of any lexicon resource for the generalization step. Instead, they use word co-occurrence—typically obtained from the same corpus as the observed headwords—for generalization. This independence from manually constructed resources gives distributional models a good cost–benefit ratio and makes them especially attractive for domain-specific applications. These models, like the 727 Computational Linguistics Vol</context>
</contexts>
<marker>Grishman, Sterling, 1992</marker>
<rawString>Grishman, Ralph and John Sterling. 1992. Acquisition of selectional patterns. In Proceedings of the 14th International Conference on Computational Linguistics, pages 658–664, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structure of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="25016" citStr="Harris (1968)" startWordPosition="3724" endWordPosition="3725"> to the range of the employed similarity function sim. For example, the range is [−1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf. Section 3.3). We discuss possible choices of both the similarity sim and the weight wtr,v in Section 3.3. 3.2 Vector Space Representations We use vector space representations for generalization. In a vector space model, each target word is represented as a vector, typically constructed from co-occurrence counts with context words in a large corpus (the so-called basis elements). The underlying assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reflects their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources based o</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Zellig. 1968. Mathematical Structure of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Hay</author>
<author>Aaron Nolan</author>
<author>Katie Drager</author>
</authors>
<title>From fush to feesh: Exemplar priming in speech perception. The Linguistic Review,</title>
<date>2006</date>
<marker>Hay, Nolan, Drager, 2006</marker>
<rawString>Hay, Jennifer, Aaron Nolan, and Katie Drager. 2006. From fush to feesh: Exemplar priming in speech perception. The Linguistic Review, 23(3):351–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="8612" citStr="Hindle 1990" startWordPosition="1265" endWordPosition="1266">es. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional models on both tasks. Finally, we investigate inverse preferences, that is, preferences that arguments have for their predicates. Although there is ample cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was first introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Pad6, Pad6, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse prefer</context>
<context position="92044" citStr="Hindle 1990" startWordPosition="14601" endWordPosition="14602">oratory data analysis of inverse selectional preferences, which shows that inverse selectional preferences show semantically coherent patterns like regular selectional preferences, but that, in contrast to most verbs, nouns tend to occur with multiple semantic groups of verbs. In Sections 7.3–7.5, we test the EPP model on a pseudodisambiguation task for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Pad6 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, Donald. 1990. Noun classification from predicate-argument structures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 268–275, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1833" citStr="Hindle and Rooth 1993" startWordPosition="256" endWordPosition="259">y remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items. 1. Introduction Selectional preferences or selectional constraints describe knowledge about possible and plausible fillers for a predicate’s argument positions. They model the fact that there is often a semantically coherent set of concepts that can fill a given argument position. Selectional preferences can help for many text analysis tasks which involve comparing different attachment decisions. Examples include syntactic disambiguation (Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD, * Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ** E-mail: pado@cl.uni-heidelberg.de. † E-mail:ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguist</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Jerry A Fodor</author>
</authors>
<title>The structure of a semantic theory.</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="11176" citStr="Katz and Fodor 1963" startWordPosition="1654" endWordPosition="1657">references are conditioned, for example, WordNet sense, FrameNet frame, or latent semantic class. VS = (DTrans, Vector space. Basis is a set of basis elements, sima similarity Basis, sim, STrans) measure, DTrans a transformation of raw counts, and STrans a transformation of the space. We write w� = (wb1, ... , wbn) for the representation of w E Lemmas in a vector space with Basis = {b1,...,bn}. wtr,v(a) Weight of argument headword a for predicate v and role r. 2.1 Historical Models In formal linguistics, selectional restrictions were employed as strict Boolean restrictions by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual disambiguation process between predicates and their modifiers. Sentences are semantically anomalous if there are no mutually consistent readings for the two words. Semantically anomalous sentences would receive no reading, whereas ambiguous sentences would receive several readings. The strict dismissal as meaningless of sentences that violate selectional restrictions was later criticized. A case in point is metaphors, which often combine predicates and arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated that “rejectin</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, Jerrold J. and Jerry A. Fodor. 1963. The structure of a semantic theory. Language, 39(2):170–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Paul M Postal</author>
</authors>
<title>An Integrated Theory of Linguistic Descriptions.</title>
<date>1964</date>
<journal>Research Monograph No.</journal>
<volume>26</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11199" citStr="Katz and Postal 1964" startWordPosition="1658" endWordPosition="1661">ioned, for example, WordNet sense, FrameNet frame, or latent semantic class. VS = (DTrans, Vector space. Basis is a set of basis elements, sima similarity Basis, sim, STrans) measure, DTrans a transformation of raw counts, and STrans a transformation of the space. We write w� = (wb1, ... , wbn) for the representation of w E Lemmas in a vector space with Basis = {b1,...,bn}. wtr,v(a) Weight of argument headword a for predicate v and role r. 2.1 Historical Models In formal linguistics, selectional restrictions were employed as strict Boolean restrictions by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual disambiguation process between predicates and their modifiers. Sentences are semantically anomalous if there are no mutually consistent readings for the two words. Semantically anomalous sentences would receive no reading, whereas ambiguous sentences would receive several readings. The strict dismissal as meaningless of sentences that violate selectional restrictions was later criticized. A case in point is metaphors, which often combine predicates and arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated that “rejecting utterances is just wh</context>
</contexts>
<marker>Katz, Postal, 1964</marker>
<rawString>Katz, Jerrold J. and Paul M. Postal. 1964. An Integrated Theory of Linguistic Descriptions. Research Monograph No. 26. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Konieczny</author>
<author>Philipp Döring</author>
</authors>
<title>Anticipation of clause-final heads. Evidence from eye-tracking and SRNs.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Conference on Cognitive Science,</booktitle>
<pages>330--335</pages>
<location>Sydney.</location>
<contexts>
<context position="93342" citStr="Konieczny and Döring 2003" startWordPosition="14788" endWordPosition="14791">ctional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-final word order (as in Japanese or in German subordinate clauses), where hearers may encounter all objects before the head. It is likely that these objects are immediately integrated into a preliminary event structure with an assumed predicate instead of being stored in short-term memory until the predicate is encountered (Konieczny and Döring 2003; Nakatani and Gibson 2009). Another strand of work is McRae et al. (2001, 2005), who have studied priming of verbs from nouns. They found that a noun engenders priming of verbs for which it is a typical agent, patient, instrument, or location. In theoretical linguistics, the idea of event knowledge being encoded in the lexical entries of nouns has been formulated in the context of Pustejovsky’s generative lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide information about the typical use of an object (book: read) and its construction (book: write), 753 Computationa</context>
</contexts>
<marker>Konieczny, Döring, 2003</marker>
<rawString>Konieczny, Lars and Philipp Döring. 2003. Anticipation of clause-final heads. Evidence from eye-tracking and SRNs. In Proceedings of the 4th International Conference on Cognitive Science, pages 330–335, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="11736" citStr="Lakoff and Johnson 1980" startWordPosition="1733" endWordPosition="1736">ict Boolean restrictions by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual disambiguation process between predicates and their modifiers. Sentences are semantically anomalous if there are no mutually consistent readings for the two words. Semantically anomalous sentences would receive no reading, whereas ambiguous sentences would receive several readings. The strict dismissal as meaningless of sentences that violate selectional restrictions was later criticized. A case in point is metaphors, which often combine predicates and arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated that “rejecting utterances is just what humans do not. They try to understand them.” He proposes to reconceptualize selectional restrictions as preferences whose violation is dispreferred, but not fatal. His proposal for a semantic interpretation mechanism still uses semantic primitives, but always produces a single most plausible interpretation by choosing the senses of each word that maximize the compatibility between selectional preferences and semantic types. In this manner, he is able to compute semantic representations for sentences that violate selectional rest</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>Lakoff, George and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="33320" citStr="Landauer and Dumais 1997" startWordPosition="4993" endWordPosition="4996">pted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipfian distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans, many studies do not perform dimensionality reduction at all. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500- dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction that identifies the directions of highest variance in a high-dimensional space. Weight functions wt. Exemplar-based models are usually applied in conjunction with a function that can assign each exemplar an individual weight, which can be interpreted cognitively as degree of activation (Nosofsky 1986). We assess a small number of weight functions to investiga</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>College Park, MA.</location>
<contexts>
<context position="30826" citStr="Lee (1999)" startWordPosition="4628" endWordPosition="4629">ntation of selectional preferences in word-dependency-relation spaces (Padó, Padó, and Erk 2007), we use asubject– object context specification that only considers co-occurrences between verbs and their subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items as basis elements. Similarity measure sim. In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similarity is very popular in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice emerged in first place. Padó and Lapata (2007) found good results with Lin’s measure for predominant word sense identification. Because it is unclear whether the findings about best similarity measures generalize to new tasks, we will investigate a range of similarity measures shown in Table 2: Cosine, the Dice and Jaccard coefficients, Hindle’s (1990) and Lin’s (1998) mutual information-based metrics, and an adaptation of Nosofsky’s (1986) Generalized Context Mode</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, Lillian. 1999. Measures of distributional similarity. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 25–32, College Park, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>112--120</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="44684" citStr="Lin 1993" startWordPosition="6748" endWordPosition="6749"> (subj). In both settings, the two potential headwords (here called headword and confounder, to be explained in more detail in the next section) to be distinguished in the pseudo-disambiguation task are noun lemmas. The verb–dependency–headword tuples of the SYN PRIMARY setting yield much more coarse-grained and noisy characterizations of selectional preferences; however, they can be extracted from corpora with only syntactic annotation. We are therefore able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could parse almost all of the corpus, resulting in 6,005,130 parsed sentences. For the SEM PRIMARY setting, we require a primary corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists fine-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exemplified with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 1</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>Lin, Dekang. 1993. Principle-based parsing without overgeneration. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 112–120, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the Joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montreal.</location>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the Joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
</authors>
<title>Towards a theory of semantic space.</title>
<date>2001</date>
<booktitle>In Proceedings of the 23rd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>576--581</pages>
<location>Edinburgh.</location>
<contexts>
<context position="27096" citStr="Lowe (2001)" startWordPosition="4049" endWordPosition="4050">tation of hunter is compared to the members of the exemplar cloud for the agent position—namely, poacher, policeman, and director. Due to the high average similarity of the hunter vector to these vectors, hunter will be judged a fairly good agent of shoot. Compare this with the result for the patient role: hunter is rather distant from roe, deer, and buck, and is therefore predicted to be a bad patient of shoot. However, note that hunter is still more plausible as a patient of shoot than, for example, director. 3.3 Formalization and Parameter Choice Vector space models have been formalized by Lowe (2001) as tuples VS = (DTrans, Basis, sim, STrans), where Basis is a set of basis elements or dimensions, DTrans is a transformation of raw co-occurrence counts, sim is a similarity measure, and STrans is a transformation of the whole space, typically dimensionality reduction. An additional parameter that becomes relevant for our use of vector spaces (cf. Equation [6]) is the weighting function wt that determines the contribution of each exemplar to the overall similarity. We discuss the parameters in turn and discuss our reasons for either exploring them or fixing them. Basis elements Basis. Tradit</context>
<context position="33133" citStr="Lowe (2001)" startWordPosition="4965" endWordPosition="4966">ased spaces. The reason is that it decomposes the basis elements into relation–word pairs (r,v). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipfian distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans, many studies do not perform dimensionality reduction at all. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500- dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction that identifies the directions of highest variance in a high-dimensional space. Weight functions wt. Exemplar-based models are usually applied in conjunction with a function </context>
</contexts>
<marker>Lowe, 2001</marker>
<rawString>Lowe, Will. 2001. Towards a theory of semantic space. In Proceedings of the 23rd Annual Conference of the Cognitive Science Society, pages 576–581, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
<author>Scott McDonald</author>
</authors>
<title>The direct route: Mediated priming in semantic space.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>675--680</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="25959" citStr="Lowe and McDonald 2000" startWordPosition="3863" endWordPosition="3866"> similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that profit from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the influence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences Figure 1 An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter). fillers of the agent and patient position of shoot, respectively. In order to judge whether a hunter is a plausible agent of shoot, the vector space representation of hunter is compared to the members of the exemplar cloud for the </context>
</contexts>
<marker>Lowe, McDonald, 2000</marker>
<rawString>Lowe, Will and Scott McDonald. 2000. The direct route: Mediated priming in semantic space. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 675–680, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="6156" citStr="Lund and Burgess 1996" startWordPosition="899" endWordPosition="902">nformation, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identified with reasonable accuracy. Its key point is to use vector space similarity (Lund and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen 1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similar predicates. 724 Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences headwords. The vector space representations which serve as a basis for computing similarity can in principle be computed from any arbitrary corpus, given that it is large enough. In particular, this need not be the same corpus as the one on which we observe predicate–headword co-occurrences. Our model thus distinguishes between a prim</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Lund, Kevin and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching alternations.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>256--263</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2978" citStr="McCarthy 2000" startWordPosition="409" endWordPosition="410">2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences for predicates and their argument positions. Given the immense effort necessary for manual semantic lexicon building and its associated reliability problems (see, e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences acquire selectional preferences automat</context>
</contexts>
<marker>McCarthy, 2000</marker>
<rawString>McCarthy, Diana. 2000. Using semantic preferences to identify verbal participation in role switching alternations. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 256–263, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2483" citStr="McCarthy and Carroll 2003" startWordPosition="345" endWordPosition="348">, word sense disambiguation (WSD, * Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ** E-mail: pado@cl.uni-heidelberg.de. † E-mail:ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argume</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>McCarthy, Diana and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>279--286</pages>
<location>Barcelona.</location>
<contexts>
<context position="25589" citStr="McCarthy et al. 2004" startWordPosition="3807" endWordPosition="3810">hich goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reflects their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that profit from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the influence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics, pages 279–286, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Detecting compositionality of verb-object combinations using selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning,</booktitle>
<pages>369--379</pages>
<location>Prague.</location>
<marker>McCarthy, Venkatapathy, Joshi, 2007</marker>
<rawString>McCarthy, Diana, Sriram Venkatapathy, and Aravind K. Joshi. 2007. Detecting compositionality of verb-object combinations using selectional preferences. In Proceedings of the 12th Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning, pages 369–379, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
<author>Chris Brew</author>
</authors>
<title>A distributional model of semantic context effects in lexical processing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<location>Barcelona.</location>
<contexts>
<context position="25914" citStr="McDonald and Brew 2004" startWordPosition="3856" endWordPosition="3859">els are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that profit from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the influence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences Figure 1 An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter). fillers of the agent and patient position of shoot, respectively. In order to judge whether a hunter is a plausible agent of shoot, the vector space representation of hunter is compared </context>
</contexts>
<marker>McDonald, Brew, 2004</marker>
<rawString>McDonald, Scott and Chris Brew. 2004. A distributional model of semantic context effects in lexical processing. In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics, pages 17–24, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>Todd Ferretti</author>
<author>Liane Amyote</author>
</authors>
<title>Thematic roles as verb-specific concepts.</title>
<date>1997</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>12--2</pages>
<marker>McRae, Ferretti, Amyote, 1997</marker>
<rawString>McRae, Ken, Todd Ferretti, and Liane Amyote. 1997. Thematic roles as verb-specific concepts. Language and Cognitive Processes, 12(2/3):137–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>Mary Hare</author>
<author>Jeffrey Elman</author>
<author>Todd Ferretti</author>
</authors>
<title>A basis for generating expectancies for verbs from nouns.</title>
<date>2005</date>
<journal>Memory and Cognition,</journal>
<volume>33</volume>
<issue>7</issue>
<contexts>
<context position="8410" citStr="McRae et al. 2005" startWordPosition="1236" endWordPosition="1239">nse disambiguation, but also for semantic role labeling. The second task is the prediction of human plausibility ratings, which is a standard task-independent benchmark for the quality of selectional preferences. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional models on both tasks. Finally, we investigate inverse preferences, that is, preferences that arguments have for their predicates. Although there is ample cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was first introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied</context>
</contexts>
<marker>McRae, Hare, Elman, Ferretti, 2005</marker>
<rawString>McRae, Ken, Mary Hare, Jeffrey Elman, and Todd Ferretti. 2005. A basis for generating expectancies for verbs from nouns. Memory and Cognition, 33(7):1174–1184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>Mary Hare</author>
<author>Todd Ferretti</author>
<author>Jeffrey Elman</author>
</authors>
<title>Activating verbs typical agents, patients, instruments, and locations via event schemas.</title>
<date>2001</date>
<booktitle>In Proceedings Computational Linguistics Volume 36, Number 4 of the Twenty-Third Annual Conference of the Cognitive Science Society,</booktitle>
<pages>617--622</pages>
<location>Mahwah, NJ.</location>
<contexts>
<context position="93415" citStr="McRae et al. (2001" startWordPosition="14801" endWordPosition="14804">uistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-final word order (as in Japanese or in German subordinate clauses), where hearers may encounter all objects before the head. It is likely that these objects are immediately integrated into a preliminary event structure with an assumed predicate instead of being stored in short-term memory until the predicate is encountered (Konieczny and Döring 2003; Nakatani and Gibson 2009). Another strand of work is McRae et al. (2001, 2005), who have studied priming of verbs from nouns. They found that a noun engenders priming of verbs for which it is a typical agent, patient, instrument, or location. In theoretical linguistics, the idea of event knowledge being encoded in the lexical entries of nouns has been formulated in the context of Pustejovsky’s generative lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide information about the typical use of an object (book: read) and its construction (book: write), 753 Computational Linguistics Volume 36, Number 4 respectively. Pustejovsky uses this kno</context>
</contexts>
<marker>McRae, Hare, Ferretti, Elman, 2001</marker>
<rawString>McRae, Ken, Mary Hare, Todd Ferretti, and Jeffrey Elman. 2001. Activating verbs typical agents, patients, instruments, and locations via event schemas. In Proceedings Computational Linguistics Volume 36, Number 4 of the Twenty-Third Annual Conference of the Cognitive Science Society, pages 617–622, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>Michael Spivey-Knowlton</author>
<author>Michael Tanenhaus</author>
</authors>
<title>Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<pages>38--283</pages>
<contexts>
<context position="71674" citStr="McRae et al., 1998" startWordPosition="11256" endWordPosition="11259">on. The Hindle measure is an underperformer in all conditions. With Lin, Jaccard, Dice, and Hindle, error rates rise sharply for less frequent arguments in many spaces. Although Cosine and nGCM also have some frequency bias, it is much less pronounced. nGCM seems to work well with sparse data sets that are not too noisy, as evidenced by the fact that it has the best performance among all EPP variants on both DEPSPACEs in 746 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences Table 6 Verb–argument position–noun triples with plausibility judgments on a 7-point scale (McRae et al., 1998). Verb Argument position Noun Plausibility shoot agent hunter 6.9 shoot patient hunter 2.8 shoot agent deer 1.0 shoot patient deer 6.4 the SYN PRIMARY setting, as well as in all SEM conditions except reduced WORDSPACE. The Lin measure seems to work well with noisier data: It is the best EPP model when using WORDSPACE in the SYN PRIMARY setting. Cosine, although never showing the top performance, is among the best models in any setting. Although dimensionality reduction only improves the overall error rates of the best models by a few points, it has two important consequences: First, dimensiona</context>
</contexts>
<marker>McRae, Spivey-Knowlton, Tanenhaus, 1998</marker>
<rawString>McRae, Ken, Michael Spivey-Knowlton, and Michael Tanenhaus. 1998. Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38:283–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="5029" citStr="Miller et al. 1990" startWordPosition="720" endWordPosition="723"> unseen headwords for a given predicate and role.1 The result is, in the ideal case, an assignment to every possible headword of some degree of compatibility (or plausibility) with the predicate’s preferences. In the case of anglicize, the desired result would be a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, first introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. Thes</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, George, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Five papers on WordNet. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Nakatani</author>
<author>Edward Gibson</author>
</authors>
<title>An on-line study of Japanese nesting complexity.</title>
<date>2009</date>
<journal>Cognitive Science,</journal>
<volume>1</volume>
<issue>34</issue>
<contexts>
<context position="93369" citStr="Nakatani and Gibson 2009" startWordPosition="14792" endWordPosition="14795">en the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-final word order (as in Japanese or in German subordinate clauses), where hearers may encounter all objects before the head. It is likely that these objects are immediately integrated into a preliminary event structure with an assumed predicate instead of being stored in short-term memory until the predicate is encountered (Konieczny and Döring 2003; Nakatani and Gibson 2009). Another strand of work is McRae et al. (2001, 2005), who have studied priming of verbs from nouns. They found that a noun engenders priming of verbs for which it is a typical agent, patient, instrument, or location. In theoretical linguistics, the idea of event knowledge being encoded in the lexical entries of nouns has been formulated in the context of Pustejovsky’s generative lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide information about the typical use of an object (book: read) and its construction (book: write), 753 Computational Linguistics Volume 36, Nu</context>
</contexts>
<marker>Nakatani, Gibson, 2009</marker>
<rawString>Nakatani, Kentaro and Edward Gibson. 2009. An on-line study of Japanese nesting complexity. Cognitive Science, 1(34):94–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Unsupervised all-words word sense disambiguation with grammatical dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>757--762</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="92414" citStr="Nastase 2008" startWordPosition="14655" endWordPosition="14656">ctional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Pad6 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-final word order (as in</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Nastase, Vivi. 2008. Unsupervised all-words word sense disambiguation with grammatical dependencies. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 757–762, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Nosofsky</author>
</authors>
<title>Attention, similarity, and the identification-categorization relationship.</title>
<date>1986</date>
<journal>Journal of Experimental Psychology: General,</journal>
<volume>115</volume>
<issue>1</issue>
<contexts>
<context position="21181" citStr="Nosofsky 1986" startWordPosition="3132" endWordPosition="3133">neralizing to unseen headwords with memory-based learning. 3. A Distributional Exemplar-Based Model of Selectional Preferences: EPP We now present the EPP model of selectional preferences. It falls into the category of distributional models. More specifically, it is an exemplar model that remembers all seen headwords for a given argument position and computes the degree of plausibility for a new headword candidate through its similarity to the stored exemplars. Exemplars are modeled as vectors in a semantic space. Exemplar models are a well-known modeling framework that is used in psychology (Nosofsky 1986), in computational linguistics (under the name of memory-based learning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonetics (Hay, Nolan, and Drager 2006). The appeal of exemplar models is that they provide a cognitively plausible process of learning as storing exemplars, and categorization as similarity computation that is grounded in features of the exemplars (e.g., formants in phonetics, and contexts in lexical semantics). The representation of selectional preferences through feature vectors also fits in well with work in psycholinguistics by McRae, Ferretti, and </context>
<context position="33861" citStr="Nosofsky 1986" startWordPosition="5072" endWordPosition="5073">. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500- dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction that identifies the directions of highest variance in a high-dimensional space. Weight functions wt. Exemplar-based models are usually applied in conjunction with a function that can assign each exemplar an individual weight, which can be interpreted cognitively as degree of activation (Nosofsky 1986). We assess a small number of weight functions to investigate their importance within the EPP model. The first one, UNI, assumes a uniform distribution, wtr,v(a) = 1. The second one, FREQ, uses the cooccurrence frequency as weight, wtr,v(a) = freq(a,r,v), with the intuition that more frequent exemplars should be both more activated and more reliable. Finally, we consider a weight function that is an analogue of inverse document frequency in Information Retrieval. It we�i.ghts words higher that occur with a smaller number of verb–role pairs: wtr,v(a) = log senrv(a)|&apos;)|, where we write Seenrv(a)</context>
</contexts>
<marker>Nosofsky, 1986</marker>
<rawString>Nosofsky, Robert. 1986. Attention, similarity, and the identification-categorization relationship. Journal of Experimental Psychology: General, 115(1):39–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="28440" citStr="Padó and Lapata 2007" startWordPosition="4256" endWordPosition="4259">rds spaces tend to group words by topics. They ignore the syntactic relation between context items and the target, which is a problem for selectional preference modeling. The top table in Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though they show complementary plausibility ratings. The reason is that deer and hunter often co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities). The bottom table in Figure 1(b) indicates a way out of this problem, namely the use of word-relation pairs as basis elements (Grefenstette 1994; Padó and Lapata 2007). This space splits the co-occurrences with context words such as shoot based on the grammatical relation between target and context word, and this split looks different for different words: whereas deer occurs exclusively as the object of shoot, hunter predominantly occurs as the subject. We find the reverse pattern for escape. In consequence, 731 Computational Linguistics Volume 36, Number 4 Table 2 Similarity measures explored in this article. Notation: We assume Basis = {b1, ... , bn}. We write I for mutual information, and BE(a) for the set of basis elements that co-occur at least once wi</context>
<context position="30170" citStr="Padó and Lapata 2007" startWordPosition="4534" endWordPosition="4537">he ability to distinguish between words like hunter and deer, based on differences in typical occurrences in argument positions. On the downside, dependency-based spaces are more expensive to compute than word-based spaces because they require a corpus with syntactic analysis. Thus, we explore both options. The word-based space records co-occurrences within a surface window of 10 (lemmatized) words.3 We refer to it as WORDSPACE. The dependencybased space, called DEPSPACE, has basis elements consisting of a grammatical function concatenated with a word, as in the bottom example in Figure 1(b) (Padó and Lapata 2007). Following earlier experiments on the representation of selectional preferences in word-dependency-relation spaces (Padó, Padó, and Erk 2007), we use asubject– object context specification that only considers co-occurrences between verbs and their subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items as basis elements. Similarity measure sim. In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similar</context>
<context position="32760" citStr="Padó and Lapata 2007" startWordPosition="4911" endWordPosition="4914">a) + BE(a&apos;) else 732 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences preserves the central idea that similarity decreases exponentially with distance (Shepard 1987). All similarity measures from Table 2 are applicable to semantic spaces with arbitrary basis elements, with the exception of the Lin measure, whose definition applies only to dependency-based spaces. The reason is that it decomposes the basis elements into relation–word pairs (r,v). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipfian distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans, many studies do not perform dimensionality reduction at all. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To </context>
</contexts>
<marker>Padó, Lapata, 2007</marker>
<rawString>Padó, Sebastian and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Ulrike Padó</author>
<author>Katrin Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning,</booktitle>
<pages>400--409</pages>
<location>Prague.</location>
<marker>Padó, Padó, Erk, 2007</marker>
<rawString>Padó, Sebastian, Ulrike Padó, and Katrin Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proceedings of the 12th Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning, pages 400–409, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Padó</author>
</authors>
<title>The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Human Sentence Processing.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University,</institution>
<location>Saarbrücken, Germany.</location>
<contexts>
<context position="75639" citStr="Padó 2007" startWordPosition="11876" endWordPosition="11877">s data set if they can capture the difference between verb argument positions as well as between individual fillers. However, because the verb–argument pairs were created by hand and with strict requirements, many of the arguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight). When FrameNet is used to annotate senses for the verbs, no appropriate senses are available for 28 of the 100 verb–argument pairs, reducing the test set to 72 data points. The second, larger data set addresses this sparseness issue. Its triples are constructed on the basis of corpus co-occurrences (Padó 2007). Eighteen verbs are combined with their three most frequent subjects and objects found in the Penn Treebank and FrameNet corpora, respectively, up to a total of 12 arguments. Each verb–argument pair was rated both as an agent and as a patient (i.e., both in the observed and an unobserved argument position), which leads to a total of 24 rated triples per verb. The data set contains ratings for 414 triples. The resulting judgments show a more even distribution of data. With FrameNet annotation for the verbs, appropriate senses are not attested for six verb–argument pairs, reducing the test set </context>
</contexts>
<marker>Padó, 2007</marker>
<rawString>Padó, Ulrike. 2007. The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Human Sentence Processing. Ph.D. thesis, Saarland University, Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Padó</author>
<author>Matthew W Crocker</author>
<author>Frank Keller</author>
</authors>
<title>A probabilistic model of semantic plausibility in sentence processing.</title>
<date>2009</date>
<journal>Cognitive Science,</journal>
<pages>33--794</pages>
<marker>Padó, Crocker, Keller, 2009</marker>
<rawString>Padó, Ulrike, Matthew W. Crocker, and Frank Keller. 2009. A probabilistic model of semantic plausibility in sentence processing. Cognitive Science, 33:794–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>564--571</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="2691" citStr="Pantel et al. 2007" startWordPosition="373" endWordPosition="376">ke.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences </context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Pantel, Patrick, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Proceedings of the Joint Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 564–571, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH.</location>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183–190, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="93778" citStr="Pustejovsky 1995" startWordPosition="14861" endWordPosition="14862">ly integrated into a preliminary event structure with an assumed predicate instead of being stored in short-term memory until the predicate is encountered (Konieczny and Döring 2003; Nakatani and Gibson 2009). Another strand of work is McRae et al. (2001, 2005), who have studied priming of verbs from nouns. They found that a noun engenders priming of verbs for which it is a typical agent, patient, instrument, or location. In theoretical linguistics, the idea of event knowledge being encoded in the lexical entries of nouns has been formulated in the context of Pustejovsky’s generative lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide information about the typical use of an object (book: read) and its construction (book: write), 753 Computational Linguistics Volume 36, Number 4 respectively. Pustejovsky uses this knowledge to account, for example, for the interpretation of logical metonymy (begin a book). Although qualia roles are instantiated with individual predicates rather than characterizations of all possible events, construction and use are arguably two very salient events for an object. Through the data exploration in Section 7.2, we hope to contribute to a linguis</context>
<context position="112702" citStr="Pustejovsky 1995" startWordPosition="17799" endWordPosition="17800"> improve our SYN PRIMARY setting models by clustering the seen headwords, and then computing plausibility of new headwords relative to the nearest cluster. A second question is the usefulness of inverse selectional preferences for the acquisition of fine-grained information about nouns. As we discussed in Section 7, the preferred verbs for a noun can often be grouped into meaning clusters. In future work, we plan to investigate whether there are groups of predicates that recur across similar nouns, and how they can be characterized. We expect some groups to correspond to Pustejovsky’s qualia (Pustejovsky 1995), which constitute particularly salient events for an object, namely, their creation and typical use. However, we expect corpus data to yield a more complex picture of the events connected to a noun, which manifest themselves in the form of additional, more specific meaning clusters. 759 Computational Linguistics Volume 36, Number 4 Acknowledgments We would like to thank Detlef Prescher and Carsten Brockmann for their implementations of the ROOTH ET AL. and RESNIK models, respectively. We are also grateful to Nate Chambers and Yves Peirsman, as well as to the anonymous reviewers, for their com</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>Pustejovsky, James. 1995. The Generative Lexicon. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="3112" citStr="Resnik 1996" startWordPosition="423" endWordPosition="424">abeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences for predicates and their argument positions. Given the immense effort necessary for manual semantic lexicon building and its associated reliability problems (see, e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences acquire selectional preferences automatically from large corpora. The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument headword (or fille</context>
<context position="4958" citStr="Resnik (1996)" startWordPosition="710" endWordPosition="711">a generalization step that infers a degree of preference for new, unseen headwords for a given predicate and role.1 The result is, in the ideal case, an assignment to every possible headword of some degree of compatibility (or plausibility) with the predicate’s preferences. In the case of anglicize, the desired result would be a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, first introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in t</context>
<context position="12575" citStr="Resnik (1996)" startWordPosition="1852" endWordPosition="1853">al. His proposal for a semantic interpretation mechanism still uses semantic primitives, but always produces a single most plausible interpretation by choosing the senses of each word that maximize the compatibility between selectional preferences and semantic types. In this manner, he is able to compute semantic representations for sentences that violate selectional restrictions, including metaphors such as “my car drinks gasoline.” 2.2 Semantic Hierarchy–Based Models The first broad-coverage computational model of selectional preferences, and still one of the best-known ones, namely that of Resnik (1996), belongs to the class of semantic hierarchy–based models. These models generalize over observed headwords using a semantic hierarchy or ontology for nouns. The two main advantages of such models are that (a) they can make predictions for all words covered by the hierarchy, even for very infrequent ones for which distributional representations tend to be unreliable; and (b) the hierarchy robustly guides generalization even for few observed headwords. Resnik’s model instantiates the set of relations Roles with grammatical functions which can be observed in syntactically analyzed corpora. More s</context>
<context position="36883" citStr="Resnik (1996)" startWordPosition="5549" endWordPosition="5550">stigate the potential of this setup in our Experiments 1 and 2. As a distributional model, EPP avoids the two pitfalls of resource-based models. One is a coverage problem due to the limited size of the resource (see the task-based evaluation in Gildea and Jurafsky [2002]). For example, the semantic role–based PADO ET AL. model resorts to class-based smoothing methods to improve coverage, which EPP does not need. The other problem of resource-based models is that the shape of the WordNet hierarchy determines the generalizations that the models make. These are not always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives a high preference because tragedy in WordNet is a type of written communication, which is a preferred argument class of answer. The ROOTH ET AL. model (Rooth et al. 1999) shares the resource independence of EPP, but has complementary benefits and problems. Querying the probabilistic ROOTH ET AL. model takes only constant time, whereas querying the exemplar-based EPP model takes time linear in the number of seen arguments for the argument position. However, the ROOTH ET AL. model requires a dedicated training phase with a space complexity linear in</context>
<context position="94657" citStr="Resnik (1996)" startWordPosition="14987" endWordPosition="14988">e, for the interpretation of logical metonymy (begin a book). Although qualia roles are instantiated with individual predicates rather than characterizations of all possible events, construction and use are arguably two very salient events for an object. Through the data exploration in Section 7.2, we hope to contribute to a linguistic characterization of inverse selectional preferences. 7.2 Empirical Analysis of Inverse Selectional Preferences The first question we ask concerns the selectional preference strength of regular and inverse selectional preferences, using the measure introduced by Resnik (1996) to determine the degree to which verbs select for nouns, and vice versa. As verb–role pairs, we re-use the same 100 pairs that were used for the pseudo-disambiguation task in Experiment 1. For the comparison, we randomly sample a total of 100 noun/inverserole pairs from the BNC, using the same five frequency bands as for the verbs (50– 100, 100–200, 200–500, 500–1,000, &gt;1,000). The sample contains approximately the same number of (inverse) subject and object roles. We adapt the selectional preference strength measure from Equation (1) to our case: Unlike Resnik, we compute KL divergence not o</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Resnik, Philip. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>College Park, MA.</location>
<contexts>
<context position="5767" citStr="Rooth et al. 1999" startWordPosition="837" endWordPosition="840">eference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identified with reasonable accuracy. Its key point is to use vector space similarity (Lund and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen 1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similar predicates. 724 Erk, Pad6, and Pad6 A Flex</context>
<context position="8632" citStr="Rooth et al. 1999" startWordPosition="1267" endWordPosition="1270">ur model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional models on both tasks. Finally, we investigate inverse preferences, that is, preferences that arguments have for their predicates. Although there is ample cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was first introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Pad6, Pad6, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse preferences, and we introd</context>
<context position="16571" citStr="Rooth et al. (1999)" startWordPosition="2448" endWordPosition="2451">se of any lexicon resource for the generalization step. Instead, they use word co-occurrence—typically obtained from the same corpus as the observed headwords—for generalization. This independence from manually constructed resources gives distributional models a good cost–benefit ratio and makes them especially attractive for domain-specific applications. These models, like the 727 Computational Linguistics Volume 36, Number 4 semantic hierarchy–based models, usually use grammatical functions as the set Roles for which selectional preferences are predicted. Pereira, Tishby, and Lee (1993) and Rooth et al. (1999) generalize by discovering latent classes of noun–verb pairs with soft clustering. They model the probability of a word a as the argument of a predicate v as the probability of generating v and a independently from the latent classes c: P(v,a) _ � �P(c,v,a) _ P(c)P(v|c)P(a|c) (3) cEC cEC Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P(c), P(v|c), and P(a|c). Their procedure supports hierarchical clustering and can optimize the number of clusters. Rooth et al. (1999) present a simpler Expectation Maximization– based estimation procedure which takes the number of </context>
<context position="37100" citStr="Rooth et al. 1999" startWordPosition="5583" endWordPosition="5586">see the task-based evaluation in Gildea and Jurafsky [2002]). For example, the semantic role–based PADO ET AL. model resorts to class-based smoothing methods to improve coverage, which EPP does not need. The other problem of resource-based models is that the shape of the WordNet hierarchy determines the generalizations that the models make. These are not always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives a high preference because tragedy in WordNet is a type of written communication, which is a preferred argument class of answer. The ROOTH ET AL. model (Rooth et al. 1999) shares the resource independence of EPP, but has complementary benefits and problems. Querying the probabilistic ROOTH ET AL. model takes only constant time, whereas querying the exemplar-based EPP model takes time linear in the number of seen arguments for the argument position. However, the ROOTH ET AL. model requires a dedicated training phase with a space complexity linear in the total number of verbs and nouns, which can lead to practical problems for large corpora (cf. Section 5.1). The separation of similarity computation and headword observation in EPP also gives the experimenter more</context>
<context position="40919" citStr="Rooth et al. 1999" startWordPosition="6183" endWordPosition="6186">. Among the distributional models, we choose ROOTH ET AL. as a model that performs soft clustering and thus shows a marked difference to the EPP model. To our knowledge, this is the first comparison of all three generalization paradigms: semantic hierarchy–based, distributional, and semantic role–based.7 As mentioned earlier, we employ two tasks to evaluate the four models: pseudodisambiguation and the prediction of human plausibility ratings. The pseudodisambiguation task (Yarowsky 1993) has become a standard evaluation measure for selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al. 1999). Given a choice of two potential headwords, the task of a selectional preference model is to pick the more plausible one to fill a particular argument position of a given predicate. Pseudo-disambiguation can be viewed as a word sense disambiguation task in which the two potential headwords together form a “pseudo-word,” for example herb/struggle from the original words herb and struggle. The task is to “disambiguate” the pseudoword to the word that fits better in the given context. It can also be viewed as an in vitro version of semantic role labeling and dependency parsing (depending on whet</context>
<context position="49338" citStr="Rooth et al. (1999)" startWordPosition="7520" endWordPosition="7523"> (v, r, a) equally from six frequency bands of arguments a: 1–50 occurrences; 50–100; 100–200; 200–500; 500-1,000; and more than 1,000 occurrences. These evaluation samples contain a total of 213,929 (SYN) and 65,902 (SEM) tuples. Next, we pair each headword with a confounder sampled from the primary corpus as described in Figure 2.8 In the literature, there have been two different approaches to choosing confounders for pseudo-disambiguation tasks: The first approach, used by Dagan, Lee, and Pereira (1999), chooses confounders to match the headword a in frequency. The second approach, used in Rooth et al. (1999), sets the probability that a word is drawn as a confounder to its relative frequency. The advantage and disadvantage of the first approach is that it largely eliminates the frequency bias that is a general problem of vector space-based approaches. This is an advantage in that it allows the generalization achieved by the model to be evaluated without any distortion from frequency bias. It is a disadvantage in that in any practical application making use of selectional preferences, the data will not be frequency-balanced. For example, selectional preferences could be used by a dependency parser</context>
<context position="92064" citStr="Rooth et al. 1999" startWordPosition="14603" endWordPosition="14606">analysis of inverse selectional preferences, which shows that inverse selectional preferences show semantically coherent patterns like regular selectional preferences, but that, in contrast to most verbs, nouns tend to occur with multiple semantic groups of verbs. In Sections 7.3–7.5, we test the EPP model on a pseudodisambiguation task for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Pad6 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word</context>
<context position="109083" citStr="Rooth et al. (1999)" startWordPosition="17239" endWordPosition="17242">1 Dice 42.4 98.8 43.4 98.7 Original nGCM 33.7 99.3 31.5 99.3 2,000 dimensions Hindle 48.8 96.0 52.2 94.6 Jaccard 36.7 99.4 44.9 98.7 Lin 32.8 98.9 27.2 98.9 Cosine 35.2 99.0 31.3 99.4 Dice 35.0 99.6 32.9 99.8 PCA nGCM 32.4 99.2 30.3 99.6 500 dimensions Hindle 44.2 99.0 48.7 99.1 Jaccard 34.8 99.6 32.6 99.8 Lin 30.6 99.8 28.8 99.8 758 Erk, Pad6, and Pad6 A Flexible, Corpus-Driven Model of Selectional Preferences of the intuition that plausibility judgments should generalize to fillers with similar meaning. Our model is simple and easy to compute. In common with other distributional models like Rooth et al. (1999), it does not depend on lexical resources. Our model derives additional flexibility from distinguishing between a primary corpus (for observing headwords) and a generalization corpus (for inducing semantic similarities). This allows it to use primary corpora with deeper semantic annotation that are too small as a basis for computing vector space representations. We have evaluated the EPP model on two tasks, a pseudo-disambiguation task that can be viewed as an abstraction of both word sense disambiguation and semantic role labeling, as well as on the prediction of human plausibility judgments.</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 104–111, College Park, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector-space model for information retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>18--613</pages>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Salton, Gerard, Anita Wong, and Chung-Shu Yang. 1975. A vector-space model for information retrieval. Journal of the American Society for Information Science, 18:613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schulte im Walde</author>
<author>Sabine</author>
</authors>
<title>Comparing computational approaches to selectional preferences: Second-order co-occurrence vs. latent semantic clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1381--1388</pages>
<marker>Walde, Sabine, 2010</marker>
<rawString>Schulte im Walde, Sabine. 2010. Comparing computational approaches to selectional preferences: Second-order co-occurrence vs. latent semantic clusters. In Proceedings of the 7th International Conference on Language Resources and Evaluation, pages 1381–1388, Valleta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schulte im Walde</author>
<author>Christian Hying Sabine</author>
<author>Christian Scheible</author>
<author>Helmut Schmid</author>
</authors>
<title>Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>496--504</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="18515" citStr="Walde et al. (2008)" startWordPosition="2748" endWordPosition="2751">is weighted by sim(w1,w). The similarity sim(w1,w) is computed on vector space representations. Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative approach to the prediction of selectional preferences. The features they use are mainly cooccurrence statistics, enriched with morphological context features to alleviate sparse data problems for low-frequency argument heads. They train one SVM per verb– argument position pair, using unobserved verb–argument combinations as negative examples, which makes their approach independent of manually annotated training data. Schulte im Walde et al. (2008) present a model that combines features of the semantic hierarchy–based and the distributional approaches by integrating WordNet into an EM-based clustering model; Schulte im Walde (2010) shows that integrating noun–modifier relations improves the prediction of human plausibility judgments. 2.4 Semantic Role–Based Models The third class of models takes advantage of semantic resources beyond simple semantic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the prediction of selectional preferences for semantic roles rather than grammatical functions. From a lingu</context>
</contexts>
<marker>Walde, Sabine, Scheible, Schmid, 2008</marker>
<rawString>Schulte im Walde, Sabine, Christian Hying, Christian Scheible, and Helmut Schmid. 2008. Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 496–504, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Shepard</author>
</authors>
<title>Towards a universal law of generalization for psychological science.</title>
<date>1987</date>
<journal>Science,</journal>
<volume>237</volume>
<issue>4820</issue>
<contexts>
<context position="32334" citStr="Shepard 1987" startWordPosition="4846" endWordPosition="4847">uence of overall target frequency, but 3 We do not remove stop words for reasons of simplicity, as there is no unequivocal definition of this set, and we do not wish to remove potentially informative contexts. 4 This context specification is available as soonly in the DependencyVectors software package (http://www.nlpado.de/∼sebastian/dv.html) starting from Release 2.5. = i simDice(a,a) 2·|BE(a)∩BE(a&apos;)| | | | | BE(a) + BE(a&apos;) else 732 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences preserves the central idea that similarity decreases exponentially with distance (Shepard 1987). All similarity measures from Table 2 are applicable to semantic spaces with arbitrary basis elements, with the exception of the Lin measure, whose definition applies only to dependency-based spaces. The reason is that it decomposes the basis elements into relation–word pairs (r,v). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood </context>
</contexts>
<marker>Shepard, 1987</marker>
<rawString>Shepard, Roger. 1987. Towards a universal law of generalization for psychological science. Science, 237(4820):1317–1323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="53987" citStr="Stolcke 2002" startWordPosition="8350" endWordPosition="8351"> v are already instantiated by verb classes, namely, FrameNet frames (see Table 3). Finally, we report three baselines. The first baseline, headword frequency (HW), is very simple. It decides between the headword a and the confounder a&apos; by comparing the frequencies f (a) and f (a&apos;). The second, more informed, baseline is triple frequency (TRIPLE). It votes for a if f (v, r, a) &gt; f (v, r, a&apos;), and vice versa. The third baseline, a bigram language model (LM), was constructed by training a 2-gram language model from the large English ukWAC Web corpus (Baroni et al. 2009) using the SRILM toolkit (Stolcke 2002) with default Good–Turing smoothing. We retained only verbs, nouns, adjectives, and adverbs in order to maximize the proximity between verbs and their subjects and objects. We defined the preference score for verb–subject triples as the probability of the sequence av, that is, Pref(v, subj, a) = P(vIa). Conversely, the preference score for verb– object triples was defined as the probability of the sequence va, that is, Pref(v,obj,a) = P(a|v). Again, the model compares Pref (v, r, a) and Pref (v, r, a&apos;) to make its decision. Evaluation. For all models, we report two evaluation figures. One is c</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM—an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christoper D Manning</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Stochastic HPSG parse selection using the Redwoods corpus.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="1857" citStr="Toutanova et al. 2005" startWordPosition="260" endWordPosition="263">ence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items. 1. Introduction Selectional preferences or selectional constraints describe knowledge about possible and plausible fillers for a predicate’s argument positions. They model the fact that there is often a semantically coherent set of concepts that can fill a given argument position. Selectional preferences can help for many text analysis tasks which involve comparing different attachment decisions. Examples include syntactic disambiguation (Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD, * Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ** E-mail: pado@cl.uni-heidelberg.de. † E-mail:ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 </context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>Toutanova, Kristina, Christoper D. Manning, Dan Flickinger, and Stephan Oepen. 2005. Stochastic HPSG parse selection using the Redwoods corpus. Journal of Research on Language and Computation, 3(1):83–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Trueswell</author>
<author>Michael Tanenhaus</author>
<author>Susan Garnsey</author>
</authors>
<title>Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution.</title>
<date>1994</date>
<journal>Journal of Memory and Language,</journal>
<pages>33--285</pages>
<marker>Trueswell, Tanenhaus, Garnsey, 1994</marker>
<rawString>Trueswell, John, Michael Tanenhaus, and Susan Garnsey. 1994. Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution. Journal of Memory and Language, 33:285–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bram Vandekerckhove</author>
<author>Dominiek Sandra</author>
<author>Walter Daelemans</author>
</authors>
<title>A robust and extensible exemplar-based model of thematic fit.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Meeting of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>826--834</pages>
<location>Athens.</location>
<marker>Vandekerckhove, Sandra, Daelemans, 2009</marker>
<rawString>Vandekerckhove, Bram, Dominiek Sandra, and Walter Daelemans. 2009. A robust and extensible exemplar-based model of thematic fit. In Proceedings of the 12th Meeting of the European Chapter of the Association for Computational Linguistics, pages 826–834, Athens.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yorick 1975 Wilks</author>
</authors>
<title>Preference semantics.</title>
<pages>329--350</pages>
<editor>In E. Keenan, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<marker>Wilks, </marker>
<rawString>Wilks, Yorick.1975. Preference semantics. In E. Keenan, editor, Formal Semantics of Natural Language. Cambridge University Press, Cambridge, UK, pages 329–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>266--271</pages>
<location>Princeton, NJ.</location>
<contexts>
<context position="7572" citStr="Yarowsky 1993" startWordPosition="1112" endWordPosition="1113">l to primary corpora with rich information that are too small for efficient generalization, such as domain-specific corpora or corpora with deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization corpus is available. We empirically demonstrate the benefit of this distinction. We use FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as generalization corpus, modeling selectional preferences for semantic roles with nearperfect coverage and low error rate.2 We evaluate our model on two tasks. The first task is pseudo-disambiguation (Yarowsky 1993), where the model decides which of two randomly chosen words is a better filler for the given argument position. This task tests model properties that are needed for concrete semantic analysis tasks, most notably word sense disambiguation, but also for semantic role labeling. The second task is the prediction of human plausibility ratings, which is a standard task-independent benchmark for the quality of selectional preferences. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributiona</context>
<context position="40794" citStr="Yarowsky 1993" startWordPosition="6166" endWordPosition="6167">systematically outperforms the others, and the RESNIK model shows the most consistent behavior across different scenarios. Among the distributional models, we choose ROOTH ET AL. as a model that performs soft clustering and thus shows a marked difference to the EPP model. To our knowledge, this is the first comparison of all three generalization paradigms: semantic hierarchy–based, distributional, and semantic role–based.7 As mentioned earlier, we employ two tasks to evaluate the four models: pseudodisambiguation and the prediction of human plausibility ratings. The pseudodisambiguation task (Yarowsky 1993) has become a standard evaluation measure for selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al. 1999). Given a choice of two potential headwords, the task of a selectional preference model is to pick the more plausible one to fill a particular argument position of a given predicate. Pseudo-disambiguation can be viewed as a word sense disambiguation task in which the two potential headwords together form a “pseudo-word,” for example herb/struggle from the original words herb and struggle. The task is to “disambiguate” the pseudoword to the word that fits better in the gi</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, David. 1993. One sense per collocation. In Proceedings of the ARPA Human Language Technology Workshop, pages 266–271, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>849--856</pages>
<location>Sydney.</location>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Zanzotto, Fabio Massimo, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In Proceedings of the Joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 849–856, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beñat Zapirain</author>
<author>Eneko Agirre</author>
<author>Lluís Màrquez</author>
</authors>
<title>Generalizing over lexical features: Selectional preferences for semantic role classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>73--76</pages>
<marker>Zapirain, Agirre, Màrquez, 2009</marker>
<rawString>Zapirain, Beñat, Eneko Agirre, and Lluís Màrquez. 2009. Generalizing over lexical features: Selectional preferences for semantic role classification. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, pages 73–76, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>