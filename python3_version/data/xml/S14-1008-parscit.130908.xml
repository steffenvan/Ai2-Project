<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008101">
<title confidence="0.9832">
Learning the Peculiar Value of Actions
</title>
<author confidence="0.846452">
Daniel Dahlmeier
</author>
<affiliation confidence="0.511015">
Research &amp; Innovation, SAP Asia, Singapore
</affiliation>
<email confidence="0.942748">
d.dahlmeier@sap.com
</email>
<sectionHeader confidence="0.996833" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867727272727">
We consider the task of automatically es-
timating the value of human actions. We
cast the problem as a supervised learning-
to-rank problem between pairs of action
descriptions. We present a large, novel
data set for this task which consists of
challenges from the I Will If You Will
Earth Hour challenge. We show that an
SVM ranking model with simple linguistic
features can accurately predict the relative
value of actions.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977181818182">
The question on how humans conceptualize value
is of great interest to researchers in various fields,
including linguistics (Jackendoff, 2006). The link
between value and language arises from the fact
that we cannot directly observe value due to its ab-
stract nature and instead often study language ex-
pressions that describe actions which have some
value attached to them. This creates an interesting
link between the semantics of the words that de-
scribe the actions and the underlying moral value
of the actions.
Jackendoff (2006) describes value as an “inter-
nal accounting system” for ethical decision pro-
cesses that exhibits both valence (good or bad)
and magnitude (better or worse). Most interest-
ingly, value is governed by a “peculiar logic” that
provides constraints on which actions are deemed
morally acceptable and which are not. In par-
ticular, the principal of reciprocity states that the
valence and magnitude of reciprocal actions (ac-
tions that are done “in return” for something else)
should match, i.e., positive valued actions should
</bodyText>
<footnote confidence="0.908308">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999509964285714">
match with positive valued reciprocal actions (re-
actions) of similar magnitude, and conversely neg-
atively valued actions should match with nega-
tive valued reciprocal actions (reactions) of similar
magnitude.
In this paper, we consider the task of automati-
cally estimating the value of actions. We present a
simple and effective method for learning the value
of actions from ranked pairs of textual action de-
scriptions based on a statistical learning-to-rank
approach. Our experiments are based on a novel
data set that we create from challenges submit-
ted to the I Will if You Will Earth Hour challenge
where participants pledge to do something daring
or challenging if other people commit to sustain-
able actions for the planet. Our method achieves
a surprisingly high accuracy of up to 94.72% in a
10-fold cross-validation experiment. The results
show that the value of actions can accurately be
estimated by machine learning methods based on
lexical descriptions of the actions.
The main contribution of this paper is that we
show how the semantics of value in language can
accurately be learned from empirical data using a
learning-to-rank approach. Our work shows an in-
teresting link between empirical research on se-
mantics in natural language processing and the
concept of value.
</bodyText>
<sectionHeader confidence="0.944884" genericHeader="method">
2 The Logic of Value
</sectionHeader>
<bodyText confidence="0.999972">
Our approach is based on the concept of value
as presented by Jackendoff (2006) who describes
value as an abstract property that is attributed to
objects, persons, and actions. He further describes
logical inference rules that humans use to deter-
mine which actions are deemed morally accept-
able and which are not. The most important in-
ference rule for our work is the principal of recip-
rocation, things that are done “in return” for some
other action (Fiengo and Lasnik, 1973). In En-
glish, this relation is often expressed by the prepo-
</bodyText>
<page confidence="0.994622">
63
</page>
<note confidence="0.753486">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 63–68,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.935914">
sition for, as shown by the following example sen-
tences (Jackendoff, 2006).
</bodyText>
<listItem confidence="0.999543">
1. Susan praised Sam for behaving nicely.
2. Fred cooked Lois dinner for fixing his com-
puter.
3. Susan insulted Sam for behaving badly.
4. Lois slashed Fred’s tires for insulting her.
</listItem>
<bodyText confidence="0.999954083333333">
The first two examples describe actions with pos-
itive value, while the last two examples describe
actions with negative value. We expect that the
valence values of reciprocal actions match: posi-
tively valued actions demand a positively valued
action in return, while negatively valued actions
trigger negatively valued responses. If we switch
the example sentences and match positive actions
with negative actions, we get sentences that sound
counter-intuitive or perhaps comical (we prefix
counter-intuitive sentences with a hash character
’#’).
</bodyText>
<listItem confidence="0.996671666666667">
1. #Susan insulted Sam for behaving nicely.
2. #Lois slashed Fred’s tires for fixing her com-
puter.
</listItem>
<bodyText confidence="0.999710333333333">
Similarly, we expect that the magnitudes of value
between reciprocal actions match. Sentences
where the magnitude of the value of the response
action does not match the magnitude of the initial
action seem odd or socially inappropriate (over-
acting/underacting).
</bodyText>
<listItem confidence="0.997194625">
1. #Fred cooked Lois dinner for saying hello to
him.
2. #Fred cooked Lois dinner for rescuing all his
relatives from certain death.
3. #Fred slashed Lois’s tires for eating too little
at dinner.
4. #Fred slashed Lois’s tires for murdering his
entire family.
</listItem>
<bodyText confidence="0.997510166666667">
We observe that reciprocal actions typically match
each other in valence and magnitude. Coming
back to our initial goal of learning the value of
actions, this gives us a method for comparing the
value of actions that were done in return to the
same initial action.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="method">
3 I Will If You Will challenge
</sectionHeader>
<bodyText confidence="0.9881505">
The I Will If You Will (IWIYW) challenge1 is part
of the World Wildlife Fund’s Earth Hour campaign
</bodyText>
<footnote confidence="0.532288">
1www.earthhour.org/i-will-if-you-will
</footnote>
<bodyText confidence="0.918566761904762">
I will quit smoking if you will start recycling.
(500 people)
I will adopt a panda if you will start recycling.
(1000 people)
I will dance gangnam style if you will plant
a tree. (100 people)
I will dye my hair red if you will upload an
IWIYW challenge. (500 people)
I will learn Java if you will upload an IWIYW
challenge. (10,000 people)
Table 1: Examples of I Will If You Will chal-
lenges.
which has the goal to increase awareness of sus-
tainability issues. In this challenge, participants
make a pledge to do something daring or challeng-
ing if a certain number of people commit to sus-
tainable actions for the planet. The challenges are
created by ordinary people on the Earth Hour cam-
paign website. Each challenge takes the form of a
simple school yard dare: I will do X, if you will do
Y, where X is typically some daring or challenging
task that the challenge creator commits to do if a
sufficient number of people commit to do action
Y which is some sustainable action for the planet.
Together with the textual description, each chal-
lenge includes the number of people that need to
commit to doing Y in order for the challenge cre-
ator to perform X. Examples of the challenges are
shown in Table 1.
It is important to note that during the challenge
creation on the IWIYW website, the X challenge
is a free text input field that allows the author to
come up with creative and interesting challenges.
The sustainable actions Y and the number of peo-
ple that need to commit to it are usually chosen
from a fixed list of choices. As a result, there is
a large number of different X actions and a com-
parably smaller number of Y actions. The col-
lected challenges provide a unique data set that al-
lows us to quantitatively measure the value of each
promised task by the number of people that need
to fulfill the sustainable action.
</bodyText>
<sectionHeader confidence="0.989597" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.999935">
In this section, we present our approach for esti-
mating the value of actions. Our approach casts
the problem as a supervised learning-to-rank prob-
lem between pairs of actions. Given, a textual de-
scription of an action a, we want to estimate its
</bodyText>
<page confidence="0.995914">
64
</page>
<bodyText confidence="0.9674565">
to create the following ranked challenge pairs.
I will quit smoking &lt; I will adopt a panda
I will dye my hair red &lt; I will learn Java (3)
value magnitude v. We represent the action a via a
set of features that are extracted from the descrip-
tion of the action. We use a linear model that com-
bines the features into a single scalar value for the
value v
</bodyText>
<equation confidence="0.74918">
v = wT xa, (1)
</equation>
<bodyText confidence="0.999974733333333">
where xa is the feature vector for action descrip-
tion a and w is a learned weight vector. The goal
is to learn a suitable weight vector w that approxi-
mates the true relationship between textual expres-
sions of actions and their magnitude of value.
Instead of estimating the value directly, we take
an alternative approach and consider the task of
learning the relative ranking of pairs of actions.
We follow the pairwise approach to ranking (Her-
brich et al., 1999; Cao et al., 2007) that reduces
ranking to a binary classification problem. Rank-
ing the values v1 and v2 of two actions a1 and a2 is
equivalent to determining the sign of the dot prod-
uct between the weight vector w and the difference
between the feature vectors xa1 and xaa.
</bodyText>
<equation confidence="0.993894666666667">
v1 &gt; v2 wT xa1 &gt; wT xaa
� wT xa1 − wT xaa &gt; 0
�? wT (xa1 − xaa) &gt; 0 (2)
</equation>
<bodyText confidence="0.999918564102564">
For each ranking pair of actions, we create two
complimentary classification instances: (xa1 −
xaa, l1) and (xaa − xa1, l2), where the labels are
l1 = +1, l2 = −1 if the first challenge has higher
value than the second challenge and l1 = −1, l2 =
+1 otherwise. We can train a standard linear clas-
sifier on the generated training instances to learn
the weight vector w.
In the case of the IWIYW data, there is no ex-
plicit ranking between actions. However, we are
able to create ranking pairs for the IWIYW data
in the following way. As we have seen, there is
only a small set of different You Will challenges
that are reciprocal actions for a diverse set of I
Will challenges. Thus, many I Will challenges will
end up having the same You Will challenge. We
can use the You Will challenges as a pivot to ef-
fectively “join” the I Will challenges. The number
of required people to perform Y induces a natu-
ral ordering between the values of the I Will ac-
tions where a higher number of required partici-
pants means that the I Will task has higher value.
For example, for the challenges displayed in Ta-
ble 1, we can use the common You Will challenges
According to the examples, adopting a panda has
higher value than quitting smoking and learning
Java has higher value than dying ones hair red.
The third challenge does not share a common You
Will challenge with any other challenge and there-
fore no ranking pairs can be formed with it.
As the IWIYW challenges are created online in
a non-controlled environment, we have to expect
that there is some noise in the automatically cre-
ated ranked challenges. However, a robust learn-
ing algorithm has to be able to handle a certain
amount of noise. We note that our method is not
limited to the IWIYW data set but can be applied
to any data set of actions where relative rankings
are provided or can be induced.
</bodyText>
<subsectionHeader confidence="0.768917">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999374785714286">
The choice of appropriate feature representations
is crucial to the success of any machine learning
method. We start by parsing each I Will If You
Will challenge with a constituency parser. Be-
cause each challenge has the same I Will If You
Will structure, it is easy to identify the subtrees that
correspond to the I Will and You Will parts of the
challenge. An example parse tree of a challenge
is shown in Figure 1. The yield of the You Will
subtree serves as a pivot to join different I Will
challenges. To represent the I Will action a as a
feature vector xa, we extract the following lexical
and syntax features from the I Will subtree of the
sentence.
</bodyText>
<listItem confidence="0.979424333333333">
• Verb: We extract the verb of the I Will clause
as a feature. To identify the verb, we pick
the left-most verb of the I Will subtree based
on its part-of-speech (POS) tag. We extract
the lowercased word token as a feature. For
example, for the sentence in Figure 1, the
verb feature is verb=quit. If the verb is
negated (the left sibling of the I Will sub-
tree spans exactly the word not), we add the
postfix NOT to the verb feature, for example
verb=quit NOT.
• Object: We take the right sibling of the I
will verb as the object of the action. If the
right sibling is a particle with constituent la-
bel PRT, e.g., travel around the UK on bike,
</listItem>
<page confidence="0.99938">
65
</page>
<figureCaption confidence="0.9981775">
Figure 1: Parse tree of a I Will If You Will challenge. The subtrees governing the I Will and You Will part
of the sentence are marked.
</figureCaption>
<figure confidence="0.997719882352941">
smoking
IN
S
if
S
you will commit to recycling.
quit
SBARY ou Will
NP
PRP
I
MD
will
VP
VPI will
NP
VB
</figure>
<bodyText confidence="0.9996086">
we skip the particle and take the next sib-
ling as the object. If the object is a prepo-
sitional phrase with constituent tag PP, e.g.,
go without electricity for a month, we take
the second child of the prepositional phrase
as the object phrase. We then extract two fea-
tures to represent the object. First, we extract
the lowercased head word of the object as a
feature. Second, we extract the concatena-
tion of all the words in the yield of the object
node as a single feature to capture the com-
plete argument for longer objects. In our ex-
ample sentence, the object head feature and
the complete object feature are identical: ob-
ject head=smoking and object=smoking.
</bodyText>
<listItem confidence="0.984786363636364">
• Unigram: We take all lowercased words that
are not stopwords in the I Will part of the
sentence as binary features. In our example
sentence, the unigram features unigr quit and
unigr smoking would be active.
• Bigram: We take all lowercased bigrams in
the I Will part of the sentence as binary fea-
tures. We do not remove stopwords for bi-
gram features. In our example sentence, the
bigram features bigr quit smoking would be
active.
</listItem>
<bodyText confidence="0.9997422">
We note that our method is not restricted to these
feature templates. More sophisticated features,
like tree kernels (Collins and Duffy, 2002) or se-
mantic role labeling (Palmer et al., 2010), can be
imagined.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999942">
We evaluate our approach using standard 10-fold
cross-validation and report macro-average accu-
racy scores for each of the feature sets. The classi-
fier in all our experiments is a linear SVM imple-
mented in SVM-light (Joachims, 2006).
</bodyText>
<subsectionHeader confidence="0.98559">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9989754">
We obtained a snapshot of 18,290 challenges cre-
ated during the 2013 IWIYW challenge. The snap-
shot was taken in mid May 2013, just 1.5 weeks
before the 2013 Earth Hour event day. We per-
form the following pre-processing. We normal-
ize the text to proper UTF-8 encoding and remove
challenges where the complete sentence contained
less than 7 tokens. These challenges were usually
empty or incomplete. We filter the challenges us-
ing the langid.py tool (Lui and Baldwin, 2012)
and only keep English challenges. We normal-
ized the casing of the sentences by first lower-
casing all texts and then re-casing each sentence
with a simple re-casing model that replaces a word
with its most frequent casing form. The re-casing
model is trained on the Brown corpus (Ku and
Francis, 1967). We tokenize the sentences with
the Penn Treebank tokenizer. We parse the sen-
tences with the Stanford parser (Klein and Man-
ning, 2003a; Klein and Manning, 2003b) to ob-
</bodyText>
<page confidence="0.983181">
66
</page>
<table confidence="0.999348272727273">
Features Accuracy
random 0.5000
verb 0.6241
unigrams 0.8481
unigrams + verb 0.8573
object 0.8904
verb + object 0.9115
bigrams 0.9251
unigrams + bigrams 0.9343
unigrams + bigrams + verb 0.9361
unigrams + bigrams + verb + object 0.9472
</table>
<tableCaption confidence="0.8989675">
Table 2: Results of 10-fold cross-validation exper-
iments.
</tableCaption>
<bodyText confidence="0.999155368421053">
tain a constituency parse tree for each challenge.
After pre-processing, we are left with 5,499 chal-
lenges (4,982 unique), with 4,474 unique I Will
challenges and 70 unique You Will challenges.
We create binary classifications examples be-
tween pairs of actions as described in Section 4.
As we create all possible combinations between I
Will challenges with common You Will challenges,
the number of ranking pairs for training is large.
In our case, we ended up with over 840,000 classi-
fication instances. We note that not every I Will ac-
tion is guaranteed to be included in the final set of
ranking pairs as challenges with a unique You Will
part that is not found in any other challenge cannot
be joined and are effectively ignored. However,
this is not a problem for our experiments. The bi-
nary classification instances are used to train and
test a ranking model for estimating the value of
actions as described in the last section.
</bodyText>
<subsectionHeader confidence="0.628388">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999968653846154">
The results of our cross-validation experiments are
shown in Table 2.
The random baseline for all experiments is 50%.
Just using the verb of the I Will action as a fea-
ture improves over the random baseline to 62.41%.
Using a unigram bag-of-words representation of
the actions achieves a very respectable score of
84.81%. When we combine unigrams with the
verb feature, we achieve 85.73%. One of the most
surprising results of our experiments is that the
object of the action alone is a very effective fea-
ture, achieving 89.04%. When combined with the
verb feature, the object feature achieves 91.15%
which shows that the verb and object carry most
of the relevant information that the model requires
to gauge the value of actions. Using bigrams as
features, seems to catch this information just as ac-
curately, achieving 92.51% accuracy. The score is
further improved by combining the different fea-
ture sets. The best result of 94.72% is obtained
by combining all the features: unigrams, bigrams,
verb, and object. In summary, these results show
that our method is able to accurately predict the
relative value of actions using simple linguistic
features, which is the main contribution of this
work.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999973">
The concept of value and reciprocity has been
extensively studied in the social sciences (Ger-
gen and Greenberg, 1980), anthropology (Sahlins,
1972), economics (Fehr and G¨achter, 2000), and
philosophy (Becker, 1990). In linguistics, value
has been studied by Jackendoff (2006). His work
forms the starting point of our approach.
In natural language processing, there has been
very little work on the concept of value. Paul et al.
(2009) and Girju and Paul (2011) address the prob-
lem of semi-automatically mining patterns that en-
code reciprocal relationships using pronoun tem-
plates. Their work focuses on mining patterns of
reciprocity while our work uses expressions of re-
ciprocal actions to learn the value of actions.
None of the above works tries to estimate the
value of actions, as we do in this work. In fact, we
are not aware of any other work that tries to esti-
mate the value of actions from lexical expressions
of value.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999897571428571">
We have presented a simple and effective method
for learning the value of actions from reciprocal
sentences. We show that our SVM-based ranking
model with simple linguistic features is able to ac-
curately rank pairs of actions from the I Will If
You Will Earth Hour challenge, achieving an ac-
curacy of up to 94.72%.
</bodyText>
<sectionHeader confidence="0.972757" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.997579833333333">
We thank Sid Das from Earth Hour for shar-
ing the IWIYW data with us. We thank Marek
Kowalkiewicz for helpful discussions. The re-
search is partially funded by the Economic Devel-
opment Board and the National Research Founda-
tion of Singapore.
</bodyText>
<page confidence="0.999292">
67
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902183333333">
Lawrence C Becker, editor. 1990. Reciprocity. Uni-
versity of Chicago Press.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24th International Conference on Machine Learning
(ICML), pages 129–136.
Michael Collins and Nigel Duffy. 2002. Convolution
kernels for natural language. In Advances in Neu-
ral Information Processing Systems 14 (NIPS 2001),
pages 625–632.
Ernst Fehr and Simon G¨achter. 2000. Cooperation
and punishment in public goods experiments. pages
980–994.
Robert Fiengo and Howard Lasnik. 1973. The logical
structure of reciprocal sentences in English. Foun-
dations of language, pages 447–468.
Kenneth J. Gergen and Willis Richard H. Greenberg,
Martin S., editors. 1980. Social exchange: Ad-
vances in theory and research. Plenum Press.
Roxana Girju and Michael J Paul. 2011. Modeling
reciprocity in social interactions with probabilistic
latent space models. Natural Language Engineer-
ing, 17(1):1–36.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support vector learning for ordinal regres-
sion. In In Proceedings of the 1999 International
Conference on Articial Neural Networks, pages 97–
102.
Ray Jackendoff. 2006. The peculiar logic of value.
Journal of Cognition and Culture, 6(3-4):375–407.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 217–226.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), pages 423–430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 423–430.
Henry Ku and W. Nelson Francis. 1967. Computa-
tional Analysis of Present-Day American English.
Brown University Press.
Marco Lui and Timothy Baldwin. 2012. An off-the-
shelf language identification tool. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL 2012), pages 25–
30.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1–103.
Michael Paul, Roxana Girju, and Chen Li. 2009. Min-
ing the web for reciprocal relationships. In Proceed-
ings of the 13th Conference on Computational Nat-
ural Language Learning (CoNLL), pages 75–83.
Marshall D. Sahlins. 1972. Stone age economics.
Transaction Publishers.
</reference>
<page confidence="0.999444">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.991005">
<title confidence="0.999956">Learning the Peculiar Value of Actions</title>
<author confidence="0.999936">Daniel Dahlmeier</author>
<affiliation confidence="0.996572">Research &amp; Innovation, SAP Asia,</affiliation>
<email confidence="0.999838">d.dahlmeier@sap.com</email>
<abstract confidence="0.999542833333333">We consider the task of automatically estimating the value of human actions. We cast the problem as a supervised learningto-rank problem between pairs of action descriptions. We present a large, novel data set for this task which consists of challenges from the I Will If You Will Earth Hour challenge. We show that an SVM ranking model with simple linguistic features can accurately predict the relative value of actions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>1990</date>
<editor>Lawrence C Becker, editor.</editor>
<publisher>Reciprocity. University of Chicago Press.</publisher>
<marker>1990</marker>
<rawString>Lawrence C Becker, editor. 1990. Reciprocity. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhe Cao</author>
<author>Tao Qin</author>
<author>Tie-Yan Liu</author>
<author>Ming-Feng Tsai</author>
<author>Hang Li</author>
</authors>
<title>Learning to rank: from pairwise approach to listwise approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning (ICML),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="8607" citStr="Cao et al., 2007" startWordPosition="1442" endWordPosition="1445">e description of the action. We use a linear model that combines the features into a single scalar value for the value v v = wT xa, (1) where xa is the feature vector for action description a and w is a learned weight vector. The goal is to learn a suitable weight vector w that approximates the true relationship between textual expressions of actions and their magnitude of value. Instead of estimating the value directly, we take an alternative approach and consider the task of learning the relative ranking of pairs of actions. We follow the pairwise approach to ranking (Herbrich et al., 1999; Cao et al., 2007) that reduces ranking to a binary classification problem. Ranking the values v1 and v2 of two actions a1 and a2 is equivalent to determining the sign of the dot product between the weight vector w and the difference between the feature vectors xa1 and xaa. v1 &gt; v2 wT xa1 &gt; wT xaa � wT xa1 − wT xaa &gt; 0 �? wT (xa1 − xaa) &gt; 0 (2) For each ranking pair of actions, we create two complimentary classification instances: (xa1 − xaa, l1) and (xaa − xa1, l2), where the labels are l1 = +1, l2 = −1 if the first challenge has higher value than the second challenge and l1 = −1, l2 = +1 otherwise. We can tra</context>
</contexts>
<marker>Cao, Qin, Liu, Tsai, Li, 2007</marker>
<rawString>Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th International Conference on Machine Learning (ICML), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 14 (NIPS</booktitle>
<pages>625--632</pages>
<contexts>
<context position="13580" citStr="Collins and Duffy, 2002" startWordPosition="2374" endWordPosition="2377">ntical: object head=smoking and object=smoking. • Unigram: We take all lowercased words that are not stopwords in the I Will part of the sentence as binary features. In our example sentence, the unigram features unigr quit and unigr smoking would be active. • Bigram: We take all lowercased bigrams in the I Will part of the sentence as binary features. We do not remove stopwords for bigram features. In our example sentence, the bigram features bigr quit smoking would be active. We note that our method is not restricted to these feature templates. More sophisticated features, like tree kernels (Collins and Duffy, 2002) or semantic role labeling (Palmer et al., 2010), can be imagined. 5 Experiments We evaluate our approach using standard 10-fold cross-validation and report macro-average accuracy scores for each of the feature sets. The classifier in all our experiments is a linear SVM implemented in SVM-light (Joachims, 2006). 5.1 Data We obtained a snapshot of 18,290 challenges created during the 2013 IWIYW challenge. The snapshot was taken in mid May 2013, just 1.5 weeks before the 2013 Earth Hour event day. We perform the following pre-processing. We normalize the text to proper UTF-8 encoding and remove </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. Convolution kernels for natural language. In Advances in Neural Information Processing Systems 14 (NIPS 2001), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernst Fehr</author>
<author>Simon G¨achter</author>
</authors>
<title>Cooperation and punishment in public goods experiments.</title>
<date>2000</date>
<pages>980--994</pages>
<marker>Fehr, G¨achter, 2000</marker>
<rawString>Ernst Fehr and Simon G¨achter. 2000. Cooperation and punishment in public goods experiments. pages 980–994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fiengo</author>
<author>Howard Lasnik</author>
</authors>
<title>The logical structure of reciprocal sentences in English. Foundations of language,</title>
<date>1973</date>
<pages>447--468</pages>
<contexts>
<context position="3610" citStr="Fiengo and Lasnik, 1973" startWordPosition="570" endWordPosition="573">ork shows an interesting link between empirical research on semantics in natural language processing and the concept of value. 2 The Logic of Value Our approach is based on the concept of value as presented by Jackendoff (2006) who describes value as an abstract property that is attributed to objects, persons, and actions. He further describes logical inference rules that humans use to determine which actions are deemed morally acceptable and which are not. The most important inference rule for our work is the principal of reciprocation, things that are done “in return” for some other action (Fiengo and Lasnik, 1973). In English, this relation is often expressed by the prepo63 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 63–68, Dublin, Ireland, August 23-24 2014. sition for, as shown by the following example sentences (Jackendoff, 2006). 1. Susan praised Sam for behaving nicely. 2. Fred cooked Lois dinner for fixing his computer. 3. Susan insulted Sam for behaving badly. 4. Lois slashed Fred’s tires for insulting her. The first two examples describe actions with positive value, while the last two examples describe actions with negative value. We expec</context>
</contexts>
<marker>Fiengo, Lasnik, 1973</marker>
<rawString>Robert Fiengo and Howard Lasnik. 1973. The logical structure of reciprocal sentences in English. Foundations of language, pages 447–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth J Gergen</author>
<author>Willis Richard H</author>
</authors>
<date>1980</date>
<booktitle>Social exchange: Advances in theory and research.</booktitle>
<editor>Greenberg, Martin S., editors.</editor>
<publisher>Plenum Press.</publisher>
<marker>Gergen, H, 1980</marker>
<rawString>Kenneth J. Gergen and Willis Richard H. Greenberg, Martin S., editors. 1980. Social exchange: Advances in theory and research. Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Michael J Paul</author>
</authors>
<title>Modeling reciprocity in social interactions with probabilistic latent space models.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="17762" citStr="Girju and Paul (2011)" startWordPosition="3073" endWordPosition="3076">thod is able to accurately predict the relative value of actions using simple linguistic features, which is the main contribution of this work. 6 Related Work The concept of value and reciprocity has been extensively studied in the social sciences (Gergen and Greenberg, 1980), anthropology (Sahlins, 1972), economics (Fehr and G¨achter, 2000), and philosophy (Becker, 1990). In linguistics, value has been studied by Jackendoff (2006). His work forms the starting point of our approach. In natural language processing, there has been very little work on the concept of value. Paul et al. (2009) and Girju and Paul (2011) address the problem of semi-automatically mining patterns that encode reciprocal relationships using pronoun templates. Their work focuses on mining patterns of reciprocity while our work uses expressions of reciprocal actions to learn the value of actions. None of the above works tries to estimate the value of actions, as we do in this work. In fact, we are not aware of any other work that tries to estimate the value of actions from lexical expressions of value. 7 Conclusion We have presented a simple and effective method for learning the value of actions from reciprocal sentences. We show t</context>
</contexts>
<marker>Girju, Paul, 2011</marker>
<rawString>Roxana Girju and Michael J Paul. 2011. Modeling reciprocity in social interactions with probabilistic latent space models. Natural Language Engineering, 17(1):1–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Klaus Obermayer</author>
</authors>
<title>Support vector learning for ordinal regression. In</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 International Conference on Articial Neural Networks,</booktitle>
<pages>97--102</pages>
<contexts>
<context position="8588" citStr="Herbrich et al., 1999" startWordPosition="1437" endWordPosition="1441">t are extracted from the description of the action. We use a linear model that combines the features into a single scalar value for the value v v = wT xa, (1) where xa is the feature vector for action description a and w is a learned weight vector. The goal is to learn a suitable weight vector w that approximates the true relationship between textual expressions of actions and their magnitude of value. Instead of estimating the value directly, we take an alternative approach and consider the task of learning the relative ranking of pairs of actions. We follow the pairwise approach to ranking (Herbrich et al., 1999; Cao et al., 2007) that reduces ranking to a binary classification problem. Ranking the values v1 and v2 of two actions a1 and a2 is equivalent to determining the sign of the dot product between the weight vector w and the difference between the feature vectors xa1 and xaa. v1 &gt; v2 wT xa1 &gt; wT xaa � wT xa1 − wT xaa &gt; 0 �? wT (xa1 − xaa) &gt; 0 (2) For each ranking pair of actions, we create two complimentary classification instances: (xa1 − xaa, l1) and (xaa − xa1, l2), where the labels are l1 = +1, l2 = −1 if the first challenge has higher value than the second challenge and l1 = −1, l2 = +1 ot</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 1999</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Support vector learning for ordinal regression. In In Proceedings of the 1999 International Conference on Articial Neural Networks, pages 97– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>The peculiar logic of value.</title>
<date>2006</date>
<journal>Journal of Cognition and Culture,</journal>
<pages>6--3</pages>
<contexts>
<context position="708" citStr="Jackendoff, 2006" startWordPosition="107" endWordPosition="108">ngapore d.dahlmeier@sap.com Abstract We consider the task of automatically estimating the value of human actions. We cast the problem as a supervised learningto-rank problem between pairs of action descriptions. We present a large, novel data set for this task which consists of challenges from the I Will If You Will Earth Hour challenge. We show that an SVM ranking model with simple linguistic features can accurately predict the relative value of actions. 1 Introduction The question on how humans conceptualize value is of great interest to researchers in various fields, including linguistics (Jackendoff, 2006). The link between value and language arises from the fact that we cannot directly observe value due to its abstract nature and instead often study language expressions that describe actions which have some value attached to them. This creates an interesting link between the semantics of the words that describe the actions and the underlying moral value of the actions. Jackendoff (2006) describes value as an “internal accounting system” for ethical decision processes that exhibits both valence (good or bad) and magnitude (better or worse). Most interestingly, value is governed by a “peculiar l</context>
<context position="3213" citStr="Jackendoff (2006)" startWordPosition="505" endWordPosition="506"> accuracy of up to 94.72% in a 10-fold cross-validation experiment. The results show that the value of actions can accurately be estimated by machine learning methods based on lexical descriptions of the actions. The main contribution of this paper is that we show how the semantics of value in language can accurately be learned from empirical data using a learning-to-rank approach. Our work shows an interesting link between empirical research on semantics in natural language processing and the concept of value. 2 The Logic of Value Our approach is based on the concept of value as presented by Jackendoff (2006) who describes value as an abstract property that is attributed to objects, persons, and actions. He further describes logical inference rules that humans use to determine which actions are deemed morally acceptable and which are not. The most important inference rule for our work is the principal of reciprocation, things that are done “in return” for some other action (Fiengo and Lasnik, 1973). In English, this relation is often expressed by the prepo63 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 63–68, Dublin, Ireland, August 23-24 2014</context>
<context position="17576" citStr="Jackendoff (2006)" startWordPosition="3042" endWordPosition="3043">ining the different feature sets. The best result of 94.72% is obtained by combining all the features: unigrams, bigrams, verb, and object. In summary, these results show that our method is able to accurately predict the relative value of actions using simple linguistic features, which is the main contribution of this work. 6 Related Work The concept of value and reciprocity has been extensively studied in the social sciences (Gergen and Greenberg, 1980), anthropology (Sahlins, 1972), economics (Fehr and G¨achter, 2000), and philosophy (Becker, 1990). In linguistics, value has been studied by Jackendoff (2006). His work forms the starting point of our approach. In natural language processing, there has been very little work on the concept of value. Paul et al. (2009) and Girju and Paul (2011) address the problem of semi-automatically mining patterns that encode reciprocal relationships using pronoun templates. Their work focuses on mining patterns of reciprocity while our work uses expressions of reciprocal actions to learn the value of actions. None of the above works tries to estimate the value of actions, as we do in this work. In fact, we are not aware of any other work that tries to estimate t</context>
</contexts>
<marker>Jackendoff, 2006</marker>
<rawString>Ray Jackendoff. 2006. The peculiar logic of value. Journal of Cognition and Culture, 6(3-4):375–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="13892" citStr="Joachims, 2006" startWordPosition="2427" endWordPosition="2428">the sentence as binary features. We do not remove stopwords for bigram features. In our example sentence, the bigram features bigr quit smoking would be active. We note that our method is not restricted to these feature templates. More sophisticated features, like tree kernels (Collins and Duffy, 2002) or semantic role labeling (Palmer et al., 2010), can be imagined. 5 Experiments We evaluate our approach using standard 10-fold cross-validation and report macro-average accuracy scores for each of the feature sets. The classifier in all our experiments is a linear SVM implemented in SVM-light (Joachims, 2006). 5.1 Data We obtained a snapshot of 18,290 challenges created during the 2013 IWIYW challenge. The snapshot was taken in mid May 2013, just 1.5 weeks before the 2013 Earth Hour event day. We perform the following pre-processing. We normalize the text to proper UTF-8 encoding and remove challenges where the complete sentence contained less than 7 tokens. These challenges were usually empty or incomplete. We filter the challenges using the langid.py tool (Lui and Baldwin, 2012) and only keep English challenges. We normalized the casing of the sentences by first lowercasing all texts and then re</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>423--430</pages>
<contexts>
<context position="14808" citStr="Klein and Manning, 2003" startWordPosition="2581" endWordPosition="2585">enges where the complete sentence contained less than 7 tokens. These challenges were usually empty or incomplete. We filter the challenges using the langid.py tool (Lui and Baldwin, 2012) and only keep English challenges. We normalized the casing of the sentences by first lowercasing all texts and then re-casing each sentence with a simple re-casing model that replaces a word with its most frequent casing form. The re-casing model is trained on the Brown corpus (Ku and Francis, 1967). We tokenize the sentences with the Penn Treebank tokenizer. We parse the sentences with the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) to ob66 Features Accuracy random 0.5000 verb 0.6241 unigrams 0.8481 unigrams + verb 0.8573 object 0.8904 verb + object 0.9115 bigrams 0.9251 unigrams + bigrams 0.9343 unigrams + bigrams + verb 0.9361 unigrams + bigrams + verb + object 0.9472 Table 2: Results of 10-fold cross-validation experiments. tain a constituency parse tree for each challenge. After pre-processing, we are left with 5,499 challenges (4,982 unique), with 4,474 unique I Will challenges and 70 unique You Will challenges. We create binary classifications examples between pairs of actions as describ</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems 15 (NIPS</booktitle>
<pages>423--430</pages>
<contexts>
<context position="14808" citStr="Klein and Manning, 2003" startWordPosition="2581" endWordPosition="2585">enges where the complete sentence contained less than 7 tokens. These challenges were usually empty or incomplete. We filter the challenges using the langid.py tool (Lui and Baldwin, 2012) and only keep English challenges. We normalized the casing of the sentences by first lowercasing all texts and then re-casing each sentence with a simple re-casing model that replaces a word with its most frequent casing form. The re-casing model is trained on the Brown corpus (Ku and Francis, 1967). We tokenize the sentences with the Penn Treebank tokenizer. We parse the sentences with the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) to ob66 Features Accuracy random 0.5000 verb 0.6241 unigrams 0.8481 unigrams + verb 0.8573 object 0.8904 verb + object 0.9115 bigrams 0.9251 unigrams + bigrams 0.9343 unigrams + bigrams + verb 0.9361 unigrams + bigrams + verb + object 0.9472 Table 2: Results of 10-fold cross-validation experiments. tain a constituency parse tree for each challenge. After pre-processing, we are left with 5,499 challenges (4,982 unique), with 4,474 unique I Will challenges and 70 unique You Will challenges. We create binary classifications examples between pairs of actions as describ</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. Advances in Neural Information Processing Systems 15 (NIPS 2002), pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Ku</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<contexts>
<context position="14674" citStr="Ku and Francis, 1967" startWordPosition="2559" endWordPosition="2562"> 2013 Earth Hour event day. We perform the following pre-processing. We normalize the text to proper UTF-8 encoding and remove challenges where the complete sentence contained less than 7 tokens. These challenges were usually empty or incomplete. We filter the challenges using the langid.py tool (Lui and Baldwin, 2012) and only keep English challenges. We normalized the casing of the sentences by first lowercasing all texts and then re-casing each sentence with a simple re-casing model that replaces a word with its most frequent casing form. The re-casing model is trained on the Brown corpus (Ku and Francis, 1967). We tokenize the sentences with the Penn Treebank tokenizer. We parse the sentences with the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) to ob66 Features Accuracy random 0.5000 verb 0.6241 unigrams 0.8481 unigrams + verb 0.8573 object 0.8904 verb + object 0.9115 bigrams 0.9251 unigrams + bigrams 0.9343 unigrams + bigrams + verb 0.9361 unigrams + bigrams + verb + object 0.9472 Table 2: Results of 10-fold cross-validation experiments. tain a constituency parse tree for each challenge. After pre-processing, we are left with 5,499 challenges (4,982 unique), with 4,474 uni</context>
</contexts>
<marker>Ku, Francis, 1967</marker>
<rawString>Henry Ku and W. Nelson Francis. 1967. Computational Analysis of Present-Day American English. Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>An off-theshelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012),</booktitle>
<pages>25--30</pages>
<contexts>
<context position="14373" citStr="Lui and Baldwin, 2012" startWordPosition="2507" endWordPosition="2510">accuracy scores for each of the feature sets. The classifier in all our experiments is a linear SVM implemented in SVM-light (Joachims, 2006). 5.1 Data We obtained a snapshot of 18,290 challenges created during the 2013 IWIYW challenge. The snapshot was taken in mid May 2013, just 1.5 weeks before the 2013 Earth Hour event day. We perform the following pre-processing. We normalize the text to proper UTF-8 encoding and remove challenges where the complete sentence contained less than 7 tokens. These challenges were usually empty or incomplete. We filter the challenges using the langid.py tool (Lui and Baldwin, 2012) and only keep English challenges. We normalized the casing of the sentences by first lowercasing all texts and then re-casing each sentence with a simple re-casing model that replaces a word with its most frequent casing form. The re-casing model is trained on the Brown corpus (Ku and Francis, 1967). We tokenize the sentences with the Penn Treebank tokenizer. We parse the sentences with the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) to ob66 Features Accuracy random 0.5000 verb 0.6241 unigrams 0.8481 unigrams + verb 0.8573 object 0.8904 verb + object 0.9115 bigrams 0.</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. An off-theshelf language identification tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 25– 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Nianwen Xue</author>
</authors>
<date>2010</date>
<booktitle>Semantic role labeling. Synthesis Lectures on Human Language Technologies,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="13628" citStr="Palmer et al., 2010" startWordPosition="2383" endWordPosition="2386">igram: We take all lowercased words that are not stopwords in the I Will part of the sentence as binary features. In our example sentence, the unigram features unigr quit and unigr smoking would be active. • Bigram: We take all lowercased bigrams in the I Will part of the sentence as binary features. We do not remove stopwords for bigram features. In our example sentence, the bigram features bigr quit smoking would be active. We note that our method is not restricted to these feature templates. More sophisticated features, like tree kernels (Collins and Duffy, 2002) or semantic role labeling (Palmer et al., 2010), can be imagined. 5 Experiments We evaluate our approach using standard 10-fold cross-validation and report macro-average accuracy scores for each of the feature sets. The classifier in all our experiments is a linear SVM implemented in SVM-light (Joachims, 2006). 5.1 Data We obtained a snapshot of 18,290 challenges created during the 2013 IWIYW challenge. The snapshot was taken in mid May 2013, just 1.5 weeks before the 2013 Earth Hour event day. We perform the following pre-processing. We normalize the text to proper UTF-8 encoding and remove challenges where the complete sentence contained</context>
</contexts>
<marker>Palmer, Gildea, Xue, 2010</marker>
<rawString>Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010. Semantic role labeling. Synthesis Lectures on Human Language Technologies, 3(1):1–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Roxana Girju</author>
<author>Chen Li</author>
</authors>
<title>Mining the web for reciprocal relationships.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>75--83</pages>
<contexts>
<context position="17736" citStr="Paul et al. (2009)" startWordPosition="3068" endWordPosition="3071">esults show that our method is able to accurately predict the relative value of actions using simple linguistic features, which is the main contribution of this work. 6 Related Work The concept of value and reciprocity has been extensively studied in the social sciences (Gergen and Greenberg, 1980), anthropology (Sahlins, 1972), economics (Fehr and G¨achter, 2000), and philosophy (Becker, 1990). In linguistics, value has been studied by Jackendoff (2006). His work forms the starting point of our approach. In natural language processing, there has been very little work on the concept of value. Paul et al. (2009) and Girju and Paul (2011) address the problem of semi-automatically mining patterns that encode reciprocal relationships using pronoun templates. Their work focuses on mining patterns of reciprocity while our work uses expressions of reciprocal actions to learn the value of actions. None of the above works tries to estimate the value of actions, as we do in this work. In fact, we are not aware of any other work that tries to estimate the value of actions from lexical expressions of value. 7 Conclusion We have presented a simple and effective method for learning the value of actions from recip</context>
</contexts>
<marker>Paul, Girju, Li, 2009</marker>
<rawString>Michael Paul, Roxana Girju, and Chen Li. 2009. Mining the web for reciprocal relationships. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL), pages 75–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marshall D Sahlins</author>
</authors>
<title>Stone age economics.</title>
<date>1972</date>
<publisher>Transaction Publishers.</publisher>
<contexts>
<context position="17447" citStr="Sahlins, 1972" startWordPosition="3024" endWordPosition="3025">features, seems to catch this information just as accurately, achieving 92.51% accuracy. The score is further improved by combining the different feature sets. The best result of 94.72% is obtained by combining all the features: unigrams, bigrams, verb, and object. In summary, these results show that our method is able to accurately predict the relative value of actions using simple linguistic features, which is the main contribution of this work. 6 Related Work The concept of value and reciprocity has been extensively studied in the social sciences (Gergen and Greenberg, 1980), anthropology (Sahlins, 1972), economics (Fehr and G¨achter, 2000), and philosophy (Becker, 1990). In linguistics, value has been studied by Jackendoff (2006). His work forms the starting point of our approach. In natural language processing, there has been very little work on the concept of value. Paul et al. (2009) and Girju and Paul (2011) address the problem of semi-automatically mining patterns that encode reciprocal relationships using pronoun templates. Their work focuses on mining patterns of reciprocity while our work uses expressions of reciprocal actions to learn the value of actions. None of the above works tr</context>
</contexts>
<marker>Sahlins, 1972</marker>
<rawString>Marshall D. Sahlins. 1972. Stone age economics. Transaction Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>