<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002497">
<title confidence="0.995392">
Unification-based Multimodal Parsing
</title>
<author confidence="0.992116">
Michael Johnston
</author>
<affiliation confidence="0.987126666666667">
Center for Human Computer Communication
Department of Computer Science and Engineering
Oregon Graduate Institute
</affiliation>
<address confidence="0.738113">
P.O. Box 91000, Portland, OR 97291-1000
</address>
<email confidence="0.998454">
johnston@cse.ogi.edu
</email>
<sectionHeader confidence="0.993718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.991805875">
In order to realize their full potential, multimodal systems
need to support not just input from multiple modes, but
also synchronized integration of modes. Johnston et al
(1997) model this integration using a unification opera-
tion over typed feature structures. This is an effective so-
lution for a broad class of systems, but limits multimodal
utterances to combinations of a single spoken phrase with
a single gesture. We show how the unification-based ap-
proach can be scaled up to provide a full multimodal
grammar formalism. In conjunction with a multidimen-
sional chart parser, this approach supports integration of
multiple elements distributed across the spatial, temporal,
and acoustic dimensions of multimodal interaction. In-
tegration strategies are stated in a high level unification-
based rule formalism supporting rapid prototyping and it-
erative development of multimodal systems.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998770433333333">
Multimodal interfaces enable more natural and effi-
cient interaction between humans and machines by
providing multiple channels through which input or
output may pass. Our concern here is with multi-
modal input, such as interfaces which support simul-
taneous input from speech and pen. Such interfaces
have clear task performance and user preference ad-
vantages over speech only interfaces, in particular
for spatial tasks such as those involving maps (Ovi-
att 1996). Our focus here is on the integration of in-
put from multiple modes and the role this plays in the
segmentation and parsing of natural human input. In
the examples given here, the modes are speech and
pen, but the architecture described is more general
in that it can support more than two input modes and
modes of other types such as 3D gestural input.
Our multimodal interface technology is imple-
mented in QuickSet (Cohen et al 1997), a work-
ing system which supports dynamic interaction with
maps and other complex visual displays. The initial
applications of QuickSet are: setting up and inter-
acting with distributed simulations (Courtemanche
and Cercanowicz 1995), logistics planning, and nav-
igation in virtual worlds. The system is distributed;
consisting of a series of agents (Figure 1) which
communicate through a shared blackboard (Cohen
et al 1994). It runs on both desktop and handheld
PCs, communicating over wired and wireless LANs.
The user interacts with a map displayed on a wireless
hand-held unit (Figure 2).
</bodyText>
<figure confidence="0.973582333333333">
( User Interface )
Speech Recognition (Gesture R ecognition)
Natural Language (Gesture interpretation)
iher
(Muttimodal Integratoi)
( Bridge .)
</figure>
<figureCaption confidence="0.9999855">
Figure 1: Multimodal Architecture
Figure 2: User Interface
</figureCaption>
<bodyText confidence="0.9998595">
They can draw directly on the map and simultane-
ously issue spoken commands. Different kinds of
entities, lines, and areas may be created by drawing
the appropriate spatial features and speaking their
type; for example, drawing an area and saying &apos;flood
zone&apos;. Orders may also be specified; for example,
by drawing a line and saying &apos;helicopter follow this
route&apos;. The speech signal is routed to an HMM-
</bodyText>
<page confidence="0.997987">
624
</page>
<bodyText confidence="0.999932433333334">
based continuous speaker-independent recognizer.
The electronic &apos;ink&apos; is routed to a neural net-based
gesture recognizer (Pittman 1991). Both generate
N-best lists of potential recognition results with as-
sociated probabilities. These results are assigned se-
mantic interpretations by natural language process-
ing and gesture interpretation agents respectively.
A multimodal integrator agent fields input from the
natural language and gesture interpretation agents
and selects the appropriate multimodal or unimodal
commands to execute. These are passed on to a
bridge agent which provides an API to the underly-
ing applications the system is used to control.
In the approach to multimodal integration pro-
posed by Johnston et al 1997, integration of spoken
and gestural input is driven by a unification opera-
tion over typed feature structures (Carpenter 1992)
representing the semantic contributions of the differ-
ent modes. This approach overcomes the limitations
of previous approaches in that it allows for a full
range of gestural input beyond simple deictic point-
ing gestures. Unlike speech-driven systems (Bolt
1980, Neal and Shapiro 1991, Koons et al 1993,
Wauchope 1994), it is fully multimodal in that all el-
ements of the content of a command can be in ei-
ther mode. Furthermore, compared to related frame-
merging strategies (Vo and Wood 1996), it provides
a well understood, generally applicable common
meaning representation for the different modes and
a formally well defined mechanism for multimodal
integration. However, while this approach provides
an efficient solution for a broad class of multimodal
systems, there are significant limitations on the ex-
pressivity and generality of the approach.
A wide range of potential multimodal utterances
fall outside the expressive potential of the previous
architecture. Empirical studies of multimodal in-
teraction (Oviatt 1996), utilizing wizard-of-oz tech-
niques, have shown that when users are free to inter-
act with any combination of speech and pen, a single
spoken utterance maybe associated with more than
one gesture. For example, a number of deictic point-
ing gestures may be associated with a single spo-
ken utterance: &apos;calculate distance from here to here&apos;,
&apos;put that there&apos;, &apos;move this team to here and prepare
to rescue residents from this building&apos;. Speech may
also be combined with a series of gestures of differ-
ent types: the user circles a vehicle on the map, says
&apos;follow this route&apos;, and draws an arrow indicating
the route to be followed.
In addition to more complex multipart multi-
modal utterances, unimodal gestural utterances may
contain several component gestures which compose
to yield a command. For example, to create an entity
with a specific orientation, a user might draw the en-
tity and then draw an arrow leading out from it (Fig-
ure 3 (a)). To specify a movement, the user might
draw an arrow indicating the extent of the move and
indicate departure and arrival times by writing ex-
pressions at the base and head (Figure 3 (b)). These
</bodyText>
<figure confidence="0.997627">
• • •
(a)
</figure>
<figureCaption confidence="0.999887">
Figure 3: Complex Unimodal Gestures
</figureCaption>
<bodyText confidence="0.9999747">
are specific examples of the more general problem of
visual parsing, which has been a focus of attention
in research on visual programming and pen-based
interfaces for the creation of complex graphical ob-
jects such as mathematical equations and flowcharts
(Lakin 1986, Wittenburg et all991, Helm et al 1991,
Crimi et al 1995).
The approach of Johnston et al 1997 also faces
fundamental architectural problems. The multi-
modal integration strategy is hard-coded into the in-
tegration agent and there is no isolatable statement
of the rules and constraints independent of the code
itself. As the range of multimodal utterances sup-
ported is extended, it becomes essential that there
be a declarative statement of the grammar of multi-
modal utterances, separate from the algorithms and
mechanisms of parsing. This will enable system de-
velopers to describe integration strategies in a high
level representation, facilitating rapid prototyping
and iterative development of multimodal systems.
</bodyText>
<sectionHeader confidence="0.911385" genericHeader="method">
2 Parsing in Multidimensional Space
</sectionHeader>
<bodyText confidence="0.998102111111111">
The integrator in Johnston et al 1997 does in essence
parse input, but the resulting structures can only be
unary or binary trees one level deep; unimodal spo-
ken or gestural commands and multimodal combina-
tions consisting of a single spoken element and a sin-
gle gesture. In order to account for a broader range
of multimodal expressions, a more general parsing
mechanism is needed.
Chart parsing methods have proven effective for
parsing strings and are commonplace in natural
language processing (Kay 1980). Chart parsing
involves population of a triangular matrix of
well-formed constituents: chart(i, j), where i and
j are numbered vertices delimiting the start and
end of the string. In its most basic formulation,
chart parsing can be defined as follows, where *
is an operator which combines two constituents in
accordance with the rules of the grammar.
</bodyText>
<equation confidence="0.922817">
chart(i, j) = U chart(i, k) * chart(k, j)
i &lt; k &lt; j
</equation>
<bodyText confidence="0.997868333333333">
Crucially, this requires the combining constituents
to be discrete and linearly ordered. However,
multimodal input does not meet these requirements:
</bodyText>
<page confidence="0.998491">
625
</page>
<bodyText confidence="0.996522583333333">
gestural input spans two (or three) spatial dimen-
sions, there is an additional non-spatial acoustic
dimension of speech, and both gesture and speech
are distributed across the temporal dimension.
Unlike words in a string, speech and gesture may
overlap temporally, and there is no single dimension
on which the input is linear and discrete. So then,
how can we parse in this multidimensional space of
speech and gesture? What is the rule for chart pars-
ing in multi-dimensional space? Our formulation of
multidimensional parsing for multimodal systems
(multichart) is as follows.
</bodyText>
<equation confidence="0.980795">
multichart(X) = U multichart(Y) * multichart(Z)
where X = YUZ ,Y n z = 0,Y 0 0, Z 0 0
</equation>
<bodyText confidence="0.999896193548387">
In place of numerical spans within a single
dimension (e.g. chart(3,5)), edges in the mul-
tidimensional chart are identified by sets (e.g.
multichartifis, 4, 2], [g, 6, 1]})) containing the
identifiers(IDs) of the terminal input elements
they contain. When two edges combine, the ID of
the resulting edge is the union of their IDs. One
constraint that linearity enforced, which we can still
maintain, is that a given piece of input can only be
used once within a single parse. This is captured by
a requirement of non-intersection between the ID
sets associated with edges being combined. This
requirement is especially important since a single
piece of spoken or gestural input may have multiple
interpretations available in the chart. To prevent
multiple interpretations of a single signal being
used, they are assigned IDs which are identical with
respect to the the non-intersection constraint. The
multichart statement enumerates all the possible
combinations that need to be considered given a set
of inputs whose IDs are contained in a set X.
The multidimensional parsing algorithm (Figure
4) runs bottom-up from the input elements, build-
ing progressively larger constituents in accordance
with the ruleset. An agenda is used to store edges
to be processed. As a simplifying assumption, rules
are assumed to be binary. It is straightforward to ex-
tend the approach to allow for non-binary rules using
techniques from active chart parsing (Earley 1970),
but this step is of limited value given the availability
of multimodal subcategorization (Section 4).
</bodyText>
<construct confidence="0.8809571">
while AGENDA # [ ] do
remove front edge from AGENDA
and make it CURRENTEDGE
for each EDGE, EDGE E CHART
if CURRENTEDGE n EDGE = 0
find set NEWEDGES = U (
(u CURRENTEDGE * EDGE)
(U EDGE * CURRENTEDGE))
add NEWEDGES to end of AGENDA
add CURRENTEDGE to CHART
</construct>
<figureCaption confidence="0.998307">
Figure 4: Multichart Parsing Algorithm
</figureCaption>
<bodyText confidence="0.999971647058823">
For use in a multimodal interface, the multidi-
mensional parsing algorithm needs to be embedded
into the integration agent in such a way that input
can be processed incrementally. Each new input re-
ceived is handled as follows. First, to avoid unnec-
essary computation, stale edges are removed from
the chart. A timeout feature indicates the shelf-
life of an edge within the chart. Second, the in-
terpretations of the new input are treated as termi-
nal edges, placed on the agenda, and combined with
edges in the chart in accordance with the algorithm
above. Third, complete edges are identified and ex-
ecuted. Unlike the typical case in string parsing, the
goal is not to find a single parse covering the whole
chart; the chart may contain several complete non-
overlapping edges which can be executed. These
are assigned to a category command as described
in the next section. The complete edges are ranked
with respect to probability. These probabilities are
a function of the recognition probabilities of the el-
ements which make up the command. The com-
bination of probabilities is specified using declar-
ative constraints, as described in the next section.
The most probable complete edge is executed first,
and all edges it intersects with are removed from the
chart. The next most probable complete edge re-
maining is then executed and the procedure contin-
ues until there are no complete edges left in the chart.
This means that selection of higher probability com-
plete edges eliminates overlapping complete edges
of lower probability from the list of edges to be ex-
ecuted. Lastly, the new chart is stored. In ongoing
work, we are exploring the introduction of other fac-
tors to the selection process. For example, sets of
disjoint complete edges which parse all of the termi-
nal edges in the chart should likely be preferred over
those that do not.
Under certain circumstances, an edge can be used
more than once. This capability supports multiple
creation of entities. For example, the user can utter
&apos;multiple helicopters&apos; point point point point in or-
der to create a series of vehicles. This significantly
speeds up the creation process and limits reliance
on speech recognition. Multiple commands are per-
sistent edges; they are not removed from the chart
after they have participated in the formation of an
executable command. They are assigned timeouts
and are removed when their alloted time runs out.
These &apos;self-destruct&apos; timers are zeroed each time an-
other entity is created, allowing creations to chain
together.
</bodyText>
<sectionHeader confidence="0.916613" genericHeader="method">
3 Unification-based Multimodal
Grammar Representation
</sectionHeader>
<bodyText confidence="0.999601333333333">
Our grammar representation for multimodal expres-
sions draws on unification-based approaches to syn-
tax and semantics (Shieber 1986) such as Head-
</bodyText>
<page confidence="0.995465">
626
</page>
<bodyText confidence="0.999816777777778">
driven phrase structure grammar (HPSG) (Pollard
and Sag 1987,1994). Spoken phrases and pen ges-
tures, which are the terminal elements of the mul-
timodal parsing process, are referred to as lexical
edges. They are assigned grammatical representa-
tions in the form of typed feature structures by the
natural language and gesture interpretation agents
respectively. For example, the spoken phrase &apos;heli-
copter&apos; is assigned the representation in Figure 5.
</bodyText>
<table confidence="0.895283555555556">
cat : unit_type
fsTYPE : create_unit
content object : [ fsTYPE : unit
type : helicopter
echelon : vehicle
location: [ fsTYPE : point
modality: speech
time: interval(.....
prob : 0.85
</table>
<figureCaption confidence="0.995545">
Figure 5: Spoken Input Edge
</figureCaption>
<bodyText confidence="0.999959333333333">
The cat feature indicates the basic category of the
element, while content specifies the semantic con-
tent. In this case, it is a create_unit command in
which the object to be created is a vehicle of type
helicopter, and the location is required to be a point.
The remaining features specify auxiliary informa-
tion such as the modality, temporal interval, and
probability associated with the edge. A point ges-
ture has the representation in Figure 6.
</bodyText>
<construct confidence="0.4861628">
[cat : spatial _gesture
content : [ ettaP:%tlilingtc., ..) 1
modality : gesture
time : interval(.....
prob : 0.69
</construct>
<figureCaption confidence="0.987628">
Figure 6: Point Gesture Edge
</figureCaption>
<bodyText confidence="0.9949898">
Multimodal grammar rules are productions of the
form LHS -4 DTR1DTR2 where LHS,DTR1,
and DT R2 are feature structures of the form indi-
cated above. Following HPSG, these are encoded
as feature structure rule schemata. One advantage
of this is that rule schemata can be hierarchically
ordered, allowing for specific rules to inherit ba-
sic constraints from general rule schemata. The ba-
sic multimodal integration strategy of Johnston et al
1997 is now just one rule among many (Figure 7).
</bodyText>
<figureCaption confidence="0.99701">
Figure 7: Basic Integration Rule Schema
</figureCaption>
<bodyText confidence="0.999479338983051">
The Ihs,dtrl, and dtr2 features correspond to
LHS,DTR1, and DTR2 in the rule above. The
constraints feature indicates an ordered series of
constraints which must be satisfied in order for the
rule to apply. Structure-sharing in the rule represen-
tation is used to impose constraints on the input fea-
ture structures, to construct the LHS category, and
to instantiate the variables in the constraints. For ex-
ample, in Figure 7, the basic constraint that the lo-
cation of a located command such as &apos;helicopter&apos;
needs to unify with the content of the gesture it com-
bines with is captured by the structure-sharing tag
[5]. This also instantiates the location of the result-
ing edge, whose content is inherited through tag [1].
The application of a rule involves unifying the
two candidate edges for combination against dtrl
and dtr2. Rules are indexed by their cat feature in
order to avoid unnecessary unification. If the edges
unify with dtrl and dtr2, then the constraints are
checked. If they are satisfied then a new edge is cre-
ated whose category is the value of lhs and whose
ID set consists of the union of the ID sets assigned
to the two input edges.
Constraints require certain temporal and spatial
relationships to hold between edges. Complex con-
straints can be formed using the basic logical op-
erators V , A, and The temporal constraint in
Figure 7, overlap([7],[10]) V follow([7], [10], 4),
states that the time of the speech [7] must either
overlap with or start within four seconds of the time
of the gesture [10]. This temporal constraint is
based on empirical investigation of multimodal in-
teraction (Oviatt et al 1997). Spatial constraints are
used for combinations of gestural inputs. For ex-
ample, close_to(X, Y) requires two gestures to be
a limited distance apart (See Figure 12 below) and
contact(X,Y) determines whether the regions oc-
cupied by two objects are in contact. The remaining
constraints in Figure 7 do not constrain the inputs per
se, rather they are used to calculate the time, prob,
and modality features for the resulting edge. For
example, the constraint combine_prob ([8] , [11], [4])
is used to combine the probabilities of two inputs
and assign a joint probability to the resulting edge.
In this case, the input probabilities are multiplied.
The assign_modality([6], [9], [2]) constraint deter-
mines the modality of the resulting edge. Auxiliary
features and constraints which are not directly rele-
vant to the discussion will be omitted.
The constraints are interpreted using a prolog
meta-interpreter. This basic back-tracking con-
straint satisfaction strategy is simplistic but adequate
for current purposes. It could readily be substi-
tuted with a more sophisticated constraint solving
strategy allowing for more interaction among con-
straints, default constraints, optimization among a
series of constraints, and so on. The addition of
functional constraints is common in HPSG and other
unification grammar formalisms (Wittenburg 1993).
</bodyText>
<equation confidence="0.9985191">
cat : command
content : [I)
Ihs : modal : [2j
time : (3
prob : [4
ccoat elnor .tdicolmocmazinodn : 151 1
dtrl. : mondalit . : 16]
time: [7 1
prob: [8
cateepst. l5
ialigeeture
dtr2 : mondaillit : 191
time : (101
prob : [II)
toverlap([7],(10t V follow([7],(10], 4)
combine_prob( 8],[11 , op
total-time([71, 10],[3i)
assign_rnodalsty(E61, 9], (21)
rhs :
constraints :
</equation>
<page confidence="0.995857">
627
</page>
<sectionHeader confidence="0.998295" genericHeader="method">
4 Multimodal Subcategorization
</sectionHeader>
<bodyText confidence="0.998132368421053">
Given that multimodal grammar rules are required to
be binary, how can the wide variety of commands in
which speech combines with more than one gestural
element be accounted for? The solution to this prob-
lem draws on the lexicalist treatment of complemen-
tation in HPSG. HPSG utilizes a sophisticated the-
ory of subcategorization to account for the different
complementation patterns that verbs and other lexi-
cal items require. Just as a verb subcategorizes for
its complements, we can think of a lexical edge in
the multimodal grammar as subcategorizing for the
edges with which it needs to combine. For example,
spoken inputs such as &apos;calculate distance from here
to here&apos; and &apos;sandbag wall from here to here&apos; (Figure
8) result in edges which subcategorize for two ges-
tures. Their multimodal subcategorization is speci-
fied in a list valued subcat feature, implemented us-
ing a recursive first/rest feature structure (Shieber
1986:27-32).
</bodyText>
<figure confidence="0.7870173">
^cat : subcat-command
fsTYPE create-line
content : object : [
color grey i
style sandbag :
fsTYPE : wall-obj
location : rfsTYPE : line 1
L coordlist : [[1], [21]]
—
fir [cat : spatiol_gesture
</figure>
<equation confidence="0.969622">
constraints : [overlap([3],[9]) V f ollow([3], (41A)]
st : content :
time : [41 [ coord : [1]
fsTYPE : Paint]]
cat : spattal_gesture
subcat :
fsTYPE : point]
[ coord : [2]
first : content :
rest time : [5]
constraints : [follow([5], [4], 5)]
rest : end
</equation>
<figureCaption confidence="0.994519">
Figure 8: &apos;Sandbag wall from here to here&apos;
</figureCaption>
<bodyText confidence="0.996792833333333">
The cat feature is subcat_command, indicating
that this is an edge with an unsaturated subcatego-
rization list. The first/rest structure indicates the
two gestures the edge needs to combine with and ter-
minates with rest: end. The temporal constraints
on expressions such as these are specific to the ex-
pressions themselves and cannot be specified in the
rule constraints. To support this, we allow for lexical
edges to carry their own specific lexical constraints,
which are held in a constraints feature at each level
in the subcat list. In this case, the first gesture is
constrained to overlap with the speech or come up
to four seconds before it and the second gesture is
required to follow the first gesture. Lexical con-
straints are inherited into the rule constraints in the
combinatory schemata described below. Edges with
subcat features are combined with other elements
in the chart in accordance with general combinatory
schemata. The first (Figure 9) applies to unsaturated
edges which have more than one element on their
subcat list. It unifies the first element of the sub-
cat list with an element in the chart and builds a new
edge of category subcat _command whose subcat list
is the value of rest.
</bodyText>
<table confidence="0.994650909090909">
Ihs : cat : subcat_command i -command
rhs , content : [1] [1] first
constraints subcat : [2] ] [ constraints
[ prob: [3] rest
cat : subcat : [91
content : : [5] I
[ dtrl : [ : [2][ ]
subcat : I [5] }
prob : [6]
dtr2 : (4J[ prob : [7]
: { combine-prob([6],[7],[3])
</table>
<figureCaption confidence="0.993563">
Figure 9: Subcat Combination Schema
</figureCaption>
<bodyText confidence="0.79067675">
The second schema (Figure 10) applies to unsat-
urated (cat: subcat_command) edges on whose sub-
cat list only one element remains and generates sat-
urated (cat: command) edges.
</bodyText>
<equation confidence="0.9916279375">
{
cat : command I
content : [1]
the• subcat : end
prob [2]
cat : subcat_command
content [1]
first [3]
rhs
dtrl :
subcat : constraints : [4]
:
rest : end
prob : [5]
dtr2 : [3] [ prob : [6] ]
constraints : combine-prob([5], [6], [2]) I [9] }
</equation>
<figureCaption confidence="0.768744">
Figure 10: Subcat Termination Schema
</figureCaption>
<bodyText confidence="0.9999217">
This specification of combinatory information in
the lexical edges constitutes a shift from rules to
representations. The ruleset is simplified to a set
of general schemata, and the lexical representa-
tion is extended to express combinatorics. How-
ever, there is still a need for rules beyond these
general schemata in order to account for construc-
tional meaning (Goldberg 1995) in multimodal in-
put, specifically with respect to complex unimodal
gestures.
</bodyText>
<sectionHeader confidence="0.995283" genericHeader="method">
5 Visual Parsing: Complex Gestures
</sectionHeader>
<bodyText confidence="0.999979454545455">
In addition to combinations of speech with more
than one gesture, the architecture supports unimodal
gestural commands consisting of several indepen-
dently recognized gestural components. For exam-
ple, lines may be created using what we term gestu-
ral diacritics. If environmental noise or other fac-
tors make speaking the type of a line infeasible, it
may be specified by drawing a simple gestural mark
or word over a line gesture. To create a barbed wire,
the user can draw a line specifying its spatial extent
and then draw an alpha to indicate its type.
</bodyText>
<figureCaption confidence="0.981298">
Figure 11: Complex Gesture for Barbed Wire
</figureCaption>
<bodyText confidence="0.995362">
This gestural construction is licensed by the rule
schema in Figure 12. It states that a line gesture
</bodyText>
<equation confidence="0.655003">
time : [3]
</equation>
<page confidence="0.989319">
628
</page>
<bodyText confidence="0.999895777777778">
(dtrl) and an alpha gesture (dtr2) can be combined,
resulting in a command to create a barbed wire. The
location information is inherited from the line ges-
ture. There is nothing inherent about alpha that
makes it mean &apos;barbed wire&apos;. That meaning is em-
bodied only in its construction with a line gesture,
which is captured in the rule schema. The close_to
constraint requires that the centroid of the alpha be
in proximity to the line.
</bodyText>
<table confidence="0.893400428571429">
Ihs [ cat : command fsTYPE : create_line
content : [ object : fsTYPE : wire_obj I
location [ color : red
style : barbed
: [I]
dtrl : [ content : [1) [
cat : spatial gesture
</table>
<equation confidence="0.954129125">
[
dtr2 : { cat : spatial-gesture
content: [ fsTYPE : alpha 1
time : [3]
centroid : IN fsTYPE : line 1
coordlist : [2] J
time : [5]
constraint. !;3,;1.,:t_vt(j(1)41[,31]2,ir
</equation>
<figureCaption confidence="0.871451">
Figure 12: Rule Schema for Unimodal Barbed Wire
</figureCaption>
<sectionHeader confidence="0.994858" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993494505495">
The multimodal language processing architecture
presented here enables parsing and interpretation of
natural human input distributed across two or three
spatial dimensions, time, and the acoustic dimension
of speech. Multimodal integration strategies are
stated declaratively in a unification-based grammar
formalism which is interpreted by an incremental
multidimensional parser. We have shown how this
architecture supports multimodal (pen/voice) inter-
faces to dynamic maps. It has been implemented and
deployed as part of QuickSet (Cohen et al 1997) and
operates in real time. A broad range of multimodal
utterances are supported including combination of
speech with multiple gestures and visual parsing of
collections of gestures into complex unimodal com-
mands. Combinatory information and constraints
may be stated either in the lexical edges or in the rule
schemata, allowing individual phenomena to be de-
scribed in the way that best suits their nature. The ar-
chitecture is sufficiently general to support other in-
put modes and devices including 3D gestural input.
The declarative statement of multimodal integration
strategies enables rapid prototyping and iterative de-
velopment of multimodal systems.
The system has undergone a form of pro-active
evaluation in that its design is informed by detailed
predictive modeling of how users interact multi-
modally, and incorporates the results of empirical
studies of multimodal interaction (Oviatt 1996, Ovi-
att et al 1997). It is currently undergoing extensive
user testing and evaluation (McGee et al 1998).
Previous work on grammars and parsing for mul-
tidimensional languages has focused on two dimen-
sional graphical expressions such as mathematical
equations, flowcharts, and visual programming lan-
guages. Lakin (1986) lays out many of the ini-
tial issues in parsing for two-dimensional draw-
ings and utilizes specialized parsers implemented in
LISP to parse specific graphical languages. Helm
et al (1991) employ a grammatical framework, con-
strained set grammars, in which constituent struc-
ture rules are augmented with spatial constraints.
Visual language parsers are build by translation of
these rules into a constraint logic programming lan-
guage. Crimi et al (1991) utilize a similar relation
grammar formalism in which a sentence consists
of a multiset of objects and relations among them.
Their rules are also augmented with constraints and
parsing is provided by a prolog axiomatization. Wit-
tenburg et al (1991) employ a unification-based
grammar formalism augmented with functional con-
straints (F-PATR, Wittenburg 1993), and a bottom-
up, incremental, Earley-style (Earley 1970) tabular
parsing algorithm.
All of these approaches face significant difficul-
ties in terms of computational complexity. At worst,
an exponential number of combinations of the in-
put elements need to be considered, and the parse
table may be of exponential size (Wittenburg et al
1991:365). Efficiency concerns drive Helm et al
(1991:111) to adopt a committed choice strategy
under which successfully applied productions can-
not be backtracked over and complex negative and
quantificational constraints are used to limit rule ap-
plication. Wittenburg et al&apos;s parsing mechanism is
directed by expander relations in the grammar for-
malism which filter out inappropriate combinations
before they are considered. Wittenburg (1996) ad-
dresses the complexity issue by adding top-down
predictive information to the parsing process.
This work is fundamentally different from all
of these approaches in that it focuses on multi-
modal systems, and this has significant implications
in terms of computational viability. The task dif-
fers greatly from parsing of mathematical equations,
flowcharts, and other complex graphical expressions
in that the number of elements to be parsed is far
smaller. Empirical investigation (Oviatt 1996, Ovi-
att et al 1997) has shown that multimodal utter-
ances rarely contain more than two or three ele-
ments. Each of those elements may have multi-
ple interpretations, but the overall number of lexi-
cal edges remains sufficiently small to enable fast
processing of all the potential combinations. Also,
the intersection constraint on combining edges lim-
its the impact of the multiple interpretations of each
piece of input. The deployment of this architecture
in an implemented system supporting real time spo-
ken and gestural interaction with a dynamic map
provides evidence of its computational viability for
real tasks. Our approach is similar to Wittenburg et
</bodyText>
<equation confidence="0.838364">
rhs :
</equation>
<page confidence="0.994233">
629
</page>
<bodyText confidence="0.999838294117647">
al 1991 in its use of a unification-based grammar for-
malism augmented with functional constraints and
a chart parser adapted for multidimensional spaces.
Our approach differs in that, given the nature of the
input, using spatial constraints and top-down predic-
tive information to guide the parse is less of a con-
cern, and as a result the parsing algorithm is signifi-
cantly more straightforward and general.
The evolution of multimodal systems is follow-
ing a trajectory which has parallels in the history
of syntactic parsing. Initial approaches to multi-
modal integration were largely algorithmic in na-
ture. The next stage is the formulation of declarative
integration rules (phrase structure rules), then comes
a shift from rules to representations (lexicalism, cat-
egorial and unification-based grammars). The ap-
proach outlined here is at representational stage, al-
though rule schemata are still used for constructional
meaning. The next phase, which syntax is under-
going, is the compilation of rules and representa-
tions back into fast, low-powered finite state devices
(Roche and Schabes 1997). At this early stage in the
development of multimodal systems, we need a high
degree of flexibility. In the future, once it is clearer
what needs to be accounted for, the next step will be
to explore compilation of multimodal grammars into
lower power devices.
Our primary areas of future research include re-
finement of the probability combination scheme for
multimodal utterances, exploration of alternative
constraint solving strategies, multiple inheritance
for rule schemata, maintenance of multimodal di-
alogue history, and experimentation with 3D input
and other combinations of modes.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999802865979382">
Bolt, R. A. 1980. &amp;quot;Put-That-There&amp;quot;: Voice and gesture at
the graphics interface. Computer Graphics, 14.3:262-
270.
Carpenter, R. 1992. The logic of ypedfeature structures.
Cambridge University Press, Cambridge, England.
Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg. 1994.
An open agent architecture. In Working Notes of the
AAAI Spring Symposium on Software Agents, 1-8.
Cohen, P. R., M. Johnston, D. McGee, S. L. Oviatt, J.
A. Pittman, I. Smith, L. Chen, and J. Clow. 1997.
QuickSet: Multimodal interaction for distributed ap-
plications. In Proceedings of the Fifth ACM Interna-
tional Multimedia Conference. 31-40.
Courtemanche, A. J., and A. Ceranowicz. 1995. Mod-
SAF development status. In Proceedings of the 5th
Conference on Computer Generated Forces and Be-
havioral Representation, 3-13.
Crimi, A, A. Guercio, G. Nota, G. Pacini, G. Tortora, and
M. Tucci. 1991. Relation grammars and their applica-
tion to multi-dimensional languages. Journal of Visual
Languages and Computing, 2:333-346.
Earley, J. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13,94-102.
Goldberg, A. 1995. Constructions: A Construction
Grammar Approach to Argument Structure. Univer-
sity of Chicago Press, Chicago.
Helm, R., K. Marriott, and M. Odersky. 1991. Building
visual language parsers. In Proceedings of Conference
on Human Factors in Computing Systems: CHI &apos;91,
ACM Press, New York, 105-112.
Johnston, M., P. R. Cohen, D. McGee, S. L. Oviatt, J. A.
Pittman, and I. Smith. 1997. Unification-based multi-
modal integration. In Proceedings of the 35th Annual
Meeting of the Associationfor Computational Linguis-
tics and 8th Conference of the European Chapter of the
Association for Computational Linguistics, 281-288.
Kay, M. 1980. Algorithm schemata and data structures
in syntactic processing. In B. J. Grosz, K. S. Jones, and
B. L. Webber (eds.) Readings in Natural Language
Processing, Morgan Kaufmann, 1986,35-70.
Koons, D. B., C. J. Sparrell, and K. R. Thorisson. 1993.
Integrating simultaneous input from speech, gaze, and
hand gestures. In M. T. Maybury (ed.) Intelligent Mul-
timedia Interfaces, MIT Press, 257-276.
Lakin, F. 1986. Spatial parsing for visual languages.
In S. K. Chang, T. Ichikawa, and P. A. Ligomenides
(ed.$), Visual Languages. Plenum Press, 35-85.
McGee, D., P. R. Co-hen, S. L. Oviatt. 1998. Confirma-
tion in multimodal systems. In Proceedings of 17th In-
ternational Conference on Computational Linguistics
and 36th Annual Meeting of the Association for Com-
putational Linguistics.
Neal, J. G., and S. C. Shapiro. 1991. Intelligent multi-
media interface technology. In J. W. Sullivan and
S. W. Tyler (eds.) Intelligent User Interfaces, ACM
Press, Addison Wesley, New York, 45-68.
Oviatt, S. L. 1996. Multimodal interfaces for dynamic
interactive maps. In Proceedings of Conference on
Human Factors in Computing Systems, 95-102.
Oviatt, S. L., A. DeAngeli, and K. Kuhn. 1997. Integra-
tion and synchronization of input modes during multi-
modal human-computer interaction. In Proceedings of
Conference on Human Factors in Computing Systems,
415-422.
Pittman, J. A. 1991. Recognizing handwritten text.
In Proceedings of Conference on Human Factors in
Computing Systems: CHI &apos;91. 271-275.
Pollard, C. J. and I. A. Sag. 1987. Information-based
syntax and semantics: Volume!, Fundamentals., CSLI
Lecture Notes Volume 13. CSLI, Stanford.
Pollard, Carl and Ivan Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press. Chicago.
Roche, E. and Y. Schabes. 1997. Finite state language
processing. MIT Press, Cambridge.
Shieber, S. M. 1986. An Introduction to unification-
based approaches to grammar. CSLI Lecture Notes
Volume 4. CSLI, Stanford.
Vo, M. T., and C. Wood. 1996. Building an applica-
tion framework for speech and pen input integration
in multimodal learning interfaces. In Proceedings of
ICASSP&apos;96.
Wauchope, K. 1994. Eucalyptus: Integrating natural
language input with a graphical user interface. Naval
Research Laboratory, Report NRL/FR/5510-94-9711.
Wittenburg, K., L. Weitzman, and J. Talley. 1991.
Unification-Based grammars and tabular parsing for
graphical languages. Journal of Visual Languages and
Computing 2:347-370.
Wittenburg, K. L. 1993. F-PATR: Functional con-
straints for unification-based grammars. Proceedings
of the 31st Annual Meeting of the Association for Com-
putational Linguistics, 216-223.
Wittenburg, K. 1996. Predictive parsing for unordered
relational languages. In H. Bunt and M. Tomita (eds.),
Recent Advances in Parsing Technologies, Kluwer,
Dordrecht, 385-407.
</reference>
<page confidence="0.997613">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978765">
<title confidence="0.99973">Unification-based Multimodal Parsing</title>
<author confidence="0.999876">Michael Johnston</author>
<affiliation confidence="0.996818">Center for Human Computer Communication Department of Computer Science and Engineering Oregon Graduate Institute</affiliation>
<address confidence="0.998891">P.O. Box 91000, Portland, OR 97291-1000</address>
<email confidence="0.9997">johnston@cse.ogi.edu</email>
<abstract confidence="0.999403176470588">In order to realize their full potential, multimodal systems need to support not just input from multiple modes, but also synchronized integration of modes. Johnston et al (1997) model this integration using a unification operation over typed feature structures. This is an effective solution for a broad class of systems, but limits multimodal utterances to combinations of a single spoken phrase with a single gesture. We show how the unification-based approach can be scaled up to provide a full multimodal grammar formalism. In conjunction with a multidimensional chart parser, this approach supports integration of multiple elements distributed across the spatial, temporal, and acoustic dimensions of multimodal interaction. Integration strategies are stated in a high level unificationbased rule formalism supporting rapid prototyping and iterative development of multimodal systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R A Bolt</author>
</authors>
<title>Put-That-There&amp;quot;: Voice and gesture at the graphics interface.</title>
<date>1980</date>
<journal>Computer Graphics,</journal>
<pages>14--3</pages>
<contexts>
<context position="4355" citStr="Bolt 1980" startWordPosition="661" endWordPosition="662">modal commands to execute. These are passed on to a bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration. However, while this approach provides an efficient solution for a broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach. A wide ran</context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Bolt, R. A. 1980. &amp;quot;Put-That-There&amp;quot;: Voice and gesture at the graphics interface. Computer Graphics, 14.3:262-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carpenter</author>
</authors>
<title>The logic of ypedfeature structures.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="4094" citStr="Carpenter 1992" startWordPosition="622" endWordPosition="623">s are assigned semantic interpretations by natural language processing and gesture interpretation agents respectively. A multimodal integrator agent fields input from the natural language and gesture interpretation agents and selects the appropriate multimodal or unimodal commands to execute. These are passed on to a bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes an</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Carpenter, R. 1992. The logic of ypedfeature structures. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>A Cheyer</author>
<author>M Wang</author>
<author>S C Baeg</author>
</authors>
<title>An open agent architecture.</title>
<date>1994</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Software Agents,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2462" citStr="Cohen et al 1994" startWordPosition="372" endWordPosition="375">ore general in that it can support more than two input modes and modes of other types such as 3D gestural input. Our multimodal interface technology is implemented in QuickSet (Cohen et al 1997), a working system which supports dynamic interaction with maps and other complex visual displays. The initial applications of QuickSet are: setting up and interacting with distributed simulations (Courtemanche and Cercanowicz 1995), logistics planning, and navigation in virtual worlds. The system is distributed; consisting of a series of agents (Figure 1) which communicate through a shared blackboard (Cohen et al 1994). It runs on both desktop and handheld PCs, communicating over wired and wireless LANs. The user interacts with a map displayed on a wireless hand-held unit (Figure 2). ( User Interface ) Speech Recognition (Gesture R ecognition) Natural Language (Gesture interpretation) iher (Muttimodal Integratoi) ( Bridge .) Figure 1: Multimodal Architecture Figure 2: User Interface They can draw directly on the map and simultaneously issue spoken commands. Different kinds of entities, lines, and areas may be created by drawing the appropriate spatial features and speaking their type; for example, drawing a</context>
</contexts>
<marker>Cohen, Cheyer, Wang, Baeg, 1994</marker>
<rawString>Cohen, P. R., A. Cheyer, M. Wang, and S. C. Baeg. 1994. An open agent architecture. In Working Notes of the AAAI Spring Symposium on Software Agents, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>M Johnston</author>
<author>D McGee</author>
<author>S L Oviatt</author>
<author>J A Pittman</author>
<author>I Smith</author>
<author>L Chen</author>
<author>J Clow</author>
</authors>
<title>QuickSet: Multimodal interaction for distributed applications.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth ACM International Multimedia Conference.</booktitle>
<pages>31--40</pages>
<contexts>
<context position="2039" citStr="Cohen et al 1997" startWordPosition="309" endWordPosition="312">en. Such interfaces have clear task performance and user preference advantages over speech only interfaces, in particular for spatial tasks such as those involving maps (Oviatt 1996). Our focus here is on the integration of input from multiple modes and the role this plays in the segmentation and parsing of natural human input. In the examples given here, the modes are speech and pen, but the architecture described is more general in that it can support more than two input modes and modes of other types such as 3D gestural input. Our multimodal interface technology is implemented in QuickSet (Cohen et al 1997), a working system which supports dynamic interaction with maps and other complex visual displays. The initial applications of QuickSet are: setting up and interacting with distributed simulations (Courtemanche and Cercanowicz 1995), logistics planning, and navigation in virtual worlds. The system is distributed; consisting of a series of agents (Figure 1) which communicate through a shared blackboard (Cohen et al 1994). It runs on both desktop and handheld PCs, communicating over wired and wireless LANs. The user interacts with a map displayed on a wireless hand-held unit (Figure 2). ( User I</context>
<context position="24750" citStr="Cohen et al 1997" startWordPosition="4038" endWordPosition="4041">,ir Figure 12: Rule Schema for Unimodal Barbed Wire 6 Conclusion The multimodal language processing architecture presented here enables parsing and interpretation of natural human input distributed across two or three spatial dimensions, time, and the acoustic dimension of speech. Multimodal integration strategies are stated declaratively in a unification-based grammar formalism which is interpreted by an incremental multidimensional parser. We have shown how this architecture supports multimodal (pen/voice) interfaces to dynamic maps. It has been implemented and deployed as part of QuickSet (Cohen et al 1997) and operates in real time. A broad range of multimodal utterances are supported including combination of speech with multiple gestures and visual parsing of collections of gestures into complex unimodal commands. Combinatory information and constraints may be stated either in the lexical edges or in the rule schemata, allowing individual phenomena to be described in the way that best suits their nature. The architecture is sufficiently general to support other input modes and devices including 3D gestural input. The declarative statement of multimodal integration strategies enables rapid prot</context>
</contexts>
<marker>Cohen, Johnston, McGee, Oviatt, Pittman, Smith, Chen, Clow, 1997</marker>
<rawString>Cohen, P. R., M. Johnston, D. McGee, S. L. Oviatt, J. A. Pittman, I. Smith, L. Chen, and J. Clow. 1997. QuickSet: Multimodal interaction for distributed applications. In Proceedings of the Fifth ACM International Multimedia Conference. 31-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Courtemanche</author>
<author>A Ceranowicz</author>
</authors>
<title>ModSAF development status.</title>
<date>1995</date>
<booktitle>In Proceedings of the 5th Conference on Computer Generated Forces and Behavioral Representation,</booktitle>
<pages>3--13</pages>
<marker>Courtemanche, Ceranowicz, 1995</marker>
<rawString>Courtemanche, A. J., and A. Ceranowicz. 1995. ModSAF development status. In Proceedings of the 5th Conference on Computer Generated Forces and Behavioral Representation, 3-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Crimi</author>
<author>A Guercio</author>
<author>G Nota</author>
<author>G Pacini</author>
<author>G Tortora</author>
<author>M Tucci</author>
</authors>
<title>Relation grammars and their application to multi-dimensional languages.</title>
<date>1991</date>
<journal>Journal of Visual Languages and Computing,</journal>
<pages>2--333</pages>
<contexts>
<context position="26420" citStr="Crimi et al (1991)" startWordPosition="4292" endWordPosition="4295">ltidimensional languages has focused on two dimensional graphical expressions such as mathematical equations, flowcharts, and visual programming languages. Lakin (1986) lays out many of the initial issues in parsing for two-dimensional drawings and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules into a constraint logic programming language. Crimi et al (1991) utilize a similar relation grammar formalism in which a sentence consists of a multiset of objects and relations among them. Their rules are also augmented with constraints and parsing is provided by a prolog axiomatization. Wittenburg et al (1991) employ a unification-based grammar formalism augmented with functional constraints (F-PATR, Wittenburg 1993), and a bottomup, incremental, Earley-style (Earley 1970) tabular parsing algorithm. All of these approaches face significant difficulties in terms of computational complexity. At worst, an exponential number of combinations of the input elem</context>
</contexts>
<marker>Crimi, Guercio, Nota, Pacini, Tortora, Tucci, 1991</marker>
<rawString>Crimi, A, A. Guercio, G. Nota, G. Pacini, G. Tortora, and M. Tucci. 1991. Relation grammars and their application to multi-dimensional languages. Journal of Visual Languages and Computing, 2:333-346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--94</pages>
<contexts>
<context position="10503" citStr="Earley 1970" startWordPosition="1657" endWordPosition="1658"> with respect to the the non-intersection constraint. The multichart statement enumerates all the possible combinations that need to be considered given a set of inputs whose IDs are contained in a set X. The multidimensional parsing algorithm (Figure 4) runs bottom-up from the input elements, building progressively larger constituents in accordance with the ruleset. An agenda is used to store edges to be processed. As a simplifying assumption, rules are assumed to be binary. It is straightforward to extend the approach to allow for non-binary rules using techniques from active chart parsing (Earley 1970), but this step is of limited value given the availability of multimodal subcategorization (Section 4). while AGENDA # [ ] do remove front edge from AGENDA and make it CURRENTEDGE for each EDGE, EDGE E CHART if CURRENTEDGE n EDGE = 0 find set NEWEDGES = U ( (u CURRENTEDGE * EDGE) (U EDGE * CURRENTEDGE)) add NEWEDGES to end of AGENDA add CURRENTEDGE to CHART Figure 4: Multichart Parsing Algorithm For use in a multimodal interface, the multidimensional parsing algorithm needs to be embedded into the integration agent in such a way that input can be processed incrementally. Each new input receive</context>
<context position="26835" citStr="Earley 1970" startWordPosition="4355" endWordPosition="4356"> constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules into a constraint logic programming language. Crimi et al (1991) utilize a similar relation grammar formalism in which a sentence consists of a multiset of objects and relations among them. Their rules are also augmented with constraints and parsing is provided by a prolog axiomatization. Wittenburg et al (1991) employ a unification-based grammar formalism augmented with functional constraints (F-PATR, Wittenburg 1993), and a bottomup, incremental, Earley-style (Earley 1970) tabular parsing algorithm. All of these approaches face significant difficulties in terms of computational complexity. At worst, an exponential number of combinations of the input elements need to be considered, and the parse table may be of exponential size (Wittenburg et al 1991:365). Efficiency concerns drive Helm et al (1991:111) to adopt a committed choice strategy under which successfully applied productions cannot be backtracked over and complex negative and quantificational constraints are used to limit rule application. Wittenburg et al&apos;s parsing mechanism is directed by expander rel</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13,94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldberg</author>
</authors>
<title>Constructions: A Construction Grammar Approach to Argument Structure.</title>
<date>1995</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="22529" citStr="Goldberg 1995" startWordPosition="3669" endWordPosition="3670">bcat : end prob [2] cat : subcat_command content [1] first [3] rhs dtrl : subcat : constraints : [4] : rest : end prob : [5] dtr2 : [3] [ prob : [6] ] constraints : combine-prob([5], [6], [2]) I [9] } Figure 10: Subcat Termination Schema This specification of combinatory information in the lexical edges constitutes a shift from rules to representations. The ruleset is simplified to a set of general schemata, and the lexical representation is extended to express combinatorics. However, there is still a need for rules beyond these general schemata in order to account for constructional meaning (Goldberg 1995) in multimodal input, specifically with respect to complex unimodal gestures. 5 Visual Parsing: Complex Gestures In addition to combinations of speech with more than one gesture, the architecture supports unimodal gestural commands consisting of several independently recognized gestural components. For example, lines may be created using what we term gestural diacritics. If environmental noise or other factors make speaking the type of a line infeasible, it may be specified by drawing a simple gestural mark or word over a line gesture. To create a barbed wire, the user can draw a line specifyi</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Goldberg, A. 1995. Constructions: A Construction Grammar Approach to Argument Structure. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Helm</author>
<author>K Marriott</author>
<author>M Odersky</author>
</authors>
<title>Building visual language parsers.</title>
<date>1991</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;91,</booktitle>
<pages>105--112</pages>
<publisher>ACM Press,</publisher>
<location>New York,</location>
<contexts>
<context position="6603" citStr="Helm et al 1991" startWordPosition="1030" endWordPosition="1033">the entity and then draw an arrow leading out from it (Figure 3 (a)). To specify a movement, the user might draw an arrow indicating the extent of the move and indicate departure and arrival times by writing expressions at the base and head (Figure 3 (b)). These • • • (a) Figure 3: Complex Unimodal Gestures are specific examples of the more general problem of visual parsing, which has been a focus of attention in research on visual programming and pen-based interfaces for the creation of complex graphical objects such as mathematical equations and flowcharts (Lakin 1986, Wittenburg et all991, Helm et al 1991, Crimi et al 1995). The approach of Johnston et al 1997 also faces fundamental architectural problems. The multimodal integration strategy is hard-coded into the integration agent and there is no isolatable statement of the rules and constraints independent of the code itself. As the range of multimodal utterances supported is extended, it becomes essential that there be a declarative statement of the grammar of multimodal utterances, separate from the algorithms and mechanisms of parsing. This will enable system developers to describe integration strategies in a high level representation, fa</context>
<context position="26156" citStr="Helm et al (1991)" startWordPosition="4252" endWordPosition="4255"> interact multimodally, and incorporates the results of empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It is currently undergoing extensive user testing and evaluation (McGee et al 1998). Previous work on grammars and parsing for multidimensional languages has focused on two dimensional graphical expressions such as mathematical equations, flowcharts, and visual programming languages. Lakin (1986) lays out many of the initial issues in parsing for two-dimensional drawings and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules into a constraint logic programming language. Crimi et al (1991) utilize a similar relation grammar formalism in which a sentence consists of a multiset of objects and relations among them. Their rules are also augmented with constraints and parsing is provided by a prolog axiomatization. Wittenburg et al (1991) employ a unification-based grammar formalism augmented with functional constraints (F-</context>
</contexts>
<marker>Helm, Marriott, Odersky, 1991</marker>
<rawString>Helm, R., K. Marriott, and M. Odersky. 1991. Building visual language parsers. In Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;91, ACM Press, New York, 105-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>P R Cohen</author>
<author>D McGee</author>
<author>S L Oviatt</author>
<author>J A Pittman</author>
<author>I Smith</author>
</authors>
<title>Unification-based multimodal integration.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Associationfor Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>281--288</pages>
<contexts>
<context position="3968" citStr="Johnston et al 1997" startWordPosition="601" endWordPosition="604">recognizer (Pittman 1991). Both generate N-best lists of potential recognition results with associated probabilities. These results are assigned semantic interpretations by natural language processing and gesture interpretation agents respectively. A multimodal integrator agent fields input from the natural language and gesture interpretation agents and selects the appropriate multimodal or unimodal commands to execute. These are passed on to a bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (V</context>
<context position="6659" citStr="Johnston et al 1997" startWordPosition="1041" endWordPosition="1044">t (Figure 3 (a)). To specify a movement, the user might draw an arrow indicating the extent of the move and indicate departure and arrival times by writing expressions at the base and head (Figure 3 (b)). These • • • (a) Figure 3: Complex Unimodal Gestures are specific examples of the more general problem of visual parsing, which has been a focus of attention in research on visual programming and pen-based interfaces for the creation of complex graphical objects such as mathematical equations and flowcharts (Lakin 1986, Wittenburg et all991, Helm et al 1991, Crimi et al 1995). The approach of Johnston et al 1997 also faces fundamental architectural problems. The multimodal integration strategy is hard-coded into the integration agent and there is no isolatable statement of the rules and constraints independent of the code itself. As the range of multimodal utterances supported is extended, it becomes essential that there be a declarative statement of the grammar of multimodal utterances, separate from the algorithms and mechanisms of parsing. This will enable system developers to describe integration strategies in a high level representation, facilitating rapid prototyping and iterative development o</context>
<context position="15311" citStr="Johnston et al 1997" startWordPosition="2447" endWordPosition="2450">as the representation in Figure 6. [cat : spatial _gesture content : [ ettaP:%tlilingtc., ..) 1 modality : gesture time : interval(..... prob : 0.69 Figure 6: Point Gesture Edge Multimodal grammar rules are productions of the form LHS -4 DTR1DTR2 where LHS,DTR1, and DT R2 are feature structures of the form indicated above. Following HPSG, these are encoded as feature structure rule schemata. One advantage of this is that rule schemata can be hierarchically ordered, allowing for specific rules to inherit basic constraints from general rule schemata. The basic multimodal integration strategy of Johnston et al 1997 is now just one rule among many (Figure 7). Figure 7: Basic Integration Rule Schema The Ihs,dtrl, and dtr2 features correspond to LHS,DTR1, and DTR2 in the rule above. The constraints feature indicates an ordered series of constraints which must be satisfied in order for the rule to apply. Structure-sharing in the rule representation is used to impose constraints on the input feature structures, to construct the LHS category, and to instantiate the variables in the constraints. For example, in Figure 7, the basic constraint that the location of a located command such as &apos;helicopter&apos; needs to </context>
</contexts>
<marker>Johnston, Cohen, McGee, Oviatt, Pittman, Smith, 1997</marker>
<rawString>Johnston, M., P. R. Cohen, D. McGee, S. L. Oviatt, J. A. Pittman, and I. Smith. 1997. Unification-based multimodal integration. In Proceedings of the 35th Annual Meeting of the Associationfor Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, 281-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm schemata and data structures in syntactic processing.</title>
<date>1980</date>
<booktitle>Readings in Natural Language Processing,</booktitle>
<pages>1986--35</pages>
<editor>In B. J. Grosz, K. S. Jones, and B. L. Webber (eds.)</editor>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="7824" citStr="Kay 1980" startWordPosition="1222" endWordPosition="1223">apid prototyping and iterative development of multimodal systems. 2 Parsing in Multidimensional Space The integrator in Johnston et al 1997 does in essence parse input, but the resulting structures can only be unary or binary trees one level deep; unimodal spoken or gestural commands and multimodal combinations consisting of a single spoken element and a single gesture. In order to account for a broader range of multimodal expressions, a more general parsing mechanism is needed. Chart parsing methods have proven effective for parsing strings and are commonplace in natural language processing (Kay 1980). Chart parsing involves population of a triangular matrix of well-formed constituents: chart(i, j), where i and j are numbered vertices delimiting the start and end of the string. In its most basic formulation, chart parsing can be defined as follows, where * is an operator which combines two constituents in accordance with the rules of the grammar. chart(i, j) = U chart(i, k) * chart(k, j) i &lt; k &lt; j Crucially, this requires the combining constituents to be discrete and linearly ordered. However, multimodal input does not meet these requirements: 625 gestural input spans two (or three) spatia</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay, M. 1980. Algorithm schemata and data structures in syntactic processing. In B. J. Grosz, K. S. Jones, and B. L. Webber (eds.) Readings in Natural Language Processing, Morgan Kaufmann, 1986,35-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Koons</author>
<author>C J Sparrell</author>
<author>K R Thorisson</author>
</authors>
<title>Integrating simultaneous input from speech, gaze, and hand gestures.</title>
<date>1993</date>
<booktitle>Intelligent Multimedia Interfaces,</booktitle>
<pages>257--276</pages>
<editor>In M. T. Maybury (ed.)</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="4396" citStr="Koons et al 1993" startWordPosition="667" endWordPosition="670">re passed on to a bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration. However, while this approach provides an efficient solution for a broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach. A wide range of potential multimodal utterances fal</context>
</contexts>
<marker>Koons, Sparrell, Thorisson, 1993</marker>
<rawString>Koons, D. B., C. J. Sparrell, and K. R. Thorisson. 1993. Integrating simultaneous input from speech, gaze, and hand gestures. In M. T. Maybury (ed.) Intelligent Multimedia Interfaces, MIT Press, 257-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lakin</author>
</authors>
<title>Spatial parsing for visual languages.</title>
<date>1986</date>
<pages>35--85</pages>
<editor>In S. K. Chang, T. Ichikawa, and P. A. Ligomenides (ed.$), Visual Languages.</editor>
<publisher>Plenum Press,</publisher>
<contexts>
<context position="6564" citStr="Lakin 1986" startWordPosition="1025" endWordPosition="1026">ic orientation, a user might draw the entity and then draw an arrow leading out from it (Figure 3 (a)). To specify a movement, the user might draw an arrow indicating the extent of the move and indicate departure and arrival times by writing expressions at the base and head (Figure 3 (b)). These • • • (a) Figure 3: Complex Unimodal Gestures are specific examples of the more general problem of visual parsing, which has been a focus of attention in research on visual programming and pen-based interfaces for the creation of complex graphical objects such as mathematical equations and flowcharts (Lakin 1986, Wittenburg et all991, Helm et al 1991, Crimi et al 1995). The approach of Johnston et al 1997 also faces fundamental architectural problems. The multimodal integration strategy is hard-coded into the integration agent and there is no isolatable statement of the rules and constraints independent of the code itself. As the range of multimodal utterances supported is extended, it becomes essential that there be a declarative statement of the grammar of multimodal utterances, separate from the algorithms and mechanisms of parsing. This will enable system developers to describe integration strate</context>
<context position="25970" citStr="Lakin (1986)" startWordPosition="4224" endWordPosition="4225">and iterative development of multimodal systems. The system has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally, and incorporates the results of empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It is currently undergoing extensive user testing and evaluation (McGee et al 1998). Previous work on grammars and parsing for multidimensional languages has focused on two dimensional graphical expressions such as mathematical equations, flowcharts, and visual programming languages. Lakin (1986) lays out many of the initial issues in parsing for two-dimensional drawings and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules into a constraint logic programming language. Crimi et al (1991) utilize a similar relation grammar formalism in which a sentence consists of a multiset of objects and relations among them. Their rules are also aug</context>
</contexts>
<marker>Lakin, 1986</marker>
<rawString>Lakin, F. 1986. Spatial parsing for visual languages. In S. K. Chang, T. Ichikawa, and P. A. Ligomenides (ed.$), Visual Languages. Plenum Press, 35-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McGee</author>
<author>P R Co-hen</author>
<author>S L Oviatt</author>
</authors>
<title>Confirmation in multimodal systems.</title>
<date>1998</date>
<booktitle>In Proceedings of 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25756" citStr="McGee et al 1998" startWordPosition="4192" endWordPosition="4195">uits their nature. The architecture is sufficiently general to support other input modes and devices including 3D gestural input. The declarative statement of multimodal integration strategies enables rapid prototyping and iterative development of multimodal systems. The system has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally, and incorporates the results of empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It is currently undergoing extensive user testing and evaluation (McGee et al 1998). Previous work on grammars and parsing for multidimensional languages has focused on two dimensional graphical expressions such as mathematical equations, flowcharts, and visual programming languages. Lakin (1986) lays out many of the initial issues in parsing for two-dimensional drawings and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules </context>
</contexts>
<marker>McGee, Co-hen, Oviatt, 1998</marker>
<rawString>McGee, D., P. R. Co-hen, S. L. Oviatt. 1998. Confirmation in multimodal systems. In Proceedings of 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Neal</author>
<author>S C Shapiro</author>
</authors>
<title>Intelligent multimedia interface technology.</title>
<date>1991</date>
<booktitle>Intelligent User Interfaces,</booktitle>
<pages>45--68</pages>
<editor>In J. W. Sullivan and S. W. Tyler (eds.)</editor>
<publisher>ACM Press, Addison Wesley,</publisher>
<location>New York,</location>
<contexts>
<context position="4378" citStr="Neal and Shapiro 1991" startWordPosition="663" endWordPosition="666">nds to execute. These are passed on to a bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration. However, while this approach provides an efficient solution for a broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach. A wide range of potential multimo</context>
</contexts>
<marker>Neal, Shapiro, 1991</marker>
<rawString>Neal, J. G., and S. C. Shapiro. 1991. Intelligent multimedia interface technology. In J. W. Sullivan and S. W. Tyler (eds.) Intelligent User Interfaces, ACM Press, Addison Wesley, New York, 45-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal interfaces for dynamic interactive maps.</title>
<date>1996</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="1604" citStr="Oviatt 1996" startWordPosition="232" endWordPosition="234">re stated in a high level unificationbased rule formalism supporting rapid prototyping and iterative development of multimodal systems. 1 Introduction Multimodal interfaces enable more natural and efficient interaction between humans and machines by providing multiple channels through which input or output may pass. Our concern here is with multimodal input, such as interfaces which support simultaneous input from speech and pen. Such interfaces have clear task performance and user preference advantages over speech only interfaces, in particular for spatial tasks such as those involving maps (Oviatt 1996). Our focus here is on the integration of input from multiple modes and the role this plays in the segmentation and parsing of natural human input. In the examples given here, the modes are speech and pen, but the architecture described is more general in that it can support more than two input modes and modes of other types such as 3D gestural input. Our multimodal interface technology is implemented in QuickSet (Cohen et al 1997), a working system which supports dynamic interaction with maps and other complex visual displays. The initial applications of QuickSet are: setting up and interacti</context>
<context position="5118" citStr="Oviatt 1996" startWordPosition="778" endWordPosition="779">e. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration. However, while this approach provides an efficient solution for a broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach. A wide range of potential multimodal utterances fall outside the expressive potential of the previous architecture. Empirical studies of multimodal interaction (Oviatt 1996), utilizing wizard-of-oz techniques, have shown that when users are free to interact with any combination of speech and pen, a single spoken utterance maybe associated with more than one gesture. For example, a number of deictic pointing gestures may be associated with a single spoken utterance: &apos;calculate distance from here to here&apos;, &apos;put that there&apos;, &apos;move this team to here and prepare to rescue residents from this building&apos;. Speech may also be combined with a series of gestures of different types: the user circles a vehicle on the map, says &apos;follow this route&apos;, and draws an arrow indicating</context>
<context position="25651" citStr="Oviatt 1996" startWordPosition="4176" endWordPosition="4177">edges or in the rule schemata, allowing individual phenomena to be described in the way that best suits their nature. The architecture is sufficiently general to support other input modes and devices including 3D gestural input. The declarative statement of multimodal integration strategies enables rapid prototyping and iterative development of multimodal systems. The system has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally, and incorporates the results of empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It is currently undergoing extensive user testing and evaluation (McGee et al 1998). Previous work on grammars and parsing for multidimensional languages has focused on two dimensional graphical expressions such as mathematical equations, flowcharts, and visual programming languages. Lakin (1986) lays out many of the initial issues in parsing for two-dimensional drawings and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules</context>
<context position="28050" citStr="Oviatt 1996" startWordPosition="4536" endWordPosition="4537">ns in the grammar formalism which filter out inappropriate combinations before they are considered. Wittenburg (1996) addresses the complexity issue by adding top-down predictive information to the parsing process. This work is fundamentally different from all of these approaches in that it focuses on multimodal systems, and this has significant implications in terms of computational viability. The task differs greatly from parsing of mathematical equations, flowcharts, and other complex graphical expressions in that the number of elements to be parsed is far smaller. Empirical investigation (Oviatt 1996, Oviatt et al 1997) has shown that multimodal utterances rarely contain more than two or three elements. Each of those elements may have multiple interpretations, but the overall number of lexical edges remains sufficiently small to enable fast processing of all the potential combinations. Also, the intersection constraint on combining edges limits the impact of the multiple interpretations of each piece of input. The deployment of this architecture in an implemented system supporting real time spoken and gestural interaction with a dynamic map provides evidence of its computational viability</context>
</contexts>
<marker>Oviatt, 1996</marker>
<rawString>Oviatt, S. L. 1996. Multimodal interfaces for dynamic interactive maps. In Proceedings of Conference on Human Factors in Computing Systems, 95-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>A DeAngeli</author>
<author>K Kuhn</author>
</authors>
<title>Integration and synchronization of input modes during multimodal human-computer interaction.</title>
<date>1997</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems,</booktitle>
<pages>415--422</pages>
<contexts>
<context position="17024" citStr="Oviatt et al 1997" startWordPosition="2740" endWordPosition="2743">new edge is created whose category is the value of lhs and whose ID set consists of the union of the ID sets assigned to the two input edges. Constraints require certain temporal and spatial relationships to hold between edges. Complex constraints can be formed using the basic logical operators V , A, and The temporal constraint in Figure 7, overlap([7],[10]) V follow([7], [10], 4), states that the time of the speech [7] must either overlap with or start within four seconds of the time of the gesture [10]. This temporal constraint is based on empirical investigation of multimodal interaction (Oviatt et al 1997). Spatial constraints are used for combinations of gestural inputs. For example, close_to(X, Y) requires two gestures to be a limited distance apart (See Figure 12 below) and contact(X,Y) determines whether the regions occupied by two objects are in contact. The remaining constraints in Figure 7 do not constrain the inputs per se, rather they are used to calculate the time, prob, and modality features for the resulting edge. For example, the constraint combine_prob ([8] , [11], [4]) is used to combine the probabilities of two inputs and assign a joint probability to the resulting edge. In this</context>
<context position="25671" citStr="Oviatt et al 1997" startWordPosition="4178" endWordPosition="4182">he rule schemata, allowing individual phenomena to be described in the way that best suits their nature. The architecture is sufficiently general to support other input modes and devices including 3D gestural input. The declarative statement of multimodal integration strategies enables rapid prototyping and iterative development of multimodal systems. The system has undergone a form of pro-active evaluation in that its design is informed by detailed predictive modeling of how users interact multimodally, and incorporates the results of empirical studies of multimodal interaction (Oviatt 1996, Oviatt et al 1997). It is currently undergoing extensive user testing and evaluation (McGee et al 1998). Previous work on grammars and parsing for multidimensional languages has focused on two dimensional graphical expressions such as mathematical equations, flowcharts, and visual programming languages. Lakin (1986) lays out many of the initial issues in parsing for two-dimensional drawings and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with </context>
<context position="28070" citStr="Oviatt et al 1997" startWordPosition="4538" endWordPosition="4542">mmar formalism which filter out inappropriate combinations before they are considered. Wittenburg (1996) addresses the complexity issue by adding top-down predictive information to the parsing process. This work is fundamentally different from all of these approaches in that it focuses on multimodal systems, and this has significant implications in terms of computational viability. The task differs greatly from parsing of mathematical equations, flowcharts, and other complex graphical expressions in that the number of elements to be parsed is far smaller. Empirical investigation (Oviatt 1996, Oviatt et al 1997) has shown that multimodal utterances rarely contain more than two or three elements. Each of those elements may have multiple interpretations, but the overall number of lexical edges remains sufficiently small to enable fast processing of all the potential combinations. Also, the intersection constraint on combining edges limits the impact of the multiple interpretations of each piece of input. The deployment of this architecture in an implemented system supporting real time spoken and gestural interaction with a dynamic map provides evidence of its computational viability for real tasks. Our</context>
</contexts>
<marker>Oviatt, DeAngeli, Kuhn, 1997</marker>
<rawString>Oviatt, S. L., A. DeAngeli, and K. Kuhn. 1997. Integration and synchronization of input modes during multimodal human-computer interaction. In Proceedings of Conference on Human Factors in Computing Systems, 415-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Pittman</author>
</authors>
<title>Recognizing handwritten text.</title>
<date>1991</date>
<booktitle>In Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;91.</booktitle>
<pages>271--275</pages>
<contexts>
<context position="3374" citStr="Pittman 1991" startWordPosition="514" endWordPosition="515">) ( Bridge .) Figure 1: Multimodal Architecture Figure 2: User Interface They can draw directly on the map and simultaneously issue spoken commands. Different kinds of entities, lines, and areas may be created by drawing the appropriate spatial features and speaking their type; for example, drawing an area and saying &apos;flood zone&apos;. Orders may also be specified; for example, by drawing a line and saying &apos;helicopter follow this route&apos;. The speech signal is routed to an HMM624 based continuous speaker-independent recognizer. The electronic &apos;ink&apos; is routed to a neural net-based gesture recognizer (Pittman 1991). Both generate N-best lists of potential recognition results with associated probabilities. These results are assigned semantic interpretations by natural language processing and gesture interpretation agents respectively. A multimodal integrator agent fields input from the natural language and gesture interpretation agents and selects the appropriate multimodal or unimodal commands to execute. These are passed on to a bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, inte</context>
</contexts>
<marker>Pittman, 1991</marker>
<rawString>Pittman, J. A. 1991. Recognizing handwritten text. In Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;91. 271-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Pollard</author>
<author>I A Sag</author>
</authors>
<title>Information-based syntax and semantics: Volume!, Fundamentals.,</title>
<date>1987</date>
<booktitle>CSLI Lecture Notes Volume 13.</booktitle>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="13680" citStr="Pollard and Sag 1987" startWordPosition="2181" endWordPosition="2184">peech recognition. Multiple commands are persistent edges; they are not removed from the chart after they have participated in the formation of an executable command. They are assigned timeouts and are removed when their alloted time runs out. These &apos;self-destruct&apos; timers are zeroed each time another entity is created, allowing creations to chain together. 3 Unification-based Multimodal Grammar Representation Our grammar representation for multimodal expressions draws on unification-based approaches to syntax and semantics (Shieber 1986) such as Head626 driven phrase structure grammar (HPSG) (Pollard and Sag 1987,1994). Spoken phrases and pen gestures, which are the terminal elements of the multimodal parsing process, are referred to as lexical edges. They are assigned grammatical representations in the form of typed feature structures by the natural language and gesture interpretation agents respectively. For example, the spoken phrase &apos;helicopter&apos; is assigned the representation in Figure 5. cat : unit_type fsTYPE : create_unit content object : [ fsTYPE : unit type : helicopter echelon : vehicle location: [ fsTYPE : point modality: speech time: interval(..... prob : 0.85 Figure 5: Spoken Input Edge T</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, C. J. and I. A. Sag. 1987. Information-based syntax and semantics: Volume!, Fundamentals., CSLI Lecture Notes Volume 13. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-driven phrase structure grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press. Chicago.</publisher>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan Sag. 1994. Head-driven phrase structure grammar. University of Chicago Press. Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Roche</author>
<author>Y Schabes</author>
</authors>
<title>Finite state language processing.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="29812" citStr="Roche and Schabes 1997" startWordPosition="4813" endWordPosition="4816">ich has parallels in the history of syntactic parsing. Initial approaches to multimodal integration were largely algorithmic in nature. The next stage is the formulation of declarative integration rules (phrase structure rules), then comes a shift from rules to representations (lexicalism, categorial and unification-based grammars). The approach outlined here is at representational stage, although rule schemata are still used for constructional meaning. The next phase, which syntax is undergoing, is the compilation of rules and representations back into fast, low-powered finite state devices (Roche and Schabes 1997). At this early stage in the development of multimodal systems, we need a high degree of flexibility. In the future, once it is clearer what needs to be accounted for, the next step will be to explore compilation of multimodal grammars into lower power devices. Our primary areas of future research include refinement of the probability combination scheme for multimodal utterances, exploration of alternative constraint solving strategies, multiple inheritance for rule schemata, maintenance of multimodal dialogue history, and experimentation with 3D input and other combinations of modes. Referenc</context>
</contexts>
<marker>Roche, Schabes, 1997</marker>
<rawString>Roche, E. and Y. Schabes. 1997. Finite state language processing. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>An Introduction to unificationbased approaches to grammar.</title>
<date>1986</date>
<journal>CSLI Lecture Notes</journal>
<volume>4</volume>
<publisher>CSLI, Stanford.</publisher>
<contexts>
<context position="13603" citStr="Shieber 1986" startWordPosition="2170" endWordPosition="2171"> significantly speeds up the creation process and limits reliance on speech recognition. Multiple commands are persistent edges; they are not removed from the chart after they have participated in the formation of an executable command. They are assigned timeouts and are removed when their alloted time runs out. These &apos;self-destruct&apos; timers are zeroed each time another entity is created, allowing creations to chain together. 3 Unification-based Multimodal Grammar Representation Our grammar representation for multimodal expressions draws on unification-based approaches to syntax and semantics (Shieber 1986) such as Head626 driven phrase structure grammar (HPSG) (Pollard and Sag 1987,1994). Spoken phrases and pen gestures, which are the terminal elements of the multimodal parsing process, are referred to as lexical edges. They are assigned grammatical representations in the form of typed feature structures by the natural language and gesture interpretation agents respectively. For example, the spoken phrase &apos;helicopter&apos; is assigned the representation in Figure 5. cat : unit_type fsTYPE : create_unit content object : [ fsTYPE : unit type : helicopter echelon : vehicle location: [ fsTYPE : point mo</context>
<context position="19694" citStr="Shieber 1986" startWordPosition="3165" endWordPosition="3166">egorization to account for the different complementation patterns that verbs and other lexical items require. Just as a verb subcategorizes for its complements, we can think of a lexical edge in the multimodal grammar as subcategorizing for the edges with which it needs to combine. For example, spoken inputs such as &apos;calculate distance from here to here&apos; and &apos;sandbag wall from here to here&apos; (Figure 8) result in edges which subcategorize for two gestures. Their multimodal subcategorization is specified in a list valued subcat feature, implemented using a recursive first/rest feature structure (Shieber 1986:27-32). ^cat : subcat-command fsTYPE create-line content : object : [ color grey i style sandbag : fsTYPE : wall-obj location : rfsTYPE : line 1 L coordlist : [[1], [21]] — fir [cat : spatiol_gesture constraints : [overlap([3],[9]) V f ollow([3], (41A)] st : content : time : [41 [ coord : [1] fsTYPE : Paint]] cat : spattal_gesture subcat : fsTYPE : point] [ coord : [2] first : content : rest time : [5] constraints : [follow([5], [4], 5)] rest : end Figure 8: &apos;Sandbag wall from here to here&apos; The cat feature is subcat_command, indicating that this is an edge with an unsaturated subcategorizatio</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, S. M. 1986. An Introduction to unificationbased approaches to grammar. CSLI Lecture Notes Volume 4. CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Vo</author>
<author>C Wood</author>
</authors>
<title>Building an application framework for speech and pen input integration in multimodal learning interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of ICASSP&apos;96.</booktitle>
<contexts>
<context position="4584" citStr="Vo and Wood 1996" startWordPosition="701" endWordPosition="704">7, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration. However, while this approach provides an efficient solution for a broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach. A wide range of potential multimodal utterances fall outside the expressive potential of the previous architecture. Empirical studies of multimodal interaction (Oviatt 1996), utilizing wizard-of-oz techniques, have shown that when users ar</context>
</contexts>
<marker>Vo, Wood, 1996</marker>
<rawString>Vo, M. T., and C. Wood. 1996. Building an application framework for speech and pen input integration in multimodal learning interfaces. In Proceedings of ICASSP&apos;96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wauchope</author>
</authors>
<title>Eucalyptus: Integrating natural language input with a graphical user interface.</title>
<date>1994</date>
<journal>Naval Research Laboratory, Report</journal>
<pages>5510--94</pages>
<contexts>
<context position="4412" citStr="Wauchope 1994" startWordPosition="671" endWordPosition="672">bridge agent which provides an API to the underlying applications the system is used to control. In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures (Carpenter 1992) representing the semantic contributions of the different modes. This approach overcomes the limitations of previous approaches in that it allows for a full range of gestural input beyond simple deictic pointing gestures. Unlike speech-driven systems (Bolt 1980, Neal and Shapiro 1991, Koons et al 1993, Wauchope 1994), it is fully multimodal in that all elements of the content of a command can be in either mode. Furthermore, compared to related framemerging strategies (Vo and Wood 1996), it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration. However, while this approach provides an efficient solution for a broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach. A wide range of potential multimodal utterances fall outside the ex</context>
</contexts>
<marker>Wauchope, 1994</marker>
<rawString>Wauchope, K. 1994. Eucalyptus: Integrating natural language input with a graphical user interface. Naval Research Laboratory, Report NRL/FR/5510-94-9711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wittenburg</author>
<author>L Weitzman</author>
<author>J Talley</author>
</authors>
<title>Unification-Based grammars and tabular parsing for graphical languages.</title>
<date>1991</date>
<journal>Journal of Visual Languages and Computing</journal>
<pages>2--347</pages>
<contexts>
<context position="26669" citStr="Wittenburg et al (1991)" startWordPosition="4331" endWordPosition="4335"> and utilizes specialized parsers implemented in LISP to parse specific graphical languages. Helm et al (1991) employ a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules into a constraint logic programming language. Crimi et al (1991) utilize a similar relation grammar formalism in which a sentence consists of a multiset of objects and relations among them. Their rules are also augmented with constraints and parsing is provided by a prolog axiomatization. Wittenburg et al (1991) employ a unification-based grammar formalism augmented with functional constraints (F-PATR, Wittenburg 1993), and a bottomup, incremental, Earley-style (Earley 1970) tabular parsing algorithm. All of these approaches face significant difficulties in terms of computational complexity. At worst, an exponential number of combinations of the input elements need to be considered, and the parse table may be of exponential size (Wittenburg et al 1991:365). Efficiency concerns drive Helm et al (1991:111) to adopt a committed choice strategy under which successfully applied productions cannot be backt</context>
</contexts>
<marker>Wittenburg, Weitzman, Talley, 1991</marker>
<rawString>Wittenburg, K., L. Weitzman, and J. Talley. 1991. Unification-Based grammars and tabular parsing for graphical languages. Journal of Visual Languages and Computing 2:347-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Wittenburg</author>
</authors>
<title>F-PATR: Functional constraints for unification-based grammars.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="18364" citStr="Wittenburg 1993" startWordPosition="2945" endWordPosition="2946">resulting edge. Auxiliary features and constraints which are not directly relevant to the discussion will be omitted. The constraints are interpreted using a prolog meta-interpreter. This basic back-tracking constraint satisfaction strategy is simplistic but adequate for current purposes. It could readily be substituted with a more sophisticated constraint solving strategy allowing for more interaction among constraints, default constraints, optimization among a series of constraints, and so on. The addition of functional constraints is common in HPSG and other unification grammar formalisms (Wittenburg 1993). cat : command content : [I) Ihs : modal : [2j time : (3 prob : [4 ccoat elnor .tdicolmocmazinodn : 151 1 dtrl. : mondalit . : 16] time: [7 1 prob: [8 cateepst. l5 ialigeeture dtr2 : mondaillit : 191 time : (101 prob : [II) toverlap([7],(10t V follow([7],(10], 4) combine_prob( 8],[11 , op total-time([71, 10],[3i) assign_rnodalsty(E61, 9], (21) rhs : constraints : 627 4 Multimodal Subcategorization Given that multimodal grammar rules are required to be binary, how can the wide variety of commands in which speech combines with more than one gestural element be accounted for? The solution to thi</context>
<context position="26778" citStr="Wittenburg 1993" startWordPosition="4347" endWordPosition="4348">y a grammatical framework, constrained set grammars, in which constituent structure rules are augmented with spatial constraints. Visual language parsers are build by translation of these rules into a constraint logic programming language. Crimi et al (1991) utilize a similar relation grammar formalism in which a sentence consists of a multiset of objects and relations among them. Their rules are also augmented with constraints and parsing is provided by a prolog axiomatization. Wittenburg et al (1991) employ a unification-based grammar formalism augmented with functional constraints (F-PATR, Wittenburg 1993), and a bottomup, incremental, Earley-style (Earley 1970) tabular parsing algorithm. All of these approaches face significant difficulties in terms of computational complexity. At worst, an exponential number of combinations of the input elements need to be considered, and the parse table may be of exponential size (Wittenburg et al 1991:365). Efficiency concerns drive Helm et al (1991:111) to adopt a committed choice strategy under which successfully applied productions cannot be backtracked over and complex negative and quantificational constraints are used to limit rule application. Wittenb</context>
</contexts>
<marker>Wittenburg, 1993</marker>
<rawString>Wittenburg, K. L. 1993. F-PATR: Functional constraints for unification-based grammars. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wittenburg</author>
</authors>
<title>Predictive parsing for unordered relational languages.</title>
<date>1996</date>
<booktitle>Recent Advances in Parsing Technologies,</booktitle>
<pages>385--407</pages>
<editor>In H. Bunt and M. Tomita (eds.),</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="27556" citStr="Wittenburg (1996)" startWordPosition="4462" endWordPosition="4463">nal complexity. At worst, an exponential number of combinations of the input elements need to be considered, and the parse table may be of exponential size (Wittenburg et al 1991:365). Efficiency concerns drive Helm et al (1991:111) to adopt a committed choice strategy under which successfully applied productions cannot be backtracked over and complex negative and quantificational constraints are used to limit rule application. Wittenburg et al&apos;s parsing mechanism is directed by expander relations in the grammar formalism which filter out inappropriate combinations before they are considered. Wittenburg (1996) addresses the complexity issue by adding top-down predictive information to the parsing process. This work is fundamentally different from all of these approaches in that it focuses on multimodal systems, and this has significant implications in terms of computational viability. The task differs greatly from parsing of mathematical equations, flowcharts, and other complex graphical expressions in that the number of elements to be parsed is far smaller. Empirical investigation (Oviatt 1996, Oviatt et al 1997) has shown that multimodal utterances rarely contain more than two or three elements. </context>
</contexts>
<marker>Wittenburg, 1996</marker>
<rawString>Wittenburg, K. 1996. Predictive parsing for unordered relational languages. In H. Bunt and M. Tomita (eds.), Recent Advances in Parsing Technologies, Kluwer, Dordrecht, 385-407.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>