<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000062">
<title confidence="0.986525">
Inducing Frame Semantic Verb Classes from WordNet and LDOCE
</title>
<author confidence="0.802332">
Rebecca Green, Bonnie J. Dorr, and Philip Resnik
*†‡ *† *†
</author>
<affiliation confidence="0.99870525">
Institute for Advanced Computer Studies
Department of Computer Science
College of Information Studies
University of Maryland
</affiliation>
<address confidence="0.888001">
College Park, MD 20742 USA
</address>
<email confidence="0.990531">
{rgreen, bonnie, resnik}@umiacs.umd.edu
</email>
<note confidence="0.902051333333333">
*
†
‡
</note>
<sectionHeader confidence="0.986052" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819529411765">
This paper presents SemFrame, a system
that induces frame semantic verb classes
from WordNet and LDOCE. Semantic
frames are thought to have significant
potential in resolving the paraphrase
problem challenging many language-
based applications.
When compared to the handcrafted
FrameNet, SemFrame achieves its best
recall-precision balance with 83.2%
recall (based on SemFrame&apos;s coverage of
FrameNet frames) and 73.8% precision
(based on SemFrame verbs’ semantic
relatedness to frame-evoking verbs). The
next best performing semantic verb
classes achieve 56.9% recall and 55.0%
precision.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998572191780822">
Semantic content can almost always be expressed
in a variety of ways. Lexical synonymy (She
esteemed him highly vs. She respected him
greatly), syntactic variation (John paid the bill vs.
The bill was paid by John), overlapping meanings
(Anna turned at Elm vs. Anna rounded the corner
at Elm), and other phenomena interact to produce
a broad range of choices for most language
generation tasks (Hirst, 2003; Rinaldi et al., 2003;
Kozlowski et al., 2003). At the same time, natural
language understanding must recognize what
remains constant across paraphrases.
The paraphrase phenomenon affects many
computational linguistic applications, including
information retrieval, information extraction,
question-answering, and machine translation. For
example, documents that express the same
content using different linguistic means should
typically be retrieved for the same queries.
Information sought to answer a question needs to
be recognized no matter how it is expressed.
Semantic frames (Fillmore, 1982; Fillmore
and Atkins, 1992) address the paraphrase problem
through their slot-and-filler templates, representing
frequently occurring, structured experiences.
Semantic frame types of an intermediate granularity
have the potential to fulfill an interlingua role within
a solution to the paraphrase problem.
Until now, semantic frames have been
generated by hand (as in Fillmore and Atkins,
1992), based on native speaker intuition; the
FrameNet project (http://www.icsi.berkeley.edu/
—framenet; Johnson et al., 2002) now couples this
generation with empirical validation. Only
recently has this project begun to achieve relative
breadth in its inventory of semantic frames. To
have a comprehensive inventory of semantic
frames, however, we need the capacity to generate
semantic frames semi-automatically (the need for
manual post-editing is assumed).
To address these challenges, we have
developed SemFrame, a system that induces
semantic frames automatically. Overall, the
system performs two primary functions: (1)
identification of sets of verb senses that evoke a
common semantic frame (in the sense that lexical
units call forth corresponding conceptual
structures); and (2) identification of the
conceptual structure of semantic frames. This
paper explores the first task of identifying frame
semantic verb classes. These classes have several
types of uses. First, they are the basis for
identifying the internal structure of the frame
proper, as set forth in Green and Dorr, 2004.
Second, they may be used to extend FrameNet.
Third, they support applications needing access to
sets of semantically related words, for example,
text segmentation and word sense disambiguation,
as explored to a limited degree in Green, 2004.
Section 2 presents related research efforts on
developing semantic verb classes. Section 3
summarizes the features of WordNet
(http://www.cogsci.princeton.edu/—wn) and
LDOCE (Procter, 1978) that support the
automatic induction of semantic verb classes, definitions and example sentences often mention
while Section 4 sets forth the approach taken by their participants using semantic-type-like nouns,
SemFrame to accomplish this task. Section 5 thus mapping easily to the corresponding frame
presents a brief synopsis of SemFrame’s results, element. Corpus data, however, are more likely
while Section 6 presents an evaluation of to include instantiated participants, which may
SemFrame’s ability to identify semantic verb not generalize to the frame element. Second,
classes of a FrameNet-like nature. Section 7 lexical resources provide a consistent amount of
summarizes our work and motivates directions for data for word senses, while the amount of data in
further development of SemFrame. a corpus for word senses is likely to vary widely.
</bodyText>
<sectionHeader confidence="0.995547" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999928611111111">
The EAGLES (1998) report on semantic
encoding differentiates between two approaches
to the development of semantic verb classes:
those based on syntactic behavior and those based
on semantic criteria.
Levin (1993) groups verbs based on an
analysis of their syntactic properties, especially
their ability to be expressed in diathesis
alternations; her approach reflects the assumption
that the syntactic behavior of a verb is determined
in large part by its meaning. Verb classes at the
bottom of Levin’s shallow network group
together (quasi-) synonyms, hierarchically related
verbs, and antonyms, alongside verbs with looser
semantic relationships.
The verb categories based on Pantel and Lin
(2002) and Lin and Pantel (2001) are induced
automatically from a large corpus, using an
unsupervised clustering algorithm, based on
syntactic dependency features. The resulting
clusters contain synonyms, hierarchically related
verbs, and antonyms, as well as verbs more
loosely related from the perspective of
paraphrase.
The handcrafted WordNet (Fellbaum, 1998a)
uses the hyperonymy/hyponymy relationship to
structure the English verb lexicon into a semantic
network. Each collection of a top-level node
supplemented by its descendants may be seen as
a semantic verb class.
In all fairness, resolution of the paraphrase
problem is not the explicit goal of most efforts to
build semantic verb classes. However, they can
process some paraphrases through lexical
synonymy, hierarchically related terms, and
antonymy.
</bodyText>
<sectionHeader confidence="0.980018" genericHeader="method">
3 Resources Used in SemFrame
</sectionHeader>
<bodyText confidence="0.99482044117647">
We adopt an approach that relies heavily on
pre-existing lexical resources. Such resources
have several advantages over corpus data in
identifying semantic frames. First, both
Third, lexical resources provide their data in a
more systematic fashion than do corpora.
Most centrally, the syntactic arguments of the
verbs used in a definition often correspond to the
semantic arguments of the verb being defined.
For example, Table 1 gives the definitions of
several verb senses in LDOCE that evoke the
COMMERCIAL TRANSACTION frame, which
includes as its semantic arguments a Buyer, a
Seller, some Merchandise, and Money. Words
corresponding to the Money (money, value), the
Merchandise (property, goods), and the Buyer
(buyer, buyers) are present in, and to some extent
shared across, the definitions; however, no words
corresponding to the Seller are present.
Verb LDOCE Definition
sense
buy 1 to obtain (something) by giving money
(or something else of value)
buy 2 to obtain in exchange for something,
often something of great value
buy 3 to be exchangeable for
purchase 1 to gain (something) at the cost of
effort, suffering, or loss of something
of value
sell 1 to give up (property or goods) to
another for money or other value
sell 2 to offer (goods) for sale
sell 3 to be bought; get a buyer or buyers;
gain a sale
</bodyText>
<tableCaption confidence="0.863841">
Table 1. LDOCE Definitions for Verbs
Evoking the COMMERCIAL TRANSACTION Frame
</tableCaption>
<bodyText confidence="0.9977205625">
Of available machine-readable dictionaries,
LDOCE appears especially useful for this
research. It uses a restricted vocabulary of about
2000 words in its definitions and example
sentences, thus increasing the likelihood that
words with closely related meanings will use
the same words in their definitions and support WordNet verb synsets and LDOCE verb senses
the pattern of discovery envisioned. LDOCE’s relies on finding matches between the data
subject field codes also accomplish some of the available for the verb senses in each resource
same type of grouping as semantic frames. (e.g., other words in the synset; words in
WordNet is a machine-readable lexico- definitions and example sentences; words closely
semantic database whose primary organizational related to these words; and stems of these words).
structure is the synset—a set of synonymous word The similarity measure used is the average of the
senses. A limited number of relationship types proportion of words on each side of the
(e.g., antonymy, hyponymy, meronymy, comparison that are matched in the other. This
troponymy, entailment) also relate synsets within mapping is used both to relate LDOCE verb senses,
a part of speech. (Version 1.7.1 was used.) that map to the same WordNet synset (fig. 3f) and to
Fellbaum (1998b) suggests that relationships translate previously paired WordNet verb synsets
in WordNet “reflect some of the structure of into LDOCE verb sense pairs.
frame semantics” (p. 5). Through the relational In the third stage, the resulting verb sense
structure of WordNet, buy, purchase, sell, and pay pairs are merged into a single data set, retaining
are related together: buy and purchase comprise one only those pairs whose cumulative support
synset; they entail paying and are opposed to sell. exceeds thresholds for either the number of
The relationship of buy, purchase, sell, and supporting data sources or strength of support,
pay to other COMMERCIAL TRANSACTION thus achieving higher precision in the merged
verbs—for example, cost, price, and the demand data set than in the input data sets. Then, the
payment sense of charge—is not made explicit in graph formed by the verb sense pairs in the
WordNet, however. Further, as Roger Chaffin merged data set is analyzed to find the fully
has noted, the specialized vocabulary of, for connected components.
example, tennis (e.g. racket, court, lob) is not co- Finally, these groups of verb senses become
located, but is dispersed across different branches input to a clustering operation (Voorhees, 1986).
of the noun network (Miller, 1998, p. 34). Those groups whose similarity (due to overlap in
</bodyText>
<sectionHeader confidence="0.963492" genericHeader="method">
4 SemFrame Approach
</sectionHeader>
<bodyText confidence="0.992141256410256">
SemFrame gathers evidence about frame
semantic relatedness between verb senses by
analyzing LDOCE and WordNet data from a
variety of perspectives. The overall approach
used is shown in Figure 1. The first stage of
processing extracts pairs of LDOCE and
WordNet verb senses that potentially evoke the
same frame. By exploiting many different clues
to semantic relatedness, we overgenerate these
pairs, favoring recall; subsequent stages improve
the precision of the resulting data.
Figures 2 and 3 give details of the algorithms
for extracting verb pairs based on different types
of evidence. These include: clustering LDOCE
verb senses/WordNet synsets on the basis of
words in their definitions and example sentences
(fig. 2); relating LDOCE verb senses defined in
terms of the same verb (fig. 3a); relating LDOCE
verb senses that share a common stem (fig. 3b);
extracting explicit sense-linking relationships in
LDOCE (fig. 3c); relating verb senses that share
general or specific subject field codes in LDOCE
(fig. 3d); and extracting (direct or extended)
semantic relationships in WordNet (fig. 3e).
In the second stage, mapping between
membership) exceed a threshold are merged
together, thus reducing the number of verb sense
groups. The verb senses within each resulting
group are hypothesized to evoke the same
semantic frame and constitute a frameset.
Extract verb sense
pairs from WordNet
Map WordNet synsets
to LDOCE senses
Merge pairs, filtering out those
not meeting threshold criteria
Build fully-connected verb groups
Cluster related verb groups
Verb sense framesets
</bodyText>
<figureCaption confidence="0.91361775">
Figure 1. Approach for Building
Frame Semantic Verb Classes
Extract verb sense
pairs from LDOCE
</figureCaption>
<bodyText confidence="0.823604666666667">
Input. SW, a set of stop words; M, a set of
(word, stem) pairs; F, a set of (word,
frequency) pairs; DE, a set of
(verb_sense_id, def+ex) pairs, where
def+exd = the set of words in the
definitions and example sentences of
verb_sense_idd
Step 1. forall d E DE, append to def+exd:
verb_sense_idd and remove from
</bodyText>
<construct confidence="0.693718142857143">
def+exd any word w E SW
Step 2. forall d E DE
forall m E M
if wordm exists in def+exd,
substitute stemm for wordm
Step 3. forall f E F
if frequencyf &gt; 1,
</construct>
<equation confidence="0.9615235">
1 ,
wgtwordf .- frequencyf
else if frequencyf == 1,
wgtwordf .01
</equation>
<figureCaption confidence="0.65193625">
Step 4. O ~Voorhees’ average link clustering
algorithm applied to DE, with initial
weights forall t in def+ex set to wgtt
Step 5. forall o E O
return all combinations of two
members from o
Figure 2. Algorithm for Generating
Clustering-based Verb Pairs
</figureCaption>
<sectionHeader confidence="0.999917" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999875">
We explored a range of thresholds in the final
stage of the algorithm. In general, the lower the
</bodyText>
<equation confidence="0.685797">
1
</equation>
<bodyText confidence="0.9958718">
threshold, the looser the verb grouping. The
number of verb senses retained (out of 12,663
non-phrasal verb senses in LDOCE) and the verb
sense groups produced by using these thresholds
are recorded in Table 2.
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999226">
One of our goals is to produce sets of verb senses
capable of extending FrameNet&apos;s coverage while
requiring reasonably little post-editing. This goal
has two subgoals: identifying new frames and
identifying additional lexical units that evoke
</bodyText>
<page confidence="0.611917">
1
</page>
<table confidence="0.9996018">
Threshold Num verb senses Num groups
0.5 6461 1338
1.0 6414 1759
1.5 5607 1421
2.0 5604 1563
</table>
<tableCaption confidence="0.99993">
Table 2. Results of Frame Clustering Process
</tableCaption>
<bodyText confidence="0.976406295454545">
previously recognized frames. We use the hand-
crafted FrameNet, which is of reliably high
precision, as a gold standard for the initial
2
evaluation of SemFrame&apos;s ability to achieve these
subgoals. For the first, we evaluate SemFrame’s
ability to generate frames that correspond to
FrameNet’s frames, reasoning that the system
must be able to identify a large proportion of
known frames if the quality of its output is good
enough to identify new frames. (At this stage we
do not measure the quality of new frames.) For
the second subgoal we can be more concrete: For
frames identified by both systems, we measure the
degree to which the verbs identified by
SemFrame can be shown to evoke those frames,
even if FrameNet has not identified them as
frame-evoking verbs.
FrameNet includes hierarchically organized
frames of varying levels of generality: Some
semantic areas are covered by a general frame,
some by a combination of specific frames, and
some by a mix of general and specific frames.
Because of this variation we determined the
degree to which SemFrame and FrameNet overlap
by automatically finding and comparing
corresponding frames instead of fully equivalent
frames. Frames correspond if the semantic scope
of one frame is included within the semantic
2Certain constraints imposed by FrameNet&apos;s
development strategy restrict its use as a full-fledged
gold standard for evaluating semantic frame induction.
(1) As of summer 2003, only 382 frames had been
identified within the FrameNet project. (2) Low recall
affects not only the set of semantic frames identified
by FrameNet, but also the sets of frame-evoking units
listed for each frame. No verbs are listed for 38.5% of
FrameNet&apos;s frames, while another 13.1% of them list
only 1 or 2 verbs. The comparison here is limited to
the 197 FrameNet frames for which at least one verb
is listed with a counterpart in LDOCE. (3) Some of
For the clustering algorithm used, the clustering FrameNet&apos;s frames are more syntactically than
threshold range is open-ended. The values semantically motivated (e.g., EXPERIENCER-OBJECT,
investigated in the evaluation are fairly low. EXPERIENCER-SUBJECT).
</bodyText>
<figureCaption confidence="0.998772023809524">
a. Relates LDOCE verb senses that are defined in terms of the same verb
Input. D, a set of (verb_sense_id, def_verb) pairs, where def_verbd = the verb in terms of which
verb_sense_idd is defined
Step 1. forall v that exist as def_verb in D, form DVv c: D, by extracting all (verb_sense_id, def_verb)
pairs where v = def_verb
Step 2. remove all DVv for which  |DVv  |&gt; 40
Step 3. forall v that exist as def_verb in D, return all combinations of two members from DVv
b. Relates LDOCE verb senses that share a common stem
Input. D, a set of (verb_sense_id, verb_stem) pairs, where verb_stemd = the stem for the verb on which
verb_sense_idd is based
Step 1. forall m that exist as verb_stem in D, form DVm c: D, by extracting all (verb_sense_id,
verb_stem) pairs where m = verb_stem
Step 2. forall m that exist as verb_stem in D, return all combinations of two members from DVv
c. Extracts explicit sense-linking relationships in LDOCE
Input. D, a set of (verb_sense_id, def) pairs, where defd = the definition for verb_sense_idd
Step 1. forall d E D, if defd contains compare or opposite note, extract related_verb from note; generate
(verb_sense_idd, related_verbd) pair
Step 2. forall d E D, if defd defines verb_sense_idd in terms of a related standalone verb (in BLOCK
CAPS), extract related_verb from definition; generate (verb_sense_idd, related_verbd) pair
Step 3. forall (verb_sense_idd, related_verbd) pairs, if there is only one sense of related_verb d, choose it
and return (verb_sense_idd, related_verb_sense_idd), else apply generalized mapping
algorithm to return (verb_sense_idd, related_verb_sense_idd) pairs where overlap occurs in
the glosses of verb_sense_idd and related_verb_sense_idd
d. Relates verb senses that share general or specific subject field codes in LDOCE
Input. D, a set of (verb_sense_id, subject_code) pairs, where subject_coded = any 2- or 4-character
subject field code assigned to verb_sense_id
Step 1. forall c that exist as subject_code in D, form DVc c: D, by extracting all (verb_sense_id,
subject_code) pairs where c = subject_code
Step 2. forall c that exist as subject_code in D,
return all combinations of two members from DVv
e. Extracts (direct or extended) semantic relationships in WordNet
Input. WordNet data file for verb synsets
Step 1. forall synset lines in input file
return (synset, related_synset) pairs for all synsets directly related through hyponymy,
antonymy, entailment, or cause_to relationships in WordNet
(for extended relationship pairs, also return (synset, related_synset) pairs for all synsets within
hyponymy tree, i.e., no matter how many levels removed)
f. Relates LDOCE verb senses that map to the same WordNet synset
Input. mapping of LDOCE verb senses to WordNet synsets
Step 1. forall lines in input file
return all combinations of two LDOCE verb senses mapped to the same WordNetłsynset
Figure 3. Algorithms for Generating Non-clustering-based Verb Pairs
</figureCaption>
<bodyText confidence="0.9993718">
scope of the other frame or if the semantic scopes SemFrame’s verb classes list specific LDOCE
of the two frames have significant overlap. Since verb senses. In extending FrameNet, verbs from
FrameNet lists evoking words, without SemFrame would be word-sense-disambiguated
specification of word sense, the comparison was in the same way that FrameNet verbs currently
done on the word level rather than on the word are, through the correspondence of lexeme and
sense level, as if LDOCE verb senses were not frame.
specified in SemFrame. However, it is clearly Incompleteness in the listing of evoking verbs
specific word senses that evoke frames, and in FrameNet and SemFrame precludes a straight-
forward detection of correspondences between incrust, and ornament. Two of the verbs—adorn
their frames. Instead, correspondence between and decorate—are shared. In addition, the frame
FrameNet and SemFrame frames is established names are semantically related through a
using either of two somewhat indirect approaches. WordNet synset consisting of decorate, adorn
In the first approach, a SemFrame frame is (which CatVar relates to ADORNING), grace,
deemed to correspond to a FrameNet frame if the ornament (which CatVar relates to
two frames meet both a minimal-overlap ORNAMENTATION), embellish, and beautify. The
criterion (i.e., there is some, perhaps small, two frames are therefore designated as
overlap between the FrameNet and SemFrame corresponding frames by meeting both the
framesets) and a frame-name-relatedness minimal-overlap and the frame-name relatedness
criterion. The minimal-overlap criterion is met if criteria.
either of two conditions is met: (1) If the In the second approach, a SemFrame frame is
FrameNet frame lists four or fewer verbs (true of deemed to correspond to a FrameNet frame if the
over one-third of the FrameNet frames that list two frames meet either of two relatively stringent
associated verbs), minimal overlap occurs when verb overlap criteria, the majority-match criterion
any one verb associated with the FrameNet frame or the majority-related criterion, in which case
matches a verb associated with a SemFrame examination of frame names is unnecessary.
frame. (2) If the FrameNet frame lists five or The majority-match criterion is met if the set
more verbs, minimal overlap occurs when two or of verbs shared by FrameNet and SemFrame
more verbs in the FrameNet frame are matched by framesets account for half or more of the verbs in
verbs in the SemFrame frame. either frameset. For example, the APPLY_HEAT
The looseness of the minimal overlap frame in FrameNet includes 22 verbs: bake,
criterion is tightened by also requiring that the blanch, boil, braise, broil, brown, char, coddle,
names of the FrameNet and SemFrame frames be cook, fry, grill, microwave, parboil, poach, roast,
closely related. Establishing this frame-name saute, scald, simmer, steam, steep, stew, and
relatedness involves identifying individual toast, while the BOILING frame in SemFrame
components of each frame name and augmenting includes 7 verbs: boil, coddle, jug, parboil,
</bodyText>
<page confidence="0.791499">
3
</page>
<bodyText confidence="0.955910292682927">
this set with morphological variants from CatVar poach, seethe, and simmer. Five of these
(Habash and Dorr 2003). The resulting set for verbs—boil, coddle, parboil, poach, and
each FrameNet and SemFrame frame name is simmer—are shared across the two frames and
then searched in both the noun and verb WordNet constitute over half of the SemFrame frameset.
networks to find all the synsets that might Therefore the two frames are deemed to
correspond to the frame name. To these sets are correspond by meeting the majority-match
also added all synsets directly related to the criterion.
synsets corresponding to the frame names. If the The majority-related criterion is met if half or
resulting set of synsets gathered for a FrameNet more of the verbs from the SemFrame frame are
frame name intersects with the set of synsets semantically related to verbs from the FrameNet
gathered for a SemFrame frame name, the two frame (that is, if the precision of the SemFrame
frame names are deemed to be semantically verb set is at least 0.5). To evaluate this criterion,
related. each FrameNet and SemFrame verb is associated
For example, the FrameNet ADORNING frame with the WordNet verb synsets it occurs in,
contains 17 verbs: adorn, blanket, cloak, coat, augmented by the synsets to which the initial sets
cover, deck, decorate, dot, encircle, envelop, of synsets are directly related. If the sets of
festoon, fill, film, line, pave, stud, and wreathe. synsets corresponding to two verbs share one or
The SemFrame ORNAMENTATION frame contains more synsets, the two verbs are deemed to be
12 verbs: adorn, caparison, decorate, embellish, semantically related. This process is extended
embroider, garland, garnish, gild, grace, hang, one further level, such that a SemFrame verb
found by this process to be semantically related to
a SemFrame verb, whose semantic relationship to
3 a FrameNet verb has already been established,
will also be designated a frame-evoking verb. If
half or more of the verbs listed for a SemFrame
frame are established as evoking the same frame
as the list of WordNet verbs, then the FrameNet
All SemFrame frame names are nouns. (See
Green and Dorr, 2004 for an explanation of their
selection.) FrameNet frame names (e.g., ABUNDANCE,
ACTIVITY_START, CAUSE_TO_BE_WET,
INCHOATIVE_ATTACHING), however, exhibit
considerable variation.
and SemFrame frames are hypothesized to bound on the task, i.e., 100% recall and 100%
correspond through the majority-related criterion. precision. The Lin &amp; Pantel results are here a
For example, the FrameNet ABUNDANCE lower bound for automatically induced semantic
frame includes 4 verbs: crawl, swarm, teem, and verb classes and probably reflect the limitations of
throng. The SemFrame FLOW frame likewise using only corpus data. Among efforts to develop
includes 4 verbs: pour, teem, stream, and semantic verb classes, SemFrame’s results
pullulate. Only one verb—teem—is shared, so correspond more closely to semantic frames than
the majority-match criterion is not met, nor is the do others.
related-frame-name criterion met, as the frame
names are not semantically related. The majority-
related criterion, however, is met through a
WordNet verb synset that includes pour, swarm,
stream, teem, and pullulate.
Of the 197 FrameNet frames that include at
least one LDOCE verb, 175 were found to have a
corresponding SemFrame frame. But this 88.8%
recall level should be balanced against the
precision ratio of SemFrame verb framesets.
After all, we could get 100% recall by listing all
verbs in every SemFrame frame.
The majority-related function computes the
precision ratio of the SemFrame frame for each
pair of FrameNet and SemFrame frames being
compared. By modifying the minimum precision
threshold, the balance between recall and
precision, as measured using F-score, can be
investigated. The best balance for the SemFrame
version is based on a clustering threshold of 2.0
and a minimum precision threshold of 0.4, which
yields a recall of 83.2% and overall precision of
73.8%.
To interpret these results meaningfully, one
would like to know if SemFrame achieves more
FrameNet-like results than do other available verb
category data, more specifically the 258 verb
classes from Levin, the 357 semantic verb classes
of WordNet 1.7.1, or the 272 verb clusters of Lin
and Pantel, as described in Section 2.
For purposes of comparison with FrameNet,
Levin’s verb class names have been hand-edited
to isolate the word that best captures the semantic
sense of the class; the name of a WordNet-based
frame is taken from the words for the root-level
synset; and the name of each Lin and Pantel
cluster is taken to be the first verb in the cluster.4
Evaluation results for the best balance
between recall and precision (i.e., the maximum
F-score) of the four comparisons are summarized
in Table 3. FrameNet itself constitutes the upper
</bodyText>
<footnote confidence="0.665922666666667">
4Lin and Pantel have taken a similar approach,
“naming” their verb clusters by the first three verbs
listed for a cluster, i.e., the three most similar verbs.
</footnote>
<table confidence="0.8783734">
Semantic verb Precision Recall Precision
classes threshold
at max F-
score
SemFrame 0.40 0.832 0.738
Levin 0.20 0.569 0.550
WordNet 0.15 0.528 0.466
Lin &amp; Pantel 0.15 0.472 0.407
Table 3. Best Recall-Precision Balance
When Compared with FrameNet
</table>
<sectionHeader confidence="0.993519" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999842">
We have demonstrated that sets of verbs evoking
a common semantic frame can be induced from
existing lexical tools. In a head-to-head
comparison with frames in FrameNet, the frame
semantic verb classes developed by the
SemFrame approach achieve a recall of 83.2%
and the verbs listed for frames achieve a precision
of 73.8%; these results far outpace those of other
semantic verb classes. On a practical level, a
large number of frame semantic verb classes have
been identified. Associated with clustering
threshold 1.5 are 1421 verb classes, averaging
14.1 WordNet verb synsets. Associated with
clustering threshold 2.0 are 1563 verb classes,
averaging 6.6 WordNet verb synsets.
Despite these promising results, we are
limited by the scope of our input data set. While
LDOCE and WordNet data are generally of high
quality, the relative sparseness of these resources
has an adverse impact on recall. In addition, the
mapping technique used for picking out
corresponding word senses in WordNet and
LDOCE is shallow, thus constraining the recall
and precision of SemFrame outputs. Finally, the
multi-step process of merging smaller verb groups
into verb groups that are intended to correspond
to frames sometimes fails to achieve an
appropriate degree of correspondence (all the verb
classes discovered are not distinct).
In our future work, we will experiment with
the more recent release of WordNet (2.0). This
version provides derivational morphology links
between nouns and verbs, which will promote far
greater precision in the linking of verb senses
based on morphology than was possible in our
initial implementation. Another significant
addition to WordNet 2.0 is the inclusion of
category domains, which co-locate words
pertaining to a subject and perform the same
function as LDOCE&apos;s subject field codes.
Finally, data sparseness issues may be
addressed by supplementing the use of the lexical
resources used here with access to, for example,
the British National Corpus, with its broad
coverage and carefully-checked parse trees.
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8618892">
This research has been supported in part by a
National Science Foundation Graduate Research
Fellowship NSF ITR grant #IIS-0326553, and
NSF CISE Research Infrastructure Award
EIA0130422.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999636734939759">
Boguraev, Bran and Ted Briscoe. 1989. Introduction. In
B. Boguraev and T. Briscoe (Eds.), Computational
Lexicography for Natural Language Processing, 1-
40. London: Longman.
EAGLES Lexicon Interest Group. 1998. EAGLES
Preliminary Recommendations on Semantic
Encoding: Interim Report, &lt;http://
www.ilc.cnr.it/EAGLES96/rep2/ rep2.html&gt;.
Fellbaum, Christiane (Ed.). 1998a. WordNet: An
Electronic Lexical Database. Cambridge, MA:
The MIT Press.
Fellbaum, Christiane. 1998b. Introduction. In C.
Fellbaum, 1998a, 1-17.
Fillmore, Charles J. 1982. Frame semantics. In
Linguistics in the Morning Calm, 111-137. Seoul:
Hanshin.
Fillmore, Charles J. and B. T. S. Atkins. 1992.
Towards a frame-based lexicon: The semantics of
RISK and its neighbors. In A. Lehrer and E. F.
Kittay (Eds.), Frames, Fields, and Contrasts, 75-
102. Hillsdale, NJ: Erlbaum.
Green, Rebecca. 2004. Inducing Semantic Frames
from Lexical Resources. Ph.D. dissertation,
University of Maryland.
Green, Rebecca and Bonnie J. Dorr. 2004. Inducing A
Semantic Frame Lexicon from WordNet Data. In
Proceedings of the 2nd Workshop on Text
Meaning and Interpretation (ACL 2004).
Habash, Nizar and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
North American Association for Computational
Linguistics, 96-102.
Hirst, Graeme. 2003. Paraphrasing paraphrased.
Keynote address for The Second International
Workshop on Paraphrasing: Paraphrase
Acquisition and Applications, ACL 2003,
&lt;http://nlp.nagaokaut.ac.jp/IWP2003/pdf/
Hirst-slides.pdf&gt;.
Johnson, Christopher R., Charles J. Fillmore,
Miriam R. L. Petruck, Collin F. Baker,
Michael Ellsworth, Josef Ruppenhofer, and
Esther J. Wood. 2002. FrameNet: Theory and
Practice, version 1 . 0 ,
&lt; h t t p : / / w w w . i c s i . b e r k e l e y . e d u /
~framenet/book/book.html&gt;.
Kozlowski, Raymond, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of
single-sentence paraphrases from
predicate/argument structure using
lexico-grammatical resources. In The Second
International Workshop on Paraphrasing:
Paraphrase Acquisition and Applications
(IWP2003), ACL 2003, 1-8.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
Chicago: University of Chicago Press.
Lin, Dekang and Patrick Pantel. 2001. Induction of
semantic classes from natural language text. In
Proceedings of ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, 317-322.
Litkowski, Ken. 2004. Senseval-3 task: Word-sense
disambiguation of WordNet glosses,
&lt;http://www.clres.com/SensWNDisamb.html&gt;.
Miller, George A. 1998. Nouns in WordNet. In C.
Fellbaum, 1998a, 23-67.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the
Eighth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, 613-
619.
Procter, Paul (Ed.). 1978. Longman Dictionary of
Contemporary English. Longman Group Ltd.,
Essex, UK.
Rinaldi, Fabio, James Dowdall, Kaarel Kaljurand,
Michael Hess, and Diego Mollá. 2003. Exploiting
paraphrases in a question answering system. In
The Second International Workshop on
Paraphrasing: Paraphrase Acquisition and
Applications (IWP2003), ACL 2003, 25-32.
Voorhees, Ellen. 1986. Implementing agglomerative
hierarchic clustering algorithms for use in
document retrieval. Information Processing &amp;
Management 22/6: 465-476.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493804">
<title confidence="0.995494">Inducing Frame Semantic Verb Classes from WordNet and LDOCE</title>
<author confidence="0.989131">Bonnie J Dorr Green</author>
<author confidence="0.989131">Resnik</author>
<affiliation confidence="0.9954334">Institute for Advanced Computer Studies Department of Computer Science College of Information Studies University of Maryland</affiliation>
<address confidence="0.999912">College Park, MD 20742 USA</address>
<email confidence="0.997232">rgreen@umiacs.umd.edu</email>
<email confidence="0.997232">bonnie@umiacs.umd.edu</email>
<email confidence="0.997232">resnik@umiacs.umd.edu</email>
<abstract confidence="0.944227761904762">This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE. Semantic frames are thought to have significant potential in resolving the paraphrase problem challenging many languagebased applications. When compared to the handcrafted FrameNet, SemFrame achieves its best recall-precision balance with 83.2% recall (based on SemFrame&apos;s coverage of FrameNet frames) and 73.8% precision (based on SemFrame verbs’ semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bran Boguraev</author>
<author>Ted Briscoe</author>
</authors>
<date>1989</date>
<booktitle>Introduction. In B. Boguraev and T. Briscoe (Eds.), Computational Lexicography for Natural Language Processing,</booktitle>
<pages>1--40</pages>
<publisher>Longman.</publisher>
<location>London:</location>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Boguraev, Bran and Ted Briscoe. 1989. Introduction. In B. Boguraev and T. Briscoe (Eds.), Computational Lexicography for Natural Language Processing, 1-40. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES Lexicon Interest Group</author>
</authors>
<date>1998</date>
<booktitle>EAGLES Preliminary Recommendations on Semantic Encoding: Interim Report, www.ilc.cnr.it/EAGLES96/rep2/ rep2.html&gt;.</booktitle>
<marker>Group, 1998</marker>
<rawString>EAGLES Lexicon Interest Group. 1998. EAGLES Preliminary Recommendations on Semantic Encoding: Interim Report, &lt;http:// www.ilc.cnr.it/EAGLES96/rep2/ rep2.html&gt;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="5780" citStr="Fellbaum, 1998" startWordPosition="831" endWordPosition="832">ge part by its meaning. Verb classes at the bottom of Levin’s shallow network group together (quasi-) synonyms, hierarchically related verbs, and antonyms, alongside verbs with looser semantic relationships. The verb categories based on Pantel and Lin (2002) and Lin and Pantel (2001) are induced automatically from a large corpus, using an unsupervised clustering algorithm, based on syntactic dependency features. The resulting clusters contain synonyms, hierarchically related verbs, and antonyms, as well as verbs more loosely related from the perspective of paraphrase. The handcrafted WordNet (Fellbaum, 1998a) uses the hyperonymy/hyponymy relationship to structure the English verb lexicon into a semantic network. Each collection of a top-level node supplemented by its descendants may be seen as a semantic verb class. In all fairness, resolution of the paraphrase problem is not the explicit goal of most efforts to build semantic verb classes. However, they can process some paraphrases through lexical synonymy, hierarchically related terms, and antonymy. 3 Resources Used in SemFrame We adopt an approach that relies heavily on pre-existing lexical resources. Such resources have several advantages ov</context>
<context position="8957" citStr="Fellbaum (1998" startWordPosition="1335" endWordPosition="1336">d example sentences; words closely semantic database whose primary organizational related to these words; and stems of these words). structure is the synset—a set of synonymous word The similarity measure used is the average of the senses. A limited number of relationship types proportion of words on each side of the (e.g., antonymy, hyponymy, meronymy, comparison that are matched in the other. This troponymy, entailment) also relate synsets within mapping is used both to relate LDOCE verb senses, a part of speech. (Version 1.7.1 was used.) that map to the same WordNet synset (fig. 3f) and to Fellbaum (1998b) suggests that relationships translate previously paired WordNet verb synsets in WordNet “reflect some of the structure of into LDOCE verb sense pairs. frame semantics” (p. 5). Through the relational In the third stage, the resulting verb sense structure of WordNet, buy, purchase, sell, and pay pairs are merged into a single data set, retaining are related together: buy and purchase comprise one only those pairs whose cumulative support synset; they entail paying and are opposed to sell. exceeds thresholds for either the number of The relationship of buy, purchase, sell, and supporting data </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane (Ed.). 1998a. WordNet: An Electronic Lexical Database. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<journal>Introduction. In C. Fellbaum,</journal>
<volume>1998</volume>
<pages>1--17</pages>
<contexts>
<context position="5780" citStr="Fellbaum, 1998" startWordPosition="831" endWordPosition="832">ge part by its meaning. Verb classes at the bottom of Levin’s shallow network group together (quasi-) synonyms, hierarchically related verbs, and antonyms, alongside verbs with looser semantic relationships. The verb categories based on Pantel and Lin (2002) and Lin and Pantel (2001) are induced automatically from a large corpus, using an unsupervised clustering algorithm, based on syntactic dependency features. The resulting clusters contain synonyms, hierarchically related verbs, and antonyms, as well as verbs more loosely related from the perspective of paraphrase. The handcrafted WordNet (Fellbaum, 1998a) uses the hyperonymy/hyponymy relationship to structure the English verb lexicon into a semantic network. Each collection of a top-level node supplemented by its descendants may be seen as a semantic verb class. In all fairness, resolution of the paraphrase problem is not the explicit goal of most efforts to build semantic verb classes. However, they can process some paraphrases through lexical synonymy, hierarchically related terms, and antonymy. 3 Resources Used in SemFrame We adopt an approach that relies heavily on pre-existing lexical resources. Such resources have several advantages ov</context>
<context position="8957" citStr="Fellbaum (1998" startWordPosition="1335" endWordPosition="1336">d example sentences; words closely semantic database whose primary organizational related to these words; and stems of these words). structure is the synset—a set of synonymous word The similarity measure used is the average of the senses. A limited number of relationship types proportion of words on each side of the (e.g., antonymy, hyponymy, meronymy, comparison that are matched in the other. This troponymy, entailment) also relate synsets within mapping is used both to relate LDOCE verb senses, a part of speech. (Version 1.7.1 was used.) that map to the same WordNet synset (fig. 3f) and to Fellbaum (1998b) suggests that relationships translate previously paired WordNet verb synsets in WordNet “reflect some of the structure of into LDOCE verb sense pairs. frame semantics” (p. 5). Through the relational In the third stage, the resulting verb sense structure of WordNet, buy, purchase, sell, and pay pairs are merged into a single data set, retaining are related together: buy and purchase comprise one only those pairs whose cumulative support synset; they entail paying and are opposed to sell. exceeds thresholds for either the number of The relationship of buy, purchase, sell, and supporting data </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane. 1998b. Introduction. In C. Fellbaum, 1998a, 1-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the Morning Calm,</booktitle>
<pages>111--137</pages>
<publisher>Seoul: Hanshin.</publisher>
<contexts>
<context position="1932" citStr="Fillmore, 1982" startWordPosition="275" endWordPosition="276">sks (Hirst, 2003; Rinaldi et al., 2003; Kozlowski et al., 2003). At the same time, natural language understanding must recognize what remains constant across paraphrases. The paraphrase phenomenon affects many computational linguistic applications, including information retrieval, information extraction, question-answering, and machine translation. For example, documents that express the same content using different linguistic means should typically be retrieved for the same queries. Information sought to answer a question needs to be recognized no matter how it is expressed. Semantic frames (Fillmore, 1982; Fillmore and Atkins, 1992) address the paraphrase problem through their slot-and-filler templates, representing frequently occurring, structured experiences. Semantic frame types of an intermediate granularity have the potential to fulfill an interlingua role within a solution to the paraphrase problem. Until now, semantic frames have been generated by hand (as in Fillmore and Atkins, 1992), based on native speaker intuition; the FrameNet project (http://www.icsi.berkeley.edu/ —framenet; Johnson et al., 2002) now couples this generation with empirical validation. Only recently has this proje</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Fillmore, Charles J. 1982. Frame semantics. In Linguistics in the Morning Calm, 111-137. Seoul: Hanshin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>B T S Atkins</author>
</authors>
<title>Towards a frame-based lexicon: The semantics of RISK and its neighbors.</title>
<date>1992</date>
<booktitle>In A. Lehrer</booktitle>
<pages>75--102</pages>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="1960" citStr="Fillmore and Atkins, 1992" startWordPosition="277" endWordPosition="280">; Rinaldi et al., 2003; Kozlowski et al., 2003). At the same time, natural language understanding must recognize what remains constant across paraphrases. The paraphrase phenomenon affects many computational linguistic applications, including information retrieval, information extraction, question-answering, and machine translation. For example, documents that express the same content using different linguistic means should typically be retrieved for the same queries. Information sought to answer a question needs to be recognized no matter how it is expressed. Semantic frames (Fillmore, 1982; Fillmore and Atkins, 1992) address the paraphrase problem through their slot-and-filler templates, representing frequently occurring, structured experiences. Semantic frame types of an intermediate granularity have the potential to fulfill an interlingua role within a solution to the paraphrase problem. Until now, semantic frames have been generated by hand (as in Fillmore and Atkins, 1992), based on native speaker intuition; the FrameNet project (http://www.icsi.berkeley.edu/ —framenet; Johnson et al., 2002) now couples this generation with empirical validation. Only recently has this project begun to achieve relative</context>
</contexts>
<marker>Fillmore, Atkins, 1992</marker>
<rawString>Fillmore, Charles J. and B. T. S. Atkins. 1992. Towards a frame-based lexicon: The semantics of RISK and its neighbors. In A. Lehrer and E. F. Kittay (Eds.), Frames, Fields, and Contrasts, 75-102. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Green</author>
</authors>
<title>Inducing Semantic Frames from Lexical Resources.</title>
<date>2004</date>
<institution>University of Maryland.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="3661" citStr="Green, 2004" startWordPosition="525" endWordPosition="526">units call forth corresponding conceptual structures); and (2) identification of the conceptual structure of semantic frames. This paper explores the first task of identifying frame semantic verb classes. These classes have several types of uses. First, they are the basis for identifying the internal structure of the frame proper, as set forth in Green and Dorr, 2004. Second, they may be used to extend FrameNet. Third, they support applications needing access to sets of semantically related words, for example, text segmentation and word sense disambiguation, as explored to a limited degree in Green, 2004. Section 2 presents related research efforts on developing semantic verb classes. Section 3 summarizes the features of WordNet (http://www.cogsci.princeton.edu/—wn) and LDOCE (Procter, 1978) that support the automatic induction of semantic verb classes, definitions and example sentences often mention while Section 4 sets forth the approach taken by their participants using semantic-type-like nouns, SemFrame to accomplish this task. Section 5 thus mapping easily to the corresponding frame presents a brief synopsis of SemFrame’s results, element. Corpus data, however, are more likely while Sect</context>
</contexts>
<marker>Green, 2004</marker>
<rawString>Green, Rebecca. 2004. Inducing Semantic Frames from Lexical Resources. Ph.D. dissertation, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Green</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Inducing A Semantic Frame Lexicon from WordNet Data.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2nd Workshop on Text Meaning and Interpretation (ACL</booktitle>
<contexts>
<context position="3419" citStr="Green and Dorr, 2004" startWordPosition="486" endWordPosition="489">e challenges, we have developed SemFrame, a system that induces semantic frames automatically. Overall, the system performs two primary functions: (1) identification of sets of verb senses that evoke a common semantic frame (in the sense that lexical units call forth corresponding conceptual structures); and (2) identification of the conceptual structure of semantic frames. This paper explores the first task of identifying frame semantic verb classes. These classes have several types of uses. First, they are the basis for identifying the internal structure of the frame proper, as set forth in Green and Dorr, 2004. Second, they may be used to extend FrameNet. Third, they support applications needing access to sets of semantically related words, for example, text segmentation and word sense disambiguation, as explored to a limited degree in Green, 2004. Section 2 presents related research efforts on developing semantic verb classes. Section 3 summarizes the features of WordNet (http://www.cogsci.princeton.edu/—wn) and LDOCE (Procter, 1978) that support the automatic induction of semantic verb classes, definitions and example sentences often mention while Section 4 sets forth the approach taken by their </context>
<context position="23857" citStr="Green and Dorr, 2004" startWordPosition="3705" endWordPosition="3708">e two verbs are deemed to be 12 verbs: adorn, caparison, decorate, embellish, semantically related. This process is extended embroider, garland, garnish, gild, grace, hang, one further level, such that a SemFrame verb found by this process to be semantically related to a SemFrame verb, whose semantic relationship to 3 a FrameNet verb has already been established, will also be designated a frame-evoking verb. If half or more of the verbs listed for a SemFrame frame are established as evoking the same frame as the list of WordNet verbs, then the FrameNet All SemFrame frame names are nouns. (See Green and Dorr, 2004 for an explanation of their selection.) FrameNet frame names (e.g., ABUNDANCE, ACTIVITY_START, CAUSE_TO_BE_WET, INCHOATIVE_ATTACHING), however, exhibit considerable variation. and SemFrame frames are hypothesized to bound on the task, i.e., 100% recall and 100% correspond through the majority-related criterion. precision. The Lin &amp; Pantel results are here a For example, the FrameNet ABUNDANCE lower bound for automatically induced semantic frame includes 4 verbs: crawl, swarm, teem, and verb classes and probably reflect the limitations of throng. The SemFrame FLOW frame likewise using only cor</context>
</contexts>
<marker>Green, Dorr, 2004</marker>
<rawString>Green, Rebecca and Bonnie J. Dorr. 2004. Inducing A Semantic Frame Lexicon from WordNet Data. In Proceedings of the 2nd Workshop on Text Meaning and Interpretation (ACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Bonnie Dorr</author>
</authors>
<title>A categorial variation database for English.</title>
<date>2003</date>
<booktitle>In Proceedings of North American Association for Computational Linguistics,</booktitle>
<pages>96--102</pages>
<contexts>
<context position="21794" citStr="Habash and Dorr 2003" startWordPosition="3367" endWordPosition="3370">me in FrameNet includes 22 verbs: bake, criterion is tightened by also requiring that the blanch, boil, braise, broil, brown, char, coddle, names of the FrameNet and SemFrame frames be cook, fry, grill, microwave, parboil, poach, roast, closely related. Establishing this frame-name saute, scald, simmer, steam, steep, stew, and relatedness involves identifying individual toast, while the BOILING frame in SemFrame components of each frame name and augmenting includes 7 verbs: boil, coddle, jug, parboil, 3 this set with morphological variants from CatVar poach, seethe, and simmer. Five of these (Habash and Dorr 2003). The resulting set for verbs—boil, coddle, parboil, poach, and each FrameNet and SemFrame frame name is simmer—are shared across the two frames and then searched in both the noun and verb WordNet constitute over half of the SemFrame frameset. networks to find all the synsets that might Therefore the two frames are deemed to correspond to the frame name. To these sets are correspond by meeting the majority-match also added all synsets directly related to the criterion. synsets corresponding to the frame names. If the The majority-related criterion is met if half or resulting set of synsets gat</context>
</contexts>
<marker>Habash, Dorr, 2003</marker>
<rawString>Habash, Nizar and Bonnie Dorr. 2003. A categorial variation database for English. In Proceedings of North American Association for Computational Linguistics, 96-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Paraphrasing paraphrased. Keynote address for</title>
<date>2003</date>
<booktitle>The Second International Workshop on Paraphrasing: Paraphrase Acquisition and Applications, ACL 2003, Hirst-slides.pdf&gt;.</booktitle>
<contexts>
<context position="1334" citStr="Hirst, 2003" startWordPosition="196" endWordPosition="197"> frames) and 73.8% precision (based on SemFrame verbs’ semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision. 1 Introduction Semantic content can almost always be expressed in a variety of ways. Lexical synonymy (She esteemed him highly vs. She respected him greatly), syntactic variation (John paid the bill vs. The bill was paid by John), overlapping meanings (Anna turned at Elm vs. Anna rounded the corner at Elm), and other phenomena interact to produce a broad range of choices for most language generation tasks (Hirst, 2003; Rinaldi et al., 2003; Kozlowski et al., 2003). At the same time, natural language understanding must recognize what remains constant across paraphrases. The paraphrase phenomenon affects many computational linguistic applications, including information retrieval, information extraction, question-answering, and machine translation. For example, documents that express the same content using different linguistic means should typically be retrieved for the same queries. Information sought to answer a question needs to be recognized no matter how it is expressed. Semantic frames (Fillmore, 1982; </context>
</contexts>
<marker>Hirst, 2003</marker>
<rawString>Hirst, Graeme. 2003. Paraphrasing paraphrased. Keynote address for The Second International Workshop on Paraphrasing: Paraphrase Acquisition and Applications, ACL 2003, &lt;http://nlp.nagaokaut.ac.jp/IWP2003/pdf/ Hirst-slides.pdf&gt;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher R Johnson</author>
<author>Charles J Fillmore</author>
<author>Miriam R L Petruck</author>
<author>Collin F Baker</author>
<author>Michael Ellsworth</author>
<author>Josef Ruppenhofer</author>
<author>Esther J Wood</author>
</authors>
<title>FrameNet: Theory and Practice,</title>
<date>2002</date>
<booktitle>version 1 . 0 , &lt; h t t p : / / w w w . i c s i . b e r k e l e y . e d u / ~framenet/book/book.html&gt;.</booktitle>
<contexts>
<context position="2448" citStr="Johnson et al., 2002" startWordPosition="341" endWordPosition="344">o answer a question needs to be recognized no matter how it is expressed. Semantic frames (Fillmore, 1982; Fillmore and Atkins, 1992) address the paraphrase problem through their slot-and-filler templates, representing frequently occurring, structured experiences. Semantic frame types of an intermediate granularity have the potential to fulfill an interlingua role within a solution to the paraphrase problem. Until now, semantic frames have been generated by hand (as in Fillmore and Atkins, 1992), based on native speaker intuition; the FrameNet project (http://www.icsi.berkeley.edu/ —framenet; Johnson et al., 2002) now couples this generation with empirical validation. Only recently has this project begun to achieve relative breadth in its inventory of semantic frames. To have a comprehensive inventory of semantic frames, however, we need the capacity to generate semantic frames semi-automatically (the need for manual post-editing is assumed). To address these challenges, we have developed SemFrame, a system that induces semantic frames automatically. Overall, the system performs two primary functions: (1) identification of sets of verb senses that evoke a common semantic frame (in the sense that lexica</context>
</contexts>
<marker>Johnson, Fillmore, Petruck, Baker, Ellsworth, Ruppenhofer, Wood, 2002</marker>
<rawString>Johnson, Christopher R., Charles J. Fillmore, Miriam R. L. Petruck, Collin F. Baker, Michael Ellsworth, Josef Ruppenhofer, and Esther J. Wood. 2002. FrameNet: Theory and Practice, version 1 . 0 , &lt; h t t p : / / w w w . i c s i . b e r k e l e y . e d u / ~framenet/book/book.html&gt;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Kozlowski</author>
<author>Kathleen F McCoy</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources.</title>
<date>2003</date>
<booktitle>In The Second International Workshop on Paraphrasing: Paraphrase Acquisition and Applications (IWP2003), ACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1381" citStr="Kozlowski et al., 2003" startWordPosition="202" endWordPosition="205"> on SemFrame verbs’ semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision. 1 Introduction Semantic content can almost always be expressed in a variety of ways. Lexical synonymy (She esteemed him highly vs. She respected him greatly), syntactic variation (John paid the bill vs. The bill was paid by John), overlapping meanings (Anna turned at Elm vs. Anna rounded the corner at Elm), and other phenomena interact to produce a broad range of choices for most language generation tasks (Hirst, 2003; Rinaldi et al., 2003; Kozlowski et al., 2003). At the same time, natural language understanding must recognize what remains constant across paraphrases. The paraphrase phenomenon affects many computational linguistic applications, including information retrieval, information extraction, question-answering, and machine translation. For example, documents that express the same content using different linguistic means should typically be retrieved for the same queries. Information sought to answer a question needs to be recognized no matter how it is expressed. Semantic frames (Fillmore, 1982; Fillmore and Atkins, 1992) address the paraphra</context>
</contexts>
<marker>Kozlowski, McCoy, Vijay-Shanker, 2003</marker>
<rawString>Kozlowski, Raymond, Kathleen F. McCoy, and K. Vijay-Shanker. 2003. Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources. In The Second International Workshop on Paraphrasing: Paraphrase Acquisition and Applications (IWP2003), ACL 2003, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>Chicago: University of Chicago Press.</publisher>
<contexts>
<context position="4937" citStr="Levin (1993)" startWordPosition="710" endWordPosition="711">ants, which may SemFrame’s ability to identify semantic verb not generalize to the frame element. Second, classes of a FrameNet-like nature. Section 7 lexical resources provide a consistent amount of summarizes our work and motivates directions for data for word senses, while the amount of data in further development of SemFrame. a corpus for word senses is likely to vary widely. 2 Previous Work The EAGLES (1998) report on semantic encoding differentiates between two approaches to the development of semantic verb classes: those based on syntactic behavior and those based on semantic criteria. Levin (1993) groups verbs based on an analysis of their syntactic properties, especially their ability to be expressed in diathesis alternations; her approach reflects the assumption that the syntactic behavior of a verb is determined in large part by its meaning. Verb classes at the bottom of Levin’s shallow network group together (quasi-) synonyms, hierarchically related verbs, and antonyms, alongside verbs with looser semantic relationships. The verb categories based on Pantel and Lin (2002) and Lin and Pantel (2001) are induced automatically from a large corpus, using an unsupervised clustering algori</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations: A Preliminary Investigation. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>317--322</pages>
<contexts>
<context position="5450" citStr="Lin and Pantel (2001)" startWordPosition="785" endWordPosition="788"> semantic verb classes: those based on syntactic behavior and those based on semantic criteria. Levin (1993) groups verbs based on an analysis of their syntactic properties, especially their ability to be expressed in diathesis alternations; her approach reflects the assumption that the syntactic behavior of a verb is determined in large part by its meaning. Verb classes at the bottom of Levin’s shallow network group together (quasi-) synonyms, hierarchically related verbs, and antonyms, alongside verbs with looser semantic relationships. The verb categories based on Pantel and Lin (2002) and Lin and Pantel (2001) are induced automatically from a large corpus, using an unsupervised clustering algorithm, based on syntactic dependency features. The resulting clusters contain synonyms, hierarchically related verbs, and antonyms, as well as verbs more loosely related from the perspective of paraphrase. The handcrafted WordNet (Fellbaum, 1998a) uses the hyperonymy/hyponymy relationship to structure the English verb lexicon into a semantic network. Each collection of a top-level node supplemented by its descendants may be seen as a semantic verb class. In all fairness, resolution of the paraphrase problem is</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, Dekang and Patrick Pantel. 2001. Induction of semantic classes from natural language text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 317-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<date>2004</date>
<booktitle>Senseval-3 task: Word-sense disambiguation of WordNet glosses,</booktitle>
<marker>Litkowski, 2004</marker>
<rawString>Litkowski, Ken. 2004. Senseval-3 task: Word-sense disambiguation of WordNet glosses, &lt;http://www.clres.com/SensWNDisamb.html&gt;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Nouns in WordNet.</title>
<date>1998</date>
<journal>In C. Fellbaum,</journal>
<volume>1998</volume>
<pages>23--67</pages>
<contexts>
<context position="10248" citStr="Miller, 1998" startWordPosition="1539" endWordPosition="1540">ing higher precision in the merged verbs—for example, cost, price, and the demand data set than in the input data sets. Then, the payment sense of charge—is not made explicit in graph formed by the verb sense pairs in the WordNet, however. Further, as Roger Chaffin merged data set is analyzed to find the fully has noted, the specialized vocabulary of, for connected components. example, tennis (e.g. racket, court, lob) is not co- Finally, these groups of verb senses become located, but is dispersed across different branches input to a clustering operation (Voorhees, 1986). of the noun network (Miller, 1998, p. 34). Those groups whose similarity (due to overlap in 4 SemFrame Approach SemFrame gathers evidence about frame semantic relatedness between verb senses by analyzing LDOCE and WordNet data from a variety of perspectives. The overall approach used is shown in Figure 1. The first stage of processing extracts pairs of LDOCE and WordNet verb senses that potentially evoke the same frame. By exploiting many different clues to semantic relatedness, we overgenerate these pairs, favoring recall; subsequent stages improve the precision of the resulting data. Figures 2 and 3 give details of the algo</context>
</contexts>
<marker>Miller, 1998</marker>
<rawString>Miller, George A. 1998. Nouns in WordNet. In C. Fellbaum, 1998a, 23-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="5424" citStr="Pantel and Lin (2002)" startWordPosition="780" endWordPosition="783">ches to the development of semantic verb classes: those based on syntactic behavior and those based on semantic criteria. Levin (1993) groups verbs based on an analysis of their syntactic properties, especially their ability to be expressed in diathesis alternations; her approach reflects the assumption that the syntactic behavior of a verb is determined in large part by its meaning. Verb classes at the bottom of Levin’s shallow network group together (quasi-) synonyms, hierarchically related verbs, and antonyms, alongside verbs with looser semantic relationships. The verb categories based on Pantel and Lin (2002) and Lin and Pantel (2001) are induced automatically from a large corpus, using an unsupervised clustering algorithm, based on syntactic dependency features. The resulting clusters contain synonyms, hierarchically related verbs, and antonyms, as well as verbs more loosely related from the perspective of paraphrase. The handcrafted WordNet (Fellbaum, 1998a) uses the hyperonymy/hyponymy relationship to structure the English verb lexicon into a semantic network. Each collection of a top-level node supplemented by its descendants may be seen as a semantic verb class. In all fairness, resolution of</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 613-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Procter</author>
</authors>
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman Group Ltd.,</publisher>
<location>Essex, UK.</location>
<contexts>
<context position="3852" citStr="Procter, 1978" startWordPosition="548" endWordPosition="549">ic verb classes. These classes have several types of uses. First, they are the basis for identifying the internal structure of the frame proper, as set forth in Green and Dorr, 2004. Second, they may be used to extend FrameNet. Third, they support applications needing access to sets of semantically related words, for example, text segmentation and word sense disambiguation, as explored to a limited degree in Green, 2004. Section 2 presents related research efforts on developing semantic verb classes. Section 3 summarizes the features of WordNet (http://www.cogsci.princeton.edu/—wn) and LDOCE (Procter, 1978) that support the automatic induction of semantic verb classes, definitions and example sentences often mention while Section 4 sets forth the approach taken by their participants using semantic-type-like nouns, SemFrame to accomplish this task. Section 5 thus mapping easily to the corresponding frame presents a brief synopsis of SemFrame’s results, element. Corpus data, however, are more likely while Section 6 presents an evaluation of to include instantiated participants, which may SemFrame’s ability to identify semantic verb not generalize to the frame element. Second, classes of a FrameNet</context>
</contexts>
<marker>Procter, 1978</marker>
<rawString>Procter, Paul (Ed.). 1978. Longman Dictionary of Contemporary English. Longman Group Ltd., Essex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Rinaldi</author>
<author>James Dowdall</author>
<author>Kaarel Kaljurand</author>
<author>Michael Hess</author>
<author>Diego Mollá</author>
</authors>
<title>Exploiting paraphrases in a question answering system.</title>
<date>2003</date>
<booktitle>In The Second International Workshop on Paraphrasing: Paraphrase Acquisition and Applications (IWP2003), ACL</booktitle>
<pages>25--32</pages>
<contexts>
<context position="1356" citStr="Rinaldi et al., 2003" startWordPosition="198" endWordPosition="201">73.8% precision (based on SemFrame verbs’ semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision. 1 Introduction Semantic content can almost always be expressed in a variety of ways. Lexical synonymy (She esteemed him highly vs. She respected him greatly), syntactic variation (John paid the bill vs. The bill was paid by John), overlapping meanings (Anna turned at Elm vs. Anna rounded the corner at Elm), and other phenomena interact to produce a broad range of choices for most language generation tasks (Hirst, 2003; Rinaldi et al., 2003; Kozlowski et al., 2003). At the same time, natural language understanding must recognize what remains constant across paraphrases. The paraphrase phenomenon affects many computational linguistic applications, including information retrieval, information extraction, question-answering, and machine translation. For example, documents that express the same content using different linguistic means should typically be retrieved for the same queries. Information sought to answer a question needs to be recognized no matter how it is expressed. Semantic frames (Fillmore, 1982; Fillmore and Atkins, 1</context>
</contexts>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Mollá, 2003</marker>
<rawString>Rinaldi, Fabio, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Mollá. 2003. Exploiting paraphrases in a question answering system. In The Second International Workshop on Paraphrasing: Paraphrase Acquisition and Applications (IWP2003), ACL 2003, 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Implementing agglomerative hierarchic clustering algorithms for use in document retrieval.</title>
<date>1986</date>
<journal>Information Processing &amp; Management</journal>
<volume>22</volume>
<pages>465--476</pages>
<contexts>
<context position="10213" citStr="Voorhees, 1986" startWordPosition="1533" endWordPosition="1534">her COMMERCIAL TRANSACTION thus achieving higher precision in the merged verbs—for example, cost, price, and the demand data set than in the input data sets. Then, the payment sense of charge—is not made explicit in graph formed by the verb sense pairs in the WordNet, however. Further, as Roger Chaffin merged data set is analyzed to find the fully has noted, the specialized vocabulary of, for connected components. example, tennis (e.g. racket, court, lob) is not co- Finally, these groups of verb senses become located, but is dispersed across different branches input to a clustering operation (Voorhees, 1986). of the noun network (Miller, 1998, p. 34). Those groups whose similarity (due to overlap in 4 SemFrame Approach SemFrame gathers evidence about frame semantic relatedness between verb senses by analyzing LDOCE and WordNet data from a variety of perspectives. The overall approach used is shown in Figure 1. The first stage of processing extracts pairs of LDOCE and WordNet verb senses that potentially evoke the same frame. By exploiting many different clues to semantic relatedness, we overgenerate these pairs, favoring recall; subsequent stages improve the precision of the resulting data. Figur</context>
</contexts>
<marker>Voorhees, 1986</marker>
<rawString>Voorhees, Ellen. 1986. Implementing agglomerative hierarchic clustering algorithms for use in document retrieval. Information Processing &amp; Management 22/6: 465-476.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>