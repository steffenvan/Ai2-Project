<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.943742">
Constituency to Dependency Translation with Forests
</title>
<author confidence="0.872968">
Haitao Mi and Qun Liu
</author>
<affiliation confidence="0.821635">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.5307">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.997916">
{htmi,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867045454546">
Tree-to-string systems (and their forest-
based extensions) have gained steady pop-
ularity thanks to their simplicity and effi-
ciency, but there is a major limitation: they
are unable to guarantee the grammatical-
ity of the output, which is explicitly mod-
eled in string-to-tree systems via target-
side syntax. We thus propose to com-
bine the advantages of both, and present
a novel constituency-to-dependency trans-
lation model, which uses constituency
forests on the source side to direct the
translation, and dependency trees on the
target side (as a language model) to en-
sure grammaticality. Medium-scale exper-
iments show an absolute and statistically
significant improvement of +0.7 BLEU
points over a state-of-the-art forest-based
tree-to-string system even with fewer
rules. This is also the first time that a tree-
to-tree model can surpass tree-to-string
counterparts.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999253375">
Linguistically syntax-based statistical machine
translation models have made promising progress
in recent years. By incorporating the syntactic an-
notations of parse trees from both or either side(s)
of the bitext, they are believed better than phrase-
based counterparts in reorderings. Depending on
the type of input, these models can be broadly di-
vided into two categories (see Table 1): the string-
based systems whose input is a string to be simul-
taneously parsed and translated by a synchronous
grammar, and the tree-based systems whose input
is already a parse tree to be directly converted into
a target tree or string. When we also take into ac-
count the type of output (tree or string), the tree-
based systems can be divided into tree-to-string
and tree-to-tree efforts.
</bodyText>
<table confidence="0.9950726">
tree on examples (partial) fast gram. BLEU
source Liu06, Huang06 + - +
target Galley06, Shen08 - + +
both Ding05, Liu09 + + -
both our work + + +
</table>
<tableCaption confidence="0.995231">
Table 1: A classification and comparison of lin-
</tableCaption>
<bodyText confidence="0.984089">
guistically syntax-based SMT systems, where
gram. denotes grammaticality of the output.
On one hand, tree-to-string systems (Liu et al.,
2006; Huang et al., 2006) have gained significant
popularity, especially after incorporating packed
forests (Mi et al., 2008; Mi and Huang, 2008; Liu
et al., 2009; Zhang et al., 2009). Compared with
their string-based counterparts, tree-based systems
are much faster in decoding (linear time vs. cu-
bic time, see (Huang et al., 2006)), do not re-
quire a binary-branching grammar as in string-
based models (Zhang et al., 2006; Huang et al.,
2009), and can have separate grammars for pars-
ing and translation (Huang et al., 2006). However,
they have a major limitation that they do not have a
principled mechanism to guarantee grammatical-
ity on the target side, since there is no linguistic
tree structure of the output.
On the other hand, string-to-tree systems ex-
plicitly model the grammaticality of the output
by using target syntactic trees. Both string-to-
constituency system (e.g., (Galley et al., 2006;
Marcu et al., 2006)) and string-to-dependency
model (Shen et al., 2008) have achieved signif-
icant improvements over the state-of-the-art for-
mally syntax-based system Hiero (Chiang, 2007).
However, those systems also have some limita-
tions that they run slowly (in cubic time) (Huang
et al., 2006), and do not utilize the useful syntactic
information on the source side.
We thus combine the advantages of both tree-to-
string and string-to-tree approaches, and propose
</bodyText>
<page confidence="0.866362">
1433
</page>
<note confidence="0.942612">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999337925925926">
a novel constituency-to-dependency model, which
uses constituency forests on the source side to di-
rect translation, and dependency trees on the tar-
get side to guarantee grammaticality of the out-
put. In contrast to conventional tree-to-tree ap-
proaches (Ding and Palmer, 2005; Quirk et al.,
2005; Xiong et al., 2007; Zhang et al., 2007;
Liu et al., 2009), which only make use of a sin-
gle type of trees, our model is able to combine
two types of trees, outperforming both phrase-
based and tree-to-string systems. Current tree-to-
tree models (Xiong et al., 2007; Zhang et al., 2007;
Liu et al., 2009) still have not outperformed the
phrase-based system Moses (Koehn et al., 2007)
significantly even with the help of forests.1
Our new constituency-to-dependency model
(Section 2) extracts rules from word-aligned pairs
of source constituency forests and target depen-
dency trees (Section 3), and translates source con-
stituency forests into target dependency trees with
a set of features (Section 4). Medium data exper-
iments (Section 5) show a statistically significant
improvement of +0.7 BLEU points over a state-
of-the-art forest-based tree-to-string system even
with less translation rules, this is also the first time
that a tree-to-tree model can surpass tree-to-string
counterparts.
</bodyText>
<sectionHeader confidence="0.989648" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.997269">
Figure 1 shows a word-aligned source con-
stituency forest Fc and target dependency tree De,
our constituency to dependency translation model
can be formalized as:
</bodyText>
<equation confidence="0.984890833333333">
P(Fc, De) = � P(Cc, De)
C.EF.
�=
C.EF.
�=
C.EF.
</equation>
<bodyText confidence="0.9991505">
where Cc is a constituency tree in Fc, o is a deriva-
tion that translates Cc to De, O is the set of deriva-
tion, r is a constituency to dependency translation
rule.
</bodyText>
<footnote confidence="0.9710548">
1According to the reports of Liu et al. (2009), their forest-
based constituency-to-constituency system achieves a com-
parable performance against Moses (Koehn et al., 2007), but
a significant improvement of +3.6 BLEU points over the 1-
best tree-based constituency-to-constituency system.
</footnote>
<subsectionHeader confidence="0.977569">
2.1 Constituency Forests on the Source Side
</subsectionHeader>
<bodyText confidence="0.999612588235294">
A constituency forest (in Figure 1 left) is a com-
pact representation of all the derivations (i.e.,
parse trees) for a given sentence under a context-
free grammar (Billot and Lang, 1989).
More formally, following Huang (2008), such
a constituency forest is a pair Fc = Gf =
(V f, Hf), where V f is the set of nodes, and Hf
the set of hyperedges. For a given source sen-
tence c1:m = c1 ... cm, each node vf E V f is
in the form of Xi,j, which denotes the recognition
of nonterminal X spanning the substring from po-
sitions i through j (that is, ci+1 ... cj). Each hy-
peredge hf E Hf is a pair (tails(hf), head(hf)),
where head(hf) E V f is the consequent node in
the deductive step, and tails(hf) E (V f)* is the
list of antecedent nodes. For example, the hyper-
edge hf0 in Figure 1 for deduction (*)
</bodyText>
<equation confidence="0.902354625">
NPB0,1 CC1,2 NPB2,3
NP0,3 , (*)
is notated:
((NPB0,1, CC1,2, NPB2,3), NP0,3).
where
head(hf0) = {NP0,3},
and
tails(hf0) = {NPB0,1,CC1,2,NPB2,3}.
</equation>
<bodyText confidence="0.999974214285714">
The solid line in Figure 1 shows the best parse
tree, while the dashed one shows the second best
tree. Note that common sub-derivations like those
for the verb VPB3,5 are shared, which allows the
forest to represent exponentially many parses in a
compact structure.
We also denote IN(vf) to be the set of in-
coming hyperedges of node vf, which represents
the different ways of deriving vf. Take node IP0,5
in Figure 1 for example, IN(IP0,5) = {hf1, hf2}.
There is also a distinguished root node TOP in
each forest, denoting the goal item in parsing,
which is simply S0,m where S is the start symbol
and m is the sentence length.
</bodyText>
<subsectionHeader confidence="0.99826">
2.2 Dependency Trees on the Target Side
</subsectionHeader>
<bodyText confidence="0.999919">
A dependency tree for a sentence represents each
word and its syntactic dependents through directed
arcs, as shown in the following examples. The
main advantage of a dependency tree is that it can
explore the long distance dependency.
</bodyText>
<equation confidence="0.998052444444444">
(1)
P(O)
�
oEO
H
rEo
P(r),
�
oEO
</equation>
<page confidence="0.832122">
1434
</page>
<figure confidence="0.92436725">
1: talk IP
NP
x1:NPB CC
x3:VPB →(x1) x3 (with (x2))
x2:NPB
a
2: held
Bush talk
a
with
Sharon
yˇu
</figure>
<figureCaption confidence="0.836260666666667">
Figure 2: Example of the rule r1. The Chinese con-
junction yˇu “and” is translated into English prepo-
sition “with”.
</figureCaption>
<bodyText confidence="0.99993425">
We use the lexicon dependency grammar (Hell-
wig, 2006) to express a projective dependency
tree. Take the dependency trees above for exam-
ple, they will be expressed:
</bodyText>
<listItem confidence="0.7776385">
1: ( a ) talk
2: ( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )
</listItem>
<bodyText confidence="0.9990879375">
where the lexicons in brackets represent the de-
pendencies, while the lexicon out the brackets is
the head.
More formally, a dependency tree is also a pair
De = Gd = (V d, Hd). For a given target sen-
tence e1:n = e1 ... en, each node vd E V d is
a word ei (1 S i S n), each hyperedge hd E
Hd is a directed arc (vdi , vdj) from node vdi to
its head node vdj. Following the formalization of
the constituency forest scenario, we denote a pair
(tails(hd), head(hd)) to be ahyperedge hd, where
head(hd) is the head node, tails(hd) is the node
where hd leaves from.
We also denote Ll(vd) and Lr(vd) to be the left
and right children sequence of node vd from the
nearest to the farthest respectively. Take the node
</bodyText>
<equation confidence="0.846296333333333">
vd2 = “held” for example:
Ll(vd2) ={Bush},
Lr(vd2) ={talk, with}.
</equation>
<subsectionHeader confidence="0.990131">
2.3 Hypergraph
</subsectionHeader>
<bodyText confidence="0.999777">
Actually, both the constituency forest and the de-
pendency tree can be formalized as a hypergraph
G, a pair (V, H). We use Gf and Gd to distinguish
them. For simplicity, we also use Fc and De to de-
note a constituency forest and a dependency tree
respectively. Specifically, the size of tails(hd) of
a hyperedge hd in a dependency tree is a constant
one.
</bodyText>
<sectionHeader confidence="0.973316" genericHeader="method">
3 Rule Extraction
</sectionHeader>
<bodyText confidence="0.999978166666667">
We extract constituency to dependency rules from
word-aligned source constituency forest and target
dependency tree pairs (Figure 1). We mainly ex-
tend the tree-to-string rule extraction algorithm of
Mi and Huang (2008) to our scenario. In this sec-
tion, we first formalize the constituency to string
translation rule (Section 3.1). Then we present
the restrictions for dependency structures as well
formed fragments (Section 3.2). Finally, we de-
scribe our rule extraction algorithm (Section 3.3),
fractional counts computation and probabilities es-
timation (Section 3.4).
</bodyText>
<subsectionHeader confidence="0.998884">
3.1 Constituency to Dependency Rule
</subsectionHeader>
<bodyText confidence="0.999925571428571">
More formally, a constituency to de-
pendency translation rule r is a tuple
(lhs(r), rhs(r), O(r)), where lhs(r) is the
source side tree fragment, whose internal nodes
are labeled by nonterminal symbols (like NP and
VP), and whose frontier nodes are labeled by
source language words ci (like “yˇu”) or variables
from a set X = {x1, x2,...}; rhs(r) is expressed
in the target language dependency structure with
words ej (like “with”) and variables from the set
X; and O(r) is a mapping from X to nontermi-
nals. Each variable xi E X occurs exactly once in
lhs(r) and exactly once in rhs(r). For example,
the rule r1 in Figure 2,
</bodyText>
<equation confidence="0.99901">
lhs(r1) = IP(NP(x1 CC(yˇu) x2) x3),
rhs(r1) = (x1) x3 (with (x2)),
O(r1) = {x1 H NPB, x2 H NPB, x3 H VPB}.
</equation>
<subsectionHeader confidence="0.998351">
3.2 Well Formed Dependency Fragment
</subsectionHeader>
<bodyText confidence="0.9998835">
Following Shen et al. (2008), we also restrict
rhs(r) to be well formed dependency fragment.
The main difference between us is that we use
more flexible restrictions. Given a dependency
</bodyText>
<page confidence="0.953946">
1435
</page>
<figure confidence="0.937256695652174">
Minimal rules extracted
NPB0,1
CC1,2
hf
0
P1,2
“with (Sharon)”
PP1,3
NPB2,3
“with”
“Sharon”
“Bush”
“with”
IP0,5
“(Bush) .. Sharon))”
hf hf
1 2
NP0,3
“(Bush) ⊔ (with (Sharon))”
VP1,5
“held .. Sharon))”
VPB3,5
“held ((a) talk)”
</figure>
<equation confidence="0.978903666666667">
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
→ (x1) x4 (x2 (x3) )
IP (x1:NPB x2:VP) → (x1) x2
VP (x1:PP x2:VPB) → x2 (x1)
PP (x1:P x2:NPB) → x1 (x2)
VPB (VV(j&apos;uxingle)) x1:NPB)
</equation>
<listItem confidence="0.981749166666667">
→ held ((a) x1)
NPB (B`ush´a) → Bush
NPB (huit´an) → talk
CC (y&apos;u) → with
P (y&apos;u) → with
NPB (Sh¯al´ong) → Sharon
</listItem>
<figure confidence="0.9992176">
B`ushi
Sh¯al´ong
j&apos;uxingle
huit´an
y&apos;u
( Bush ) held ( ( a ) talk ) ( with ( Sharon) )
NPB4,5
VV3,4
“held ((a)*)”
“talk”
</figure>
<figureCaption confidence="0.99999">
Figure 1: Forest-based constituency to dependency rule extraction.
</figureCaption>
<bodyText confidence="0.998273">
fragment di:j composed by the words from i to j,
two kinds of well formed structures are defined as
follows:
Fixed on one node vdone, fixed for short, if it
meets the following conditions:
</bodyText>
<listItem confidence="0.714972666666667">
•the head of vdone is out of [i, j], i.e.: ∀hd, if
tails(hd) = vdone ⇒ head(hd) ∈/ ei:j.
• the heads of other nodes except vdone are in
</listItem>
<equation confidence="0.580055">
[i, j], i.e.: ∀k ∈ [i, j] and vdk =6 vdone, ∀hd if
tails(hd) = vdk ⇒ head(hd) ∈ ei:j.
</equation>
<bodyText confidence="0.769407">
Floating with multi nodes M, floating for
short, if it meets the following conditions:
</bodyText>
<listItem confidence="0.9996925">
• all nodes in M have a same head node,
i.e.: ∃x ∈/ [i, j], ∀hd if tails(hd) ∈ M ⇒
head(hd) = vhx.
• the heads of other nodes not in M are in
</listItem>
<equation confidence="0.8841605">
[i, j], i.e.: ∀k ∈ [i, j] and vdk ∈/ M, ∀hd if
tails(hd) = vdk ⇒ head(hd) ∈ ei:j.
</equation>
<bodyText confidence="0.9991418">
Take the “ (Bush) held ((a) talk))(with (Sharon))
” for example: partial fixed examples are “ (Bush)
held ” and “ held ((a) talk)”; while the partial float-
ing examples are “ (talk) (with (Sharon)) ” and “
((a) talk) (with (Sharon)) ”. Please note that the
floating structure “ (talk) (with (Sharon)) ” can not
be allowed in Shen et al. (2008)’s model.
The dependency structure “ held ((a))” is not a
well formed structure, since the head of word “a”
is out of scope of this structure.
</bodyText>
<subsectionHeader confidence="0.999342">
3.3 Rule Extraction Algorithm
</subsectionHeader>
<bodyText confidence="0.996813166666667">
The algorithm shown in this Section is mainly ex-
tended from the forest-based tree-to-string extrac-
tion algorithm (Mi and Huang, 2008). We extract
rules from word-aligned source constituency for-
est and target dependency tree pairs (see Figure 1)
in three steps:
</bodyText>
<listItem confidence="0.999407">
(1) frontier set computation,
(2) fragmentation,
(3) composition.
</listItem>
<bodyText confidence="0.9999813">
The frontier set (Galley et al., 2004) is the po-
tential points to “cut” the forest and dependency
tree pair into fragments, each of which will form a
minimal rule (Galley et al., 2006).
However, not every fragment can be used for
rule extraction, since it may or may not respect
to the restrictions, such as word alignments and
well formed dependency structures. So we say a
fragment is extractable if it respects to all re-
strictions. The root node of every extractable tree
fragment corresponds to a faithful structure on
the target side, in which case there is a “transla-
tional equivalence” between the subtree rooted at
the node and the corresponding target structure.
For example, in Figure 1, every node in the forest
is annotated with its corresponding English struc-
ture. The NP0,3 node maps to a non-contiguous
structure “(Bush) ⊔ (with (Sharon))”, the VV3,4
node maps to a contiguous but non-faithful struc-
ture “held ((a) *)”.
</bodyText>
<page confidence="0.96639">
1436
</page>
<table confidence="0.674313666666667">
Algorithm 1 Forest-based constituency to dependency rule extraction.
Input: Source constituency forest Fc, target dependency tree De, and alignment a
Output: Minimal rule set R
</table>
<listItem confidence="0.969472384615385">
1: fs +– FRONTIER(Fc, De, a) &gt; compute frontier set
2: for each vf E fs do
3: open +– {(0, {vf})} &gt; initial queue of growing fragments
4: while open =� 0 do
5: (hs, exps) +– open.pop() &gt; extract a fragment
6: if exps = 0 then &gt; nothing to expand?
7: generate a rule r using fragment hs &gt; generate a rule
8: R.append(r)
9: else &gt; incomplete: further expand
10: v′ +– exps.pop() &gt; a non-frontier node
11: for each hf E IN (v′) do
12: newexps +– exps U (tails(hf) \ fs) &gt; expand
13: open.append((hs U {hf}, newexps))
</listItem>
<bodyText confidence="0.996639">
Following Mi and Huang (2008), given a source
target sentence pair (c1:m, e1:n) with an alignment
a, the span of node vf on source forest is the set
of target words aligned to leaf nodes under vf:
</bodyText>
<equation confidence="0.60032">
span(vf) °_ {ei E e1:n  |3cj E yield(vf), (cj, ei) E a}.
</equation>
<bodyText confidence="0.998372555555556">
where the yield(vf) is all the leaf nodes un-
der vf. For each span(vf), we also denote
dep(vf) to be its corresponding dependency struc-
ture, which represents the dependency struc-
ture of all the words in span(vf). Take the
span(PP1,3) ={with, Sharon} for example, the
corresponding dep(PP1,3) is “with (Sharon)”. A
dep(vf) is faithful structure to node vf if it meets
the following restrictions:
</bodyText>
<listItem confidence="0.993208428571428">
• all words in span(vf) form a continuous sub-
string ei:j,
• every word in span(vf) is only aligned to leaf
nodes of vf, i.e.: bei E span(vf), (cj, ei) E
a ==&gt;- cj E yield(vf),
• dep(vf) is a well formed dependency struc-
ture.
</listItem>
<bodyText confidence="0.999167555555556">
For example, node VV3,4 has a non-faithful
structure (crossed out in Figure 1), since its
dep(VV3,4 = “ held ((a) *)” is not a well formed
structure, where the head of word “a” lies in the
outside of its words covered. Nodes with faithful
structure form the frontier set (shaded nodes in
Figure 1) which serve as potential cut points for
rule extraction.
Given the frontier set, fragmentation step is to
“cut” the forest at all frontier nodes and form
tree fragments, each of which forms a rule with
variables matching the frontier descendant nodes.
For example, the forest in Figure 1 is cut into 10
pieces, each of which corresponds to a minimal
rule listed on the right.
Our rule extraction algorithm is formalized in
Algorithm 1. After we compute the frontier set
fs (line 1). We visit each frontier node vf E fs
on the source constituency forest Fc, and keep a
queue open of growing fragments rooted at vf. We
keep expanding incomplete fragments from open,
and extract a rule if a complete fragment is found
(line 7). Each fragment hs in open is associated
with a list of expansion sites (exps in line 5) being
the subset of leaf nodes of the current fragment
that are not in the frontier set. So each fragment
along hyperedge h is associated with
</bodyText>
<equation confidence="0.625583">
exps = tails(hf) \ fs.
</equation>
<bodyText confidence="0.999582">
A fragment is complete if its expansion sites is
empty (line 6), otherwise we pop one expansion
node v′ to grow and spin-off new fragments by
following hyperedges of v′, adding new expansion
sites (lines 11-13), until all active fragments are
complete and open queue is empty (line 4).
After we get all the minimal rules, we glue them
together to form composed rules following Galley
et al. (2006). For example, the composed rule r1
in Figure 2 is glued by the following two minimal
rules:
</bodyText>
<page confidence="0.991789">
1437
</page>
<tableCaption confidence="0.813138">
a target side dependency tree De(o):
</tableCaption>
<table confidence="0.96694175">
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) o* = arg max λ1 logP(o  |Tc)
r2 TcEFc,oEO
→ (x1) x4 (x2 (x3) )
CC (yˇu) → with r3
</table>
<bodyText confidence="0.690511">
where x2:CC in r2 is replaced with r3 accordingly.
</bodyText>
<subsectionHeader confidence="0.989877">
3.4 Fractional Counts and Rule Probabilities
</subsectionHeader>
<bodyText confidence="0.99673775">
Following Mi and Huang (2008), we penalize a
rule r by the posterior probability of the corre-
sponding constituent tree fragment lhs(r), which
can be computed in an Inside-Outside fashion, be-
ing the product of the outside probability of its
root node, the inside probabilities of its leaf nodes,
and the probabilities of hyperedges involved in the
fragment.
</bodyText>
<equation confidence="0.9944012">
αβ(lhs(r)) =α(root(r))
Y· P(hf)
hf ∈ lhs(r) (2)
Y· β(vf)
vf ∈ leaves(lhs(r))
</equation>
<bodyText confidence="0.999952285714286">
where root(r) is the root of the rule r, α(v) and
β(v) are the outside and inside probabilities of
node v, and leaves(lhs(r)) returns the leaf nodes
of a tree fragment lhs(r).
We use fractional counts to compute three con-
ditional probabilities for each rule, which will be
used in the next section:
</bodyText>
<equation confidence="0.99792275">
c(r)
P(r  |lhs(r)) = Pr′:lhs(r′)=lhs(r) c(rf), (3)
c(r)
P(r  |rhs(r)) = Pr′:rhs(r′)=rhs(r) c(rf), (4)
</equation>
<sectionHeader confidence="0.997139" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.9992565">
Given a source forest Fc, the decoder searches for
the best derivation o* among the set of all possible
derivations O, each of which forms a source side
constituent tree Tc(o), a target side string e(o), and
</bodyText>
<equation confidence="0.999646">
+λ2 log Plm(e(o))
+λ3 log PDLMw(De(o)) (6)
+λ4 log PDLM1,(De(o))
+λ5 log P(Tc(o))
+λsill(o) + λ7|o |+ λ8|e(o)|,
</equation>
<bodyText confidence="0.999569571428571">
where the first two terms are translation and lan-
guage model probabilities, e(o) is the target string
(English sentence) for derivation o, the third and
forth items are the dependency language model
probabilities on the target side computed with
words and POS tags separately, De(o) is the target
dependency tree of o, the fifth one is the parsing
probability of the source side tree Tc(o) ∈ Fc, the
ill(o) is the penalty for the number of ill-formed
dependency structures in o, and the last two terms
are derivation and translation length penalties, re-
spectively. The conditional probability P(o  |Tc)
is decomposes into the product of rule probabili-
ties:
</bodyText>
<equation confidence="0.96446075">
P(o  |Tc) = Y P(r), (7)
rEo
where each P(r) is the product of five probabili-
ties:
P(r) =P(r  |lhs(r))λ9 · P(r  |rhs(r))λ10
· P(r  |root(lhs(r)))λ11
· Pl&apos;�x(lhs(r)  |rhs(r))λ1� (8)
· Pl&apos;�x(rhs(r)  |lhs(r))λ1�,
</equation>
<bodyText confidence="0.994536941176471">
where the first three are conditional probabilities
based on fractional counts of rules defined in Sec-
tion 3.4, and the last two are lexical probabilities.
When computing the lexical translation probabili-
ties described in (Koehn et al., 2003), we only take
into accout the terminals in a rule. If there is no
terminal, we set the lexical probability to 1.
The decoding algorithm works in a bottom-up
search fashion by traversing each node in forest
Fc. We first use pattern-matching algorithm of Mi
et al. (2008) to convert Fc into a translation for-
est, each hyperedge of which is associated with a
constituency to dependency translation rule. How-
ever, pattern-matching failure2 at a node vf will
2Pattern-matching failure at a node vf means there is no
translation rule can be matched at vf or no translation hyper-
edge can be constructed at vf.
</bodyText>
<equation confidence="0.98825925">
c(r)
P(r  |root(r)) =
P
r′: root (r′)=root(r) c(rf). (5)
</equation>
<page confidence="0.963634">
1438
</page>
<bodyText confidence="0.9996195">
cut the derivation path and lead to translation fail-
ure. To tackle this problem, we construct a pseudo
translation rule for each parse hyperedge hf ∈
IN (vf) by mapping the CFG rule into a target de-
pendency tree using the head rules of Magerman
(1995). Take the hyperedge hf0 in Figure1 for ex-
ample, the corresponding pseudo translation rule
is:
</bodyText>
<equation confidence="0.965757">
NP(x1:NPB x2:CC x3:NPB) → (x1) (x2) x3,
</equation>
<bodyText confidence="0.995932882352941">
since the x3:NPB is the head word of the CFG
rule: NP → NPB CC NPB.
After the translation forest is constructed, we
traverse each node in translation forest also in
bottom-up fashion. For each node, we use the
cube pruning technique (Chiang, 2007; Huang
and Chiang, 2007) to produce partial hypotheses
and compute all the feature scores including the
dependency language model score (Section 4.1).
If all the nodes are visited, we trace back along
the 1-best derivation at goal item S0,,,, and build
a target side dependency tree. For k-best search
after getting 1-best derivation, we use the lazy Al-
gorithm 3 of Huang and Chiang (2005) that works
backwards from the root node, incrementally com-
puting the second, third, through the kth best alter-
natives.
</bodyText>
<subsectionHeader confidence="0.97251">
4.1 Dependency Language Model Computing
</subsectionHeader>
<bodyText confidence="0.999370285714286">
We compute the score of a dependency language
model for a dependency tree De in the same way
proposed by Shen et al. (2008). For each nonter-
minal node vh = eh in De and its children se-
quences Ll = el1, el2...elz and Lr = er1, er2...erg,
the probability of a trigram is computed as fol-
lows:
</bodyText>
<equation confidence="0.576125">
P(Ll, Lr  |eh§) = P(Ll  |eh§) · P(Lr  |eh§), (9)
</equation>
<bodyText confidence="0.876784176470588">
where the P(Ll  |eh§) is decomposed to be:
P(Ll  |eh§) =P(el�  |eh§) (10)
· P(el2  |el1, eh§)
...
·P(el.  |el.−1, el.−2).
We use the suffix “§” to distinguish the head
word and child words in the dependency language
model.
In order to alleviate the problem of data sparse,
we also compute a dependency language model
for POS tages over a dependency tree. We store
the POS tag information on the target side for each
constituency-to-dependency rule. So we will also
generate a POS taged dependency tree simulta-
neously at the decoding time. We calculate this
dependency language model by simply replacing
each ez in equation 9 with its tag t(ez).
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999284">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999990942857143">
Our training corpus consists of 239K sentence
pairs with about 6.9M/8.9M words in Chi-
nese/English, respectively. We first word-align
them by GIZA++ (Och and Ney, 2000) with re-
finement option “grow-diag-and” (Koehn et al.,
2003), and then parse the Chinese sentences using
the parser of Xiong et al. (2005) into parse forests,
which are pruned into relatively small forests with
a pruning threshold 3. We also parse the English
sentences using the parser of Charniak (2000) into
1-best constituency trees, which will be converted
into dependency trees using Magerman (1995)’s
head rules. We also store the POS tag informa-
tion for each word in dependency trees, and com-
pute two different dependency language models
for words and POS tags in dependency tree sepa-
rately. Finally, we apply translation rule extraction
algorithm described in Section 3. We use SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
4-gram language model with Kneser-Ney smooth-
ing on the first 1/3 of the Xinhua portion of Giga-
word corpus. At the decoding step, we again parse
the input sentences into forests and prune them
with a threshold 10, which will direct the trans-
lation (Section 4).
We use the 2002 NIST MT Evaluation test set
as our development set and the 2005 NIST MT
Evaluation test set as our test set. We evaluate the
translation quality using the BLEU-4 metric (Pap-
ineni et al., 2002), which is calculated by the script
mteval-v11b.pl with its default setting which is
case-insensitive matching of n-grams. We use the
standard minimum error-rate training (Och, 2003)
to tune the feature weights to maximize the sys-
tem’s BLEU score on development set.
</bodyText>
<subsectionHeader confidence="0.88571">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999784">
Table 2 shows the results on the test set. Our
baseline system is a state-of-the-art forest-based
constituency-to-string model (Mi et al., 2008), or
forest cls for short, which translates a source for-
est into a target string by pattern-matching the
</bodyText>
<page confidence="0.990003">
1439
</page>
<bodyText confidence="0.996258528301887">
constituency-to-string (c2s) rules and the bilin-
gual phrases (s2s). The baseline system extracts
31.9M c2s rules, 77.9M s2s rules respectively and
achieves a BLEU score of 34.17 on the test set3.
At first, we investigate the influence of differ-
ent rule sets on the performance of baseline sys-
tem. We first restrict the target side of transla-
tion rules to be well-formed structures, and we
extract 13.8M constituency-to-dependency (c2d)
rules, which is 43% of c2s rules. We also extract
9.0M string-to-dependency (s2d) rules, which is
only 11.6% of s2s rules. Then we convert c2d and
s2d rules to c2s and s2s rules separately by re-
moving the target-dependency structures and feed
them into the baseline system. As shown in the
third line in the column of BLEU score, the per-
formance drops 1.7 BLEU points over baseline
system due to the poorer rule coverage. However,
when we further use all s2s rules instead of s2d
rules in our next experiment, it achieves a BLEU
score of 34.03, which is very similar to the base-
line system. Those results suggest that restrictions
on c2s rules won’t hurt the performance, but re-
strictions on s2s will hurt the translation quality
badly. So we should utilize all the s2s rules in or-
der to preserve a good coverage of translation rule
set.
The last two lines in Table 2 show the results of
our new forest-based constituency-to-dependency
model (forest c2d for short). When we only use
c2d and s2d rules, our system achieves a BLEU
score of 33.25, which is lower than the baseline
system in the first line. But, with the same rule set,
our model still outperform the result in the sec-
ond line. This suggests that using dependency lan-
guage model really improves the translation qual-
ity by less than 1 BLEU point.
In order to utilize all the s2s rules and increase
the rule coverage, we parse the target strings of
the s2s rules into dependency fragments, and con-
struct the pseudo s2d rules (s2s-dep). Then we
use c2d and s2s-dep rules to direct the translation.
With the help of the dependency language model,
our new model achieves a significant improvement
of +0.7 BLEU points over the forest c2s baseline
system (p &lt; 0.05, using the sign-test suggested by
3According to the reports of Liu et al. (2009), with a more
larger training corpus (FBIS plus 30K) but no name entity
translations (+1 BLEU points if it is used), their forest-based
constituency-to-constituency model achieves a BLEU score
of 30.6, which is similar to Moses (Koehn et al., 2007). So our
baseline system is much better than the BLEU score (30.6+1)
of the constituency-to-constituency system and Moses.
</bodyText>
<equation confidence="0.863956">
Rule Set
#
31.9M
77.9M
32.48(↓1.7)
34.03(↓0.1)
33.25(↓0.9)
forest c2d
34.88(T0.7)
</equation>
<tableCaption confidence="0.982341666666667">
Table 2: Statistics of different types of rules ex-
tracted on training corpus and the BLEU scores
on the test set.
</tableCaption>
<bodyText confidence="0.907833">
Collins et al. (2005)). For the first time, a tree-to-
tree model can surpass tree-to-string counterparts
significantly even with fewer rules.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999974392857143">
The concept of packed forest has been used in
machine translation for several years. For exam-
ple, Huang and Chiang (2007) use forest to char-
acterize the search space of decoding with in-
tegrated language models. Mi et al. (2008) and
Mi and Huang (2008) use forest to direct trans-
lation and extract rules rather than 1-best tree in
order to weaken the influence of parsing errors,
this is also the first time to use forest directly
in machine translation. Following this direction,
Liu et al. (2009) and Zhang et al. (2009) apply
forest into tree-to-tree (Zhang et al., 2007) and
tree-sequence-to-string models(Liu et al., 2007)
respectively. Different from Liu et al. (2009), we
apply forest into a new constituency tree to de-
pendency tree translation model rather than con-
stituency tree-to-tree model.
Shen et al. (2008) present a string-to-
dependency model. They define the well-formed
dependency structures to reduce the size of
translation rule set, and integrate a dependency
language model in decoding step to exploit long
distance word relations. This model shows a
significant improvement over the state-of-the-art
hierarchical phrase-based system (Chiang, 2005).
Compared with this work, we put fewer restric-
tions on the definition of well-formed dependency
structures in order to extract more rules; the
</bodyText>
<figure confidence="0.998653608695652">
System
forestc2s
Type
c2s
s2s
c2d
s2d
13.8M
9.0M
BLEU
34.17
c2d
s2s
13.8M
77.9M
c2d
s2d
13.8M
9.0M
c2d
s2s-dep
13.8M
77.9M
</figure>
<page confidence="0.965352">
1440
</page>
<bodyText confidence="0.999895894736842">
other difference is that we can also extract more
expressive constituency to dependency rules,
since the source side of our rule can encode
multi-level reordering and contain more variables
being larger than two; furthermore, our rules can
be pattern-matched at high level, which is more
reasonable than using glue rules in Shen et al.
(2008)’s scenario; finally, the most important one
is that our model runs very faster.
Liu et al. (2009) propose a forest-based
constituency-to-constituency model, they put
more emphasize on how to utilize parse forest
to increase the tree-to-tree rule coverage. By
contrast, we only use 1-best dependency trees
on the target side to explore long distance rela-
tions and extract translation rules. Theoretically,
we can extract more rules since dependency
tree has the best inter-lingual phrasal cohesion
properties (Fox, 2002).
</bodyText>
<sectionHeader confidence="0.992084" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999986366666667">
In this paper, we presented a novel forest-based
constituency-to-dependency translation model,
which combines the advantages of both tree-to-
string and string-to-tree systems, runs fast and
guarantees grammaticality of the output. To learn
the constituency-to-dependency translation rules,
we first identify the frontier set for all the
nodes in the constituency forest on the source
side. Then we fragment them and extract mini-
mal rules. Finally, we glue them together to be
composed rules. At the decoding step, we first
parse the input sentence into a constituency for-
est. Then we convert it into a translation for-
est by patter-matching the constituency to string
rules. Finally, we traverse the translation forest
in a bottom-up fashion and translate it into a tar-
get dependency tree by incorporating string-based
and dependency-based language models. Using all
constituency-to-dependency translation rules and
bilingual phrases, our model achieves +0.7 points
improvement in BLEU score significantly over a
state-of-the-art forest-based tree-to-string system.
This is also the first time that a tree-to-tree model
can surpass tree-to-string counterparts.
In the future, we will do more experiments
on rule coverage to compare the constituency-to-
constituency model with our model. Furthermore,
we will replace 1-best dependency trees on the
target side with dependency forests to further in-
crease the rule coverage.
</bodyText>
<sectionHeader confidence="0.945672" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999824571428572">
The authors were supported by National Natural
Science Foundation of China, Contracts 60736014
and 90920004, and 863 State Key Project No.
2006AA010108. We thank the anonymous review-
ers for their insightful comments. We are also
grateful to Liang Huang for his valuable sugges-
tions.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99958805">
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
ofACL ’89, pages 143–151.
Eugene Charniak. 2000. A maximum-entropy inspired
parser. In Proceedings ofNAACL, pages 132–139.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings ofACL, pages 263–270, Ann Arbor, Michi-
gan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings ofACL, pages 531–540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL, pages
541–548, June.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In In Proceedings of EMNLP-
02.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings ofHLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961–968, July.
Peter Hellwig. 2006. Parsing with Dependency Gram-
mars, volume II. An International Handbook of
Contemporary Research.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings ofIWPT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings ofACL, pages 144–151, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings ofAMTA.
</reference>
<page confidence="0.826937">
1441
</page>
<reference confidence="0.9995977625">
Liang Huang, Hao Zhang, Daniel Gildea, , and Kevin
Knight. 2009. Binarization of synchronous context-
free grammars. Comput. Linguist.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127–133, Edmon-
ton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings ofACL, pages 177–180, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609–616, Sydney, Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proceedings ofACL, pages 704–711, June.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings ofACL/IJCNLP, August.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276–283, June.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP, pages 44–52, July.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP
2008, pages 206–214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192–199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440–447.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
ofACL, pages 311–318, Philadephia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings ofACL, pages
271–279, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-08: HLT, June.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901–904.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge. In Proceedings of IJCNLP
2005, pages 70–81.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A
dependency treelet string correspondence model for
statistical machine translation. In Proceedings of
SMT, pages 40–47.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. ofHLT-NAACL.
Min Zhang, Hongfei Jiang, Aiti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings ofMT-Summit.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of the
ACL/IJCNLP 2009.
</reference>
<page confidence="0.99452">
1442
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.766949">
<title confidence="0.992014">Constituency to Dependency Translation with Forests</title>
<author confidence="0.9989">Mi Liu</author>
<affiliation confidence="0.949557">Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.994642">P.O. Box 2704, Beijing 100190, China</address>
<abstract confidence="0.996009347826087">Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings ofACL ’89,</booktitle>
<pages>143--151</pages>
<contexts>
<context position="5983" citStr="Billot and Lang, 1989" startWordPosition="941" endWordPosition="944"> that translates Cc to De, O is the set of derivation, r is a constituency to dependency translation rule. 1According to the reports of Liu et al. (2009), their forestbased constituency-to-constituency system achieves a comparable performance against Moses (Koehn et al., 2007), but a significant improvement of +3.6 BLEU points over the 1- best tree-based constituency-to-constituency system. 2.1 Constituency Forests on the Source Side A constituency forest (in Figure 1 left) is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a contextfree grammar (Billot and Lang, 1989). More formally, following Huang (2008), such a constituency forest is a pair Fc = Gf = (V f, Hf), where V f is the set of nodes, and Hf the set of hyperedges. For a given source sentence c1:m = c1 ... cm, each node vf E V f is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, ci+1 ... cj). Each hyperedge hf E Hf is a pair (tails(hf), head(hf)), where head(hf) E V f is the consequent node in the deductive step, and tails(hf) E (V f)* is the list of antecedent nodes. For example, the hyperedge hf0 in Figure 1 for dedu</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings ofACL ’89, pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="23222" citStr="Charniak (2000)" startWordPosition="4001" endWordPosition="4002">. We calculate this dependency language model by simply replacing each ez in equation 9 with its tag t(ez). 5 Experiments 5.1 Data Preparation Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentenc</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy inspired parser. In Proceedings ofNAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="21579" citStr="Chiang (2005)" startWordPosition="3718" endWordPosition="3719"> head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,,,, and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. 4.1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al. (2008). For each nonterminal node vh = eh in De and its children sequences Ll = el1, el2...elz and Lr = er1, er2...erg, the probability of a trigram is computed as follows: P(Ll, Lr |eh§) = P(Ll |eh§) · P(Lr |eh§), (9) where the P(Ll |eh§) is decomposed to be: P(Ll |eh§) =P(el� |eh§) (10) · P(el2 |el1, eh§) ... ·P(el. |el</context>
<context position="28790" citStr="Chiang, 2005" startWordPosition="4929" endWordPosition="4930">hang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the System forestc2s Type c2s s2s c2d s2d 13.8M 9.0M BLEU 34.17 c2d s2s 13.8M 77.9M c2d s2d 13.8M 9.0M c2d s2s-dep 13.8M 77.9M 1440 other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3334" citStr="Chiang, 2007" startWordPosition="522" endWordPosition="523">or parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and depende</context>
<context position="21189" citStr="Chiang, 2007" startWordPosition="3653" endWordPosition="3654">ath and lead to translation failure. To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN (vf) by mapping the CFG rule into a target dependency tree using the head rules of Magerman (1995). Take the hyperedge hf0 in Figure1 for example, the corresponding pseudo translation rule is: NP(x1:NPB x2:CC x3:NPB) → (x1) (x2) x3, since the x3:NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,,,, and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. 4.1 Dependency Language Model Computing We compute the score of a dependency language mode</context>
<context position="27743" citStr="Chiang (2007)" startWordPosition="4765" endWordPosition="4766">ehn et al., 2007). So our baseline system is much better than the BLEU score (30.6+1) of the constituency-to-constituency system and Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree t</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>531--540</pages>
<contexts>
<context position="27487" citStr="Collins et al. (2005)" startWordPosition="4722" endWordPosition="4725"> the reports of Liu et al. (2009), with a more larger training corpus (FBIS plus 30K) but no name entity translations (+1 BLEU points if it is used), their forest-based constituency-to-constituency model achieves a BLEU score of 30.6, which is similar to Moses (Koehn et al., 2007). So our baseline system is much better than the BLEU score (30.6+1) of the constituency-to-constituency system and Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following t</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings ofACL, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="4080" citStr="Ding and Palmer, 2005" startWordPosition="632" endWordPosition="635">lize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (S</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of ACL, pages 541–548, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation. In</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP02.</booktitle>
<contexts>
<context position="29924" citStr="Fox, 2002" startWordPosition="5109" endWordPosition="5110">atched at high level, which is more reasonable than using glue rules in Shen et al. (2008)’s scenario; finally, the most important one is that our model runs very faster. Liu et al. (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002). 7 Conclusion and Future Work In this paper, we presented a novel forest-based constituency-to-dependency translation model, which combines the advantages of both tree-tostring and string-to-tree systems, runs fast and guarantees grammaticality of the output. To learn the constituency-to-dependency translation rules, we first identify the frontier set for all the nodes in the constituency forest on the source side. Then we fragment them and extract minimal rules. Finally, we glue them together to be composed rules. At the decoding step, we first parse the input sentence into a constituency fo</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In In Proceedings of EMNLP02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="13141" citStr="Galley et al., 2004" startWordPosition="2247" endWordPosition="2250">ting structure “ (talk) (with (Sharon)) ” can not be allowed in Shen et al. (2008)’s model. The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. 3.3 Rule Extraction Algorithm The algorithm shown in this Section is mainly extended from the forest-based tree-to-string extraction algorithm (Mi and Huang, 2008). We extract rules from word-aligned source constituency forest and target dependency tree pairs (see Figure 1) in three steps: (1) frontier set computation, (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006). However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. So we say a fragment is extractable if it respects to all restrictions. The root node of every extractable tree fragment corresponds to a faithful structure on the target side, in which case there is a “translational equivalence” between the subtree rooted at the node and th</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="3146" citStr="Galley et al., 2006" startWordPosition="494" endWordPosition="497">near time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16</context>
<context position="13287" citStr="Galley et al., 2006" startWordPosition="2274" endWordPosition="2277">ormed structure, since the head of word “a” is out of scope of this structure. 3.3 Rule Extraction Algorithm The algorithm shown in this Section is mainly extended from the forest-based tree-to-string extraction algorithm (Mi and Huang, 2008). We extract rules from word-aligned source constituency forest and target dependency tree pairs (see Figure 1) in three steps: (1) frontier set computation, (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006). However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. So we say a fragment is extractable if it respects to all restrictions. The root node of every extractable tree fragment corresponds to a faithful structure on the target side, in which case there is a “translational equivalence” between the subtree rooted at the node and the corresponding target structure. For example, in Figure 1, every node in the forest is annotated with its corresponding English structure. The NP</context>
<context position="17279" citStr="Galley et al. (2006)" startWordPosition="2985" endWordPosition="2988">sociated with a list of expansion sites (exps in line 5) being the subset of leaf nodes of the current fragment that are not in the frontier set. So each fragment along hyperedge h is associated with exps = tails(hf) \ fs. A fragment is complete if its expansion sites is empty (line 6), otherwise we pop one expansion node v′ to grow and spin-off new fragments by following hyperedges of v′, adding new expansion sites (lines 11-13), until all active fragments are complete and open queue is empty (line 4). After we get all the minimal rules, we glue them together to form composed rules following Galley et al. (2006). For example, the composed rule r1 in Figure 2 is glued by the following two minimal rules: 1437 a target side dependency tree De(o): IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) o* = arg max λ1 logP(o |Tc) r2 TcEFc,oEO → (x1) x4 (x2 (x3) ) CC (yˇu) → with r3 where x2:CC in r2 is replaced with r3 accordingly. 3.4 Fractional Counts and Rule Probabilities Following Mi and Huang (2008), we penalize a rule r by the posterior probability of the corresponding constituent tree fragment lhs(r), which can be computed in an Inside-Outside fashion, being the product of the outside probability of its root node, t</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hellwig</author>
</authors>
<title>Parsing with Dependency Grammars, volume II.</title>
<date>2006</date>
<booktitle>An International Handbook of Contemporary Research.</booktitle>
<contexts>
<context position="7944" citStr="Hellwig, 2006" startWordPosition="1304" endWordPosition="1306">ymbol and m is the sentence length. 2.2 Dependency Trees on the Target Side A dependency tree for a sentence represents each word and its syntactic dependents through directed arcs, as shown in the following examples. The main advantage of a dependency tree is that it can explore the long distance dependency. (1) P(O) � oEO H rEo P(r), � oEO 1434 1: talk IP NP x1:NPB CC x3:VPB →(x1) x3 (with (x2)) x2:NPB a 2: held Bush talk a with Sharon yˇu Figure 2: Example of the rule r1. The Chinese conjunction yˇu “and” is translated into English preposition “with”. We use the lexicon dependency grammar (Hellwig, 2006) to express a projective dependency tree. Take the dependency trees above for example, they will be expressed: 1: ( a ) talk 2: ( Bush ) held ( ( a ) talk ) ( with ( Sharon ) ) where the lexicons in brackets represent the dependencies, while the lexicon out the brackets is the head. More formally, a dependency tree is also a pair De = Gd = (V d, Hd). For a given target sentence e1:n = e1 ... en, each node vd E V d is a word ei (1 S i S n), each hyperedge hd E Hd is a directed arc (vdi , vdj) from node vdi to its head node vdj. Following the formalization of the constituency forest scenario, we</context>
</contexts>
<marker>Hellwig, 2006</marker>
<rawString>Peter Hellwig. 2006. Parsing with Dependency Grammars, volume II. An International Handbook of Contemporary Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<contexts>
<context position="21579" citStr="Huang and Chiang (2005)" startWordPosition="3716" endWordPosition="3719">NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,,,, and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. 4.1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al. (2008). For each nonterminal node vh = eh in De and its children sequences Ll = el1, el2...elz and Lr = er1, er2...erg, the probability of a trigram is computed as follows: P(Ll, Lr |eh§) = P(Ll |eh§) · P(Lr |eh§), (9) where the P(Ll |eh§) is decomposed to be: P(Ll |eh§) =P(el� |eh§) (10) · P(el2 |el1, eh§) ... ·P(el. |el</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="21214" citStr="Huang and Chiang, 2007" startWordPosition="3655" endWordPosition="3658">o translation failure. To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN (vf) by mapping the CFG rule into a target dependency tree using the head rules of Magerman (1995). Take the hyperedge hf0 in Figure1 for example, the corresponding pseudo translation rule is: NP(x1:NPB x2:CC x3:NPB) → (x1) (x2) x3, since the x3:NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,,,, and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. 4.1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree D</context>
<context position="27743" citStr="Huang and Chiang (2007)" startWordPosition="4763" endWordPosition="4766"> Moses (Koehn et al., 2007). So our baseline system is much better than the BLEU score (30.6+1) of the constituency-to-constituency system and Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree t</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings ofACL, pages 144–151, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="2270" citStr="Huang et al., 2006" startWordPosition="352" endWordPosition="355">he tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. tree on examples (partial) fast gram. BLEU source Liu06, Huang06 + - + target Galley06, Shen08 - + + both Ding05, Liu09 + + - both our work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee gram</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Binarization of synchronous contextfree grammars.</title>
<date>2009</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="2687" citStr="Huang et al., 2009" startWordPosition="420" endWordPosition="423">: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art fo</context>
</contexts>
<marker>Huang, Zhang, Gildea, 2009</marker>
<rawString>Liang Huang, Hao Zhang, Daniel Gildea, , and Kevin Knight. 2009. Binarization of synchronous contextfree grammars. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2389" citStr="Huang, 2008" startWordPosition="371" endWordPosition="372">ake into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. tree on examples (partial) fast gram. BLEU source Liu06, Huang06 + - + target Galley06, Shen08 - + + both Ding05, Liu09 + + - both our work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-</context>
<context position="6022" citStr="Huang (2008)" startWordPosition="948" endWordPosition="949">tion, r is a constituency to dependency translation rule. 1According to the reports of Liu et al. (2009), their forestbased constituency-to-constituency system achieves a comparable performance against Moses (Koehn et al., 2007), but a significant improvement of +3.6 BLEU points over the 1- best tree-based constituency-to-constituency system. 2.1 Constituency Forests on the Source Side A constituency forest (in Figure 1 left) is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a contextfree grammar (Billot and Lang, 1989). More formally, following Huang (2008), such a constituency forest is a pair Fc = Gf = (V f, Hf), where V f is the set of nodes, and Hf the set of hyperedges. For a given source sentence c1:m = c1 ... cm, each node vf E V f is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, ci+1 ... cj). Each hyperedge hf E Hf is a pair (tails(hf), head(hf)), where head(hf) E V f is the consequent node in the deductive step, and tails(hf) E (V f)* is the list of antecedent nodes. For example, the hyperedge hf0 in Figure 1 for deduction (*) NPB0,1 CC1,2 NPB2,3 NP0,3 , (</context>
<context position="9497" citStr="Huang (2008)" startWordPosition="1596" endWordPosition="1597"> with}. 2.3 Hypergraph Actually, both the constituency forest and the dependency tree can be formalized as a hypergraph G, a pair (V, H). We use Gf and Gd to distinguish them. For simplicity, we also use Fc and De to denote a constituency forest and a dependency tree respectively. Specifically, the size of tails(hd) of a hyperedge hd in a dependency tree is a constant one. 3 Rule Extraction We extract constituency to dependency rules from word-aligned source constituency forest and target dependency tree pairs (Figure 1). We mainly extend the tree-to-string rule extraction algorithm of Mi and Huang (2008) to our scenario. In this section, we first formalize the constituency to string translation rule (Section 3.1). Then we present the restrictions for dependency structures as well formed fragments (Section 3.2). Finally, we describe our rule extraction algorithm (Section 3.3), fractional counts computation and probabilities estimation (Section 3.4). 3.1 Constituency to Dependency Rule More formally, a constituency to dependency translation rule r is a tuple (lhs(r), rhs(r), O(r)), where lhs(r) is the source side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP an</context>
<context position="12909" citStr="Huang, 2008" startWordPosition="2214" endWordPosition="2215">ith (Sharon)) ” for example: partial fixed examples are “ (Bush) held ” and “ held ((a) talk)”; while the partial floating examples are “ (talk) (with (Sharon)) ” and “ ((a) talk) (with (Sharon)) ”. Please note that the floating structure “ (talk) (with (Sharon)) ” can not be allowed in Shen et al. (2008)’s model. The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. 3.3 Rule Extraction Algorithm The algorithm shown in this Section is mainly extended from the forest-based tree-to-string extraction algorithm (Mi and Huang, 2008). We extract rules from word-aligned source constituency forest and target dependency tree pairs (see Figure 1) in three steps: (1) frontier set computation, (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006). However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. So we say a fragment is extractable if it r</context>
<context position="14763" citStr="Huang (2008)" startWordPosition="2536" endWordPosition="2537">arget dependency tree De, and alignment a Output: Minimal rule set R 1: fs +– FRONTIER(Fc, De, a) &gt; compute frontier set 2: for each vf E fs do 3: open +– {(0, {vf})} &gt; initial queue of growing fragments 4: while open =� 0 do 5: (hs, exps) +– open.pop() &gt; extract a fragment 6: if exps = 0 then &gt; nothing to expand? 7: generate a rule r using fragment hs &gt; generate a rule 8: R.append(r) 9: else &gt; incomplete: further expand 10: v′ +– exps.pop() &gt; a non-frontier node 11: for each hf E IN (v′) do 12: newexps +– exps U (tails(hf) \ fs) &gt; expand 13: open.append((hs U {hf}, newexps)) Following Mi and Huang (2008), given a source target sentence pair (c1:m, e1:n) with an alignment a, the span of node vf on source forest is the set of target words aligned to leaf nodes under vf: span(vf) °_ {ei E e1:n |3cj E yield(vf), (cj, ei) E a}. where the yield(vf) is all the leaf nodes under vf. For each span(vf), we also denote dep(vf) to be its corresponding dependency structure, which represents the dependency structure of all the words in span(vf). Take the span(PP1,3) ={with, Sharon} for example, the corresponding dep(PP1,3) is “with (Sharon)”. A dep(vf) is faithful structure to node vf if it meets the follow</context>
<context position="17656" citStr="Huang (2008)" startWordPosition="3056" endWordPosition="3057"> of v′, adding new expansion sites (lines 11-13), until all active fragments are complete and open queue is empty (line 4). After we get all the minimal rules, we glue them together to form composed rules following Galley et al. (2006). For example, the composed rule r1 in Figure 2 is glued by the following two minimal rules: 1437 a target side dependency tree De(o): IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) o* = arg max λ1 logP(o |Tc) r2 TcEFc,oEO → (x1) x4 (x2 (x3) ) CC (yˇu) → with r3 where x2:CC in r2 is replaced with r3 accordingly. 3.4 Fractional Counts and Rule Probabilities Following Mi and Huang (2008), we penalize a rule r by the posterior probability of the corresponding constituent tree fragment lhs(r), which can be computed in an Inside-Outside fashion, being the product of the outside probability of its root node, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment. αβ(lhs(r)) =α(root(r)) Y· P(hf) hf ∈ lhs(r) (2) Y· β(vf) vf ∈ leaves(lhs(r)) where root(r) is the root of the rule r, α(v) and β(v) are the outside and inside probabilities of node v, and leaves(lhs(r)) returns the leaf nodes of a tree fragment lhs(r). We use fractional c</context>
<context position="27873" citStr="Huang (2008)" startWordPosition="4789" endWordPosition="4790">d Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="19892" citStr="Koehn et al., 2003" startWordPosition="3423" endWordPosition="3426"> the last two terms are derivation and translation length penalties, respectively. The conditional probability P(o |Tc) is decomposes into the product of rule probabilities: P(o |Tc) = Y P(r), (7) rEo where each P(r) is the product of five probabilities: P(r) =P(r |lhs(r))λ9 · P(r |rhs(r))λ10 · P(r |root(lhs(r)))λ11 · Pl&apos;�x(lhs(r) |rhs(r))λ1� (8) · Pl&apos;�x(rhs(r) |lhs(r))λ1�, where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.4, and the last two are lexical probabilities. When computing the lexical translation probabilities described in (Koehn et al., 2003), we only take into accout the terminals in a rule. If there is no terminal, we set the lexical probability to 1. The decoding algorithm works in a bottom-up search fashion by traversing each node in forest Fc. We first use pattern-matching algorithm of Mi et al. (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node vf will 2Pattern-matching failure at a node vf means there is no translation rule can be matched at vf or no translation hyperedge can be constructed at </context>
<context position="22977" citStr="Koehn et al., 2003" startWordPosition="3958" endWordPosition="3961"> a dependency language model for POS tages over a dependency tree. We store the POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ez in equation 9 with its tag t(ez). 5 Experiments 5.1 Data Preparation Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm desc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="4480" citStr="Koehn et al., 2007" startWordPosition="704" endWordPosition="707">onstituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into target dependency trees with a set of features (Section 4). Medium data experiments (Section 5) show a statistically significant improvement of +0.7 BLEU points over a stateof-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string counter</context>
<context position="27147" citStr="Koehn et al., 2007" startWordPosition="4668" endWordPosition="4671">ependency fragments, and construct the pseudo s2d rules (s2s-dep). Then we use c2d and s2s-dep rules to direct the translation. With the help of the dependency language model, our new model achieves a significant improvement of +0.7 BLEU points over the forest c2s baseline system (p &lt; 0.05, using the sign-test suggested by 3According to the reports of Liu et al. (2009), with a more larger training corpus (FBIS plus 30K) but no name entity translations (+1 BLEU points if it is used), their forest-based constituency-to-constituency model achieves a BLEU score of 30.6, which is similar to Moses (Koehn et al., 2007). So our baseline system is much better than the BLEU score (30.6+1) of the constituency-to-constituency system and Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL, pages 177–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2249" citStr="Liu et al., 2006" startWordPosition="348" endWordPosition="351">ous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. tree on examples (partial) fast gram. BLEU source Liu06, Huang06 + - + target Galley06, Shen08 - + + both Ding05, Liu09 + + - both our work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechan</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609–616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>704--711</pages>
<contexts>
<context position="28248" citStr="Liu et al., 2007" startWordPosition="4848" endWordPosition="4851">pt of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on th</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proceedings ofACL, pages 704–711, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL/IJCNLP,</booktitle>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings ofACL/IJCNLP, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="20808" citStr="Magerman (1995)" startWordPosition="3587" endWordPosition="3588">on forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node vf will 2Pattern-matching failure at a node vf means there is no translation rule can be matched at vf or no translation hyperedge can be constructed at vf. c(r) P(r |root(r)) = P r′: root (r′)=root(r) c(rf). (5) 1438 cut the derivation path and lead to translation failure. To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN (vf) by mapping the CFG rule into a target dependency tree using the head rules of Magerman (1995). Take the hyperedge hf0 in Figure1 for example, the corresponding pseudo translation rule is: NP(x1:NPB x2:CC x3:NPB) → (x1) (x2) x3, since the x3:NPB is the head word of the CFG rule: NP → NPB CC NPB. After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion. For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best deriva</context>
<context position="23322" citStr="Magerman (1995)" startWordPosition="4015" endWordPosition="4016"> t(ez). 5 Experiments 5.1 Data Preparation Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). W</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of ACL, pages 276–283, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Spmt: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="3167" citStr="Marcu et al., 2006" startWordPosition="498" endWordPosition="501">ime, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 As</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phrases. In Proceedings of EMNLP, pages 44–52, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008,</booktitle>
<pages>206--214</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2389" citStr="Mi and Huang, 2008" startWordPosition="369" endWordPosition="372"> also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. tree on examples (partial) fast gram. BLEU source Liu06, Huang06 + - + target Galley06, Shen08 - + + both Ding05, Liu09 + + - both our work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-</context>
<context position="9497" citStr="Mi and Huang (2008)" startWordPosition="1594" endWordPosition="1597">={talk, with}. 2.3 Hypergraph Actually, both the constituency forest and the dependency tree can be formalized as a hypergraph G, a pair (V, H). We use Gf and Gd to distinguish them. For simplicity, we also use Fc and De to denote a constituency forest and a dependency tree respectively. Specifically, the size of tails(hd) of a hyperedge hd in a dependency tree is a constant one. 3 Rule Extraction We extract constituency to dependency rules from word-aligned source constituency forest and target dependency tree pairs (Figure 1). We mainly extend the tree-to-string rule extraction algorithm of Mi and Huang (2008) to our scenario. In this section, we first formalize the constituency to string translation rule (Section 3.1). Then we present the restrictions for dependency structures as well formed fragments (Section 3.2). Finally, we describe our rule extraction algorithm (Section 3.3), fractional counts computation and probabilities estimation (Section 3.4). 3.1 Constituency to Dependency Rule More formally, a constituency to dependency translation rule r is a tuple (lhs(r), rhs(r), O(r)), where lhs(r) is the source side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP an</context>
<context position="12909" citStr="Mi and Huang, 2008" startWordPosition="2212" endWordPosition="2215">alk))(with (Sharon)) ” for example: partial fixed examples are “ (Bush) held ” and “ held ((a) talk)”; while the partial floating examples are “ (talk) (with (Sharon)) ” and “ ((a) talk) (with (Sharon)) ”. Please note that the floating structure “ (talk) (with (Sharon)) ” can not be allowed in Shen et al. (2008)’s model. The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. 3.3 Rule Extraction Algorithm The algorithm shown in this Section is mainly extended from the forest-based tree-to-string extraction algorithm (Mi and Huang, 2008). We extract rules from word-aligned source constituency forest and target dependency tree pairs (see Figure 1) in three steps: (1) frontier set computation, (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006). However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. So we say a fragment is extractable if it r</context>
<context position="14763" citStr="Mi and Huang (2008)" startWordPosition="2534" endWordPosition="2537">t Fc, target dependency tree De, and alignment a Output: Minimal rule set R 1: fs +– FRONTIER(Fc, De, a) &gt; compute frontier set 2: for each vf E fs do 3: open +– {(0, {vf})} &gt; initial queue of growing fragments 4: while open =� 0 do 5: (hs, exps) +– open.pop() &gt; extract a fragment 6: if exps = 0 then &gt; nothing to expand? 7: generate a rule r using fragment hs &gt; generate a rule 8: R.append(r) 9: else &gt; incomplete: further expand 10: v′ +– exps.pop() &gt; a non-frontier node 11: for each hf E IN (v′) do 12: newexps +– exps U (tails(hf) \ fs) &gt; expand 13: open.append((hs U {hf}, newexps)) Following Mi and Huang (2008), given a source target sentence pair (c1:m, e1:n) with an alignment a, the span of node vf on source forest is the set of target words aligned to leaf nodes under vf: span(vf) °_ {ei E e1:n |3cj E yield(vf), (cj, ei) E a}. where the yield(vf) is all the leaf nodes under vf. For each span(vf), we also denote dep(vf) to be its corresponding dependency structure, which represents the dependency structure of all the words in span(vf). Take the span(PP1,3) ={with, Sharon} for example, the corresponding dep(PP1,3) is “with (Sharon)”. A dep(vf) is faithful structure to node vf if it meets the follow</context>
<context position="17656" citStr="Mi and Huang (2008)" startWordPosition="3054" endWordPosition="3057">eredges of v′, adding new expansion sites (lines 11-13), until all active fragments are complete and open queue is empty (line 4). After we get all the minimal rules, we glue them together to form composed rules following Galley et al. (2006). For example, the composed rule r1 in Figure 2 is glued by the following two minimal rules: 1437 a target side dependency tree De(o): IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) o* = arg max λ1 logP(o |Tc) r2 TcEFc,oEO → (x1) x4 (x2 (x3) ) CC (yˇu) → with r3 where x2:CC in r2 is replaced with r3 accordingly. 3.4 Fractional Counts and Rule Probabilities Following Mi and Huang (2008), we penalize a rule r by the posterior probability of the corresponding constituent tree fragment lhs(r), which can be computed in an Inside-Outside fashion, being the product of the outside probability of its root node, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment. αβ(lhs(r)) =α(root(r)) Y· P(hf) hf ∈ lhs(r) (2) Y· β(vf) vf ∈ leaves(lhs(r)) where root(r) is the root of the rule r, α(v) and β(v) are the outside and inside probabilities of node v, and leaves(lhs(r)) returns the leaf nodes of a tree fragment lhs(r). We use fractional c</context>
<context position="27873" citStr="Mi and Huang (2008)" startWordPosition="4787" endWordPosition="4790">stem and Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency </context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP 2008, pages 206–214, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT,</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2369" citStr="Mi et al., 2008" startWordPosition="365" endWordPosition="368">r string. When we also take into account the type of output (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. tree on examples (partial) fast gram. BLEU source Liu06, Huang06 + - + target Galley06, Shen08 - + + both Ding05, Liu09 + + - both our work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the ot</context>
<context position="20162" citStr="Mi et al. (2008)" startWordPosition="3471" endWordPosition="3474">λ9 · P(r |rhs(r))λ10 · P(r |root(lhs(r)))λ11 · Pl&apos;�x(lhs(r) |rhs(r))λ1� (8) · Pl&apos;�x(rhs(r) |lhs(r))λ1�, where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.4, and the last two are lexical probabilities. When computing the lexical translation probabilities described in (Koehn et al., 2003), we only take into accout the terminals in a rule. If there is no terminal, we set the lexical probability to 1. The decoding algorithm works in a bottom-up search fashion by traversing each node in forest Fc. We first use pattern-matching algorithm of Mi et al. (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. However, pattern-matching failure2 at a node vf will 2Pattern-matching failure at a node vf means there is no translation rule can be matched at vf or no translation hyperedge can be constructed at vf. c(r) P(r |root(r)) = P r′: root (r′)=root(r) c(rf). (5) 1438 cut the derivation path and lead to translation failure. To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN (vf) by mapping the CFG rule into a target dependenc</context>
<context position="24552" citStr="Mi et al., 2008" startWordPosition="4220" endWordPosition="4223"> NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest cls for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s). The baseline system extracts 31.9M c2s rules, 77.9M s2s rules respectively and achieves a BLEU score of 34.17 on the test set3. At first, we investigate the influence of different rule sets on the performance of baseline system. We first restrict the target side of translation rules to be well-formed structures, and we extract 13.8M constituency-to-dependency (c2d) rules, which is 43% of c2s rules. We also extract 9.0M</context>
<context position="27849" citStr="Mi et al. (2008)" startWordPosition="4782" endWordPosition="4785">cy-to-constituency system and Moses. Rule Set # 31.9M 77.9M 32.48(↓1.7) 34.03(↓0.1) 33.25(↓0.9) forest c2d 34.88(T0.7) Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set. Collins et al. (2005)). For the first time, a tree-totree model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) presen</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08:HLT, pages 192–199, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="22917" citStr="Och and Ney, 2000" startWordPosition="3949" endWordPosition="3952">er to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store the POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ez in equation 9 with its tag t(ez). 5 Experiments 5.1 Data Preparation Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="24311" citStr="Och, 2003" startWordPosition="4183" endWordPosition="4184">oothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest cls for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s). The baseline system extracts 31.9M c2s rules, 77.9M s2s rules respectively and achieves a BLEU score of 34.17 on the test set3. At first, we investigate the influence of different r</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<location>Philadephia, USA,</location>
<contexts>
<context position="24129" citStr="Papineni et al., 2002" startWordPosition="4154" endWordPosition="4158">parately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. 5.2 Results Table 2 shows the results on the test set. Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest cls for short, which translates a source forest into a target string by pattern-matching the 1439 constituency-to-string (c2s) rules and the bilingual phrases (s2s).</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318, Philadephia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="4100" citStr="Quirk et al., 2005" startWordPosition="636" endWordPosition="639">ic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and trans</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings ofACL, pages 271–279, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<contexts>
<context position="3219" citStr="Shen et al., 2008" startWordPosition="505" endWordPosition="508">ry-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007). However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics a novel cons</context>
<context position="10680" citStr="Shen et al. (2008)" startWordPosition="1794" endWordPosition="1797"> by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source language words ci (like “yˇu”) or variables from a set X = {x1, x2,...}; rhs(r) is expressed in the target language dependency structure with words ej (like “with”) and variables from the set X; and O(r) is a mapping from X to nonterminals. Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r). For example, the rule r1 in Figure 2, lhs(r1) = IP(NP(x1 CC(yˇu) x2) x3), rhs(r1) = (x1) x3 (with (x2)), O(r1) = {x1 H NPB, x2 H NPB, x3 H VPB}. 3.2 Well Formed Dependency Fragment Following Shen et al. (2008), we also restrict rhs(r) to be well formed dependency fragment. The main difference between us is that we use more flexible restrictions. Given a dependency 1435 Minimal rules extracted NPB0,1 CC1,2 hf 0 P1,2 “with (Sharon)” PP1,3 NPB2,3 “with” “Sharon” “Bush” “with” IP0,5 “(Bush) .. Sharon))” hf hf 1 2 NP0,3 “(Bush) ⊔ (with (Sharon))” VP1,5 “held .. Sharon))” VPB3,5 “held ((a) talk)” IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) → (x1) x4 (x2 (x3) ) IP (x1:NPB x2:VP) → (x1) x2 VP (x1:PP x2:VPB) → x2 (x1) PP (x1:P x2:NPB) → x1 (x2) VPB (VV(j&apos;uxingle)) x1:NPB) → held ((a) x1) NPB (B`ush´a) → Bush NPB (h</context>
<context position="12603" citStr="Shen et al. (2008)" startWordPosition="2161" endWordPosition="2164">ng for short, if it meets the following conditions: • all nodes in M have a same head node, i.e.: ∃x ∈/ [i, j], ∀hd if tails(hd) ∈ M ⇒ head(hd) = vhx. • the heads of other nodes not in M are in [i, j], i.e.: ∀k ∈ [i, j] and vdk ∈/ M, ∀hd if tails(hd) = vdk ⇒ head(hd) ∈ ei:j. Take the “ (Bush) held ((a) talk))(with (Sharon)) ” for example: partial fixed examples are “ (Bush) held ” and “ held ((a) talk)”; while the partial floating examples are “ (talk) (with (Sharon)) ” and “ ((a) talk) (with (Sharon)) ”. Please note that the floating structure “ (talk) (with (Sharon)) ” can not be allowed in Shen et al. (2008)’s model. The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. 3.3 Rule Extraction Algorithm The algorithm shown in this Section is mainly extended from the forest-based tree-to-string extraction algorithm (Mi and Huang, 2008). We extract rules from word-aligned source constituency forest and target dependency tree pairs (see Figure 1) in three steps: (1) frontier set computation, (2) fragmentation, (3) composition. The frontier set (Galley et al., 2004) is the potential points to “cut” the forest and dependency tr</context>
<context position="21862" citStr="Shen et al. (2008)" startWordPosition="3764" endWordPosition="3767"> and compute all the feature scores including the dependency language model score (Section 4.1). If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,,,, and build a target side dependency tree. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. 4.1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al. (2008). For each nonterminal node vh = eh in De and its children sequences Ll = el1, el2...elz and Lr = er1, er2...erg, the probability of a trigram is computed as follows: P(Ll, Lr |eh§) = P(Ll |eh§) · P(Lr |eh§), (9) where the P(Ll |eh§) is decomposed to be: P(Ll |eh§) =P(el� |eh§) (10) · P(el2 |el1, eh§) ... ·P(el. |el.−1, el.−2). We use the suffix “§” to distinguish the head word and child words in the dependency language model. In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. We store the POS tag information on t</context>
<context position="28442" citStr="Shen et al. (2008)" startWordPosition="4879" endWordPosition="4882"> models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the System forestc2s Type c2s s2s c2d s2d 13.8M 9.0M BLEU 34.17 c2d s2s 13.8M 77.9M c2d s2d 13.8M 9.0M c2d s2s-d</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-08: HLT, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="23649" citStr="Stolcke, 2002" startWordPosition="4069" endWordPosition="4070"> Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus. At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set. We evaluate the translation quality using the BLEU-4 metric (Papineni et al., 2002), which is calculated by the script mteval-v11b.pl with its default setting which is case-insensitive matching of n-gram</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Parsing the Penn Chinese Treebank with Semantic Knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>70--81</pages>
<contexts>
<context position="23055" citStr="Xiong et al. (2005)" startWordPosition="3972" endWordPosition="3975">e POS tag information on the target side for each constituency-to-dependency rule. So we will also generate a POS taged dependency tree simultaneously at the decoding time. We calculate this dependency language model by simply replacing each ez in equation 9 with its tag t(ez). 5 Experiments 5.1 Data Preparation Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively. We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option “grow-diag-and” (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al. (2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3. We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. Finally, we apply translation rule extraction algorithm described in Section 3. We use SRI Language Modeling Toolkit (Stolcke, 2002) to tr</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the Penn Chinese Treebank with Semantic Knowledge. In Proceedings of IJCNLP 2005, pages 70–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A dependency treelet string correspondence model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of SMT,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="4120" citStr="Xiong et al., 2007" startWordPosition="640" endWordPosition="643">e source side. We thus combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constit</context>
</contexts>
<marker>Xiong, Liu, Lin, 2007</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical machine translation. In Proceedings of SMT, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation. In</title>
<date>2006</date>
<booktitle>Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="2666" citStr="Zhang et al., 2006" startWordPosition="416" endWordPosition="419">r work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees. Both string-toconstituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over th</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Jun Sun</author>
<author>Sheng Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>A tree-to-tree alignmentbased model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofMT-Summit.</booktitle>
<contexts>
<context position="4140" citStr="Zhang et al., 2007" startWordPosition="644" endWordPosition="647">us combine the advantages of both tree-tostring and string-to-tree approaches, and propose 1433 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output. In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrasebased and tree-to-string systems. Current tree-totree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into t</context>
<context position="28195" citStr="Zhang et al., 2007" startWordPosition="4842" endWordPosition="4845">icantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Com</context>
</contexts>
<marker>Zhang, Jiang, Aw, Sun, Li, Tan, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Jun Sun, Sheng Li, and Chew Lim Tan. 2007. A tree-to-tree alignmentbased model for statistical machine translation. In Proceedings ofMT-Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
</authors>
<title>Forest-based tree sequence to string translation model.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL/IJCNLP</booktitle>
<contexts>
<context position="2428" citStr="Zhang et al., 2009" startWordPosition="377" endWordPosition="380">tput (tree or string), the treebased systems can be divided into tree-to-string and tree-to-tree efforts. tree on examples (partial) fast gram. BLEU source Liu06, Huang06 + - + target Galley06, Shen08 - + + both Ding05, Liu09 + + - both our work + + + Table 1: A classification and comparison of linguistically syntax-based SMT systems, where gram. denotes grammaticality of the output. On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009). Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in stringbased models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006). However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. On the other hand, string-to-tree systems explicitly model the gramm</context>
<context position="28143" citStr="Zhang et al. (2009)" startWordPosition="4834" endWordPosition="4837">model can surpass tree-to-string counterparts significantly even with fewer rules. 6 Related Work The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. Mi et al. (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and Zhang et al. (2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art </context>
</contexts>
<marker>Zhang, Zhang, Li, Aw, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proceedings of the ACL/IJCNLP 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>