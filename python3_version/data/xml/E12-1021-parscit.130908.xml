<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007961">
<title confidence="0.994363">
Incorporating Lexical Priors into Topic Models
</title>
<author confidence="0.996534">
Jagadeesh Jagarlamudi Hal Daum´e III Raghavendra Udupa
</author>
<affiliation confidence="0.999864">
University of Maryland University of Maryland Microsoft Research
</affiliation>
<address confidence="0.64926">
College Park, USA College Park, USA Bangalore, India
</address>
<email confidence="0.999134">
jags@umiacs.umd.edu hal@umiacs.umd.edu raghavu@microsoft.com
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999393181818182">
Topic models have great potential for help-
ing users understand document corpora.
This potential is stymied by their purely un-
supervised nature, which often leads to top-
ics that are neither entirely meaningful nor
effective in extrinsic tasks (Chang et al.,
2009). We propose a simple and effective
way to guide topic models to learn topics
of specific interest to a user. We achieve
this by providing sets of seed words that a
user believes are representative of the un-
derlying topics in a corpus. Our model
uses these seeds to improve both topic-
word distributions (by biasing topics to pro-
duce appropriate seed words) and to im-
prove document-topic distributions (by bi-
asing documents to select topics related to
the seed words they contain). Extrinsic
evaluation on a document clustering task
reveals a significant improvement when us-
ing seed information, even over other mod-
els that use seed information naively.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999778">
Topic models such as Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) have emerged as a pow-
erful tool to analyze document collections in an
unsupervised fashion. When fit to a document
collection, topic models implicitly use document
level co-occurrence information to group seman-
tically related words into a single topic. Since the
objective of these models is to maximize the prob-
ability of the observed data, they have a tendency
to explain only the most obvious and superficial
aspects of a corpus. They effectively sacrifice per-
formance on rare topics to do abetter job in mod-
eling frequently occurring words. The user is then
left with a skewed impression of the corpus, and
perhaps one that does not perform well in extrin-
sic tasks.
To illustrate this problem, we ran LDA on
the most frequent five categories of the Reuters-
21578 (Lewis et al., 2004) text corpus. This doc-
ument distribution is very skewed: more than half
of the collection belongs to the most frequent cat-
egory (“Earn”). The five topics identified by the
LDA are shown in Table 1. A brief observation
of the topics reveals that LDA has roughly allo-
cated topics 1 &amp; 2 for the most frequent class
(“Earn”) and one topic for the subsequent two
frequent classes (“Acquisition” and “Forex”) and
merged the least two frequent classes (“Crude”
and “Grain”) into a single topic. The red colored
words in topic 5 correspond to the “Crude” class
and blue words are from the “Grain” class.
This leads to the situation where the topics
identified by LDA are not in accordance with the
underlying topical structure of the corpus. This
is a problem not just with LDA: it is potentially
a problem with any extension thereof that have
focused on improving the semantic coherence of
the words in each topic (Griffiths et al., 2005;
Wallach, 2005; Griffiths et al., 2007), the doc-
ument topic distributions (Blei and McAuliffe,
2008; Lacoste-Julien et al., 2008) or other aspects
(Blei. and Lafferty., 2009).
We address this problem by providing some ad-
ditional information to the model. Initially, along
with the document collection, a user may provide
higher level view of the document collection. For
instance, as discussed in Section 4.4, when run
on historical NIPS papers, LDA fails to find top-
ics related to Brain Imaging, Cognitive Science or
Hardware, even though we know from the call for
</bodyText>
<page confidence="0.981117">
204
</page>
<note confidence="0.981496">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204–213,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.7567566">
mln, dlrs, billion, year, pct, company, share, april, record, cts, quarter, march, earnings, stg, first, pay
mln, NUM, cts, loss, net, dlrs, shr, profit, revs, year, note, oper, avg, shrs, sales, includes
lt, company, shares, corp, dlrs, stock, offer, group, share, common, board, acquisition, shareholders
bank, market, dollar, pct, exchange, foreign, trade, rate, banks, japan, yen, government, rates, today
oil, tonnes, prices, mln, wheat, production, pct, gas, year, grain, crude, price, corn, dlrs, bpd, opec
</bodyText>
<tableCaption confidence="0.9563155">
Table 1: Topics identified by LDA on the frequent-5 categories of the Reuters corpus. The categories are Earn,
Acquisition, Forex, Grain and Crude (in the order document frequency).
</tableCaption>
<figure confidence="0.9735066">
1
2
3
4
5
</figure>
<tableCaption confidence="0.970447">
Table 2: An example for sets of seed words (seed top-
</tableCaption>
<bodyText confidence="0.9136864375">
ics) for the frequent-5 categories of the Reuters-21578
categorization corpus. We use them as running exam-
ple in the rest of the paper.
papers that such topics should exist in the corpus.
By allowing the user to provide some seed words
related to these underrepresented topics, we en-
courage the model to find evidence of these top-
ics in the data. Importantly, we only encourage
the model to follow the seed sets and do not force
it. So if it has compelling evidence in the data
to overcome the seed information then it still has
the freedom to do so. Our seeding approach in
combination with the interactive topic modeling
(Hu et al., 2011) will allow a user to both explore
a corpus, and also guide the exploration towards
the distinctions that he/she finds more interesting.
</bodyText>
<sectionHeader confidence="0.99739" genericHeader="introduction">
2 Incorporating Seeds
</sectionHeader>
<bodyText confidence="0.999711064516129">
Our approach to allowing a user to guide the topic
discovery process is to let him provide seed infor-
mation at the level of word type. Namely, the user
provides sets of seed words that are representative
of the corpus. Table 2 shows an example of seed
sets one might use for the Reuters corpus. This
kind of supervision is similar to the seeding in
bootstrapping literature (Thelen and Riloff, 2002)
or prototype-based learning (Haghighi and Klein,
2006). Our reliance on seed sets is orthogonal
to existing approaches that use external knowl-
edge, which operate at the level of documents
(Blei and McAuliffe, 2008), tokens (Andrzejew-
ski and Zhu, 2009) or pair-wise constraints (An-
drzejewski et al., 2009).
We build a model that uses the seed words
in two ways: to improve both topic-word and
document-topic probability distributions. For
ease of exposition, we present these ideas sep-
arately and then in combination (Section 2.3).
To improve topic-word distributions, we set up
a model in which each topic prefers to gener-
ate words that are related to the words in a seed
set (Section 2.1). To improve document-topic
distributions, we encourage the model to select
document-level topics based on the existence of
input seed words in that document (Section 2.2).
Before moving on to the details of our mod-
els, we briefly recall the generative story of the
LDA model and the reader is encouraged to refer
to (Blei et al., 2003) for further details.
</bodyText>
<listItem confidence="0.999270166666667">
1. For each topic k = 1 • • • T,
• choose Ok — Dir(Q).
2. For each document d, choose Bd — Dir(α).
• For each token i = 1 • • • Nd:
(a) Select a topic zz — Mult(Bd).
(b) Select a word wz — Mult(Ozi).
</listItem>
<bodyText confidence="0.99977025">
where T is the number of topics, α, Q are hyper-
parameters of the model and Ok and Bd are topic-
word and document-topic Multinomial probabil-
ity distributions respectively.
</bodyText>
<subsectionHeader confidence="0.985505">
2.1 Word-Topic Distributions (Model 1)
</subsectionHeader>
<bodyText confidence="0.999368882352941">
In regular topic models, each topic k is defined
by a Multinomial distribution Ok over words. We
extend this notion and instead define a topic as a
mixture of two Multinomial distributions: a “seed
topic” distribution and a “regular topic” distribu-
tion. The seed topic distribution is constrained to
only generate words from a corresponding seed
set. The regular topic distribution may generate
any word (including seed words). For example,
seed topic 4 (in Table 2) can only generate the
five words in its set. The word “oil” can be gener-
ated by seed topics 4 and 5, as well as any regular
company, billion, quarter, shrs, earnings
acquisition, procurement, merge
exchange, currency, trading, rate, euro
grain, wheat, corn, oilseed, oil
natural, gas, oil, fuel, products, petrol
</bodyText>
<page confidence="0.996522">
205
</page>
<figure confidence="0.64455">
doc
</figure>
<figureCaption confidence="0.976618">
Figure 1: Tree representation of a document in Model
1.
</figureCaption>
<bodyText confidence="0.999745631578947">
topic. We want to emphasize that, like any regular
topic, each seed topic is a non-uniform probabil-
ity distribution over the words in its set. The user
only inputs the sets of seed words and the model
will infer their probability distributions.
For the sake of simplicity, we describe our
model by assuming a one-to-one correspondence
between seed and regular topics. This assumption
can be easily relaxed by duplicating the seed top-
ics when there are more regular topics. As shown
in Fig. 1, each document is a mixture over T top-
ics, where each of those topics is a mixture of
a regular topic (φr� ) and its associated seed topic
(φs� ) distributions. The parameter πk controls the
probability of drawing a word from the seed topic
distribution versus the regular topic distribution.
For our first model, we assume that the corpus is
generated based on the following generative pro-
cess (its graphical notation is shown in Fig. 2(a)):
</bodyText>
<listItem confidence="0.999415928571429">
1. For each topic k=1· · · T,
(a) Choose regular topic φrk — Dir(βr).
(b) Choose seed topic φsk — Dir(βs).
(c) Choose πk — Beta(1,1).
2. For each document d, choose θd — Dir(α).
• For each token i = 1 · · · Nd:
(a) Select a topic zi — Mult(θd).
(b) Select an indicator xi — Bern(πzi)
(c) if xi is 0
– Select a word wi — Mult(φrzi).
// choose from regular topic
(d) if xi is 1
– Select a word wi — Mult(φszi).
// choose from seed topic
</listItem>
<bodyText confidence="0.997392512820513">
The first step is to generate Multinomial distribu-
tions for both seed topics and regular topics. The
seed topics are drawn in a way that constrains
their distribution to only generate words in the
corresponding seed set. Then, for each token in a
document, we first generate a topic. After choos-
ing a topic, we flip a (biased) coin to pick either
the seed or the regular topic distribution. Once
this distribution is selected we generate a word
from it. It is important to note that although there
are 2xT topic-word distributions in total, each
document is still a mixture of only T topics (as
shown in Fig. 1). This is crucial in relating seed
and regular topics and is similar to the way top-
ics and aspects are tied in TAM model (Paul and
Girju, 2010).
To understand how this model gathers words
related to seed words, consider a seed topic (say
the fourth row in Table 2) with seed words {grain,
wheat, corn, etc. }. Now by assigning all the re-
lated words such as “tonnes”, “agriculture”, “pro-
duction” etc. to its corresponding regular topic,
the model can potentially put high probability
mass on topic z = 4 for agriculture related doc-
uments. Instead, if it places these words in an-
other regular topic, say z = 3, then the document
probability mass has to be distributed among top-
ics 3 and 4 and as a result the model will pay a
steeper penalty. Thus the model uses seed topic
to gather related words into its associated regu-
lar topic and as a consequence the document-topic
distributions also become focussed.
We have experimented with two ways of choos-
ing the binary variable xi (step 2b) of the gener-
ative story. In the first method, we fix this sam-
pling probability to a constant value which is in-
dependent of the chosen topic (i.e. πi = fr, Vi =
1 · · · T). And in the second method we learn the
probability as well (Sec. 4).
</bodyText>
<subsectionHeader confidence="0.998101">
2.2 Document-Topic distributions (Model 2)
</subsectionHeader>
<bodyText confidence="0.999989076923077">
In the previous model we used seed words to im-
prove topic-word probability distributions. Here
we propose a model to explore the use of seed
words to improve document-topic probability dis-
tributions. Unlike the previous model, we will
present this model in the general case where the
number of seed topics is not equal to the number
of regular topics. Hence, we associate each seed
set (we refer seed set as group for conciseness)
with a Multinomial distribution over the regular
topics which we call group-topic distribution.
To give an overview of our model, first, we
transfer the seed information from words onto
</bodyText>
<equation confidence="0.9918336">
z=1 z=2 ······ · · · z=T
1 — π1 π1 1 — πT πT
φs1 φr φs
T T
φr1
</equation>
<page confidence="0.989903">
206
</page>
<figure confidence="0.999636277777778">
βr
φs
α θ
φr
T
x z
w
Nd
D
βr
α ψ θ
φr
T
γ
b~
τ
ζ
g
w
z
Nd
D
βr
α ψ θ
φr
φs
T
γ
b~
x z
τ
ζ
g
wNd
D
(a) Model 1 (b) Model 2 (c) SeededLDA
</figure>
<figureCaption confidence="0.999768">
Figure 2: The graphical notation of all the three models. In Model 1 we use seed topics to improve the topic-word
</figureCaption>
<bodyText confidence="0.952940714285714">
probability distributions. In Model 2, the seed topic information is first transfered to the document level based
on the document tokens and then it is used to improve document-topic distributions. In the final, SeededLDA,
model we combine both the models. In Model 1 and SeededLDA, we dropped the dependency of φ3 on hyper
parameter β3 since it is observed. And, for clarity, we also dropped the dependency of x on π.
the documents that contain them. Then, the
document-topic distribution is drawn in a two step
process: we sample a seed set (g for group) and
then use its group-topic distribution (ψg) as prior
to draw the document-topic distribution (θd). We
used this two step process, to allow flexible num-
ber of seed and regular topics, and to tie the topic
distributions of all the documents within a group.
We assume the following generative story and its
graphical notation is shown in Fig. 2(b).
</bodyText>
<listItem confidence="0.9811858">
1. For each k = 1··· T,
(a) Choose φr � ∼ Dir(βr).
2. For each seed set s = 1· · · S,
(a) Choose group-topic distributionψs ∼
Dir(α). // the topic distribution for sth
group (seed set) – a vector of length T.
3. For each document d,
(a) Choose a binary vector ~b of length S.
(b) Choose a document-group distribution
ζd ∼ Dir(τ~b).
(c) Choose a group variable g ∼ Mult(ζd)
(d) Choose θd ∼ Dir(ψg). // of length T
(e) For each token i = 1 · · · Nd:
i. Select a topic zi ∼ Mult(θd).
ii. Select a word wi ∼ Mult(φr�z).
</listItem>
<bodyText confidence="0.999941352941176">
We first generate T topic-word distributions
(φ0 and S group-topic distributions (ψs). Then
for each document, we generate a list of seed sets
that are allowed for this document. This list is
represented using the binary vector ~b. This bi-
nary vector can be populated based on the docu-
ment words and hence it is treated as an observed
variable. For example, consider the (very short!)
document “oil companies have merged”. Accord-
ing to the seed sets from Table 2, we define a bi-
nary vector that denotes which seed topics contain
words in this document. In this case, this vec-
tor b~ = h1, 1, 0,1,1i, indicating the presence of
seeds from sets 1, 2, 4 and 5.1 As discussed in
(Williamson et al., 2010), generating binary vec-
tor is crucial if we want a document to talk about
topics that are less prominent in the corpus.
The binary vector ~b, that indicates which seeds
exist in this document, defines a mean of a
Dirichlet distribution from which we sample a
document-group distribution, ζd (step 3b). We
set the concentration of this Dirichlet to a hy-
perparamter τ, which we set by hand (Sec. 4);
thus, ζd ∼ Dir(τ~b). From the resulting multino-
mial, we draw a group variable g for this docu-
ment. This group variable brings clustering struc-
ture among the documents by grouping the docu-
ments that are likely to talk about same seed set.
Once the group variable (g) is drawn, we
choose the document-topic distribution (θd) from
a Dirichlet distribution with the group’s-topic dis-
tribution as the prior (step 3d). This step ensures
that the topic distributions of documents within
each group are related. The remaining sampling
</bodyText>
<footnote confidence="0.9831235">
1As a special case, if no seed word is found in the docu-
ment, b is defined as the all-ones vector.
</footnote>
<page confidence="0.994997">
207
</page>
<bodyText confidence="0.999953166666667">
process proceeds like LDA. We sample a topic
for each word and then generate a word from its
corresponding topic-word distribution. Observe
that, if the binary vector is all ones and if we
set θd = ζd then this model reduces to the LDA
model with τ and βr as the hyperparameters.
</bodyText>
<subsectionHeader confidence="0.982576">
2.3 SeededLDA
</subsectionHeader>
<bodyText confidence="0.9999575">
Both of our models use seed words in different
ways to improve topic-word and document-topic
distributions respectively. We can combine both
the above models easily. We refer to the combined
model as SeededLDA and its generative story is
as follows (its graphical notation is shown in Fig.
2(c)). The variables have same semantics as in the
previous models.
</bodyText>
<listItem confidence="0.99480325">
1. For each k=1. • • T,
(a) Choose regular topic φrk N Dir(βr).
(b) Choose seed topic φsk N Dir(βs).
(c) Choose πk N Beta(1,1).
</listItem>
<subsectionHeader confidence="0.993865">
2.4 Automatic Seed Selection
</subsectionHeader>
<bodyText confidence="0.99984095">
In (Andrzejewski and Zhu, 2009; Andrzejewski
et al., 2009), the seed information is provided
manually. Here, we describe the use of feature se-
lection techniques, prevalent in the classification
literature, to automatically derive the seed sets. If
we want the topicality structure identified by the
LDA to align with the underlying class structure,
then the seed words need to be representative of
the underlying topicality structure. To enable this,
we first take class labeled data (doesn’t need to
be multi-class labeled data unlike (Ramage et al.,
2009)) and identify the discriminating features for
each class. Then we choose these discriminating
features as the initial sets of seed words. In prin-
ciple, this is similar to the prototype driven unsu-
pervised learning (Haghighi and Klein, 2006).
We use Information Gain (Mitchell, 1997) to
identify the required discriminating features. The
Information Gain (IG) of a word (w) in a class (c)
is given by
</bodyText>
<figure confidence="0.7751348">
2. For each seed set s = 1• • • S, N IG(c, w) = H(c) − H(c|w)
(a) Choose group-topic distributionψs
Dir(α).
3. For each document d,
(a) Choose a binary vector ~b of length S.
</figure>
<listItem confidence="0.989243545454546">
(b) Choose a document-group distribution
ζd N Dir(τ~b).
(c) Choose a group variable g N Mult(ζd).
(d) Choose θd N Dir(ψg). // of length T
(e) For each token i = 1 • • • Nd:
i. Select a topic zi N Mult(θd).
ii. Select an indicator xi N Bern(πzi).
iii. if xi is 0
• Select a word wi N Mult(φrzi).
iv. if xi is 1
• Select a word wi N Mult(φszi).
</listItem>
<bodyText confidence="0.99995765">
In the SeededLDA model, the process for gen-
erating group variable of a document is same as
the one described in the Model 2. And like in the
Model 2, we sample a document-topic probability
distribution as a Dirichlet draw with the group-
topic distribution of the chosen group as prior.
Subsequently, we choose a topic for each token
and then flip a biased coin. We choose either the
seed or the regular topic based on the result of the
coin toss and then generate a word from its distri-
bution.
where H(c) is the entropy of the class and H(c|w)
is the conditional entropy of the class given the
word. In computing Information Gain, we bina-
rize the document vectors and consider whether a
word occurs in any document of a given class or
not. Thus obtained ranked list of words for each
class are filtered for ambiguous words and then
used as initial sets of seed words to be input to the
model.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99865675">
Seed-based supervision is closely related to the
idea of seeding in the bootstrapping literature for
learning semantic lexicons (Thelen and Riloff,
2002). The goals are similar as well: growing
a small set of seed examples into a much larger
set. A key difference is the type of semantic in-
formation that the two approaches aim to capture:
semantic lexicons are based on much more spe-
cific notions of semantics (e.g. all the country
names) than the generic “topic” semantics of topic
models. The idea of seeding has also been used
in prototype-driven learning (Haghighi and Klein,
2006) and shown similar efficacies for these semi-
supervised learning approaches.
LDAWN (Boyd-Graber et al., 2007) models
sets of words for the word sense disambiguation
</bodyText>
<page confidence="0.992115">
208
</page>
<bodyText confidence="0.999969873239437">
task. It assumes that a topic is a distribution
over synsets and relies on the Wordnet to obtain
the synsets. The most related prior work is that
of (Andrzejewski et al., 2009), who propose the
use Dirichlet Forest priors to incorporate Must
Link and Cannot Link constraints into the topic
models. This work is analogous to constrained
K-means clustering (Wagstaff et al., 2001; Basu
et al., 2008). A must link between a pair word
types represents that the model should encourage
both the words to have either high or low prob-
ability in any particular topic. A cannot link be-
tween a word pair indicates both the words should
not have high probability in a single topic. In the
Dirichlet Forest approach, the constraints are first
converted into trees with words as the leaves and
edges having pre-defined weights. All the trees
are joined to a dummy node to form a forest. The
sampling for a word translates into a random walk
on the forest: starting from the root and selecting
one of its children based on the edge weights until
you reach a leaf node.
While the Dirichlet Forest method requires su-
pervision in terms of Must link and Cannot link
information, the Topics In Sets (Andrzejewski and
Zhu, 2009) model proposes a different approach.
Here, the supervision is provided at the token
level. The user chooses specific tokens and re-
strict them to occur only with in a specified list of
topics. While this needs minimal changes to the
inference process of LDA, it requires information
at the level of tokens. The word type level seed
information can be converted into token level in-
formation (like we do in Sec. 4) but this prevents
their model from distinguishing the tokens based
on the word senses.
Several models have been proposed which use
supervision at the document level. Supervised
LDA (Blei and McAuliffe, 2008) and DiscLDA
(Lacoste-Julien et al., 2008) try to predict the cat-
egory labels (e.g. sentiment classification) for
the input documents based on a document labeled
data. Of these models, the most related one to
SeededLDA is the LabeledLDA model (Ramage
et al., 2009). Their model operates on multi-class
labeled corpus. Each document is assumed to be
a mixture over a known subset of topics (classes)
with each topic being a distribution over words.
The process of generating document topic distri-
bution in LabeledLDA is similar to the process
of generating group distribution in our Model 2
(Sec. 2.2). However our model differs from La-
beledLDA in the subsequent steps. Rather than
using the group distribution directly, we sam-
ple a group variable and use it to constrain the
document-topic distributions of all the documents
within this group. Moreover, in their model the
binary vector is observed directly in the form of
document labels while, in our case, it is automat-
ically populated based on the document tokens.
Interactive topic modeling brings the user into
the loop, by allowing him/her to make suggestions
on how to improve the quality of the topics at each
iteration (Hu et al., 2011). In their approach, the
authors use Dirichlet Forest method to incorpo-
rate the user’s preferences. In our experiments
(Sec. 4), we show that SeededLDA performs bet-
ter than Dirichlet Forest method, so SeededLDA
when used with their framework can allow an user
to explore a document collection in a more mean-
ingful manner.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999858931034483">
We evaluate different aspects of the model sep-
arately. Our experimental setup proceeds as fol-
lows: a) Using an existing model, we evaluate the
effectiveness of automatically derived constraints
indicating the potential benefits of adding seed
words into the topic models. b) We evaluate each
of our proposed models in different settings and
compare with multiple baseline systems.
Since our aim is to overcome the domi-
nance of majority topics by encouraging the
topicality structure identified by the topic mod-
els to align with that of the document cor-
pus, we choose extrinsic evaluation as the
primary evaluation method. We use docu-
ment clustering task and use frequent-5 cate-
gories of Reuters-21578 corpus (Lewis et al.,
2004) and four classes from the 20 News-
groups data set (i.e.‘rec.autos’, ‘sci.electronics’,
‘comp.hardware’ and ‘alt.atheism’). For both
the corpora we do the standard preprocessing
of removing stopwords and infrequent words
(Williamson et al., 2010).
For all the models, we use a Collapsed Gibbs
sampler (Griffiths and Steyvers, 2004) for the in-
ference process. We use the standard hyperparam-
eters values α = 1.0, Q = 0.01 and T = 1.0 and
run the sampler for 1000 iterations, but one can
use techniques like slice sampling to estimate the
hyperparameters (Johnson and Goldwater, 2009).
</bodyText>
<page confidence="0.997337">
209
</page>
<table confidence="0.9966524">
Reuters 20 Newsgroups
F-measure VI F-measure VI
LDA 0.64 (±.05) 1.26 (±.16) 0.77 (±.06) 0.9 (±.13)
Dirichlet Forest 0.67* (±.02) 1.17 (±.11) 0.79(±.01) 0.83*(±.03)
A over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%)
</table>
<tableCaption confidence="0.990511">
Table 3: The effect of adding constraints by Dirichlet Forest Encoding. For Variational Information (VI) a lower
score indicates a better clustering. * indicates statistical significance at p = 0.01 as measured by the t-test. All
the four improvements are significant at p = 0.05.
</tableCaption>
<bodyText confidence="0.996392071428571">
We run all the models with the same number of
topics as the number of clusters. Then, for each
document, we find the topic that has maximum
probability in the posterior document-topic distri-
bution and assign it to that cluster. The accuracy
of the document clustering is measured in terms
of F-measure and Variation of Information. F-
measure is calculated based on the pairs of doc-
uments, i.e. if two documents belong to a cluster
in both ground truth and the clustering proposed
by the system then it is counted as correct, other-
wise it is counted as wrong. Variational Informa-
tion (VI) of two clusterings X and Y is given as
(Meil˘a, 2007):
</bodyText>
<equation confidence="0.997018">
VI(X,Y) = H(X) + H(Y ) − 2I(X, Y )
</equation>
<bodyText confidence="0.999618428571428">
where H(X) denotes the entropy of the clustering
X and I(X, Y ) denotes the mutual information
between the two clusterings. For VI, a lower value
indicates a better clustering. All the accuracies are
averaged over 25 different random initializations
and all the significance results are measured using
the t-test at p = 0.01.
</bodyText>
<subsectionHeader confidence="0.999255">
4.1 Seed Extraction
</subsectionHeader>
<bodyText confidence="0.999954130434783">
The seeds were extracted automatically (Sec. 2.4)
based on a small sample of labeled data other than
the test data. We first extract 25 seeds words per
each class and then remove the seed words that
appear in more than one class. After this filtering,
on an average, we are left with 9 and 15 words per
each seed topic for Reuters and 20 Newsgroups
corpora respectively.
We use the existing Dirichlet Forest method to
evaluate the effectiveness of the automatically ex-
tracted seed words. The Must and Cannot links
required for the supervision (Andrzejewski et al.,
2009) are automatically obtained by adding a
must-link between every pair of words belonging
to the same seed set and a split constraint between
every pair of words belonging to different sets.
The accuracies are averaged over 25 different ran-
dom initializations and are shown in Table 3. We
have also indicated the relative performance gains
compared to LDA. The significant improvement
over the plain LDA demonstrates the effectiveness
of the automatic extraction of seed words in topic
models.
</bodyText>
<subsectionHeader confidence="0.995478">
4.2 Document Clustering
</subsectionHeader>
<bodyText confidence="0.999994291666667">
In the next experiment, we compare our models
with LDA and other baselines. The first baseline
(maxCluster) simply counts the number of tokens
in each document from each of the seed topics and
assigns the document to the seed topic that has
most tokens. This results in a clustering of doc-
uments based on the seed topic they are assigned
to. This baseline evaluates the effectiveness of the
seed words with respect to the underlying cluster-
ing. Apart from the maxCluster baseline, we use
LDA and z-labels (Andrzejewski and Zhu, 2009)
as our baselines. For z-labels, we treat all the to-
kens of a seed word in the same way. Table 4
shows the comparison of our models with respect
to the baseline systems.2 Comparing the perfor-
mance of maxCluster to that of LDA, we observe
that the seed words themselves do a poor job in
clustering the documents.
We experimented with two variants of Model 1.
In the first run (Model 1) we sample the Irk value,
i.e. the probability of choosing a seed topic for
each topic. While in the ‘Model 1 (fr = 0.7)’ run,
we fix this probability to a constant value of 0.7 ir-
respective of the topic.3 Though both the models
</bodyText>
<footnote confidence="0.804877">
2The code used for LDA baseline in Tables 3 and 4
are different. For Table 3, we use the code available from
http://pages.cs.wisc.edu/∼andrzeje/research/df lda.html.
We use our own version for Table 4. We tried to produce
a comparable baseline by running the former for more
iterations and with different hyperparameters. In Table 3,
we report their best results.
3We chose this value based on intuition; it is not tuned.
</footnote>
<page confidence="0.988418">
210
</page>
<table confidence="0.998184363636364">
Reuters 20 Newsgroups
F-measure VI F-measure VI
maxCluster 0.53 1.75 0.58 1.44
LDA 0.66 (f.04) 1.2 (f.12) 0.76 (f.06) 0.9 (f.14)
z-labels 0.73 (f.01) 1.04 (f.01) 0.8 (f.00) 0.82 (f.01)
A over LDA (+10.6%) (-13.3%) (+5.26%) (-8.8%)
Model 1 0.69 (f.00) 1.13 (f.01) 0.8 (f.01) 0.81 (f.02)
Model 1 (fr = 0.7) 0.73 (f.00) 1.09 (f.01) 0.8 (f.01) 0.81 (f.02)
Model 2 0.66 (f.04) 1.22 (f.1) 0.77 (f.07) 0.85 (f.12)
SeededLDA 0.76* (f.01) 0.99* (f.03) 0.81* (f.01) 0.75* (f.02)
A over LDA (+15.5%) (-17.5%) (+6.58%) (-16.7%)
</table>
<tableCaption confidence="0.941051666666667">
Table 4: Accuracies on document clustering task with different models. * indicates significant improvement
compared to the z-labels approach, as measured by the t-test with p = 0.01. The relative performance gains are
with respect to the LDA model and are provided for comparison with Dirichlet Forest method (in Table 3.)
</tableCaption>
<bodyText confidence="0.999911979166667">
performed better than LDA, fixing the probabil-
ity gave better results. When we attempt to learn
this value, the model chooses to explain some of
the seed words by the regular topics. On the other
hand, when π is fixed, it explains almost all the
seed words based on the seed topics. The next
row (Model 2) indicates the performance of our
second model on the same data sets. The first
model seems to be performing better than the sec-
ond model, which is justifiable since the latter
uses seed topics indirectly. Though the variants
of Model 1 and Model 2 performed better than
the LDA, they fell short of the z-labels approach.
Table 4 also shows the performance of our com-
bined model (SeededLDA) on both the corpora.
When the models are combined, the performance
improves over each of them and is also better than
the baseline systems. As explained before, our in-
dividual models improve both the topic-word and
document-topic distributions respectively. But it
turns out that the knowledge learnt by both the in-
dividual models is complementary to each other.
As a result the combined model performed better
than the individual models and other baseline sys-
tems. Comparing the last rows of Tables 4 and 3,
we notice that the relative performance gains ob-
served in the case of SeededLDA is significantly
higher than the performance gains obtained by
incorporating the constraints using the Dirichlet
Forest method. Moreover, as indicated in the Ta-
ble 4, SeededLDA achieves significant gains over
the z-labels approach as well.
We have also provided the standard intervals
for each of the approaches. A quick inspection of
these intervals reveals the superior performance
of SeededLDA compared to all the baselines. The
standard deviation of the F-measures over dif-
ferent random initializations of our our model is
about 1% for both the corpora while it is 4% and
6% for the LDA on Reuters and 20 Newsgroups
corpora respectively. The reduction in the vari-
ance, across all the approaches that use seed infor-
mation, shows the increased robustness of the in-
ference process when using seed words. From the
accuracies in both the tables, it is clear that Seed-
edLDA model out-performs other models which
try to incorporate seed information into the topic
models.
</bodyText>
<subsectionHeader confidence="0.999702">
4.3 Effect of Ambiguous Seeds
</subsectionHeader>
<bodyText confidence="0.999995071428571">
In the following experiment we study the effect
of ambiguous seeds. We allow a seed word to oc-
cur in multiple seed sets. Table 6 shows the cor-
responding results. The performance drops when
we add ambiguous seed words, but it is still higher
than that of the LDA model. This suggests that the
quality of the seed topics is determined by the dis-
criminative power of the seed words rather than
the number of seed words in each seed topic. The
topics identified by the SeededLDA on Reuters
corpus are shown in the Table 5. With the help of
the seed sets, the model is able to split the ‘Grain’
and ‘Crude’ into two separate topics which were
merged into a single topic by the plain LDA.
</bodyText>
<subsectionHeader confidence="0.994202">
4.4 Qualitative Evaluation on NIPS papers
</subsectionHeader>
<bodyText confidence="0.999895666666667">
We ran LDA and SeededLDA models on the NIPS
papers from 2001 to 2010. For this corpus, the
seed words are chosen from the call for proposal.
</bodyText>
<page confidence="0.994419">
211
</page>
<bodyText confidence="0.964188">
group, offer, common, cash, agreement, shareholders, acquisition, stake, merger, board, sale
oil, price, prices, production, lt, gas, crude, 1987, 1985, bpd, opec, barrels, energy, first, petroleum
0, mln, cts, net, loss, 2, dlrs, shr, 3, profit, 4, 5, 6, revs, 7, 9, 8, year, note, 1986, 10, 0, sales
tonnes, wheat, mln, grain, week, corn, department, year, export, program, agriculture, 0, soviet, prices
bank, market, pct, dollar, exchange, billion, stg, today, foreign, rate, banks, japan, yen, rates, trade
</bodyText>
<tableCaption confidence="0.998913">
Table 5: Topics identified by SeededLDA on the frequent-5 categories of Reuters corpus
</tableCaption>
<table confidence="0.999718666666667">
Reuters 20 Newsgroups
F VI F VI
LDA 0.66 1.2 0.76 0.9
SeededLDA 0.76 0.99 0.81 0.75
SeededLDA 0.71 1.08 0.79 0.78
(amb)
</table>
<tableCaption confidence="0.991376">
Table 6: Effect of ambiguous seed words on Seed-
edLDA.
</tableCaption>
<bodyText confidence="0.999952066666667">
There are 10 major areas with sub areas under
each of them. We ran both the models with 10 top-
ics. For SeededLDA, the words in each of the ar-
eas are selected as seed words and we filter out the
ambiguous seed words. Upon a qualitative obser-
vation of the output topics, we found that LDA has
identified seven major topics and left out “Brain
Imaging”, “Cognitive Science and Artificial In-
telligence” and “Hardware Technologies” areas.
Not surprisingly, but reassuringly, these areas are
underrepresented among the NIPS papers. On the
other hand, SeededLDA successfully identifies all
of the major topics. The topics identified by LDA
and SeededLDA are shown in the supplementary
material.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999713039215687">
In traditional topic models, a symmetric Dirich-
let distribution is used as prior for topic-word dis-
tributions. A first attempt method to incorporate
seed words into the model is to use an asymmetric
Dirichlet distribution as prior for the topic-word
distributions (also called as Informed priors). For
example, to encourage Topic 5 to align with a seed
set we can choose an asymmetric prior of the form
Q5 = {Q, · · · , Q + c, · · · , Q1, i.e. we increase
the component values corresponding to the seed
words by a positive constant value. This favors
the desired seed words to be drawn with a higher
probability from this topic. But, it is argued else-
where that words drawn from such distributions
rarely pick words other than the seed words (An-
drzejewski et al., 2009). Moreover, since, in our
method each seed topic is a distribution over the
seed words, the convex combination of regular
and seed topics can be seen as adding different
weights (cz) to different components of the prior
vector. Thus our Model 1 can be seen as an asym-
metric generalization of the Informed priors.
For comparability purposes, in this paper, we
experimented with same number of regular topics
as the number of seed topics. But as explained in
the modeling part, our model is general enough
to handle situation with unequal number of seed
and regular topics. In this case, we assume that
the seed topics indicate a higher level of topical-
ity structure of the corpus and associate each seed
topic (or group) with a distribution over the regu-
lar topics. On the other hand, in many NLP appli-
cations, we tend to have only a partial information
rather than high-level supervision. In such cases,
one can create some empty seed sets and tweak
the model 2 to output a 1 in the binary vector cor-
responding to these seed sets. In this paper, we
used information gain to select the discriminating
seed words. But in the real world applications,
one can use publicly available ODP categorization
data to obtain the higher level seed words and thus
explore the corporal in a more meaningful way.
In this paper, we have explored two methods
to incorporate lexical prior into the topic mod-
els, combining them into a single model that we
call SeededLDA. From our experimental analysis,
we found that automatically derived seed words
can improve clustering performance significantly.
Moreover, we found out that allowing a seed word
to be shared across multiple sets of seed words de-
grades the performance.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999538">
We thank the anonymous reviewers for their help-
ful comments. This material is partially supported
by the National Science Foundation under Grant
No. IIS-1153487.
</bodyText>
<page confidence="0.997231">
212
</page>
<sectionHeader confidence="0.998334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866037383178">
Andrzejewski, D. and Zhu, X. (2009). Latent dirichlet
allocation with topic-in-set knowledge. In Proceed-
ings of the NAACL HLT 2009 Workshop on Semi-
Supervised Learning for Natural Language Pro-
cessing, SemiSupLearn ’09, pages 43–48, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andrzejewski, D., Zhu, X., and Craven, M. (2009). In-
corporating domain knowledge into topic modeling
via dirichlet forest priors. In ICML ’09: Proceed-
ings of the 26th Annual International Conference
on Machine Learning, pages 25–32, New York, NY,
USA. ACM.
Basu, S., Ian, D., and Wagstaff, K. (2008). Con-
strained Clustering: Advances in Algorithms, The-
ory, and Applications. Chapman &amp; Hall/CRC Pres.
Blei, D. and McAuliffe, J. (2008). Supervised topic
models. In Advances in Neural Information Pro-
cessing Systems 20, pages 121–128, Cambridge,
MA. MIT Press.
Blei., D. M. and Lafferty., J. (2009). Topic models. In
Text Mining: Theory and Applications. Taylor and
Francis.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). La-
tent dirichlet allocation. Journal ofMachingLearn-
ing Research, 3:993–1022.
Boyd-Graber, J., Blei, D. M., and Zhu, X. (2007). A
topic model for word sense disambiguation. In Em-
pirical Methods in Natural Language Processing.
Chang, J., Boyd-Graber, J., Wang, C., Gerrish, S., and
Blei, D. M. (2009). Reading tea leaves: How hu-
mans interpret topic models. In Neural Information
Processing Systems.
Griffiths, T., Steyvers, M., and Tenenbaum, J. (2007).
Topics in semantic representation. Psychological
Review, 114(2):211–244.
Griffiths, T. L. and Steyvers, M. (2004). Finding sci-
entific topics. Proceedings of National Academy of
Sciences USA, 101 Suppl 1:5228–5235.
Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-
baum, J. B. (2005). Integrating topics and syntax.
In Advances in Neural Information Processing Sys-
tems, volume 17, pages 537–544.
Haghighi, A. and Klein, D. (2006). Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguis-
tics, HLT-NAACL ’06, pages 320–327, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hu, Y., Boyd-Graber, J., and Satinoff, B. (2011). In-
teractive topic modeling. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ’11, pages 248–257, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Johnson, M. and Goldwater, S. (2009). Improving
nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adap-
tor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages
317–325, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Lacoste-Julien, S., Sha, F., and Jordan, M. (2008).
DiscLDA: Discriminative learning for dimensional-
ity reduction and classification. In Proceedings of
NIPS ’08.
Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. (2004).
Rcv1: A new benchmark collection for text catego-
rization research. J. Mach. Learn. Res., 5:361–397.
Meil˘a, M. (2007). Comparing clusterings—an infor-
mation based distance. J. Multivar. Anal., 98:873–
895.
Mitchell, T. M. (1997). Machine Learning. McGraw-
Hill, New York.
Paul, M. and Girju, R. (2010). A two-dimensional
topic-aspect model for discovering multi-faceted
topics. In AAAI.
Ramage, D., Hall, D., Nallapati, R., and Manning,
C. D. (2009). Labeled LDA: a supervised topic
model for credit attribution in multi-labeled cor-
pora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 1 - Volume 1, EMNLP ’09, pages 248–
256, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Thelen, M. and Riloff, E. (2002). A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In In Proc. 2002 Conf. Empir-
ical Methods in NLP (EMNLP).
Wagstaff, K., Cardie, C., Rogers, S., and Schr¨odl, S.
(2001). Constrained k-means clustering with back-
ground knowledge. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, ICML ’01, pages 577–584, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Wallach, H. M. (2005). Topic modeling: beyond bag-
of-words. In NIPS 2005 Workshop on Bayesian
Methods for Natural Language Processing.
Williamson, S., Wang, C., Heller, K. A., and Blei,
D. M. (2010). The IBP compound dirichlet pro-
cess and its application to focused topic modeling.
In ICML, pages 1151–1158.
</reference>
<page confidence="0.999395">
213
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.576889">
<title confidence="0.999676">Incorporating Lexical Priors into Topic Models</title>
<author confidence="0.99773">Jagadeesh Jagarlamudi Hal Daum´e Raghavendra Udupa</author>
<affiliation confidence="0.796193">University of Maryland University of Maryland Microsoft Research College Park, USA College Park, USA Bangalore,</affiliation>
<email confidence="0.999832">jags@umiacs.umd.eduhal@umiacs.umd.eduraghavu@microsoft.com</email>
<abstract confidence="0.998943260869565">Topic models have great potential for helping users understand document corpora. This potential is stymied by their purely unsupervised nature, which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks (Chang et al., 2009). We propose a simple and effective way to guide topic models to learn topics of specific interest to a user. We achieve by providing of seed words a user believes are representative of the underlying topics in a corpus. Our model these seeds to improve topicword distributions (by biasing topics to produce appropriate seed words) and to improve document-topic distributions (by biasing documents to select topics related to the seed words they contain). Extrinsic evaluation on a document clustering task reveals a significant improvement when using seed information, even over other models that use seed information naively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Andrzejewski</author>
<author>X Zhu</author>
</authors>
<title>Latent dirichlet allocation with topic-in-set knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on SemiSupervised Learning for Natural Language Processing, SemiSupLearn ’09,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6000" citStr="Andrzejewski and Zhu, 2009" startWordPosition="978" endWordPosition="982"> a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set (Section 2.1). To improve document-topic distributions, we encourage the model to select document-level topics based on the existence of input seed words in that document (Sec</context>
<context position="16221" citStr="Andrzejewski and Zhu, 2009" startWordPosition="2846" endWordPosition="2849">del reduces to the LDA model with τ and βr as the hyperparameters. 2.3 SeededLDA Both of our models use seed words in different ways to improve topic-word and document-topic distributions respectively. We can combine both the above models easily. We refer to the combined model as SeededLDA and its generative story is as follows (its graphical notation is shown in Fig. 2(c)). The variables have same semantics as in the previous models. 1. For each k=1. • • T, (a) Choose regular topic φrk N Dir(βr). (b) Choose seed topic φsk N Dir(βs). (c) Choose πk N Beta(1,1). 2.4 Automatic Seed Selection In (Andrzejewski and Zhu, 2009; Andrzejewski et al., 2009), the seed information is provided manually. Here, we describe the use of feature selection techniques, prevalent in the classification literature, to automatically derive the seed sets. If we want the topicality structure identified by the LDA to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure. To enable this, we first take class labeled data (doesn’t need to be multi-class labeled data unlike (Ramage et al., 2009)) and identify the discriminating features for each class. Then we choose</context>
<context position="20535" citStr="Andrzejewski and Zhu, 2009" startWordPosition="3605" endWordPosition="3608"> a word pair indicates both the words should not have high probability in a single topic. In the Dirichlet Forest approach, the constraints are first converted into trees with words as the leaves and edges having pre-defined weights. All the trees are joined to a dummy node to form a forest. The sampling for a word translates into a random walk on the forest: starting from the root and selecting one of its children based on the edge weights until you reach a leaf node. While the Dirichlet Forest method requires supervision in terms of Must link and Cannot link information, the Topics In Sets (Andrzejewski and Zhu, 2009) model proposes a different approach. Here, the supervision is provided at the token level. The user chooses specific tokens and restrict them to occur only with in a specified list of topics. While this needs minimal changes to the inference process of LDA, it requires information at the level of tokens. The word type level seed information can be converted into token level information (like we do in Sec. 4) but this prevents their model from distinguishing the tokens based on the word senses. Several models have been proposed which use supervision at the document level. Supervised LDA (Blei </context>
<context position="27132" citStr="Andrzejewski and Zhu, 2009" startWordPosition="4700" endWordPosition="4703">ectiveness of the automatic extraction of seed words in topic models. 4.2 Document Clustering In the next experiment, we compare our models with LDA and other baselines. The first baseline (maxCluster) simply counts the number of tokens in each document from each of the seed topics and assigns the document to the seed topic that has most tokens. This results in a clustering of documents based on the seed topic they are assigned to. This baseline evaluates the effectiveness of the seed words with respect to the underlying clustering. Apart from the maxCluster baseline, we use LDA and z-labels (Andrzejewski and Zhu, 2009) as our baselines. For z-labels, we treat all the tokens of a seed word in the same way. Table 4 shows the comparison of our models with respect to the baseline systems.2 Comparing the performance of maxCluster to that of LDA, we observe that the seed words themselves do a poor job in clustering the documents. We experimented with two variants of Model 1. In the first run (Model 1) we sample the Irk value, i.e. the probability of choosing a seed topic for each topic. While in the ‘Model 1 (fr = 0.7)’ run, we fix this probability to a constant value of 0.7 irrespective of the topic.3 Though bot</context>
</contexts>
<marker>Andrzejewski, Zhu, 2009</marker>
<rawString>Andrzejewski, D. and Zhu, X. (2009). Latent dirichlet allocation with topic-in-set knowledge. In Proceedings of the NAACL HLT 2009 Workshop on SemiSupervised Learning for Natural Language Processing, SemiSupLearn ’09, pages 43–48, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Andrzejewski</author>
<author>X Zhu</author>
<author>M Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>25--32</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6053" citStr="Andrzejewski et al., 2009" startWordPosition="986" endWordPosition="990"> him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set (Section 2.1). To improve document-topic distributions, we encourage the model to select document-level topics based on the existence of input seed words in that document (Section 2.2). Before moving on to the details of our mod</context>
<context position="16249" citStr="Andrzejewski et al., 2009" startWordPosition="2850" endWordPosition="2853"> with τ and βr as the hyperparameters. 2.3 SeededLDA Both of our models use seed words in different ways to improve topic-word and document-topic distributions respectively. We can combine both the above models easily. We refer to the combined model as SeededLDA and its generative story is as follows (its graphical notation is shown in Fig. 2(c)). The variables have same semantics as in the previous models. 1. For each k=1. • • T, (a) Choose regular topic φrk N Dir(βr). (b) Choose seed topic φsk N Dir(βs). (c) Choose πk N Beta(1,1). 2.4 Automatic Seed Selection In (Andrzejewski and Zhu, 2009; Andrzejewski et al., 2009), the seed information is provided manually. Here, we describe the use of feature selection techniques, prevalent in the classification literature, to automatically derive the seed sets. If we want the topicality structure identified by the LDA to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure. To enable this, we first take class labeled data (doesn’t need to be multi-class labeled data unlike (Ramage et al., 2009)) and identify the discriminating features for each class. Then we choose these discriminating featur</context>
<context position="19504" citStr="Andrzejewski et al., 2009" startWordPosition="3426" endWordPosition="3429"> the two approaches aim to capture: semantic lexicons are based on much more specific notions of semantics (e.g. all the country names) than the generic “topic” semantics of topic models. The idea of seeding has also been used in prototype-driven learning (Haghighi and Klein, 2006) and shown similar efficacies for these semisupervised learning approaches. LDAWN (Boyd-Graber et al., 2007) models sets of words for the word sense disambiguation 208 task. It assumes that a topic is a distribution over synsets and relies on the Wordnet to obtain the synsets. The most related prior work is that of (Andrzejewski et al., 2009), who propose the use Dirichlet Forest priors to incorporate Must Link and Cannot Link constraints into the topic models. This work is analogous to constrained K-means clustering (Wagstaff et al., 2001; Basu et al., 2008). A must link between a pair word types represents that the model should encourage both the words to have either high or low probability in any particular topic. A cannot link between a word pair indicates both the words should not have high probability in a single topic. In the Dirichlet Forest approach, the constraints are first converted into trees with words as the leaves </context>
<context position="26083" citStr="Andrzejewski et al., 2009" startWordPosition="4528" endWordPosition="4531">sured using the t-test at p = 0.01. 4.1 Seed Extraction The seeds were extracted automatically (Sec. 2.4) based on a small sample of labeled data other than the test data. We first extract 25 seeds words per each class and then remove the seed words that appear in more than one class. After this filtering, on an average, we are left with 9 and 15 words per each seed topic for Reuters and 20 Newsgroups corpora respectively. We use the existing Dirichlet Forest method to evaluate the effectiveness of the automatically extracted seed words. The Must and Cannot links required for the supervision (Andrzejewski et al., 2009) are automatically obtained by adding a must-link between every pair of words belonging to the same seed set and a split constraint between every pair of words belonging to different sets. The accuracies are averaged over 25 different random initializations and are shown in Table 3. We have also indicated the relative performance gains compared to LDA. The significant improvement over the plain LDA demonstrates the effectiveness of the automatic extraction of seed words in topic models. 4.2 Document Clustering In the next experiment, we compare our models with LDA and other baselines. The firs</context>
<context position="34407" citStr="Andrzejewski et al., 2009" startWordPosition="5948" endWordPosition="5952">orate seed words into the model is to use an asymmetric Dirichlet distribution as prior for the topic-word distributions (also called as Informed priors). For example, to encourage Topic 5 to align with a seed set we can choose an asymmetric prior of the form Q5 = {Q, · · · , Q + c, · · · , Q1, i.e. we increase the component values corresponding to the seed words by a positive constant value. This favors the desired seed words to be drawn with a higher probability from this topic. But, it is argued elsewhere that words drawn from such distributions rarely pick words other than the seed words (Andrzejewski et al., 2009). Moreover, since, in our method each seed topic is a distribution over the seed words, the convex combination of regular and seed topics can be seen as adding different weights (cz) to different components of the prior vector. Thus our Model 1 can be seen as an asymmetric generalization of the Informed priors. For comparability purposes, in this paper, we experimented with same number of regular topics as the number of seed topics. But as explained in the modeling part, our model is general enough to handle situation with unequal number of seed and regular topics. In this case, we assume that</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>Andrzejewski, D., Zhu, X., and Craven, M. (2009). Incorporating domain knowledge into topic modeling via dirichlet forest priors. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, pages 25–32, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Basu</author>
<author>D Ian</author>
<author>K Wagstaff</author>
</authors>
<title>Constrained Clustering: Advances in Algorithms, Theory, and Applications.</title>
<date>2008</date>
<publisher>Chapman &amp; Hall/CRC Pres.</publisher>
<contexts>
<context position="19725" citStr="Basu et al., 2008" startWordPosition="3461" endWordPosition="3464"> prototype-driven learning (Haghighi and Klein, 2006) and shown similar efficacies for these semisupervised learning approaches. LDAWN (Boyd-Graber et al., 2007) models sets of words for the word sense disambiguation 208 task. It assumes that a topic is a distribution over synsets and relies on the Wordnet to obtain the synsets. The most related prior work is that of (Andrzejewski et al., 2009), who propose the use Dirichlet Forest priors to incorporate Must Link and Cannot Link constraints into the topic models. This work is analogous to constrained K-means clustering (Wagstaff et al., 2001; Basu et al., 2008). A must link between a pair word types represents that the model should encourage both the words to have either high or low probability in any particular topic. A cannot link between a word pair indicates both the words should not have high probability in a single topic. In the Dirichlet Forest approach, the constraints are first converted into trees with words as the leaves and edges having pre-defined weights. All the trees are joined to a dummy node to form a forest. The sampling for a word translates into a random walk on the forest: starting from the root and selecting one of its childre</context>
</contexts>
<marker>Basu, Ian, Wagstaff, 2008</marker>
<rawString>Basu, S., Ian, D., and Wagstaff, K. (2008). Constrained Clustering: Advances in Algorithms, Theory, and Applications. Chapman &amp; Hall/CRC Pres.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20,</booktitle>
<pages>121--128</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3111" citStr="Blei and McAuliffe, 2008" startWordPosition="505" endWordPosition="508"> merged the least two frequent classes (“Crude” and “Grain”) into a single topic. The red colored words in topic 5 correspond to the “Crude” class and blue words are from the “Grain” class. This leads to the situation where the topics identified by LDA are not in accordance with the underlying topical structure of the corpus. This is a problem not just with LDA: it is potentially a problem with any extension thereof that have focused on improving the semantic coherence of the words in each topic (Griffiths et al., 2005; Wallach, 2005; Griffiths et al., 2007), the document topic distributions (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) or other aspects (Blei. and Lafferty., 2009). We address this problem by providing some additional information to the model. Initially, along with the document collection, a user may provide higher level view of the document collection. For instance, as discussed in Section 4.4, when run on historical NIPS papers, LDA fails to find topics related to Brain Imaging, Cognitive Science or Hardware, even though we know from the call for 204 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204–213, Avign</context>
<context position="5963" citStr="Blei and McAuliffe, 2008" startWordPosition="973" endWordPosition="976">ting Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set (Section 2.1). To improve document-topic distributions, we encourage the model to select document-level topics based on the existence of i</context>
<context position="21155" citStr="Blei and McAuliffe, 2008" startWordPosition="3709" endWordPosition="3712">2009) model proposes a different approach. Here, the supervision is provided at the token level. The user chooses specific tokens and restrict them to occur only with in a specified list of topics. While this needs minimal changes to the inference process of LDA, it requires information at the level of tokens. The word type level seed information can be converted into token level information (like we do in Sec. 4) but this prevents their model from distinguishing the tokens based on the word senses. Several models have been proposed which use supervision at the document level. Supervised LDA (Blei and McAuliffe, 2008) and DiscLDA (Lacoste-Julien et al., 2008) try to predict the category labels (e.g. sentiment classification) for the input documents based on a document labeled data. Of these models, the most related one to SeededLDA is the LabeledLDA model (Ramage et al., 2009). Their model operates on multi-class labeled corpus. Each document is assumed to be a mixture over a known subset of topics (classes) with each topic being a distribution over words. The process of generating document topic distribution in LabeledLDA is similar to the process of generating group distribution in our Model 2 (Sec. 2.2)</context>
</contexts>
<marker>Blei, McAuliffe, 2008</marker>
<rawString>Blei, D. and McAuliffe, J. (2008). Supervised topic models. In Advances in Neural Information Processing Systems 20, pages 121–128, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J Lafferty</author>
</authors>
<title>Topic models. In Text Mining: Theory and Applications. Taylor and Francis.</title>
<date>2009</date>
<marker>Blei, Lafferty, 2009</marker>
<rawString>Blei., D. M. and Lafferty., J. (2009). Topic models. In Text Mining: Theory and Applications. Taylor and Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal ofMachingLearning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1294" citStr="Blei et al., 2003" startWordPosition="194" endWordPosition="197">s by providing sets of seed words that a user believes are representative of the underlying topics in a corpus. Our model uses these seeds to improve both topicword distributions (by biasing topics to produce appropriate seed words) and to improve document-topic distributions (by biasing documents to select topics related to the seed words they contain). Extrinsic evaluation on a document clustering task reveals a significant improvement when using seed information, even over other models that use seed information naively. 1 Introduction Topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have emerged as a powerful tool to analyze document collections in an unsupervised fashion. When fit to a document collection, topic models implicitly use document level co-occurrence information to group semantically related words into a single topic. Since the objective of these models is to maximize the probability of the observed data, they have a tendency to explain only the most obvious and superficial aspects of a corpus. They effectively sacrifice performance on rare topics to do abetter job in modeling frequently occurring words. The user is then left with a skewed impression of the </context>
<context position="6774" citStr="Blei et al., 2003" startWordPosition="1110" endWordPosition="1113">probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set (Section 2.1). To improve document-topic distributions, we encourage the model to select document-level topics based on the existence of input seed words in that document (Section 2.2). Before moving on to the details of our models, we briefly recall the generative story of the LDA model and the reader is encouraged to refer to (Blei et al., 2003) for further details. 1. For each topic k = 1 • • • T, • choose Ok — Dir(Q). 2. For each document d, choose Bd — Dir(α). • For each token i = 1 • • • Nd: (a) Select a topic zz — Mult(Bd). (b) Select a word wz — Mult(Ozi). where T is the number of topics, α, Q are hyperparameters of the model and Ok and Bd are topicword and document-topic Multinomial probability distributions respectively. 2.1 Word-Topic Distributions (Model 1) In regular topic models, each topic k is defined by a Multinomial distribution Ok over words. We extend this notion and instead define a topic as a mixture of two Multin</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal ofMachingLearning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D M Blei</author>
<author>X Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="19268" citStr="Boyd-Graber et al., 2007" startWordPosition="3384" endWordPosition="3387">he bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). The goals are similar as well: growing a small set of seed examples into a much larger set. A key difference is the type of semantic information that the two approaches aim to capture: semantic lexicons are based on much more specific notions of semantics (e.g. all the country names) than the generic “topic” semantics of topic models. The idea of seeding has also been used in prototype-driven learning (Haghighi and Klein, 2006) and shown similar efficacies for these semisupervised learning approaches. LDAWN (Boyd-Graber et al., 2007) models sets of words for the word sense disambiguation 208 task. It assumes that a topic is a distribution over synsets and relies on the Wordnet to obtain the synsets. The most related prior work is that of (Andrzejewski et al., 2009), who propose the use Dirichlet Forest priors to incorporate Must Link and Cannot Link constraints into the topic models. This work is analogous to constrained K-means clustering (Wagstaff et al., 2001; Basu et al., 2008). A must link between a pair word types represents that the model should encourage both the words to have either high or low probability in any</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Boyd-Graber, J., Blei, D. M., and Zhu, X. (2007). A topic model for word sense disambiguation. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>C Wang</author>
<author>S Gerrish</author>
<author>D M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Chang, J., Boyd-Graber, J., Wang, C., Gerrish, S., and Blei, D. M. (2009). Reading tea leaves: How humans interpret topic models. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>J Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="3051" citStr="Griffiths et al., 2007" startWordPosition="496" endWordPosition="499">equent two frequent classes (“Acquisition” and “Forex”) and merged the least two frequent classes (“Crude” and “Grain”) into a single topic. The red colored words in topic 5 correspond to the “Crude” class and blue words are from the “Grain” class. This leads to the situation where the topics identified by LDA are not in accordance with the underlying topical structure of the corpus. This is a problem not just with LDA: it is potentially a problem with any extension thereof that have focused on improving the semantic coherence of the words in each topic (Griffiths et al., 2005; Wallach, 2005; Griffiths et al., 2007), the document topic distributions (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) or other aspects (Blei. and Lafferty., 2009). We address this problem by providing some additional information to the model. Initially, along with the document collection, a user may provide higher level view of the document collection. For instance, as discussed in Section 4.4, when run on historical NIPS papers, LDA fails to find topics related to Brain Imaging, Cognitive Science or Hardware, even though we know from the call for 204 Proceedings of the 13th Conference of the European Chapter of the Ass</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Griffiths, T., Steyvers, M., and Tenenbaum, J. (2007). Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of National Academy of Sciences USA, 101 Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="23745" citStr="Griffiths and Steyvers, 2004" startWordPosition="4127" endWordPosition="4130">jority topics by encouraging the topicality structure identified by the topic models to align with that of the document corpus, we choose extrinsic evaluation as the primary evaluation method. We use document clustering task and use frequent-5 categories of Reuters-21578 corpus (Lewis et al., 2004) and four classes from the 20 Newsgroups data set (i.e.‘rec.autos’, ‘sci.electronics’, ‘comp.hardware’ and ‘alt.atheism’). For both the corpora we do the standard preprocessing of removing stopwords and infrequent words (Williamson et al., 2010). For all the models, we use a Collapsed Gibbs sampler (Griffiths and Steyvers, 2004) for the inference process. We use the standard hyperparameters values α = 1.0, Q = 0.01 and T = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009). 209 Reuters 20 Newsgroups F-measure VI F-measure VI LDA 0.64 (±.05) 1.26 (±.16) 0.77 (±.06) 0.9 (±.13) Dirichlet Forest 0.67* (±.02) 1.17 (±.11) 0.79(±.01) 0.83*(±.03) A over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%) Table 3: The effect of adding constraints by Dirichlet Forest Encoding. For Variational Information (VI) a lower score indicates a bette</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Griffiths, T. L. and Steyvers, M. (2004). Finding scientific topics. Proceedings of National Academy of Sciences USA, 101 Suppl 1:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>17</volume>
<pages>537--544</pages>
<contexts>
<context position="3011" citStr="Griffiths et al., 2005" startWordPosition="490" endWordPosition="493">ass (“Earn”) and one topic for the subsequent two frequent classes (“Acquisition” and “Forex”) and merged the least two frequent classes (“Crude” and “Grain”) into a single topic. The red colored words in topic 5 correspond to the “Crude” class and blue words are from the “Grain” class. This leads to the situation where the topics identified by LDA are not in accordance with the underlying topical structure of the corpus. This is a problem not just with LDA: it is potentially a problem with any extension thereof that have focused on improving the semantic coherence of the words in each topic (Griffiths et al., 2005; Wallach, 2005; Griffiths et al., 2007), the document topic distributions (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) or other aspects (Blei. and Lafferty., 2009). We address this problem by providing some additional information to the model. Initially, along with the document collection, a user may provide higher level view of the document collection. For instance, as discussed in Section 4.4, when run on historical NIPS papers, LDA fails to find topics related to Brain Imaging, Cognitive Science or Hardware, even though we know from the call for 204 Proceedings of the 13th Confe</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenenbaum, J. B. (2005). Integrating topics and syntax. In Advances in Neural Information Processing Systems, volume 17, pages 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5803" citStr="Haghighi and Klein, 2006" startWordPosition="947" endWordPosition="950">t al., 2011) will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting. 2 Incorporating Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to th</context>
<context position="16989" citStr="Haghighi and Klein, 2006" startWordPosition="2966" endWordPosition="2969">n the classification literature, to automatically derive the seed sets. If we want the topicality structure identified by the LDA to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure. To enable this, we first take class labeled data (doesn’t need to be multi-class labeled data unlike (Ramage et al., 2009)) and identify the discriminating features for each class. Then we choose these discriminating features as the initial sets of seed words. In principle, this is similar to the prototype driven unsupervised learning (Haghighi and Klein, 2006). We use Information Gain (Mitchell, 1997) to identify the required discriminating features. The Information Gain (IG) of a word (w) in a class (c) is given by 2. For each seed set s = 1• • • S, N IG(c, w) = H(c) − H(c|w) (a) Choose group-topic distributionψs Dir(α). 3. For each document d, (a) Choose a binary vector ~b of length S. (b) Choose a document-group distribution ζd N Dir(τ~b). (c) Choose a group variable g N Mult(ζd). (d) Choose θd N Dir(ψg). // of length T (e) For each token i = 1 • • • Nd: i. Select a topic zi N Mult(θd). ii. Select an indicator xi N Bern(πzi). iii. if xi is 0 • S</context>
<context position="19160" citStr="Haghighi and Klein, 2006" startWordPosition="3369" endWordPosition="3372"> be input to the model. 3 Related Work Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). The goals are similar as well: growing a small set of seed examples into a much larger set. A key difference is the type of semantic information that the two approaches aim to capture: semantic lexicons are based on much more specific notions of semantics (e.g. all the country names) than the generic “topic” semantics of topic models. The idea of seeding has also been used in prototype-driven learning (Haghighi and Klein, 2006) and shown similar efficacies for these semisupervised learning approaches. LDAWN (Boyd-Graber et al., 2007) models sets of words for the word sense disambiguation 208 task. It assumes that a topic is a distribution over synsets and relies on the Wordnet to obtain the synsets. The most related prior work is that of (Andrzejewski et al., 2009), who propose the use Dirichlet Forest priors to incorporate Must Link and Cannot Link constraints into the topic models. This work is analogous to constrained K-means clustering (Wagstaff et al., 2001; Basu et al., 2008). A must link between a pair word t</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Haghighi, A. and Klein, D. (2006). Prototype-driven learning for sequence models. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 320–327, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hu</author>
<author>J Boyd-Graber</author>
<author>B Satinoff</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>248--257</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5190" citStr="Hu et al., 2011" startWordPosition="844" endWordPosition="847">categories of the Reuters-21578 categorization corpus. We use them as running example in the rest of the paper. papers that such topics should exist in the corpus. By allowing the user to provide some seed words related to these underrepresented topics, we encourage the model to find evidence of these topics in the data. Importantly, we only encourage the model to follow the seed sets and do not force it. So if it has compelling evidence in the data to overcome the seed information then it still has the freedom to do so. Our seeding approach in combination with the interactive topic modeling (Hu et al., 2011) will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting. 2 Incorporating Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and</context>
<context position="22351" citStr="Hu et al., 2011" startWordPosition="3907" endWordPosition="3910">r Model 2 (Sec. 2.2). However our model differs from LabeledLDA in the subsequent steps. Rather than using the group distribution directly, we sample a group variable and use it to constrain the document-topic distributions of all the documents within this group. Moreover, in their model the binary vector is observed directly in the form of document labels while, in our case, it is automatically populated based on the document tokens. Interactive topic modeling brings the user into the loop, by allowing him/her to make suggestions on how to improve the quality of the topics at each iteration (Hu et al., 2011). In their approach, the authors use Dirichlet Forest method to incorporate the user’s preferences. In our experiments (Sec. 4), we show that SeededLDA performs better than Dirichlet Forest method, so SeededLDA when used with their framework can allow an user to explore a document collection in a more meaningful manner. 4 Experiments We evaluate different aspects of the model separately. Our experimental setup proceeds as follows: a) Using an existing model, we evaluate the effectiveness of automatically derived constraints indicating the potential benefits of adding seed words into the topic </context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, 2011</marker>
<rawString>Hu, Y., Boyd-Graber, J., and Satinoff, B. (2011). Interactive topic modeling. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 248–257, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23995" citStr="Johnson and Goldwater, 2009" startWordPosition="4172" endWordPosition="4175">ies of Reuters-21578 corpus (Lewis et al., 2004) and four classes from the 20 Newsgroups data set (i.e.‘rec.autos’, ‘sci.electronics’, ‘comp.hardware’ and ‘alt.atheism’). For both the corpora we do the standard preprocessing of removing stopwords and infrequent words (Williamson et al., 2010). For all the models, we use a Collapsed Gibbs sampler (Griffiths and Steyvers, 2004) for the inference process. We use the standard hyperparameters values α = 1.0, Q = 0.01 and T = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009). 209 Reuters 20 Newsgroups F-measure VI F-measure VI LDA 0.64 (±.05) 1.26 (±.16) 0.77 (±.06) 0.9 (±.13) Dirichlet Forest 0.67* (±.02) 1.17 (±.11) 0.79(±.01) 0.83*(±.03) A over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%) Table 3: The effect of adding constraints by Dirichlet Forest Encoding. For Variational Information (VI) a lower score indicates a better clustering. * indicates statistical significance at p = 0.01 as measured by the t-test. All the four improvements are significant at p = 0.05. We run all the models with the same number of topics as the number of clusters. Then, for each document, </context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Johnson, M. and Goldwater, S. (2009). Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 317–325, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>F Sha</author>
<author>M Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS ’08.</booktitle>
<contexts>
<context position="3141" citStr="Lacoste-Julien et al., 2008" startWordPosition="509" endWordPosition="512">uent classes (“Crude” and “Grain”) into a single topic. The red colored words in topic 5 correspond to the “Crude” class and blue words are from the “Grain” class. This leads to the situation where the topics identified by LDA are not in accordance with the underlying topical structure of the corpus. This is a problem not just with LDA: it is potentially a problem with any extension thereof that have focused on improving the semantic coherence of the words in each topic (Griffiths et al., 2005; Wallach, 2005; Griffiths et al., 2007), the document topic distributions (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) or other aspects (Blei. and Lafferty., 2009). We address this problem by providing some additional information to the model. Initially, along with the document collection, a user may provide higher level view of the document collection. For instance, as discussed in Section 4.4, when run on historical NIPS papers, LDA fails to find topics related to Brain Imaging, Cognitive Science or Hardware, even though we know from the call for 204 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204–213, Avignon, France, April 23 - 27 2012</context>
<context position="21197" citStr="Lacoste-Julien et al., 2008" startWordPosition="3715" endWordPosition="3718">ch. Here, the supervision is provided at the token level. The user chooses specific tokens and restrict them to occur only with in a specified list of topics. While this needs minimal changes to the inference process of LDA, it requires information at the level of tokens. The word type level seed information can be converted into token level information (like we do in Sec. 4) but this prevents their model from distinguishing the tokens based on the word senses. Several models have been proposed which use supervision at the document level. Supervised LDA (Blei and McAuliffe, 2008) and DiscLDA (Lacoste-Julien et al., 2008) try to predict the category labels (e.g. sentiment classification) for the input documents based on a document labeled data. Of these models, the most related one to SeededLDA is the LabeledLDA model (Ramage et al., 2009). Their model operates on multi-class labeled corpus. Each document is assumed to be a mixture over a known subset of topics (classes) with each topic being a distribution over words. The process of generating document topic distribution in LabeledLDA is similar to the process of generating group distribution in our Model 2 (Sec. 2.2). However our model differs from LabeledLD</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Lacoste-Julien, S., Sha, F., and Jordan, M. (2008). DiscLDA: Discriminative learning for dimensionality reduction and classification. In Proceedings of NIPS ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T G Rose</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>5--361</pages>
<contexts>
<context position="2081" citStr="Lewis et al., 2004" startWordPosition="328" endWordPosition="331">el co-occurrence information to group semantically related words into a single topic. Since the objective of these models is to maximize the probability of the observed data, they have a tendency to explain only the most obvious and superficial aspects of a corpus. They effectively sacrifice performance on rare topics to do abetter job in modeling frequently occurring words. The user is then left with a skewed impression of the corpus, and perhaps one that does not perform well in extrinsic tasks. To illustrate this problem, we ran LDA on the most frequent five categories of the Reuters21578 (Lewis et al., 2004) text corpus. This document distribution is very skewed: more than half of the collection belongs to the most frequent category (“Earn”). The five topics identified by the LDA are shown in Table 1. A brief observation of the topics reveals that LDA has roughly allocated topics 1 &amp; 2 for the most frequent class (“Earn”) and one topic for the subsequent two frequent classes (“Acquisition” and “Forex”) and merged the least two frequent classes (“Crude” and “Grain”) into a single topic. The red colored words in topic 5 correspond to the “Crude” class and blue words are from the “Grain” class. This</context>
<context position="23415" citStr="Lewis et al., 2004" startWordPosition="4079" endWordPosition="4082">n existing model, we evaluate the effectiveness of automatically derived constraints indicating the potential benefits of adding seed words into the topic models. b) We evaluate each of our proposed models in different settings and compare with multiple baseline systems. Since our aim is to overcome the dominance of majority topics by encouraging the topicality structure identified by the topic models to align with that of the document corpus, we choose extrinsic evaluation as the primary evaluation method. We use document clustering task and use frequent-5 categories of Reuters-21578 corpus (Lewis et al., 2004) and four classes from the 20 Newsgroups data set (i.e.‘rec.autos’, ‘sci.electronics’, ‘comp.hardware’ and ‘alt.atheism’). For both the corpora we do the standard preprocessing of removing stopwords and infrequent words (Williamson et al., 2010). For all the models, we use a Collapsed Gibbs sampler (Griffiths and Steyvers, 2004) for the inference process. We use the standard hyperparameters values α = 1.0, Q = 0.01 and T = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009). 209 Reuters 20 New</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. (2004). Rcv1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meil˘a</author>
</authors>
<title>Comparing clusterings—an information based distance.</title>
<date>2007</date>
<journal>J. Multivar. Anal.,</journal>
<volume>98</volume>
<pages>895</pages>
<marker>Meil˘a, 2007</marker>
<rawString>Meil˘a, M. (2007). Comparing clusterings—an information based distance. J. Multivar. Anal., 98:873– 895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGrawHill,</publisher>
<location>New York.</location>
<contexts>
<context position="17031" citStr="Mitchell, 1997" startWordPosition="2974" endWordPosition="2975">rive the seed sets. If we want the topicality structure identified by the LDA to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure. To enable this, we first take class labeled data (doesn’t need to be multi-class labeled data unlike (Ramage et al., 2009)) and identify the discriminating features for each class. Then we choose these discriminating features as the initial sets of seed words. In principle, this is similar to the prototype driven unsupervised learning (Haghighi and Klein, 2006). We use Information Gain (Mitchell, 1997) to identify the required discriminating features. The Information Gain (IG) of a word (w) in a class (c) is given by 2. For each seed set s = 1• • • S, N IG(c, w) = H(c) − H(c|w) (a) Choose group-topic distributionψs Dir(α). 3. For each document d, (a) Choose a binary vector ~b of length S. (b) Choose a document-group distribution ζd N Dir(τ~b). (c) Choose a group variable g N Mult(ζd). (d) Choose θd N Dir(ψg). // of length T (e) For each token i = 1 • • • Nd: i. Select a topic zi N Mult(θd). ii. Select an indicator xi N Bern(πzi). iii. if xi is 0 • Select a word wi N Mult(φrzi). iv. if xi is</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Mitchell, T. M. (1997). Machine Learning. McGrawHill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>R Girju</author>
</authors>
<title>A two-dimensional topic-aspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="10173" citStr="Paul and Girju, 2010" startWordPosition="1729" endWordPosition="1732">hat constrains their distribution to only generate words in the corresponding seed set. Then, for each token in a document, we first generate a topic. After choosing a topic, we flip a (biased) coin to pick either the seed or the regular topic distribution. Once this distribution is selected we generate a word from it. It is important to note that although there are 2xT topic-word distributions in total, each document is still a mixture of only T topics (as shown in Fig. 1). This is crucial in relating seed and regular topics and is similar to the way topics and aspects are tied in TAM model (Paul and Girju, 2010). To understand how this model gathers words related to seed words, consider a seed topic (say the fourth row in Table 2) with seed words {grain, wheat, corn, etc. }. Now by assigning all the related words such as “tonnes”, “agriculture”, “production” etc. to its corresponding regular topic, the model can potentially put high probability mass on topic z = 4 for agriculture related documents. Instead, if it places these words in another regular topic, say z = 3, then the document probability mass has to be distributed among topics 3 and 4 and as a result the model will pay a steeper penalty. Th</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>Paul, M. and Girju, R. (2010). A two-dimensional topic-aspect model for discovering multi-faceted topics. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>D Hall</author>
<author>R Nallapati</author>
<author>C D Manning</author>
</authors>
<title>Labeled LDA: a supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>248--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16748" citStr="Ramage et al., 2009" startWordPosition="2928" endWordPosition="2931">r(βs). (c) Choose πk N Beta(1,1). 2.4 Automatic Seed Selection In (Andrzejewski and Zhu, 2009; Andrzejewski et al., 2009), the seed information is provided manually. Here, we describe the use of feature selection techniques, prevalent in the classification literature, to automatically derive the seed sets. If we want the topicality structure identified by the LDA to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure. To enable this, we first take class labeled data (doesn’t need to be multi-class labeled data unlike (Ramage et al., 2009)) and identify the discriminating features for each class. Then we choose these discriminating features as the initial sets of seed words. In principle, this is similar to the prototype driven unsupervised learning (Haghighi and Klein, 2006). We use Information Gain (Mitchell, 1997) to identify the required discriminating features. The Information Gain (IG) of a word (w) in a class (c) is given by 2. For each seed set s = 1• • • S, N IG(c, w) = H(c) − H(c|w) (a) Choose group-topic distributionψs Dir(α). 3. For each document d, (a) Choose a binary vector ~b of length S. (b) Choose a document-gr</context>
<context position="21419" citStr="Ramage et al., 2009" startWordPosition="3752" endWordPosition="3755">quires information at the level of tokens. The word type level seed information can be converted into token level information (like we do in Sec. 4) but this prevents their model from distinguishing the tokens based on the word senses. Several models have been proposed which use supervision at the document level. Supervised LDA (Blei and McAuliffe, 2008) and DiscLDA (Lacoste-Julien et al., 2008) try to predict the category labels (e.g. sentiment classification) for the input documents based on a document labeled data. Of these models, the most related one to SeededLDA is the LabeledLDA model (Ramage et al., 2009). Their model operates on multi-class labeled corpus. Each document is assumed to be a mixture over a known subset of topics (classes) with each topic being a distribution over words. The process of generating document topic distribution in LabeledLDA is similar to the process of generating group distribution in our Model 2 (Sec. 2.2). However our model differs from LabeledLDA in the subsequent steps. Rather than using the group distribution directly, we sample a group variable and use it to constrain the document-topic distributions of all the documents within this group. Moreover, in their m</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Ramage, D., Hall, D., Nallapati, R., and Manning, C. D. (2009). Labeled LDA: a supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 248– 256, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In</title>
<date>2002</date>
<booktitle>In Proc. 2002 Conf. Empirical Methods in NLP (EMNLP).</booktitle>
<contexts>
<context position="5748" citStr="Thelen and Riloff, 2002" startWordPosition="940" endWordPosition="943"> combination with the interactive topic modeling (Hu et al., 2011) will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting. 2 Incorporating Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each</context>
<context position="18727" citStr="Thelen and Riloff, 2002" startWordPosition="3294" endWordPosition="3297">oin toss and then generate a word from its distribution. where H(c) is the entropy of the class and H(c|w) is the conditional entropy of the class given the word. In computing Information Gain, we binarize the document vectors and consider whether a word occurs in any document of a given class or not. Thus obtained ranked list of words for each class are filtered for ambiguous words and then used as initial sets of seed words to be input to the model. 3 Related Work Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). The goals are similar as well: growing a small set of seed examples into a much larger set. A key difference is the type of semantic information that the two approaches aim to capture: semantic lexicons are based on much more specific notions of semantics (e.g. all the country names) than the generic “topic” semantics of topic models. The idea of seeding has also been used in prototype-driven learning (Haghighi and Klein, 2006) and shown similar efficacies for these semisupervised learning approaches. LDAWN (Boyd-Graber et al., 2007) models sets of words for the word sense disambiguation 208</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Thelen, M. and Riloff, E. (2002). A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In In Proc. 2002 Conf. Empirical Methods in NLP (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wagstaff</author>
<author>C Cardie</author>
<author>S Rogers</author>
<author>S Schr¨odl</author>
</authors>
<title>Constrained k-means clustering with background knowledge.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>577--584</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Wagstaff, Cardie, Rogers, Schr¨odl, 2001</marker>
<rawString>Wagstaff, K., Cardie, C., Rogers, S., and Schr¨odl, S. (2001). Constrained k-means clustering with background knowledge. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 577–584, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
</authors>
<title>Topic modeling: beyond bagof-words.</title>
<date>2005</date>
<booktitle>In NIPS 2005 Workshop on Bayesian Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="3026" citStr="Wallach, 2005" startWordPosition="494" endWordPosition="495">ic for the subsequent two frequent classes (“Acquisition” and “Forex”) and merged the least two frequent classes (“Crude” and “Grain”) into a single topic. The red colored words in topic 5 correspond to the “Crude” class and blue words are from the “Grain” class. This leads to the situation where the topics identified by LDA are not in accordance with the underlying topical structure of the corpus. This is a problem not just with LDA: it is potentially a problem with any extension thereof that have focused on improving the semantic coherence of the words in each topic (Griffiths et al., 2005; Wallach, 2005; Griffiths et al., 2007), the document topic distributions (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) or other aspects (Blei. and Lafferty., 2009). We address this problem by providing some additional information to the model. Initially, along with the document collection, a user may provide higher level view of the document collection. For instance, as discussed in Section 4.4, when run on historical NIPS papers, LDA fails to find topics related to Brain Imaging, Cognitive Science or Hardware, even though we know from the call for 204 Proceedings of the 13th Conference of the Eu</context>
</contexts>
<marker>Wallach, 2005</marker>
<rawString>Wallach, H. M. (2005). Topic modeling: beyond bagof-words. In NIPS 2005 Workshop on Bayesian Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williamson</author>
<author>C Wang</author>
<author>K A Heller</author>
<author>D M Blei</author>
</authors>
<title>The IBP compound dirichlet process and its application to focused topic modeling.</title>
<date>2010</date>
<booktitle>In ICML,</booktitle>
<pages>1151--1158</pages>
<contexts>
<context position="14354" citStr="Williamson et al., 2010" startWordPosition="2520" endWordPosition="2523">tions (ψs). Then for each document, we generate a list of seed sets that are allowed for this document. This list is represented using the binary vector ~b. This binary vector can be populated based on the document words and hence it is treated as an observed variable. For example, consider the (very short!) document “oil companies have merged”. According to the seed sets from Table 2, we define a binary vector that denotes which seed topics contain words in this document. In this case, this vector b~ = h1, 1, 0,1,1i, indicating the presence of seeds from sets 1, 2, 4 and 5.1 As discussed in (Williamson et al., 2010), generating binary vector is crucial if we want a document to talk about topics that are less prominent in the corpus. The binary vector ~b, that indicates which seeds exist in this document, defines a mean of a Dirichlet distribution from which we sample a document-group distribution, ζd (step 3b). We set the concentration of this Dirichlet to a hyperparamter τ, which we set by hand (Sec. 4); thus, ζd ∼ Dir(τ~b). From the resulting multinomial, we draw a group variable g for this document. This group variable brings clustering structure among the documents by grouping the documents that are </context>
<context position="23660" citStr="Williamson et al., 2010" startWordPosition="4113" endWordPosition="4116">with multiple baseline systems. Since our aim is to overcome the dominance of majority topics by encouraging the topicality structure identified by the topic models to align with that of the document corpus, we choose extrinsic evaluation as the primary evaluation method. We use document clustering task and use frequent-5 categories of Reuters-21578 corpus (Lewis et al., 2004) and four classes from the 20 Newsgroups data set (i.e.‘rec.autos’, ‘sci.electronics’, ‘comp.hardware’ and ‘alt.atheism’). For both the corpora we do the standard preprocessing of removing stopwords and infrequent words (Williamson et al., 2010). For all the models, we use a Collapsed Gibbs sampler (Griffiths and Steyvers, 2004) for the inference process. We use the standard hyperparameters values α = 1.0, Q = 0.01 and T = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009). 209 Reuters 20 Newsgroups F-measure VI F-measure VI LDA 0.64 (±.05) 1.26 (±.16) 0.77 (±.06) 0.9 (±.13) Dirichlet Forest 0.67* (±.02) 1.17 (±.11) 0.79(±.01) 0.83*(±.03) A over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%) Table 3: The effect of adding constraints by Dirich</context>
</contexts>
<marker>Williamson, Wang, Heller, Blei, 2010</marker>
<rawString>Williamson, S., Wang, C., Heller, K. A., and Blei, D. M. (2010). The IBP compound dirichlet process and its application to focused topic modeling. In ICML, pages 1151–1158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>