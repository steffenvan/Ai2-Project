<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.674531666666667">
Re-evaluating the Role of BLEU in Machine Translation Research
Chris Callison-Burch Miles Osborne Philipp Koehn
School on Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
</title>
<email confidence="0.992927">
callison-burch@ed.ac.uk
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847916666667">
We argue that the machine translation
community is overly reliant on the Bleu
machine translation evaluation metric. We
show that an improved Bleu score is nei-
ther necessary nor sufficient for achieving
an actual improvement in translation qual-
ity, and give two significant counterex-
amples to Bleu’s correlation with human
judgments of quality. This offers new po-
tential for research which was previously
deemed unpromising by an inability to im-
prove upon Bleu scores.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999776509803921">
Over the past five years progress in machine trans-
lation, and to a lesser extent progress in natural
language generation tasks such as summarization,
has been driven by optimizing against n-gram-
based evaluation metrics such as Bleu (Papineni
et al., 2002). The statistical machine translation
community relies on the Bleu metric for the pur-
poses of evaluating incremental system changes
and optimizing systems through minimum er-
ror rate training (Och, 2003). Conference pa-
pers routinely claim improvements in translation
quality by reporting improved Bleu scores, while
neglecting to show any actual example transla-
tions. Workshops commonly compare systems us-
ing Bleu scores, often without confirming these
rankings through manual evaluation. All these
uses of Bleu are predicated on the assumption that
it correlates with human judgments of translation
quality, which has been shown to hold in many
cases (Doddington, 2002; Coughlin, 2003).
However, there is a question as to whether min-
imizing the error rate with respect to Bleu does in-
deed guarantee genuine translation improvements.
If Bleu’s correlation with human judgments has
been overestimated, then the field needs to ask it-
self whether it should continue to be driven by
Bleu to the extent that it currently is. In this
paper we give a number of counterexamples for
Bleu’s correlation with human judgments. We
show that under some circumstances an improve-
ment in Bleu is not sufficient to reflect a genuine
improvement in translation quality, and in other
circumstances that it is not necessary to improve
Bleu in order to achieve a noticeable improvement
in translation quality.
We argue that Bleu is insufficient by showing
that Bleu admits a huge amount of variation for
identically scored hypotheses. Typically there are
millions of variations on a hypothesis translation
that receive the same Bleu score. Because not all
these variations are equally grammatically or se-
mantically plausible there are translations which
have the same Bleu score but a worse human eval-
uation. We further illustrate that in practice a
higher Bleu score is not necessarily indicative of
better translation quality by giving two substantial
examples of Bleu vastly underestimating the trans-
lation quality of systems. Finally, we discuss ap-
propriate uses for Bleu and suggest that for some
research projects it may be preferable to use a fo-
cused, manual evaluation instead.
</bodyText>
<sectionHeader confidence="0.996222" genericHeader="introduction">
2 BLEU Detailed
</sectionHeader>
<bodyText confidence="0.999972857142857">
The rationale behind the development of Bleu (Pa-
pineni et al., 2002) is that human evaluation of ma-
chine translation can be time consuming and ex-
pensive. An automatic evaluation metric, on the
other hand, can be used for frequent tasks like
monitoring incremental system changes during de-
velopment, which are seemingly infeasible in a
manual evaluation setting.
The way that Bleu and other automatic evalu-
ation metrics work is to compare the output of a
machine translation system against reference hu-
man translations. Machine translation evaluation
metrics differ from other metrics that use a refer-
ence, like the word error rate metric that is used
</bodyText>
<page confidence="0.998267">
249
</page>
<tableCaption confidence="0.693161833333333">
Appeared calm when he was taken to
the American plane, which will to Miami,
Florida.
Table 1: A set of four reference translations, and
a hypothesis translation from the 2005 NIST MT
Evaluation
</tableCaption>
<bodyText confidence="0.99714945">
in speech recognition, because translations have a
degree of variation in terms of word choice and in
terms of variant ordering of some phrases.
Bleu attempts to capture allowable variation in
word choice through the use of multiple reference
translations (as proposed in Thompson (1991)).
In order to overcome the problem of variation in
phrase order, Bleu uses modified n-gram precision
instead of WER’s more strict string edit distance.
Bleu’s n-gram precision is modified to elimi-
nate repetitions that occur across sentences. For
example, even though the bigram “to Miami” is
repeated across all four reference translations in
Table 1, it is counted only once in a hypothesis
translation. Table 2 shows the n-gram sets created
from the reference translations.
Papineni et al. (2002) calculate their modified
precision score, pn, for each n-gram length by
summing over the matches for every hypothesis
sentence S in the complete corpus C as:
</bodyText>
<equation confidence="0.8390775">
E EngramES Countmatched(ngram)
SEC
E EngramES Count(ngram)
SEC
</equation>
<bodyText confidence="0.999581875">
Counting punctuation marks as separate tokens,
the hypothesis translation given in Table 1 has 15
unigram matches, 10 bigram matches, 5 trigram
matches (these are shown in bold in Table 2), and
three 4-gram matches (not shown). The hypoth-
esis translation contains a total of 18 unigrams,
17 bigrams, 16 trigrams, and 15 4-grams. If the
complete corpus consisted of this single sentence
</bodyText>
<note confidence="0.576401">
1-grams: American, Florida, Miami, Orejuela, ap-
</note>
<bodyText confidence="0.981240708333333">
peared, as, being, calm, carry, escorted, he, him, in, led,
plane, quite, seemed, take, that, the, to, to, to, was , was,
which, while, will, would,,,.
2-grams: American plane, Florida., Miami,, Miami
in, Orejuela appeared, Orejuela seemed, appeared calm,
as he, being escorted, being led, calm as, calm while, carry
him, escorted to, he was, him to, in Florida, led to, plane
that, plane which, quite calm, seemed quite, take him, that
was, that would, the American, the plane, to Miami, to
carry, to the, was being, was led, was to, which will, while
being, will take, would take, , Florida
3-grams: American plane that, American plane which,
Miami , Florida, Miami in Florida, Orejuela appeared
calm, Orejuela seemed quite, appeared calm as, appeared
calm while, as he was, being escorted to, being led to, calm
as he, calm while being, carry him to, escorted to the, he
was being, he was led, him to Miami, in Florida., led to
the, plane that was, plane that would, plane which will,
quite calm as, seemed quite calm, take him to, that was to,
that would take, the American plane, the plane that, to
Miami,, to Miami in, to carry him, to the American, to
the plane, was being led, was led to, was to carry, which
will take, while being escorted, will take him, would take
him, , Florida .
</bodyText>
<tableCaption confidence="0.907553">
Table 2: The n-grams extracted from the refer-
ence translations, with matches from the hypoth-
esis translation in bold
</tableCaption>
<bodyText confidence="0.996585727272727">
then the modified precisions would be p1 = .83,
p2 = .59, p3 = .31, and p4 = .2. Each pn is com-
bined and can be weighted by specifying a weight
wn. In practice each pn is generally assigned an
equal weight.
Because Bleu is precision based, and because
recall is difficult to formulate over multiple refer-
ence translations, a brevity penalty is introduced to
compensate for the possibility of proposing high-
precision hypothesis translations which are too
short. The brevity penalty is calculated as:
</bodyText>
<equation confidence="0.902212666666667">
�
1 if c &gt; r
BP = e1−r/c if c ≤ r
</equation>
<bodyText confidence="0.990328">
where c is the length of the corpus of hypothesis
translations, and r is the effective reference corpus
length.1
Thus, the Bleu score is calculated as
</bodyText>
<equation confidence="0.988886666666667">
N
Bleu = BP ∗ exp( E wn logpn)
n=1
</equation>
<bodyText confidence="0.942347210526316">
A Bleu score can range from 0 to 1, where
higher scores indicate closer matches to the ref-
erence translations, and where a score of 1 is as-
signed to a hypothesis translation which exactly
1The effective reference corpus length is calculated as the
sum of the single reference translation from each set which is
closest to the hypothesis translation.
Orejuela appeared calm as he was led to the
American plane which will take him to Mi-
ami, Florida.
Orejuela appeared calm while being escorted
to the plane that would take him to Miami,
Florida.
Orejuela appeared calm as he was being led
to the American plane that was to carry him
to Miami in Florida.
Orejuela seemed quite calm as he was being
led to the American plane that would take
him to Miami in Florida.
</bodyText>
<equation confidence="0.88923">
pn =
</equation>
<page confidence="0.948204">
250
</page>
<bodyText confidence="0.999929238095238">
matches one of the reference translations. A score
of 1 is also assigned to a hypothesis translation
which has matches for all its n-grams (up to the
maximum n measured by Bleu) in the clipped ref-
erence n-grams, and which has no brevity penalty.
The primary reason that Bleu is viewed as a use-
ful stand-in for manual evaluation is that it has
been shown to correlate with human judgments of
translation quality. Papineni et al. (2002) showed
that Bleu correlated with human judgments in
its rankings of five Chinese-to-English machine
translation systems, and in its ability to distinguish
between human and machine translations. Bleu’s
correlation with human judgments has been fur-
ther tested in the annual NIST Machine Transla-
tion Evaluation exercise wherein Bleu’s rankings
of Arabic-to-English and Chinese-to-English sys-
tems is verified by manual evaluation.
In the next section we discuss theoretical rea-
sons why Bleu may not always correlate with hu-
man judgments.
</bodyText>
<sectionHeader confidence="0.963307" genericHeader="method">
3 Variations Allowed By BLEU
</sectionHeader>
<bodyText confidence="0.999962892857143">
While Bleu attempts to capture allowable variation
in translation, it goes much further than it should.
In order to allow some amount of variant order in
phrases, Bleu places no explicit constraints on the
order that matching n-grams occur in. To allow
variation in word choice in translation Bleu uses
multiple reference translations, but puts very few
constraints on how n-gram matches can be drawn
from the multiple reference translations. Because
Bleu is underconstrained in these ways, it allows a
tremendous amount of variation – far beyond what
could reasonably be considered acceptable varia-
tion in translation.
In this section we examine various permutations
and substitutions allowed by Bleu. We show that
for an average hypothesis translation there are mil-
lions of possible variants that would each receive
a similar Bleu score. We argue that because the
number of translations that score the same is so
large, it is unlikely that all of them will be judged
to be identical in quality by human annotators.
This means that it is possible to have items which
receive identical Bleu scores but are judged by hu-
mans to be worse. It is also therefore possible to
have a higher Bleu score without any genuine im-
provement in translation quality. In Sections 3.1
and 3.2 we examine ways of synthetically produc-
ing such variant translations.
</bodyText>
<subsectionHeader confidence="0.999831">
3.1 Permuting phrases
</subsectionHeader>
<bodyText confidence="0.986796044444445">
One way in which variation can be introduced is
by permuting phrases within a hypothesis trans-
lation. A simple way of estimating a lower bound
on the number of ways that phrases in a hypothesis
translation can be reordered is to examine bigram
mismatches. Phrases that are bracketed by these
bigram mismatch sites can be freely permuted be-
cause reordering a hypothesis translation at these
points will not reduce the number of matching n-
grams and thus will not reduce the overall Bleu
score.
Here we denote bigram mismatches for the hy-
pothesis translation given in Table 1 with vertical
bars:
Appeared calm  |when  |he was  |taken |
to the American plane  |,  |which will |
to Miami, Florida.
We can randomly produce other hypothesis trans-
lations that have the same Bleu score but are rad-
ically different from each other. Because Bleu
only takes order into account through rewarding
matches of higher order n-grams, a hypothesis
sentence may be freely permuted around these
bigram mismatch sites and without reducing the
Bleu score. Thus:
which will  |he was  |,  |when  |taken |
Appeared calm  |to the American plane
 |to Miami , Florida.
receives an identical score to the hypothesis trans-
lation in Table 1.
If b is the number of bigram matches in a hy-
pothesis translation, and k is its length, then there
are
(k − b)! (1)
possible ways to generate similarly scored items
using only the words in the hypothesis transla-
tion.2 Thus for the example hypothesis transla-
tion there are at least 40,320 different ways of per-
muting the sentence and receiving a similar Bleu
score. The number of permutations varies with
respect to sentence length and number of bigram
mismatches. Therefore as a hypothesis translation
approaches being an identical match to one of the
reference translations, the amount of variance de-
creases significantly. So, as translations improve
</bodyText>
<footnote confidence="0.97569625">
2Note that in some cases randomly permuting the sen-
tence in this way may actually result in a greater number of
n-gram matches; however, one would not expect random per-
mutation to increase the human evaluation.
</footnote>
<page confidence="0.990004">
251
</page>
<figureCaption confidence="0.93291925">
Figure 1: Scatterplot of the length of each trans-
lation against its number of possible permutations
due to bigram mismatches for an entry in the 2005
NIST MT Eval
</figureCaption>
<bodyText confidence="0.999686428571429">
spurious variation goes down. However, at today’s
levels the amount of variation that Bleu admits is
unacceptably high. Figure 1 gives a scatterplot
of each of the hypothesis translations produced by
the second best Bleu system from the 2005 NIST
MT Evaluation. The number of possible permuta-
tions for some translations is greater than 1073.
</bodyText>
<subsectionHeader confidence="0.919991">
3.2 Drawing different items from the
reference set
</subsectionHeader>
<bodyText confidence="0.999979833333334">
In addition to the factorial number of ways that
similarly scored Bleu items can be generated
by permuting phrases around bigram mismatch
points, additional variation may be synthesized
by drawing different items from the reference n-
grams. For example, since the hypothesis trans-
lation from Table 1 has a length of 18 with 15
unigram matches, 10 bigram matches, 5 trigram
matches, and three 4-gram matches, we can arti-
ficially construct an identically scored hypothesis
by drawing an identical number of matching n-
grams from the reference translations. Therefore
the far less plausible:
was being led to the  |calm as he was |
would take  |carry him  |seemed quite |
when  |taken
would receive the same Bleu score as the hypoth-
esis translation from Table 1, even though human
judges would assign it a much lower score.
This problem is made worse by the fact that
Bleu equally weights all items in the reference
sentences (Babych and Hartley, 2004). There-
fore omitting content-bearing lexical items does
not carry a greater penalty than omitting function
words.
The problem is further exacerbated by Bleu not
having any facilities for matching synonyms or
lexical variants. Therefore words in the hypothesis
that did not appear in the references (such as when
and taken in the hypothesis from Table 1) can be
substituted with arbitrary words because they do
not contribute towards the Bleu score. Under Bleu,
we could just as validly use the words black and
helicopters as we could when and taken.
The lack of recall combined with naive token
identity means that there can be overlap between
similar items in the multiple reference transla-
tions. For example we can produce a translation
which contains both the words carry and take even
though they arise from the same source word. The
chance of problems of this sort being introduced
increases as we add more reference translations.
</bodyText>
<subsectionHeader confidence="0.79209">
3.3 Implication: BLEU cannot guarantee
correlation with human judgments
</subsectionHeader>
<bodyText confidence="0.997101333333333">
Bleu’s inability to distinguish between randomly
generated variations in translation hints that it may
not correlate with human judgments of translation
quality in some cases. As the number of identi-
cally scored variants goes up, the likelihood that
they would all be judged equally plausible goes
down. This is a theoretical point, and while the
variants are artificially constructed, it does high-
light the fact that Bleu is quite a crude measure-
ment of translation quality.
A number of prominent factors contribute to
Bleu’s crudeness:
</bodyText>
<listItem confidence="0.832823777777778">
• Synonyms and paraphrases are only handled
if they are in the set of multiple reference
translations.
• The scores for words are equally weighted
so missing out on content-bearing material
brings no additional penalty.
• The brevity penalty is a stop-gap measure to
compensate for the fairly serious problem of
not being able to calculate recall.
</listItem>
<bodyText confidence="0.9991415">
Each of these failures contributes to an increased
amount of inappropriately indistinguishable trans-
lations in the analysis presented above.
Given that Bleu can theoretically assign equal
scoring to translations of obvious different qual-
ity, it is logical that a higher Bleu score may not
</bodyText>
<figure confidence="0.877255210526316">
120
100
80
60
40
20
0
1 1e+10 1e+20 1e+30 1e+40 1e+50 1e+60 1e+70 1e+80
Number of Permutations
Sentence Length
252
Fluency
How do you judge the fluency of this translation?
5 = Flawless English
4 = Good English
3 = Non-native English
2 = Disfluent English
1 = Incomprehensible
Adequacy
</figure>
<bodyText confidence="0.927782333333333">
How much of the meaning expressed in the refer-
ence translation is also expressed in the hypothesis
translation?
</bodyText>
<sectionHeader confidence="0.9942518" genericHeader="method">
5 = All
4 = Most
3 = Much
2 = Little
1 = None
</sectionHeader>
<tableCaption confidence="0.988704">
Table 3: The scales for manually assigned ade-
quacy and fluency scores
</tableCaption>
<figureCaption confidence="0.996007066666667">
Iran has already stated that Kharazi’s state-
ments to the conference because of the Jor-
danian King Abdullah II in which he stood
accused Iran of interfering in Iraqi affairs.
n-gram matches: 27 unigrams, 20 bigrams,
15 trigrams, and ten 4-grams
human scores: Adequacy:3,2 Fluency:3,2
Iran already announced that Kharrazi will not
attend the conference because of the state-
ments made by the Jordanian Monarch Ab-
dullah II who has accused Iran of interfering
in Iraqi affairs.
n-gram matches: 24 unigrams, 19 bigrams,
15 trigrams, and 12 4-grams
human scores: Adequacy:5,4 Fluency:5,4
</figureCaption>
<bodyText confidence="0.998933727272727">
Reference: Iran had already announced
Kharazi would boycott the conference after
Jordan’s King Abdullah II accused Iran of
meddling in Iraq’s affairs.
necessarily be indicative of a genuine improve-
ment in translation quality. This begs the question
as to whether this is only a theoretical concern or
whether Bleu’s inadequacies can come into play
in practice. In the next section we give two signif-
icant examples that show that Bleu can indeed fail
to correlate with human judgments in practice.
</bodyText>
<sectionHeader confidence="0.9843725" genericHeader="method">
4 Failures in Practice: the 2005 NIST
MT Eval, and Systran v. SMT
</sectionHeader>
<bodyText confidence="0.997932723404255">
The NIST Machine Translation Evaluation exer-
cise has run annually for the past five years as
part of DARPA’s TIDES program. The quality of
Chinese-to-English and Arabic-to-English transla-
tion systems is evaluated both by using Bleu score
and by conducting a manual evaluation. As such,
the NIST MT Eval provides an excellent source
of data that allows Bleu’s correlation with hu-
man judgments to be verified. Last year’s eval-
uation exercise (Lee and Przybocki, 2005) was
startling in that Bleu’s rankings of the Arabic-
English translation systems failed to fully corre-
spond to the manual evaluation. In particular, the
entry that was ranked 1st in the human evaluation
was ranked 6th by Bleu. In this section we exam-
ine Bleu’s failure to correctly rank this entry.
The manual evaluation conducted for the NIST
MT Eval is done by English speakers without ref-
erence to the original Arabic or Chinese docu-
ments. Two judges assigned each sentence in
Table 4: Two hypothesis translations with similar
Bleu scores but different human scores, and one of
four reference translations
the hypothesis translations a subjective 1–5 score
along two axes: adequacy and fluency (LDC,
2005). Table 3 gives the interpretations of the
scores. When first evaluating fluency, the judges
are shown only the hypothesis translation. They
are then shown a reference translation and are
asked to judge the adequacy of the hypothesis sen-
tences.
Table 4 gives a comparison between the output
of the system that was ranked 2nd by Bleu3 (top)
and of the entry that was ranked 6th in Bleu but
1st in the human evaluation (bottom). The exam-
ple is interesting because the number of match-
ing n-grams for the two hypothesis translations
is roughly similar but the human scores are quite
different. The first hypothesis is less adequate
because it fails to indicated that Kharazi is boy-
cotting the conference, and because it inserts the
word stood before accused which makes the Ab-
dullah’s actions less clear. The second hypothe-
sis contains all of the information of the reference,
but uses some synonyms and paraphrases which
would not picked up on by Bleu: will not attend
for would boycott and interfering for meddling.
</bodyText>
<footnote confidence="0.97856">
3The output of the system that was ranked 1st by Bleu is
not publicly available.
</footnote>
<page confidence="0.99804">
253
</page>
<figure confidence="0.9125875">
0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.52
Bleu Score
</figure>
<figureCaption confidence="0.644780333333333">
Figure 2: Bleu scores plotted against human judg-
ments of adequacy, with R2 = 0.14 when the out-
lier entry is included
</figureCaption>
<figure confidence="0.876647">
0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.52
Bleu Score
</figure>
<figureCaption confidence="0.95411">
Figure 3: Bleu scores plotted against human judg-
</figureCaption>
<bodyText confidence="0.4164235">
ments of fluency, with R2 = 0.002 when the out-
lier entry is included
</bodyText>
<figure confidence="0.936169428571429">
Adequacy
Correlation
Fluency
Correlation
Human Score 4
3.5
3
2.5
2
Human Score 4
3.5
3
2.5
2
</figure>
<bodyText confidence="0.994263469387755">
Figures 2 and 3 plot the average human score
for each of the seven NIST entries against its
Bleu score. It is notable that one entry received
a much higher human score than would be antici-
pated from its low Bleu score. The offending en-
try was unusual in that it was not fully automatic
machine translation; instead the entry was aided
by monolingual English speakers selecting among
alternative automatic translations of phrases in the
Arabic source sentences and post-editing the result
(Callison-Burch, 2005). The remaining six entries
were all fully automatic machine translation sys-
tems; in fact, they were all phrase-based statistical
machine translation system that had been trained
on the same parallel corpus and most used Bleu-
based minimum error rate training (Och, 2003) to
optimize the weights of their log linear models’
feature functions (Och and Ney, 2002).
This opens the possibility that in order for Bleu
to be valid only sufficiently similar systems should
be compared with one another. For instance, when
measuring correlation using Pearson’s we get a
very low correlation of R2 = 0.14 when the out-
lier in Figure 2 is included, but a strong R2 = 0.87
when it is excluded. Similarly Figure 3 goes from
R2 = 0.002 to a much stronger R2 = 0.742.
Systems which explore different areas of transla-
tion space may produce output which has differ-
ing characteristics, and might end up in different
regions of the human scores / Bleu score graph.
We investigated this by performing a manual
evaluation comparing the output of two statisti-
cal machine translation systems with a rule-based
machine translation, and seeing whether Bleu cor-
rectly ranked the systems. We used Systran for the
rule-based system, and used the French-English
portion of the Europarl corpus (Koehn, 2005) to
train the SMT systems and to evaluate all three
systems. We built the first phrase-based SMT sys-
tem with the complete set of Europarl data (14-
15 million words per language), and optimized its
feature functions using minimum error rate train-
ing in the standard way (Koehn, 2004). We eval-
uated it and the Systran system with Bleu using
a set of 2,000 held out sentence pairs, using the
same normalization and tokenization schemes on
both systems’ output. We then built a number of
SMT systems with various portions of the training
corpus, and selected one that was trained with 1
</bodyText>
<page confidence="0.784153">
64
</page>
<bodyText confidence="0.9996820625">
of the data, which had a Bleu score that was close
to, but still higher than that for the rule-based sys-
tem.
We then performed a manual evaluation where
we had three judges assign fluency and adequacy
ratings for the English translations of 300 French
sentences for each of the three systems. These
scores are plotted against the systems’ Bleu scores
in Figure 4. The graph shows that the Bleu score
for the rule-based system (Systran) vastly under-
estimates its actual quality. This serves as another
significant counter-example to Bleu’s correlation
with human judgments of translation quality, and
further increases the concern that Bleu may not be
appropriate for comparing systems which employ
different translation strategies.
</bodyText>
<page confidence="0.996579">
254
</page>
<figure confidence="0.9347695">
0.18 0.2 0.22 0.24 0.26 0.28 0.3
Bleu Score
</figure>
<figureCaption confidence="0.6769215">
Figure 4: Bleu scores plotted against human
judgments of fluency and adequacy, showing that
Bleu vastly underestimates the quality of a non-
statistical system
</figureCaption>
<sectionHeader confidence="0.999968" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999980820512821">
A number of projects in the past have looked into
ways of extending and improving the Bleu met-
ric. Doddington (2002) suggested changing Bleu’s
weighted geometric average of n-gram matches to
an arithmetic average, and calculating the brevity
penalty in a slightly different manner. Hovy and
Ravichandra (2003) suggested increasing Bleu’s
sensitivity to inappropriate phrase movement by
matching part-of-speech tag sequences against ref-
erence translations in addition to Bleu’s n-gram
matches. Babych and Hartley (2004) extend Bleu
by adding frequency weighting to lexical items
through TF/IDF as a way of placing greater em-
phasis on content-bearing words and phrases.
Two alternative automatic translation evaluation
metrics do a much better job at incorporating re-
call than Bleu does. Melamed et al. (2003) for-
mulate a metric which measures translation accu-
racy in terms of precision and recall directly rather
than precision and a brevity penalty. Banerjee and
Lavie (2005) introduce the Meteor metric, which
also incorporates recall on the unigram level and
further provides facilities incorporating stemming,
and WordNet synonyms as a more flexible match.
Lin and Hovy (2003) as well as Soricut and Brill
(2004) present ways of extending the notion of n-
gram co-occurrence statistics over multiple refer-
ences, such as those used in Bleu, to other natural
language generation tasks such as summarization.
Both these approaches potentially suffer from the
same weaknesses that Bleu has in machine trans-
lation evaluation.
Coughlin (2003) performs a large-scale inves-
tigation of Bleu’s correlation with human judg-
ments, and finds one example that fails to corre-
late. Her future work section suggests that she
has preliminary evidence that statistical machine
translation systems receive a higher Bleu score
than their non-n-gram-based counterparts.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.996326095238095">
In this paper we have shown theoretical and prac-
tical evidence that Bleu may not correlate with hu-
man judgment to the degree that it is currently be-
lieved to do. We have shown that Bleu’s rather
coarse model of allowable variation in translation
can mean that an improved Bleu score is not suffi-
cient to reflect a genuine improvement in transla-
tion quality. We have further shown that it is not
necessary to receive a higher Bleu score in order
to be judged to have better translation quality by
human subjects, as illustrated in the 2005 NIST
Machine Translation Evaluation and our experi-
ment manually evaluating Systran and SMT trans-
lations.
What conclusions can we draw from this?
Should we give up on using Bleu entirely? We
think that the advantages of Bleu are still very
strong; automatic evaluation metrics are inexpen-
sive, and do allow many tasks to be performed
that would otherwise be impossible. The impor-
tant thing therefore is to recognize which uses of
Bleu are appropriate and which uses are not.
Appropriate uses for Bleu include tracking
broad, incremental changes to a single system,
comparing systems which employ similar trans-
lation strategies (such as comparing phrase-based
statistical machine translation systems with other
phrase-based statistical machine translation sys-
tems), and using Bleu as an objective function to
optimize the values of parameters such as feature
weights in log linear translation models, until a
better metric has been proposed.
Inappropriate uses for Bleu include comparing
systems which employ radically different strate-
gies (especially comparing phrase-based statistical
machine translation systems against systems that
do not employ similar n-gram-based approaches),
trying to detect improvements for aspects of trans-
lation that are not modeled well by Bleu, and
monitoring improvements that occur infrequently
within a test corpus.
These comments do not apply solely to Bleu.
</bodyText>
<figure confidence="0.978054416666667">
Rule-based System
(Systran)
SMT System 2
Adequacy
Fluency
SMT System 1
Human Score 4.5
4
3.5
3
2.5
2
</figure>
<page confidence="0.995741">
255
</page>
<bodyText confidence="0.999465722222223">
Meteor (Banerjee and Lavie, 2005), Precision and
Recall (Melamed et al., 2003), and other such au-
tomatic metrics may also be affected to a greater
or lesser degree because they are all quite rough
measures of translation similarity, and have inex-
act models of allowable variation in translation.
Finally, that the fact that Bleu’s correlation with
human judgments has been drawn into question
may warrant a re-examination of past work which
failed to show improvements in Bleu. For ex-
ample, work which failed to detect improvements
in translation quality with the integration of word
sense disambiguation (Carpuat and Wu, 2005), or
work which attempted to integrate syntactic infor-
mation but which failed to improve Bleu (Char-
niak et al., 2003; Och et al., 2004) may deserve a
second look with a more targeted manual evalua-
tion.
</bodyText>
<sectionHeader confidence="0.999521" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9605246">
The authors are grateful to Amittai Axelrod,
Frank Keller, Beata Kouchnir, Jean Senellart, and
Matthew Stone for their feedback on drafts of this
paper, and to Systran for providing translations of
the Europarl test set.
</bodyText>
<sectionHeader confidence="0.997954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843422535211">
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing the Bleu MT evaluation method with frequency
weightings. In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, Michigan.
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of ACL.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proceedings of MT Summit IX.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Human Language Technol-
ogy: Notebook Proceedings, pages 128–132, San
Diego.
Eduard Hovy and Deepak Ravichandra. 2003. Holy
and unholy grails. Panel Discussion at MT Summit
IX.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee and Mark Przybocki. 2005. NIST 2005
machine translation evaluation official results. Of-
ficial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Ed Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT-NAACL.
Dan Melamed, Ryan Green, and Jospeh P. Turian.
2003. Precision and recall of machine translation.
In Proceedings of HLT/NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of NAACL-04, Boston.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Radu Soricut and Eric Brill. 2004. A unified frame-
work for automatic evaluation using n-gram co-
occurrence statistics. In Proceedings of ACL.
Henry Thompson. 1991. Automatic evaluation of
translation quality: Outline of methodology and re-
port on pilot experiment. In (ISSCO) Proceedings
of the Evaluators Forum, pages 215–223, Geneva,
Switzerland.
</reference>
<page confidence="0.99844">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.650472">
<title confidence="0.997646">the Role of Machine Translation Research</title>
<author confidence="0.999955">Chris Callison-Burch Miles Osborne Philipp Koehn</author>
<affiliation confidence="0.896830333333333">School on Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.976238">Edinburgh, EH8 9LW</address>
<email confidence="0.996944">callison-burch@ed.ac.uk</email>
<abstract confidence="0.997167076923077">We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Extending the Bleu MT evaluation method with frequency weightings.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14265" citStr="Babych and Hartley, 2004" startWordPosition="2361" endWordPosition="2364">with 15 unigram matches, 10 bigram matches, 5 trigram matches, and three 4-gram matches, we can artificially construct an identically scored hypothesis by drawing an identical number of matching ngrams from the reference translations. Therefore the far less plausible: was being led to the |calm as he was | would take |carry him |seemed quite | when |taken would receive the same Bleu score as the hypothesis translation from Table 1, even though human judges would assign it a much lower score. This problem is made worse by the fact that Bleu equally weights all items in the reference sentences (Babych and Hartley, 2004). Therefore omitting content-bearing lexical items does not carry a greater penalty than omitting function words. The problem is further exacerbated by Bleu not having any facilities for matching synonyms or lexical variants. Therefore words in the hypothesis that did not appear in the references (such as when and taken in the hypothesis from Table 1) can be substituted with arbitrary words because they do not contribute towards the Bleu score. Under Bleu, we could just as validly use the words black and helicopters as we could when and taken. The lack of recall combined with naive token ident</context>
<context position="24646" citStr="Babych and Hartley (2004)" startWordPosition="4080" endWordPosition="4083">nd adequacy, showing that Bleu vastly underestimates the quality of a nonstatistical system 5 Related Work A number of projects in the past have looked into ways of extending and improving the Bleu metric. Doddington (2002) suggested changing Bleu’s weighted geometric average of n-gram matches to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and W</context>
</contexts>
<marker>Babych, Hartley, 2004</marker>
<rawString>Bogdan Babych and Anthony Hartley. 2004. Extending the Bleu MT evaluation method with frequency weightings. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="25103" citStr="Banerjee and Lavie (2005)" startWordPosition="4153" endWordPosition="4156">to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Coughlin (2003) performs a large-scale investigation </context>
<context position="28059" citStr="Banerjee and Lavie, 2005" startWordPosition="4616" endWordPosition="4619">a better metric has been proposed. Inappropriate uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus. These comments do not apply solely to Bleu. Rule-based System (Systran) SMT System 2 Adequacy Fluency SMT System 1 Human Score 4.5 4 3.5 3 2.5 2 255 Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al., 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation. Finally, that the fact that Bleu’s correlation with human judgments has been drawn into question may warrant a re-examination of past work which failed to show improvements in Bleu. For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or w</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Linear B system description for the 2005 NIST MT evaluation exercise.</title>
<date>2005</date>
<booktitle>In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="21312" citStr="Callison-Burch, 2005" startWordPosition="3539" endWordPosition="3540">entry is included Adequacy Correlation Fluency Correlation Human Score 4 3.5 3 2.5 2 Human Score 4 3.5 3 2.5 2 Figures 2 and 3 plot the average human score for each of the seven NIST entries against its Bleu score. It is notable that one entry received a much higher human score than would be anticipated from its low Bleu score. The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models’ feature functions (Och and Ney, 2002). This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another. For instance, when measuring correlation using Pearson’s we get a very low correlation of R2 = 0.14 when </context>
</contexts>
<marker>Callison-Burch, 2005</marker>
<rawString>Chris Callison-Burch. 2005. Linear B system description for the 2005 NIST MT evaluation exercise. In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Word sense disambiguation vs. statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Marine Carpuat and Dekai Wu. 2005. Word sense disambiguation vs. statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX.</booktitle>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for machine translation. In Proceedings of MT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Coughlin</author>
</authors>
<title>Correlating automated and human assessments of machine translation quality.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX.</booktitle>
<contexts>
<context position="1652" citStr="Coughlin, 2003" startWordPosition="245" endWordPosition="246">ric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements. If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is. In this paper we give a number of counterexamples for Bleu’s correlation with human judgments. We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it </context>
<context position="25665" citStr="Coughlin (2003)" startWordPosition="4241" endWordPosition="4242">sion and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Coughlin (2003) performs a large-scale investigation of Bleu’s correlation with human judgments, and finds one example that fails to correlate. Her future work section suggests that she has preliminary evidence that statistical machine translation systems receive a higher Bleu score than their non-n-gram-based counterparts. 6 Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation can mean that an improved Ble</context>
</contexts>
<marker>Coughlin, 2003</marker>
<rawString>Deborah Coughlin. 2003. Correlating automated and human assessments of machine translation quality. In Proceedings of MT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Human Language Technology: Notebook Proceedings,</booktitle>
<pages>128--132</pages>
<location>San Diego.</location>
<contexts>
<context position="1635" citStr="Doddington, 2002" startWordPosition="243" endWordPosition="244">es on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements. If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is. In this paper we give a number of counterexamples for Bleu’s correlation with human judgments. We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circu</context>
<context position="24244" citStr="Doddington (2002)" startWordPosition="4028" endWordPosition="4029">mates its actual quality. This serves as another significant counter-example to Bleu’s correlation with human judgments of translation quality, and further increases the concern that Bleu may not be appropriate for comparing systems which employ different translation strategies. 254 0.18 0.2 0.22 0.24 0.26 0.28 0.3 Bleu Score Figure 4: Bleu scores plotted against human judgments of fluency and adequacy, showing that Bleu vastly underestimates the quality of a nonstatistical system 5 Related Work A number of projects in the past have looked into ways of extending and improving the Bleu metric. Doddington (2002) suggested changing Bleu’s weighted geometric average of n-gram matches to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Human Language Technology: Notebook Proceedings, pages 128–132, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Deepak Ravichandra</author>
</authors>
<title>Holy and unholy grails. Panel Discussion at MT Summit IX.</title>
<date>2003</date>
<contexts>
<context position="24437" citStr="Hovy and Ravichandra (2003)" startWordPosition="4053" endWordPosition="4056">Bleu may not be appropriate for comparing systems which employ different translation strategies. 254 0.18 0.2 0.22 0.24 0.26 0.28 0.3 Bleu Score Figure 4: Bleu scores plotted against human judgments of fluency and adequacy, showing that Bleu vastly underestimates the quality of a nonstatistical system 5 Related Work A number of projects in the past have looked into ways of extending and improving the Bleu metric. Doddington (2002) suggested changing Bleu’s weighted geometric average of n-gram matches to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rath</context>
</contexts>
<marker>Hovy, Ravichandra, 2003</marker>
<rawString>Eduard Hovy and Deepak Ravichandra. 2003. Holy and unholy grails. Panel Discussion at MT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="22871" citStr="Koehn, 2004" startWordPosition="3800" endWordPosition="3801"> investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (14- 15 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training corpus, and selected one that was trained with 1 64 of the data, which had a Bleu score that was close to, but still higher than that for the rule-based system. We then performed a manual evaluation where we had three judges assign fluency and adequacy ratings for the English translations of 300 French sentences for each of the three systems. The</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT-Summit.</booktitle>
<contexts>
<context position="22588" citStr="Koehn, 2005" startWordPosition="3751" endWordPosition="3752"> it is excluded. Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742. Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (14- 15 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training corpus, and selected one that was trained with 1 64 of the data, </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. A parallel corpus for statistical machine translation. In Proceedings of MT-Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Linguistic data annotation specification: Assessment of fluency and adequacy in translations.</title>
<date>2005</date>
<journal>Revision</journal>
<volume>1</volume>
<contexts>
<context position="19278" citStr="LDC, 2005" startWordPosition="3185" endWordPosition="3186">orrespond to the manual evaluation. In particular, the entry that was ranked 1st in the human evaluation was ranked 6th by Bleu. In this section we examine Bleu’s failure to correctly rank this entry. The manual evaluation conducted for the NIST MT Eval is done by English speakers without reference to the original Arabic or Chinese documents. Two judges assigned each sentence in Table 4: Two hypothesis translations with similar Bleu scores but different human scores, and one of four reference translations the hypothesis translations a subjective 1–5 score along two axes: adequacy and fluency (LDC, 2005). Table 3 gives the interpretations of the scores. When first evaluating fluency, the judges are shown only the hypothesis translation. They are then shown a reference translation and are asked to judge the adequacy of the hypothesis sentences. Table 4 gives a comparison between the output of the system that was ranked 2nd by Bleu3 (top) and of the entry that was ranked 6th in Bleu but 1st in the human evaluation (bottom). The example is interesting because the number of matching n-grams for the two hypothesis translations is roughly similar but the human scores are quite different. The first </context>
</contexts>
<marker>LDC, 2005</marker>
<rawString>LDC. 2005. Linguistic data annotation specification: Assessment of fluency and adequacy in translations. Revision 1.5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Lee</author>
<author>Mark Przybocki</author>
</authors>
<title>machine translation evaluation official results. Official release of automatic evaluation scores for all submissions,</title>
<date>2005</date>
<publisher>NIST</publisher>
<contexts>
<context position="18571" citStr="Lee and Przybocki, 2005" startWordPosition="3067" endWordPosition="3070"> examples that show that Bleu can indeed fail to correlate with human judgments in practice. 4 Failures in Practice: the 2005 NIST MT Eval, and Systran v. SMT The NIST Machine Translation Evaluation exercise has run annually for the past five years as part of DARPA’s TIDES program. The quality of Chinese-to-English and Arabic-to-English translation systems is evaluated both by using Bleu score and by conducting a manual evaluation. As such, the NIST MT Eval provides an excellent source of data that allows Bleu’s correlation with human judgments to be verified. Last year’s evaluation exercise (Lee and Przybocki, 2005) was startling in that Bleu’s rankings of the ArabicEnglish translation systems failed to fully correspond to the manual evaluation. In particular, the entry that was ranked 1st in the human evaluation was ranked 6th by Bleu. In this section we examine Bleu’s failure to correctly rank this entry. The manual evaluation conducted for the NIST MT Eval is done by English speakers without reference to the original Arabic or Chinese documents. Two judges assigned each sentence in Table 4: Two hypothesis translations with similar Bleu scores but different human scores, and one of four reference trans</context>
</contexts>
<marker>Lee, Przybocki, 2005</marker>
<rawString>Audrey Lee and Mark Przybocki. 2005. NIST 2005 machine translation evaluation official results. Official release of automatic evaluation scores for all submissions, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Ed Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="25307" citStr="Lin and Hovy (2003)" startWordPosition="4183" endWordPosition="4186"> to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Coughlin (2003) performs a large-scale investigation of Bleu’s correlation with human judgments, and finds one example that fails to correlate. Her future work section suggests that she has preliminary evidence that statistical machine translation systems r</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Ed Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
<author>Ryan Green</author>
<author>Jospeh P Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="24935" citStr="Melamed et al. (2003)" startWordPosition="4126" endWordPosition="4129">s to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. </context>
<context position="28104" citStr="Melamed et al., 2003" startWordPosition="4623" endWordPosition="4626">uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus. These comments do not apply solely to Bleu. Rule-based System (Systran) SMT System 2 Adequacy Fluency SMT System 1 Human Score 4.5 4 3.5 3 2.5 2 255 Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al., 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation. Finally, that the fact that Bleu’s correlation with human judgments has been drawn into question may warrant a re-examination of past work which failed to show improvements in Bleu. For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic in</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>Dan Melamed, Ryan Green, and Jospeh P. Turian. 2003. Precision and recall of machine translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21672" citStr="Och and Ney, 2002" startWordPosition="3594" endWordPosition="3597">n that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models’ feature functions (Och and Ney, 2002). This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another. For instance, when measuring correlation using Pearson’s we get a very low correlation of R2 = 0.14 when the outlier in Figure 2 is included, but a strong R2 = 0.87 when it is excluded. Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742. Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-04,</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of NAACL-04, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1169" citStr="Och, 2003" startWordPosition="174" endWordPosition="175">s of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee gen</context>
<context position="21582" citStr="Och, 2003" startWordPosition="3581" endWordPosition="3582">an would be anticipated from its low Bleu score. The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models’ feature functions (Och and Ney, 2002). This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another. For instance, when measuring correlation using Pearson’s we get a very low correlation of R2 = 0.14 when the outlier in Figure 2 is included, but a strong R2 = 0.87 when it is excluded. Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742. Systems which explore different areas of translation space may produce output which has differing characteristics, and</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of ACL, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="967" citStr="Papineni et al., 2002" startWordPosition="142" endWordPosition="145">We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation q</context>
<context position="3214" citStr="Papineni et al., 2002" startWordPosition="496" endWordPosition="500">not all these variations are equally grammatically or semantically plausible there are translations which have the same Bleu score but a worse human evaluation. We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examples of Bleu vastly underestimating the translation quality of systems. Finally, we discuss appropriate uses for Bleu and suggest that for some research projects it may be preferable to use a focused, manual evaluation instead. 2 BLEU Detailed The rationale behind the development of Bleu (Papineni et al., 2002) is that human evaluation of machine translation can be time consuming and expensive. An automatic evaluation metric, on the other hand, can be used for frequent tasks like monitoring incremental system changes during development, which are seemingly infeasible in a manual evaluation setting. The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations. Machine translation evaluation metrics differ from other metrics that use a reference, like the word error rate metric that is used 249 Appeared calm</context>
<context position="4781" citStr="Papineni et al. (2002)" startWordPosition="749" endWordPosition="752">pture allowable variation in word choice through the use of multiple reference translations (as proposed in Thompson (1991)). In order to overcome the problem of variation in phrase order, Bleu uses modified n-gram precision instead of WER’s more strict string edit distance. Bleu’s n-gram precision is modified to eliminate repetitions that occur across sentences. For example, even though the bigram “to Miami” is repeated across all four reference translations in Table 1, it is counted only once in a hypothesis translation. Table 2 shows the n-gram sets created from the reference translations. Papineni et al. (2002) calculate their modified precision score, pn, for each n-gram length by summing over the matches for every hypothesis sentence S in the complete corpus C as: E EngramES Countmatched(ngram) SEC E EngramES Count(ngram) SEC Counting punctuation marks as separate tokens, the hypothesis translation given in Table 1 has 15 unigram matches, 10 bigram matches, 5 trigram matches (these are shown in bold in Table 2), and three 4-gram matches (not shown). The hypothesis translation contains a total of 18 unigrams, 17 bigrams, 16 trigrams, and 15 4-grams. If the complete corpus consisted of this single s</context>
<context position="8770" citStr="Papineni et al. (2002)" startWordPosition="1452" endWordPosition="1455">led to the American plane that was to carry him to Miami in Florida. Orejuela seemed quite calm as he was being led to the American plane that would take him to Miami in Florida. pn = 250 matches one of the reference translations. A score of 1 is also assigned to a hypothesis translation which has matches for all its n-grams (up to the maximum n measured by Bleu) in the clipped reference n-grams, and which has no brevity penalty. The primary reason that Bleu is viewed as a useful stand-in for manual evaluation is that it has been shown to correlate with human judgments of translation quality. Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations. Bleu’s correlation with human judgments has been further tested in the annual NIST Machine Translation Evaluation exercise wherein Bleu’s rankings of Arabic-to-English and Chinese-to-English systems is verified by manual evaluation. In the next section we discuss theoretical reasons why Bleu may not always correlate with human judgments. 3 Variations Allowed By BLEU While Bleu attempts to capture allo</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Eric Brill</author>
</authors>
<title>A unified framework for automatic evaluation using n-gram cooccurrence statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="25343" citStr="Soricut and Brill (2004)" startWordPosition="4190" endWordPosition="4193">F as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Coughlin (2003) performs a large-scale investigation of Bleu’s correlation with human judgments, and finds one example that fails to correlate. Her future work section suggests that she has preliminary evidence that statistical machine translation systems receive a higher Bleu score than thei</context>
</contexts>
<marker>Soricut, Brill, 2004</marker>
<rawString>Radu Soricut and Eric Brill. 2004. A unified framework for automatic evaluation using n-gram cooccurrence statistics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Thompson</author>
</authors>
<title>Automatic evaluation of translation quality: Outline of methodology and report on pilot experiment.</title>
<date>1991</date>
<booktitle>In (ISSCO) Proceedings of the Evaluators Forum,</booktitle>
<pages>215--223</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="4282" citStr="Thompson (1991)" startWordPosition="672" endWordPosition="673">chine translation evaluation metrics differ from other metrics that use a reference, like the word error rate metric that is used 249 Appeared calm when he was taken to the American plane, which will to Miami, Florida. Table 1: A set of four reference translations, and a hypothesis translation from the 2005 NIST MT Evaluation in speech recognition, because translations have a degree of variation in terms of word choice and in terms of variant ordering of some phrases. Bleu attempts to capture allowable variation in word choice through the use of multiple reference translations (as proposed in Thompson (1991)). In order to overcome the problem of variation in phrase order, Bleu uses modified n-gram precision instead of WER’s more strict string edit distance. Bleu’s n-gram precision is modified to eliminate repetitions that occur across sentences. For example, even though the bigram “to Miami” is repeated across all four reference translations in Table 1, it is counted only once in a hypothesis translation. Table 2 shows the n-gram sets created from the reference translations. Papineni et al. (2002) calculate their modified precision score, pn, for each n-gram length by summing over the matches for</context>
</contexts>
<marker>Thompson, 1991</marker>
<rawString>Henry Thompson. 1991. Automatic evaluation of translation quality: Outline of methodology and report on pilot experiment. In (ISSCO) Proceedings of the Evaluators Forum, pages 215–223, Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>