<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043128">
<title confidence="0.9990935">
Brutus: A Semantic Role Labeling System Incorporating CCG, CFG, and
Dependency Features
</title>
<author confidence="0.998996">
Stephen A. Boxwell, Dennis Mehay, and Chris Brew
</author>
<affiliation confidence="0.9946135">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.999257">
{boxwe11,mehay,cbrew}@1ing.ohio-state.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967578947369">
We describe a semantic role labeling system
that makes primary use of CCG-based fea-
tures. Most previously developed systems
are CFG-based and make extensive use of a
treepath feature, which suffers from data spar-
sity due to its use of explicit tree configura-
tions. CCG affords ways to augment treepath-
based features to overcome these data sparsity
issues. By adding features over CCG word-
word dependencies and lexicalized verbal sub-
categorization frames (“supertags”), we can
obtain an F-score that is substantially better
than a previous CCG-based SRL system and
competitive with the current state of the art. A
manual error analysis reveals that parser errors
account for many of the errors of our system.
This analysis also suggests that simultaneous
incremental parsing and semantic role labeling
may lead to performance gains in both tasks.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999505777777778">
Semantic Role Labeling (SRL) is the process of assign-
ing semantic roles to strings of words in a sentence ac-
cording to their relationship to the semantic predicates
expressed in the sentence. The task is difficult because
the relationship between syntactic relations like “sub-
ject” and “object” do not always correspond to seman-
tic relations like “agent” and “patient”. An effective
semantic role labeling system must recognize the dif-
ferences between different configurations:
</bodyText>
<listItem confidence="0.99949225">
(a) [The man]Arg0 opened [the door]Arg1 [for
him]Arg3 [today]ArgM−TMP.
(b) [The door]Arg1 opened.
(c) [The door]Arg1 was opened by [a man]Arg0.
</listItem>
<bodyText confidence="0.999965830188679">
We use Propbank (Palmer et al., 2005), a corpus of
newswire text annotated with verb predicate semantic
role information that is widely used in the SRL litera-
ture (M`arquez et al., 2008). Rather than describe se-
mantic roles in terms of “agent” or “patient”, Propbank
defines semantic roles on a verb-by-verb basis. For ex-
ample, the verb open encodes the OPENER as Arg0, the
OPENEE as Arg1, and the beneficiary of the OPENING
action as Arg3. Propbank also defines a set of adjunct
roles, denoted by the letter M instead of a number. For
example, ArgM-TMP denotes a temporal role, like “to-
day”. By using verb-specific roles, Propbank avoids
specific claims about parallels between the roles of dif-
ferent verbs.
We follow the approach in (Punyakanok et al., 2008)
in framing the SRL problem as a two-stage pipeline:
identification followed by labeling. During identifica-
tion, every word in the sentence is labeled either as
bearing some (as yet undetermined) semantic role or
not . This is done for each verb. Next, during label-
ing, the precise verb-specific roles for each word are
determined. In contrast to the approach in (Punyakanok
et al., 2008), which tags constituents directly, we tag
headwords and then associate them with a constituent,
as in a previous CCG-based approach (Gildea and
Hockenmaier, 2003). Another difference is our choice
of parsers. Brutus uses the CCG parser of (Clark and
Curran, 2007, henceforth the C&amp;C parser), Charniak’s
parser (Charniak, 2001) for additional CFG-based fea-
tures, and MALT parser (Nivre et al., 2007) for de-
pendency features, while (Punyakanok et al., 2008)
use results from an ensemble of parses from Char-
niak’s Parser and a Collins parser (Collins, 2003; Bikel,
2004). Finally, the system described in (Punyakanok et
al., 2008) uses a joint inference model to resolve dis-
crepancies between multiple automatic parses. We do
not employ a similar strategy due to the differing no-
tions of constituency represented in our parsers (CCG
having a much more fluid notion of constituency and
the MALT parser using a different approach entirely).
For the identification and labeling steps, we train
a maximum entropy classifier (Berger et al., 1996)
over sections 02-21 of a version of the CCGbank cor-
pus (Hockenmaier and Steedman, 2007) that has been
augmented by projecting the Propbank semantic anno-
tations (Boxwell and White, 2008). We evaluate our
SRL system’s argument predictions at the word string
level, making our results directly comparable for each
argument labeling.1
In the following, we briefly introduce the CCG
grammatical formalism and motivate its use in SRL
(Sections 2–3). Our main contribution is to demon-
strate that CCG — arguably a more expressive and lin-
</bodyText>
<footnote confidence="0.996921">
1This is guaranteed by our string-to-string mapping from
the original Propbank to the CCGbank.
</footnote>
<page confidence="0.979943">
37
</page>
<note confidence="0.9996135">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37–45,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9995581">
guistically appealing syntactic framework than vanilla
CFGs — is a viable basis for the SRL task. This is sup-
ported by our experimental results, the setup and details
of which we give in Sections 4–10. In particular, using
CCG enables us to map semantic roles directly onto
verbal categories, an innovation of our approach that
leads to performance gains (Section 7). We conclude
with an error analysis (Section 11), which motivates
our discussion of future research for computational se-
mantics with CCG (Section 12).
</bodyText>
<sectionHeader confidence="0.990467" genericHeader="introduction">
2 Combinatory Categorial Grammar
</sectionHeader>
<bodyText confidence="0.999411939393939">
Combinatory Categorial Grammar (Steedman, 2000)
is a grammatical framework that describes syntactic
structure in terms of the combinatory potential of the
lexical (word-level) items. Rather than using standard
part-of-speech tags and grammatical rules, CCG en-
codes much of the combinatory potential of each word
by assigning a syntactically informative category. For
example, the verb loves has the category (s\np)/np,
which could be read “the kind of word that would be
a sentence if it could combine with a noun phrase on
the right and a noun phrase on the left”. Further, CCG
has the advantage of a transparent interface between the
way the words combine and their dependencies with
other words. Word-word dependencies in the CCG-
bank are encoded using predicate-argument (PARG)
relations. PARG relations are defined by the functor
word, the argument word, the category of the functor
word and which argument slot of the functor category
is being filled. For example, in the sentence John loves
Mary (figure 1), there are two slots on the verbal cat-
egory to be filled by NP arguments. The first argu-
ment (the subject) fills slot 1. This can be encoded
as &lt;loves,john,(s\np)/np,1&gt;, indicating the head of
the functor, the head of the argument, the functor cat-
egory and the argument slot. The second argument
(the direct object) fills slot 2. This can be encoded as
&lt;loves,mary,(s\np)/np,2&gt;. One of the potential ad-
vantages to using CCGbank-style PARG relations is
that they uniformly encode both local and long-range
dependencies — e.g., the noun phrase the Mary that
John loves expresses the same set of two dependencies.
We will show this to be a valuable tool for semantic
role prediction.
</bodyText>
<sectionHeader confidence="0.994482" genericHeader="method">
3 Potential Advantages to using CCG
</sectionHeader>
<bodyText confidence="0.999987774193549">
There are many potential advantages to using the CCG
formalism in SRL. One is the uniformity with which
CCG can express equivalence classes of local and long-
range (including unbounded) dependencies. CFG-
based approaches often rely on examining potentially
long sequences of categories (or treepaths) between the
verb and the target word. Because there are a number of
different treepaths that correspond to a single relation
(figure 2), this approach can suffer from data sparsity.
CCG, however, can encode all treepath-distinct expres-
sions of a single grammatical relation into a single
predicate-argument relationship (figure 3). This fea-
ture has been shown (Gildea and Hockenmaier, 2003)
to be an effective substitute for treepath-based features.
But while predicate-argument-based features are very
effective, they are still vulnerable both to parser er-
rors and to cases where the semantics of a sentence
do not correspond directly to syntactic dependencies.
To counteract this, we use both kinds of features with
the expectation that the treepath feature will provide
low-level detail to compensate for missed, incorrect or
syntactically impossible dependencies.
Another advantage of a CCG-based approach (and
lexicalist approaches in general) is the ability to en-
code verb-specific argument mappings. An argument
mapping is a link between the CCG category and the
semantic roles that are likely to go with each of its ar-
guments. The projection of argument mappings onto
CCG verbal categories is explored in (Boxwell and
White, 2008). We describe this feature in more detail
in section 7.
</bodyText>
<sectionHeader confidence="0.995602" genericHeader="method">
4 Identification and Labeling Models
</sectionHeader>
<bodyText confidence="0.998977304347826">
As in previous approaches to SRL, Brutus uses a two-
stage pipeline of maximum entropy classifiers. In ad-
dition, we train an argument mapping classifier (de-
scribed in more detail below) whose predictions are
used as features for the labeling model. The same
features are extracted for both treebank and automatic
parses. Automatic parses were generated using the
C&amp;C CCG parser (Clark and Curran, 2007) with its
derivation output format converted to resemble that of
the CCGbank. This involved following the derivational
bracketings of the C&amp;C parser’s output and recon-
structing the backpointers to the lexical heads using an
in-house implementation of the basic CCG combina-
tory operations. All classifiers were trained to 500 iter-
ations of L-BFGS training — a quasi-Newton method
from the numerical optimization literature (Liu and No-
cedal, 1989) — using Zhang Le’s maxent toolkit.2 To
prevent overfitting we used Gaussian priors with global
variances of 1 and 5 for the identifier and labeler, re-
spectively.3 The Gaussian priors were determined em-
pirically by testing on the development set.
Both the identifier and the labeler use the following
features:
</bodyText>
<listItem confidence="0.9991766">
(1) Words. Words drawn from a 3 word window
around the target word,4 with each word asso-
ciated with a binary indicator feature.
(2) Part of Speech. Part of Speech tags drawn
from a 3 word window around the target word,
</listItem>
<footnote confidence="0.995198875">
2Available for download at http://homepages.
inf.ed.ac.uk/s0450736/maxent_toolkit.
html.
3Gaussian priors achieve a smoothing effect (to prevent
overfitting) by penalizing very large feature weights.
4The size of the window was determined experimentally
on the development set – we use the same window sizes
throughout.
</footnote>
<page confidence="0.997126">
38
</page>
<figure confidence="0.999153666666667">
Robin fixed the car
np (s\np)/np np/n n
np
s\np
s
John loves Mary
&gt;
&gt;
&lt;
np (s[dcl]\np)/np np
s[dcl]\np &gt;
s[dcl] &lt;
</figure>
<figureCaption confidence="0.998645">
Figure 1: This sentence has two depen-
</figureCaption>
<figure confidence="0.859616666666667">
dencies: &lt;loves,mary,(s\np)/np,2&gt; and
&lt;loves,john,(s\np)/np,1&gt;
fixed
</figure>
<figureCaption confidence="0.9879282">
Figure 2: The semantic relation (Arg1) between ‘car’
and ‘fixed’ in both phrases is the same, but the
treepaths — traced with arrows above — are differ-
ent: (V&gt;VP&lt;NP&lt;N and V&gt;VP&gt;S&gt;RC&gt;N&lt;N, re-
spectively).
</figureCaption>
<figure confidence="0.99870625">
the car that Robin fixed
np/n n (np\np)/(s/np) np (s\np)/np
&gt;T
s/(s\np)
&gt; &gt;s
np s/np
&gt;
&lt;
</figure>
<figureCaption confidence="0.97068725">
Figure 3: CCG word-word dependencies are passed
up through subordinate clauses, encoding the rela-
tion between car and fixed the same in both cases:
(s\np)/np.2.→ (Gildea and Hockenmaier, 2003)
</figureCaption>
<bodyText confidence="0.989726">
with each associated with a binary indicator
feature.
</bodyText>
<listItem confidence="0.976377">
(3) CCG Categories. CCG categories drawn from
</listItem>
<bodyText confidence="0.9339255">
a 3 word window around the target word, with
each associated with a binary indicator feature.
</bodyText>
<listItem confidence="0.996847333333333">
(4) Predicate. The lemma of the predicate we are
tagging. E.g. fix is the lemma offixed.
(5) Result Category Detail. The grammatical fea-
</listItem>
<bodyText confidence="0.9692336">
ture on the category of the predicate (indicat-
ing declarative, passive, progressive, etc). This
can be read off the verb category: declarative
for eats: (s[dclJ\np)/np or progressive for run-
ning:s[ngJ\np.
</bodyText>
<listItem confidence="0.998417">
(6) Before/After. A binary indicator variable indi-
cating whether the target word is before or after
the verb.
(7) Treepath. The sequence of CCG categories
</listItem>
<bodyText confidence="0.977341714285714">
representing the path through the derivation
from the predicate to the target word. For
the relationship between fixed and car in the
first sentence of figure 3, the treepath is
(s[dcl]\np)/np&gt;s[dcl]\np&lt;np&lt;n, with &gt; and
&lt; indicating movement up and down the tree,
respectively.
</bodyText>
<listItem confidence="0.968023">
(8) Short Treepath. Similar to the above treepath
</listItem>
<bodyText confidence="0.99480975">
feature, except the path stops at the highest
node under the least common subsumer that
is headed by the target word (this is the con-
stituent that the role would be marked on if we
identified this terminal as a role-bearing word).
Again, for the relationship between fixed and
car in the first sentence of figure 3, the short
treepath is (s[dcl]\np)/np&gt;s[dcl]\np&lt;np.
</bodyText>
<listItem confidence="0.998810333333333">
(9) NP Modified. A binary indicator feature indi-
cating whether the target word is modified by
an NP modifier.5
</listItem>
<footnote confidence="0.978032">
5This is easily read off of the CCG PARG relationships.
</footnote>
<figure confidence="0.998280896551724">
S
�� � ���
VP
� ��
�
NP
Robin V NP
� �
fixed Det N
the
car
that
NP
VP
Robin
V
the
N
car
Rel
^S
NP
�� � ���
Det
RC
N
�� � ���
np\np
np
</figure>
<page confidence="0.986406">
39
</page>
<listItem confidence="0.986017">
(10) Subcategorization. A sequence of the cate-
</listItem>
<bodyText confidence="0.984422428571429">
gories that the verb combines with in the CCG
derivation tree. For the first sentence in fig-
ure 3, the correct subcategorization would be
np,np. Notice that this is not necessarily a re-
statement of the verbal category – in the second
sentence of figure 3, the correct subcategoriza-
tion is s/(s\np),(np\np)/(s[dcl]/np),np.
</bodyText>
<listItem confidence="0.972123">
(11) PARG feature. We follow a previous CCG-
</listItem>
<bodyText confidence="0.994982933333333">
based approach (Gildea and Hockenmaier,
2003) in using a feature to describe the PARG
relationship between the two words, if one ex-
ists. If there is a dependency in the PARG
structure between the two words, then this fea-
ture is defined as the conjunction of (1) the cat-
egory of the functor, (2) the argument slot that
is being filled in the functor category, and (3)
an indication as to whether the functor (→) or
the argument (+—) is the lexical head. For ex-
ample, to indicate the relationship between car
and fixed in both sentences of figure 3, the fea-
ture is (s\np)/np.2.→.
The labeler uses all of the previous features, plus the
following:
</bodyText>
<listItem confidence="0.962071">
(12) Headship. A binary indicator feature as to
</listItem>
<bodyText confidence="0.953057">
whether the functor or the argument is the lex-
ical head of the dependency between the two
words, if one exists.
</bodyText>
<listItem confidence="0.830328">
(13) Predicate and Before/After. The conjunction
</listItem>
<bodyText confidence="0.9310345">
of two earlier features: the predicate lemma
and the Before/After feature.
</bodyText>
<listItem confidence="0.96166">
(14) Rel Clause. Whether the path from predicate
</listItem>
<bodyText confidence="0.945085333333333">
to target word passes through a relative clause
(e.g., marked by the word ‘that’ or any other
word with a relativizer category).
</bodyText>
<listItem confidence="0.957636">
(15) PP features. When the target word is a prepo-
</listItem>
<bodyText confidence="0.974523125">
sition, we define binary indicator features for
the word, POS, and CCG category of the head
of the topmost NP in the prepositional phrase
headed by a preposition (a.k.a. the ‘lexical
head’ of the PP). So, if on heads the phrase ‘on
the third Friday’, then we extract features re-
lating to Friday for the preposition on. This is
null when the target word is not a preposition.
</bodyText>
<listItem confidence="0.989874">
(16) Argument Mappings. If there is a PARG rela-
</listItem>
<bodyText confidence="0.986950789473684">
tion between the predicate and the target word,
the argument mapping is the most likely pre-
dicted role to go with that argument. These
mappings are predicted using a separate classi-
fier that is trained primarily on lexical informa-
tion of the verb, its immediate string-level con-
text, and its observed arguments in the train-
ing data. This feature is null when there is
no PARG relation between the predicate and
the target word. The Argument Mapping fea-
ture can be viewed as a simple prediction about
some of the non-modifier semantic roles that a
verb is likely to express. We use this informa-
tion as a feature and not a hard constraint to
allow other features to overrule the recommen-
dation made by the argument mapping classi-
fier. The features used in the argument map-
ping classifier are described in detail in section
7.
</bodyText>
<sectionHeader confidence="0.995902" genericHeader="method">
5 CFG based Features
</sectionHeader>
<bodyText confidence="0.999555391304348">
In addition to CCG-based features, features can be
drawn from a traditional CFG-style approach when
they are available. Our motivation for this is twofold.
First, others (Punyakanok et al., 2008, e.g.), have found
that different parsers have different error patterns, and
so using multiple parsers can yield complementary
sources of correct information. Second, we noticed
that, although the CCG-based system performed well
on head word labeling, performance dropped when
projecting these labels to the constituent level (see sec-
tions 8 and 9 for more). This may have to do with the
fact that CCG is not centered around a constituency-
based analysis, as well as with inconsistencies between
CCG and Penn Treebank-style bracketings (the latter
being what was annotated in the original Propbank).
Penn Treebank-derived features are used in the iden-
tifier, labeler, and argument mapping classifiers. For
automatic parses, we use Charniak’s parser (Charniak,
2001). For gold-standard parses, we remove func-
tional tag and trace information from the Penn Tree-
bank parses before we extract features over them, so as
to simulate the conditions of an automatic parse. The
Penn Treebank features are as follows:
</bodyText>
<listItem confidence="0.9969571">
(17) CFG Treepath. A sequence of traditional
CFG-style categories representing the path
from the verb to the target word.
(18) CFG Short Treepath. Analogous to the CCG-
based short treepath feature.
(19) CFG Subcategorization. Analogous to the
CCG-based subcategorization feature.
(20) CFG Least Common Subsumer. The cate-
gory of the root of the smallest tree that domi-
nates both the verb and the target word.
</listItem>
<sectionHeader confidence="0.995586" genericHeader="method">
6 Dependency Parser Features
</sectionHeader>
<bodyText confidence="0.999940555555556">
Finally, several features can be extracted from a de-
pendency representation of the same sentence. Au-
tomatic dependency relations were produced by the
MALT parser. We incorporate MALT into our col-
lection of parses because it provides detailed informa-
tion on the exact syntactic relations between word pairs
(subject, object, adverb, etc) that is not found in other
automatic parsers. The features used from the depen-
dency parses are listed below:
</bodyText>
<page confidence="0.992576">
40
</page>
<listItem confidence="0.986290333333333">
(21) DEP-Exists A binary indicator feature show-
ing whether or not there is a dependency be-
tween the target word and the predicate.
(22) DEP-Type If there is a dependency between
the target word and the predicate, what type of
dependency it is (SUBJ, OBJ, etc).
</listItem>
<sectionHeader confidence="0.992634" genericHeader="method">
7 Argument Mapping Model
</sectionHeader>
<bodyText confidence="0.999088588235294">
An innovation in our approach is to use a separate clas-
sifier to predict an argument mapping feature. An ar-
gument mapping is a mapping from the syntactic argu-
ments of a verbal category to the semantic arguments
that should correspond to them (Boxwell and White,
2008). In order to generate examples of the argument
mapping for training purposes, it is necessary to em-
ploy the PARG relations for a given sentence to identify
the headwords of each of the verbal arguments. That is,
we use the PARG relations to identify the headwords of
each of the constituents that are arguments of the verb.
Next, the appropriate semantic role that corresponds to
that headword (given by Propbank) is identified. This
is done by climbing the CCG derivation tree towards
the root until we find a semantic role corresponding to
the verb in question — i.e., by finding the point where
the constituent headed by the verbal category combines
with the constituent headed by the argument in ques-
tion. These semantic roles are then marked on the cor-
responding syntactic argument of the verb.
As an example, consider the sentence The boy loves
a girl. (figure 4). By examining the arguments that the
verbal category combines with in the treebank, we can
identify the corresponding semantic role for each argu-
ment that is marked on the verbal category. We then use
these tags to train the Argument Mapping model, which
will predict likely argument mappings for verbal cate-
gories based on their local surroundings and the head-
words of their arguments, similar to the supertagging
approaches used to label the informative syntactic cat-
egories of the verbs (Bangalore and Joshi, 1999; Clark,
2002), except tagging “one level above” the syntax.
The Argument Mapping Predictor uses the following
features:
</bodyText>
<listItem confidence="0.9980903">
(23) Predicate. The lemma of the predicate, as be-
fore.
(24) Words. Words drawn from a 5 word window
around the target word, with each word associ-
ated with a binary indicator feature, as before.
(25) Parts of Speech. Part of Speech tags drawn
from a 5 word window around the target word,
with each tag associated with a binary indicator
feature, as before.
(26) CCG Categories. CCG categories drawn from
</listItem>
<bodyText confidence="0.999012">
a 5 word window around the target word, with
each category associated with a binary indica-
tor feature, as before.
</bodyText>
<figure confidence="0.994924166666667">
the boy loves a girl
np/n n (s[dcl]\npArg0)/npArg1 np/n n
� �
np − Arg0 np − Arg1
�
s[dcl]
</figure>
<figureCaption confidence="0.96327125">
Figure 4: By looking at the constituents that the verb
combines with, we can identify the semantic roles cor-
responding to the arguments marked on the verbal cat-
egory.
</figureCaption>
<listItem confidence="0.995812933333333">
(27) Argument Data. The word, POS, and CCG
category, and treepath of the headwords of each
of the verbal arguments (i.e., PARG depen-
dents), each encoded as a separate binary in-
dicator feature.
(28) Number of arguments. The number of argu-
ments marked on the verb.
(29) Words of Arguments. The head words of each
of the verb’s arguments.
(30) Subcategorization. The CCG categories that
combine with this verb. This includes syntactic
adjuncts as well as arguments.
(31) CFG-Sisters. The POS categories of the sis-
ters of this predicate in the CFG representation.
(32) DEP-dependencies. The individual depen-
</listItem>
<bodyText confidence="0.998321">
dency types of each of the dependencies re-
lating to the verb (SBJ, OBJ, ADV, etc) taken
from the dependency parse. We also incorpo-
rate a single feature representing the entire set
of dependency types associated with this verb
into a single feature, representing the set of de-
pendencies as a whole.
Given these features with gold standard parses, our
argument mapping model can predict entire argument
mappings with an accuracy rate of 87.96% on the test
set, and 87.70% on the development set. We found the
features generated by this model to be very useful for
semantic role prediction, as they enable us to make de-
cisions about entire sets of semantic roles associated
with individual lemmas, rather than choosing them in-
dependently of each other.
</bodyText>
<sectionHeader confidence="0.987717" genericHeader="method">
8 Enabling Cross-System Comparison
</sectionHeader>
<bodyText confidence="0.9981925">
The Brutus system is designed to label headwords of
semantic roles, rather than entire constituents. How-
ever, because most SRL systems are designed to label
constituents rather than headwords, it is necessary to
project the roles up the derivation to the correct con-
stituent in order to make a meaningful comparison of
the system’s performance. This introduces the poten-
tial for further error, so we report results on the ac-
curacy of headwords as well as the correct string of
words. We deterministically move the role to the high-
est constituent in the derivation that is headed by the
s[dcl]\np
</bodyText>
<page confidence="0.998527">
41
</page>
<table confidence="0.954576">
�
�
np − speak.Arg0
s
P R F
P. et al (treebank) 86.22% 87.40% 86.81%
Brutus (treebank) 88.29% 86.39% 87.33%
P. et al (automatic) 77.09% 75.51% 76.29%
Brutus (automatic) 76.73% 70.45% 73.45%
</table>
<figure confidence="0.9922665">
a man with glasses spoke
np/n n (np\np)/np np s\np
&gt; &gt;
np np\np
</figure>
<figureCaption confidence="0.992335">
Figure 5: The role is moved towards the root until the
original node is no longer the head of the marked con-
stituent.
</figureCaption>
<table confidence="0.9999304">
P R F
G&amp;H (treebank) 67.5% 60.0% 63.5%
Brutus (treebank) 88.18% 85.00% 86.56%
G&amp;H (automatic) 55.7% 49.5% 52.4%
Brutus (automatic) 76.06% 70.15% 72.99%
</table>
<tableCaption confidence="0.9950245">
Table 1: Accuracy of semantic role prediction using
only CCG based features.
</tableCaption>
<bodyText confidence="0.998554142857143">
originally tagged terminal. In most cases, this corre-
sponds to the node immediately dominated by the low-
est common subsuming node of the the target word and
the verb (figure 5). In some cases, the highest con-
stituent that is headed by the target word is not imme-
diately dominated by the lowest common subsuming
node (figure 6).
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="method">
9 Results
</sectionHeader>
<bodyText confidence="0.997670714285714">
Using a version of Brutus incorporating only the CCG-
based features described above, we achieve better re-
sults than a previous CCG based system (Gildea and
Hockenmaier, 2003, henceforth G&amp;H). This could be
due to a number of factors, including the fact that our
system employs a different CCG parser, uses a more
complete mapping of the Propbank onto the CCGbank,
uses a different machine learning approach,6 and has a
richer feature set. The results for constituent tagging
accuracy are shown in table 1.
As expected, by incorporating Penn Treebank-based
features and dependency features, we obtain better re-
sults than with the CCG-only system. The results for
gold standard parses are comparable to the winning
system of the CoNLL 2005 shared task on semantic
role labeling (Punyakanok et al., 2008). Other systems
(Toutanova et al., 2008; Surdeanu et al., 2007; Johans-
son and Nugues, 2008) have also achieved comparable
results – we compare our system to (Punyakanok et
al., 2008) due to the similarities in our approaches. The
performance of the full system is shown in table 2.
Table 3 shows the ability of the system to predict
the correct headwords of semantic roles. This is a nec-
essary condition for correctness of the full constituent,
but not a sufficient one. In parser evaluation, Carroll,
Minnen, and Briscoe (Carroll et al., 2003) have argued
6G&amp;H use a generative model with a back-off lattice,
whereas we use a maximum entropy classifier.
</bodyText>
<tableCaption confidence="0.9961865">
Table 2: Accuracy of semantic role prediction using
CCG, CFG, and MALT based features.
</tableCaption>
<table confidence="0.9999784">
P R F
Headword (treebank) 88.94% 86.98% 87.95%
Boundary (treebank) 88.29% 86.39% 87.33%
Headword (automatic) 82.36% 75.97% 79.04%
Boundary (automatic) 76.33% 70.59% 73.35%
</table>
<tableCaption confidence="0.9321252">
Table 3: Accuracy of the system for labeling semantic
roles on both constituent boundaries and headwords.
Headwords are easier to predict than boundaries, re-
flecting CCG’s focus on word-word relations rather
than constituency.
</tableCaption>
<bodyText confidence="0.999915">
for dependencies as a more appropriate means of eval-
uation, reflecting the focus on headwords from con-
stituent boundaries. We argue that, especially in the
heavily lexicalized CCG framework, headword evalu-
ation is more appropriate, reflecting the emphasis on
headword combinatorics in the CCG formalism.
</bodyText>
<sectionHeader confidence="0.956645" genericHeader="method">
10 The Contribution of the New Features
</sectionHeader>
<bodyText confidence="0.9995849">
Two features which are less frequently used in SRL
research play a major role in the Brutus system: The
PARG feature (Gildea and Hockenmaier, 2003) and
the argument mapping feature. Removing them has
a strong effect on accuracy when labeling treebank
parses, as shown in our feature ablation results in ta-
ble 4. We do not report results including the Argu-
ment Mapping feature but not the PARG feature, be-
cause some predicate-argument relation information is
assumed in generating the Argument Mapping feature.
</bodyText>
<table confidence="0.9995365">
P R F
+PARG +AM 88.77% 86.15% 87.44%
+PARG -AM 88.42% 85.78% 87.08%
-PARG -AM 87.92% 84.65% 86.26%
</table>
<tableCaption confidence="0.990924">
Table 4: The effects of removing key features from the
system on gold standard parses.
</tableCaption>
<bodyText confidence="0.982428">
The same is true for automatic parses, as shown in ta-
ble 5.
</bodyText>
<sectionHeader confidence="0.970137" genericHeader="method">
11 Error Analysis
</sectionHeader>
<bodyText confidence="0.999740333333333">
Many of the errors made by the Brutus system can be
traced directly to erroneous parses, either in the auto-
matic or treebank parse. In some cases, PP attachment
</bodyText>
<page confidence="0.994685">
42
</page>
<figure confidence="0.9930385">
with even brief exposures causing symptoms
(((vp\vp)/vp[ng])/np n/n n/n n (s[ng]\np)/np np
� �
n s[ng]\np
�
n
np − cause.Arg0
�
(vp\vp)/vp[ng] �
vp\vp
</figure>
<figureCaption confidence="0.9933215">
Figure 6: In this case, with is the head of with even brief exposures, so the role is correctly marked on even brief
exposures (based on wsj 0003.2).
</figureCaption>
<table confidence="0.962250083333333">
P R F
+PARG +AM 74.14% 62.09% 67.58%
+PARG -AM 70.02% 64.68% 67.25%
-PARG -AM 73.90% 61.15% 66.93%
a form of asbestos used to make filters
np (np\np)/np np np\np
�
�
np − Arg1
np\np
�
np
</table>
<tableCaption confidence="0.984443">
Table 5: The effects of removing key features from the
system on automatic parses.
</tableCaption>
<bodyText confidence="0.999342135135135">
ambiguities cause a role to be marked too high in the
derivation. In the sentence the company stopped using
asbestos in 1956 (figure 7), the correct Arg1 of stopped
is using asbestos. However, because in 1956 is erro-
neously modifying the verb using rather than the verb
stopped in the treebank parse, the system trusts the syn-
tactic analysis and places Arg1 of stopped on using as-
bestos in 1956. This particular problem is caused by an
annotation error in the original Penn Treebank that was
carried through in the conversion to CCGbank.
Another common error deals with genitive construc-
tions. Consider the phrase a form of asbestos used
to make filters. By CCG combinatorics, the relative
clause could either attach to asbestos or to a form of
asbestos. The gold standard CCG parse attaches the
relative clause to a form of asbestos (figure 8). Prop-
bank agrees with this analysis, assigning Arg1 of use
to the constituent a form of asbestos. The automatic
parser, however, attaches the relative clause low – to
asbestos (figure 9). When the system is given the au-
tomatically generated parse, it incorrectly assigns the
semantic role to asbestos. In cases where the parser at-
taches the relative clause correctly, the system is much
more likely to assign the role correctly.
Problems with relative clause attachment to genitives
are not limited to automatic parses – errors in gold-
standard treebank parses cause similar problems when
Treebank parses disagree with Propbank annotator in-
tuitions. In the phrase a group of workers exposed to
asbestos (figure 10), the gold standard CCG parse at-
taches the relative clause to workers. Propbank, how-
ever, annotates a group of workers as Arg1 of exposed,
rather than following the parse and assigning the role
only to workers. The system again follows the parse
and incorrectly assigns the role to workers instead of a
group of workers. Interestingly, the C&amp;C parser opts
for high attachment in this instance, resulting in the
</bodyText>
<figureCaption confidence="0.997958">
Figure 8: CCGbank gold-standard parse of a relative
clause attachment. The system correctly identifies a
form of asbestos as Arg1 of used. (wsj 0003.1)
</figureCaption>
<figure confidence="0.997376">
a form of asbestos used to make filters
np (np\np)/np np − Arg1 np\np
�
�
np\np
�
</figure>
<figureCaption confidence="0.97992075">
Figure 9: Automatic parse of the noun phrase in fig-
ure 8. Incorrect relative clause attachment causes the
misidentification of asbestos as a semantic role bearing
unit. (wsj 0003.1)
</figureCaption>
<bodyText confidence="0.980606">
correct prediction of a group of workers as Arg1 of ex-
posed in the automatic parse.
</bodyText>
<sectionHeader confidence="0.991043" genericHeader="discussions">
12 Future Work
</sectionHeader>
<bodyText confidence="0.999993466666667">
As described in the error analysis section, a large num-
ber of errors in the system are attributable to errors in
the CCG derivation, either in the gold standard or in
automatically generated parses. Potential future work
may focus on developing an improved CCG parser us-
ing the revised (syntactic) adjunct-argument distinc-
tions (guided by the Propbank annotation) described in
(Boxwell and White, 2008). This resource, together
with the reasonable accuracy (,: 90%) with which ar-
gument mappings can be predicted, suggests the possi-
bility of an integrated, simultaneous syntactic-semantic
parsing process, similar to that of (Musillo and Merlo,
2006; Merlo and Musillo, 2008). We expect this would
improve the reliability and accuracy of both the syntac-
tic and semantic analysis components.
</bodyText>
<sectionHeader confidence="0.985228" genericHeader="acknowledgments">
13 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99924">
This research was funded by NSF grant IIS-0347799.
We are deeply indebted to Julia Hockenmaier for the
</bodyText>
<figure confidence="0.9714915">
np
np
</figure>
<page confidence="0.924342">
43
</page>
<figure confidence="0.99791025">
the company stopped using asbestos in 1956
np ((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)/np np (s\np)\(s\np)
�
s[ng]\np − stop.Arg1
�
�
s[dcl]\np
s[dcl]
</figure>
<figureCaption confidence="0.993408666666667">
Figure 7: An example of how incorrect PP attachment can cause an incorrect labeling. Stop.Arg1 should cover us-
ing asbestos rather than using asbestos in 1956. This sentence is based on wsj 0003.3, with the structure simplified
for clarity.
</figureCaption>
<figure confidence="0.999344">
a group of workers exposed to asbestos
np (np\np)/np np − exposed.Arg1 np\np
�
�
�
np
np\np
np
</figure>
<figureCaption confidence="0.9978205">
Figure 10: Propbank annotates a group of workers as Arg1 of exposed, while CCGbank attaches the relative clause
low. The system incorrectly labels workers as a role bearing unit. (Gold standard – wsj 0003.1)
</figureCaption>
<bodyText confidence="0.931147">
s[ng]\np
�
use of her PARG generation tool.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795375">
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237–265.
Adam L. Berger, S. Della Pietra, and V. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
D.M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Stephen A. Boxwell and Michael White. 2008.
Projecting propbank roles onto the ccgbank. In
Proceedings of the Sixth International Language
Resources and Evaluation Conference (LREC-08),
Marrakech, Morocco.
J. Carroll, G. Minnen, and T. Briscoe. 2003. Parser
evaluation. Treebanks: Building and Using Parsed
Corpora, pages 299–316.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proc. ACL-01, volume 39, pages
116–123.
Stephen Clark and James R. Curran. 2007. Wide-
coverage Efficient Statistical Parsing with CCG and
Log-linear Models. Computational Linguistics,
33(4):493–552.
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+6), pages 19–24,
Venice, Italy.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589–637.
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using Combinatory Categorial
Grammar. In Proc. EMNLP-03.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396.
R. Johansson and P. Nugues. 2008. Dependency-
based syntactic–semantic analysis with PropBank
and NomBank. Proceedings of CoNLL–2008.
D C Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Math-
ematical Programming B, 45(3).
Lluis M`arquez, Xavier Carreras, Kenneth C. Litowski,
and Suzanne Stevenson. 2008. Semantic Role La-
beling: An Introduction to the Special Issue. Com-
putational Linguistics, 34(2):145–159.
Paola Merlo and Gabrile Musillo. 2008. Semantic
parsing for high-precision semantic role labelling. In
Proceedings of CONLL-08, Manchester, UK.
Gabriele Musillo and Paola Merlo. 2006. Robust pars-
ing of the proposition bank. In Proceedings of the
EACL 2006 Workshop ROMAND, Trento.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95–135.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.
</reference>
<page confidence="0.979552">
44
</page>
<reference confidence="0.999498846153846">
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257–287.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
M. Surdeanu, L. M`arquez, X. Carreras, and P. Comas.
2007. Combination strategies for semantic role la-
beling. Journal of Artificial Intelligence Research,
29:105–151.
K. Toutanova, A. Haghighi, and C.D. Manning. 2008.
A global joint model for semantic role labeling.
Computational Linguistics, 34(2):161–191.
</reference>
<page confidence="0.999392">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983256">
<title confidence="0.9960135">Brutus: A Semantic Role Labeling System Incorporating CCG, CFG, and Dependency Features</title>
<author confidence="0.99984">Stephen A Boxwell</author>
<author confidence="0.99984">Dennis Mehay</author>
<author confidence="0.99984">Chris Brew</author>
<affiliation confidence="0.9974565">Department of Linguistics The Ohio State University</affiliation>
<abstract confidence="0.99980915">We describe a semantic role labeling system that makes primary use of CCG-based features. Most previously developed systems are CFG-based and make extensive use of a treepath feature, which suffers from data sparsity due to its use of explicit tree configurations. CCG affords ways to augment treepathbased features to overcome these data sparsity issues. By adding features over CCG wordword dependencies and lexicalized verbal subcategorization frames (“supertags”), we can obtain an F-score that is substantially better than a previous CCG-based SRL system and competitive with the current state of the art. A manual error analysis reveals that parser errors account for many of the errors of our system. This analysis also suggests that simultaneous incremental parsing and semantic role labeling may lead to performance gains in both tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="19552" citStr="Bangalore and Joshi, 1999" startWordPosition="3187" endWordPosition="3190">onding syntactic argument of the verb. As an example, consider the sentence The boy loves a girl. (figure 4). By examining the arguments that the verbal category combines with in the treebank, we can identify the corresponding semantic role for each argument that is marked on the verbal category. We then use these tags to train the Argument Mapping model, which will predict likely argument mappings for verbal categories based on their local surroundings and the headwords of their arguments, similar to the supertagging approaches used to label the informative syntactic categories of the verbs (Bangalore and Joshi, 1999; Clark, 2002), except tagging “one level above” the syntax. The Argument Mapping Predictor uses the following features: (23) Predicate. The lemma of the predicate, as before. (24) Words. Words drawn from a 5 word window around the target word, with each word associated with a binary indicator feature, as before. (25) Parts of Speech. Part of Speech tags drawn from a 5 word window around the target word, with each tag associated with a binary indicator feature, as before. (26) CCG Categories. CCG categories drawn from a 5 word window around the target word, with each category associated with a</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3910" citStr="Berger et al., 1996" startWordPosition="612" endWordPosition="615">ures, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We evaluate our SRL system’s argument predictions at the word string level, making our results directly comparable for each argument labeling.1 In the following, we briefly introduce the CCG grammatical formalism and motivate its use in SRL (Sections 2–3). Our main contribution is to demonstrate that CCG — arguably a more expressive and lin1This is guaranteed by our string-to-string mapping from the original </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3439" citStr="Bikel, 2004" startWordPosition="539" endWordPosition="540">to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&amp;C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting th</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D.M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen A Boxwell</author>
<author>Michael White</author>
</authors>
<title>Projecting propbank roles onto the ccgbank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation Conference (LREC-08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="4096" citStr="Boxwell and White, 2008" startWordPosition="642" endWordPosition="645">n (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We evaluate our SRL system’s argument predictions at the word string level, making our results directly comparable for each argument labeling.1 In the following, we briefly introduce the CCG grammatical formalism and motivate its use in SRL (Sections 2–3). Our main contribution is to demonstrate that CCG — arguably a more expressive and lin1This is guaranteed by our string-to-string mapping from the original Propbank to the CCGbank. 37 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37–45, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP guist</context>
<context position="8509" citStr="Boxwell and White, 2008" startWordPosition="1344" endWordPosition="1347">t correspond directly to syntactic dependencies. To counteract this, we use both kinds of features with the expectation that the treepath feature will provide low-level detail to compensate for missed, incorrect or syntactically impossible dependencies. Another advantage of a CCG-based approach (and lexicalist approaches in general) is the ability to encode verb-specific argument mappings. An argument mapping is a link between the CCG category and the semantic roles that are likely to go with each of its arguments. The projection of argument mappings onto CCG verbal categories is explored in (Boxwell and White, 2008). We describe this feature in more detail in section 7. 4 Identification and Labeling Models As in previous approaches to SRL, Brutus uses a twostage pipeline of maximum entropy classifiers. In addition, we train an argument mapping classifier (described in more detail below) whose predictions are used as features for the labeling model. The same features are extracted for both treebank and automatic parses. Automatic parses were generated using the C&amp;C CCG parser (Clark and Curran, 2007) with its derivation output format converted to resemble that of the CCGbank. This involved following the d</context>
<context position="18165" citStr="Boxwell and White, 2008" startWordPosition="2954" endWordPosition="2957">ers. The features used from the dependency parses are listed below: 40 (21) DEP-Exists A binary indicator feature showing whether or not there is a dependency between the target word and the predicate. (22) DEP-Type If there is a dependency between the target word and the predicate, what type of dependency it is (SUBJ, OBJ, etc). 7 Argument Mapping Model An innovation in our approach is to use a separate classifier to predict an argument mapping feature. An argument mapping is a mapping from the syntactic arguments of a verbal category to the semantic arguments that should correspond to them (Boxwell and White, 2008). In order to generate examples of the argument mapping for training purposes, it is necessary to employ the PARG relations for a given sentence to identify the headwords of each of the verbal arguments. That is, we use the PARG relations to identify the headwords of each of the constituents that are arguments of the verb. Next, the appropriate semantic role that corresponds to that headword (given by Propbank) is identified. This is done by climbing the CCG derivation tree towards the root until we find a semantic role corresponding to the verb in question — i.e., by finding the point where t</context>
<context position="30054" citStr="Boxwell and White, 2008" startWordPosition="4951" endWordPosition="4954">ncorrect relative clause attachment causes the misidentification of asbestos as a semantic role bearing unit. (wsj 0003.1) correct prediction of a group of workers as Arg1 of exposed in the automatic parse. 12 Future Work As described in the error analysis section, a large number of errors in the system are attributable to errors in the CCG derivation, either in the gold standard or in automatically generated parses. Potential future work may focus on developing an improved CCG parser using the revised (syntactic) adjunct-argument distinctions (guided by the Propbank annotation) described in (Boxwell and White, 2008). This resource, together with the reasonable accuracy (,: 90%) with which argument mappings can be predicted, suggests the possibility of an integrated, simultaneous syntactic-semantic parsing process, similar to that of (Musillo and Merlo, 2006; Merlo and Musillo, 2008). We expect this would improve the reliability and accuracy of both the syntactic and semantic analysis components. 13 Acknowledgments This research was funded by NSF grant IIS-0347799. We are deeply indebted to Julia Hockenmaier for the np np 43 the company stopped using asbestos in 1956 np ((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)</context>
</contexts>
<marker>Boxwell, White, 2008</marker>
<rawString>Stephen A. Boxwell and Michael White. 2008. Projecting propbank roles onto the ccgbank. In Proceedings of the Sixth International Language Resources and Evaluation Conference (LREC-08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>T Briscoe</author>
</authors>
<title>Parser evaluation. Treebanks: Building and Using Parsed Corpora,</title>
<date>2003</date>
<pages>299--316</pages>
<contexts>
<context position="24718" citStr="Carroll et al., 2003" startWordPosition="4061" endWordPosition="4064">he CoNLL 2005 shared task on semantic role labeling (Punyakanok et al., 2008). Other systems (Toutanova et al., 2008; Surdeanu et al., 2007; Johansson and Nugues, 2008) have also achieved comparable results – we compare our system to (Punyakanok et al., 2008) due to the similarities in our approaches. The performance of the full system is shown in table 2. Table 3 shows the ability of the system to predict the correct headwords of semantic roles. This is a necessary condition for correctness of the full constituent, but not a sufficient one. In parser evaluation, Carroll, Minnen, and Briscoe (Carroll et al., 2003) have argued 6G&amp;H use a generative model with a back-off lattice, whereas we use a maximum entropy classifier. Table 2: Accuracy of semantic role prediction using CCG, CFG, and MALT based features. P R F Headword (treebank) 88.94% 86.98% 87.95% Boundary (treebank) 88.29% 86.39% 87.33% Headword (automatic) 82.36% 75.97% 79.04% Boundary (automatic) 76.33% 70.59% 73.35% Table 3: Accuracy of the system for labeling semantic roles on both constituent boundaries and headwords. Headwords are easier to predict than boundaries, reflecting CCG’s focus on word-word relations rather than constituency. for</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 2003</marker>
<rawString>J. Carroll, G. Minnen, and T. Briscoe. 2003. Parser evaluation. Treebanks: Building and Using Parsed Corpora, pages 299–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proc. ACL-01,</booktitle>
<volume>39</volume>
<pages>116--123</pages>
<contexts>
<context position="3198" citStr="Charniak, 2001" startWordPosition="499" endWordPosition="500">ntification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&amp;C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach e</context>
<context position="16485" citStr="Charniak, 2001" startWordPosition="2678" endWordPosition="2679">ormation. Second, we noticed that, although the CCG-based system performed well on head word labeling, performance dropped when projecting these labels to the constituent level (see sections 8 and 9 for more). This may have to do with the fact that CCG is not centered around a constituencybased analysis, as well as with inconsistencies between CCG and Penn Treebank-style bracketings (the latter being what was annotated in the original Propbank). Penn Treebank-derived features are used in the identifier, labeler, and argument mapping classifiers. For automatic parses, we use Charniak’s parser (Charniak, 2001). For gold-standard parses, we remove functional tag and trace information from the Penn Treebank parses before we extract features over them, so as to simulate the conditions of an automatic parse. The Penn Treebank features are as follows: (17) CFG Treepath. A sequence of traditional CFG-style categories representing the path from the verb to the target word. (18) CFG Short Treepath. Analogous to the CCGbased short treepath feature. (19) CFG Subcategorization. Analogous to the CCG-based subcategorization feature. (20) CFG Least Common Subsumer. The category of the root of the smallest tree t</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In Proc. ACL-01, volume 39, pages 116–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage Efficient Statistical Parsing with CCG and Log-linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="3134" citStr="Clark and Curran, 2007" startWordPosition="489" endWordPosition="492">s a two-stage pipeline: identification followed by labeling. During identification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&amp;C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion </context>
<context position="9002" citStr="Clark and Curran, 2007" startWordPosition="1424" endWordPosition="1427">ith each of its arguments. The projection of argument mappings onto CCG verbal categories is explored in (Boxwell and White, 2008). We describe this feature in more detail in section 7. 4 Identification and Labeling Models As in previous approaches to SRL, Brutus uses a twostage pipeline of maximum entropy classifiers. In addition, we train an argument mapping classifier (described in more detail below) whose predictions are used as features for the labeling model. The same features are extracted for both treebank and automatic parses. Automatic parses were generated using the C&amp;C CCG parser (Clark and Curran, 2007) with its derivation output format converted to resemble that of the CCGbank. This involved following the derivational bracketings of the C&amp;C parser’s output and reconstructing the backpointers to the lexical heads using an in-house implementation of the basic CCG combinatory operations. All classifiers were trained to 500 iterations of L-BFGS training — a quasi-Newton method from the numerical optimization literature (Liu and Nocedal, 1989) — using Zhang Le’s maxent toolkit.2 To prevent overfitting we used Gaussian priors with global variances of 1 and 5 for the identifier and labeler, respec</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage Efficient Statistical Parsing with CCG and Log-linear Models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Supertagging for combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6),</booktitle>
<pages>pages</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="19566" citStr="Clark, 2002" startWordPosition="3191" endWordPosition="3192">f the verb. As an example, consider the sentence The boy loves a girl. (figure 4). By examining the arguments that the verbal category combines with in the treebank, we can identify the corresponding semantic role for each argument that is marked on the verbal category. We then use these tags to train the Argument Mapping model, which will predict likely argument mappings for verbal categories based on their local surroundings and the headwords of their arguments, similar to the supertagging approaches used to label the informative syntactic categories of the verbs (Bangalore and Joshi, 1999; Clark, 2002), except tagging “one level above” the syntax. The Argument Mapping Predictor uses the following features: (23) Predicate. The lemma of the predicate, as before. (24) Words. Words drawn from a 5 word window around the target word, with each word associated with a binary indicator feature, as before. (25) Parts of Speech. Part of Speech tags drawn from a 5 word window around the target word, with each tag associated with a binary indicator feature, as before. (26) CCG Categories. CCG categories drawn from a 5 word window around the target word, with each category associated with a binary indica</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Stephen Clark. 2002. Supertagging for combinatory categorial grammar. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6), pages 19–24, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="3425" citStr="Collins, 2003" startWordPosition="537" endWordPosition="538">d. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&amp;C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Identifying semantic roles using Combinatory Categorial Grammar. In</title>
<date>2003</date>
<booktitle>Proc. EMNLP-03.</booktitle>
<contexts>
<context position="3034" citStr="Gildea and Hockenmaier, 2003" startWordPosition="472" endWordPosition="475"> roles of different verbs. We follow the approach in (Punyakanok et al., 2008) in framing the SRL problem as a two-stage pipeline: identification followed by labeling. During identification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Brutus uses the CCG parser of (Clark and Curran, 2007, henceforth the C&amp;C parser), Charniak’s parser (Charniak, 2001) for additional CFG-based features, and MALT parser (Nivre et al., 2007) for dependency features, while (Punyakanok et al., 2008) use results from an ensemble of parses from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to th</context>
<context position="7661" citStr="Gildea and Hockenmaier, 2003" startWordPosition="1214" endWordPosition="1217">sm in SRL. One is the uniformity with which CCG can express equivalence classes of local and longrange (including unbounded) dependencies. CFGbased approaches often rely on examining potentially long sequences of categories (or treepaths) between the verb and the target word. Because there are a number of different treepaths that correspond to a single relation (figure 2), this approach can suffer from data sparsity. CCG, however, can encode all treepath-distinct expressions of a single grammatical relation into a single predicate-argument relationship (figure 3). This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for treepath-based features. But while predicate-argument-based features are very effective, they are still vulnerable both to parser errors and to cases where the semantics of a sentence do not correspond directly to syntactic dependencies. To counteract this, we use both kinds of features with the expectation that the treepath feature will provide low-level detail to compensate for missed, incorrect or syntactically impossible dependencies. Another advantage of a CCG-based approach (and lexicalist approaches in general) is the ability to encode verb-specific ar</context>
<context position="11002" citStr="Gildea and Hockenmaier, 2003" startWordPosition="1742" endWordPosition="1745">p (s[dcl]\np)/np np s[dcl]\np &gt; s[dcl] &lt; Figure 1: This sentence has two dependencies: &lt;loves,mary,(s\np)/np,2&gt; and &lt;loves,john,(s\np)/np,1&gt; fixed Figure 2: The semantic relation (Arg1) between ‘car’ and ‘fixed’ in both phrases is the same, but the treepaths — traced with arrows above — are different: (V&gt;VP&lt;NP&lt;N and V&gt;VP&gt;S&gt;RC&gt;N&lt;N, respectively). the car that Robin fixed np/n n (np\np)/(s/np) np (s\np)/np &gt;T s/(s\np) &gt; &gt;s np s/np &gt; &lt; Figure 3: CCG word-word dependencies are passed up through subordinate clauses, encoding the relation between car and fixed the same in both cases: (s\np)/np.2.→ (Gildea and Hockenmaier, 2003) with each associated with a binary indicator feature. (3) CCG Categories. CCG categories drawn from a 3 word window around the target word, with each associated with a binary indicator feature. (4) Predicate. The lemma of the predicate we are tagging. E.g. fix is the lemma offixed. (5) Result Category Detail. The grammatical feature on the category of the predicate (indicating declarative, passive, progressive, etc). This can be read off the verb category: declarative for eats: (s[dclJ\np)/np or progressive for running:s[ngJ\np. (6) Before/After. A binary indicator variable indicating whether</context>
<context position="13148" citStr="Gildea and Hockenmaier, 2003" startWordPosition="2108" endWordPosition="2111"> off of the CCG PARG relationships. S �� � ��� VP � �� � NP Robin V NP � � fixed Det N the car that NP VP Robin V the N car Rel ^S NP �� � ��� Det RC N �� � ��� np\np np 39 (10) Subcategorization. A sequence of the categories that the verb combines with in the CCG derivation tree. For the first sentence in figure 3, the correct subcategorization would be np,np. Notice that this is not necessarily a restatement of the verbal category – in the second sentence of figure 3, the correct subcategorization is s/(s\np),(np\np)/(s[dcl]/np),np. (11) PARG feature. We follow a previous CCGbased approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. If there is a dependency in the PARG structure between the two words, then this feature is defined as the conjunction of (1) the category of the functor, (2) the argument slot that is being filled in the functor category, and (3) an indication as to whether the functor (→) or the argument (+—) is the lexical head. For example, to indicate the relationship between car and fixed in both sentences of figure 3, the feature is (s\np)/np.2.→. The labeler uses all of the previous features, plus the following: </context>
<context position="23546" citStr="Gildea and Hockenmaier, 2003" startWordPosition="3867" endWordPosition="3870"> Brutus (automatic) 76.06% 70.15% 72.99% Table 1: Accuracy of semantic role prediction using only CCG based features. originally tagged terminal. In most cases, this corresponds to the node immediately dominated by the lowest common subsuming node of the the target word and the verb (figure 5). In some cases, the highest constituent that is headed by the target word is not immediately dominated by the lowest common subsuming node (figure 6). 9 Results Using a version of Brutus incorporating only the CCGbased features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&amp;H). This could be due to a number of factors, including the fact that our system employs a different CCG parser, uses a more complete mapping of the Propbank onto the CCGbank, uses a different machine learning approach,6 and has a richer feature set. The results for constituent tagging accuracy are shown in table 1. As expected, by incorporating Penn Treebank-based features and dependency features, we obtain better results than with the CCG-only system. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeli</context>
<context position="25806" citStr="Gildea and Hockenmaier, 2003" startWordPosition="4226" endWordPosition="4229">s and headwords. Headwords are easier to predict than boundaries, reflecting CCG’s focus on word-word relations rather than constituency. for dependencies as a more appropriate means of evaluation, reflecting the focus on headwords from constituent boundaries. We argue that, especially in the heavily lexicalized CCG framework, headword evaluation is more appropriate, reflecting the emphasis on headword combinatorics in the CCG formalism. 10 The Contribution of the New Features Two features which are less frequently used in SRL research play a major role in the Brutus system: The PARG feature (Gildea and Hockenmaier, 2003) and the argument mapping feature. Removing them has a strong effect on accuracy when labeling treebank parses, as shown in our feature ablation results in table 4. We do not report results including the Argument Mapping feature but not the PARG feature, because some predicate-argument relation information is assumed in generating the Argument Mapping feature. P R F +PARG +AM 88.77% 86.15% 87.44% +PARG -AM 88.42% 85.78% 87.08% -PARG -AM 87.92% 84.65% 86.26% Table 4: The effects of removing key features from the system on gold standard parses. The same is true for automatic parses, as shown in </context>
</contexts>
<marker>Gildea, Hockenmaier, 2003</marker>
<rawString>Daniel Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using Combinatory Categorial Grammar. In Proc. EMNLP-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="3998" citStr="Hockenmaier and Steedman, 2007" startWordPosition="627" endWordPosition="630">from Charniak’s Parser and a Collins parser (Collins, 2003; Bikel, 2004). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. We do not employ a similar strategy due to the differing notions of constituency represented in our parsers (CCG having a much more fluid notion of constituency and the MALT parser using a different approach entirely). For the identification and labeling steps, we train a maximum entropy classifier (Berger et al., 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We evaluate our SRL system’s argument predictions at the word string level, making our results directly comparable for each argument labeling.1 In the following, we briefly introduce the CCG grammatical formalism and motivate its use in SRL (Sections 2–3). Our main contribution is to demonstrate that CCG — arguably a more expressive and lin1This is guaranteed by our string-to-string mapping from the original Propbank to the CCGbank. 37 Proceedings of the 47th Annual Meeting of the ACL and the 4t</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Dependencybased syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>Proceedings of CoNLL–2008.</booktitle>
<contexts>
<context position="24265" citStr="Johansson and Nugues, 2008" startWordPosition="3983" endWordPosition="3987">tem employs a different CCG parser, uses a more complete mapping of the Propbank onto the CCGbank, uses a different machine learning approach,6 and has a richer feature set. The results for constituent tagging accuracy are shown in table 1. As expected, by incorporating Penn Treebank-based features and dependency features, we obtain better results than with the CCG-only system. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al., 2008). Other systems (Toutanova et al., 2008; Surdeanu et al., 2007; Johansson and Nugues, 2008) have also achieved comparable results – we compare our system to (Punyakanok et al., 2008) due to the similarities in our approaches. The performance of the full system is shown in table 2. Table 3 shows the ability of the system to predict the correct headwords of semantic roles. This is a necessary condition for correctness of the full constituent, but not a sufficient one. In parser evaluation, Carroll, Minnen, and Briscoe (Carroll et al., 2003) have argued 6G&amp;H use a generative model with a back-off lattice, whereas we use a maximum entropy classifier. Table 2: Accuracy of semantic role p</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>R. Johansson and P. Nugues. 2008. Dependencybased syntactic–semantic analysis with PropBank and NomBank. Proceedings of CoNLL–2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="9447" citStr="Liu and Nocedal, 1989" startWordPosition="1491" endWordPosition="1495">for the labeling model. The same features are extracted for both treebank and automatic parses. Automatic parses were generated using the C&amp;C CCG parser (Clark and Curran, 2007) with its derivation output format converted to resemble that of the CCGbank. This involved following the derivational bracketings of the C&amp;C parser’s output and reconstructing the backpointers to the lexical heads using an in-house implementation of the basic CCG combinatory operations. All classifiers were trained to 500 iterations of L-BFGS training — a quasi-Newton method from the numerical optimization literature (Liu and Nocedal, 1989) — using Zhang Le’s maxent toolkit.2 To prevent overfitting we used Gaussian priors with global variances of 1 and 5 for the identifier and labeler, respectively.3 The Gaussian priors were determined empirically by testing on the development set. Both the identifier and the labeler use the following features: (1) Words. Words drawn from a 3 word window around the target word,4 with each word associated with a binary indicator feature. (2) Part of Speech. Part of Speech tags drawn from a 3 word window around the target word, 2Available for download at http://homepages. inf.ed.ac.uk/s0450736/max</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D C Liu and Jorge Nocedal. 1989. On the limited memory method for large scale optimization. Mathematical Programming B, 45(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis M`arquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic Role Labeling: An Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litowski, Stevenson, 2008</marker>
<rawString>Lluis M`arquez, Xavier Carreras, Kenneth C. Litowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An Introduction to the Special Issue. Computational Linguistics, 34(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Gabrile Musillo</author>
</authors>
<title>Semantic parsing for high-precision semantic role labelling.</title>
<date>2008</date>
<booktitle>In Proceedings of CONLL-08,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="30326" citStr="Merlo and Musillo, 2008" startWordPosition="4991" endWordPosition="4994">e number of errors in the system are attributable to errors in the CCG derivation, either in the gold standard or in automatically generated parses. Potential future work may focus on developing an improved CCG parser using the revised (syntactic) adjunct-argument distinctions (guided by the Propbank annotation) described in (Boxwell and White, 2008). This resource, together with the reasonable accuracy (,: 90%) with which argument mappings can be predicted, suggests the possibility of an integrated, simultaneous syntactic-semantic parsing process, similar to that of (Musillo and Merlo, 2006; Merlo and Musillo, 2008). We expect this would improve the reliability and accuracy of both the syntactic and semantic analysis components. 13 Acknowledgments This research was funded by NSF grant IIS-0347799. We are deeply indebted to Julia Hockenmaier for the np np 43 the company stopped using asbestos in 1956 np ((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)/np np (s\np)\(s\np) � s[ng]\np − stop.Arg1 � � s[dcl]\np s[dcl] Figure 7: An example of how incorrect PP attachment can cause an incorrect labeling. Stop.Arg1 should cover using asbestos rather than using asbestos in 1956. This sentence is based on wsj 0003.3, with the s</context>
</contexts>
<marker>Merlo, Musillo, 2008</marker>
<rawString>Paola Merlo and Gabrile Musillo. 2008. Semantic parsing for high-precision semantic role labelling. In Proceedings of CONLL-08, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Musillo</author>
<author>Paola Merlo</author>
</authors>
<title>Robust parsing of the proposition bank.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop ROMAND,</booktitle>
<location>Trento.</location>
<contexts>
<context position="30300" citStr="Musillo and Merlo, 2006" startWordPosition="4987" endWordPosition="4990"> analysis section, a large number of errors in the system are attributable to errors in the CCG derivation, either in the gold standard or in automatically generated parses. Potential future work may focus on developing an improved CCG parser using the revised (syntactic) adjunct-argument distinctions (guided by the Propbank annotation) described in (Boxwell and White, 2008). This resource, together with the reasonable accuracy (,: 90%) with which argument mappings can be predicted, suggests the possibility of an integrated, simultaneous syntactic-semantic parsing process, similar to that of (Musillo and Merlo, 2006; Merlo and Musillo, 2008). We expect this would improve the reliability and accuracy of both the syntactic and semantic analysis components. 13 Acknowledgments This research was funded by NSF grant IIS-0347799. We are deeply indebted to Julia Hockenmaier for the np np 43 the company stopped using asbestos in 1956 np ((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)/np np (s\np)\(s\np) � s[ng]\np − stop.Arg1 � � s[dcl]\np s[dcl] Figure 7: An example of how incorrect PP attachment can cause an incorrect labeling. Stop.Arg1 should cover using asbestos rather than using asbestos in 1956. This sentence is based</context>
</contexts>
<marker>Musillo, Merlo, 2006</marker>
<rawString>Gabriele Musillo and Paola Merlo. 2006. Robust parsing of the proposition bank. In Proceedings of the EACL 2006 Workshop ROMAND, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. MaltParser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1760" citStr="Palmer et al., 2005" startWordPosition="262" endWordPosition="265">cess of assigning semantic roles to strings of words in a sentence according to their relationship to the semantic predicates expressed in the sentence. The task is difficult because the relationship between syntactic relations like “subject” and “object” do not always correspond to semantic relations like “agent” and “patient”. An effective semantic role labeling system must recognize the differences between different configurations: (a) [The man]Arg0 opened [the door]Arg1 [for him]Arg3 [today]ArgM−TMP. (b) [The door]Arg1 opened. (c) [The door]Arg1 was opened by [a man]Arg0. We use Propbank (Palmer et al., 2005), a corpus of newswire text annotated with verb predicate semantic role information that is widely used in the SRL literature (M`arquez et al., 2008). Rather than describe semantic roles in terms of “agent” or “patient”, Propbank defines semantic roles on a verb-by-verb basis. For example, the verb open encodes the OPENER as Arg0, the OPENEE as Arg1, and the beneficiary of the OPENING action as Arg3. Propbank also defines a set of adjunct roles, denoted by the letter M instead of a number. For example, ArgM-TMP denotes a temporal role, like “today”. By using verb-specific roles, Propbank avoid</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<date>2008</date>
<booktitle>The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. Computational Linguistics,</booktitle>
<pages>34--2</pages>
<contexts>
<context position="2483" citStr="Punyakanok et al., 2008" startWordPosition="384" endWordPosition="387">used in the SRL literature (M`arquez et al., 2008). Rather than describe semantic roles in terms of “agent” or “patient”, Propbank defines semantic roles on a verb-by-verb basis. For example, the verb open encodes the OPENER as Arg0, the OPENEE as Arg1, and the beneficiary of the OPENING action as Arg3. Propbank also defines a set of adjunct roles, denoted by the letter M instead of a number. For example, ArgM-TMP denotes a temporal role, like “today”. By using verb-specific roles, Propbank avoids specific claims about parallels between the roles of different verbs. We follow the approach in (Punyakanok et al., 2008) in framing the SRL problem as a two-stage pipeline: identification followed by labeling. During identification, every word in the sentence is labeled either as bearing some (as yet undetermined) semantic role or not . This is done for each verb. Next, during labeling, the precise verb-specific roles for each word are determined. In contrast to the approach in (Punyakanok et al., 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Another difference is our choice of parsers. Br</context>
<context position="15720" citStr="Punyakanok et al., 2008" startWordPosition="2560" endWordPosition="2563">arget word. The Argument Mapping feature can be viewed as a simple prediction about some of the non-modifier semantic roles that a verb is likely to express. We use this information as a feature and not a hard constraint to allow other features to overrule the recommendation made by the argument mapping classifier. The features used in the argument mapping classifier are described in detail in section 7. 5 CFG based Features In addition to CCG-based features, features can be drawn from a traditional CFG-style approach when they are available. Our motivation for this is twofold. First, others (Punyakanok et al., 2008, e.g.), have found that different parsers have different error patterns, and so using multiple parsers can yield complementary sources of correct information. Second, we noticed that, although the CCG-based system performed well on head word labeling, performance dropped when projecting these labels to the constituent level (see sections 8 and 9 for more). This may have to do with the fact that CCG is not centered around a constituencybased analysis, as well as with inconsistencies between CCG and Penn Treebank-style bracketings (the latter being what was annotated in the original Propbank). </context>
<context position="24174" citStr="Punyakanok et al., 2008" startWordPosition="3969" endWordPosition="3972">nceforth G&amp;H). This could be due to a number of factors, including the fact that our system employs a different CCG parser, uses a more complete mapping of the Propbank onto the CCGbank, uses a different machine learning approach,6 and has a richer feature set. The results for constituent tagging accuracy are shown in table 1. As expected, by incorporating Penn Treebank-based features and dependency features, we obtain better results than with the CCG-only system. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al., 2008). Other systems (Toutanova et al., 2008; Surdeanu et al., 2007; Johansson and Nugues, 2008) have also achieved comparable results – we compare our system to (Punyakanok et al., 2008) due to the similarities in our approaches. The performance of the full system is shown in table 2. Table 3 shows the ability of the system to predict the correct headwords of semantic roles. This is a necessary condition for correctness of the full constituent, but not a sufficient one. In parser evaluation, Carroll, Minnen, and Briscoe (Carroll et al., 2003) have argued 6G&amp;H use a generative model with a back-off</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008. The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5289" citStr="Steedman, 2000" startWordPosition="834" endWordPosition="835">09 ACL and AFNLP guistically appealing syntactic framework than vanilla CFGs — is a viable basis for the SRL task. This is supported by our experimental results, the setup and details of which we give in Sections 4–10. In particular, using CCG enables us to map semantic roles directly onto verbal categories, an innovation of our approach that leads to performance gains (Section 7). We conclude with an error analysis (Section 11), which motivates our discussion of future research for computational semantics with CCG (Section 12). 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammatical framework that describes syntactic structure in terms of the combinatory potential of the lexical (word-level) items. Rather than using standard part-of-speech tags and grammatical rules, CCG encodes much of the combinatory potential of each word by assigning a syntactically informative category. For example, the verb loves has the category (s\np)/np, which could be read “the kind of word that would be a sentence if it could combine with a noun phrase on the right and a noun phrase on the left”. Further, CCG has the advantage of a transparent interface between the way the wor</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>P Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>29--105</pages>
<marker>Surdeanu, M`arquez, Carreras, Comas, 2007</marker>
<rawString>M. Surdeanu, L. M`arquez, X. Carreras, and P. Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research, 29:105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="24213" citStr="Toutanova et al., 2008" startWordPosition="3975" endWordPosition="3978">ber of factors, including the fact that our system employs a different CCG parser, uses a more complete mapping of the Propbank onto the CCGbank, uses a different machine learning approach,6 and has a richer feature set. The results for constituent tagging accuracy are shown in table 1. As expected, by incorporating Penn Treebank-based features and dependency features, we obtain better results than with the CCG-only system. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al., 2008). Other systems (Toutanova et al., 2008; Surdeanu et al., 2007; Johansson and Nugues, 2008) have also achieved comparable results – we compare our system to (Punyakanok et al., 2008) due to the similarities in our approaches. The performance of the full system is shown in table 2. Table 3 shows the ability of the system to predict the correct headwords of semantic roles. This is a necessary condition for correctness of the full constituent, but not a sufficient one. In parser evaluation, Carroll, Minnen, and Briscoe (Carroll et al., 2003) have argued 6G&amp;H use a generative model with a back-off lattice, whereas we use a maximum entr</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>K. Toutanova, A. Haghighi, and C.D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161–191.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>