<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000455">
<title confidence="0.987941">
Joint Learning Improves Semantic Role Labeling
</title>
<author confidence="0.996835">
Kristina Toutanova
</author>
<affiliation confidence="0.9865495">
Dept of Computer Science
Stanford University
</affiliation>
<address confidence="0.96427">
Stanford, CA, 94305
</address>
<email confidence="0.999232">
kristina@cs.stanford.edu
</email>
<author confidence="0.991753">
Aria Haghighi
</author>
<affiliation confidence="0.9864905">
Dept of Computer Science
Stanford University
</affiliation>
<address confidence="0.963967">
Stanford, CA, 94305
</address>
<email confidence="0.998343">
aria42@stanford.edu
</email>
<author confidence="0.995723">
Christopher D. Manning
</author>
<affiliation confidence="0.9865255">
Dept of Computer Science
Stanford University
</affiliation>
<address confidence="0.964345">
Stanford, CA, 94305
</address>
<email confidence="0.999478">
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.993916" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963823529412">
Despite much recent progress on accu-
rate semantic role labeling, previous work
has largely used independent classifiers,
possibly combined with separate label se-
quence models via Viterbi decoding. This
stands in stark contrast to the linguistic
observation that a core argument frame is
a joint structure, with strong dependen-
cies between arguments. We show how to
build a joint model of argument frames,
incorporating novel features that model
these interactions into discriminative log-
linear models. This system achieves an
error reduction of 22% on all arguments
and 32% on core arguments over a state-
of-the art independent classifier for gold-
standard parse trees on PropBank.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999851864864865">
The release of semantically annotated corpora such
as FrameNet (Baker et al., 1998) and PropBank
(Palmer et al., 2003) has made it possible to develop
high-accuracy statistical models for automated se-
mantic role labeling (Gildea and Jurafsky, 2002;
Pradhan et al., 2004; Xue and Palmer, 2004). Such
systems have identified several linguistically mo-
tivated features for discriminating arguments and
their labels (see Table 1). These features usually
characterize aspects of individual arguments and the
predicate.
It is evident that the labels and the features of ar-
guments are highly correlated. For example, there
are hard constraints – that arguments cannot overlap
with each other or the predicate, and also soft con-
straints – for example, is it unlikely that a predicate
will have two or more AGENT arguments, or that a
predicate used in the active voice will have a THEME
argument prior to an AGENT argument. Several sys-
tems have incorporated such dependencies, for ex-
ample, (Gildea and Jurafsky, 2002; Pradhan et al.,
2004; Thompson et al., 2003) and several systems
submitted in the CoNLL-2004 shared task (Carreras
and M`arquez, 2004). However, we show that there
are greater gains to be had by modeling joint infor-
mation about a verb’s argument structure.
We propose a discriminative log-linear joint
model for semantic role labeling, which incorpo-
rates more global features and achieves superior
performance in comparison to state-of-the-art mod-
els. To deal with the computational complexity of
the task, we employ dynamic programming and re-
ranking approaches. We present performance re-
sults on the February 2004 version of PropBank on
gold-standard parse trees as well as results on auto-
matic parses generated by Charniak’s parser (Char-
niak, 2000).
</bodyText>
<sectionHeader confidence="0.717859" genericHeader="introduction">
2 Semantic Role Labeling: Task Definition
and Architectures
</sectionHeader>
<bodyText confidence="0.827966">
Consider the pair of sentences,
</bodyText>
<listItem confidence="0.9943495">
• [The GM-Jaguar pact]AGENT gives
[the car market]RECIPIENT
[a much-needed boost]T.EME
• [A much-needed boost]T.EME was given to
[the car market]RECIPIENT
by [the GM-Jaguar pact]AGENT
</listItem>
<bodyText confidence="0.7683235">
Despite the different syntactic positions of the la-
beled phrases, we recognize that each plays the same
</bodyText>
<page confidence="0.97313">
589
</page>
<note confidence="0.991616">
Proceedings of the 43rd Annual Meeting of the ACL, pages 589–596,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999794405405405">
role – indicated by the label – in the meaning of
this sense of the verb give. We call such phrases
fillers of semantic roles and our task is, given a sen-
tence and a target verb, to return all such phrases
along with their correct labels. Therefore one sub-
task is to group the words of a sentence into phrases
or constituents. As in most previous work on se-
mantic role labeling, we assume the existence of a
separate parsing model that can assign a parse tree t
to each sentence, and the task then is to label each
node in the parse tree with the semantic role of the
phrase it dominates, or NONE, if the phrase does not
fill any role. We do stress however that the joint
framework and features proposed here can also be
used when only a shallow parse (chunked) represen-
tation is available as in the CoNLL-2004 shared task
(Carreras and M`arquez, 2004).
In the February 2004 version of the PropBank cor-
pus, annotations are done on top of the Penn Tree-
Bank II parse trees (Marcus et al., 1993). Possi-
ble labels of arguments in this corpus are the core
argument labels ARG[0-5], and the modifier argu-
ment labels. The core arguments ARG[3-5] do not
have consistent global roles and tend to be verb spe-
cific. There are about 14 modifier labels such as
ARGM-LOC and ARGM-TMP, for location and tem-
poral modifiers respectively.&apos; Figure 1 shows an ex-
ample parse tree annotated with semantic roles.
We distinguish between models that learn to la-
bel nodes in the parse tree independently, called lo-
cal models, and models that incorporate dependen-
cies among the labels of multiple nodes, called joint
models. We build both local and joint models for se-
mantic role labeling, and evaluate the gains achiev-
able by incorporating joint information. We start
by introducing our local models, and later build on
them to define joint models.
</bodyText>
<sectionHeader confidence="0.987917" genericHeader="method">
3 Local Classifiers
</sectionHeader>
<bodyText confidence="0.9527319">
In the context of role labeling, we call a classifier
local if it assigns a probability (or score) to the label
of an individual parse tree node ni independently of
the labels of other nodes.
We use the standard separation of the task of se-
mantic role labeling into identification and classifi-
&apos;For a full listing of PropBank argument labels see (Palmer
et al., 2003)
cation phases. In identification, our task is to clas-
sify nodes of t as either ARG, an argument (includ-
ing modifiers), or NONE, a non-argument. In clas-
sification, we are given a set of arguments in t and
must label each one with its appropriate semantic
role. Formally, let L denote a mapping of the nodes
in t to a label set of semantic roles (including NONE)
and let Id(L) be the mapping which collapses L’s
non-NONE values into ARG. Then we can decom-
pose the probability of a labeling L into probabili-
ties according to an identification model PID and a
classification model PCLS.
</bodyText>
<equation confidence="0.9998645">
PSFL(L|t,v) = PID(Id(L)|t,v) x
PCLS(L|t, v, Id(L)) (1)
</equation>
<bodyText confidence="0.999983272727273">
This decomposition does not encode any indepen-
dence assumptions, but is a useful way of thinking
about the problem. Our local models for semantic
role labeling use this decomposition. Previous work
has also made this distinction because, for example,
different features have been found to be more effec-
tive for the two tasks, and it has been a good way
to make training and search during testing more ef-
ficient.
Here we use the same features for local identifi-
cation and classification models, but use the decom-
position for efficiency of training. The identification
models are trained to classify each node in a parse
tree as ARG or NONE, and the classification models
are trained to label each argument node in the train-
ing set with its specific label. In this way the train-
ing set for the classification models is smaller. Note
that we don’t do any hard pruning at the identifica-
tion stage in testing and can find the exact labeling
of the complete parse tree, which is the maximizer
of Equation 1. Thus we do not have accuracy loss
as in the two-pass hard prune strategy described in
(Pradhan et al., 2005).
In previous work, various machine learning meth-
ods have been used to learn local classifiers for role
labeling. Examples are linearly interpolated rela-
tive frequency models (Gildea and Jurafsky, 2002),
SVMs (Pradhan et al., 2004), decision trees (Sur-
deanu et al., 2003), and log-linear models (Xue and
Palmer, 2004). In this work we use log-linear mod-
els for multi-class classification. One advantage of
log-linear models over SVMs for us is that they pro-
duce probability distributions and thus identification
</bodyText>
<page confidence="0.976391">
590
</page>
<bodyText confidence="0.999980666666667">
A problem with this approach is that a maximizing
labeling of the nodes could possibly violate the con-
straint that argument nodes should not overlap with
each other. Therefore, to produce a consistent set of
arguments with local classifiers, we must have a way
of enforcing the non-overlapping constraint.
</bodyText>
<subsectionHeader confidence="0.999477">
3.1 Enforcing the Non-overlapping Constraint
</subsectionHeader>
<bodyText confidence="0.999996153846154">
Here we describe a fast exact dynamic programming
algorithm to find the most likely non-overlapping
(consistent) labeling of all nodes in the parse tree,
according to a product of probabilities from local
models, as in Equation 2. For simplicity, we de-
scribe the dynamic program for the case where only
two classes are possible – ARG and NONE. The gen-
eralization to more classes is straightforward. In-
tuitively, the algorithm is similar to the Viterbi al-
gorithm for context-free grammars, because we can
describe the non-overlapping constraint by a “gram-
mar” that disallows ARG nodes to have ARG descen-
dants.
Below we will talk about maximizing the sum of
the logs of local probabilities rather than the prod-
uct of local probabilities, which is equivalent. The
dynamic program works from the leaves of the tree
up and finds a best assignment for each tree, using
already computed assignments for its children. Sup-
pose we want the most likely consistent assignment
for subtree t with children trees t1, ... , tk each stor-
ing the most likely consistent assignment of nodes
it dominates as well as the log-probability of the as-
signment of all nodes it dominates to NONE. The
most likely assignment for t is the one that corre-
sponds to the maximum of:
</bodyText>
<listItem confidence="0.994257142857143">
• The sum of the log-probabilities of the most
likely assignments of the children subtrees
t1, ... , tk plus the log-probability for assigning
the node t to NONE
• The sum of the log-probabilities for assign-
ing all of ti’s nodes to NONE plus the log-
probability for assigning the node t to ARG.
</listItem>
<bodyText confidence="0.87442825">
Propagating this procedure from the leaves to the
root of t, we have our most likely non-overlapping
assignment. By slightly modifying this procedure,
we obtain the most likely assignment according to
</bodyText>
<table confidence="0.975569607142857">
Standard Features (Gildea and Jurafsky, 2002)
PHRASE TYPE: Syntactic Category of node
PREDICATE LEMMA: Stemmed Verb
PATH: Path from node to predicate
POSITION: Before or after predicate?
VOICE: Active or passive relative to predicate
HEAD WORD OF PHRASE
SUB-CAT: CFG expansion of predicate’s parent
Additional Features (Pradhan et al., 2004)
FIRST/LAST WORD
LEFT/RIGHT SISTER PHRASE-TYPE
LEFT/RIGHT SISTER HEAD WORD/POS
PARENT PHRASE-TYPE
PARENT POS/HEAD-WORD
ORDINAL TREE DISTANCE: Phrase Type with
appended length of PATH feature
NODE-LCA PARTIAL PATH Path from constituent
to Lowest Common Ancestor with predicate node
PP PARENT HEAD WORD If parent is a PP
return parent’s head word
PP NP HEAD WORD/POS For a PP, retrieve
the head Word / POS of its rightmost NP
Selected Pairs (Xue and Palmer, 2004)
PREDICATE LEMMA &amp; PATH
PREDICATE LEMMA &amp; HEAD WORD
PREDICATE LEMMA &amp; PHRASE TYPE
VOICE &amp; POSITION
PREDICATE LEMMA &amp; PP PARENT HEAD WORD
</table>
<tableCaption confidence="0.996941">
Table 1: Baseline Features
</tableCaption>
<bodyText confidence="0.999254631578947">
and classification models can be chained in a princi-
pled way, as in Equation 1.
The features we used for local identification and
classification models are outlined in Table 1. These
features are a subset of features used in previous
work. The standard features at the top of the table
were defined by (Gildea and Jurafsky, 2002), and
the rest are other useful lexical and structural fea-
tures identified in more recent work (Pradhan et al.,
2004; Surdeanu et al., 2003; Xue and Palmer, 2004).
The most direct way to use trained local identifi-
cation and classification models in testing is to se-
lect a labeling L of the parse tree that maximizes
the product of the probabilities according to the two
models as in Equation 1. Since these models are lo-
cal, this is equivalent to independently maximizing
the product of the probabilities of the two models
for the label li of each parse tree node ni as shown
below in Equation 2.
</bodyText>
<equation confidence="0.9056385">
P� SRL(L|t, v) = � PID(Id(li)|t, v) (2)
nzEt
�� PCLS(li|t, v, Id(li))
nzEt
</equation>
<page confidence="0.981166">
591
</page>
<bodyText confidence="0.9983592">
a product of local identification and classification
models. We use the local models in conjunction with
this search procedure to select a most likely labeling
in testing. Test set results for our local model PsRL
are given in Table 2.
</bodyText>
<sectionHeader confidence="0.99343" genericHeader="method">
4 Joint Classifiers
</sectionHeader>
<bodyText confidence="0.999150555555556">
As discussed in previous work, there are strong de-
pendencies among the labels of the semantic argu-
ment nodes of a verb. A drawback of local models
is that, when they decide the label of a parse tree
node, they cannot use information about the labels
and features of other nodes in the tree.
Furthermore, these dependencies are highly non-
local. For instance, to avoid repeating argument la-
bels in a frame, we need to add a dependency from
each node label to the labels of all other nodes.
A factorized sequence model that assumes a finite
Markov horizon, such as a chain Conditional Ran-
dom Field (Lafferty et al., 2001), would not be able
to encode such dependencies.
The need for Re-ranking
For argument identification, the number of possi-
ble assignments for a parse tree with n nodes is
2n. This number can run into the hundreds of bil-
lions for a normal-sized tree. For argument label-
ing, the number of possible assignments is Pt� 201,
if m is the number of arguments of a verb (typi-
cally between 2 and 5), and 20 is the approximate
number of possible labels if considering both core
and modifying arguments. Training a model which
has such huge number of classes is infeasible if the
model does not factorize due to strong independence
assumptions. Therefore, in order to be able to in-
corporate long-range dependencies in our models,
we chose to adopt a re-ranking approach (Collins,
2000), which selects from likely assignments gener-
ated by a model which makes stronger independence
assumptions. We utilize the top N assignments of
our local semantic role labeling model PSRL to gen-
erate likely assignments. As can be seen from Table
3, for relatively small values of N, our re-ranking
approach does not present a serious bottleneck to
performance. We used a value of N = 20 for train-
ing. In Table 3 we can see that if we could pick, us-
ing an oracle, the best assignment out for the top 20
assignments according to the local model, we would
achieve an F-Measure of 98.8 on all arguments. In-
creasing the number of N to 30 results in a very
small gain in the upper bound on performance and
a large increase in memory requirements. We there-
fore selected N = 20 as a good compromise.
Generation of top N most likely joint
assignments
We generate the top N most likely non-
overlapping joint assignments of labels to nodes in
a parse tree according to a local model PsRL, by
an exact dynamic programming algorithm, which
is a generalization of the algorithm for finding the
top non-overlapping assignment described in section
3.1.
</bodyText>
<subsectionHeader confidence="0.951759">
Parametric Models
</subsectionHeader>
<bodyText confidence="0.999982583333333">
We learn log-linear re-ranking models for joint se-
mantic role labeling, which use feature maps from a
parse tree and label sequence to a vector space. The
form of the models is as follows. Let 4b(t, v, L) E
R&apos; denote a feature map from a tree t, target verb
v, and joint assignment L of the nodes of the tree,
to the vector space R&apos;. Let L1, L2, · · · , LN denote
top N possible joint assignments. We learn a log-
linear model with a parameter vector W, with one
weight for each of the s dimensions of the feature
vector. The probability (or score) of an assignment
L according to this re-ranking model is defined as:
</bodyText>
<equation confidence="0.996198333333333">
e(,D(t,v,L),W)
P�SRL(L|t, v) = EN e(�(t,v,L ).W) (3)
j=1
</equation>
<bodyText confidence="0.999226857142857">
The score of an assignment L not in the top N
is zero. We train the model to maximize the sum
of log-likelihoods of the best assignments minus a
quadratic regularization term.
In this framework, we can define arbitrary fea-
tures of labeled trees that capture general properties
of predicate-argument structure.
</bodyText>
<subsectionHeader confidence="0.925033">
Joint Model Features
</subsectionHeader>
<bodyText confidence="0.99995575">
We will introduce the features of the joint re-
ranking model in the context of the example parse
tree shown in Figure 1. We model dependencies not
only between the label of a node and the labels of
</bodyText>
<page confidence="0.978233">
592
</page>
<figure confidence="0.644734">
S1
NP1-ARG1 VP1
to 108.1 million shares
</figure>
<figureCaption confidence="0.995241">
Figure 1: An example tree from the PropBank with Semantic Role Annotations.
</figureCaption>
<figure confidence="0.956702125">
PP1 ARG4
NP3 ARGM-TMP
VBD1 PRED
accelerated
Final-hour trading
NP2
TO1
yesterday
</figure>
<bodyText confidence="0.9930554">
other nodes, but also dependencies between the la-
bel of a node and input features of other argument
nodes. The features are specified by instantiation of
templates and the value of a feature is the number of
times a particular pattern occurs in the labeled tree.
</bodyText>
<subsectionHeader confidence="0.471961">
Templates
</subsectionHeader>
<bodyText confidence="0.986127125">
For a tree t, predicate v, and joint assignment L
of labels to the nodes of the tree, we define the can-
didate argument sequence as the sequence of non-
NONE labeled nodes [n1, l1, ... , vPRED, nm, lm] (li
is the label of node ni). A reasonable candidate ar-
gument sequence usually contains very few of the
nodes in the tree – about 2 to 7 nodes, as this is the
typical number of arguments for a verb. To make
it more convenient to express our feature templates,
we include the predicate node v in the sequence.
This sequence of labeled nodes is defined with re-
spect to the left-to-right order of constituents in the
parse tree. Since non-NONE labeled nodes do not
overlap, there is a strict left-to-right order among
these nodes. The candidate argument sequence that
corresponds to the correct assignment in Figure 1
will be:
[NP1-ARG1,VBD1-PRED,PP1-ARG4,NP3-ARGM-TMP]
Features from Local Models: All features included
in the local models are also included in our joint
models. In particular, each template for local fea-
tures is included as a joint template that concatenates
the local template and the node label. For exam-
ple, for the local feature PATH, we define a joint fea-
ture template, that extracts PATH from every node in
the candidate argument sequence and concatenates
it with the label of the node. Both a feature with
the specific argument label is created and a feature
with the generic back-off ARG label. This is similar
to adding features from identification and classifi-
cation models. In the case of the example candidate
argument sequence above, for the node NP1 we have
</bodyText>
<equation confidence="0.577354">
the features:
(NPISI)-ARG1, (NPISI)-ARG
</equation>
<bodyText confidence="0.995202314285715">
When comparing a local and a joint model, we use
the same set of local feature templates in the two
models.
Whole Label Sequence: As observed in previous
work (Gildea and Jurafsky, 2002; Pradhan et al.,
2004), including information about the set or se-
quence of labels assigned to argument nodes should
be very helpful for disambiguation. For example, in-
cluding such information will make the model less
likely to pick multiple fillers for the same role or
to come up with a labeling that does not contain an
obligatory argument. We added a whole label se-
quence feature template that extracts the labels of
all argument nodes, and preserves information about
the position of the predicate. The template also
includes information about the voice of the predi-
cate. For example, this template will be instantiated
as follows for the example candidate argument se-
quence:
[ voice:active ARG1,PRED,ARG4,ARGM-TMP]
We also add a variant of this feature which uses a
generic ARG label instead of specific labels. This
feature template has the effect of counting the num-
ber of arguments to the left and right of the predi-
cate, which provides useful global information about
argument structure. As previously observed (Prad-
han et al., 2004), including modifying arguments in
sequence features is not helpful. This was confirmed
in our experiments and we redefined the whole label
sequence features to exclude modifying arguments.
One important variation of this feature uses the
actual predicate lemma in addition to “voice:active”.
Additionally, we define variations of these feature
templates that concatenate the label sequence with
features of individual nodes. We experimented with
</bodyText>
<page confidence="0.997405">
593
</page>
<bodyText confidence="0.999826853658537">
variations, and found that including the phrase type
and the head of a directly dominating PP – if one
exists – was most helpful. We also add a feature that
detects repetitions of the same label in a candidate
argument sequence, together with the phrase types
of the nodes labeled with that label. For example,
(NP-ARG0,WHNP-ARG0) is a common pattern of this
form.
Frame Features: Another very effective class of fea-
tures we defined are features that look at the label of
a single argument node and internal features of other
argument nodes. The idea of these features is to cap-
ture knowledge about the label of a constituent given
the syntactic realization of all arguments of the verb.
This is helpful to capture syntactic alternations, such
as the dative alternation. For example, consider
the sentence (i) “[Shaw Publishing]ARG0 offered [Mr.
Smith]ARG2 [a reimbursement]ARG, ” and the alterna-
tive realization (ii) “[Shaw Publishing]ARG0 offered
[a reimbursement]ARG, [to Mr. Smith]ARG2”. When
classifying the NP in object position, it is useful to
know whether the following argument is a PP. If
yes, the NP will more likely be an ARG1, and if not,
it will more likely be an ARG2. A feature template
that captures such information extracts, for each ar-
gument node, its phrase type and label in the con-
text of the phrase types for all other arguments. For
example, the instantiation of such a template for [a
reimbursement] in (ii) would be
[ voice:active NP,PRED,NP-ARG1,PP]
We also add a template that concatenates the identity
of the predicate lemma itself.
We should note that Xue and Palmer (2004) define
a similar feature template, called syntactic frame,
which often captures similar information. The im-
portant difference is that their template extracts con-
textual information from noun phrases surrounding
the predicate, rather than from the sequence of ar-
gument nodes. Because our model is joint, we are
able to use information about other argument nodes
when labeling a node.
</bodyText>
<subsectionHeader confidence="0.861889">
Final Pipeline
</subsectionHeader>
<bodyText confidence="0.994250529411765">
Here we describe the application in testing of a
joint model for semantic role labeling, using a local
model P`SRL, and a joint re-ranking model PrSRL.
P`SRL is used to generate top N non-overlapping
joint assignments L1, ... , LN.
One option is to select the best Li according to
PrSRL, as in Equation 3, ignoring the score from
the local model. In our experiments, we noticed that
for larger values of N, the performance of our re-
ranking model PrSRL decreased. This was probably
due to the fact that at test time the local classifier
produces very poor argument frames near the bot-
tom of the top N for large N. Since the re-ranking
model is trained on relatively few good argument
frames, it cannot easily rule out very bad frames. It
makes sense then to incorporate the local model into
our final score. Our final score is given by:
</bodyText>
<equation confidence="0.488337">
PSRL(L|t, v) _ (P`SRL(L|t, v))α PrSRL(L|t, v)
</equation>
<bodyText confidence="0.94388475">
where α is a tunable parameter 2 for how much in-
fluence the local score has in the final score. Such in-
terpolation with a score from a first-pass model was
also used for parse re-ranking in (Collins, 2000).
Given this score, at test time we choose among the
top N local assignments L1, ... , LN according to:
arg max α log P`SRL(L|t, v) + log PrSRL(L|t, v)
LE{L1,...,LN}
</bodyText>
<sectionHeader confidence="0.989768" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999865882352941">
For our experiments we used the February 2004 re-
lease of PropBank. 3 As is standard, we used the
annotations from sections 02–21 for training, 24 for
development, and 23 for testing. As is done in
some previous work on semantic role labeling, we
discard the relatively infrequent discontinuous argu-
ments from both the training and test sets. In addi-
tion to reporting the standard results on individual
argument F-Measure, we also report Frame Accu-
racy (Acc.), the fraction of sentences for which we
successfully label all nodes. There are reasons to
prefer Frame Accuracy as a measure of performance
over individual-argument statistics. Foremost, po-
tential applications of role labeling may require cor-
rect labeling of all (or at least the core) arguments
in a sentence in order to be effective, and partially
correct labelings may not be very useful.
</bodyText>
<footnote confidence="0.971775">
2We found α = 0.5 to work best
3Although the first official release ofPropBank was recently
released, we have not had time to test on it.
</footnote>
<page confidence="0.987793">
594
</page>
<table confidence="0.9989988">
Task CORE ARGM
F1 Acc. F1 Acc.
Identification 95.1 84.0 95.2 80.5
Classification 96.0 93.3 93.6 85.6
Id+Classification 92.2 80.7 89.9 71.8
</table>
<tableCaption confidence="0.909855">
Table 2: Performance of local classifiers on identification, classification, and identification+classification on
section 23, using gold-standard parse trees.
</tableCaption>
<table confidence="0.999903833333333">
N CORE ARGM
F1 Acc. F1 Acc.
1 92.2 80.7 89.9 71.8
5 97.8 93.9 96.8 89.5
20 99.2 97.4 98.8 95.3
30 99.3 97.9 99.0 96.2
</table>
<tableCaption confidence="0.998859">
Table 3: Oracle upper bounds for performance on the complete identification+classification task, using
varying numbers of top N joint labelings according to local classifiers.
</tableCaption>
<table confidence="0.9997045">
Model CORE ARGM
F1 Acc. F1 Acc.
Local 92.2 80.7 89.9 71.8
Joint 94.7 88.2 92.1 79.4
</table>
<tableCaption confidence="0.965802">
Table 4: Performance of local and joint models on identification+classification on section 23, using gold-
standard parse trees.
</tableCaption>
<bodyText confidence="0.999907294117647">
We report results for two variations of the seman-
tic role labeling task. For CORE, we identify and
label only core arguments. For ARGM, we identify
and label core as well as modifier arguments. We
report results for local and joint models on argu-
ment identification, argument classification, and the
complete identification and classification pipeline.
Our local models use the features listed in Table 1
and the technique for enforcing the non-overlapping
constraint discussed in Section 3.1.
The labeling of the tree in Figure 1 is a specific
example of the kind of errors fixed by the joint mod-
els. The local classifier labeled the first argument in
the tree as ARG0 instead of ARG1, probably because
an ARG0 label is more likely for the subject position.
All joint models for these experiments used the
whole sequence and frame features. As can be seen
from Table 4, our joint models achieve error reduc-
tions of 32% and 22% over our local models in F-
Measure on CORE and ARGM respectively. With re-
spect to the Frame Accuracy metric, the joint error
reduction is 38% and 26% for CORE and ARGM re-
spectively.
We also report results on automatic parses (see
Table 5). We trained and tested on automatic parse
trees from Charniak’s parser (Charniak, 2000). For
approximately 5.6% of the argument constituents
in the test set, we could not find exact matches in
the automatic parses. Instead of discarding these
arguments, we took the largest constituent in the
automatic parse having the same head-word as the
gold-standard argument constituent. Also, 19 of the
propositions in the test set were discarded because
Charniak’s parser altered the tokenization of the in-
put sentence and tokens could not be aligned. As our
results show, the error reduction of our joint model
with respect to the local model is more modest in this
setting. One reason for this is the lower upper bound,
due largely to the the much poorer performance of
the identification model on automatic parses. For
ARGM, the local identification model achieves 85.9
F-Measure and 59.4 Frame Accuracy; the local clas-
sification model achieves 92.3 F-Measure and 83.1
Frame Accuracy. It seems that the largest boost
would come from features that can identify argu-
ments in the presence of parser errors, rather than
the features of our joint model, which ensure global
coherence of the argument frame. We still achieve
10.7% and 18.5% error reduction for CORE argu-
ments in F-Measure and Frame Accuracy respec-
tively.
</bodyText>
<page confidence="0.997326">
595
</page>
<table confidence="0.997354">
Model CORE ARGM
F1 Acc. F1 Acc.
Local 84.1 66.5 81.4 55.6
Joint 85.8 72.7 82.9 60.8
</table>
<tableCaption confidence="0.9958035">
Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak
automatically generated parse trees.
</tableCaption>
<sectionHeader confidence="0.999885" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999956888888889">
Several semantic role labeling systems have success-
fully utilized joint information. (Gildea and Juraf-
sky, 2002) used the empirical probability of the set
of proposed arguments as a prior distribution. (Prad-
han et al., 2004) train a language model over label
sequences. (Punyakanok et al., 2004) use a linear
programming framework to ensure that the only ar-
gument frames which get probability mass are ones
that respect global constraints on argument labels.
The key differences of our approach compared
to previous work are that our model has all of the
following properties: (i) we do not assume a finite
Markov horizon for dependencies among node la-
bels, (ii) we include features looking at the labels
of multiple argument nodes and internal features of
these nodes, and (iii) we train a discriminative model
capable of incorporating these long-distance depen-
dencies.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999664">
Reflecting linguistic intuition and in line with cur-
rent work, we have shown that there are substantial
gains to be had by jointly modeling the argument
frames of verbs. This is especially true when we
model the dependencies with discriminative models
capable of incorporating long-distance features.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.994143142857143">
The authors would like to thank the review-
ers for their helpful comments and Dan Juraf-
sky for his insightful suggestions and useful dis-
cussions. This work was supported in part by
the Advanced Research and Development Activity
(ARDA)’s Advanced Question Answering for Intel-
ligence (AQUAINT) Program.
</bodyText>
<sectionHeader confidence="0.998182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999748809523809">
Collin Baker, Charles Fillmore, and John Lowe. 1998. The
Berkeley Framenet project. In Proceedings of COLING-
ACL-1998.
Xavier Carreras and Luis M`arquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings ofNAACL, pages 132–139.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings ofICML-2000.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245–288.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-2001.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313–330.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,
and Dan Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of HLT/NAACL-
2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support vec-
tor learning for semantic argument classification. Machine
Learning Journal.
Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zimak, and
Yuancheng Tu. 2004. Semantic role labeling via generalized
inference over classifiers. In Proceedings of CoNLL-2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings ofACL-2003.
Cynthia A. Thompson, Roger Levy, and Christopher D. Man-
ning. 2003. A generative model for semantic role labeling.
In Proceedings ofECML-2003.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings ofEMNLP-2004.
</reference>
<page confidence="0.998655">
596
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898942">
<title confidence="0.999677">Joint Learning Improves Semantic Role Labeling</title>
<author confidence="0.999628">Kristina Toutanova</author>
<affiliation confidence="0.999958">Dept of Computer Science Stanford University</affiliation>
<address confidence="0.999898">Stanford, CA, 94305</address>
<email confidence="0.99965">kristina@cs.stanford.edu</email>
<author confidence="0.999193">Aria Haghighi</author>
<affiliation confidence="0.9999775">Dept of Computer Science Stanford University</affiliation>
<address confidence="0.99981">Stanford, CA, 94305</address>
<email confidence="0.997925">aria42@stanford.edu</email>
<author confidence="0.999822">Christopher D Manning</author>
<affiliation confidence="0.9999615">Dept of Computer Science Stanford University</affiliation>
<address confidence="0.999766">Stanford, CA, 94305</address>
<email confidence="0.999624">manning@cs.stanford.edu</email>
<abstract confidence="0.994608888888889">Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an reduction of all arguments core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley Framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL-1998.</booktitle>
<contexts>
<context position="1154" citStr="Baker et al., 1998" startWordPosition="160" endWordPosition="163">sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap with each other</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. The Berkeley Framenet project. In Proceedings of COLINGACL-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>Xavier Carreras and Luis M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2829" citStr="Charniak, 2000" startWordPosition="426" endWordPosition="428"> However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure. We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models. To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches. We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak’s parser (Charniak, 2000). 2 Semantic Role Labeling: Task Definition and Architectures Consider the pair of sentences, • [The GM-Jaguar pact]AGENT gives [the car market]RECIPIENT [a much-needed boost]T.EME • [A much-needed boost]T.EME was given to [the car market]RECIPIENT by [the GM-Jaguar pact]AGENT Despite the different syntactic positions of the labeled phrases, we recognize that each plays the same 589 Proceedings of the 43rd Annual Meeting of the ACL, pages 589–596, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics role – indicated by the label – in the meaning of this sense of the verb give</context>
<context position="26022" citStr="Charniak, 2000" startWordPosition="4376" endWordPosition="4377">st argument in the tree as ARG0 instead of ARG1, probably because an ARG0 label is more likely for the subject position. All joint models for these experiments used the whole sequence and frame features. As can be seen from Table 4, our joint models achieve error reductions of 32% and 22% over our local models in FMeasure on CORE and ARGM respectively. With respect to the Frame Accuracy metric, the joint error reduction is 38% and 26% for CORE and ARGM respectively. We also report results on automatic parses (see Table 5). We trained and tested on automatic parse trees from Charniak’s parser (Charniak, 2000). For approximately 5.6% of the argument constituents in the test set, we could not find exact matches in the automatic parses. Instead of discarding these arguments, we took the largest constituent in the automatic parse having the same head-word as the gold-standard argument constituent. Also, 19 of the propositions in the test set were discarded because Charniak’s parser altered the tokenization of the input sentence and tokens could not be aligned. As our results show, the error reduction of our joint model with respect to the local model is more modest in this setting. One reason for this</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML-2000.</booktitle>
<contexts>
<context position="13555" citStr="Collins, 2000" startWordPosition="2257" endWordPosition="2258">n nodes is 2n. This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is Pt� 201, if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such huge number of classes is infeasible if the model does not factorize due to strong independence assumptions. Therefore, in order to be able to incorporate long-range dependencies in our models, we chose to adopt a re-ranking approach (Collins, 2000), which selects from likely assignments generated by a model which makes stronger independence assumptions. We utilize the top N assignments of our local semantic role labeling model PSRL to generate likely assignments. As can be seen from Table 3, for relatively small values of N, our re-ranking approach does not present a serious bottleneck to performance. We used a value of N = 20 for training. In Table 3 we can see that if we could pick, using an oracle, the best assignment out for the top 20 assignments according to the local model, we would achieve an F-Measure of 98.8 on all arguments. </context>
<context position="22780" citStr="Collins, 2000" startWordPosition="3833" endWordPosition="3834">robably due to the fact that at test time the local classifier produces very poor argument frames near the bottom of the top N for large N. Since the re-ranking model is trained on relatively few good argument frames, it cannot easily rule out very bad frames. It makes sense then to incorporate the local model into our final score. Our final score is given by: PSRL(L|t, v) _ (P`SRL(L|t, v))α PrSRL(L|t, v) where α is a tunable parameter 2 for how much influence the local score has in the final score. Such interpolation with a score from a first-pass model was also used for parse re-ranking in (Collins, 2000). Given this score, at test time we choose among the top N local assignments L1, ... , LN according to: arg max α log P`SRL(L|t, v) + log PrSRL(L|t, v) LE{L1,...,LN} 5 Experiments and Results For our experiments we used the February 2004 release of PropBank. 3 As is standard, we used the annotations from sections 02–21 for training, 24 for development, and 23 for testing. As is done in some previous work on semantic role labeling, we discard the relatively infrequent discontinuous arguments from both the training and test sets. In addition to reporting the standard results on individual argume</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings ofICML-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1318" citStr="Gildea and Jurafsky, 2002" startWordPosition="185" endWordPosition="188"> dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the a</context>
<context position="7529" citStr="Gildea and Jurafsky, 2002" startWordPosition="1233" endWordPosition="1236">abel each argument node in the training set with its specific label. In this way the training set for the classification models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other. Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-over</context>
<context position="9982" citStr="Gildea and Jurafsky, 2002" startWordPosition="1643" endWordPosition="1646">NONE. The most likely assignment for t is the one that corresponds to the maximum of: • The sum of the log-probabilities of the most likely assignments of the children subtrees t1, ... , tk plus the log-probability for assigning the node t to NONE • The sum of the log-probabilities for assigning all of ti’s nodes to NONE plus the logprobability for assigning the node t to ARG. Propagating this procedure from the leaves to the root of t, we have our most likely non-overlapping assignment. By slightly modifying this procedure, we obtain the most likely assignment according to Standard Features (Gildea and Jurafsky, 2002) PHRASE TYPE: Syntactic Category of node PREDICATE LEMMA: Stemmed Verb PATH: Path from node to predicate POSITION: Before or after predicate? VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicate’s parent Additional Features (Pradhan et al., 2004) FIRST/LAST WORD LEFT/RIGHT SISTER PHRASE-TYPE LEFT/RIGHT SISTER HEAD WORD/POS PARENT PHRASE-TYPE PARENT POS/HEAD-WORD ORDINAL TREE DISTANCE: Phrase Type with appended length of PATH feature NODE-LCA PARTIAL PATH Path from constituent to Lowest Common Ancestor with predicate node PP PARENT HEAD WORD If </context>
<context position="11232" citStr="Gildea and Jurafsky, 2002" startWordPosition="1845" endWordPosition="1848">rent’s head word PP NP HEAD WORD/POS For a PP, retrieve the head Word / POS of its rightmost NP Selected Pairs (Xue and Palmer, 2004) PREDICATE LEMMA &amp; PATH PREDICATE LEMMA &amp; HEAD WORD PREDICATE LEMMA &amp; PHRASE TYPE VOICE &amp; POSITION PREDICATE LEMMA &amp; PP PARENT HEAD WORD Table 1: Baseline Features and classification models can be chained in a principled way, as in Equation 1. The features we used for local identification and classification models are outlined in Table 1. These features are a subset of features used in previous work. The standard features at the top of the table were defined by (Gildea and Jurafsky, 2002), and the rest are other useful lexical and structural features identified in more recent work (Pradhan et al., 2004; Surdeanu et al., 2003; Xue and Palmer, 2004). The most direct way to use trained local identification and classification models in testing is to select a labeling L of the parse tree that maximizes the product of the probabilities according to the two models as in Equation 1. Since these models are local, this is equivalent to independently maximizing the product of the probabilities of the two models for the label li of each parse tree node ni as shown below in Equation 2. P� </context>
<context position="18195" citStr="Gildea and Jurafsky, 2002" startWordPosition="3070" endWordPosition="3073">e, that extracts PATH from every node in the candidate argument sequence and concatenates it with the label of the node. Both a feature with the specific argument label is created and a feature with the generic back-off ARG label. This is similar to adding features from identification and classification models. In the case of the example candidate argument sequence above, for the node NP1 we have the features: (NPISI)-ARG1, (NPISI)-ARG When comparing a local and a joint model, we use the same set of local feature templates in the two models. Whole Label Sequence: As observed in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2004), including information about the set or sequence of labels assigned to argument nodes should be very helpful for disambiguation. For example, including such information will make the model less likely to pick multiple fillers for the same role or to come up with a labeling that does not contain an obligatory argument. We added a whole label sequence feature template that extracts the labels of all argument nodes, and preserves information about the position of the predicate. The template also includes information about the voice of the predicate. For example, this templ</context>
<context position="27608" citStr="Gildea and Jurafsky, 2002" startWordPosition="4629" endWordPosition="4633">dentify arguments in the presence of parser errors, rather than the features of our joint model, which ensure global coherence of the argument frame. We still achieve 10.7% and 18.5% error reduction for CORE arguments in F-Measure and Frame Accuracy respectively. 595 Model CORE ARGM F1 Acc. F1 Acc. Local 84.1 66.5 81.4 55.6 Joint 85.8 72.7 82.9 60.8 Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak automatically generated parse trees. 6 Related Work Several semantic role labeling systems have successfully utilized joint information. (Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution. (Pradhan et al., 2004) train a language model over label sequences. (Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels. The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of mult</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-2001.</booktitle>
<contexts>
<context position="12782" citStr="Lafferty et al., 2001" startWordPosition="2121" endWordPosition="2124">ers As discussed in previous work, there are strong dependencies among the labels of the semantic argument nodes of a verb. A drawback of local models is that, when they decide the label of a parse tree node, they cannot use information about the labels and features of other nodes in the tree. Furthermore, these dependencies are highly nonlocal. For instance, to avoid repeating argument labels in a frame, we need to add a dependency from each node label to the labels of all other nodes. A factorized sequence model that assumes a finite Markov horizon, such as a chain Conditional Random Field (Lafferty et al., 2001), would not be able to encode such dependencies. The need for Re-ranking For argument identification, the number of possible assignments for a parse tree with n nodes is 2n. This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is Pt� 201, if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such huge number of classes is infeasible if the model does not factorize due to </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4344" citStr="Marcus et al., 1993" startWordPosition="688" endWordPosition="691">eling, we assume the existence of a separate parsing model that can assign a parse tree t to each sentence, and the task then is to label each node in the parse tree with the semantic role of the phrase it dominates, or NONE, if the phrase does not fill any role. We do stress however that the joint framework and features proposed here can also be used when only a shallow parse (chunked) representation is available as in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees (Marcus et al., 1993). Possible labels of arguments in this corpus are the core argument labels ARG[0, 1, 2, 3, 4, 5], and the modifier argument labels. The core arguments ARG[3, 4, 5] do not have consistent global roles and tend to be verb specific. There are about 14 modifier labels such as ARGM-LOC and ARGM-TMP, for location and temporal modifiers respectively.&apos; Figure 1 shows an example parse tree annotated with semantic roles. We distinguish between models that learn to label nodes in the parse tree independently, called local models, and models that incorporate dependencies among the labels of multiple nodes</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2003</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="1189" citStr="Palmer et al., 2003" startWordPosition="166" endWordPosition="169">ng. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft co</context>
<context position="5578" citStr="Palmer et al., 2003" startWordPosition="895" endWordPosition="898"> models. We build both local and joint models for semantic role labeling, and evaluate the gains achievable by incorporating joint information. We start by introducing our local models, and later build on them to define joint models. 3 Local Classifiers In the context of role labeling, we call a classifier local if it assigns a probability (or score) to the label of an individual parse tree node ni independently of the labels of other nodes. We use the standard separation of the task of semantic role labeling into identification and classifi&apos;For a full listing of PropBank argument labels see (Palmer et al., 2003) cation phases. In identification, our task is to classify nodes of t as either ARG, an argument (including modifiers), or NONE, a non-argument. In classification, we are given a set of arguments in t and must label each one with its appropriate semantic role. Formally, let L denote a mapping of the nodes in t to a label set of semantic roles (including NONE) and let Id(L) be the mapping which collapses L’s non-NONE values into ARG. Then we can decompose the probability of a labeling L into probabilities according to an identification model PID and a classification model PCLS. PSFL(L|t,v) = PI</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2003</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL2004.</booktitle>
<contexts>
<context position="1340" citStr="Pradhan et al., 2004" startWordPosition="189" endWordPosition="192">ents. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have </context>
<context position="7558" citStr="Pradhan et al., 2004" startWordPosition="1238" endWordPosition="1241">ining set with its specific label. In this way the training set for the classification models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other. Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-overlapping constraint. 3.1 Enfor</context>
<context position="10278" citStr="Pradhan et al., 2004" startWordPosition="1687" endWordPosition="1690">of ti’s nodes to NONE plus the logprobability for assigning the node t to ARG. Propagating this procedure from the leaves to the root of t, we have our most likely non-overlapping assignment. By slightly modifying this procedure, we obtain the most likely assignment according to Standard Features (Gildea and Jurafsky, 2002) PHRASE TYPE: Syntactic Category of node PREDICATE LEMMA: Stemmed Verb PATH: Path from node to predicate POSITION: Before or after predicate? VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicate’s parent Additional Features (Pradhan et al., 2004) FIRST/LAST WORD LEFT/RIGHT SISTER PHRASE-TYPE LEFT/RIGHT SISTER HEAD WORD/POS PARENT PHRASE-TYPE PARENT POS/HEAD-WORD ORDINAL TREE DISTANCE: Phrase Type with appended length of PATH feature NODE-LCA PARTIAL PATH Path from constituent to Lowest Common Ancestor with predicate node PP PARENT HEAD WORD If parent is a PP return parent’s head word PP NP HEAD WORD/POS For a PP, retrieve the head Word / POS of its rightmost NP Selected Pairs (Xue and Palmer, 2004) PREDICATE LEMMA &amp; PATH PREDICATE LEMMA &amp; HEAD WORD PREDICATE LEMMA &amp; PHRASE TYPE VOICE &amp; POSITION PREDICATE LEMMA &amp; PP PARENT HEAD WORD Ta</context>
<context position="18218" citStr="Pradhan et al., 2004" startWordPosition="3074" endWordPosition="3077">every node in the candidate argument sequence and concatenates it with the label of the node. Both a feature with the specific argument label is created and a feature with the generic back-off ARG label. This is similar to adding features from identification and classification models. In the case of the example candidate argument sequence above, for the node NP1 we have the features: (NPISI)-ARG1, (NPISI)-ARG When comparing a local and a joint model, we use the same set of local feature templates in the two models. Whole Label Sequence: As observed in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2004), including information about the set or sequence of labels assigned to argument nodes should be very helpful for disambiguation. For example, including such information will make the model less likely to pick multiple fillers for the same role or to come up with a labeling that does not contain an obligatory argument. We added a whole label sequence feature template that extracts the labels of all argument nodes, and preserves information about the position of the predicate. The template also includes information about the voice of the predicate. For example, this template will be instantiate</context>
<context position="27720" citStr="Pradhan et al., 2004" startWordPosition="4648" endWordPosition="4652">coherence of the argument frame. We still achieve 10.7% and 18.5% error reduction for CORE arguments in F-Measure and Frame Accuracy respectively. 595 Model CORE ARGM F1 Acc. F1 Acc. Local 84.1 66.5 81.4 55.6 Joint 85.8 72.7 82.9 60.8 Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak automatically generated parse trees. 6 Related Work Several semantic role labeling systems have successfully utilized joint information. (Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution. (Pradhan et al., 2004) train a language model over label sequences. (Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels. The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of i</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of HLT/NAACL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="7327" citStr="Pradhan et al., 2005" startWordPosition="1203" endWordPosition="1206">els, but use the decomposition for efficiency of training. The identification models are trained to classify each node in a parse tree as ARG or NONE, and the classification models are trained to label each argument node in the training set with its specific label. In this way the training set for the classification models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 A problem with this approach is that a maximizing labeling of the nodes could possib</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James Martin, and Dan Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
<author>Dav Zimak</author>
<author>Yuancheng Tu</author>
</authors>
<title>Semantic role labeling via generalized inference over classifiers.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="27791" citStr="Punyakanok et al., 2004" startWordPosition="4660" endWordPosition="4663">ror reduction for CORE arguments in F-Measure and Frame Accuracy respectively. 595 Model CORE ARGM F1 Acc. F1 Acc. Local 84.1 66.5 81.4 55.6 Joint 85.8 72.7 82.9 60.8 Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak automatically generated parse trees. 6 Related Work Several semantic role labeling systems have successfully utilized joint information. (Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution. (Pradhan et al., 2004) train a language model over label sequences. (Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels. The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies. 7 Conclusions Reflecting</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, Tu, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zimak, and Yuancheng Tu. 2004. Semantic role labeling via generalized inference over classifiers. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL-2003.</booktitle>
<contexts>
<context position="7598" citStr="Surdeanu et al., 2003" startWordPosition="1244" endWordPosition="1248">his way the training set for the classification models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other. Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-overlapping constraint. 3.1 Enforcing the Non-overlapping Constraint Here</context>
<context position="11371" citStr="Surdeanu et al., 2003" startWordPosition="1869" endWordPosition="1872">LEMMA &amp; PATH PREDICATE LEMMA &amp; HEAD WORD PREDICATE LEMMA &amp; PHRASE TYPE VOICE &amp; POSITION PREDICATE LEMMA &amp; PP PARENT HEAD WORD Table 1: Baseline Features and classification models can be chained in a principled way, as in Equation 1. The features we used for local identification and classification models are outlined in Table 1. These features are a subset of features used in previous work. The standard features at the top of the table were defined by (Gildea and Jurafsky, 2002), and the rest are other useful lexical and structural features identified in more recent work (Pradhan et al., 2004; Surdeanu et al., 2003; Xue and Palmer, 2004). The most direct way to use trained local identification and classification models in testing is to select a labeling L of the parse tree that maximizes the product of the probabilities according to the two models as in Equation 1. Since these models are local, this is equivalent to independently maximizing the product of the probabilities of the two models for the label li of each parse tree node ni as shown below in Equation 2. P� SRL(L|t, v) = � PID(Id(li)|t, v) (2) nzEt �� PCLS(li|t, v, Id(li)) nzEt 591 a product of local identification and classification models. We</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings ofACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative model for semantic role labeling.</title>
<date>2003</date>
<booktitle>In Proceedings ofECML-2003.</booktitle>
<contexts>
<context position="2123" citStr="Thompson et al., 2003" startWordPosition="316" endWordPosition="319">e features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument. Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure. We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models. To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches. We present performance results on the February 2004 version of PropBank on gold-s</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>Cynthia A. Thompson, Roger Levy, and Christopher D. Manning. 2003. A generative model for semantic role labeling. In Proceedings ofECML-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP-2004.</booktitle>
<contexts>
<context position="1363" citStr="Xue and Palmer, 2004" startWordPosition="193" endWordPosition="196">uild a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior </context>
<context position="7644" citStr="Xue and Palmer, 2004" startWordPosition="1252" endWordPosition="1255"> models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other. Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-overlapping constraint. 3.1 Enforcing the Non-overlapping Constraint Here we describe a fast exact dynamic programming </context>
<context position="10739" citStr="Xue and Palmer, 2004" startWordPosition="1760" endWordPosition="1763">cate? VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicate’s parent Additional Features (Pradhan et al., 2004) FIRST/LAST WORD LEFT/RIGHT SISTER PHRASE-TYPE LEFT/RIGHT SISTER HEAD WORD/POS PARENT PHRASE-TYPE PARENT POS/HEAD-WORD ORDINAL TREE DISTANCE: Phrase Type with appended length of PATH feature NODE-LCA PARTIAL PATH Path from constituent to Lowest Common Ancestor with predicate node PP PARENT HEAD WORD If parent is a PP return parent’s head word PP NP HEAD WORD/POS For a PP, retrieve the head Word / POS of its rightmost NP Selected Pairs (Xue and Palmer, 2004) PREDICATE LEMMA &amp; PATH PREDICATE LEMMA &amp; HEAD WORD PREDICATE LEMMA &amp; PHRASE TYPE VOICE &amp; POSITION PREDICATE LEMMA &amp; PP PARENT HEAD WORD Table 1: Baseline Features and classification models can be chained in a principled way, as in Equation 1. The features we used for local identification and classification models are outlined in Table 1. These features are a subset of features used in previous work. The standard features at the top of the table were defined by (Gildea and Jurafsky, 2002), and the rest are other useful lexical and structural features identified in more recent work (Pradhan et </context>
<context position="21294" citStr="Xue and Palmer (2004)" startWordPosition="3573" endWordPosition="3576">]ARG2”. When classifying the NP in object position, it is useful to know whether the following argument is a PP. If yes, the NP will more likely be an ARG1, and if not, it will more likely be an ARG2. A feature template that captures such information extracts, for each argument node, its phrase type and label in the context of the phrase types for all other arguments. For example, the instantiation of such a template for [a reimbursement] in (ii) would be [ voice:active NP,PRED,NP-ARG1,PP] We also add a template that concatenates the identity of the predicate lemma itself. We should note that Xue and Palmer (2004) define a similar feature template, called syntactic frame, which often captures similar information. The important difference is that their template extracts contextual information from noun phrases surrounding the predicate, rather than from the sequence of argument nodes. Because our model is joint, we are able to use information about other argument nodes when labeling a node. Final Pipeline Here we describe the application in testing of a joint model for semantic role labeling, using a local model P`SRL, and a joint re-ranking model PrSRL. P`SRL is used to generate top N non-overlapping j</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings ofEMNLP-2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>