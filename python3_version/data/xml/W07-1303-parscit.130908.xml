<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<note confidence="0.468161">
BayesianIdentificationofCognatesandCorrespondences
</note>
<author confidence="0.597166">
T. Mark Ellison
</author>
<affiliation confidence="0.8214515">
Linguistics, University of Western Australia,
and Analith Ltd
</affiliation>
<email confidence="0.960087">
mark@markellison.net
</email>
<sectionHeader confidence="0.952061" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99671690625">
ThispaperpresentsaBayesianapproach
tocomparinglanguages:identifyingcog-
natesandtheregularcorrespondences
thatcomposethem.Asimplemodelof
languageisextendedtoincludetheseno-
tionsinanaccountofparentlanguages.
Anexpressionisdevelopedforthepos-
teriorprobabilityofchildlanguageforms
givenaparentlanguage.Bayes&apos;Theo-
remoffersaschemaforevaluatingchoices
ofcognatesandcorrespondencestoex-
plainsemanticallymatcheddata.Anim-
plementationoptimisingthisvaluewith
gradientdescentisshowntodistinguish
cognatesfromnon-cognatesindatafrom
PolishandRussian.
Modern historical linguistics addresses ques-
tions like the following. How did language
originate? What were historically-recorded lan-
guages like? How related are languages? What
were the ancestors of modern languages like?
Recently, computation has become a key tool in
addressing such questions.
Kirby (2002) gives an overview of current cur-
rent work on how language evolved, much of it
based on computational models and simulations.
Ellison (1992) presents a linguistically motivated
method for classifying consonants as consonants
or vowels. An unexpected result for the dead
languageGothicprovidesaddedweighttoone
oftwocompetingphonologicalinterpretationsof
theorthographyofthisdeadlanguage.
</bodyText>
<page confidence="0.973224">
15
</page>
<bodyText confidence="0.988995741573034">
Other recent work has applied computational
methods for phylogenetics to measuring linguis-
tic distances, and/or constructing taxonomic
trees from distances between languages and di-
alects (Dyen et al., 1992; Ringe et al., 2002; Gray
and Atkinson, 2003; McMahon and McMahon,
2003; Nakleh et al., 2005; Ellison and Kirby,
2006).
A central focus of historical linguistics is the
reconstruction of parent languages from the ev-
idence of their descendents. In historical lin-
guistics proper, this is done by the compara-
tive method (Jeffers and Lehiste, 1989; Hock,
1991) in which shared arbitrary structure is as-
sumed to reflect common origin. At the phono-
logical level, reconstruction identifies cognates
and correspondences, and then constructs sound
changes which explain them.
This paper presents a Bayesian approach to
assessing cognates and correspondences. Best
sets of cognates and correspondences can then
be identified by gradient ascent on this evalua-
tion measure. While the work is motivated by
the eventual goal of offering software solutions
to historical linguistics, it also hopes to show
thatBayes&apos;theoremappliedtoanexplicit,sim-
plemodeloflanguagecanleadtoaprincipled
and tractable method for identifying cognates.
The structure of the paper is as follows. The
next section details the notions of historical lin-
guistics needed for this paper. Section 2 for-
mally defines a model of language and parent
language. The subsequent section situates the
work amongst similar work in the literature,
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 15–22,
Prague, June 2007. c�2007 Association for Computational Linguistics
makinguseofconceptsdescribedintheearlier
sections.Section4describesthecalculationof
the probability of wordlist data given a hypoth-
esised parent language. This is combined with
Bayes&apos; theorem and gradient search in an algo-
rithm to find the best parent language for the
data. Section 5 describes the results of apply-
ing an implementation of the algorithm to data
from Polish and Russian. The final section sum-
marises the paper and suggests further work.
1Cognates,Correspondencesand
Reconstruction
Intheneo-Grammarianmodeloflanguage
change, a population speaking a uniform lan-
guage divides, and then the two populations un-
dergo separate language changes.
Word forms with continuous histories in re-
spective daughter languages descending which
from a common word-form ancestor are called
cognate, no matter what has happened to their
semantics. Cognate word forms may have un-
dergone deformations to make them less simi-
lar to each other, these deformations resulting
from regular, phonological changes. Note that
inthefieldsofappliedlinguistics,secondlan-
guageacquisition,andmachinetranslation,the
termcognateisusedtomeananywordsthatare
phonologicallysimilartoeachother.Thisisnot
thesensemeanthere.
Phonological change produces modifications
to the segmental inventory, replacing one seg-
ment by another in all or only some contexts.
This sometimes has the effect of collapsing seg-
ment types together. Other changes may di-
vide one segment type into two, depending on
a contextual condition. The relation of parent-
language segments to daughter-language seg-
ments is, usually, a many-to-many relation.
Parent-child segmental relations are reflected
in the correspondences between segment in-
ventories in the daughter languages. Cor-
respondences are pairings of segments from
daughter languages which have derived from
a common parent segment. For example, p
in Latin frequently corresponds to f in En-
glish, as in words like pater and father. Both
segmentshavedevelopedfroma(postulated)
Proto-IndoEuropean*p.Becausecorrespon-
dencesonlyoccurbetweencognates,identify-
ingthetwoisoftenabootstrapprocess:cor-
ralingcognateshelpsfindmorecorrespondences,
and forms sharing a number correspondences are
probably cognate.
</bodyText>
<subsectionHeader confidence="0.686258">
2FormalStructures
</subsectionHeader>
<bodyText confidence="0.999012428571428">
Themethodpresentedinthispaperisbasedon
a formal model of language. This is described in
section 2.1. The subsequent section extends the
model to define a parent language, whose seg-
mental inventory is correspondences and whose
lexicon is cognates linking two descendent lan-
guages.
</bodyText>
<subsectionHeader confidence="0.899208">
2.1 Language model
</subsectionHeader>
<bodyText confidence="0.9926265">
The language model is based on three assump-
tions.
</bodyText>
<figure confidence="0.946745571428571">
Assumption1Thereisauniversal,discrete
setMofmeanings.
Assumption2AlanguageLhasitsownsetof
segmentsE(L).
Assumption 3 The lexicon A of a language L is
a partial map of meanings to strings of segments
A : M → E(L)*.
</figure>
<bodyText confidence="0.999204461538462">
On the basis of these assumptions, we can de-
fine a language L to be a triple (M, E(L), A(L))
of meanings, segments and mappings from mean-
ings onto strings of segments.
For example, consider written Polish. The
set of meanings contains concepts as To TAKE-
perfect-infinitive, TREE-nominative-singular,
and so on. The segmental inventory contains
the 32 segments agbccdegfghijkl
I m n n o o p r s s t u w y z z z, ignoring
capitalisation. The lexicon matches meanings to
strings of segments, To TAKE-perfect-infinitive
to wziQc, TREE-nominative-singular to drzewo.
</bodyText>
<subsectionHeader confidence="0.950545">
2.2 Parent language model
</subsectionHeader>
<construct confidence="0.80306">
Definition 1 A degree-(u, v) correspondence
between L1 and L2 is a pair of strings (s, t) ∈
E(L1) × E(L2) over the segments of L1 and L2
</construct>
<page confidence="0.998238">
16
</page>
<bodyText confidence="0.990716409090909">
respectively,withlengthsatleastuandnomore
thanv.
As an example of a correspondence, consider
the pair of small strings from Polish and Russian,
(c,Tb). This is a degree-(1,2) correspondence
because its members have lengths as low as one
and as high as two. It is also a degree-(u, v)
correspondence for any uG 1 and v &gt; 2.
Any correspondence can be mapped onto its
components by projection functions.
Definition 2 The projections 7r1 and 7r2 map
a correspondence ( s, t) onto its first 7r1 (s, t) = s
or second 7r2(s, t) = tcomponent string respec-
tively.
The first projection function will map (c,Tb)
onto c, while the second maps (c,Tb) onto Tb.
Correspondences can be formed into strings.
These strings also have projections.
Definition 3 The projections 7r1 and 7r2 map
a string of correspondences c1.. ck onto the con-
catenation of the projections of each correspon-
dence.
</bodyText>
<equation confidence="0.995398666666667">
7r 1 ( c 1 .. c ) =
k 7r1(c1)7r1(c2)..7r1(ck),
7r2(c1..ck) = 7r2(c1)7r2(c2)..7r2(ck)
</equation>
<bodyText confidence="0.993726333333333">
Suppose we sequence four correspondences
into the string (w,B) (z,3) (i4,A) (C,Tb). This
string has first and second projections, wzigc
and B3ATb, formed by concatenating the respec-
tive projections of each correspondence.
We can now define a parent language.
Definition 4 A degree-(u, v) parent L0 of two
languages L1 , L2 is a triple (M, E(L0), A(L0))
where E(L0) is a set of degree-(u, v) correspon-
dences between L1 and L2, excluding the pair of
null strings, and A(L0) is a partial mapping from
M onto E(L0) which obeys
</bodyText>
<equation confidence="0.630309">
7r1 o A(L0) C A(L1), 7r2 o A(L0) C A(L2)
</equation>
<bodyText confidence="0.99856103125">
Thecirclestandsforfunctioncomposition.
Continuing our past example, we will focus
on the two meanings TO TAKE-perfect-infinitive
and TREE-nominative-singular. The segment in-
ventory for the parent language contains degree-
(0, 2) correspondences: (,e), (c,Tb), (d,q),
(e,e), (i�,A) (o,o), (rz,p), (w,B), (z,3). The
lexical function maps TO TAKE-perfect-infinitive
onto the string of correspondences (w,B) (z,3)
(iq,A) (c,Tb) while TREE-nominative-singular
maps to (d,R) (,e) (rz,p) (e,e) (w,B) (o,o).
The parent language condition is verified by
checking the projections of the two correspon-
dence strings. The first string has projec-
tions wzigc and B3ATb, which are forms for
the meaning TO TAKE-perfect-infinitive in Pol-
ish and Russian respectively. The second string
has projections drzewo and gepeBo, which are
forms for the meaning TREE-nominative-singular
in Polish and Russian respectively. So the pro-
jection condition is satisfied. If the lexical func-
tion is only defined on these two meanings, then
thisisavalidparentlanguage.
It is worth emphasising that the projection
condition for qualifying as a parent language ap-
plies only for those meanings for which the par-
ent lexical mapping is defined. The correspond-
ing forms in the child languages are said to be
cognate in this model. Where no parent form
isreconstructed,theformsarenotcognate,and
are to be accounted for in some way other than
the parent language.
</bodyText>
<subsectionHeader confidence="0.472225">
3RelatedWork
</subsectionHeader>
<bodyText confidence="0.9964068125">
Thecurrentworkis,ofcourse,farfromthefirst
toseektoidentifycognatesand/orcorrespon-
dences.Hereisanabbreviatedoverviewofpre-
viousworkinthefield&apos;.Moredetailedsurveys
canbefoundinchapter3ofKondrak&apos;s(2002)
PhDthesisorLowe&apos;sonlinesurvey2ofpriorart
inthisfield.
In perhaps the first computational work on
historical linguistics, Kay (1964) described an al-
gorithm for determining correspondences given
a list of cognate pairs across two daughter lan-
guages. His method seeks to find the smallest set
&apos;An anonymous reviewer suggests that the current
work shares features with that of Kessler (2001). I have
been unable to access this book in time to include dis-
cussion of it in this paper.
</bodyText>
<footnote confidence="0.780429">
2linguistics.berkeley.edu/ ˜jblowe/REWWW/PriorArt.html
</footnote>
<page confidence="0.994696">
17
</page>
<bodyText confidence="0.976640928571429">
ofcorrespondenceswhichallowsadegree-(1, oo)
alignment for each cognate pair. Unfortunately,
the complexity of the problem has precluded its
application to significant daa sets.
Frantz (1970) developed a PL/1 programming
which returned numerical evaluations of corre-
spondences and cognacy, given a list of possi-
ble cognate word-pairs. Each word pair must be
supplied as a degree-(0,1) reconstruction, that
is, aligning single segments with each other or
withgaps.
Guy (1984; 1994) presented a program called
COGNATE which finds regular correspondences
and identifies cognates using statistical tech-
niques.
For his Master&apos;s, Broza (1998) developed
MDL-based software called candid which identi-
fies correspondences from cognates and expresses
these as contextual phonological transformation
rules.
Kondrak&apos;s (2002) doctoral dissertation com-
bines phonological and semantic similarity meth-
ods with correspondance-learning. The algo-
rithms for learning correspondences are taken
from Melamed&apos;s (2000) probabilistic methods
for identifying word-word translation equiva-
lence. These methods, like the current work,
are Bayesian. Because Melamed&apos;s problem seeks
partial rather than complete explanation of the
inputs in terms of correspondences, the match-
ing problem is somewhat more difficult theoret-
ically. As a result, he does not arrive at the de-
composition of the sum of the probability of two
inputsgiventhesetofpossiblecorrespondences,
approximating this with a high probability align-
ment.
4ConditionalProbabilityofthe
Data
The core of any Bayesian model is the condi-
tional probability of the data given the hypoth-
esis. This section details how probabilities as-
signed to data, and the assumptions on which
this assignment is based.
The data is the mapping of meanings onto
forms in two daughter languages. If those two
languages are L1 and L2, we want to determine
P(A(L1), A(L2)jh).Thenatureofhwillbedis-
cussedinsection4.6.
Forbrevity,wewillwriteAiforA(Li).
4.1Meaningindependence
The first step in defining the conditional prob-
ability of the data is to decompose it into
meaning-by-meaning probabilities. This can be
achieved by adopting the following two assump-
tions.
Assumption 4 In a given language, the forms
for different meanings are selected indepen-
dently.
This assumption states that within a single
language choosing, for example, a form wzigc
for meaning TO TAKE-perfect-infinitive is no help
in predicting the form which expresses TREE-
nominative-singular.
Assumption 5 Across different languages, the
forms corresponding to different meanings are
independent.
According to this assumption, the Polish word
wzigc and the Russian word B3ÿTb can be
structurally dependent because they express the
same meaning. In contrast, we can only ex-
pect a chance relationship between the Rus-
sian word B3ÿTb meaning TO TAKE-perfect-
infinitive, and the Polish word drzewo express-
ing TREE-nominative-singular.
Together, these two assumptions imply that
the only dependencies possible between any four
forms expressing the two meanings m1 and m2 in
two languages L1 and L2 are between A(m1) and
A(m1) on the one hand and A(m2) and A(m2) on
the other.
Consequently the probability of generating the
word forms in two languages can be decomposed
into the product of generating the two language-
particular forms for each meaning.
</bodyText>
<equation confidence="0.874933">
P(A1, A2jh) = 11 P(A1(m), A2(m)jh)
MEM
</equation>
<page confidence="0.821671">
18
</page>
<bodyText confidence="0.986387482758621">
4.2Cognacyandindependence
Thenextassumptionholdsthatstructuralcor-
relationbetweencorrespondingformsshouldbe
explainedasresultingfromcognacy.
Assumption 6 Across different languages,
forms corresponding to the same meaning are
dependent only if the forms are cognate.
If the words for a particular meaning do not
derive from a common ancestral form, then they
are uncorrelated. To return to our Polish and
Russian examples, we can expect dependencies
in structure between the cognate words drzewo
and pgåðåâî. But we should expect no such cor-
relation in the non-cognate pair pomarancza
and àïåJlüñIií meaning ORANGE-nominative-
singular.
Let us write Mi for the domain of the lexical
function in language Li. This is the set of mean-
ings for which this language has defined a word
form. The set of cognates is the domain of the
lexical function of the parent language, M0. We
can decompose the evidential words into three
sets:M0ofcognates,M1 \ M0 of meanings only
expressed in language L1, and M2 \M0 of mean-
ings only expressed in language L2. Words in the
second and third categories are non-cognate, and
so probabilistically independent of each other.
The conditional probability of the data can
thus be expressed as follows.
</bodyText>
<equation confidence="0.99909275">
P (A1, A2|h) = 11 P(A1(m), A2(m)|h)
mEM0
11 P(A1(m)|h) 11 P(A2(m)|h)
mEM1\M0 mEM2\M0
</equation>
<subsectionHeader confidence="0.910006">
4.3 Probability of a word
</subsectionHeader>
<bodyText confidence="0.998741">
We now turn to the probability of generating a
string in a language. The first assumption de-
fines the distribution over word-length.
</bodyText>
<construct confidence="0.463186666666667">
Assumption 7 The probability of a word hav-
ing a particular length is negative exponential in
that length.
</construct>
<bodyText confidence="0.960244722222222">
The second assumption allows segment prob-
ability to depend only on the segment identity,
and not on its neighbourhood.
Assumption8Segmentchoiceiscontext-
independent.
These two assumptions together imply that
the probability of strings is determined by a fixed
distribution over E(Li) U {#}, where # is an
end-of-word marker. For the descendent lan-
guages, this distribution can be taken as the rela-
tive frequencies of the segments and end-of-word
marker.DenotethisdistributionforlanguageLi
byfi.
The probability of generating a word in a lan-
guage, given relative frequencies fi, is the prod-
uct of the relative frequencies for each lettern in
the word, multiplied by the relative frequency of
the end-of-word marker.
</bodyText>
<equation confidence="0.9987835">
P(Ai(m)|h) = fi(#) 11 fi(a)
aEAi(m)
</equation>
<bodyText confidence="0.96021925">
Note that this expression only holds for words
that are independent of all others, such as com-
ponents of non-cognate pairs.
4.4 Probability of generating a cognate
pair
The probability of generating a cognate pair of
words is similar to the above, because descen-
dent forms are deterministically derivable from
the parent forms. If (A1 (m), A2 (m)) are a pair of
cognatesderivedfromanancestralformA0(m),
thenthereisunitprobabilitythatthedescen-
dentformsarewhattheyaregiventheparent:
</bodyText>
<equation confidence="0.819171">
P(A1(m),A2(m)|A0(m)) = 1.
</equation>
<bodyText confidence="0.997879875">
Since a cognate pair is derivable from a par-
ent form, the probability of a cognate pair is
the sum of the probabilities of all parent forms
which will generate the two descendents. Write
W (m) = W (A1 (m), A2 (m)) for the set of pos-
sible correspondence strings in the parent which
project onto wordforms A1(m) and A2(m). Then
the probability of the word pair is given by:
</bodyText>
<equation confidence="0.9993375">
P(A1(m),A2(m)|h) = E P(A0(m) = s|h)
sEW (m)
</equation>
<bodyText confidence="0.99517475">
Thesummationposesaslightproblem,however.
Howdowesumoverallpossiblestringswith
givenprojections?Fortunately,wecandecom-
posethesummation.Startbyrecognisingthat
</bodyText>
<page confidence="0.998167">
19
</page>
<bodyText confidence="0.9983742">
theparentlanguageisalsoalanguage,andso
theprobabilityofformsinthelanguageisde-
terminedbyadistributionoversegments in
thiscasecorrespondences andtheend-of-word
marker.Forconsistency,wecallthisdistribution
f0.
The only parent form which projects onto two
empty strings is the empty string, consisting
only of the end-of-word marker. For brevity,
we will drop the lambdas, writing P(x, y|h) for
</bodyText>
<equation confidence="0.941923">
P(A1(m) = x, A2(m) = y|h)
</equation>
<bodyText confidence="0.9997374">
The recursive definition of W in terms of dis-
joint unions and concatenation can be trans-
formed into a recursive definition for the proba-
bility P0(s, t|h) of constructing a member of the
set. Disjoint union is replaced by summation,
concatenation by product. The probability of
an individual correspondence (a, c) is its (un-
known) relative frequency f0(a, c) in the parent
language. Once again, we hide the implicit u, v
parameters.
</bodyText>
<equation confidence="0.998364666666667">
P0(0, 0|h) = f0(#)
P(0, 0|h) = f0(#) E f0(a, c)P(b, d|h)
P0(s, t|h) =
</equation>
<bodyText confidence="0.9987006">
We assume, without loss of generality, that
the segmental inventory of the parent language
consists of all degree-(u, v) correspondences be-
tween L1 and L2. Parent segments which are
never used can be excluded by giving them zero
relative frequency in f0.
The function Pre(s; u, v) returns the set of bi-
nary divisions (a, b) of the string s, such that the
lengthofthefirstpartaisatleastuandatmost
v.
</bodyText>
<equation confidence="0.814347">
Pre(s; u, v) = {(a, b)|ab = s, m &lt; |a |&lt; n}
</equation>
<bodyText confidence="0.986933714285714">
With this function, we can recursively define a
function W (s, t; u, v) on pairs of strings (s, t)
v
whichreturnsthesetofalldegree-(u, )parent
language strings which project onto s and t. For
brevity, we will treat all u, v arguments as im-
plicit.
</bodyText>
<equation confidence="0.995045">
W(0,0) = {0}
</equation>
<bodyText confidence="0.99947025">
Bydefinition,theonlyparentlanguagestring
whichcanmapontotheemptystringinboth
descendentsistheemptystring.
The recursive step breaks the strings s and
t into all possible prefixes a and c respectively.
The correspondence (a, c) is then preposed on all
strings returned by W when it is applied to the
remainders of s and t.
</bodyText>
<equation confidence="0.991981666666667">
W (s, t) = � 1� (a, c)W (b, d)
(a,b)EPre(s) (c,d)EPre(t)
NotethatthisisthesetW (m)wedefinedearlier.
W (m) = W (A1(m), A2(m); u, v)
(a,b)EPre(s) (c,d)EPre(t)
4.5Probabilityofaform-pair
</equation>
<bodyText confidence="0.9997986">
We now have the pieces to specify the probabil-
ity of finding any particular form as the form-
pair for the descendent languages. The prob-
ability of the pair in the case of cognacy is
P0 (A1 (m), A2 (m)  |h). If the pair are not cognate,
then they are independent, and their probabil-
ity is P1(A1(m))P2(A2(m)|h). If we write c(m|h)
for the likelihood that the pair is cognate, we
can combine these two values to given a total
probabilityofthetwoforms.
</bodyText>
<equation confidence="0.8737585">
P0(A1(m), A2(m)|h)c(m|h)
+P1(A1(m))P2(A2(m)|h)(1.0 − c(m|h))
</equation>
<bodyText confidence="0.985990588235294">
Because the word-pairs are independent (as-
sumption 4), the product of the above probabil-
ity for each meaning m gives the probability of
the data given the hypothesis.
4.6Hypothesis
One burning question remains, however. What
is the hypothesis? The simple answer is that it
is exactly those free variables in the specification
of the probability of the data
There were two groups of unknowns in the
probability of the data. The first is the rela-
tive frequency f0 assigned to correspondences in
parent-language forms. The second is the like-
lihood of cognacy c, a vector of values between
zero and one indexed by meanings.
A hypothesis is therefore any setting of values
for the pair of vectors ( f, c).
</bodyText>
<page confidence="0.986678">
20
</page>
<bodyText confidence="0.9999564">
Note that while the degree variables u, v were
not fixed in the above derivation, they will be
held constant for any particular search, and thus
do not define a dimension in the hypothesis
space.
</bodyText>
<subsectionHeader confidence="0.934429">
4.7 Search
</subsectionHeader>
<bodyText confidence="0.988937775510204">
In this section, we have derived P(D|h), the like-
lihood of our data given a hypothesis.
For simplicity, we choose a flat prior over hy-
potheses, rendering the MAP Bayesian approach
an instance of maximum likelihood determina-
tion. The value for the likelihood is differentiable
in each of the parameters. Consequently, gradi-
ent descent can be used to find the hypothesis
which maximises the probability of the data.
5Results
Inconstructingthemethod,wemadeanumber
ofassumptionsaboutindependenceofforms.It
issensiblethatfortesting,themethodisapplied
todatathatconformsreasonablywelltothese
assumptions. The alternative is to apply it to
data which contradicts its fundamental assump-
tions, consequently hampering its effectiveness.
5.1Thedata
PolishandRussianwerechosentoprovidethe
databecausetheyapproximatelyobeyassump-
tion6:wordshavedependentstructuresifand
onlyiftheyarecognate.Forourtwolan-
guages,thismeansthatborrowingsfromcom-
monsourcesareuncommon(numbering45in
ourdataset),atleastincomparisonwiththe
numberofcognates(numbering156).
The data was harvested from two online
dictionaries (Wordgumbo, 2007a; Wordgumbo,
2007b), one English-Polish, the other English-
Russian. Multiple translations were simplified,
with the shortest translation retained. The En-
glish glosses were used as the meanings for the
words. Where the gloss contained a capital let-
ter, indicating a proper noun, this was elimi-
nated from the data.
The data should also conform to assumption
4, that words for different meanings with a lan-
guage are independent. So where two meanings
in the data sets were realised with the same form,
thesemeaningsweredeemedtobestructurally
dependent,andsoonlythefirstwasretainedin
thewordlist.
The remaining data contains 407 aligned
Polish-Russian word pairs.
Polish and Russian both use a great deal of
derivational and inflectional morphology. The
simple language model used here does not take
this into account, so this will be a disturbing
influence on the results.
</bodyText>
<subsectionHeader confidence="0.951154">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.988455142857143">
The aligned wordlists were hand-tagged as cog-
nate, common borrowing or non-cognate. A per-
missive rule of cognacy was used: if the roots
of words in the two languages were cognate,
theywerecognate,evenifrepresentedwithnon-
cognatederivationaland/orinflectionalmor-
phology.
</bodyText>
<figureCaption confidence="0.5529005">
Figure 1 shows the evaluation of the program&apos;s
performance on the data.
</figureCaption>
<figure confidence="0.941542333333333">
Borrowings as: cognates non-cognates
Found f 162 119
Missed m 41 37
Errant e 6 49
Accuracy f /(f + e) 96% 71%
Recall f /(f + m) 81% 76%
</figure>
<figureCaption confidence="0.9497012">
Figure 1: Evaluation of program performance
on 407 meaning-matched pairs of Polish-Russian
words. Common borrowings are scored as cog-
nates in the first column, non-cognates in the
second.
</figureCaption>
<bodyText confidence="0.999982111111111">
The scores show that the method works well
in identifying cognates, particularly if common
borrowings are accepted as cognates, or excluded
manually. If common borrowings are scored as
non-cognates, then the accuracy falls.
Of the correspondences found between Polish
and Russian, 67 have a phonological basis. The
remaining 27 result from mismatch morphology
in cognates or differences in common borrowings.
</bodyText>
<sectionHeader confidence="0.254778" genericHeader="acknowledgments">
6Conclusion
</sectionHeader>
<bodyText confidence="0.779254333333333">
Thispaperhaspresentedamodeloflanguage
whichallowsthecalculationoftheposterior
probabilityofformsarisinginthecaseswhere
</bodyText>
<page confidence="0.997364">
21
</page>
<bodyText confidence="0.987411666666667">
theyarecognate,andwheretheyarenot.Bayes&apos;
theoremrelatestheseprobabilitiestotheposte-
riorlikelihoodofparticularcorrespondencesand
cognacyrelationships.Gradientdescentcanbe
usedtosearchthisspaceforthebestdistribution
overcorrespondences,andbestcognacyevalua-
tionsformeaning-pairedwords.Theapplication
to data from Polish and Russian shows remark-
able success identifying both cognates and non-
cognates.
Future work will proceed by relaxing con-
straints on the parent language. The parent in-
ventory will be widened to include multisegment
correspondences. Multiple parent languages will
be permitted, to the end of separating borrow-
ings from cognates. Finally, richer models of
language,incorporatingsyllablestructure,will
allowmoreinformationtoidentifycognates.
</bodyText>
<sectionHeader confidence="0.986537" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999469">
GilBroza.1998.Inter-languageregularity:the
transformationlearningproblem.Master&apos;sthesis,
InstituteofComputerScience,HebrewUniversity
ofJerusalem,October.
IsidoreDyen,JosephB.Kruskal,andPaulBlack.
1992. An Indo-European classification: a lexico-
statistical experiment. Transactions of the Amer-
ican Philosophical Society, 82(5).
T.MarkEllisonandSimonKirby.2006.Measuring
languagedivergencebyintra-lexicalcomparison.
InACL,pages273~280,Sydney.
T.MarkEllison.1992.TheMachineLearningof
Phonological Structure. Ph.D. thesis, University
of Western Australia.
DonaldG.Frantz.1970.APL/1programtoassist
the comparative linguist. Communications of the
ACM, 13(6):353~356.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the ana-
tolian theory of indo-european origin. Nature,
426:435~439.
JacquesB.M.Guy.1984.Analgorithmforidenti-
fyingcognatesbetweenrelatedlanguages.In10th
International Conference on Computational Lin-
guistics and 22nd Annual Meeting of the Asso-
ciation for Computational Linguistics. Available
online as http://acl.ldc.upenn.edu/P/P84/P84-
1091.pdf.
Jacques B. M. Guy. 1994. An algorithm for identi-
fying cognates in bilingual wordlists and its appli-
cability to machine translation. Journal of Quan-
titative Linguistics, 1(1):35~42.
Hans Heinrich Hock. 1991. Principles of Historical
Linguistics. Mouton de Gruyter, Berlin.
Robert J. Jeffers and Ilse Lehiste. 1989. Princi-
ples and Methods for Historical Linguistics. MIT
Press, Cambridge, MA.
Martin Kay. 1964. The logic of cognate recognition
inhistoricallinguistics.TechnicalReportRM-
4224-PR,TheRANDCorporation,SantaMonica,
CA,September.
BrettKessler.2001.TheSignificanceofWordLists.
CSLIPublications,Stanford,CA.
SimonKirby.2002.Naturallanguagefromartificial
life. Artificial Life, 8(2):185~215.
Grzegorz Kondrak. 2002. Algorithms for Lan-
guage Reconstruction. Ph.D. thesis, University of
Toronto.
AprilMcMahonandRobertMcMahon.2003.Find-
ingfamilies:quantitativemethodsinlanguage
classification.TransactionsofthePhilologicalSo-
ciety,101:7~55.
I.DanMelamed.2000.Modelsoftranslational
equivalence among words. Computational Lin-
guistics, 26(2):221~249.
Luay Nakleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of
phylogenetic reconstruction methods on an ie
dataset. Transactions of the Philological Society,
103(2):171~192.
D. Ringe, Tandy Warnow, and A. Taylor.
2002. Indo-European and computational cladis-
tics. Transactions of the Philological Society,
100(1):59~129.
Wordgumbo.2007a./ie/sla/pol/erengpol.htm.
Website http://www.wordgumbo.com/.
Wordgumbo. 2007b. /ie/sla/rus/erengrus.htm.
Websitehttp://www.wordgumbo.com/.
</reference>
<page confidence="0.99902">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001430">
<title confidence="0.99905">BayesianIdentificationofCognatesandCorrespondences</title>
<author confidence="0.99966">T Mark</author>
<affiliation confidence="0.8179755">Linguistics, University of Western and Analith</affiliation>
<email confidence="0.80597">mark@markellison.net</email>
<abstract confidence="0.987990909395974">ThispaperpresentsaBayesianapproach tocomparinglanguages:identifyingcognatesandtheregularcorrespondences thatcomposethem.Asimplemodelof languageisextendedtoincludethesenotionsinanaccountofparentlanguages. Anexpressionisdevelopedfortheposteriorprobabilityofchildlanguageforms givenaparentlanguage.Bayes&apos;Theoremoffersaschemaforevaluatingchoices ofcognatesandcorrespondencestoexplainsemanticallymatcheddata.Animplementationoptimisingthisvaluewith gradientdescentisshowntodistinguish cognatesfromnon-cognatesindatafrom PolishandRussian. Modern historical linguistics addresses questions like the following. How did language originate? What were historically-recorded languages like? How related are languages? What were the ancestors of modern languages like? Recently, computation has become a key tool in addressing such questions. Kirby (2002) gives an overview of current current work on how language evolved, much of it based on computational models and simulations. Ellison (1992) presents a linguistically motivated method for classifying consonants as consonants or vowels. An unexpected result for the dead languageGothicprovidesaddedweighttoone oftwocompetingphonologicalinterpretationsof theorthographyofthisdeadlanguage. 15 Other recent work has applied computational methods for phylogenetics to measuring linguistic distances, and/or constructing taxonomic trees from distances between languages and dialects (Dyen et al., 1992; Ringe et al., 2002; Gray and Atkinson, 2003; McMahon and McMahon, 2003; Nakleh et al., 2005; Ellison and Kirby, 2006). A central focus of historical linguistics is the reconstruction of parent languages from the evidence of their descendents. In historical linguistics proper, this is done by the comparative method (Jeffers and Lehiste, 1989; Hock, 1991) in which shared arbitrary structure is assumed to reflect common origin. At the phonological level, reconstruction identifies cognates and correspondences, and then constructs sound changes which explain them. This paper presents a Bayesian approach to assessing cognates and correspondences. Best sets of cognates and correspondences can then be identified by gradient ascent on this evaluation measure. While the work is motivated by the eventual goal of offering software solutions to historical linguistics, it also hopes to show thatBayes&apos;theoremappliedtoanexplicit,simplemodeloflanguagecanleadtoaprincipled and tractable method for identifying cognates. The structure of the paper is as follows. The next section details the notions of historical linguistics needed for this paper. Section 2 formally defines a model of language and parent language. The subsequent section situates the work amongst similar work in the literature, of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and pages 15–22, June 2007. Association for Computational Linguistics makinguseofconceptsdescribedintheearlier sections.Section4describesthecalculationof the probability of wordlist data given a hypothesised parent language. This is combined with Bayes&apos; theorem and gradient search in an algorithm to find the best parent language for the data. Section 5 describes the results of applying an implementation of the algorithm to data from Polish and Russian. The final section summarises the paper and suggests further work. 1Cognates,Correspondencesand Reconstruction Intheneo-Grammarianmodeloflanguage change, a population speaking a uniform language divides, and then the two populations undergo separate language changes. Word forms with continuous histories in respective daughter languages descending which from a common word-form ancestor are called cognate, no matter what has happened to their semantics. Cognate word forms may have undergone deformations to make them less similar to each other, these deformations resulting from regular, phonological changes. Note that inthefieldsofappliedlinguistics,secondlanguageacquisition,andmachinetranslation,the phonologicallysimilartoeachother.Thisisnot thesensemeanthere. Phonological change produces modifications to the segmental inventory, replacing one segment by another in all or only some contexts. This sometimes has the effect of collapsing segment types together. Other changes may divide one segment type into two, depending on a contextual condition. The relation of parentlanguage segments to daughter-language segments is, usually, a many-to-many relation. Parent-child segmental relations are reflected the correspondences between segment inin the daughter languages. respondences are pairings of segments from daughter languages which have derived from a common parent segment. For example, p in Latin frequently corresponds to f in English, as in words like pater and father. Both segmentshavedevelopedfroma(postulated) Proto-IndoEuropean*p.Becausecorrespondencesonlyoccurbetweencognates,identifyingthetwoisoftenabootstrapprocess:corralingcognateshelpsfindmorecorrespondences, and forms sharing a number correspondences are probably cognate. 2FormalStructures Themethodpresentedinthispaperisbasedon a formal model of language. This is described in section 2.1. The subsequent section extends the model to define a parent language, whose segmental inventory is correspondences and whose lexicon is cognates linking two descendent languages. 2.1 Language model The language model is based on three assumptions. 3 lexicon a language a partial map of meanings to strings of segments On the basis of these assumptions, we can dea language be a triple of meanings, segments and mappings from meanings onto strings of segments. For example, consider written Polish. The of meanings contains concepts as To TAKEperfect-infinitive, and so on. The segmental inventory contains the 32 segments agbccdegfghijkl n n o o p r s s t u w y z z z, ignoring capitalisation. The lexicon matches meanings to strings of segments, To TAKE-perfect-infinitive to wziQc, TREE-nominative-singular to drzewo. 2.2 Parent language model 1 a pair of strings the segments of 16 As an example of a correspondence, consider the pair of small strings from Polish and Russian, This is a because its members have lengths as low as one as high as two. It is also a for any Any correspondence can be mapped onto its components by projection functions. 2 projections correspondence its first = second = string respectively. The first projection function will map (c,Tb) onto c, while the second maps (c,Tb) onto Tb. Correspondences can be formed into strings. These strings also have projections. 3 projections string of correspondences the concatenation of the projections of each correspondence. 1( 1.. c = = Suppose we sequence four correspondences into the string (w,B) (z,3) (i4,A) (C,Tb). This string has first and second projections, wzigc and B3ATb, formed by concatenating the respective projections of each correspondence. We can now define a parent language. 4 two a triple a set of corresponbetween excluding the pair of strings, and a partial mapping from obeys Thecirclestandsforfunctioncomposition. Continuing our past example, we will focus on the two meanings TO TAKE-perfect-infinitive and TREE-nominative-singular. The segment infor the parent language contains degree- (,e), (c,Tb), (e,e), (i�,A) (o,o), (rz,p), (w,B), (z,3). The lexical function maps TO TAKE-perfect-infinitive onto the string of correspondences (w,B) (z,3) (iq,A) (c,Tb) while TREE-nominative-singular maps to (d,R) (,e) (rz,p) (e,e) (w,B) (o,o). The parent language condition is verified by the projections of the two corresponstrings. The first string has tions wzigc and B3ATb, which are forms for the meaning TO TAKE-perfect-infinitive in Polish and Russian respectively. The second string has projections drzewo and gepeBo, which are forms for the meaning TREE-nominative-singular in Polish and Russian respectively. So the projection condition is satisfied. If the lexical function is only defined on these two meanings, then thisisavalidparentlanguage. It is worth emphasising that the projection condition for qualifying as a parent language applies only for those meanings for which the parent lexical mapping is defined. The corresponding forms in the child languages are said to be cognate in this model. Where no parent form isreconstructed,theformsarenotcognate,and are to be accounted for in some way other than the parent language. 3RelatedWork Thecurrentworkis,ofcourse,farfromthefirst toseektoidentifycognatesand/orcorrespondences.Hereisanabbreviatedoverviewofprecanbefoundinchapter3ofKondrak&apos;s(2002) inthisfield. In perhaps the first computational work on historical linguistics, Kay (1964) described an algorithm for determining correspondences given a list of cognate pairs across two daughter languages. His method seeks to find the smallest set anonymous reviewer suggests that the current work shares features with that of Kessler (2001). I have been unable to access this book in time to include discussion of it in this paper. 17 alignment for each cognate pair. Unfortunately, the complexity of the problem has precluded its application to significant daa sets. Frantz (1970) developed a PL/1 programming which returned numerical evaluations of correspondences and cognacy, given a list of possible cognate word-pairs. Each word pair must be as a that is, aligning single segments with each other or withgaps. Guy (1984; 1994) presented a program called COGNATE which finds regular correspondences and identifies cognates using statistical techniques. For his Master&apos;s, Broza (1998) developed software called identifies correspondences from cognates and expresses these as contextual phonological transformation rules. Kondrak&apos;s (2002) doctoral dissertation combines phonological and semantic similarity methods with correspondance-learning. The algorithms for learning correspondences are taken from Melamed&apos;s (2000) probabilistic methods for identifying word-word translation equivalence. These methods, like the current work, are Bayesian. Because Melamed&apos;s problem seeks partial rather than complete explanation of the inputs in terms of correspondences, the matching problem is somewhat more difficult theoretically. As a result, he does not arrive at the decomposition of the sum of the probability of two inputsgiventhesetofpossiblecorrespondences, approximating this with a high probability alignment. 4ConditionalProbabilityofthe Data The core of any Bayesian model is the conditional probability of the data given the hypothesis. This section details how probabilities assigned to data, and the assumptions on which this assignment is based. The data is the mapping of meanings onto forms in two daughter languages. If those two are we want to determine cussedinsection4.6. 4.1Meaningindependence The first step in defining the conditional probability of the data is to decompose it into meaning-by-meaning probabilities. This can be achieved by adopting the following two assumptions. 4 a given language, the forms for different meanings are selected independently. assumption states that a single for example, a form wzigc for meaning TO TAKE-perfect-infinitive is no help in predicting the form which expresses TREEnominative-singular. 5 different languages, the forms corresponding to different meanings are independent. According to this assumption, the Polish word wzigc and the Russian word B3ÿTb can be structurally dependent because they express the same meaning. In contrast, we can only expect a chance relationship between the Russian word B3ÿTb meaning TO TAKE-perfectinfinitive, and the Polish word drzewo expressing TREE-nominative-singular. Together, these two assumptions imply that the only dependencies possible between any four expressing the two meanings languages between the one hand and the other. Consequently the probability of generating the word forms in two languages can be decomposed into the product of generating the two languageparticular forms for each meaning. = 11 18 4.2Cognacyandindependence Thenextassumptionholdsthatstructuralcorrelationbetweencorrespondingformsshouldbe explainedasresultingfromcognacy. 6 different languages, forms corresponding to the same meaning are dependent only if the forms are cognate. If the words for a particular meaning do not derive from a common ancestral form, then they are uncorrelated. To return to our Polish and Russian examples, we can expect dependencies in structure between the cognate words drzewo and pgåðåâî. But we should expect no such correlation in the non-cognate pair pomarancza àïåJlüñIií meaning singular. us write the domain of the lexical in language This is the set of meanings for which this language has defined a word form. The set of cognates is the domain of the function of the parent language, We can decompose the evidential words into three meanings only in language and meanonly expressed in language Words in the second and third categories are non-cognate, and so probabilistically independent of each other. The conditional probability of the data can thus be expressed as follows. = 11 11 4.3 Probability of a word We now turn to the probability of generating a string in a language. The first assumption defines the distribution over word-length. 7 probability of a word having a particular length is negative exponential in that length. The second assumption allows segment probability to depend only on the segment identity, and not on its neighbourhood. Assumption8Segmentchoiceiscontextindependent. These two assumptions together imply that the probability of strings is determined by a fixed over where an end-of-word marker. For the descendent languages, this distribution can be taken as the relative frequencies of the segments and end-of-word The probability of generating a word in a langiven relative frequencies is the product of the relative frequencies for each lettern in the word, multiplied by the relative frequency of the end-of-word marker. = 11 Note that this expression only holds for words that are independent of all others, such as components of non-cognate pairs. 4.4 Probability of generating a cognate pair The probability of generating a cognate pair of words is similar to the above, because descendent forms are deterministically derivable from parent forms. If a pair of thenthereisunitprobabilitythatthedescendentformsarewhattheyaregiventheparent: = Since a cognate pair is derivable from a parent form, the probability of a cognate pair is the sum of the probabilities of all parent forms which will generate the two descendents. Write = the set of possible correspondence strings in the parent which onto wordforms Then the probability of the word pair is given by: = E= Thesummationposesaslightproblem,however. Howdowesumoverallpossiblestringswith givenprojections?Fortunately,wecandecomposethesummation.Startbyrecognisingthat 19 theparentlanguageisalsoalanguage,andso theprobabilityofformsinthelanguageisdeterminedbyadistributionoversegments thiscasecorrespondences andtheend-of-word marker.Forconsistency,wecallthisdistribution The only parent form which projects onto two empty strings is the empty string, consisting only of the end-of-word marker. For brevity, will drop the lambdas, writing = = recursive definition of terms of disjoint unions and concatenation can be transformed into a recursive definition for the probaconstructing a member of the set. Disjoint union is replaced by summation, concatenation by product. The probability of individual correspondence its (unrelative frequency the parent Once again, we hide the implicit v parameters. = = E = We assume, without loss of generality, that the segmental inventory of the parent language of all be- Parent segments which are never used can be excluded by giving them zero frequency in function the set of bidivisions the string s, such that the = m &lt; With this function, we can recursively define a pairs of strings v strings which project onto For we will treat all v as implicit. = Bydefinition,theonlyparentlanguagestring whichcanmapontotheemptystringinboth descendentsistheemptystring. recursive step breaks the strings all possible prefixes correspondence then preposed on all returned by it is applied to the of = = 4.5Probabilityofaform-pair We now have the pieces to specify the probability of finding any particular form as the formpair for the descendent languages. The probability of the pair in the case of cognacy is If the pair are not cognate, then they are independent, and their probabilis If we write for the likelihood that the pair is cognate, we can combine these two values to given a total probabilityofthetwoforms. Because the word-pairs are independent (assumption 4), the product of the above probabilfor each meaning the probability of the data given the hypothesis. 4.6Hypothesis One burning question remains, however. What is the hypothesis? The simple answer is that it is exactly those free variables in the specification of the probability of the data There were two groups of unknowns in the probability of the data. The first is the relafrequency to correspondences in parent-language forms. The second is the likeof cognacy a vector of values between zero and one indexed by meanings. A hypothesis is therefore any setting of values the pair of vectors 20 that while the degree variables v not fixed in the above derivation, they will be held constant for any particular search, and thus do not define a dimension in the hypothesis space. 4.7 Search this section, we have derived the likelihood of our data given a hypothesis. For simplicity, we choose a flat prior over hypotheses, rendering the MAP Bayesian approach an instance of maximum likelihood determination. The value for the likelihood is differentiable in each of the parameters. Consequently, gradient descent can be used to find the hypothesis which maximises the probability of the data. 5Results Inconstructingthemethod,wemadeanumber ofassumptionsaboutindependenceofforms.It issensiblethatfortesting,themethodisapplied todatathatconformsreasonablywelltothese assumptions. The alternative is to apply it to data which contradicts its fundamental assumptions, consequently hampering its effectiveness. 5.1Thedata PolishandRussianwerechosentoprovidethe databecausetheyapproximatelyobeyassumption6:wordshavedependentstructuresifand onlyiftheyarecognate.Forourtwolanguages,thismeansthatborrowingsfromcommonsourcesareuncommon(numbering45in ourdataset),atleastincomparisonwiththe numberofcognates(numbering156). The data was harvested from two online dictionaries (Wordgumbo, 2007a; Wordgumbo, 2007b), one English-Polish, the other English- Russian. Multiple translations were simplified, with the shortest translation retained. The English glosses were used as the meanings for the words. Where the gloss contained a capital letter, indicating a proper noun, this was eliminated from the data. The data should also conform to assumption 4, that words for different meanings with a language are independent. So where two meanings in the data sets were realised with the same form, thesemeaningsweredeemedtobestructurally dependent,andsoonlythefirstwasretainedin thewordlist. The remaining data contains 407 aligned Polish-Russian word pairs. Polish and Russian both use a great deal of derivational and inflectional morphology. The simple language model used here does not take this into account, so this will be a disturbing influence on the results. 5.2 Evaluation The aligned wordlists were hand-tagged as cognate, common borrowing or non-cognate. A permissive rule of cognacy was used: if the roots of words in the two languages were cognate, theywerecognate,evenifrepresentedwithnoncognatederivationaland/orinflectionalmorphology. Figure 1 shows the evaluation of the program&apos;s performance on the data. Borrowings as: cognates non-cognates 119 37 49 71% 76% Figure 1: Evaluation of program performance on 407 meaning-matched pairs of Polish-Russian words. Common borrowings are scored as cognates in the first column, non-cognates in the second. The scores show that the method works well in identifying cognates, particularly if common borrowings are accepted as cognates, or excluded manually. If common borrowings are scored as non-cognates, then the accuracy falls. Of the correspondences found between Polish and Russian, 67 have a phonological basis. The remaining 27 result from mismatch morphology in cognates or differences in common borrowings. 6Conclusion Thispaperhaspresentedamodeloflanguage whichallowsthecalculationoftheposterior probabilityofformsarisinginthecaseswhere 21 theyarecognate,andwheretheyarenot.Bayes&apos; theoremrelatestheseprobabilitiestotheposteriorlikelihoodofparticularcorrespondencesand cognacyrelationships.Gradientdescentcanbe usedtosearchthisspaceforthebestdistribution overcorrespondences,andbestcognacyevaluationsformeaning-pairedwords.Theapplication to data from Polish and Russian shows remarkable success identifying both cognates and noncognates. Future work will proceed by relaxing constraints on the parent language. The parent inventory will be widened to include multisegment correspondences. Multiple parent languages will be permitted, to the end of separating borrowings from cognates. Finally, richer models of language,incorporatingsyllablestructure,will allowmoreinformationtoidentifycognates. References GilBroza.1998.Inter-languageregularity:the transformationlearningproblem.Master&apos;sthesis, InstituteofComputerScience,HebrewUniversity ofJerusalem,October. IsidoreDyen,JosephB.Kruskal,andPaulBlack. 1992. An Indo-European classification: a lexicoexperiment. of the Amer- Philosophical 82(5). T.MarkEllisonandSimonKirby.2006.Measuring languagedivergencebyintra-lexicalcomparison.</abstract>
<note confidence="0.903790542857143">Ph.D. thesis, University of Western Australia. DonaldG.Frantz.1970.APL/1programtoassist comparative linguist. of the 13(6):353~356. Russell D. Gray and Quentin D. Atkinson. 2003. Language-tree divergence times support the anatheory of indo-european origin. 426:435~439. JacquesB.M.Guy.1984.Analgorithmforidenti- International Conference on Computational Linguistics and 22nd Annual Meeting of the Assofor Computational Available online as http://acl.ldc.upenn.edu/P/P84/P84- 1091.pdf. Jacques B. M. Guy. 1994. An algorithm for identifying cognates in bilingual wordlists and its applito machine translation. of Quan- 1(1):35~42. Heinrich Hock. 1991. of Historical Mouton de Gruyter, Berlin. J. Jeffers and Ilse Lehiste. 1989. Princiand Methods for Historical MIT Press, Cambridge, MA. Martin Kay. 1964. The logic of cognate recognition inhistoricallinguistics.TechnicalReportRM- 4224-PR,TheRANDCorporation,SantaMonica, CA,September. CSLIPublications,Stanford,CA. SimonKirby.2002.Naturallanguagefromartificial 8(2):185~215. Kondrak. 2002. for Lan- Ph.D. thesis, University of Toronto. AprilMcMahonandRobertMcMahon.2003.Find-</note>
<email confidence="0.574656">ingfamilies:quantitativemethodsinlanguage</email>
<abstract confidence="0.762235411764706">classification.TransactionsofthePhilologicalSo- I.DanMelamed.2000.Modelsoftranslational among words. Lin- 26(2):221~249. Luay Nakleh, Tandy Warnow, Don Ringe, and Steven N. Evans. 2005. A comparison phylogenetic reconstruction methods on an ie of the Philological 103(2):171~192. D. Ringe, Tandy Warnow, and A. Taylor. Indo-European and computational cladisof the Philological 100(1):59~129. Wordgumbo.2007a./ie/sla/pol/erengpol.htm. Website http://www.wordgumbo.com/. Wordgumbo. 2007b. /ie/sla/rus/erengrus.htm. Websitehttp://www.wordgumbo.com/.</abstract>
<intro confidence="0.526814">22</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>GilBroza.1998.Inter-languageregularity:the transformationlearningproblem.Master&apos;sthesis, InstituteofComputerScience,HebrewUniversity ofJerusalem,October.</booktitle>
<marker></marker>
<rawString>GilBroza.1998.Inter-languageregularity:the transformationlearningproblem.Master&apos;sthesis, InstituteofComputerScience,HebrewUniversity ofJerusalem,October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JosephB Kruskal IsidoreDyen</author>
<author>andPaulBlack</author>
</authors>
<title>An Indo-European classification: a lexicostatistical experiment.</title>
<date>1992</date>
<journal>Transactions of the American Philosophical Society,</journal>
<volume>82</volume>
<issue>5</issue>
<marker>IsidoreDyen, andPaulBlack, 1992</marker>
<rawString>IsidoreDyen,JosephB.Kruskal,andPaulBlack. 1992. An Indo-European classification: a lexicostatistical experiment. Transactions of the American Philosophical Society, 82(5).</rawString>
</citation>
<citation valid="false">
<booktitle>T.MarkEllisonandSimonKirby.2006.Measuring languagedivergencebyintra-lexicalcomparison. InACL,pages273~280,Sydney.</booktitle>
<marker></marker>
<rawString>T.MarkEllisonandSimonKirby.2006.Measuring languagedivergencebyintra-lexicalcomparison. InACL,pages273~280,Sydney.</rawString>
</citation>
<citation valid="false">
<tech>T.MarkEllison.1992.TheMachineLearningof Phonological Structure. Ph.D. thesis,</tech>
<institution>University of Western Australia.</institution>
<marker></marker>
<rawString>T.MarkEllison.1992.TheMachineLearningof Phonological Structure. Ph.D. thesis, University of Western Australia.</rawString>
</citation>
<citation valid="false">
<title>DonaldG.Frantz.1970.APL/1programtoassist the comparative linguist.</title>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>6</issue>
<marker></marker>
<rawString>DonaldG.Frantz.1970.APL/1programtoassist the comparative linguist. Communications of the ACM, 13(6):353~356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell D Gray</author>
<author>Quentin D Atkinson</author>
</authors>
<title>Language-tree divergence times support the anatolian theory of indo-european origin.</title>
<date>2003</date>
<journal>Nature,</journal>
<pages>426--435</pages>
<contexts>
<context position="1640" citStr="Gray and Atkinson, 2003" startWordPosition="152" endWordPosition="155">w of current current work on how language evolved, much of it based on computational models and simulations. Ellison (1992) presents a linguistically motivated method for classifying consonants as consonants or vowels. An unexpected result for the dead languageGothicprovidesaddedweighttoone oftwocompetingphonologicalinterpretationsof theorthographyofthisdeadlanguage. 15 Other recent work has applied computational methods for phylogenetics to measuring linguistic distances, and/or constructing taxonomic trees from distances between languages and dialects (Dyen et al., 1992; Ringe et al., 2002; Gray and Atkinson, 2003; McMahon and McMahon, 2003; Nakleh et al., 2005; Ellison and Kirby, 2006). A central focus of historical linguistics is the reconstruction of parent languages from the evidence of their descendents. In historical linguistics proper, this is done by the comparative method (Jeffers and Lehiste, 1989; Hock, 1991) in which shared arbitrary structure is assumed to reflect common origin. At the phonological level, reconstruction identifies cognates and correspondences, and then constructs sound changes which explain them. This paper presents a Bayesian approach to assessing cognates and corresponde</context>
</contexts>
<marker>Gray, Atkinson, 2003</marker>
<rawString>Russell D. Gray and Quentin D. Atkinson. 2003. Language-tree divergence times support the anatolian theory of indo-european origin. Nature, 426:435~439.</rawString>
</citation>
<citation valid="false">
<booktitle>JacquesB.M.Guy.1984.Analgorithmforidentifyingcognatesbetweenrelatedlanguages.In10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics. Available online as</booktitle>
<pages>84--84</pages>
<marker></marker>
<rawString>JacquesB.M.Guy.1984.Analgorithmforidentifyingcognatesbetweenrelatedlanguages.In10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics. Available online as http://acl.ldc.upenn.edu/P/P84/P84-1091.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques B M Guy</author>
</authors>
<title>An algorithm for identifying cognates in bilingual wordlists and its applicability to machine translation.</title>
<date>1994</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Guy, 1994</marker>
<rawString>Jacques B. M. Guy. 1994. An algorithm for identifying cognates in bilingual wordlists and its applicability to machine translation. Journal of Quantitative Linguistics, 1(1):35~42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Heinrich Hock</author>
</authors>
<title>Principles of Historical Linguistics. Mouton de Gruyter,</title>
<date>1991</date>
<location>Berlin.</location>
<contexts>
<context position="1952" citStr="Hock, 1991" startWordPosition="204" endWordPosition="205">retationsof theorthographyofthisdeadlanguage. 15 Other recent work has applied computational methods for phylogenetics to measuring linguistic distances, and/or constructing taxonomic trees from distances between languages and dialects (Dyen et al., 1992; Ringe et al., 2002; Gray and Atkinson, 2003; McMahon and McMahon, 2003; Nakleh et al., 2005; Ellison and Kirby, 2006). A central focus of historical linguistics is the reconstruction of parent languages from the evidence of their descendents. In historical linguistics proper, this is done by the comparative method (Jeffers and Lehiste, 1989; Hock, 1991) in which shared arbitrary structure is assumed to reflect common origin. At the phonological level, reconstruction identifies cognates and correspondences, and then constructs sound changes which explain them. This paper presents a Bayesian approach to assessing cognates and correspondences. Best sets of cognates and correspondences can then be identified by gradient ascent on this evaluation measure. While the work is motivated by the eventual goal of offering software solutions to historical linguistics, it also hopes to show thatBayes&apos;theoremappliedtoanexplicit,simplemodeloflanguagecanlead</context>
</contexts>
<marker>Hock, 1991</marker>
<rawString>Hans Heinrich Hock. 1991. Principles of Historical Linguistics. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert J Jeffers</author>
<author>Ilse Lehiste</author>
</authors>
<title>Principles and Methods for Historical Linguistics.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1939" citStr="Jeffers and Lehiste, 1989" startWordPosition="200" endWordPosition="203">competingphonologicalinterpretationsof theorthographyofthisdeadlanguage. 15 Other recent work has applied computational methods for phylogenetics to measuring linguistic distances, and/or constructing taxonomic trees from distances between languages and dialects (Dyen et al., 1992; Ringe et al., 2002; Gray and Atkinson, 2003; McMahon and McMahon, 2003; Nakleh et al., 2005; Ellison and Kirby, 2006). A central focus of historical linguistics is the reconstruction of parent languages from the evidence of their descendents. In historical linguistics proper, this is done by the comparative method (Jeffers and Lehiste, 1989; Hock, 1991) in which shared arbitrary structure is assumed to reflect common origin. At the phonological level, reconstruction identifies cognates and correspondences, and then constructs sound changes which explain them. This paper presents a Bayesian approach to assessing cognates and correspondences. Best sets of cognates and correspondences can then be identified by gradient ascent on this evaluation measure. While the work is motivated by the eventual goal of offering software solutions to historical linguistics, it also hopes to show thatBayes&apos;theoremappliedtoanexplicit,simplemodelofla</context>
</contexts>
<marker>Jeffers, Lehiste, 1989</marker>
<rawString>Robert J. Jeffers and Ilse Lehiste. 1989. Principles and Methods for Historical Linguistics. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>The logic of cognate recognition</title>
<date>1964</date>
<booktitle>inhistoricallinguistics.TechnicalReportRM4224-PR,TheRANDCorporation,SantaMonica,</booktitle>
<pages>CA,September.</pages>
<contexts>
<context position="9869" citStr="Kay (1964)" startWordPosition="1328" endWordPosition="1329">s for which the parent lexical mapping is defined. The corresponding forms in the child languages are said to be cognate in this model. Where no parent form isreconstructed,theformsarenotcognate,and are to be accounted for in some way other than the parent language. 3RelatedWork Thecurrentworkis,ofcourse,farfromthefirst toseektoidentifycognatesand/orcorrespondences.Hereisanabbreviatedoverviewofpreviousworkinthefield&apos;.Moredetailedsurveys canbefoundinchapter3ofKondrak&apos;s(2002) PhDthesisorLowe&apos;sonlinesurvey2ofpriorart inthisfield. In perhaps the first computational work on historical linguistics, Kay (1964) described an algorithm for determining correspondences given a list of cognate pairs across two daughter languages. His method seeks to find the smallest set &apos;An anonymous reviewer suggests that the current work shares features with that of Kessler (2001). I have been unable to access this book in time to include discussion of it in this paper. 2linguistics.berkeley.edu/ ˜jblowe/REWWW/PriorArt.html 17 ofcorrespondenceswhichallowsadegree-(1, oo) alignment for each cognate pair. Unfortunately, the complexity of the problem has precluded its application to significant daa sets. Frantz (1970) dev</context>
</contexts>
<marker>Kay, 1964</marker>
<rawString>Martin Kay. 1964. The logic of cognate recognition inhistoricallinguistics.TechnicalReportRM4224-PR,TheRANDCorporation,SantaMonica, CA,September.</rawString>
</citation>
<citation valid="false">
<note>BrettKessler.2001.TheSignificanceofWordLists. CSLIPublications,Stanford,CA.</note>
<marker></marker>
<rawString>BrettKessler.2001.TheSignificanceofWordLists. CSLIPublications,Stanford,CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>SimonKirby 2002 Naturallanguagefromartificial life</author>
</authors>
<journal>Artificial Life,</journal>
<volume>8</volume>
<issue>2</issue>
<marker>life, </marker>
<rawString>SimonKirby.2002.Naturallanguagefromartificial life. Artificial Life, 8(2):185~215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Algorithms for Language Reconstruction.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto.</institution>
<marker>Kondrak, 2002</marker>
<rawString>Grzegorz Kondrak. 2002. Algorithms for Language Reconstruction. Ph.D. thesis, University of Toronto.</rawString>
</citation>
<citation valid="false">
<note>AprilMcMahonandRobertMcMahon.2003.Findingfamilies:quantitativemethodsinlanguage classification.TransactionsofthePhilologicalSociety,101:7~55.</note>
<marker></marker>
<rawString>AprilMcMahonandRobertMcMahon.2003.Findingfamilies:quantitativemethodsinlanguage classification.TransactionsofthePhilologicalSociety,101:7~55.</rawString>
</citation>
<citation valid="false">
<title>I.DanMelamed.2000.Modelsoftranslational equivalence among words.</title>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<marker></marker>
<rawString>I.DanMelamed.2000.Modelsoftranslational equivalence among words. Computational Linguistics, 26(2):221~249.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Luay Nakleh</author>
</authors>
<location>Tandy Warnow, Don Ringe, and</location>
<marker>Nakleh, </marker>
<rawString>Luay Nakleh, Tandy Warnow, Don Ringe, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven N Evans</author>
</authors>
<title>A comparison of phylogenetic reconstruction methods on an ie dataset.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<marker>Evans, 2005</marker>
<rawString>Steven N. Evans. 2005. A comparison of phylogenetic reconstruction methods on an ie dataset. Transactions of the Philological Society, 103(2):171~192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ringe</author>
<author>Tandy Warnow</author>
<author>A Taylor</author>
</authors>
<title>Indo-European and computational cladistics.</title>
<date>2002</date>
<journal>Transactions of the Philological Society,</journal>
<volume>100</volume>
<issue>1</issue>
<contexts>
<context position="1615" citStr="Ringe et al., 2002" startWordPosition="148" endWordPosition="151">02) gives an overview of current current work on how language evolved, much of it based on computational models and simulations. Ellison (1992) presents a linguistically motivated method for classifying consonants as consonants or vowels. An unexpected result for the dead languageGothicprovidesaddedweighttoone oftwocompetingphonologicalinterpretationsof theorthographyofthisdeadlanguage. 15 Other recent work has applied computational methods for phylogenetics to measuring linguistic distances, and/or constructing taxonomic trees from distances between languages and dialects (Dyen et al., 1992; Ringe et al., 2002; Gray and Atkinson, 2003; McMahon and McMahon, 2003; Nakleh et al., 2005; Ellison and Kirby, 2006). A central focus of historical linguistics is the reconstruction of parent languages from the evidence of their descendents. In historical linguistics proper, this is done by the comparative method (Jeffers and Lehiste, 1989; Hock, 1991) in which shared arbitrary structure is assumed to reflect common origin. At the phonological level, reconstruction identifies cognates and correspondences, and then constructs sound changes which explain them. This paper presents a Bayesian approach to assessing</context>
</contexts>
<marker>Ringe, Warnow, Taylor, 2002</marker>
<rawString>D. Ringe, Tandy Warnow, and A. Taylor. 2002. Indo-European and computational cladistics. Transactions of the Philological Society, 100(1):59~129.</rawString>
</citation>
<citation valid="false">
<booktitle>Wordgumbo.2007a./ie/sla/pol/erengpol.htm. Website http://www.wordgumbo.com/. Wordgumbo. 2007b. /ie/sla/rus/erengrus.htm. Websitehttp://www.wordgumbo.com/.</booktitle>
<marker></marker>
<rawString>Wordgumbo.2007a./ie/sla/pol/erengpol.htm. Website http://www.wordgumbo.com/. Wordgumbo. 2007b. /ie/sla/rus/erengrus.htm. Websitehttp://www.wordgumbo.com/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>