<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.939561">
Density Maximization in Context-Sense Metric Space
for All-words WSD
</title>
<note confidence="0.5037805">
Koichi Tanigakit$ Mitsuteru Shibat Tatsuji Munakat Yoshinori Sagisakat
† Information Technology R&amp;D Center, Mitsubishi Electric Corporation
</note>
<address confidence="0.79691">
5-1-1 Ofuna, Kamakura, Kanagawa 247-8501, Japan
$ Global Information and Telecommunication Institute, Waseda University
1-3-10 Nishi-Waseda, Shinjuku-ku, Tokyo 169-0051, Japan
</address>
<sectionHeader confidence="0.94615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953826086957">
This paper proposes a novel smoothing
model with a combinatorial optimization
scheme for all-words word sense disam-
biguation from untagged corpora. By gen-
eralizing discrete senses to a continuum,
we introduce a smoothing in context-sense
space to cope with data-sparsity result-
ing from a large variety of linguistic con-
text and sense, as well as to exploit sense-
interdependency among the words in the
same text string. Through the smoothing,
all the optimal senses are obtained at one
time under maximum marginal likelihood
criterion, by competitive probabilistic ker-
nels made to reinforce one another among
nearby words, and to suppress conflicting
sense hypotheses within the same word.
Experimental results confirmed the superi-
ority of the proposed method over conven-
tional ones by showing the better perfor-
mances beyond most-frequent-sense base-
line performance where none of SemEval-
2 unsupervised systems reached.
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998742653061224">
Word Sense Disambiguation (WSD) is a task to
identify the intended sense of a word based on its
context. All-words WSD is its variant, where all
the unrestricted running words in text are expected
to be disambiguated. In the all-words task, all
the senses in a dictionary are potentially the target
destination of classification, and purely supervised
approaches inherently suffer from data-sparsity
problem. The all-words task is also character-
ized by sense-interdependency of target words. As
the target words are typically taken from the same
text string, they are naturally expected to be inter-
related. Disambiguation of a word should affect
other words as an important clue.
From such characteristics of the task,
knowledge-based unsupervised approaches
have been extensively studied. They compute
dictionary-based sense similarity to find the most
related senses among the words within a certain
range of text. (For reviews, see (Agirre and
Edmonds, 2006; Navigli, 2009).) In recent years,
graph-based methods have attracted considerable
attentions (Mihalcea, 2005; Navigli and Lapata,
2007; Agirre and Soroa, 2009). On the graph
structure of lexical knowledge base (LKB),
random-walk or other well-known graph-based
techniques have been applied to find mutually
related senses among target words. Unlike
earlier studies disambiguating word-by-word, the
graph-based methods obtain sense-interdependent
solution for target words. However, those
methods mainly focus on modeling sense dis-
tribution and have less attention to contextual
smoothing/generalization beyond immediate
context.
There exist several studies that enrich immedi-
ate context with large corpus statistics. McCarthy
et al. (2004) proposed a method to combine sense
similarity with distributional similarity and config-
ured predominant sense score. Distributional sim-
ilarity was used to weight the influence of context
words, based on large-scale statistics. The method
achieved successful WSD accuracy. Agirre et al.
(2009) used a k-nearest words on distributional
similarity as context words. They apply a LKB
graph-based WSD to a target word together with
the distributional context words, and showed that
it yields better results on a domain dataset than
just using immediate context words. Though these
</bodyText>
<page confidence="0.977872">
884
</page>
<note confidence="0.914071">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 884–893,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99972385">
studies are word-by-word WSD for target words,
they demonstrated the effectiveness to enrich im-
mediate context by corpus statistics.
This paper proposes a smoothing model that in-
tegrates dictionary-based semantic similarity and
corpus-based context statistics, where a combina-
torial optimization scheme is employed to deal
with sense interdependency of the all-words WSD
task. The rest of this paper is structured as fol-
lows. We first describe our smoothing model in the
following section. The combinatorial optimization
method with the model is described in Section 3.
Section 4 describes a specific implementation for
evaluation. The evaluation is performed with the
SemEval-2 English all-words dataset. We present
the performance in Section 5. In Section 6 we dis-
cuss whether the intended context-to-sense map-
ping and the sense-interdependency are properly
modeled. Finally we review related studies in Sec-
tion 7 and conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.994644" genericHeader="introduction">
2 Smoothing Model
</sectionHeader>
<bodyText confidence="0.997888555555556">
Let us introduce in this section the basic idea for
modeling context-to-sense mapping. The distance
(or similarity) metrics are assumed to be given for
context and for sense. A specific implementation
of these metrics is described later in this paper, for
now the context metric is generalized with a dis-
tance function dx(·, ·) and the sense metric with
ds(·, ·). Actually these functions may be arbitrary
ones that accept two elements and return a positive
real number.
Now suppose we are given a dataset concern-
ing N number of target words. This dataset is
denoted by X = {xi}Ni=1, where xi corresponds
to the context of the i-th word but not the word
by itself. For each xi, the intended sense of the
word is to be found in a set of sense candidates
Si = {sij}M�
j=1 C_ S, where Mi is the number of
sense candidates for the i-th word, S is the whole
set of sense inventories in a dictionary. Let the
two-tuple hij = (xi, sij) be the hypothesis that
the intended sense in xi is sij. The hypothesis is
an element of the direct product H = X x S. As
(X, dx) and (S, ds) each composes a metric space,
H is also a metric space, provided a proper dis-
tance definition with dx and ds.
Here, we treat the space H as a continuous one,
which means that we assume the relationship be-
tween context and sense can be generalized in con-
tinuous fashion. In natural language processing,
continuity has been sometimes assumed for lin-
guistic phenomena including word context for cor-
pus based WSD. As for classes or senses, it may
not be a common assumption. However, when
the classes for all-words WSD are enormous, fine-
grained, and can be associated with distance, we
can rather naturally assume the continuity also for
senses. According to the nature of continuity, once
given a hypothesis hij for a certain word, we can
extrapolate the hypothesis for another word of an-
other sense hi′j′ = (xi′, si′j′) sufficiently close to
hij. Using a Gaussian kernel (Parzen, 1962) as a
smoothing model, the probability density extrapo-
lated at hi′j′ given hij is defined by their distance
as follows:
</bodyText>
<equation confidence="0.98573775">
K(hij, hi′j′) (1)
1 p 2Qx2 2Qg2 r_ dx2(xi,xi′) _ ds2(sij,si′j′)1
2πσxσs
ex L J ,
</equation>
<bodyText confidence="0.999992933333333">
where σx and σs are parameters of positive real
number σx, σs E R+ called kernel bandwidths.
They control the smoothing intensity in context
and in sense, respectively.
Our objective is to determine the optimal sense
for all the target words simultaneously. It is es-
sentially a 0-1 integer programing problem, and
is not computationally tractable. We relax the
integer constraints by introducing a sense prob-
ability parameter πij corresponding to each hij.
πij denotes the probability by which hij is true.
As πij is a probability, it satisfies the constraints
Vi Ej πij = 1 and Vi, j 0 &lt; πij &lt; 1. The proba-
bility density extrapolated at hi′j′ by a probabilis-
tic hypothesis hij is given as follows:
</bodyText>
<equation confidence="0.737413">
2ij(hi′j′) a πij K(hij, hi′j′). (2)
</equation>
<bodyText confidence="0.999924266666667">
The proposed model is illustrated in Figure 1.
Due to the limitation of drawing, both the context
metric space and the sense metric space are drawn
schematically as 1-dimensional spaces (axes), ac-
tually arbitrary metric spaces similarity-based or
feature-based are applicable. The product metric
space of the context metric space and the sense
metric space composes a hypothesis space. In the
hypothesis space, n sense hypotheses for a cer-
tain word is represented as n points on the hyper-
plane that spreads across the sense metric space.
The two small circles in the middle of the fig-
ure represent the two sense hypotheses for a sin-
gle word. The position of a hypothesis represents
which sense is assigned to the current word in
</bodyText>
<page confidence="0.997067">
885
</page>
<figure confidence="0.448949">
&amp;quot;Invasive, exotic plants cause particular problems for wildlife.&amp;quot;
</figure>
<figureCaption confidence="0.9708895">
Figure 1: Proposed probability distribution model
for context-to-sense mapping space.
</figureCaption>
<bodyText confidence="0.986433214285714">
what context. The upward arrow on a hypothesis
represents the magnitude of its probability.
Centered on each hypotheses, a Gaussian ker-
nel is placed as a smoothing model. It extrapo-
lates the hypotheses of other words around it. In
accordance with geometric intuition, intensity of
extrapolation is affected by the distance from a hy-
pothesis, and by the probability of the hypothesis
by itself. Extrapolated probability density is rep-
resented by shadow thickness and surface height.
If there is another word in nearby context, the ker-
nels can validate the sense of that word. In the
figure, there are two kernels in the context “Inva-
sive, exotic ...”. They are two competing hypothe-
sis for the senses decoy and flora of the word
plants. These kernels affect the senses of another
ambiguous word tree in nearby context “Exotic
...”, and extrapolate the most at the sense tree
nearby flora. The extrapolation has non-linear
effect. It affects little to the word far away in con-
text or in sense as is the case for the background
word in the figure. Strength of smoothing is deter-
mined by kernel bandwidths. Wider bandwidths
bring stronger effect of generalization to further
hypotheses, but too wide bandwidths smooth out
detailed structure. The bandwidths are the key for
disambiguation, therefore they are to be optimized
on a dataset together with sense probabilities.
</bodyText>
<sectionHeader confidence="0.998057" genericHeader="method">
3 Simultaneous Optimization of
All-words WSD
</sectionHeader>
<bodyText confidence="0.99965575">
Given the smoothing model to extrapolate the
senses of other words, we now make its in-
stances interact to obtain the optimal combination
of senses for all the words.
</bodyText>
<subsectionHeader confidence="0.99915">
3.1 Likelihood Definition
</subsectionHeader>
<bodyText confidence="0.9997498">
Let us first define the likelihood of model param-
eters for a given dataset. The parameters con-
sist of a context bandwidth Qx, a sense bandwidth
Qs, and sense probabilities 7rij for all i and j.
For convenience of description, the sense proba-
bilities are all together denoted as a vector -7r =
(... , 7rij,... )⊤, in which actual order is not the
matter.
Now remind that our dataset X = {xi}Ni=1 is
composed of N instances of unlabeled word con-
text. We consider all the mappings from context
to sense are latent, and find the optimal parameters
by maximizing marginal pseudo likelihood based
on probability density. The likelihood is defined
as follows:
</bodyText>
<equation confidence="0.987667">
∏L(7r, Qx, Qs; X) ≡ ln ∑ 7rijQ(hij), (3)
i j
</equation>
<bodyText confidence="0.9960965">
where ∏i denotes the product over xi ∈ X, ∑
j
denotes the summation over all possible senses
sij ∈ Si for the current i-th context. Q(hij)
denotes the probability density at hij. We com-
pute Q(hij) using leave-one-out cross-validation
(LOOCV), so as to prevent kernels from over-
fitting to themselves, as follows:
</bodyText>
<equation confidence="0.999346">
Q(hij) (4)
≡ N − Ni
1 ∑ ∑
</equation>
<bodyText confidence="0.995107181818182">
i′: wi′̸=wi j′ 7ri′j′K(hij, hi′j′),
where Ni denotes the number of occurrences of a
word type wi in X, and ∑i′: wi′̸=wi denotes the
summation over xi′ ∈ X except the case that the
word type wi′ equals to wi. ∑j′ denotes the sum-
mation over si′j′ ∈ Si′. We take as the unit of
LOOCV not a word instance but a word type, be-
cause the instances of the same word type invari-
ably have the same sense candidates, which still
cause over-fitting when optimizing the sense band-
width.
</bodyText>
<subsectionHeader confidence="0.998509">
3.2 Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.999975333333333">
We are now ready to calculate the optimal senses.
The optimal parameters ,7r∗, Q∗x, Q∗s are obtained by
maximizing the likelihood L subject to the con-
straints on 7r, that is ∀i ∑j 7rij = 1 1. Using the
Lagrange multipliers {Ai}Ni=1 for every i-th con-
straint, the solution for the constrained maximiza-
</bodyText>
<footnote confidence="0.9758705">
1It is guaranteed that the other constraints `di, j 0 &lt; iris &lt;
1 are satisfied according to Equation (7).
</footnote>
<figure confidence="0.999547352941176">
Context (Input)
Context Metric
Space
H.B. Tree(actor)
Sense Metric
Space
&amp;quot;Exotic tree
tree diagram
&amp;quot;
decoy
tree
Hypothesis
Sense Probability
flora
Sense (Class)
Extrapolated
Density
</figure>
<page confidence="0.993219">
886
</page>
<bodyText confidence="0.9871485">
tion of L is obtained as the solution for the equiv-
alent unconstrained maximization of L� as follows:
</bodyText>
<equation confidence="0.991685">
7r∗, σ∗x, σ∗s = arg max G, (5)
7&lt;, σ., σB
</equation>
<bodyText confidence="0.754747">
where
</bodyText>
<equation confidence="0.906741">
L� ≡ L + ∑
i λi πij − 1 . (6)
(∑ )
j
</equation>
<bodyText confidence="0.998925857142857">
When we optimize the parameters, the first term
of Equation (6) in the right-hand side acts to re-
inforce nearby hypotheses among different words,
whereas the second term acts to suppress conflict-
ing hypotheses of the same word.
Taking ∇ L� = 0, erasing λi, and rearranging,
we obtain the optimal parameters as follows:
</bodyText>
<equation confidence="0.996096076923077">
Rij
i′j′ + ∑ i′, j′ Ri′j′
ij
wz′ ̸�wz (7)
Ri′j′
ij
wz′ ̸�wz
1 ∑ σx2 =
N i, i′, j, j′
wz′ ̸�wz
1 ∑ σs2 =
N i, i′, j, j′
wz′ ̸�wz
</equation>
<bodyText confidence="0.9985972">
where Rij
i′j′ denotes the responsibility of hi′j′ to
hij: the ratio of total expected density at hij, taken
up by the expected density extrapolated by hi′j′,
normalized to the total for xi be 1. It is defined as
</bodyText>
<equation confidence="0.90190675">
Rij
i′j′ ≡ πijQi′j′(hij)
∑ (10)
j πijQ(hij).
</equation>
<bodyText confidence="0.9907055">
Qi′j′(hij) denotes the probability density at hij
extrapolated by hi′j′ alone, defined as follows:
</bodyText>
<equation confidence="0.9975">
1
Qi′j′(hij) ≡ πi′j′K(hij, hi′j′). (11)
N − Ni
</equation>
<bodyText confidence="0.999838193548387">
Intuitively, Equations (7)-(9) are interpreted as
follows. As for Equation (7), the right-hand side
of the equation can be divided as the left term and
the right term both in the numerator and in the
denominator. The left term requires πij to agree
with the ratio of responsibility of the whole to hij.
The right term requires πij to agree with the ra-
tio of responsibility of hij to the whole. As for
Equation (8), (9), the optimal solution is the mean
squared distance in context, and in sense, weighted
by responsibility.
To obtain the actual values of the optimal pa-
rameters, EM algorithm (Dempster et al., 1977)
is applied. This is because Equations (7)-(9) are
circular definitions, which include the objective
parameters implicitly in the right hand side, thus
the solution is not obtained analytically. EM al-
gorithm is an iterative method for finding maxi-
mum likelihood estimates of parameters in statis-
tical models, where the model depends on unob-
served latent variables. Applying the EM algo-
rithm to our model, we obtain the following steps:
Step 1. Initialization: Set initial values to 7r,
σx, and σs. As for sense probabilities,
we set the uniform probability in accor-
dance with the number of sense candidates,
thereby πij ← |Si|−1, where |Si |denotes
the size of Si. As for bandwidths, we set
the mean squared distance in each metric;
thereby σx2 ← N−1 ∑i, i′ dx2(xi, xi′)
for context bandwidth, and σs2 ←
</bodyText>
<equation confidence="0.881933666666667">
(∑i|Si|)−1 ∑ ∑j, j′ ds2(sij, si′j′) for
i, i′
sense bandwidth.
</equation>
<bodyText confidence="0.919944965517241">
Step 2. Expectation: Using the current parame-
ters 7r, σx, and σs, calculate the responsibili-
ties Rij
i′j′ according to Equation (10).
Step 3. Maximization: Using the current respon-
sibility Rij
i′j′, update the parameters 7r, σx,
and σs, according to Equation (7)-(9).
Step 4. Convergence test: Compute the likeli-
hood. If its ratio to the previous iteration is
sufficiently small, or predetermined number
of iterations has been reached, then terminate
the iteration. Otherwise go back to Step 2.
To visualize how it works, we applied the above
EM algorithm to pseudo 2-dimensional data. The
results are shown in Figure 2. It simulates WSD
for an N = 5 words dataset, whose contexts are
depicted by five lines. The sense hypotheses are
depicted by twelve upward arrows. At the base of
each arrow, there is a Gaussian kernel. Shadow
thickness and surface height represents the com-
posite probability distribution of all the twelve
kernels. Through the iterative parameter update,
sense probabilities and kernel bandwidths were
optimized to the dataset. Figure 2(a) illustrates the
initial status, where all the sense hypothesis are
equivalently probable, thus they are in the most
ambiguous status. Initial bandwidths are set to the
mean squared distance of all the hypotheses pairs,
</bodyText>
<equation confidence="0.998591444444444">
∑ i′, j′
wz′ ̸�wz
πij =
1 + ∑ ∑
j i′, j′
Rij
i′j′ dx2(xi,xi′) (8)
Rij
i′j′ ds2(sij, si′j′), (9)
</equation>
<page confidence="0.988181">
887
</page>
<figure confidence="0.9960178">
Context Context Context
(Input) (Input) (Input)
Sense Sense Sense
(Class) (Class) (Class)
(a) Initial status. (b) Status after the 7th iteration. (c) Converged status after 25 iterations.
</figure>
<figureCaption confidence="0.908306">
Figure 2: Pseudo 2D data simulation to visualize the dynamics of the proposed simultaneous all-words
WSD with ambiguous five words and twelve sense hypotheses. (There are twelve Gaussian kernels at
</figureCaption>
<bodyText confidence="0.974266766666667">
the base of each arrow, though the figure shows just their composite distribution. Those kernels reinforce
and compete one another while being fitted their affecting range, and finally settle down to the most
consistent interpretation for the words with appropriate generalization. For the dynamics with an actual
dataset, see Figure 5.)
which is rather broad and makes kernels strongly
smoothed, thus the model captures general struc-
ture of space. Figure 2(b) shows the status after the
7th iteration. Bandwidths are shrinking especially
in context, and two context clusters, so to speak,
two usages, are found. Figure 2(c) shows the sta-
tus of convergence after 25 iterations. All the ar-
row lengths incline to either 1 or 0 along with their
neighbors, thus all the five words are now disam-
biguated.
Note that this is not the conventional cluster-
ing of observed data. If, for instance, the Gaus-
sian mixture clustering of 2-mixtures is applied
to the positions of these hypotheses, it will find
the clusters just like Figure 2(b) and will stop.
The cluster centers are located at the means of hy-
potheses including miscellaneous alternatives not
intended, thus the estimated probability distribu-
tion is, roughly speaking, offset toward the center
of WordNet, which is not what we want. In con-
trast, the proposed method proceeds to Figure 2(c)
and finds clusters in the data after conflicting data
is erased. This is because our method is aim-
ing at modeling not the disambiguation of cluster-
memberships but the disambiguation of senses for
each word.
</bodyText>
<sectionHeader confidence="0.992222" genericHeader="method">
4 Metric Space Implementation
</sectionHeader>
<bodyText confidence="0.999949">
So far, we have dealt with general metrics for con-
text and for sense. This section describes a spe-
cific implementation of those metrics employed in
the evaluation. We followed the previous study
by McCarthy et al. (2004), (2007), and imple-
mented a type-based WSD. The context of word
instances are tied to the distributional context of
the word type in a large corpus. To calculate sense
similarities, we used the WordNet similarity pack-
age by Pedersen et al. (2004), version 2.05. Two
measures proposed by Jiang and Conrath (1997)
and Lesk (1986) were examined, which performed
best in the previous study (McCarthy et al., 2004).
Distributional similarity (Lin, 1998) was
computed among target words, based on the
statistics of the test set and the background text
provided as the official dataset of the SemEval-2
English all-words task (Agirre et al., 2010). Those
texts were parsed using RASP parser (Briscoe
et al., 2006) version 3.1, to obtain grammatical
relations for the distributional similarity, as well
as to obtain lemmata and part-of-speech (POS)
tags which are required to look up the sense
inventory of WordNet. Based on the distributional
similarity, we just used k-nearest neighbor words
as the context of each target word. Although it is
an approximation, we can expect reliability im-
provement often seen by ignoring the lower part.
In addition, this limitation of interactions highly
reduces computational cost in particular when
applying to larger-scale problems. To do this, the
exhaustive sum Ei, i′: wi̸�wi′ in Equation (7)-(9)
is altered by the local sum Ei, i′: (wi,wi′)EPkNN,
where PkNN denotes the set of word pairs of
which either is a k-nearest neighbors of the
other. The normalizing factors 1, N, and N − Ni
in Equation (7), (8)-(9), and (11) are altered
by the actual sum of responsibilities within
</bodyText>
<footnote confidence="0.3916865">
i
those neighbors as Ei′, j, j′: (wi,wi′)EPkNN �i′j′,
</footnote>
<page confidence="0.872478">
888
</page>
<equation confidence="0.94766825">
/7 ij
Ei, i′, j, j′: (wi,wi′)E:PkNN /[.i,j′, and
Eι, i′, j, j′: (w,,,wi′)EPkNN n ι̸=i Rιj
i′j′, respectively.
</equation>
<bodyText confidence="0.998705545454546">
To treat the above similarity functions of con-
text and of sense as distance functions, we use the
conversion: d(·, ·) - −a ln(f(·, ·)/fmax), where
d denotes the objective distance function, i.e., dx
for context and ds for sense, while f and fmax de-
note the original similarity function and its max-
imum, respectively. a is a standardization co-
efficient, which is determined so that the mean
squared distance be 1 in a dataset. According to
this standardization, initial values of ax2, as2 are
always 1.
</bodyText>
<sectionHeader confidence="0.998678" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999984222222222">
To confirm the effect of the proposed smoothing
model and its combinatorial optimization scheme,
we conducted WSD evaluations. The primary
evaluations compare our method with conven-
tional ones, in Section 5.2. Supplementary eval-
uations are described in the subsequent sections
that include the comparison with SemEval-2 par-
ticipating systems, and the analysis of model dy-
namics with the experimental data.
</bodyText>
<subsectionHeader confidence="0.993763">
5.1 Evaluation Scheme
</subsectionHeader>
<bodyText confidence="0.999987491803279">
To make the evaluation comparable to state-of-
the-art systems, we used the official dataset of the
SemEval-2 English all-words WSD task (Agirre
et al., 2010), which is currently the latest pub-
lic dataset available with published results. The
dataset consists of test data and background doc-
uments of the same environment domain. The
test data consists of 1,398 target words (1,032
nouns and 366 verbs) in 5.3K running words. The
background documents consists of 2.7M running
words, which was used to compute distributional
similarity.
Precisions and recalls were all computed us-
ing the official evaluation tool scorer2 in fine-
grained measure. The tool accepts answers either
in probabilistic format (senses with probabilities
for each target word) or in deterministic format
(most likely senses, with no score information).
As the proposed method is a probability model, we
evaluated in the probabilistic way unless explicitly
noted otherwise. For this reason, we evaluated all
the sense probabilities as they were. Disambigua-
tions were executed in separate runs for nouns and
verbs, because no interaction takes place across
POS in this metric implementation. The two runs’
results were combined later to a single answer to
be input to scorer2.
The context metric space was composed by k-
nearest neighbor words of distributional similarity
(Lin, 1998), as is described in Section 4. The value
of k was evaluated for 12, 3, 5, 10, 20, 30, 50, 100,
200, 3001. As for sense metric space, we evalu-
ated two measures i.e., (Jiang and Conrath, 1997)
denoted as JCN, and (Lesk, 1986) denoted as Lesk.
In every condition, stopping criterion of iteration
is always the number of iteration (500 times), irre-
spective of the convergence in likelihood.
Primary evaluations compared our method with
two conventional methods. Those methods differ
to ours only in scoring schemes. The first one
is the method by McCarthy et al. (2004), which
determines the word sense based on sense simi-
larity and distributional similarity to the k-nearest
neighbor words of a target word by distributional
similarity. Our major advantage is the combina-
torial optimization framework, while the conven-
tional one employs word-by-word scheme. The
second one is based on the method by Patwardhan
et al. (2007), which determines the word sense by
maximizing the sum of sense similarity to the k
immediate neighbor words of a target word. The k
words were forced to be selected from other target
words of the same POS to the word of interest, so
as to make information resource equivalent to the
other comparable two methods. It is also a word-
by-word method. It exploits no distributional simi-
larity. Our major advantages are the combinatorial
optimization scheme and the smoothing model to
integrate distributional similarity. In the following
section, these comparative methods are referred to
as Mc2004 and Pat2007, respectively.
</bodyText>
<subsectionHeader confidence="0.999751">
5.2 Comparison with Conventional Methods
</subsectionHeader>
<bodyText confidence="0.999885153846154">
Let us first confirm our advantages compared to
the conventional methods of Mc2004 and Pat2007.
The comparative results are shown in Figure 3 in
recall measure. Precisions are simply omitted be-
cause the difference to the recalls are always the
number of failures on referring to WordNet by
mislabeling of lemmata or POSs, which is always
the same for the three methods. Vertical range de-
picts 95% confidence intervals. The graphs also
indicate the most-frequent-sense (MFS) baseline
estimated from out-of-domain corpora, whose re-
call is 0.505 (Agirre et al., 2010).
As we can see in Figure 3(a) and 3(b), higher
</bodyText>
<page confidence="0.994011">
889
</page>
<figure confidence="0.9998413">
Rank
Proposed (best)
MFS
0.3 0.4 0.5
Recall
Mc2004
Pat2007
0.4 0.4
1 10 100 1000
Proposed
Mc2004
Pat2007
1 10 100 1000
0.5
Proposed
MFS
0.5
MFS
Recall
Recall
</figure>
<figureCaption confidence="0.982341">
Figure 4: Comparison with the all 20 knowledge-
based systems in SemEval-2 (JCN/k =5).
</figureCaption>
<figure confidence="0.992755">
Context k-NN Context k-NN
(a) JCN (b) Lesk
</figure>
<figureCaption confidence="0.9530295">
Figure 3: Comparison to the conventional methods
that differ to our method only in scoring schemes.
</figureCaption>
<tableCaption confidence="0.839076">
Table 1: Comparison with the top-5 knowledge-
based systems in SemEval-2 (JCN/k =5).
</tableCaption>
<table confidence="0.86115775">
Rank Participants R P Rn Rv
- Proposed (best) 50.8 51.0 52.5 46.2
- MFS Baseline 50.5 50.5 52.7 44.3
1 Kulkarni et al. (2010) 49.5 51.2 51.6 43.4
</table>
<listItem confidence="0.634110666666667">
2 Tran et al. (2010) 49.3 50.6 51.6 42.6
3 Tran et al. (2010) 49.1 50.4 51.5 42.5
4 Soroa et al. (2010) 48.1 48.1 48.7 46.2
5 Tran et al. (2010) 47.9 49.2 49.4 43.4
... ... ... ... ... ...
- Random Baseline 23.2 23.2 25.3 17.2
</listItem>
<figureCaption confidence="0.9551215">
Figure 5: Model dynamics through iteration with
SemEval-2 nouns (JCN/k=5).
</figureCaption>
<figure confidence="0.988577">
1
0 100 200 300 400 500
1.09
0.5 1.08
0 100 200 300 400 500
Context Bandwidth
σX2
Iteration
Sense Probability πij
0
1
2
σs
Sense Bandwidth
</figure>
<bodyText confidence="0.9948724375">
recalls are obtained in the order of the proposed
method, Mc2004, and Pat2007 on the whole.
Comparing JCN and Lesk, difference among the
three is smaller in Lesk. It is possibly because
Lesk is a score not normalized for different word
pairs, which makes the effect of distributional sim-
ilarity unsteady especially when combining many
k-nearest words. Therefore the recalls are ex-
pected to improve if proper normalization is ap-
plied to the proposed method and Mc2004. In
JCN, the recalls of the proposed method signif-
icantly improve compared to Pat2007. Our best
recall is 0.508 with JCN and k = 5. Thus we
can conclude that, though significance depends on
metrics, our smoothing model and the optimiza-
tion scheme are effective to improve accuracies.
</bodyText>
<subsectionHeader confidence="0.999493">
5.3 Comparison with SemEval-2 Systems
</subsectionHeader>
<bodyText confidence="0.999858181818182">
We compared our best results with the participat-
ing systems of the task. Table 1 compares the
details to the top-5 systems, which only includes
unsupervised/knowledge-based ones and excludes
supervised/weakly-supervised ones. Those values
are transcribed from the official report (Agirre et
al., 2010). “R” and “P” denote the recall and the
precision for the whole dataset, while “Rn” and
“Rv” denote the recall for nouns and verbs, re-
spectively. The results are ranked by “R”, in ac-
cordance with the original report. As shown in the
table, our best results outperform all of the systems
and the MFS baseline.
Overall rankings are depicted in Figure 4. It
maps our best results in the distribution of all
the 20 unsupervised/knowledge-based participat-
ing systems. The ranges spreading left and right
are 95% confidence intervals. As is seen from
the figure, our best results are located above the
top group, which are outside the confidence inter-
vals of the other participants ranked intermediate
or lower.
</bodyText>
<subsectionHeader confidence="0.998525">
5.4 Analysis on Model Dynamics
</subsectionHeader>
<bodyText confidence="0.995254">
This section examines the model dynamics with
the SemEval-2 data, which has been illustrated
</bodyText>
<page confidence="0.988177">
890
</page>
<figure confidence="0.996417">
0.5
Recall
0.4
0 100 200 300 400 500
Iteration
</figure>
<figureCaption confidence="0.9907985">
Figure 6: Recall improvement via iteration with
SemEval-2 all POSs (JCN/k=30, Lesk/k=10).
</figureCaption>
<bodyText confidence="0.999424526315789">
with pseudo data in Section 3.2. Let us start by
looking at the upper half of Figure 5, which shows
the change of sense probabilities through itera-
tion. At the initial status (iteration 0), the prob-
abilities were all 1/2 for bi-semous words, all 1/3
for tri-semous words, and so forth. As iteration
proceeded, the probabilities gradually spread out
to either side of 1 or 0, and finally at iteration
500, we can observe that almost all the words were
clearly disambiguated. The lower half of Figure 5
shows the dynamics in bandwidths. Vertical axis
on the left is for the sense bandwidth, and on the
right is for the context bandwidth. We can ob-
serve those bandwidths became narrower as iter-
ation proceeded. Intensity of smoothing was dy-
namically adjusted by the whole disambiguation
status. These behaviors confirm that even with an
actual dataset, it works as is expected, just as illus-
trated in Figure 2.
</bodyText>
<sectionHeader confidence="0.997934" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999984">
This section discusses the validity of the proposed
method as to i) sense-interdependent disambigua-
tion and ii) reliability of data smoothing. We here
analyze the second peak conditions at k = 30
(JCN) and k = 10 (Lesk) instead of the first peak
at k = 5, because we can observe tendency the
better with the larger number of word interactions.
</bodyText>
<subsectionHeader confidence="0.982645">
6.1 Effects of Sense-interdependent
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999985789473684">
Let us first examine the effect of our sense-
interdependent disambiguation. We would like
to confirm that how the progressive disambigua-
tion is carried out. Figure 6 shows the change
of recall through iteration for JCN (k = 30) and
Lesk (k = 10). Those recalls were obtained by
evaluating the status after each iteration. The re-
calls were here evaluated both in probabilistic for-
mat and in deterministic format. From the fig-
ure we can observe that the deterministic recalls
also increased as well as the probabilistic recalls.
This means that the ranks of sense candidates for
each word were frequently altered through itera-
tion, which further means that some new infor-
mation not obtained earlier was delivered one af-
ter another to sense disambiguation for each word.
From these results, we could confirm the expected
sense-interdependency effect that a sense disam-
biguation of certain word affected to other words.
</bodyText>
<subsectionHeader confidence="0.99987">
6.2 Reliability of Smoothing as Supervision
</subsectionHeader>
<bodyText confidence="0.979905351351351">
Let us now discuss the reliability of our smoothing
model. In our method, sense disambiguation of a
word is guided by its nearby words’ extrapolation
(smoothing). Sense accuracy fully depends on the
reliability of the extrapolation. Generally speak-
ing, statistical reliability increases as the number
of random sampling increases. If we take suffi-
cient number of random words as nearby words,
the sense distribution comes close to the true dis-
tribution, and then we expect the statistically true
sense distribution should find out the true sense of
the target word, according to the distributional hy-
potheses (Harris, 1954). On the contrary, if we
take nearby words that are biased to particular
words, the sense distribution also becomes biased,
and the extrapolation becomes less reliable.
We can compute the randomness of words that
affect for sense disambiguation, by word per-
plexity. Let the word of interest be w E V .
The word perplexity is calculated as 2HJw, where
H|w denotes the entropy defined as H|w �
− Ew′EV \{w} p(w′|w)lo92 p(w′|w). The con-
ditional probability p(w′|w) denotes the proba-
bility with which a certain word w′ E V �
{w} determines the sense of w, which can
be defined as the density ratio: p(w′|w) a
� � �j,j′ 2i′j′(hij).
i: wi=w i′: wi′=w′
The relation between word perplexity and prob-
ability change for ground-truth senses of nouns
(JCN/k = 30) is shown in Figure 7. The upper his-
togram shows the change in iteration 1-100, and
the lower shows that of iteration 101-500. We di-
vide the analysis at iteration 100, because roughly
until the 100th iteration, the change in bandwidths
converged, and the number of words to interact
settled, as can be seen in Figure 5. The bars that
</bodyText>
<figure confidence="0.99043196">
Probabilistic
Deterministic
Probabilistic
Deterministic
JCN
Lesk
891
150
100
50
0
-50
15
10
5
0
-5
-10
-15
EOProb. of Ground-truth Sense
0 20 40 60 80 100
Correct Iteration 101 to 500
Wrong
0 20 40 60 80 100
Perplexity of Extrapolator Words
</figure>
<figureCaption confidence="0.9991615">
Figure 7: Correlation between reliability and per-
plexity with SemEval-2 nouns (JCN/k =30).
</figureCaption>
<bodyText confidence="0.999208857142857">
extend upward represent the sum of the amount
raised (correct change), and the bars that extend
downward represent the sum of the amount re-
duced (wrong change). From these figures, we
observe that when perplexity is sufficiently large
(&gt; 30), change occurred largely (79%) to the cor-
rect direction. In contrast, at the lower left of the
figure, where perplexity is small (&lt; 30) and band-
widths has been narrowed at iteration 101-500,
correct change occupied only 32% of the whole.
Therefore, we can conclude that if sufficiently ran-
dom samples of nearby words are provided, our
smoothing model is reliable, though it is trained in
an unsupervised fashion.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999958755102041">
As described in Section 1, graph-based WSD has
been extensively studied, since graphs are favor-
able structure to deal with interactions of data on
vertices. Conventional studies typically consider
as vertices the instances of input or target class,
e.g. knowledge-based approaches typically regard
senses as vertices (see Section 1), and corpus-
based approaches such as (V´eronis, 2004) regard
words as vertices or (Niu et al., 2005) regards con-
text as vertices. Our method can be viewed as one
of graph-based methods, but it regards input-to-
class mapping as vertices, and the edges represent
the relations both together in context and in sense.
Mihalcea (2005) proposed graph-based methods,
whose vertices are sense label hypotheses on word
sequence. Our method generalize context repre-
sentation.
In the evaluation, our method was compared
to SemEval-2 systems. The main subject of the
SemEval-2 task was domain adaptation, therefore
those systems each exploited their own adaptation
techniques. Kulkarni et al. (2010) used a Word-
Net pre-pruning. Disambiguation is performed by
considering only those candidate synsets that be-
long to the top-k largest connected components
of the WordNet on domain corpus. Tran et al.
(2010) used over 3TB domain documents acquired
by Web search. They parsed those documents
and extracted the statistics on dependency relation
for disambiguation. Soroa et al. (2010) used the
method by Agirre et al. (2009) described in Sec-
tion 1. They disambiguated each target word us-
ing its distributionally similar words instead of its
immediate context words.
The proposed method is an extension of density
estimation (Parzen, 1962), which is a construc-
tion of an estimate based on observed data. Our
method naturally extends the density estimation in
two points, which make it applicable to unsuper-
vised knowledge-based WSD. First, we introduce
stochastic treatment of data, which is no longer ob-
servations but hypotheses having ambiguity. This
extension makes the hypotheses possible to cross-
validate the plausibility each other. Second, we
extend the definition of density from Euclidean
distance to general metric, which makes the pro-
posed method applicable to a wide variety of
corpus-based context similarities and dictionary-
based sense similarities.
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999980722222222">
We proposed a novel smoothing model with a
combinatorial optimization scheme for all-words
WSD from untagged corpora. Experimental re-
sults showed that our method significantly im-
proves the accuracy of conventional methods by
exceeding most-frequent-sense baseline perfor-
mance where none of SemEval-2 unsupervised
systems reached. Detailed inspection of dynam-
ics clearly show that the proposed optimization
method effectively exploit the sense-dependency
of all-words. Moreover, our smoothing model,
though unsupervised, provides reliable supervi-
sion when sufficiently random samples of words
are available as nearby words. Thus it was con-
firmed that this method is valid for finding the op-
timal combination of word senses with large un-
tagged corpora. We hope this study would elicit
further investigation in this important area.
</bodyText>
<figure confidence="0.939338666666667">
Correct
Wrong
Iteration 1 to 100
</figure>
<page confidence="0.988286">
892
</page>
<sectionHeader confidence="0.992653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999688019417476">
Eneko Agirre and Philip Edmonds. 2006. Word sense
disambiguation: Algorithms and applications, vol-
ume 33. Springer Science+ Business Media.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33–41.
Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa,
and Informatika Fakultatea. 2009. Knowledge-
based wsd on specific domains: performing better
than generic supervised wsd. In Proceedings of the
21st international jont conference on Artifical intel-
ligence, pages 1501–1506.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 75–80.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceed-
ings of the COLING/ACL on Interactive presenta-
tion sessions, pages 77–80.
Arthur Pentland Dempster, Nan McKenzie Laird, and
Donald Bruce Rubin. 1977. Maximum likelihood
from incomplete data via the em algorithm. Journal
of the Royal Statistical Society. Series B (Method-
ological), pages 1–38.
Zellig Sabbetai Harris. 1954. Distributional structure.
Word.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. arXiv preprint cmp-lg/9709008.
Anup Kulkarni, Mitesh M. Khapra, Saurabh Sohoney,
and Pushpak Bhattacharyya. 2010. CFILT: Re-
source conscious approaches for all-words domain
specific. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 421–426.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th annual international conference on Systems
documentation, pages 24–26.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768–774.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, pages 279–286.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553–590.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 411–418.
Roberto Navigli and Mirella Lapata. 2007. Graph
connectivity measures for unsupervised word sense
disambiguation. In Proceedings of the 20th inter-
national joint conference on Artifical intelligence,
pages 1683–1688.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.
2005. Word sense disambiguation using label prop-
agation based semi-supervised learning. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 395–402.
Emanuel Parzen. 1962. On estimation of a probability
density function and mode. The annals of mathe-
matical statistics, 33(3):1065–1076.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2007. UMND1: Unsupervised word
sense disambiguation using contextual semantic re-
latedness. In proceedings of the 4th International
Workshop on Semantic Evaluations, pages 390–393.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, pages 38–41.
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle,
Monica Monachini, Jessie Lo, Shu-Kai Hsieh,
Wauter Bosma, and Piek Vossen. 2010. Kyoto: An
integrated system for specific domain WSD. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 417–420.
Andrew Tran, Chris Bowes, David Brown, Ping Chen,
Max Choly, and Wei Ding. 2010. TreeMatch: A
fully unsupervised WSD system using dependency
knowledge on a specific domain. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 396–401.
Jean V´eronis. 2004. HyperLex: lexical cartography
for information retrieval. Computer Speech &amp; Lan-
guage, 18(3):223–252.
</reference>
<page confidence="0.9993">
893
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.205288">
<title confidence="0.992356">Density Maximization in Context-Sense Metric for All-words WSD</title>
<author confidence="0.830278">Technology R&amp;D Center</author>
<author confidence="0.830278">Mitsubishi Electric</author>
<address confidence="0.510619333333333">5-1-1 Ofuna, Kamakura, Kanagawa 247-8501, Information and Telecommunication Institute, Waseda 1-3-10 Nishi-Waseda, Shinjuku-ku, Tokyo 169-0051, Japan</address>
<abstract confidence="0.984662">paper proposes a novel a optimization all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense to cope with resulting from a large variety of linguistic conand sense, as well as to exploit sensethe words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval- 2 unsupervised systems reached.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Philip Edmonds</author>
</authors>
<title>Word sense disambiguation: Algorithms and applications,</title>
<date>2006</date>
<volume>33</volume>
<publisher>Springer Science+ Business Media.</publisher>
<contexts>
<context position="2287" citStr="Agirre and Edmonds, 2006" startWordPosition="331" endWordPosition="334">rely supervised approaches inherently suffer from data-sparsity problem. The all-words task is also characterized by sense-interdependency of target words. As the target words are typically taken from the same text string, they are naturally expected to be interrelated. Disambiguation of a word should affect other words as an important clue. From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied. They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. (For reviews, see (Agirre and Edmonds, 2006; Navigli, 2009).) In recent years, graph-based methods have attracted considerable attentions (Mihalcea, 2005; Navigli and Lapata, 2007; Agirre and Soroa, 2009). On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generaliza</context>
</contexts>
<marker>Agirre, Edmonds, 2006</marker>
<rawString>Eneko Agirre and Philip Edmonds. 2006. Word sense disambiguation: Algorithms and applications, volume 33. Springer Science+ Business Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="2448" citStr="Agirre and Soroa, 2009" startWordPosition="352" endWordPosition="355"> target words are typically taken from the same text string, they are naturally expected to be interrelated. Disambiguation of a word should affect other words as an important clue. From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied. They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. (For reviews, see (Agirre and Edmonds, 2006; Navigli, 2009).) In recent years, graph-based methods have attracted considerable attentions (Mihalcea, 2005; Navigli and Lapata, 2007; Agirre and Soroa, 2009). On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context. There exist several studies that enrich immediate context with large corpus statistics. McCarthy et al. (2004) proposed a method t</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez De Lacalle, Aitor Soroa, and Informatika Fakultatea.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence,</booktitle>
<pages>1501--1506</pages>
<marker>Agirre, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and Informatika Fakultatea. 2009. Knowledgebased wsd on specific domains: performing better than generic supervised wsd. In Proceedings of the 21st international jont conference on Artifical intelligence, pages 1501–1506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Christiane Fellbaum</author>
<author>Shu-Kai Hsieh</author>
<author>Maurizio Tesconi</author>
<author>Monica Monachini</author>
<author>Piek Vossen</author>
<author>Roxanne Segers</author>
</authors>
<title>Semeval-2010 task 17: All-words word sense disambiguation on a specific domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>75--80</pages>
<marker>Agirre, de Lacalle, Fellbaum, Hsieh, Tesconi, Monachini, Vossen, Segers, 2010</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Christiane Fellbaum, Shu-Kai Hsieh, Maurizio Tesconi, Monica Monachini, Piek Vossen, and Roxanne Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 75–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the rasp system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<pages>77--80</pages>
<contexts>
<context position="19054" citStr="Briscoe et al., 2006" startWordPosition="3144" endWordPosition="3147"> the distributional context of the word type in a large corpus. To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. (2004), version 2.05. Two measures proposed by Jiang and Conrath (1997) and Lesk (1986) were examined, which performed best in the previous study (McCarthy et al., 2004). Distributional similarity (Lin, 1998) was computed among target words, based on the statistics of the test set and the background text provided as the official dataset of the SemEval-2 English all-words task (Agirre et al., 2010). Those texts were parsed using RASP parser (Briscoe et al., 2006) version 3.1, to obtain grammatical relations for the distributional similarity, as well as to obtain lemmata and part-of-speech (POS) tags which are required to look up the sense inventory of WordNet. Based on the distributional similarity, we just used k-nearest neighbor words as the context of each target word. Although it is an approximation, we can expect reliability improvement often seen by ignoring the lower part. In addition, this limitation of interactions highly reduces computational cost in particular when applying to larger-scale problems. To do this, the exhaustive sum Ei, i′: wi</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the rasp system. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 77–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Pentland Dempster</author>
<author>Nan McKenzie Laird</author>
<author>Donald Bruce Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>1--38</pages>
<contexts>
<context position="13917" citStr="Dempster et al., 1977" startWordPosition="2301" endWordPosition="2304">N − Ni Intuitively, Equations (7)-(9) are interpreted as follows. As for Equation (7), the right-hand side of the equation can be divided as the left term and the right term both in the numerator and in the denominator. The left term requires πij to agree with the ratio of responsibility of the whole to hij. The right term requires πij to agree with the ratio of responsibility of hij to the whole. As for Equation (8), (9), the optimal solution is the mean squared distance in context, and in sense, weighted by responsibility. To obtain the actual values of the optimal parameters, EM algorithm (Dempster et al., 1977) is applied. This is because Equations (7)-(9) are circular definitions, which include the objective parameters implicitly in the right hand side, thus the solution is not obtained analytically. EM algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. Applying the EM algorithm to our model, we obtain the following steps: Step 1. Initialization: Set initial values to 7r, σx, and σs. As for sense probabilities, we set the uniform probability in accordance with the number of sense cand</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Pentland Dempster, Nan McKenzie Laird, and Donald Bruce Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Sabbetai Harris</author>
</authors>
<date>1954</date>
<note>Distributional structure. Word.</note>
<contexts>
<context position="30636" citStr="Harris, 1954" startWordPosition="5056" endWordPosition="5057"> discuss the reliability of our smoothing model. In our method, sense disambiguation of a word is guided by its nearby words’ extrapolation (smoothing). Sense accuracy fully depends on the reliability of the extrapolation. Generally speaking, statistical reliability increases as the number of random sampling increases. If we take sufficient number of random words as nearby words, the sense distribution comes close to the true distribution, and then we expect the statistically true sense distribution should find out the true sense of the target word, according to the distributional hypotheses (Harris, 1954). On the contrary, if we take nearby words that are biased to particular words, the sense distribution also becomes biased, and the extrapolation becomes less reliable. We can compute the randomness of words that affect for sense disambiguation, by word perplexity. Let the word of interest be w E V . The word perplexity is calculated as 2HJw, where H|w denotes the entropy defined as H|w � − Ew′EV \{w} p(w′|w)lo92 p(w′|w). The conditional probability p(w′|w) denotes the probability with which a certain word w′ E V � {w} determines the sense of w, which can be defined as the density ratio: p(w′|</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Sabbetai Harris. 1954. Distributional structure. Word.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy. arXiv preprint cmp-lg/9709008.</title>
<date>1997</date>
<contexts>
<context position="18659" citStr="Jiang and Conrath (1997)" startWordPosition="3081" endWordPosition="3084">termemberships but the disambiguation of senses for each word. 4 Metric Space Implementation So far, we have dealt with general metrics for context and for sense. This section describes a specific implementation of those metrics employed in the evaluation. We followed the previous study by McCarthy et al. (2004), (2007), and implemented a type-based WSD. The context of word instances are tied to the distributional context of the word type in a large corpus. To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. (2004), version 2.05. Two measures proposed by Jiang and Conrath (1997) and Lesk (1986) were examined, which performed best in the previous study (McCarthy et al., 2004). Distributional similarity (Lin, 1998) was computed among target words, based on the statistics of the test set and the background text provided as the official dataset of the SemEval-2 English all-words task (Agirre et al., 2010). Those texts were parsed using RASP parser (Briscoe et al., 2006) version 3.1, to obtain grammatical relations for the distributional similarity, as well as to obtain lemmata and part-of-speech (POS) tags which are required to look up the sense inventory of WordNet. Bas</context>
<context position="22612" citStr="Jiang and Conrath, 1997" startWordPosition="3721" endWordPosition="3724">plicitly noted otherwise. For this reason, we evaluated all the sense probabilities as they were. Disambiguations were executed in separate runs for nouns and verbs, because no interaction takes place across POS in this metric implementation. The two runs’ results were combined later to a single answer to be input to scorer2. The context metric space was composed by knearest neighbor words of distributional similarity (Lin, 1998), as is described in Section 4. The value of k was evaluated for 12, 3, 5, 10, 20, 30, 50, 100, 200, 3001. As for sense metric space, we evaluated two measures i.e., (Jiang and Conrath, 1997) denoted as JCN, and (Lesk, 1986) denoted as Lesk. In every condition, stopping criterion of iteration is always the number of iteration (500 times), irrespective of the convergence in likelihood. Primary evaluations compared our method with two conventional methods. Those methods differ to ours only in scoring schemes. The first one is the method by McCarthy et al. (2004), which determines the word sense based on sense similarity and distributional similarity to the k-nearest neighbor words of a target word by distributional similarity. Our major advantage is the combinatorial optimization fr</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. arXiv preprint cmp-lg/9709008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anup Kulkarni</author>
<author>Mitesh M Khapra</author>
<author>Saurabh Sohoney</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>CFILT: Resource conscious approaches for all-words domain specific.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>421--426</pages>
<contexts>
<context position="25215" citStr="Kulkarni et al. (2010)" startWordPosition="4148" endWordPosition="4151">e 3(a) and 3(b), higher 889 Rank Proposed (best) MFS 0.3 0.4 0.5 Recall Mc2004 Pat2007 0.4 0.4 1 10 100 1000 Proposed Mc2004 Pat2007 1 10 100 1000 0.5 Proposed MFS 0.5 MFS Recall Recall Figure 4: Comparison with the all 20 knowledgebased systems in SemEval-2 (JCN/k =5). Context k-NN Context k-NN (a) JCN (b) Lesk Figure 3: Comparison to the conventional methods that differ to our method only in scoring schemes. Table 1: Comparison with the top-5 knowledgebased systems in SemEval-2 (JCN/k =5). Rank Participants R P Rn Rv - Proposed (best) 50.8 51.0 52.5 46.2 - MFS Baseline 50.5 50.5 52.7 44.3 1 Kulkarni et al. (2010) 49.5 51.2 51.6 43.4 2 Tran et al. (2010) 49.3 50.6 51.6 42.6 3 Tran et al. (2010) 49.1 50.4 51.5 42.5 4 Soroa et al. (2010) 48.1 48.1 48.7 46.2 5 Tran et al. (2010) 47.9 49.2 49.4 43.4 ... ... ... ... ... ... - Random Baseline 23.2 23.2 25.3 17.2 Figure 5: Model dynamics through iteration with SemEval-2 nouns (JCN/k=5). 1 0 100 200 300 400 500 1.09 0.5 1.08 0 100 200 300 400 500 Context Bandwidth σX2 Iteration Sense Probability πij 0 1 2 σs Sense Bandwidth recalls are obtained in the order of the proposed method, Mc2004, and Pat2007 on the whole. Comparing JCN and Lesk, difference among the t</context>
<context position="33741" citStr="Kulkarni et al. (2010)" startWordPosition="5571" endWordPosition="5574">ds as vertices or (Niu et al., 2005) regards context as vertices. Our method can be viewed as one of graph-based methods, but it regards input-toclass mapping as vertices, and the edges represent the relations both together in context and in sense. Mihalcea (2005) proposed graph-based methods, whose vertices are sense label hypotheses on word sequence. Our method generalize context representation. In the evaluation, our method was compared to SemEval-2 systems. The main subject of the SemEval-2 task was domain adaptation, therefore those systems each exploited their own adaptation techniques. Kulkarni et al. (2010) used a WordNet pre-pruning. Disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components of the WordNet on domain corpus. Tran et al. (2010) used over 3TB domain documents acquired by Web search. They parsed those documents and extracted the statistics on dependency relation for disambiguation. Soroa et al. (2010) used the method by Agirre et al. (2009) described in Section 1. They disambiguated each target word using its distributionally similar words instead of its immediate context words. The proposed method is an extension o</context>
</contexts>
<marker>Kulkarni, Khapra, Sohoney, Bhattacharyya, 2010</marker>
<rawString>Anup Kulkarni, Mitesh M. Khapra, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. CFILT: Resource conscious approaches for all-words domain specific. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 421–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="18675" citStr="Lesk (1986)" startWordPosition="3086" endWordPosition="3087">iguation of senses for each word. 4 Metric Space Implementation So far, we have dealt with general metrics for context and for sense. This section describes a specific implementation of those metrics employed in the evaluation. We followed the previous study by McCarthy et al. (2004), (2007), and implemented a type-based WSD. The context of word instances are tied to the distributional context of the word type in a large corpus. To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. (2004), version 2.05. Two measures proposed by Jiang and Conrath (1997) and Lesk (1986) were examined, which performed best in the previous study (McCarthy et al., 2004). Distributional similarity (Lin, 1998) was computed among target words, based on the statistics of the test set and the background text provided as the official dataset of the SemEval-2 English all-words task (Agirre et al., 2010). Those texts were parsed using RASP parser (Briscoe et al., 2006) version 3.1, to obtain grammatical relations for the distributional similarity, as well as to obtain lemmata and part-of-speech (POS) tags which are required to look up the sense inventory of WordNet. Based on the distri</context>
<context position="22645" citStr="Lesk, 1986" startWordPosition="3729" endWordPosition="3730">evaluated all the sense probabilities as they were. Disambiguations were executed in separate runs for nouns and verbs, because no interaction takes place across POS in this metric implementation. The two runs’ results were combined later to a single answer to be input to scorer2. The context metric space was composed by knearest neighbor words of distributional similarity (Lin, 1998), as is described in Section 4. The value of k was evaluated for 12, 3, 5, 10, 20, 30, 50, 100, 200, 3001. As for sense metric space, we evaluated two measures i.e., (Jiang and Conrath, 1997) denoted as JCN, and (Lesk, 1986) denoted as Lesk. In every condition, stopping criterion of iteration is always the number of iteration (500 times), irrespective of the convergence in likelihood. Primary evaluations compared our method with two conventional methods. Those methods differ to ours only in scoring schemes. The first one is the method by McCarthy et al. (2004), which determines the word sense based on sense similarity and distributional similarity to the k-nearest neighbor words of a target word by distributional similarity. Our major advantage is the combinatorial optimization framework, while the conventional o</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguisticsVolume 2,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="18796" citStr="Lin, 1998" startWordPosition="3103" endWordPosition="3104"> for sense. This section describes a specific implementation of those metrics employed in the evaluation. We followed the previous study by McCarthy et al. (2004), (2007), and implemented a type-based WSD. The context of word instances are tied to the distributional context of the word type in a large corpus. To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. (2004), version 2.05. Two measures proposed by Jiang and Conrath (1997) and Lesk (1986) were examined, which performed best in the previous study (McCarthy et al., 2004). Distributional similarity (Lin, 1998) was computed among target words, based on the statistics of the test set and the background text provided as the official dataset of the SemEval-2 English all-words task (Agirre et al., 2010). Those texts were parsed using RASP parser (Briscoe et al., 2006) version 3.1, to obtain grammatical relations for the distributional similarity, as well as to obtain lemmata and part-of-speech (POS) tags which are required to look up the sense inventory of WordNet. Based on the distributional similarity, we just used k-nearest neighbor words as the context of each target word. Although it is an approxim</context>
<context position="22421" citStr="Lin, 1998" startWordPosition="3685" endWordPosition="3686"> word) or in deterministic format (most likely senses, with no score information). As the proposed method is a probability model, we evaluated in the probabilistic way unless explicitly noted otherwise. For this reason, we evaluated all the sense probabilities as they were. Disambiguations were executed in separate runs for nouns and verbs, because no interaction takes place across POS in this metric implementation. The two runs’ results were combined later to a single answer to be input to scorer2. The context metric space was composed by knearest neighbor words of distributional similarity (Lin, 1998), as is described in Section 4. The value of k was evaluated for 12, 3, 5, 10, 20, 30, 50, 100, 200, 3001. As for sense metric space, we evaluated two measures i.e., (Jiang and Conrath, 1997) denoted as JCN, and (Lesk, 1986) denoted as Lesk. In every condition, stopping criterion of iteration is always the number of iteration (500 times), irrespective of the convergence in likelihood. Primary evaluations compared our method with two conventional methods. Those methods differ to ours only in scoring schemes. The first one is the method by McCarthy et al. (2004), which determines the word sense </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguisticsVolume 2, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>279--286</pages>
<contexts>
<context position="3028" citStr="McCarthy et al. (2004)" startWordPosition="429" endWordPosition="432"> and Lapata, 2007; Agirre and Soroa, 2009). On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context. There exist several studies that enrich immediate context with large corpus statistics. McCarthy et al. (2004) proposed a method to combine sense similarity with distributional similarity and configured predominant sense score. Distributional similarity was used to weight the influence of context words, based on large-scale statistics. The method achieved successful WSD accuracy. Agirre et al. (2009) used a k-nearest words on distributional similarity as context words. They apply a LKB graph-based WSD to a target word together with the distributional context words, and showed that it yields better results on a domain dataset than just using immediate context words. Though these 884 Proceedings of the </context>
<context position="18348" citStr="McCarthy et al. (2004)" startWordPosition="3029" endWordPosition="3032">bability distribution is, roughly speaking, offset toward the center of WordNet, which is not what we want. In contrast, the proposed method proceeds to Figure 2(c) and finds clusters in the data after conflicting data is erased. This is because our method is aiming at modeling not the disambiguation of clustermemberships but the disambiguation of senses for each word. 4 Metric Space Implementation So far, we have dealt with general metrics for context and for sense. This section describes a specific implementation of those metrics employed in the evaluation. We followed the previous study by McCarthy et al. (2004), (2007), and implemented a type-based WSD. The context of word instances are tied to the distributional context of the word type in a large corpus. To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. (2004), version 2.05. Two measures proposed by Jiang and Conrath (1997) and Lesk (1986) were examined, which performed best in the previous study (McCarthy et al., 2004). Distributional similarity (Lin, 1998) was computed among target words, based on the statistics of the test set and the background text provided as the official dataset of the SemEval-2 Engl</context>
<context position="22987" citStr="McCarthy et al. (2004)" startWordPosition="3781" endWordPosition="3784"> neighbor words of distributional similarity (Lin, 1998), as is described in Section 4. The value of k was evaluated for 12, 3, 5, 10, 20, 30, 50, 100, 200, 3001. As for sense metric space, we evaluated two measures i.e., (Jiang and Conrath, 1997) denoted as JCN, and (Lesk, 1986) denoted as Lesk. In every condition, stopping criterion of iteration is always the number of iteration (500 times), irrespective of the convergence in likelihood. Primary evaluations compared our method with two conventional methods. Those methods differ to ours only in scoring schemes. The first one is the method by McCarthy et al. (2004), which determines the word sense based on sense similarity and distributional similarity to the k-nearest neighbor words of a target word by distributional similarity. Our major advantage is the combinatorial optimization framework, while the conventional one employs word-by-word scheme. The second one is based on the method by Patwardhan et al. (2007), which determines the word sense by maximizing the sum of sense similarity to the k immediate neighbor words of a target word. The k words were forced to be selected from other target words of the same POS to the word of interest, so as to make</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics, 33(4):553–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="2397" citStr="Mihalcea, 2005" startWordPosition="346" endWordPosition="347">se-interdependency of target words. As the target words are typically taken from the same text string, they are naturally expected to be interrelated. Disambiguation of a word should affect other words as an important clue. From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied. They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. (For reviews, see (Agirre and Edmonds, 2006; Navigli, 2009).) In recent years, graph-based methods have attracted considerable attentions (Mihalcea, 2005; Navigli and Lapata, 2007; Agirre and Soroa, 2009). On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context. There exist several studies that enrich immediate context with large corpus sta</context>
<context position="33383" citStr="Mihalcea (2005)" startWordPosition="5522" endWordPosition="5523">s been extensively studied, since graphs are favorable structure to deal with interactions of data on vertices. Conventional studies typically consider as vertices the instances of input or target class, e.g. knowledge-based approaches typically regard senses as vertices (see Section 1), and corpusbased approaches such as (V´eronis, 2004) regard words as vertices or (Niu et al., 2005) regards context as vertices. Our method can be viewed as one of graph-based methods, but it regards input-toclass mapping as vertices, and the edges represent the relations both together in context and in sense. Mihalcea (2005) proposed graph-based methods, whose vertices are sense label hypotheses on word sequence. Our method generalize context representation. In the evaluation, our method was compared to SemEval-2 systems. The main subject of the SemEval-2 task was domain adaptation, therefore those systems each exploited their own adaptation techniques. Kulkarni et al. (2010) used a WordNet pre-pruning. Disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components of the WordNet on domain corpus. Tran et al. (2010) used over 3TB domain documents acqu</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>Graph connectivity measures for unsupervised word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>1683--1688</pages>
<contexts>
<context position="2423" citStr="Navigli and Lapata, 2007" startWordPosition="348" endWordPosition="351">cy of target words. As the target words are typically taken from the same text string, they are naturally expected to be interrelated. Disambiguation of a word should affect other words as an important clue. From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied. They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. (For reviews, see (Agirre and Edmonds, 2006; Navigli, 2009).) In recent years, graph-based methods have attracted considerable attentions (Mihalcea, 2005; Navigli and Lapata, 2007; Agirre and Soroa, 2009). On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context. There exist several studies that enrich immediate context with large corpus statistics. McCarthy et al. (</context>
</contexts>
<marker>Navigli, Lapata, 2007</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2007. Graph connectivity measures for unsupervised word sense disambiguation. In Proceedings of the 20th international joint conference on Artifical intelligence, pages 1683–1688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="2303" citStr="Navigli, 2009" startWordPosition="335" endWordPosition="336"> inherently suffer from data-sparsity problem. The all-words task is also characterized by sense-interdependency of target words. As the target words are typically taken from the same text string, they are naturally expected to be interrelated. Disambiguation of a word should affect other words as an important clue. From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied. They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. (For reviews, see (Agirre and Edmonds, 2006; Navigli, 2009).) In recent years, graph-based methods have attracted considerable attentions (Mihalcea, 2005; Navigli and Lapata, 2007; Agirre and Soroa, 2009). On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond imme</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew Lim Tan</author>
</authors>
<title>Word sense disambiguation using label propagation based semi-supervised learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>395--402</pages>
<contexts>
<context position="33155" citStr="Niu et al., 2005" startWordPosition="5482" endWordPosition="5485">fore, we can conclude that if sufficiently random samples of nearby words are provided, our smoothing model is reliable, though it is trained in an unsupervised fashion. 7 Related Work As described in Section 1, graph-based WSD has been extensively studied, since graphs are favorable structure to deal with interactions of data on vertices. Conventional studies typically consider as vertices the instances of input or target class, e.g. knowledge-based approaches typically regard senses as vertices (see Section 1), and corpusbased approaches such as (V´eronis, 2004) regard words as vertices or (Niu et al., 2005) regards context as vertices. Our method can be viewed as one of graph-based methods, but it regards input-toclass mapping as vertices, and the edges represent the relations both together in context and in sense. Mihalcea (2005) proposed graph-based methods, whose vertices are sense label hypotheses on word sequence. Our method generalize context representation. In the evaluation, our method was compared to SemEval-2 systems. The main subject of the SemEval-2 task was domain adaptation, therefore those systems each exploited their own adaptation techniques. Kulkarni et al. (2010) used a WordNe</context>
</contexts>
<marker>Niu, Ji, Tan, 2005</marker>
<rawString>Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005. Word sense disambiguation using label propagation based semi-supervised learning. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 395–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel Parzen</author>
</authors>
<title>On estimation of a probability density function and mode. The annals of mathematical statistics,</title>
<date>1962</date>
<pages>33--3</pages>
<contexts>
<context position="6710" citStr="Parzen, 1962" startWordPosition="1041" endWordPosition="1042">In natural language processing, continuity has been sometimes assumed for linguistic phenomena including word context for corpus based WSD. As for classes or senses, it may not be a common assumption. However, when the classes for all-words WSD are enormous, finegrained, and can be associated with distance, we can rather naturally assume the continuity also for senses. According to the nature of continuity, once given a hypothesis hij for a certain word, we can extrapolate the hypothesis for another word of another sense hi′j′ = (xi′, si′j′) sufficiently close to hij. Using a Gaussian kernel (Parzen, 1962) as a smoothing model, the probability density extrapolated at hi′j′ given hij is defined by their distance as follows: K(hij, hi′j′) (1) 1 p 2Qx2 2Qg2 r_ dx2(xi,xi′) _ ds2(sij,si′j′)1 2πσxσs ex L J , where σx and σs are parameters of positive real number σx, σs E R+ called kernel bandwidths. They control the smoothing intensity in context and in sense, respectively. Our objective is to determine the optimal sense for all the target words simultaneously. It is essentially a 0-1 integer programing problem, and is not computationally tractable. We relax the integer constraints by introducing a s</context>
<context position="34376" citStr="Parzen, 1962" startWordPosition="5673" endWordPosition="5674">uning. Disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components of the WordNet on domain corpus. Tran et al. (2010) used over 3TB domain documents acquired by Web search. They parsed those documents and extracted the statistics on dependency relation for disambiguation. Soroa et al. (2010) used the method by Agirre et al. (2009) described in Section 1. They disambiguated each target word using its distributionally similar words instead of its immediate context words. The proposed method is an extension of density estimation (Parzen, 1962), which is a construction of an estimate based on observed data. Our method naturally extends the density estimation in two points, which make it applicable to unsupervised knowledge-based WSD. First, we introduce stochastic treatment of data, which is no longer observations but hypotheses having ambiguity. This extension makes the hypotheses possible to crossvalidate the plausibility each other. Second, we extend the definition of density from Euclidean distance to general metric, which makes the proposed method applicable to a wide variety of corpus-based context similarities and dictionaryb</context>
</contexts>
<marker>Parzen, 1962</marker>
<rawString>Emanuel Parzen. 1962. On estimation of a probability density function and mode. The annals of mathematical statistics, 33(3):1065–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>UMND1: Unsupervised word sense disambiguation using contextual semantic relatedness.</title>
<date>2007</date>
<booktitle>In proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>390--393</pages>
<contexts>
<context position="23342" citStr="Patwardhan et al. (2007)" startWordPosition="3836" endWordPosition="3839">always the number of iteration (500 times), irrespective of the convergence in likelihood. Primary evaluations compared our method with two conventional methods. Those methods differ to ours only in scoring schemes. The first one is the method by McCarthy et al. (2004), which determines the word sense based on sense similarity and distributional similarity to the k-nearest neighbor words of a target word by distributional similarity. Our major advantage is the combinatorial optimization framework, while the conventional one employs word-by-word scheme. The second one is based on the method by Patwardhan et al. (2007), which determines the word sense by maximizing the sum of sense similarity to the k immediate neighbor words of a target word. The k words were forced to be selected from other target words of the same POS to the word of interest, so as to make information resource equivalent to the other comparable two methods. It is also a wordby-word method. It exploits no distributional similarity. Our major advantages are the combinatorial optimization scheme and the smoothing model to integrate distributional similarity. In the following section, these comparative methods are referred to as Mc2004 and P</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2007</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2007. UMND1: Unsupervised word sense disambiguation using contextual semantic relatedness. In proceedings of the 4th International Workshop on Semantic Evaluations, pages 390–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL</title>
<date>2004</date>
<pages>38--41</pages>
<contexts>
<context position="18594" citStr="Pedersen et al. (2004)" startWordPosition="3071" endWordPosition="3074">our method is aiming at modeling not the disambiguation of clustermemberships but the disambiguation of senses for each word. 4 Metric Space Implementation So far, we have dealt with general metrics for context and for sense. This section describes a specific implementation of those metrics employed in the evaluation. We followed the previous study by McCarthy et al. (2004), (2007), and implemented a type-based WSD. The context of word instances are tied to the distributional context of the word type in a large corpus. To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. (2004), version 2.05. Two measures proposed by Jiang and Conrath (1997) and Lesk (1986) were examined, which performed best in the previous study (McCarthy et al., 2004). Distributional similarity (Lin, 1998) was computed among target words, based on the statistics of the test set and the background text provided as the official dataset of the SemEval-2 English all-words task (Agirre et al., 2010). Those texts were parsed using RASP parser (Briscoe et al., 2006) version 3.1, to obtain grammatical relations for the distributional similarity, as well as to obtain lemmata and part-of-speech (POS) tags </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Soroa</author>
</authors>
<title>Eneko Agirre, Oier Lopez</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>417--420</pages>
<marker>Soroa, 2010</marker>
<rawString>Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle, Monica Monachini, Jessie Lo, Shu-Kai Hsieh, Wauter Bosma, and Piek Vossen. 2010. Kyoto: An integrated system for specific domain WSD. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 417–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Tran</author>
<author>Chris Bowes</author>
<author>David Brown</author>
<author>Ping Chen</author>
<author>Max Choly</author>
<author>Wei Ding</author>
</authors>
<title>TreeMatch: A fully unsupervised WSD system using dependency knowledge on a specific domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>396--401</pages>
<contexts>
<context position="25256" citStr="Tran et al. (2010)" startWordPosition="4157" endWordPosition="4160">st) MFS 0.3 0.4 0.5 Recall Mc2004 Pat2007 0.4 0.4 1 10 100 1000 Proposed Mc2004 Pat2007 1 10 100 1000 0.5 Proposed MFS 0.5 MFS Recall Recall Figure 4: Comparison with the all 20 knowledgebased systems in SemEval-2 (JCN/k =5). Context k-NN Context k-NN (a) JCN (b) Lesk Figure 3: Comparison to the conventional methods that differ to our method only in scoring schemes. Table 1: Comparison with the top-5 knowledgebased systems in SemEval-2 (JCN/k =5). Rank Participants R P Rn Rv - Proposed (best) 50.8 51.0 52.5 46.2 - MFS Baseline 50.5 50.5 52.7 44.3 1 Kulkarni et al. (2010) 49.5 51.2 51.6 43.4 2 Tran et al. (2010) 49.3 50.6 51.6 42.6 3 Tran et al. (2010) 49.1 50.4 51.5 42.5 4 Soroa et al. (2010) 48.1 48.1 48.7 46.2 5 Tran et al. (2010) 47.9 49.2 49.4 43.4 ... ... ... ... ... ... - Random Baseline 23.2 23.2 25.3 17.2 Figure 5: Model dynamics through iteration with SemEval-2 nouns (JCN/k=5). 1 0 100 200 300 400 500 1.09 0.5 1.08 0 100 200 300 400 500 Context Bandwidth σX2 Iteration Sense Probability πij 0 1 2 σs Sense Bandwidth recalls are obtained in the order of the proposed method, Mc2004, and Pat2007 on the whole. Comparing JCN and Lesk, difference among the three is smaller in Lesk. It is possibly b</context>
<context position="33947" citStr="Tran et al. (2010)" startWordPosition="5604" endWordPosition="5607">both together in context and in sense. Mihalcea (2005) proposed graph-based methods, whose vertices are sense label hypotheses on word sequence. Our method generalize context representation. In the evaluation, our method was compared to SemEval-2 systems. The main subject of the SemEval-2 task was domain adaptation, therefore those systems each exploited their own adaptation techniques. Kulkarni et al. (2010) used a WordNet pre-pruning. Disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components of the WordNet on domain corpus. Tran et al. (2010) used over 3TB domain documents acquired by Web search. They parsed those documents and extracted the statistics on dependency relation for disambiguation. Soroa et al. (2010) used the method by Agirre et al. (2009) described in Section 1. They disambiguated each target word using its distributionally similar words instead of its immediate context words. The proposed method is an extension of density estimation (Parzen, 1962), which is a construction of an estimate based on observed data. Our method naturally extends the density estimation in two points, which make it applicable to unsupervise</context>
</contexts>
<marker>Tran, Bowes, Brown, Chen, Choly, Ding, 2010</marker>
<rawString>Andrew Tran, Chris Bowes, David Brown, Ping Chen, Max Choly, and Wei Ding. 2010. TreeMatch: A fully unsupervised WSD system using dependency knowledge on a specific domain. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 396–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>HyperLex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. HyperLex: lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>