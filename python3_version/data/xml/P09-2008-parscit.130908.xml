<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.093607">
<title confidence="0.9994125">
A Novel Word Segmentation Approach for
Written Languages with Word Boundary Markers
</title>
<author confidence="0.995878">
Han-Cheol Cho†, Do-Gil Lee§, Jung-Tae Lee§, Pontus Stenetorp†, Jun’ichi Tsujii†and Hae-Chang Rim§
</author>
<affiliation confidence="0.993633">
†Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan
§Dept. of Computer &amp; Radio Communications Engineering, Korea University, Seoul, Korea
</affiliation>
<email confidence="0.997288">
{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994009375">
Most NLP applications work under the as-
sumption that a user input is error-free;
thus, word segmentation (WS) for written
languages that use word boundary mark-
ers (WBMs), such as spaces, has been re-
garded as a trivial issue. However, noisy
real-world texts, such as blogs, e-mails,
and SMS, may contain spacing errors that
require correction before further process-
ing may take place. For the Korean lan-
guage, many researchers have adopted a
traditional WS approach, which eliminates
all spaces in the user input and re-inserts
proper word boundaries. Unfortunately,
such an approach often exacerbates the
word spacing quality for user input, which
has few or no spacing errors; such is the
case, because a perfect WS model does
not exist. In this paper, we propose a
novel WS method that takes into consider-
ation the initial word spacing information
of the user input. Our method generates
a better output than the original user in-
put, even if the user input has few spacing
errors. Moreover, the proposed method
significantly outperforms a state-of-the-art
Korean WS model when the user input ini-
tially contains less than 10% spacing er-
rors, and performs comparably for cases
containing more spacing errors. We be-
lieve that the proposed method will be a
very practical pre-processing module.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999752913043478">
Word segmentation (WS) has been a fundamen-
tal research issue for languages that do not have
word boundary markers (WBMs); on the con-
trary, other languages that do have WBMs have re-
garded the issue as a trivial task. Texts segmented
with such WBMs, however, could contain a hu-
man writer’s intentional or un-intentional spacing
errors; and even a few spacing errors can cause
error-propagation for further NLP stages.
For written languages that have WBMs, such as
for the Korean language, the majority of recent
research has been based on a traditional WS ap-
proach (Nakagawa, 2004). The first step of the
traditional approach is to eliminate all spaces in
the user input, and then re-locate the proper places
to insert WBMs. One state-of-the-art Korean WS
model (Lee et al., 2007) is known to achieve a per-
formance of 90.31% word-unit precision, which is
comparable with other WS models for the Chinese
or Japanese language.
Still, there is a downside to the evaluation
method. If the user input has a few or no spac-
ing errors, traditional WS models may cause more
spacing errors than it correct because they produce
the same output regardless the word spacing states
of the user input.
In this paper, we propose a new WS method that
takes into account the word spacing information
from the user input. Our proposed method first
generates the best word spacing states for the user
input by using a traditional WS model; however
the method does not immediately apply the out-
put. Secondly, the method estimates a threshold
based on the word spacing quality of the user in-
put. Finally, the method uses the new word spac-
ing states that have probabilities that are higher
than the threshold.
The most important contribution of the pro-
posed method is that, for most cases, the method
generates an output that is better than the user in-
put. The experimental results show that the pro-
posed method produces a better output than the
user input even if the user input has less than 1%
spacing errors in terms of the character-unit pre-
cision. Moreover, the proposed method outper-
forms (Lee et al., 2007) significantly, when the
</bodyText>
<page confidence="0.989972">
29
</page>
<note confidence="0.925472">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29–32,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9996988">
user input initially contains less than 10% spacing
errors, and even performs comparably, when the
input contains more than 10% errors. Based on
these results, we believe that the proposed method
would be a very practical pre-processing module
for other NLP applications.
The paper is organized as follows: Section 2 ex-
plains the proposed method. Section 3 shows the
experimental results. Finally, the last section de-
scribes the contributions of the proposed method.
</bodyText>
<sectionHeader confidence="0.977975" genericHeader="method">
2 The Proposed Method
</sectionHeader>
<bodyText confidence="0.9999475">
The proposed method consists of three steps: a
baseline WS model, confidence and threshold es-
timation, and output optimization. The following
sections will explain the steps in detail.
</bodyText>
<subsectionHeader confidence="0.981767">
2.1 Baseline Word Segmentation Model
</subsectionHeader>
<bodyText confidence="0.999980791666667">
We use the tri-gram Hidden Markov Model
(HMM) of (Lee et al., 2007) as the baseline WS
model; however, we adopt the Maximum Like-
lihood (ML) decoding strategy to independently
find the best word spacing states. ML-decoding
allows us to directly compare each output to the
threshold. There is little discrepancy in accuracy
when using ML-decoding, as compared to Viterbi-
decoding, as mentioned in (Merialdo, 1994).1
Let o1,n be a sequence of n-character user input
without WBMs, xt be the best word spacing state
for ot where 1 &lt; t &lt; n. Assume that xt is either 1
(space after ot) or 0 (no space after ot). Then each
best word spacing state xt for all t can be found by
using Equation 1.
eliminating the constant parts. Every part of Equa-
tion 3 can be calculated by adding the probabilities
of all possible combinations of xt−2, xt−1, xt+1
and xt+2 values.
The model is trained by using the relative fre-
quency information of the training data, and a
smoothing technique is applied to relieve the data-
sparseness problem which is the linear interpola-
tion of n-grams that are used in (Lee et al., 2007).
</bodyText>
<subsectionHeader confidence="0.558889">
2.2 Confidence and Threshold Estimation
</subsectionHeader>
<bodyText confidence="0.9999825">
We set a variable threshold that is proportional to
the word spacing quality of the user input, Confi-
dence. Formally, we can define the threshold T as
a function of a confidence C, as in Equation 4.
</bodyText>
<equation confidence="0.999642">
T = f(C) (4)
</equation>
<bodyText confidence="0.985175923076923">
Then, we define the confidence as is done in
Equation 5. Because calculating such a variable
is impossible, we estimate the value by substi-
tuting the word spacing states produced by the
baseline WS model, xW S
1,n , with the correct word
spacing states, xcorrect
1,n , as is done in Equation 6.
This estimation is based on the assumption that
the word spacing states of the WS model is suf-
ficiently similar to the correct word spacing states
in the character-unit precision.2
# of xinput same to xcorrect
</bodyText>
<equation confidence="0.9946559">
t t
C = (5)
# of xinput
t
# of xinput same to xW S
t t
≈ (6)
# of xinput
t
St = argmax P(xt = i|o1,n) (1) f n P(xinput
iE(0,1) P(o1,n, xt = i) (2) Y k |o1,n) (7)
= argmax k=1
iE(0,1)
X= argmax P(xt = i|xt−2, ot−1, xt−1, ot)
iE(0,1)
xt−2,xt−1
X× P(ot+1|ot−1, xt−1, ot, xt = i)
xt−1
X× P(ot+2|ot, xt = i, ot+1, xt+1) (3)
xt+1
</equation>
<bodyText confidence="0.99849325">
Equation 2 is derived by applying the Bayes’
rule and by eliminating the constant denominator.
Moreover, the equation is simplified, as is Equa-
tion 3, by using the Markov assumption, and by
</bodyText>
<footnote confidence="0.7602875">
1In the preliminary experiment, Viterbi-decoding showed
a 0.5% higher word-unit precision.
</footnote>
<bodyText confidence="0.99964075">
To handle the estimation error for short sen-
tences, we use the probability generating word
spacing states of the user input with the length nor-
malization as shown in Equation 7.
</bodyText>
<figureCaption confidence="0.9220206">
Figure 1 shows that the estimated confidence of
Equation 7 is almost linearly proportional to the
true confidence of Equation 5, thus suggesting that
the threshold T can be defined as a function of the
estimated confidence of Equation 7.3
</figureCaption>
<footnote confidence="0.972349">
2In the experiment with the development data, the base-
line WS model shows about 97% character-unit precision.
3The development data is generated by randomly intro-
ducing spacing errors into correctly spaced sentences. We
think that this reflects various intentional and un-intentional
error patterns of individuals.
</footnote>
<page confidence="0.994795">
30
</page>
<figure confidence="0.999494">
100%
90%
80%
70%
60%
50%
40%
30%
20%
100% 96% 92% 88% 84% 80%
True Confidence
</figure>
<figureCaption confidence="0.984036">
Figure 1: The relationship between estimated con-
fidence and true confidence
</figureCaption>
<bodyText confidence="0.99981">
To keep the focus on the research subject of this
paper, we simply assume f(x) = x as in Equation
8, for the threshold function f.
</bodyText>
<equation confidence="0.999102">
T � f(C) = C (8)
</equation>
<bodyText confidence="0.99997125">
In the experimental results, we confirm that
even this simple threshold function can be help-
ful in improving the performance of the proposed
method against traditional WS models.
</bodyText>
<subsectionHeader confidence="0.991735">
2.3 Output Optimization
</subsectionHeader>
<bodyText confidence="0.999987866666667">
After completing the two steps described in Sec-
tion 2.1 and 2.2, we have acquired the new spacing
states for the user input generated by the baseline
WS model, and the threshold measuring the word
spacing quality of the user input.
The proposed method only applies a part of the
new word spacing states to the user input, which
have probabilities that are higher than the thresh-
old; further the method discards the other new
word spacing states that have probabilities that are
lower than the threshold. By rejecting the unreli-
able output of the baseline WS model in this way,
the proposed method can effectively improve the
performance when the user input contains a rela-
tively small number of spacing errors.
</bodyText>
<sectionHeader confidence="0.997007" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.9996455">
Two types of experiments have been performed.
In the first experiment, we investigate the level of
performance improvement based on different set-
tings of the user input’s word spacing error rate.
Because it is nearly impossible to obtain enough
test data for any error rate, we generate pseudo test
data in the same way that we generate develop-
ment data.4 In the second experiment, we attempt
</bodyText>
<footnote confidence="0.725577">
4See Footnote 3.
</footnote>
<bodyText confidence="0.999338666666667">
figuring out whether the proposed method really
improves the word spacing quality of the user in-
put in a real-world setting.
</bodyText>
<subsectionHeader confidence="0.782267333333333">
3.1 Performance Improvement according to
the Word Spacing Error Rate of User
Input
</subsectionHeader>
<bodyText confidence="0.999974972222222">
For the first experiment, we use the Sejong corpus5
from 1998-1999 (1,000,000 Korean sentences) for
the training data, and ETRI corpus (30,000 sen-
tences) for the test data (ETRI, 1999). To gener-
ate the test data that have spacing errors, we make
twenty one copies of the test data and randomly
insert spacing errors from 0% to 20% in the same
way in which we made the development data. We
feel that this strategy can model both the inten-
tional and un-intentional human error patterns.
In Figure 2, the x-axis indicates the word spac-
ing error rate of the user input in terms of the
character-unit precision, and the y-axis shows the
word-unit precision of the output. Each graph de-
picts the word-unit precision of the test corpus,
a state-of-the-art Korean WS model (Lee et al.,
2007), the baseline WS model, and the proposed
method.
Although Lee’s model is known to perform
comparably with state-of-the-art Chinese and
Japanese WS models, it does not necessarily sug-
gest that the word spacing quality of the model’s
output is better than the user input. In Figure 2,
Lee’s model exacerbates the user input when it has
spacing errors that are lower than 3%.
The proposed method, however, produces a bet-
ter output, even if the user input has 1% spacing er-
rors. Moreover, the proposed method shows a con-
siderably better performance within the 10% spac-
ing error range, as compared to Lee’s model, al-
though the baseline WS model itself does not out-
performs Lee’s model. The performance improve-
ment in this error range is fairly significant be-
cause we found that the spacing error rate of texts
collected for the second experiment was about
9.1%.
</bodyText>
<subsectionHeader confidence="0.9854395">
3.2 Performance Comparison with Web Text
having Usual Error Rate
</subsectionHeader>
<bodyText confidence="0.999866">
In the second experiment, we attempt finding out
whether the proposed method can be beneficial un-
der real-world circumstances. Web texts, which
consist of 1,000 erroneous sentences from famous
</bodyText>
<footnote confidence="0.972935">
5Details available at: http://www.sejong.or.kr/eindex.php
</footnote>
<page confidence="0.999847">
31
</page>
<figure confidence="0.994325583333333">
100%
98%
96%
94%
92%
90%
88%
86%
84%
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
word spacing error rate of user input (in character-unit precision)
Test corpus Lee&apos;s model Baseline WS model Proposed method
</figure>
<figureCaption confidence="0.995072">
Figure 2: Performance improvement according to the word spacing error rate of user input
</figureCaption>
<table confidence="0.9992878">
Method Web Text
Test Corpus 70.89%
Lee’s Model 70.45%
Baseline WS Model 69.13%
Proposed Method 73.74%
</table>
<tableCaption confidence="0.999929">
Table 1: Performance comparison with Web text
</tableCaption>
<bodyText confidence="0.999965826086957">
Web portals and personal blogs, were collected
and used as the test data. Since the test data tend
to have a similar error rate to the narrow standard
deviation, we computed the overall performance
over the average word spacing error rate, which is
9.1%. The baseline WS model is trained on the
Sejong corpus, described in Section 3.1.
The test result is shown in Table 1. The
overall performance of Lee’s model, the baseline
WS model and the proposed method decreased
by roughly 18%. We hypothesize that the per-
formance degradation probably results from the
spelling errors of the test data, and the inconsis-
tencies that exist between the training data and the
test data. However, the proposed method still im-
proves the word spacing quality of the user input
by 3%, while the two traditional WS models de-
grades the quality. Such a result indicates that
the proposed method is effective for real-world
environments, as we had intended. Furthermore,
we also believe that the performance can be im-
proved if a proper training corpus is provided, or
if a spelling correction method is integrated.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999933315789474">
In this paper, we proposed a new WS method that
uses the word spacing information of the user in-
put, for languages with WBMs. By utilizing the
user input, the proposed method effectively refines
the output of the baseline WS model and improves
the overall performance.
The most important contribution of this work is
that it produces an output that is better than the
user input even if it contains few spacing errors.
Therefore, the proposed method can be applied as
a pre-processing module for practical NLP appli-
cations without introducing a risk that would gen-
erate a worse output than the user input. Moreover,
the performance is notably better than a state-of-
the-art Korean WS model (Lee et al., 2007) within
the 10% spacing error range, which human writers
seldom exceed. It also performs comparably, even
if the user input contains more than 10% spacing
errors.
</bodyText>
<sectionHeader confidence="0.999302" genericHeader="conclusions">
5 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999573">
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999628466666667">
ETRI. 1999. Pos-tag guidelines. Technical report.
Electronics and Telecomminications Research Insti-
tute.
Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.
2007. Automatic Word Spacing Using Probabilistic
Models Based on Character n-grams. IEEE Intelli-
gent Systems, 22(1):28–35.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comput. Linguist., 20(2):155–
171.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level
information. In COLING ’04, page 466, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658818">
<title confidence="0.999233">A Novel Word Segmentation Approach for Written Languages with Word Boundary Markers</title>
<author confidence="0.992668">Do-Gil Jung-Tae Pontus Jun’ichi Hae-Chang</author>
<affiliation confidence="0.979794">School of Information Science and Technology, The University of Tokyo, Tokyo, Japan</affiliation>
<address confidence="0.67121">of Computer &amp; Radio Communications Engineering, Korea University, Seoul, Korea</address>
<abstract confidence="0.999824454545455">Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue. However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place. For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and re-inserts proper word boundaries. Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist. In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input. Our method generates a better output than the original user input, even if the user input has few spacing errors. Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors. We believe that the proposed method will be a very practical pre-processing module.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ETRI</author>
</authors>
<title>Pos-tag guidelines. Technical report.</title>
<date>1999</date>
<journal>Electronics and Telecomminications Research Institute.</journal>
<contexts>
<context position="9947" citStr="ETRI, 1999" startWordPosition="1672" endWordPosition="1673">ate. Because it is nearly impossible to obtain enough test data for any error rate, we generate pseudo test data in the same way that we generate development data.4 In the second experiment, we attempt 4See Footnote 3. figuring out whether the proposed method really improves the word spacing quality of the user input in a real-world setting. 3.1 Performance Improvement according to the Word Spacing Error Rate of User Input For the first experiment, we use the Sejong corpus5 from 1998-1999 (1,000,000 Korean sentences) for the training data, and ETRI corpus (30,000 sentences) for the test data (ETRI, 1999). To generate the test data that have spacing errors, we make twenty one copies of the test data and randomly insert spacing errors from 0% to 20% in the same way in which we made the development data. We feel that this strategy can model both the intentional and un-intentional human error patterns. In Figure 2, the x-axis indicates the word spacing error rate of the user input in terms of the character-unit precision, and the y-axis shows the word-unit precision of the output. Each graph depicts the word-unit precision of the test corpus, a state-of-the-art Korean WS model (Lee et al., 2007),</context>
</contexts>
<marker>ETRI, 1999</marker>
<rawString>ETRI. 1999. Pos-tag guidelines. Technical report. Electronics and Telecomminications Research Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Do-Gil Lee</author>
<author>Hae-Chang Rim</author>
<author>Dongsuk Yook</author>
</authors>
<title>Automatic Word Spacing Using Probabilistic Models Based on Character n-grams.</title>
<date>2007</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2533" citStr="Lee et al., 2007" startWordPosition="392" endWordPosition="395">o have WBMs have regarded the issue as a trivial task. Texts segmented with such WBMs, however, could contain a human writer’s intentional or un-intentional spacing errors; and even a few spacing errors can cause error-propagation for further NLP stages. For written languages that have WBMs, such as for the Korean language, the majority of recent research has been based on a traditional WS approach (Nakagawa, 2004). The first step of the traditional approach is to eliminate all spaces in the user input, and then re-locate the proper places to insert WBMs. One state-of-the-art Korean WS model (Lee et al., 2007) is known to achieve a performance of 90.31% word-unit precision, which is comparable with other WS models for the Chinese or Japanese language. Still, there is a downside to the evaluation method. If the user input has a few or no spacing errors, traditional WS models may cause more spacing errors than it correct because they produce the same output regardless the word spacing states of the user input. In this paper, we propose a new WS method that takes into account the word spacing information from the user input. Our proposed method first generates the best word spacing states for the user</context>
<context position="3844" citStr="Lee et al., 2007" startWordPosition="622" endWordPosition="625">ut. Secondly, the method estimates a threshold based on the word spacing quality of the user input. Finally, the method uses the new word spacing states that have probabilities that are higher than the threshold. The most important contribution of the proposed method is that, for most cases, the method generates an output that is better than the user input. The experimental results show that the proposed method produces a better output than the user input even if the user input has less than 1% spacing errors in terms of the character-unit precision. Moreover, the proposed method outperforms (Lee et al., 2007) significantly, when the 29 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29–32, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP user input initially contains less than 10% spacing errors, and even performs comparably, when the input contains more than 10% errors. Based on these results, we believe that the proposed method would be a very practical pre-processing module for other NLP applications. The paper is organized as follows: Section 2 explains the proposed method. Section 3 shows the experimental results. Finally, the last section describes the contributions o</context>
<context position="5807" citStr="Lee et al., 2007" startWordPosition="950" endWordPosition="953">be the best word spacing state for ot where 1 &lt; t &lt; n. Assume that xt is either 1 (space after ot) or 0 (no space after ot). Then each best word spacing state xt for all t can be found by using Equation 1. eliminating the constant parts. Every part of Equation 3 can be calculated by adding the probabilities of all possible combinations of xt−2, xt−1, xt+1 and xt+2 values. The model is trained by using the relative frequency information of the training data, and a smoothing technique is applied to relieve the datasparseness problem which is the linear interpolation of n-grams that are used in (Lee et al., 2007). 2.2 Confidence and Threshold Estimation We set a variable threshold that is proportional to the word spacing quality of the user input, Confidence. Formally, we can define the threshold T as a function of a confidence C, as in Equation 4. T = f(C) (4) Then, we define the confidence as is done in Equation 5. Because calculating such a variable is impossible, we estimate the value by substituting the word spacing states produced by the baseline WS model, xW S 1,n , with the correct word spacing states, xcorrect 1,n , as is done in Equation 6. This estimation is based on the assumption that the</context>
<context position="10546" citStr="Lee et al., 2007" startWordPosition="1777" endWordPosition="1780"> data (ETRI, 1999). To generate the test data that have spacing errors, we make twenty one copies of the test data and randomly insert spacing errors from 0% to 20% in the same way in which we made the development data. We feel that this strategy can model both the intentional and un-intentional human error patterns. In Figure 2, the x-axis indicates the word spacing error rate of the user input in terms of the character-unit precision, and the y-axis shows the word-unit precision of the output. Each graph depicts the word-unit precision of the test corpus, a state-of-the-art Korean WS model (Lee et al., 2007), the baseline WS model, and the proposed method. Although Lee’s model is known to perform comparably with state-of-the-art Chinese and Japanese WS models, it does not necessarily suggest that the word spacing quality of the model’s output is better than the user input. In Figure 2, Lee’s model exacerbates the user input when it has spacing errors that are lower than 3%. The proposed method, however, produces a better output, even if the user input has 1% spacing errors. Moreover, the proposed method shows a considerably better performance within the 10% spacing error range, as compared to Lee</context>
</contexts>
<marker>Lee, Rim, Yook, 2007</marker>
<rawString>Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook. 2007. Automatic Word Spacing Using Probabilistic Models Based on Character n-grams. IEEE Intelligent Systems, 22(1):28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Comput. Linguist.,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>171</pages>
<contexts>
<context position="5121" citStr="Merialdo, 1994" startWordPosition="823" endWordPosition="824">od consists of three steps: a baseline WS model, confidence and threshold estimation, and output optimization. The following sections will explain the steps in detail. 2.1 Baseline Word Segmentation Model We use the tri-gram Hidden Markov Model (HMM) of (Lee et al., 2007) as the baseline WS model; however, we adopt the Maximum Likelihood (ML) decoding strategy to independently find the best word spacing states. ML-decoding allows us to directly compare each output to the threshold. There is little discrepancy in accuracy when using ML-decoding, as compared to Viterbidecoding, as mentioned in (Merialdo, 1994).1 Let o1,n be a sequence of n-character user input without WBMs, xt be the best word spacing state for ot where 1 &lt; t &lt; n. Assume that xt is either 1 (space after ot) or 0 (no space after ot). Then each best word spacing state xt for all t can be found by using Equation 1. eliminating the constant parts. Every part of Equation 3 can be calculated by adding the probabilities of all possible combinations of xt−2, xt−1, xt+1 and xt+2 values. The model is trained by using the relative frequency information of the training data, and a smoothing technique is applied to relieve the datasparseness pr</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Comput. Linguist., 20(2):155– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Chinese and Japanese word segmentation using word-level and character-level information.</title>
<date>2004</date>
<booktitle>In COLING ’04,</booktitle>
<pages>466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2334" citStr="Nakagawa, 2004" startWordPosition="360" endWordPosition="361">e-processing module. 1 Introduction Word segmentation (WS) has been a fundamental research issue for languages that do not have word boundary markers (WBMs); on the contrary, other languages that do have WBMs have regarded the issue as a trivial task. Texts segmented with such WBMs, however, could contain a human writer’s intentional or un-intentional spacing errors; and even a few spacing errors can cause error-propagation for further NLP stages. For written languages that have WBMs, such as for the Korean language, the majority of recent research has been based on a traditional WS approach (Nakagawa, 2004). The first step of the traditional approach is to eliminate all spaces in the user input, and then re-locate the proper places to insert WBMs. One state-of-the-art Korean WS model (Lee et al., 2007) is known to achieve a performance of 90.31% word-unit precision, which is comparable with other WS models for the Chinese or Japanese language. Still, there is a downside to the evaluation method. If the user input has a few or no spacing errors, traditional WS models may cause more spacing errors than it correct because they produce the same output regardless the word spacing states of the user i</context>
</contexts>
<marker>Nakagawa, 2004</marker>
<rawString>Tetsuji Nakagawa. 2004. Chinese and Japanese word segmentation using word-level and character-level information. In COLING ’04, page 466, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>