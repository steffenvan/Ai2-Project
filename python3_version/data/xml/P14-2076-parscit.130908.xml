<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015250">
<title confidence="0.968125">
Cheap and easy entity evaluation
</title>
<author confidence="0.997378">
Ben Hachey Joel Nothman Will Radford
</author>
<affiliation confidence="0.995208">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.434514">
NSW 2006, Australia
</address>
<email confidence="0.989081">
ben.hachey@sydney.edu.au
{joel,wradford}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919166666667">
The AIDA-YAGO dataset is a popular tar-
get for whole-document entity recogni-
tion and disambiguation, despite lacking a
shared evaluation tool. We review eval-
uation regimens in the literature while
comparing the output of three approaches,
and identify research opportunities. This
utilises our open, accessible evaluation
tool. We exemplify a new paradigm of
distributed, shared evaluation, in which
evaluation software and standardised, ver-
sioned system outputs are provided online.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999247368421053">
Modern entity annotation systems detect mentions
in text and disambiguate them to a knowledge base
(KB). Disambiguation typically returns the corre-
sponding Wikipedia page or NIL if none exists.
Named entity linking (NEL) work is driven by
the TAC shared tasks on query-driven knowledge
base population (Ji and Grishman, 2011). Eval-
uation focuses on disambiguating queried names
and clustering NIL mentions, but most systems
internally perform whole-document named en-
tity recognition, coreference, and disambiguation
(Cucerzan and Sil, 2013; Pink et al., 2013; Cheng
et al., 2013; Fahrni et al., 2013). Wikification
work generally evaluates end-to-end entity anno-
tation including KB-driven mention spotting and
disambiguation (Milne and Witten, 2008b; Kulka-
rni et al., 2009; Ratinov et al., 2011; Ferragina and
Scaiella, 2010). Despite important differences in
mention handling, NEL and wikification work have
followed a similar trajectory. Yet to our knowl-
edge, there are no comparative whole-document
evaluations of NEL and wikification systems.
Public data sets have also driven research
in whole-document entity disambiguation
(Cucerzan, 2007; Milne and Witten, 2008b;
Kulkarni et al., 2009; Bentivogli et al., 2010; Hof-
fart et al., 2011; Meij et al., 2012). However, with
many task variants and evaluation methodologies
proposed, it is very difficult to synthesise a clear
picture of the state of the art.
We present an evaluation suite for named entity
linking, leveraging and advocating for the AIDA
disambiguation annotations (Hoffart et al., 2011)
over the large and widely used CoNLL NER data
(Tjong Kim Sang and Meulder, 2003). This builds
on recent rationalisation and benchmarking work
(Cornolti et al., 2013), adding an isolated evalua-
tion of disambiguation. Contributions include:
</bodyText>
<listItem confidence="0.9964316">
• a simple, open-source evaluation suite for
end-to-end, whole-document NEL;
• disambiguation evaluation facilitated by
gold-standard mentions;
• reference outputs from state-of-the-art NEL
and wikification systems published with the
suite for easy comparison;
• implementation of statistical significance and
error sub-type analysis, which are often lack-
ing in entity linking evaluation;
• a venue for publishing benchmark results
continuously, complementing the annual cy-
cle of shared tasks;
• a repository for versioned corrections to
ground truth annotation.
</listItem>
<bodyText confidence="0.999501">
We see this repository, at https://github.
com/wikilinks/conll03_nel_eval,asa
model for the future of informal shared evaluation.
We survey entity annotation tasks and evalua-
tion, proposing a core suite of metrics for end-to-
end linking and tagging, and settings that isolate
mention detection and disambiguation. A compar-
ison of state-of-the-art NEL and wikification sys-
tems illustrates how key differences in mention
handling affect performance. Analysis suggests
that focusing evaluation too tightly on subtasks
like candidate ranking can lead to results that do
not reflect end-to-end performance.
</bodyText>
<page confidence="0.993283">
464
</page>
<bodyText confidence="0.5682595">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 464–469,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.875222" genericHeader="introduction">
2 Tasks and metrics
</sectionHeader>
<bodyText confidence="0.999925846153846">
The literature includes many variants of the en-
tity annotation task and even more evaluation ap-
proaches. Systems can be invoked under two set-
tings: given text with expressions to be linked
(gold mentions); or given plain text only (system
mentions). The former enables a diagnostic evalu-
ation of disambiguation, while the latter simulates
a realistic end-to-end application setting.
Within each setting, metrics may consider
different subsets of the gold (G) and system (S)
annotations. Given sets of (doc, token span, kbid)
tuples, we define precision, recall and F1 score
with respect to some annotation filter f:
</bodyText>
<equation confidence="0.9669375">
|f(G) ∩ f(S) ||f(G) ∩ f(S)|
Pf = |f(S) |, Rf = |f(G) |We advocate two core metrics, corresponding to
</equation>
<bodyText confidence="0.999868230769231">
the major whole-document entity annotation tasks.
Link annotation measures performance over every
linked mention. Its filter fL matches spans and
link targets, disregarding NILs. This is particularly
apt when entity annotation is a step in an informa-
tion extraction pipeline. Tag annotation measures
performance over document-level entity sets: fT
disregards span information and NILs. This is ap-
propriate when entity annotation is used, e.g., for
document indexing or social media mining (Mi-
halcea and Csomai, 2007; Meij et al., 2012). We
proceed to ground these metrics and diagnostic
variants in the literature.
</bodyText>
<subsectionHeader confidence="0.970135">
2.1 End-to-end evaluation
</subsectionHeader>
<bodyText confidence="0.9994166">
We follow Cornolti et al. (2013) in evaluating end-
to-end entity annotation, including both mention
detection and disambiguation. In this context,
fL equates to Cornolti et al.’s strong annotation
match; fT measures what they call entity match.
</bodyText>
<subsectionHeader confidence="0.99991">
2.2 Mention evaluation
</subsectionHeader>
<bodyText confidence="0.999935785714286">
Mention detection performance may be evaluated
regardless of linking decisions. A filter fM dis-
cards the link target (kbid). Of the present metrics,
only this considers NIL-linked system mentions as
different from non-mentions. For comparability
with wikification, we consider an additional filter
fMKB to NEL output that retains only linked men-
tions. fM and fMKB are equivalent to Cucerzan’s
(2007) mention evaluation and Cornolti et al.’s
strong mention match respectively. fM is com-
parable to the NER evaluation from the CoNLL
2003 shared task (Tjong Kim Sang and Meulder,
2003): span equivalence is handled the same way,
but metrics here ignore mention types.
</bodyText>
<subsectionHeader confidence="0.999679">
2.3 Disambiguation evaluation
</subsectionHeader>
<bodyText confidence="0.999985454545455">
Most NEL and wikification literature focuses on
disambiguation, evaluating the quality of link tar-
get annotations in isolation from NER error. Pro-
viding systems with ground truth mentions makes
fL equivalent to Mihalcea and Csomai’s (2007)
sense disambiguation evaluation and Milne and
Witten’s (2008b) disambiguation evaluation. It
differs from Kulkarni et al.’s (2009) metric in be-
ing micro-averaged (equal weight to each men-
tion), rather than macro-averaged across docu-
ments. fL recall is comparable to TAC’s KB recall
(Ji and Grishman, 2011). It differs in that all men-
tions are evaluated rather than specific queries.
Related evaluations have also isolated disam-
biguation performance by: considering the links
of only correctly identified mentions (Cucerzan,
2007); or only true mentions where the correct
entity appears among top candidates before dis-
ambiguation (Ratinov et al., 2011; Hoffart et al.,
2011; Pilz and Paass, 2012). We do not prefer this
approach as it makes system comparison difficult.
For comparability, we implement a filter fLHOF
that retains only Hoffart-linkable mentions having
a YAGO means relation to the correct entity.
Tag annotation (fT) with ground truth men-
tions is equivalent to Milne and Witten’s (2008b)
link evaluation, Mihalcea and Csomai’s (2007)
keyword extraction evaluation and Ratinov et
al.’s (2011) bag-of-titles evaluation. It is compara-
ble to Pilz and Paass’s (2012) bag-of-titles evalua-
tion, but does not account for sequential order and
keeps all gold-standard links regardless of whether
they are found by candidate generation.
</bodyText>
<subsectionHeader confidence="0.999545">
2.4 Further diagnostics and rank evaluation
</subsectionHeader>
<bodyText confidence="0.999963">
Several evaluations in the literature are beyond the
scope of this paper but planned for future versions
of the code. This includes further diagnostic sub-
task evaluation, particularly candidate set recall
(Hachey et al., 2013), NIL accuracy (Ji and Grish-
man, 2011) and weak mention matching (Cornolti
et al., 2013). With a score for each prediction, fur-
ther metrics are possible: rank evaluation of tag
annotation with r-precision, mean reciprocal rank
and mean average precision (Meij et al., 2012);
and rank evaluation of mentions for comparison to
Hoffart et al. (2011) and Pilz and Paass (2012).
</bodyText>
<page confidence="0.999324">
465
</page>
<sectionHeader confidence="0.997742" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999838324324324">
The CoNLL-YAGO dataset (Hoffart et al., 2011)
is an excellent target for end-to-end, whole-
document entity annotation. It is public, free and
much larger than most entity annotation data sets.
It is based on the widely used NER data from
the CoNLL 2003 shared task (Tjong Kim Sang
and Meulder, 2003), building disambiguation on
ground truth mentions. It has standard training
and development splits that are representative of
the held-out test data, all being sourced from the
Reuters text categorisation corpus (Lewis et al.,
2004), which is provided free for research pur-
poses. Training and development comprise 1,162
stories from 22-31 August 1996 and held-out test
comprises 231 stories from 6-7 December 1996.
The layered annotation provides useful informa-
tion for analysis including categorisation topics
(e.g., general news, markets, sport) and NE type
markup (PER, ORG, LOC, MISC).
The primary drawback is that KB annotations
are currently present only if there is a YAGO
means relation between the mention string and
the correct entity. This means that there are a
number of CoNLL entity mentions referring to
entities that exist in Wikipedia that are nonethe-
less marked NIL in the ground truth (e.g. ‘DSE’
for ‘Dhaka Stock Exchange’). This may be ad-
dressed by using a shared repository to adopt ver-
sioned improvements to the ground truth. Anno-
tation over CoNLL tokenisation sometimes results
in strange mentions (e.g., ‘Washington-based’ in-
stead of ‘Washington’). However, prescribed to-
kenisation simplifies comparison and analysis.
Another concern is that link annotation goes
stale, since Wikipedia titles are only canonical
with respect to a particular point in time. This is
because pages may be renamed or reorganised:
</bodyText>
<listItem confidence="0.997308">
• to improve editorial structure, such as down-
grading an entity from having a page of its
own, to a mere section in another page;
• to account for newly notable entities, such as
creating a disambiguation page for a title that
formerly had a single known referent; or
• because of changes in fact, such as corporate
mergers and name changes.
</listItem>
<bodyText confidence="0.9996676">
All systems compared provide Wikipedia titles as
labels, which are mapped to current titles for com-
parison: for each entity title t linked in the gold
data, we query the Wikipedia API to find t’s canon-
ical form tc and retrieve titles of all redirects to tc.
</bodyText>
<sectionHeader confidence="0.994301" genericHeader="method">
4 Reference systems
</sectionHeader>
<bodyText confidence="0.999865125">
Even on public data sets, comparison to published
results can be very difficult and extremely costly
(Fokkens et al., 2013). We include reference sys-
tem output in our repository for simple compar-
ison. Other researchers are welcome to add ref-
erence output, providing a continuous benchmark
that complements the annual cycle of large shared
tasks like TAC KBP.
</bodyText>
<subsectionHeader confidence="0.982465">
4.1 TagMe
</subsectionHeader>
<bodyText confidence="0.9999885">
TagMe (Ferragina and Scaiella, 2010) is an end-
to-end wikification system specialising in short
texts. TagMe performs best among publicly avail-
able wikification systems (Cornolti et al., 2013).
Mention detection uses a dictionary of anchor
text from links between Wikipedia pages. Candi-
date ranking is based on entity relatedness (Milne
and Witten, 2008a), followed by mention prun-
ing. We use thresholds on annotation scores sup-
plied by Marco Cornolti (personal communica-
tion) of 0.289 and 0.336 respectively for men-
tion/link and tag evaluation. TagMe annota-
tions may not align with CoNLL token bound-
aries, e.g., &lt;annot title=“Oakland, New Jer-
sey”&gt;OAKLAND, N.J&lt;/annot&gt;. Before evalua-
tion, we extend annotations to overlapping tokens.
</bodyText>
<subsectionHeader confidence="0.906766">
4.2 AIDA
</subsectionHeader>
<bodyText confidence="0.999866857142857">
AIDA (Hoffart et al., 2011) is the system pre-
sented with the CoNLL-YAGO dataset and places
emphasis on state-of-the-art ranking of candi-
date entity sets. Mentions are ground truth from
the CoNLL data to isolate ranking performance,
equivalent to applying the fLaoa filter. Ranking is
informed by a graph model of entity compatibility.
</bodyText>
<subsectionHeader confidence="0.996643">
4.3 Schwa
</subsectionHeader>
<bodyText confidence="0.999874923076923">
Schwa (Radford et al., 2012) is a heuristic NEL
system based on a TAC 2012 shared task en-
trant. Mention detection uses a NER model trained
on news text followed by rule-based corefer-
ence. Disambiguation uses an unweighted com-
bination of KB statistics, document compatibil-
ity (Cucerzan, 2007), graph similarity and targeted
textual similarity. Candidates that score below
a threshold learned from TAC data are linked to
NIL. The system is very competitive, performing
at 93% and 97% respectively of the best accuracy
numbers we know of on 2011 and 2012 TAC eval-
uation data (Cucerzan and Sil, 2013).
</bodyText>
<page confidence="0.998039">
466
</page>
<table confidence="0.9699356">
System Mentions Filter P R F1
Cucerzan System f 82.2 84.8 83.5
Schwa System f 86.9 76.7 81.5
TagMe System f KB 75.2 60.4 67.0
Schwa System f KB 82.5 74.5 78.3
</table>
<tableCaption confidence="0.9445995">
Table 1: Mention detection results. Cucerzan re-
sults as reported (Cucerzan, 2007).
</tableCaption>
<sectionHeader confidence="0.999855" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.9998155">
We briefly report results over the reference sys-
tems to highlight characteristics of the evaluation
metrics and task settings. Results hinge upon
Schwa since we have obtained only its output in all
settings. Except where noted, all differences are
significant (p &lt; 0.05) according to approximate
randomisation (Noreen, 1989), permuting annota-
tions over whole documents.
</bodyText>
<subsectionHeader confidence="0.996389">
5.1 Mention evaluation
</subsectionHeader>
<bodyText confidence="0.998477578947369">
Table 1 evaluates mentions with and without NILs.
None of the systems reported use a CoNLL-
trained NER tagger, for which top shared task par-
ticipants approached 90% Fi in a stricter evalu-
ation than fm. We note the impressive numbers
reported by Cucerzan (2007) using a novel ap-
proach to mention detection based on capitalisa-
tion and corpus co-occurrence statistics, and the
similar performance1 to Schwa, whose NER com-
ponent is trained on another news corpus.
In wikification, NIL-linked mentions may not
be relevant, and it may suffice to identify only
the most canonical forms of names, rather than
all mentions in a coreference chain. With fmKB,
Schwa has much higher recall than TagMe, though
TagMe’s precision is understated because it gener-
ates non-NE annotations that are not present in the
CoNLL-YAGO ground truth (e.g., linking ‘striker’
to Forward (association football)).
</bodyText>
<subsectionHeader confidence="0.999411">
5.2 Disambiguation evaluation
</subsectionHeader>
<bodyText confidence="0.999895625">
Table 2 contains results isolating disambiguation
performance. AIDA ranking outperforms Schwa
according to both the link (fLHOF) and tag metrics
(fTHOF). If we remove the Hoffart et al. (2011)
linkable constraint, we observe that Schwa disam-
biguation performance loses about 8 points in pre-
cision on the link metric (fL) and 2 points on the
tag metric (fT). This suggests that disambiguation
</bodyText>
<footnote confidence="0.909802">
1Significance cannot be tested since we do not have the
Cucerzan (2007) output.
</footnote>
<table confidence="0.998431714285714">
System Mentions Filter P R F1
Schwa Gold fL 67.5 78.3 72.5
Schwa Gold fLHOF 79.7 78.3 79.0
AIDA Gold fLHOF 83.2 83.2 83.2
Schwa Gold fT 77.8 77.7 77.7
Schwa Gold fTHOF 80.1 77.6 78.8
AIDA Gold fTHOF 87.7 84.2 85.9
</table>
<tableCaption confidence="0.776925">
Table 2: Disambiguation results for mention-level
linking and document-level tagging.
</tableCaption>
<table confidence="0.9994454">
System Mentions Filter P R F1
TagMe System fL 63.2 50.7 56.3
Schwa System fL 67.6 61.0 64.2
TagMe System fT 65.0 65.4 65.2
Schwa System fT 71.2 62.6 66.6
</table>
<tableCaption confidence="0.9165445">
Table 3: End-to-end results for mention-level link-
ing and document-level tagging.
</tableCaption>
<bodyText confidence="0.999593538461538">
evaluation without the linkable constraint is im-
portant, especially if the application requires de-
tecting and disambiguating all mentions.
The comparison here highlights a notable eval-
uation intricacy. The Schwa system disambiguates
all gold mentions rather than those with KB links,
and the document compatibility approach means
that evidence from a NIL mention may offer con-
founding evidence when linking linkable men-
tions. Further, although using the same mentions,
systems use search resources with different recall
characteristics, so the Schwa system may not re-
trieve the correct candidate to disambiguate.
</bodyText>
<subsectionHeader confidence="0.979528">
5.3 End-to-end evaluation
</subsectionHeader>
<bodyText confidence="0.999926">
Finally, Table 3 contains end-to-end entity anno-
tation results. Again, these results highlight key
differences in mention handling between NEL and
wikification. Coreference modelling helps NEL
detect and link ambiguous names (e.g., ‘Presi-
dent Bush’) that refer to the same entity as unam-
biguous names in the same text (e.g., ‘George W.
Bush’). And restricting the the universe to named
entities is appropriate for the CoNLL-YAGO data.
The advantage is marked in the mention-level link
evaluation (fL). However, the systems are statis-
tically indistinguishable in the document-level tag
evaluation (fT). Thus the extra NER and corefer-
ence machinery may not be justified if the applica-
tion is document indexing or social media mining
(Meij et al., 2012), wherein a KB-driven mention
detector may be favourable for other reasons.
</bodyText>
<page confidence="0.997935">
467
</page>
<figure confidence="0.743118769230769">
Error
wrong link
link as nil
nil as link
missing
extra
Sports
Domestic Politics
Corporate / Industrial
Internat’l Relations
Markets
War / Civil War
Crime / Law Enforc’t
</figure>
<table confidence="0.991565375">
fLHOF fL Schwa
AIDA Schwa TagMe
752 896 429 605
- 79 - 111
- - 183 337
- - 1,780 1,031
- - 1,663 927
0 20 40 60 80 100
</table>
<tableCaption confidence="0.997695">
Table 4: fLHOF and fL error profiles.
</tableCaption>
<figure confidence="0.9859656">
LOC
ORG
PER
MISC
0 20 40 60 80 100
</figure>
<figureCaption confidence="0.99989">
Figure 1: Schwa fL and fLHOF F1 for NE types
</figureCaption>
<sectionHeader confidence="0.989765" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.991252625">
We analyse the types of error that a system makes.
We also harness the multi-layered annotation to
quantify the effect of NE type and document topic.
By error type Table 4 shows error counts based
on the disambiguation link evaluation with the
linkable constraint (fLHOF) and the end-to-end
link evaluation (fL). Errors are divided as follows:
wrong link: mention linked to wrong KB entry
link as nil: KB-entity mention linked to NIL
nil as link: NIL mention linked to the KB
missing: true mention not detected
extra: mention detected spuriously
AIDA outperforms Schwa under the linkable eval-
uation, making fewer wrong link errors. Schwa
also overgenerates NIL, which may reflect candi-
date recall errors or a conservative disambiguation
threshold. On the end-to-end evaluation, Schwa
makes more linking errors (wrong link, link as nil,
nil as link) than TagMe, but fewer in mention de-
tection, leading to higher overall performance.
By entity type Figure 1 evaluates only men-
tions where the CoNLL 2003 corpus (Tjong Kim
Sang and Meulder, 2003) marks a NE mention of
each type. This is based on the link evaluation
of Schwa. The left and right bars correspond to
end-to-end (fL) and disambiguation (fLHOF) F1
respectively. In accord with TAC results (Ji and
Grishman, 2011), high accuracy can be achieved
on PER when a full name is given, while ORG is
substantially more challenging. MISC entities are
somewhat difficult to disambiguate, with identifi-
cation errors hampering end-to-end performance.
</bodyText>
<figureCaption confidence="0.970601">
Figure 2: Schwa fL and fLHOF F1 for top topics
</figureCaption>
<bodyText confidence="0.740355666666667">
By topical category The underlying Reuters
Corpus documents are labelled with topic, country
and industry codes (Lewis et al., 2004). Figure 2
reports F1 on test documents from each frequent
topic. It highlights that much ambiguity remains
unresolved in Sports, while very high performance
linking is attainable in categories such as Markets
and Domestic Politics, only when given ground
truth linkable mentions.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999994909090909">
We surveyed entity annotation tasks and advocated
a core set of metrics for mention, disambiguation
and end-to-end evaluation. This enabled a direct
comparison of state-of-the-art NEL and wikifica-
tion systems, highlighting the effect of key differ-
ences. In particular, NER and coreference mod-
ules make NEL approaches suitable for applica-
tions that require all mentions, including ambigu-
ous names and entities that are not in the KB. For
applications where document-level entity tags are
appropriate, the NEL and wikification approaches
we evaluate have similar performance.
The big picture we wish to convey is a new
approach to community evaluation that makes
benchmarking and qualitative comparison cheap
and easy. In addition to the code being open
source, we use the repository to store reference
system output, and – we hope – emendations to
the ground truth. We encourage other researchers
to contribute reference output and hope that this
will provide a continuous benchmark to comple-
ment the current cycle of shared tasks.
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994635857142857">
Many thanks to Johannes Hoffart, Marco Cornolti,
Xiao Ling and Edgar Meij for reference outputs
and guidance. Ben Hachey is the recipient of
an Australian Research Council Discovery Early
Career Researcher Award (DE120102900). The
other authors were supported by the Capital Mar-
kets CRC Computable News project.
</bodyText>
<page confidence="0.999273">
468
</page>
<sectionHeader confidence="0.995859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999741480769231">
Luisa Bentivogli, Pamela Forner, Claudio Giu-
liano, Alessandro Marchetti, Emanuele Pianta, and
Kateryna Tymoshenko. 2010. Extending English
ACE 2005 corpus annotation with ground-truth links
to Wikipedia. In COLING Workshop on The Peo-
ple’s Web Meets NLP: Collaboratively Constructed
Semantic Resources, pages 19–27.
Xiao Cheng, Bingling Chen, Rajhans Samdani, Kai-
Wei Chang, Zhiye Fei, Mark Sammons, John Wi-
eting, Subhro Roy, Chizheng Wang, and Dan Roth.
2013. Illinois cognitive computation group UI-CCG
TAC 2013 entity linking and slot filler validation
systems. In Text Analysis Conference.
Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmark-
ing entity-annotation systems. In 22nd International
Conference on the World Wide Web, pages 249–260.
Silviu Cucerzan and Avirup Sil. 2013. The MSR sys-
tems for entity linking and temporal slot filling at
TAC 2013. In Text Analysis Conference.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 708–716.
Angela Fahrni, Benjamin Heinzerling, Thierry G¨ockel,
and Michael Strube. 2013. HITS’ monolingual and
cross-lingual entity linking system at TAC 2013. In
Text Analysis Conference.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
On-the-fly annotation of short text fragments (by
Wikipedia entities). In 19th International Confer-
ence on Information and Knowledge Management,
pages 1625–1628.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted
Pedersen, Piek Vossen, and Nuno Freire. 2013. Off-
spring from reproduction problems: What replica-
tion failure teaches us. In 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1691–1701.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evaluat-
ing entity linking with Wikipedia. Artificial Intel-
ligence, 194:130–150.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Conference on Empirical Methods
in Natural Language Processing, pages 782–792.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In 49th Annual Meeting of the Association for Com-
putational Linguistics, pages 1148–1158.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of Wikipedia entities in web text. In 15th Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 457–466.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361–397.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In 5th
International Conference on Web Search and Data
Mining, pages 563–572.
Rada Mihalcea and Andras Csomai. 2007. Wik-
ify! Linking documents to encyclopedic knowledge.
In 16th Conference on Information and Knowledge
Management, pages 233–242.
David Milne and Ian H. Witten. 2008a. An effec-
tive, low-cost measure of semantic relatedness ob-
tained from Wikipedia links. In AAAI Workshop on
Wikipedia and Articial Intelligence, pages 25–30.
David Milne and Ian H. Witten. 2008b. Learning
to link with Wikipedia. In 17th Conference on In-
formation and Knowledge Management, pages 509–
518.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. John Wiley &amp; Sons.
Anja Pilz and Gerhard Paass. 2012. Collective
search for concept disambiguation. In 24th Inter-
national Conference on Computational Linguistics,
pages 2243–2258.
Glen Pink, Will Radford, Will Cannings, Andrew
Naoum, Joel Nothman, Daniel Tse, and James R.
Curran. 2013. SYDNEY CMCRC at TAC 2013. In
Text Analysis Conference.
Will Radford, Will Cannings, Andrew Naoum, Joel
Nothman, Glen Pink, Daniel Tse, and James R.
Curran. 2012. (Almost) Total Recall – SYD-
NEY CMCRC at TAC 2012. In Text Analysis Con-
ference.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1375–1384.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Conference On Computational Natural Language
Learning, pages 142–147.
</reference>
<page confidence="0.999626">
469
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958510">
<title confidence="0.999841">Cheap and easy entity evaluation</title>
<author confidence="0.999984">Ben Hachey Joel Nothman Will Radford</author>
<affiliation confidence="0.999815">School of Information University of Sydney</affiliation>
<address confidence="0.983823">NSW 2006, Australia</address>
<abstract confidence="0.998045307692308">is a popular target for whole-document entity recognition and disambiguation, despite lacking a shared evaluation tool. We review evaluation regimens in the literature while comparing the output of three approaches, and identify research opportunities. This utilises our open, accessible evaluation tool. We exemplify a new paradigm of distributed, shared evaluation, in which evaluation software and standardised, versioned system outputs are provided online.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Pamela Forner</author>
<author>Claudio Giuliano</author>
<author>Alessandro Marchetti</author>
<author>Emanuele Pianta</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>corpus annotation with ground-truth links to Wikipedia.</title>
<date>2010</date>
<journal>Extending English ACE</journal>
<booktitle>In COLING Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1927" citStr="Bentivogli et al., 2010" startWordPosition="268" endWordPosition="271">2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a clear picture of the state of the art. We present an evaluation suite for named entity linking, leveraging and advocating for the AIDA disambiguation annotations (Hoffart et al., 2011) over the large and widely used CoNLL NER data (Tjong Kim Sang and Meulder, 2003). This builds on recent rationalisation and benchmarking work (Cornolti et al., 2013), adding an isolated evaluation of disambiguation. Contributions include: • a simple, open-source</context>
</contexts>
<marker>Bentivogli, Forner, Giuliano, Marchetti, Pianta, Tymoshenko, 2010</marker>
<rawString>Luisa Bentivogli, Pamela Forner, Claudio Giuliano, Alessandro Marchetti, Emanuele Pianta, and Kateryna Tymoshenko. 2010. Extending English ACE 2005 corpus annotation with ground-truth links to Wikipedia. In COLING Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Cheng</author>
<author>Bingling Chen</author>
<author>Rajhans Samdani</author>
<author>KaiWei Chang</author>
<author>Zhiye Fei</author>
<author>Mark Sammons</author>
<author>John Wieting</author>
<author>Subhro Roy</author>
<author>Chizheng Wang</author>
<author>Dan Roth</author>
</authors>
<title>entity linking and slot filler validation systems.</title>
<date>2013</date>
<booktitle>Illinois cognitive computation group UI-CCG TAC</booktitle>
<contexts>
<context position="1287" citStr="Cheng et al., 2013" startWordPosition="177" endWordPosition="180">uts are provided online. 1 Introduction Modern entity annotation systems detect mentions in text and disambiguate them to a knowledge base (KB). Disambiguation typically returns the corresponding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkar</context>
</contexts>
<marker>Cheng, Chen, Samdani, Chang, Fei, Sammons, Wieting, Roy, Wang, Roth, 2013</marker>
<rawString>Xiao Cheng, Bingling Chen, Rajhans Samdani, KaiWei Chang, Zhiye Fei, Mark Sammons, John Wieting, Subhro Roy, Chizheng Wang, and Dan Roth. 2013. Illinois cognitive computation group UI-CCG TAC 2013 entity linking and slot filler validation systems. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Cornolti</author>
<author>Paolo Ferragina</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>A framework for benchmarking entity-annotation systems.</title>
<date>2013</date>
<booktitle>In 22nd International Conference on the World Wide Web,</booktitle>
<pages>249--260</pages>
<contexts>
<context position="2430" citStr="Cornolti et al., 2013" startWordPosition="349" endWordPosition="352">document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a clear picture of the state of the art. We present an evaluation suite for named entity linking, leveraging and advocating for the AIDA disambiguation annotations (Hoffart et al., 2011) over the large and widely used CoNLL NER data (Tjong Kim Sang and Meulder, 2003). This builds on recent rationalisation and benchmarking work (Cornolti et al., 2013), adding an isolated evaluation of disambiguation. Contributions include: • a simple, open-source evaluation suite for end-to-end, whole-document NEL; • disambiguation evaluation facilitated by gold-standard mentions; • reference outputs from state-of-the-art NEL and wikification systems published with the suite for easy comparison; • implementation of statistical significance and error sub-type analysis, which are often lacking in entity linking evaluation; • a venue for publishing benchmark results continuously, complementing the annual cycle of shared tasks; • a repository for versioned cor</context>
<context position="5293" citStr="Cornolti et al. (2013)" startWordPosition="769" endWordPosition="772">ink annotation measures performance over every linked mention. Its filter fL matches spans and link targets, disregarding NILs. This is particularly apt when entity annotation is a step in an information extraction pipeline. Tag annotation measures performance over document-level entity sets: fT disregards span information and NILs. This is appropriate when entity annotation is used, e.g., for document indexing or social media mining (Mihalcea and Csomai, 2007; Meij et al., 2012). We proceed to ground these metrics and diagnostic variants in the literature. 2.1 End-to-end evaluation We follow Cornolti et al. (2013) in evaluating endto-end entity annotation, including both mention detection and disambiguation. In this context, fL equates to Cornolti et al.’s strong annotation match; fT measures what they call entity match. 2.2 Mention evaluation Mention detection performance may be evaluated regardless of linking decisions. A filter fM discards the link target (kbid). Of the present metrics, only this considers NIL-linked system mentions as different from non-mentions. For comparability with wikification, we consider an additional filter fMKB to NEL output that retains only linked mentions. fM and fMKB a</context>
<context position="8161" citStr="Cornolti et al., 2013" startWordPosition="1203" endWordPosition="1206">n and Ratinov et al.’s (2011) bag-of-titles evaluation. It is comparable to Pilz and Paass’s (2012) bag-of-titles evaluation, but does not account for sequential order and keeps all gold-standard links regardless of whether they are found by candidate generation. 2.4 Further diagnostics and rank evaluation Several evaluations in the literature are beyond the scope of this paper but planned for future versions of the code. This includes further diagnostic subtask evaluation, particularly candidate set recall (Hachey et al., 2013), NIL accuracy (Ji and Grishman, 2011) and weak mention matching (Cornolti et al., 2013). With a score for each prediction, further metrics are possible: rank evaluation of tag annotation with r-precision, mean reciprocal rank and mean average precision (Meij et al., 2012); and rank evaluation of mentions for comparison to Hoffart et al. (2011) and Pilz and Paass (2012). 465 3 Data The CoNLL-YAGO dataset (Hoffart et al., 2011) is an excellent target for end-to-end, wholedocument entity annotation. It is public, free and much larger than most entity annotation data sets. It is based on the widely used NER data from the CoNLL 2003 shared task (Tjong Kim Sang and Meulder, 2003), bui</context>
<context position="11371" citStr="Cornolti et al., 2013" startWordPosition="1724" endWordPosition="1727">d retrieve titles of all redirects to tc. 4 Reference systems Even on public data sets, comparison to published results can be very difficult and extremely costly (Fokkens et al., 2013). We include reference system output in our repository for simple comparison. Other researchers are welcome to add reference output, providing a continuous benchmark that complements the annual cycle of large shared tasks like TAC KBP. 4.1 TagMe TagMe (Ferragina and Scaiella, 2010) is an endto-end wikification system specialising in short texts. TagMe performs best among publicly available wikification systems (Cornolti et al., 2013). Mention detection uses a dictionary of anchor text from links between Wikipedia pages. Candidate ranking is based on entity relatedness (Milne and Witten, 2008a), followed by mention pruning. We use thresholds on annotation scores supplied by Marco Cornolti (personal communication) of 0.289 and 0.336 respectively for mention/link and tag evaluation. TagMe annotations may not align with CoNLL token boundaries, e.g., &lt;annot title=“Oakland, New Jersey”&gt;OAKLAND, N.J&lt;/annot&gt;. Before evaluation, we extend annotations to overlapping tokens. 4.2 AIDA AIDA (Hoffart et al., 2011) is the system present</context>
</contexts>
<marker>Cornolti, Ferragina, Ciaramita, 2013</marker>
<rawString>Marco Cornolti, Paolo Ferragina, and Massimiliano Ciaramita. 2013. A framework for benchmarking entity-annotation systems. In 22nd International Conference on the World Wide Web, pages 249–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Avirup Sil</author>
</authors>
<title>The MSR systems for entity linking and temporal slot filling at TAC</title>
<date>2013</date>
<booktitle>In Text Analysis Conference.</booktitle>
<contexts>
<context position="1248" citStr="Cucerzan and Sil, 2013" startWordPosition="169" endWordPosition="172">are and standardised, versioned system outputs are provided online. 1 Introduction Modern entity annotation systems detect mentions in text and disambiguate them to a knowledge base (KB). Disambiguation typically returns the corresponding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan</context>
<context position="12863" citStr="Cucerzan and Sil, 2013" startWordPosition="1962" endWordPosition="1965">y compatibility. 4.3 Schwa Schwa (Radford et al., 2012) is a heuristic NEL system based on a TAC 2012 shared task entrant. Mention detection uses a NER model trained on news text followed by rule-based coreference. Disambiguation uses an unweighted combination of KB statistics, document compatibility (Cucerzan, 2007), graph similarity and targeted textual similarity. Candidates that score below a threshold learned from TAC data are linked to NIL. The system is very competitive, performing at 93% and 97% respectively of the best accuracy numbers we know of on 2011 and 2012 TAC evaluation data (Cucerzan and Sil, 2013). 466 System Mentions Filter P R F1 Cucerzan System f 82.2 84.8 83.5 Schwa System f 86.9 76.7 81.5 TagMe System f KB 75.2 60.4 67.0 Schwa System f KB 82.5 74.5 78.3 Table 1: Mention detection results. Cucerzan results as reported (Cucerzan, 2007). 5 Results We briefly report results over the reference systems to highlight characteristics of the evaluation metrics and task settings. Results hinge upon Schwa since we have obtained only its output in all settings. Except where noted, all differences are significant (p &lt; 0.05) according to approximate randomisation (Noreen, 1989), permuting annota</context>
</contexts>
<marker>Cucerzan, Sil, 2013</marker>
<rawString>Silviu Cucerzan and Avirup Sil. 2013. The MSR systems for entity linking and temporal slot filling at TAC 2013. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="1854" citStr="Cucerzan, 2007" startWordPosition="258" endWordPosition="259">il, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a clear picture of the state of the art. We present an evaluation suite for named entity linking, leveraging and advocating for the AIDA disambiguation annotations (Hoffart et al., 2011) over the large and widely used CoNLL NER data (Tjong Kim Sang and Meulder, 2003). This builds on recent rationalisation and benchmarking work (Cornolti et al., 2013), adding an isolated eva</context>
<context position="6991" citStr="Cucerzan, 2007" startWordPosition="1025" endWordPosition="1026">ystems with ground truth mentions makes fL equivalent to Mihalcea and Csomai’s (2007) sense disambiguation evaluation and Milne and Witten’s (2008b) disambiguation evaluation. It differs from Kulkarni et al.’s (2009) metric in being micro-averaged (equal weight to each mention), rather than macro-averaged across documents. fL recall is comparable to TAC’s KB recall (Ji and Grishman, 2011). It differs in that all mentions are evaluated rather than specific queries. Related evaluations have also isolated disambiguation performance by: considering the links of only correctly identified mentions (Cucerzan, 2007); or only true mentions where the correct entity appears among top candidates before disambiguation (Ratinov et al., 2011; Hoffart et al., 2011; Pilz and Paass, 2012). We do not prefer this approach as it makes system comparison difficult. For comparability, we implement a filter fLHOF that retains only Hoffart-linkable mentions having a YAGO means relation to the correct entity. Tag annotation (fT) with ground truth mentions is equivalent to Milne and Witten’s (2008b) link evaluation, Mihalcea and Csomai’s (2007) keyword extraction evaluation and Ratinov et al.’s (2011) bag-of-titles evaluati</context>
<context position="12558" citStr="Cucerzan, 2007" startWordPosition="1913" endWordPosition="1914"> 2011) is the system presented with the CoNLL-YAGO dataset and places emphasis on state-of-the-art ranking of candidate entity sets. Mentions are ground truth from the CoNLL data to isolate ranking performance, equivalent to applying the fLaoa filter. Ranking is informed by a graph model of entity compatibility. 4.3 Schwa Schwa (Radford et al., 2012) is a heuristic NEL system based on a TAC 2012 shared task entrant. Mention detection uses a NER model trained on news text followed by rule-based coreference. Disambiguation uses an unweighted combination of KB statistics, document compatibility (Cucerzan, 2007), graph similarity and targeted textual similarity. Candidates that score below a threshold learned from TAC data are linked to NIL. The system is very competitive, performing at 93% and 97% respectively of the best accuracy numbers we know of on 2011 and 2012 TAC evaluation data (Cucerzan and Sil, 2013). 466 System Mentions Filter P R F1 Cucerzan System f 82.2 84.8 83.5 Schwa System f 86.9 76.7 81.5 TagMe System f KB 75.2 60.4 67.0 Schwa System f KB 82.5 74.5 78.3 Table 1: Mention detection results. Cucerzan results as reported (Cucerzan, 2007). 5 Results We briefly report results over the re</context>
<context position="13773" citStr="Cucerzan (2007)" startWordPosition="2114" endWordPosition="2115">ference systems to highlight characteristics of the evaluation metrics and task settings. Results hinge upon Schwa since we have obtained only its output in all settings. Except where noted, all differences are significant (p &lt; 0.05) according to approximate randomisation (Noreen, 1989), permuting annotations over whole documents. 5.1 Mention evaluation Table 1 evaluates mentions with and without NILs. None of the systems reported use a CoNLLtrained NER tagger, for which top shared task participants approached 90% Fi in a stricter evaluation than fm. We note the impressive numbers reported by Cucerzan (2007) using a novel approach to mention detection based on capitalisation and corpus co-occurrence statistics, and the similar performance1 to Schwa, whose NER component is trained on another news corpus. In wikification, NIL-linked mentions may not be relevant, and it may suffice to identify only the most canonical forms of names, rather than all mentions in a coreference chain. With fmKB, Schwa has much higher recall than TagMe, though TagMe’s precision is understated because it generates non-NE annotations that are not present in the CoNLL-YAGO ground truth (e.g., linking ‘striker’ to Forward (a</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela Fahrni</author>
<author>Benjamin Heinzerling</author>
<author>Thierry G¨ockel</author>
<author>Michael Strube</author>
</authors>
<title>HITS’ monolingual and cross-lingual entity linking system at TAC</title>
<date>2013</date>
<booktitle>In Text Analysis Conference.</booktitle>
<marker>Fahrni, Heinzerling, G¨ockel, Strube, 2013</marker>
<rawString>Angela Fahrni, Benjamin Heinzerling, Thierry G¨ockel, and Michael Strube. 2013. HITS’ monolingual and cross-lingual entity linking system at TAC 2013. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>TAGME: On-the-fly annotation of short text fragments (by Wikipedia entities).</title>
<date>2010</date>
<booktitle>In 19th International Conference on Information and Knowledge Management,</booktitle>
<pages>1625--1628</pages>
<contexts>
<context position="1534" citStr="Ferragina and Scaiella, 2010" startWordPosition="212" endWordPosition="215">med entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a clear picture of the state of the art. We present an e</context>
<context position="11216" citStr="Ferragina and Scaiella, 2010" startWordPosition="1701" endWordPosition="1704">s, which are mapped to current titles for comparison: for each entity title t linked in the gold data, we query the Wikipedia API to find t’s canonical form tc and retrieve titles of all redirects to tc. 4 Reference systems Even on public data sets, comparison to published results can be very difficult and extremely costly (Fokkens et al., 2013). We include reference system output in our repository for simple comparison. Other researchers are welcome to add reference output, providing a continuous benchmark that complements the annual cycle of large shared tasks like TAC KBP. 4.1 TagMe TagMe (Ferragina and Scaiella, 2010) is an endto-end wikification system specialising in short texts. TagMe performs best among publicly available wikification systems (Cornolti et al., 2013). Mention detection uses a dictionary of anchor text from links between Wikipedia pages. Candidate ranking is based on entity relatedness (Milne and Witten, 2008a), followed by mention pruning. We use thresholds on annotation scores supplied by Marco Cornolti (personal communication) of 0.289 and 0.336 respectively for mention/link and tag evaluation. TagMe annotations may not align with CoNLL token boundaries, e.g., &lt;annot title=“Oakland, N</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>Paolo Ferragina and Ugo Scaiella. 2010. TAGME: On-the-fly annotation of short text fragments (by Wikipedia entities). In 19th International Conference on Information and Knowledge Management, pages 1625–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antske Fokkens</author>
<author>Marieke van Erp</author>
<author>Marten Postma</author>
<author>Ted Pedersen</author>
<author>Piek Vossen</author>
<author>Nuno Freire</author>
</authors>
<title>Offspring from reproduction problems: What replication failure teaches us.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1691--1701</pages>
<marker>Fokkens, van Erp, Postma, Pedersen, Vossen, Freire, 2013</marker>
<rawString>Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring from reproduction problems: What replication failure teaches us. In 51st Annual Meeting of the Association for Computational Linguistics, pages 1691–1701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--130</pages>
<contexts>
<context position="8073" citStr="Hachey et al., 2013" startWordPosition="1188" endWordPosition="1191">n’s (2008b) link evaluation, Mihalcea and Csomai’s (2007) keyword extraction evaluation and Ratinov et al.’s (2011) bag-of-titles evaluation. It is comparable to Pilz and Paass’s (2012) bag-of-titles evaluation, but does not account for sequential order and keeps all gold-standard links regardless of whether they are found by candidate generation. 2.4 Further diagnostics and rank evaluation Several evaluations in the literature are beyond the scope of this paper but planned for future versions of the code. This includes further diagnostic subtask evaluation, particularly candidate set recall (Hachey et al., 2013), NIL accuracy (Ji and Grishman, 2011) and weak mention matching (Cornolti et al., 2013). With a score for each prediction, further metrics are possible: rank evaluation of tag annotation with r-precision, mean reciprocal rank and mean average precision (Meij et al., 2012); and rank evaluation of mentions for comparison to Hoffart et al. (2011) and Pilz and Paass (2012). 465 3 Data The CoNLL-YAGO dataset (Hoffart et al., 2011) is an excellent target for end-to-end, wholedocument entity annotation. It is public, free and much larger than most entity annotation data sets. It is based on the wide</context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2013</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity linking with Wikipedia. Artificial Intelligence, 194:130–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Conference on Empirical Methods in Natural Language Processing, pages 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1148--1158</pages>
<contexts>
<context position="1034" citStr="Ji and Grishman, 2011" startWordPosition="141" endWordPosition="144">ring the output of three approaches, and identify research opportunities. This utilises our open, accessible evaluation tool. We exemplify a new paradigm of distributed, shared evaluation, in which evaluation software and standardised, versioned system outputs are provided online. 1 Introduction Modern entity annotation systems detect mentions in text and disambiguate them to a knowledge base (KB). Disambiguation typically returns the corresponding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a simil</context>
<context position="6767" citStr="Ji and Grishman, 2011" startWordPosition="991" endWordPosition="994">me way, but metrics here ignore mention types. 2.3 Disambiguation evaluation Most NEL and wikification literature focuses on disambiguation, evaluating the quality of link target annotations in isolation from NER error. Providing systems with ground truth mentions makes fL equivalent to Mihalcea and Csomai’s (2007) sense disambiguation evaluation and Milne and Witten’s (2008b) disambiguation evaluation. It differs from Kulkarni et al.’s (2009) metric in being micro-averaged (equal weight to each mention), rather than macro-averaged across documents. fL recall is comparable to TAC’s KB recall (Ji and Grishman, 2011). It differs in that all mentions are evaluated rather than specific queries. Related evaluations have also isolated disambiguation performance by: considering the links of only correctly identified mentions (Cucerzan, 2007); or only true mentions where the correct entity appears among top candidates before disambiguation (Ratinov et al., 2011; Hoffart et al., 2011; Pilz and Paass, 2012). We do not prefer this approach as it makes system comparison difficult. For comparability, we implement a filter fLHOF that retains only Hoffart-linkable mentions having a YAGO means relation to the correct e</context>
<context position="8111" citStr="Ji and Grishman, 2011" startWordPosition="1194" endWordPosition="1198">a and Csomai’s (2007) keyword extraction evaluation and Ratinov et al.’s (2011) bag-of-titles evaluation. It is comparable to Pilz and Paass’s (2012) bag-of-titles evaluation, but does not account for sequential order and keeps all gold-standard links regardless of whether they are found by candidate generation. 2.4 Further diagnostics and rank evaluation Several evaluations in the literature are beyond the scope of this paper but planned for future versions of the code. This includes further diagnostic subtask evaluation, particularly candidate set recall (Hachey et al., 2013), NIL accuracy (Ji and Grishman, 2011) and weak mention matching (Cornolti et al., 2013). With a score for each prediction, further metrics are possible: rank evaluation of tag annotation with r-precision, mean reciprocal rank and mean average precision (Meij et al., 2012); and rank evaluation of mentions for comparison to Hoffart et al. (2011) and Pilz and Paass (2012). 465 3 Data The CoNLL-YAGO dataset (Hoffart et al., 2011) is an excellent target for end-to-end, wholedocument entity annotation. It is public, free and much larger than most entity annotation data sets. It is based on the widely used NER data from the CoNLL 2003 s</context>
<context position="18591" citStr="Ji and Grishman, 2011" startWordPosition="2899" endWordPosition="2902">nerates NIL, which may reflect candidate recall errors or a conservative disambiguation threshold. On the end-to-end evaluation, Schwa makes more linking errors (wrong link, link as nil, nil as link) than TagMe, but fewer in mention detection, leading to higher overall performance. By entity type Figure 1 evaluates only mentions where the CoNLL 2003 corpus (Tjong Kim Sang and Meulder, 2003) marks a NE mention of each type. This is based on the link evaluation of Schwa. The left and right bars correspond to end-to-end (fL) and disambiguation (fLHOF) F1 respectively. In accord with TAC results (Ji and Grishman, 2011), high accuracy can be achieved on PER when a full name is given, while ORG is substantially more challenging. MISC entities are somewhat difficult to disambiguate, with identification errors hampering end-to-end performance. Figure 2: Schwa fL and fLHOF F1 for top topics By topical category The underlying Reuters Corpus documents are labelled with topic, country and industry codes (Lewis et al., 2004). Figure 2 reports F1 on test documents from each frequent topic. It highlights that much ambiguity remains unresolved in Sports, while very high performance linking is attainable in categories s</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>Heng Ji and Ralph Grishman. 2011. Knowledge base population: Successful approaches and challenges. In 49th Annual Meeting of the Association for Computational Linguistics, pages 1148–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In 15th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>457--466</pages>
<contexts>
<context position="1481" citStr="Kulkarni et al., 2009" startWordPosition="203" endWordPosition="207">ding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a c</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of Wikipedia entities in web text. In 15th International Conference on Knowledge Discovery and Data Mining, pages 457–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="8989" citStr="Lewis et al., 2004" startWordPosition="1337" endWordPosition="1340"> mentions for comparison to Hoffart et al. (2011) and Pilz and Paass (2012). 465 3 Data The CoNLL-YAGO dataset (Hoffart et al., 2011) is an excellent target for end-to-end, wholedocument entity annotation. It is public, free and much larger than most entity annotation data sets. It is based on the widely used NER data from the CoNLL 2003 shared task (Tjong Kim Sang and Meulder, 2003), building disambiguation on ground truth mentions. It has standard training and development splits that are representative of the held-out test data, all being sourced from the Reuters text categorisation corpus (Lewis et al., 2004), which is provided free for research purposes. Training and development comprise 1,162 stories from 22-31 August 1996 and held-out test comprises 231 stories from 6-7 December 1996. The layered annotation provides useful information for analysis including categorisation topics (e.g., general news, markets, sport) and NE type markup (PER, ORG, LOC, MISC). The primary drawback is that KB annotations are currently present only if there is a YAGO means relation between the mention string and the correct entity. This means that there are a number of CoNLL entity mentions referring to entities that</context>
<context position="18996" citStr="Lewis et al., 2004" startWordPosition="2962" endWordPosition="2965">tion of each type. This is based on the link evaluation of Schwa. The left and right bars correspond to end-to-end (fL) and disambiguation (fLHOF) F1 respectively. In accord with TAC results (Ji and Grishman, 2011), high accuracy can be achieved on PER when a full name is given, while ORG is substantially more challenging. MISC entities are somewhat difficult to disambiguate, with identification errors hampering end-to-end performance. Figure 2: Schwa fL and fLHOF F1 for top topics By topical category The underlying Reuters Corpus documents are labelled with topic, country and industry codes (Lewis et al., 2004). Figure 2 reports F1 on test documents from each frequent topic. It highlights that much ambiguity remains unresolved in Sports, while very high performance linking is attainable in categories such as Markets and Domestic Politics, only when given ground truth linkable mentions. 7 Conclusion We surveyed entity annotation tasks and advocated a core set of metrics for mention, disambiguation and end-to-end evaluation. This enabled a direct comparison of state-of-the-art NEL and wikification systems, highlighting the effect of key differences. In particular, NER and coreference modules make NEL </context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Meij</author>
<author>Wouter Weerkamp</author>
<author>Maarten de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In 5th International Conference on Web Search and Data Mining,</booktitle>
<pages>563--572</pages>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In 5th International Conference on Web Search and Data Mining, pages 563–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify! Linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In 16th Conference on Information and Knowledge Management,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="5135" citStr="Mihalcea and Csomai, 2007" startWordPosition="743" endWordPosition="747">ilter f: |f(G) ∩ f(S) ||f(G) ∩ f(S)| Pf = |f(S) |, Rf = |f(G) |We advocate two core metrics, corresponding to the major whole-document entity annotation tasks. Link annotation measures performance over every linked mention. Its filter fL matches spans and link targets, disregarding NILs. This is particularly apt when entity annotation is a step in an information extraction pipeline. Tag annotation measures performance over document-level entity sets: fT disregards span information and NILs. This is appropriate when entity annotation is used, e.g., for document indexing or social media mining (Mihalcea and Csomai, 2007; Meij et al., 2012). We proceed to ground these metrics and diagnostic variants in the literature. 2.1 End-to-end evaluation We follow Cornolti et al. (2013) in evaluating endto-end entity annotation, including both mention detection and disambiguation. In this context, fL equates to Cornolti et al.’s strong annotation match; fT measures what they call entity match. 2.2 Mention evaluation Mention detection performance may be evaluated regardless of linking decisions. A filter fM discards the link target (kbid). Of the present metrics, only this considers NIL-linked system mentions as differen</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify! Linking documents to encyclopedic knowledge. In 16th Conference on Information and Knowledge Management, pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective, low-cost measure of semantic relatedness obtained from Wikipedia links.</title>
<date>2008</date>
<booktitle>In AAAI Workshop on Wikipedia and Articial Intelligence,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="1457" citStr="Milne and Witten, 2008" startWordPosition="199" endWordPosition="202">lly returns the corresponding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very dif</context>
<context position="11532" citStr="Milne and Witten, 2008" startWordPosition="1749" endWordPosition="1752">y (Fokkens et al., 2013). We include reference system output in our repository for simple comparison. Other researchers are welcome to add reference output, providing a continuous benchmark that complements the annual cycle of large shared tasks like TAC KBP. 4.1 TagMe TagMe (Ferragina and Scaiella, 2010) is an endto-end wikification system specialising in short texts. TagMe performs best among publicly available wikification systems (Cornolti et al., 2013). Mention detection uses a dictionary of anchor text from links between Wikipedia pages. Candidate ranking is based on entity relatedness (Milne and Witten, 2008a), followed by mention pruning. We use thresholds on annotation scores supplied by Marco Cornolti (personal communication) of 0.289 and 0.336 respectively for mention/link and tag evaluation. TagMe annotations may not align with CoNLL token boundaries, e.g., &lt;annot title=“Oakland, New Jersey”&gt;OAKLAND, N.J&lt;/annot&gt;. Before evaluation, we extend annotations to overlapping tokens. 4.2 AIDA AIDA (Hoffart et al., 2011) is the system presented with the CoNLL-YAGO dataset and places emphasis on state-of-the-art ranking of candidate entity sets. Mentions are ground truth from the CoNLL data to isolate</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008a. An effective, low-cost measure of semantic relatedness obtained from Wikipedia links. In AAAI Workshop on Wikipedia and Articial Intelligence, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with Wikipedia.</title>
<date>2008</date>
<booktitle>In 17th Conference on Information and Knowledge Management,</booktitle>
<pages>509--518</pages>
<contexts>
<context position="1457" citStr="Milne and Witten, 2008" startWordPosition="199" endWordPosition="202">lly returns the corresponding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very dif</context>
<context position="11532" citStr="Milne and Witten, 2008" startWordPosition="1749" endWordPosition="1752">y (Fokkens et al., 2013). We include reference system output in our repository for simple comparison. Other researchers are welcome to add reference output, providing a continuous benchmark that complements the annual cycle of large shared tasks like TAC KBP. 4.1 TagMe TagMe (Ferragina and Scaiella, 2010) is an endto-end wikification system specialising in short texts. TagMe performs best among publicly available wikification systems (Cornolti et al., 2013). Mention detection uses a dictionary of anchor text from links between Wikipedia pages. Candidate ranking is based on entity relatedness (Milne and Witten, 2008a), followed by mention pruning. We use thresholds on annotation scores supplied by Marco Cornolti (personal communication) of 0.289 and 0.336 respectively for mention/link and tag evaluation. TagMe annotations may not align with CoNLL token boundaries, e.g., &lt;annot title=“Oakland, New Jersey”&gt;OAKLAND, N.J&lt;/annot&gt;. Before evaluation, we extend annotations to overlapping tokens. 4.2 AIDA AIDA (Hoffart et al., 2011) is the system presented with the CoNLL-YAGO dataset and places emphasis on state-of-the-art ranking of candidate entity sets. Mentions are ground truth from the CoNLL data to isolate</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008b. Learning to link with Wikipedia. In 17th Conference on Information and Knowledge Management, pages 509– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="13445" citStr="Noreen, 1989" startWordPosition="2060" endWordPosition="2061">n data (Cucerzan and Sil, 2013). 466 System Mentions Filter P R F1 Cucerzan System f 82.2 84.8 83.5 Schwa System f 86.9 76.7 81.5 TagMe System f KB 75.2 60.4 67.0 Schwa System f KB 82.5 74.5 78.3 Table 1: Mention detection results. Cucerzan results as reported (Cucerzan, 2007). 5 Results We briefly report results over the reference systems to highlight characteristics of the evaluation metrics and task settings. Results hinge upon Schwa since we have obtained only its output in all settings. Except where noted, all differences are significant (p &lt; 0.05) according to approximate randomisation (Noreen, 1989), permuting annotations over whole documents. 5.1 Mention evaluation Table 1 evaluates mentions with and without NILs. None of the systems reported use a CoNLLtrained NER tagger, for which top shared task participants approached 90% Fi in a stricter evaluation than fm. We note the impressive numbers reported by Cucerzan (2007) using a novel approach to mention detection based on capitalisation and corpus co-occurrence statistics, and the similar performance1 to Schwa, whose NER component is trained on another news corpus. In wikification, NIL-linked mentions may not be relevant, and it may suf</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Pilz</author>
<author>Gerhard Paass</author>
</authors>
<title>Collective search for concept disambiguation.</title>
<date>2012</date>
<booktitle>In 24th International Conference on Computational Linguistics,</booktitle>
<pages>2243--2258</pages>
<contexts>
<context position="7157" citStr="Pilz and Paass, 2012" startWordPosition="1050" endWordPosition="1053">ion evaluation. It differs from Kulkarni et al.’s (2009) metric in being micro-averaged (equal weight to each mention), rather than macro-averaged across documents. fL recall is comparable to TAC’s KB recall (Ji and Grishman, 2011). It differs in that all mentions are evaluated rather than specific queries. Related evaluations have also isolated disambiguation performance by: considering the links of only correctly identified mentions (Cucerzan, 2007); or only true mentions where the correct entity appears among top candidates before disambiguation (Ratinov et al., 2011; Hoffart et al., 2011; Pilz and Paass, 2012). We do not prefer this approach as it makes system comparison difficult. For comparability, we implement a filter fLHOF that retains only Hoffart-linkable mentions having a YAGO means relation to the correct entity. Tag annotation (fT) with ground truth mentions is equivalent to Milne and Witten’s (2008b) link evaluation, Mihalcea and Csomai’s (2007) keyword extraction evaluation and Ratinov et al.’s (2011) bag-of-titles evaluation. It is comparable to Pilz and Paass’s (2012) bag-of-titles evaluation, but does not account for sequential order and keeps all gold-standard links regardless of wh</context>
<context position="8445" citStr="Pilz and Paass (2012)" startWordPosition="1249" endWordPosition="1252">cs and rank evaluation Several evaluations in the literature are beyond the scope of this paper but planned for future versions of the code. This includes further diagnostic subtask evaluation, particularly candidate set recall (Hachey et al., 2013), NIL accuracy (Ji and Grishman, 2011) and weak mention matching (Cornolti et al., 2013). With a score for each prediction, further metrics are possible: rank evaluation of tag annotation with r-precision, mean reciprocal rank and mean average precision (Meij et al., 2012); and rank evaluation of mentions for comparison to Hoffart et al. (2011) and Pilz and Paass (2012). 465 3 Data The CoNLL-YAGO dataset (Hoffart et al., 2011) is an excellent target for end-to-end, wholedocument entity annotation. It is public, free and much larger than most entity annotation data sets. It is based on the widely used NER data from the CoNLL 2003 shared task (Tjong Kim Sang and Meulder, 2003), building disambiguation on ground truth mentions. It has standard training and development splits that are representative of the held-out test data, all being sourced from the Reuters text categorisation corpus (Lewis et al., 2004), which is provided free for research purposes. Training</context>
</contexts>
<marker>Pilz, Paass, 2012</marker>
<rawString>Anja Pilz and Gerhard Paass. 2012. Collective search for concept disambiguation. In 24th International Conference on Computational Linguistics, pages 2243–2258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Pink</author>
<author>Will Radford</author>
<author>Will Cannings</author>
<author>Andrew Naoum</author>
<author>Joel Nothman</author>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<date>2013</date>
<journal>SYDNEY CMCRC at TAC</journal>
<booktitle>In Text Analysis Conference.</booktitle>
<contexts>
<context position="1267" citStr="Pink et al., 2013" startWordPosition="173" endWordPosition="176">rsioned system outputs are provided online. 1 Introduction Modern entity annotation systems detect mentions in text and disambiguate them to a knowledge base (KB). Disambiguation typically returns the corresponding Wikipedia page or NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and W</context>
</contexts>
<marker>Pink, Radford, Cannings, Naoum, Nothman, Tse, Curran, 2013</marker>
<rawString>Glen Pink, Will Radford, Will Cannings, Andrew Naoum, Joel Nothman, Daniel Tse, and James R. Curran. 2013. SYDNEY CMCRC at TAC 2013. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Radford</author>
<author>Will Cannings</author>
<author>Andrew Naoum</author>
<author>Joel Nothman</author>
<author>Glen Pink</author>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<date>2012</date>
<journal>(Almost) Total Recall – SYDNEY CMCRC at TAC</journal>
<booktitle>In Text Analysis Conference.</booktitle>
<contexts>
<context position="12295" citStr="Radford et al., 2012" startWordPosition="1868" endWordPosition="1871"> respectively for mention/link and tag evaluation. TagMe annotations may not align with CoNLL token boundaries, e.g., &lt;annot title=“Oakland, New Jersey”&gt;OAKLAND, N.J&lt;/annot&gt;. Before evaluation, we extend annotations to overlapping tokens. 4.2 AIDA AIDA (Hoffart et al., 2011) is the system presented with the CoNLL-YAGO dataset and places emphasis on state-of-the-art ranking of candidate entity sets. Mentions are ground truth from the CoNLL data to isolate ranking performance, equivalent to applying the fLaoa filter. Ranking is informed by a graph model of entity compatibility. 4.3 Schwa Schwa (Radford et al., 2012) is a heuristic NEL system based on a TAC 2012 shared task entrant. Mention detection uses a NER model trained on news text followed by rule-based coreference. Disambiguation uses an unweighted combination of KB statistics, document compatibility (Cucerzan, 2007), graph similarity and targeted textual similarity. Candidates that score below a threshold learned from TAC data are linked to NIL. The system is very competitive, performing at 93% and 97% respectively of the best accuracy numbers we know of on 2011 and 2012 TAC evaluation data (Cucerzan and Sil, 2013). 466 System Mentions Filter P R</context>
</contexts>
<marker>Radford, Cannings, Naoum, Nothman, Pink, Tse, Curran, 2012</marker>
<rawString>Will Radford, Will Cannings, Andrew Naoum, Joel Nothman, Glen Pink, Daniel Tse, and James R. Curran. 2012. (Almost) Total Recall – SYDNEY CMCRC at TAC 2012. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to Wikipedia.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1375--1384</pages>
<contexts>
<context position="1503" citStr="Ratinov et al., 2011" startWordPosition="208" endWordPosition="211">NIL if none exists. Named entity linking (NEL) work is driven by the TAC shared tasks on query-driven knowledge base population (Ji and Grishman, 2011). Evaluation focuses on disambiguating queried names and clustering NIL mentions, but most systems internally perform whole-document named entity recognition, coreference, and disambiguation (Cucerzan and Sil, 2013; Pink et al., 2013; Cheng et al., 2013; Fahrni et al., 2013). Wikification work generally evaluates end-to-end entity annotation including KB-driven mention spotting and disambiguation (Milne and Witten, 2008b; Kulkarni et al., 2009; Ratinov et al., 2011; Ferragina and Scaiella, 2010). Despite important differences in mention handling, NEL and wikification work have followed a similar trajectory. Yet to our knowledge, there are no comparative whole-document evaluations of NEL and wikification systems. Public data sets have also driven research in whole-document entity disambiguation (Cucerzan, 2007; Milne and Witten, 2008b; Kulkarni et al., 2009; Bentivogli et al., 2010; Hoffart et al., 2011; Meij et al., 2012). However, with many task variants and evaluation methodologies proposed, it is very difficult to synthesise a clear picture of the st</context>
<context position="7112" citStr="Ratinov et al., 2011" startWordPosition="1042" endWordPosition="1045">n and Milne and Witten’s (2008b) disambiguation evaluation. It differs from Kulkarni et al.’s (2009) metric in being micro-averaged (equal weight to each mention), rather than macro-averaged across documents. fL recall is comparable to TAC’s KB recall (Ji and Grishman, 2011). It differs in that all mentions are evaluated rather than specific queries. Related evaluations have also isolated disambiguation performance by: considering the links of only correctly identified mentions (Cucerzan, 2007); or only true mentions where the correct entity appears among top candidates before disambiguation (Ratinov et al., 2011; Hoffart et al., 2011; Pilz and Paass, 2012). We do not prefer this approach as it makes system comparison difficult. For comparability, we implement a filter fLHOF that retains only Hoffart-linkable mentions having a YAGO means relation to the correct entity. Tag annotation (fT) with ground truth mentions is equivalent to Milne and Witten’s (2008b) link evaluation, Mihalcea and Csomai’s (2007) keyword extraction evaluation and Ratinov et al.’s (2011) bag-of-titles evaluation. It is comparable to Pilz and Paass’s (2012) bag-of-titles evaluation, but does not account for sequential order and k</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to Wikipedia. In 49th Annual Meeting of the Association for Computational Linguistics, pages 1375–1384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Conference On Computational Natural Language Learning,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Conference On Computational Natural Language Learning, pages 142–147.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>