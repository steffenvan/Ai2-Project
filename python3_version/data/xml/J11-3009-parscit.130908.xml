<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005532">
<subsectionHeader confidence="0.434791333333333">
Book Reviews
Computational Modeling of Human Language Acquisition
Afra Alishahi
</subsectionHeader>
<bodyText confidence="0.737339">
(University of the Saarland)
Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 11), 2010, xiv+93 pp; paperbound, ISBN 978-1-60845-339-9,
$40.00; ebook, ISBN 978-1-60845-340-5, $30.00 or by subscription
</bodyText>
<figure confidence="0.4793445">
Reviewed by
Sharon Goldwater
</figure>
<affiliation confidence="0.650027">
University of Edinburgh
</affiliation>
<bodyText confidence="0.990504448275863">
For much of the last 25 years or more, researchers in natural language processing
(NLP) and those interested in human language acquisition have had little to say to
one another. NLP researchers were increasingly focusing on data-intensive supervised
learning methods, mostly using structured representations, while models of language
acquisition were typically based either on symbolic nativist accounts (Dresher and
Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations
of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996).
Moreover, language acquisition researchers have understandably been more interested
in unsupervised than supervised learning, and (perhaps due to the much more difficult
nature of this problem) have often focused on learning from toy data sets rather than
large naturalistic corpora.
Recent developments in both fields have led to a narrowing of the research gap,
however. NLP researchers have become increasingly interested in unsupervised and
minimally supervised methods, and the rise of probabilistic models of cognition (Chater
and Oaksford 1998; Griffiths, Kemp, and Tenenbaum 2008) means there is now a grow-
ing number of cognitive scientists who are well-versed in many of the same statistical
methods that are used in NLP. Thus, a short introductory text on computational mod-
eling of human language acquisition seems particularly apt at this time. Alishahi’s slim
volume is not intended to be comprehensive, but rather to provide a brief overview
of the goals and methods of the field for researchers in related areas—either language
acquisition researchers with little computational experience or NLP researchers with-
out much knowledge of cognitive science. It aims for intuitive explanations rather
than highly technical ones, and includes a number of figures and diagrams, but no
equations.
The book can be divided logically into two parts followed by a brief concluding
chapter. The first part (Chapters 1 and 2) provides an overview of the major research
questions and methodologies in the field. Chapter 1 begins by introducing some of the
main theoretical debates in the field of language acquisition—questions of modularity
and learnability. NLP researchers with some background in linguistics will probably
already be familiar with these debates at the level of detail presented here; these sections
will be more useful to those with a straight computer science background. Also included
in Chapter 1 is a section motivating the use of computational models as an alternative
to behavioral studies for studying language acquisition. This section will be useful to
anyone who is new to the idea of computational modeling of cognition, as will Chapter 2
of the book. Chapter 2 discusses Marr’s (1982) influential analysis of the different kinds
© 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 3
of explanations that models can provide, as well as the criteria for cognitive plausibility
against which models are judged, and the main frameworks for model development
(symbolic, connectionist, probabilistic). Finally, it describes the various ways models
can be evaluated, along with a list of corpus resources.
In the second part of the book (Chapters 3–5), Alishahi focuses on three areas of
language acquisition in particular: learning the meanings of words (Chapter 3), learning
morphology and syntax (Chapter 4), and learning relationships between syntax and
semantics, such as verb–argument structure and semantic roles (Chapter 5). Each chap-
ter is divided into sections focusing on more specific topics—for example, Chapter 4
includes sections on morphology, syntactic categories, and syntactic structure. Each
section begins by reviewing the most salient empirical facts about children’s acquisition
in that domain, along with relevant linguistic concepts and theories that have influenced
the modeling community (e.g., Mutual Exclusivity in word learning; Construction
Grammar and the Principles and Parameters theory in syntactic acquisition; theories
of selectional restrictions in the acquisition of verb–argument structure). The empirical
and theoretical background is followed by an overview of many of the models that have
been proposed in the area, and finally one or more “case studies” (more on these in a
moment).
In general the structure and content of this book is appropriate for researchers from
neighboring fields (including NLP) who want to get a quick taste of what computational
modeling of human language acquisition is all about. They can read the first couple of
chapters to get a general idea of the questions and methodologies, and then pick and
choose any topics from the remaining chapters that might be of interest. There are very
few dependencies between sections in the second part of the book, because most of the
relevant background for each section is provided in that section itself. Someone who is
interested in pursuing the field further (either a beginning graduate student or a more
advanced researcher moving into the field) will also find many useful references, at least
in the three areas that Alishahi focuses on. Many other active areas of modeling are not
covered at all (for example phonetic and phonological acquisition), although this choice
is understandable given the length of the book.
The main weakness of the book is in the execution of the case studies. Each case
study (with some exceptions, as noted subsequently) details a single model, with a half-
page to two-page description of the model’s primary methods and assumptions, as well
as the input and results. It is an excellent idea to provide concrete examples showing
how models can be used to address important questions in language acquisition. But
in practice, most of the case studies fall short of this goal, as they are too light on
motivation and analysis. Alishahi is not always clear about why certain models, rather
than others, were chosen for case studies (are they ground-breaking in some way, or
merely a simple example of a particular theoretical idea put into practice?), nor how
each model’s assumptions and results relate to the broader goals of modelling set out
in the first two chapters. In addition, three out of the four “case studies” in Chapter 4
are really just additional review sections, covering models of the English past tense,
learning algorithms based on Principles and Parameters, and distributional models of
syntactic structure (all worthy topics, but not case studies). This leaves only one true
case study in Chapter 4, on the MOSAIC model of grammar induction (Jones, Gobet,
and Pine 2000). Finally, although there are case studies of symbolic, connectionist, and
probabilistic models, no Bayesian models are given a detailed look (though they are
included in the review sections). Bayesian models are, of course, a subset of probabilistic
models, but take a very different philosophical approach to most other models, includ-
ing the type of incremental probabilistic model that Alishahi discusses in more detail.
</bodyText>
<page confidence="0.996333">
628
</page>
<subsectionHeader confidence="0.849937">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999816909090909">
Bayesian modeling is now an important force in cognitive science generally and has
begun to make an impact in language acquisition specifically (Xu and Tenenbaum 2007;
Foraker et al. 2009; Goldwater, Griffiths, and Johnson 2009); as such it is worth taking a
bit more space to explain the ideas behind at least one of these models.
Despite these weaknesses in the case studies, there is enough useful material in
this book to make reading it worthwhile to any researcher who wants to get a quick
overview of the main goals and approaches in the field, along with some of the many
models that have been developed over the years. It will also be helpful to those who
are looking for a starting point for a more in-depth study of models in one of the three
areas of acquisition that Alishahi focuses on. Overall, it is a very accessible, if necessarily
selective, brief introduction to the field.
</bodyText>
<sectionHeader confidence="0.995643" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.9563375">
Chater, Nicholas and Mike Oaksford, editors.
1998. Rational Models of Cognition. Oxford
University Press, Oxford.
Dresher, B. Elan and Jonathan Kaye. 1990.
A computational learning model for
metrical phonology. Cognition,
34(2):137–195.
Elman, Jeffrey, Elizabeth Bates, Mark H.
Johnson, Anette Karmiloff-Smith,
Domenico Parisi, and Kim Plunkett. 1996.
Rethinking Innateness: A Connectionist
Perspective on Development. MIT
Press/Bradford Books, Cambridge, MA.
Foraker, Stephanie, Terry Regier, Naveen
Khetarpal, Amy Perfors, and Joshua B.
Tenenbaum. 2009. Indirect evidence and
the poverty of the stimulus: The case of
anaphoric one. Cognitive Science,
33(2):287–300.
Gibson, Edward and Kenneth Wexler.
1994. Triggers. Linguistic Inquiry,
25(3):407–454.
Goldwater, Sharon, Thomas L. Griffiths,
and Mark Johnson. 2009. A Bayesian
framework for word segmentation:
This book review was edited by Pierre Isabelle.
Exploring the effects of context. Cognition,
112(1):21–54.
Griffiths, Thomas L., Charles Kemp, and
Joshua B. Tenenbaum. 2008. Bayesian
models of cognition. In Ron Sun, editor,
Cambridge Handbook of Computational
Cognitive Modeling. Cambridge University
Press, Cambridge, chapter 3.
Jones, Gary, Fernand Gobet, and Julian M.
Pine. 2000. A process model of children’s
early verb use. In Proceedings of the 22nd
Meeting of the Cognitive Science Society,
pages 723–728, Philadelphia, PA.
Marr, David. 1982. Vision: A Computational
Approach. Freeman &amp; Co., San
Francisco, CA.
Rumelhart, David and James McClelland.
1986. On learning the past tenses of
English verbs. In David Rumelhart and
James McClelland, editors, Parallel
Distributed Processing: Explorations in the
Microstructure of Cognition. MIT Press,
Cambridge, MA, chapter 18.
Xu, Fei and Joshua B. Tenenbaum. 2007.
Word learning as Bayesian inference.
Psychological Review, 114(2):245–272.
Sharon Goldwater is a Lecturer in the Institute for Language, Cognition and Computation, School
of Informatics, University of Edinburgh. Her research interests include unsupervised learning of
linguistic structure (including word segmentation, phonology, morphology, and syntax) and the
application of machine learning techniques, especially Bayesian methods, to the computational
study of human language acquisition. Goldwater’s address is Informatics Forum, 10 Crichton
Street, Edinburgh EH8 9AB, United Kingdom; e-mail: sgwater@inf.ed.ac.uk.
</reference>
<page confidence="0.998687">
629
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.074135">
<title confidence="0.9966765">Book Reviews Computational Modeling of Human Language Acquisition</title>
<author confidence="0.95678">Afra Alishahi</author>
<affiliation confidence="0.808675">(University of the Saarland)</affiliation>
<author confidence="0.759841">xiv pp</author>
<author confidence="0.759841">ISBN paperbound</author>
<keyword confidence="0.294521">40.00; ebook, ISBN 978-1-60845-340-5, $30.00 or by subscription</keyword>
<note confidence="0.948979">Reviewed by</note>
<author confidence="0.999285">Sharon Goldwater</author>
<affiliation confidence="0.997777">University of Edinburgh</affiliation>
<abstract confidence="0.987369515151515">For much of the last 25 years or more, researchers in natural language processing (NLP) and those interested in human language acquisition have had little to say to one another. NLP researchers were increasingly focusing on data-intensive supervised learning methods, mostly using structured representations, while models of language acquisition were typically based either on symbolic nativist accounts (Dresher and Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996). Moreover, language acquisition researchers have understandably been more interested and (perhaps due to the much more difficult nature of this problem) have often focused on learning from toy data sets rather than large naturalistic corpora. Recent developments in both fields have led to a narrowing of the research gap, however. NLP researchers have become increasingly interested in unsupervised and minimally supervised methods, and the rise of probabilistic models of cognition (Chater and Oaksford 1998; Griffiths, Kemp, and Tenenbaum 2008) means there is now a growing number of cognitive scientists who are well-versed in many of the same statistical methods that are used in NLP. Thus, a short introductory text on computational modeling of human language acquisition seems particularly apt at this time. Alishahi’s slim volume is not intended to be comprehensive, but rather to provide a brief overview of the goals and methods of the field for researchers in related areas—either language acquisition researchers with little computational experience or NLP researchers without much knowledge of cognitive science. It aims for intuitive explanations rather than highly technical ones, and includes a number of figures and diagrams, but no equations. The book can be divided logically into two parts followed by a brief concluding chapter. The first part (Chapters 1 and 2) provides an overview of the major research questions and methodologies in the field. Chapter 1 begins by introducing some of the main theoretical debates in the field of language acquisition—questions of modularity and learnability. NLP researchers with some background in linguistics will probably already be familiar with these debates at the level of detail presented here; these sections will be more useful to those with a straight computer science background. Also included in Chapter 1 is a section motivating the use of computational models as an alternative to behavioral studies for studying language acquisition. This section will be useful to anyone who is new to the idea of computational modeling of cognition, as will Chapter 2 of the book. Chapter 2 discusses Marr’s (1982) influential analysis of the different kinds © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 3 of explanations that models can provide, as well as the criteria for cognitive plausibility against which models are judged, and the main frameworks for model development (symbolic, connectionist, probabilistic). Finally, it describes the various ways models can be evaluated, along with a list of corpus resources. In the second part of the book (Chapters 3–5), Alishahi focuses on three areas of language acquisition in particular: learning the meanings of words (Chapter 3), learning morphology and syntax (Chapter 4), and learning relationships between syntax and semantics, such as verb–argument structure and semantic roles (Chapter 5). Each chapter is divided into sections focusing on more specific topics—for example, Chapter 4 includes sections on morphology, syntactic categories, and syntactic structure. Each section begins by reviewing the most salient empirical facts about children’s acquisition in that domain, along with relevant linguistic concepts and theories that have influenced the modeling community (e.g., Mutual Exclusivity in word learning; Construction Grammar and the Principles and Parameters theory in syntactic acquisition; theories of selectional restrictions in the acquisition of verb–argument structure). The empirical and theoretical background is followed by an overview of many of the models that have been proposed in the area, and finally one or more “case studies” (more on these in a moment). In general the structure and content of this book is appropriate for researchers from neighboring fields (including NLP) who want to get a quick taste of what computational modeling of human language acquisition is all about. They can read the first couple of chapters to get a general idea of the questions and methodologies, and then pick and choose any topics from the remaining chapters that might be of interest. There are very few dependencies between sections in the second part of the book, because most of the relevant background for each section is provided in that section itself. Someone who is interested in pursuing the field further (either a beginning graduate student or a more advanced researcher moving into the field) will also find many useful references, at least in the three areas that Alishahi focuses on. Many other active areas of modeling are not covered at all (for example phonetic and phonological acquisition), although this choice is understandable given the length of the book. The main weakness of the book is in the execution of the case studies. Each case study (with some exceptions, as noted subsequently) details a single model, with a halfpage to two-page description of the model’s primary methods and assumptions, as well as the input and results. It is an excellent idea to provide concrete examples showing how models can be used to address important questions in language acquisition. But in practice, most of the case studies fall short of this goal, as they are too light on motivation and analysis. Alishahi is not always clear about why certain models, rather than others, were chosen for case studies (are they ground-breaking in some way, or merely a simple example of a particular theoretical idea put into practice?), nor how each model’s assumptions and results relate to the broader goals of modelling set out in the first two chapters. In addition, three out of the four “case studies” in Chapter 4 are really just additional review sections, covering models of the English past tense, learning algorithms based on Principles and Parameters, and distributional models of syntactic structure (all worthy topics, but not case studies). This leaves only one true case study in Chapter 4, on the MOSAIC model of grammar induction (Jones, Gobet, and Pine 2000). Finally, although there are case studies of symbolic, connectionist, and probabilistic models, no Bayesian models are given a detailed look (though they are included in the review sections). Bayesian models are, of course, a subset of probabilistic models, but take a very different philosophical approach to most other models, including the type of incremental probabilistic model that Alishahi discusses in more detail. 628 Book Reviews Bayesian modeling is now an important force in cognitive science generally and has begun to make an impact in language acquisition specifically (Xu and Tenenbaum 2007; Foraker et al. 2009; Goldwater, Griffiths, and Johnson 2009); as such it is worth taking a bit more space to explain the ideas behind at least one of these models. Despite these weaknesses in the case studies, there is enough useful material in this book to make reading it worthwhile to any researcher who wants to get a quick overview of the main goals and approaches in the field, along with some of the many models that have been developed over the years. It will also be helpful to those who are looking for a starting point for a more in-depth study of models in one of the three areas of acquisition that Alishahi focuses on. Overall, it is a very accessible, if necessarily</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Rational Models of Cognition.</title>
<date>1998</date>
<editor>Chater, Nicholas and Mike Oaksford, editors.</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker>1998</marker>
<rawString>Chater, Nicholas and Mike Oaksford, editors. 1998. Rational Models of Cognition. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Elan Dresher</author>
<author>Jonathan Kaye</author>
</authors>
<title>A computational learning model for metrical phonology.</title>
<date>1990</date>
<journal>Cognition,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="812" citStr="Dresher and Kaye 1990" startWordPosition="107" endWordPosition="110">aeme Hirst, volume 11), 2010, xiv+93 pp; paperbound, ISBN 978-1-60845-339-9, $40.00; ebook, ISBN 978-1-60845-340-5, $30.00 or by subscription Reviewed by Sharon Goldwater University of Edinburgh For much of the last 25 years or more, researchers in natural language processing (NLP) and those interested in human language acquisition have had little to say to one another. NLP researchers were increasingly focusing on data-intensive supervised learning methods, mostly using structured representations, while models of language acquisition were typically based either on symbolic nativist accounts (Dresher and Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996). Moreover, language acquisition researchers have understandably been more interested in unsupervised than supervised learning, and (perhaps due to the much more difficult nature of this problem) have often focused on learning from toy data sets rather than large naturalistic corpora. Recent developments in both fields have led to a narrowing of the research gap, however. NLP researchers have become increasingly interested in unsupervised </context>
</contexts>
<marker>Dresher, Kaye, 1990</marker>
<rawString>Dresher, B. Elan and Jonathan Kaye. 1990. A computational learning model for metrical phonology. Cognition, 34(2):137–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Elman</author>
<author>Elizabeth Bates</author>
<author>Mark H Johnson</author>
<author>Anette Karmiloff-Smith</author>
<author>Domenico Parisi</author>
<author>Kim Plunkett</author>
</authors>
<title>Rethinking Innateness: A Connectionist Perspective on Development.</title>
<date>1996</date>
<publisher>MIT Press/Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="969" citStr="Elman et al. 1996" startWordPosition="129" endWordPosition="132"> Goldwater University of Edinburgh For much of the last 25 years or more, researchers in natural language processing (NLP) and those interested in human language acquisition have had little to say to one another. NLP researchers were increasingly focusing on data-intensive supervised learning methods, mostly using structured representations, while models of language acquisition were typically based either on symbolic nativist accounts (Dresher and Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996). Moreover, language acquisition researchers have understandably been more interested in unsupervised than supervised learning, and (perhaps due to the much more difficult nature of this problem) have often focused on learning from toy data sets rather than large naturalistic corpora. Recent developments in both fields have led to a narrowing of the research gap, however. NLP researchers have become increasingly interested in unsupervised and minimally supervised methods, and the rise of probabilistic models of cognition (Chater and Oaksford 1998; Griffiths, Kemp, and Tenenbaum 2008) means the</context>
</contexts>
<marker>Elman, Bates, Johnson, Karmiloff-Smith, Parisi, Plunkett, 1996</marker>
<rawString>Elman, Jeffrey, Elizabeth Bates, Mark H. Johnson, Anette Karmiloff-Smith, Domenico Parisi, and Kim Plunkett. 1996. Rethinking Innateness: A Connectionist Perspective on Development. MIT Press/Bradford Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Foraker</author>
<author>Terry Regier</author>
<author>Naveen Khetarpal</author>
<author>Amy Perfors</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Indirect evidence and the poverty of the stimulus: The case of anaphoric one.</title>
<date>2009</date>
<journal>Cognitive Science,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="7702" citStr="Foraker et al. 2009" startWordPosition="1176" endWordPosition="1179">, although there are case studies of symbolic, connectionist, and probabilistic models, no Bayesian models are given a detailed look (though they are included in the review sections). Bayesian models are, of course, a subset of probabilistic models, but take a very different philosophical approach to most other models, including the type of incremental probabilistic model that Alishahi discusses in more detail. 628 Book Reviews Bayesian modeling is now an important force in cognitive science generally and has begun to make an impact in language acquisition specifically (Xu and Tenenbaum 2007; Foraker et al. 2009; Goldwater, Griffiths, and Johnson 2009); as such it is worth taking a bit more space to explain the ideas behind at least one of these models. Despite these weaknesses in the case studies, there is enough useful material in this book to make reading it worthwhile to any researcher who wants to get a quick overview of the main goals and approaches in the field, along with some of the many models that have been developed over the years. It will also be helpful to those who are looking for a starting point for a more in-depth study of models in one of the three areas of acquisition that Alishah</context>
</contexts>
<marker>Foraker, Regier, Khetarpal, Perfors, Tenenbaum, 2009</marker>
<rawString>Foraker, Stephanie, Terry Regier, Naveen Khetarpal, Amy Perfors, and Joshua B. Tenenbaum. 2009. Indirect evidence and the poverty of the stimulus: The case of anaphoric one. Cognitive Science, 33(2):287–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
<author>Kenneth Wexler</author>
</authors>
<date>1994</date>
<journal>Triggers. Linguistic Inquiry,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="837" citStr="Gibson and Wexler 1994" startWordPosition="111" endWordPosition="114"> 2010, xiv+93 pp; paperbound, ISBN 978-1-60845-339-9, $40.00; ebook, ISBN 978-1-60845-340-5, $30.00 or by subscription Reviewed by Sharon Goldwater University of Edinburgh For much of the last 25 years or more, researchers in natural language processing (NLP) and those interested in human language acquisition have had little to say to one another. NLP researchers were increasingly focusing on data-intensive supervised learning methods, mostly using structured representations, while models of language acquisition were typically based either on symbolic nativist accounts (Dresher and Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996). Moreover, language acquisition researchers have understandably been more interested in unsupervised than supervised learning, and (perhaps due to the much more difficult nature of this problem) have often focused on learning from toy data sets rather than large naturalistic corpora. Recent developments in both fields have led to a narrowing of the research gap, however. NLP researchers have become increasingly interested in unsupervised and minimally supervised </context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>Gibson, Edward and Kenneth Wexler. 1994. Triggers. Linguistic Inquiry, 25(3):407–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: This book review was edited by Pierre Isabelle. Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Goldwater, Sharon, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: This book review was edited by Pierre Isabelle. Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Charles Kemp</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Bayesian models of cognition.</title>
<date>2008</date>
<booktitle>Cambridge Handbook of Computational Cognitive Modeling.</booktitle>
<editor>In Ron Sun, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<note>chapter 3.</note>
<marker>Griffiths, Kemp, Tenenbaum, 2008</marker>
<rawString>Griffiths, Thomas L., Charles Kemp, and Joshua B. Tenenbaum. 2008. Bayesian models of cognition. In Ron Sun, editor, Cambridge Handbook of Computational Cognitive Modeling. Cambridge University Press, Cambridge, chapter 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Jones</author>
<author>Fernand Gobet</author>
<author>Julian M Pine</author>
</authors>
<title>A process model of children’s early verb use.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Meeting of the Cognitive Science Society,</booktitle>
<pages>723--728</pages>
<location>Philadelphia, PA.</location>
<marker>Jones, Gobet, Pine, 2000</marker>
<rawString>Jones, Gary, Fernand Gobet, and Julian M. Pine. 2000. A process model of children’s early verb use. In Proceedings of the 22nd Meeting of the Cognitive Science Society, pages 723–728, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Marr</author>
</authors>
<title>Vision: A Computational Approach.</title>
<date>1982</date>
<publisher>Freeman &amp; Co.,</publisher>
<location>San Francisco, CA.</location>
<marker>Marr, 1982</marker>
<rawString>Marr, David. 1982. Vision: A Computational Approach. Freeman &amp; Co., San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rumelhart</author>
<author>James McClelland</author>
</authors>
<title>On learning the past tenses of English verbs.</title>
<date>1986</date>
<booktitle>Parallel Distributed Processing: Explorations in the Microstructure of Cognition.</booktitle>
<editor>In David Rumelhart and James McClelland, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<note>chapter 18.</note>
<contexts>
<context position="949" citStr="Rumelhart and McClelland 1986" startWordPosition="125" endWordPosition="128">subscription Reviewed by Sharon Goldwater University of Edinburgh For much of the last 25 years or more, researchers in natural language processing (NLP) and those interested in human language acquisition have had little to say to one another. NLP researchers were increasingly focusing on data-intensive supervised learning methods, mostly using structured representations, while models of language acquisition were typically based either on symbolic nativist accounts (Dresher and Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996). Moreover, language acquisition researchers have understandably been more interested in unsupervised than supervised learning, and (perhaps due to the much more difficult nature of this problem) have often focused on learning from toy data sets rather than large naturalistic corpora. Recent developments in both fields have led to a narrowing of the research gap, however. NLP researchers have become increasingly interested in unsupervised and minimally supervised methods, and the rise of probabilistic models of cognition (Chater and Oaksford 1998; Griffiths, Kemp, and Tenen</context>
</contexts>
<marker>Rumelhart, McClelland, 1986</marker>
<rawString>Rumelhart, David and James McClelland. 1986. On learning the past tenses of English verbs. In David Rumelhart and James McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press, Cambridge, MA, chapter 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xu</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Word learning as Bayesian inference.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="7681" citStr="Xu and Tenenbaum 2007" startWordPosition="1172" endWordPosition="1175">and Pine 2000). Finally, although there are case studies of symbolic, connectionist, and probabilistic models, no Bayesian models are given a detailed look (though they are included in the review sections). Bayesian models are, of course, a subset of probabilistic models, but take a very different philosophical approach to most other models, including the type of incremental probabilistic model that Alishahi discusses in more detail. 628 Book Reviews Bayesian modeling is now an important force in cognitive science generally and has begun to make an impact in language acquisition specifically (Xu and Tenenbaum 2007; Foraker et al. 2009; Goldwater, Griffiths, and Johnson 2009); as such it is worth taking a bit more space to explain the ideas behind at least one of these models. Despite these weaknesses in the case studies, there is enough useful material in this book to make reading it worthwhile to any researcher who wants to get a quick overview of the main goals and approaches in the field, along with some of the many models that have been developed over the years. It will also be helpful to those who are looking for a starting point for a more in-depth study of models in one of the three areas of acq</context>
</contexts>
<marker>Xu, Tenenbaum, 2007</marker>
<rawString>Xu, Fei and Joshua B. Tenenbaum. 2007. Word learning as Bayesian inference. Psychological Review, 114(2):245–272.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sharon</author>
</authors>
<title>Goldwater is a Lecturer in the Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh. Her research interests include unsupervised learning of linguistic structure (including word segmentation, phonology, morphology, and syntax) and the application of machine learning techniques, especially Bayesian methods, to the computational study of human language acquisition.</title>
<booktitle>Goldwater’s address is Informatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, United Kingdom; e-mail:</booktitle>
<pages>sgwater@inf.ed.ac.uk.</pages>
<marker>Sharon, </marker>
<rawString>Sharon Goldwater is a Lecturer in the Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh. Her research interests include unsupervised learning of linguistic structure (including word segmentation, phonology, morphology, and syntax) and the application of machine learning techniques, especially Bayesian methods, to the computational study of human language acquisition. Goldwater’s address is Informatics Forum, 10 Crichton Street, Edinburgh EH8 9AB, United Kingdom; e-mail: sgwater@inf.ed.ac.uk.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>