<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000356">
<title confidence="0.995783">
Transition-based Spinal Parsing
</title>
<author confidence="0.998118">
Miguel Ballesteros&apos;,2 Xavier Carreras3
</author>
<affiliation confidence="0.7783385">
&apos;NLP Group, Pompeu Fabra University 2Carnegie Mellon University
3Xerox Research Centre Europe
</affiliation>
<email confidence="0.9966">
miguel.ballesteros@upf.edu xavier.carreras@xrce.xerox.com
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995971875">
We present a transition-based arc-eager
model to parse spinal trees, a dependency-
based representation that includes phrase-
structure information in the form of con-
stituent spines assigned to tokens. As a
main advantage, the arc-eager model can
use a rich set of features combining depen-
dency and constituent information, while
parsing in linear time. We describe a set
of conditions for the arc-eager system to
produce valid spinal structures. In experi-
ments using beam search we show that the
model obtains a good trade-off between
speed and accuracy, and yields state of the
art performance for both dependency and
constituent parsing measures.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963112903226">
There are two main representations of the syntac-
tic structure of sentences, namely constituent and
dependency-based structures. In terms of statisti-
cal modeling, an advantage of dependency repre-
sentations is that they are naturally lexicalized, and
this allows the statistical model to capture a rich
set of lexico-syntactic features. The recent liter-
ature has shown that such lexical features greatly
favor the accuracy of statistical models for pars-
ing (Collins, 1999; Nivre, 2003; McDonald et al.,
2005). Constituent structure, on the other hand,
might still provide valuable syntactic information
that is not captured by standard dependencies.
In this work we investigate transition-based sta-
tistical models that produce spinal trees, a rep-
resentation that combines dependency and con-
stituent structures. Statistical models that use both
representations jointly were pioneered by Collins
(1999), who used constituent trees annotated with
head-child information in order to define lexical-
ized PCFG models, i.e. extensions of classic
constituent-based PCFG that make a central use
of lexical dependencies.
An alternative approach is to view the com-
bined representation as a dependency structure
augmented with constituent information. This ap-
proach was first explored by Collins (1996), who
defined a dependency-based probabilistic model
that associates a triple of constituents with each
dependency. In our case, we follow the representa-
tions proposed by Carreras et al. (2008), which we
call spinal trees. In a spinal tree (see Figure 1 for
an example), each token is associated with a spine
of constituents, and head-modifier dependencies
are attached to nodes in the spine, thus combin-
ing the two sources of information in a tight man-
ner. Since spinal trees are inherently dependency-
based, it is possible to extend dependency mod-
els for such representations, as shown by Carreras
et al. (2008) using a so-called graph-based model.
The main advantage of such models is that they
allow a large family of rich features that include
dependency features, constituent features and con-
junctions of the two. However, the consequence
is that the additional spinal structure greatly in-
creases the number of dependency relations. Even
though a graph-based model remains parseable in
cubic time, it is impractical unless some pruning
strategy is used (Carreras et al., 2008).
In this paper we propose a transition-based
parser for spinal parsing, based on the arc-eager
strategy by Nivre (2003). Since transition-based
parsers run in linear time, our aim is to speed
up spinal parsing while taking advantage of the
rich representation it provides. Thus, the re-
search question underlying this paper is whether
we can accurately learn to take greedy parsing
decisions for rich but complex structures such as
spinal trees. To control the trade-off, we use
beam search for transition-based parsing, which
has been shown to be successful (Zhang and Clark,
2011b). The main contributions of this paper are
</bodyText>
<page confidence="0.977494">
289
</page>
<note confidence="0.9772765">
Proceedings of the 19th Conference on Computational Language Learning, pages 289–299,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<listItem confidence="0.955688333333333">
the following:
• We define an arc-eager statistical model for
spinal parsing that is based on the triplet re-
lations by Collins (1996). Such relations, in
conjunction with the partial spinal structure
available in the stack of the parser, provide a
very rich set of features.
• We describe a set of conditions that an arc-
eager strategy must guarantee in order to pro-
duce valid spinal structures.
• In experiments using beam search we show
that our method obtains a good trade-
off between speed and accuracy for both
dependency-based attachment scores and
constituent measures.
</listItem>
<sectionHeader confidence="0.984862" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.871543">
2.1 Spinal Trees
</subsectionHeader>
<bodyText confidence="0.996690660377359">
A spinal tree is a generalization of a dependency
tree that adds constituent structure to the depen-
dencies in the form of spines. In this section we
describe the spinal trees used by Carreras et al.
(2008). A spine is a sequence of constituent nodes
associated with a word in the sentence. From a
linguistic perspective, a spine corresponds to the
projection of the word in the constituent tree. In
other words, the spine of a word consists of the
constituents whose head is the word. See Figure
1 for an example of a sentence and its constituent
and spinal trees. In the example the spine of each
token is the vertical sequence on top of it.
Formally a spinal tree for a sentence x1:n is a
pair (V, E), where V is a sequence of n spinal
nodes and E is a set of n spinal dependencies. The
i-th node in V is a pair (xi, Qi), where xi is the i-th
word of the sentence and Qi is its spine.
A spine Q is a vertical sequence of constituent
nodes. We denote by N the set of constituent
nodes, and we use * V N to denote a special ter-
minal node. We denote by l(Q) the length of a
spine. A spine Q is always non-empty, l(Q) &gt; 1,
its first node is always *, and for any 2 &lt; j &lt; l(Q)
the j-th node of the spine is an element of N.
A spinal dependency is a tuple (h, d, p) that rep-
resents a directed dependency from the p-th node
of Qh to the d-th node of V . Thus, a spinal de-
pendency is a regular dependency between a head
token h and a dependent token d augmented with
a position p in the head spine. It must be that
1 &lt; h, d &lt; n and that 1 &lt;p&lt;l(Qh).
The set of spinal dependencies E satisfies the
standard conditions of forming a rooted directed
projected tree (K¨ubler et al., 2009). Plus, E satis-
fies that the dependencies are correctly nested with
respect to the constituent structure that the spines
represent. Formally, let (h, d1, p1) and (h, d2, p2)
be two spinal dependencies associated with the
same head h. For left dependencies, correct nest-
ing means that if d1 &lt; d2 &lt; h then p1 &gt; p2. For
right dependents, if h &lt; d1 &lt; d2 then p1 &lt; p2.
In practice, it is straightforward to obtain spinal
trees from a treebank of constituent trees with
head-child annotations in each constituent (Car-
reras et al., 2008): starting from a token, its spine
consists of the non-terminal labels of the con-
stituents whose head is the token; the parent node
of the top of the spine gives information about the
lexical head (by following the head children of the
parent) and the position where the spine attaches
to. Given a spinal tree it is trivial to recover the
constituent and dependency trees.
</bodyText>
<subsectionHeader confidence="0.999327">
2.2 Arc-Eager Transition-Based Parsing
</subsectionHeader>
<bodyText confidence="0.999920538461538">
The arc-eager transition-based parser (Nivre,
2003) parses a sentence from left to right in linear
time. It makes use of a stack that stores tokens that
are already processed (partially built dependency
structures) and it chooses the highest-scoring pars-
ing action at each point. The arc-eager algorithm
adds every arc at the earliest possible opportunity
and it can only parse projective trees.
The training process is performed with an ora-
cle (a set of transitions to a parse for a given sen-
tence, (see Figure 2)) and it learns the best transi-
tion given a configuration. The SHIFT transition
removes the first node from the buffer and puts
it on the stack. The REDUCE transition removes
the top node from the stack. The LEFT-ARCt tran-
sition introduces a labeled dependency edge be-
tween the first element of the buffer and the top
element of the stack with the label t. The top el-
ement is removed from the stack (reduce transi-
tion). The RIGHT-ARCt transition introduces a la-
beled dependency edge between the top element of
the stack and the first element in the buffer with a
label d, and it performs a shift transition. Each ac-
tion can have constraints (Nivre et al., 2014), Fig-
ure 2 and Section 3.2 describe the constraints of
the spinal parser.
</bodyText>
<page confidence="0.990268">
290
</page>
<figure confidence="0.99989405">
(a) — Constituent Tree with head-children annotations
NP VP .
DT
This
NN
market
VBN
has
VP
.
VBN VP
been
very
RB
ADVP
ADV
badly
VBN
damaged
(b) — Spinal Tree
</figure>
<figureCaption confidence="0.9981605">
Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the
underlined child annotates the head child of the constituent. (b) The corresponding spinal tree.
</figureCaption>
<figure confidence="0.994455">
*
This
*
market
*
has
*
been
*
very
*
badly
*
damaged
*
.
S
ADVP
VP
NP
VP
VP
</figure>
<bodyText confidence="0.9995901">
In this paper, we took the already existent im-
plementation of arc-eager from ZPar1 (Zhang and
Clark, 2009) which is a beam-search parser imple-
mented in C++ focused on efficiency. ZPar gives
competitive accuracies, yielding state-of-the-art
results, and very fast parsing speeds for depen-
dency parsing. In the case of ZPar, the parsing
process starts with a root node at the top of the
stack (see Figure 3) and the buffer contains the
words/tokens to be parsed.
</bodyText>
<sectionHeader confidence="0.955558" genericHeader="method">
3 Transition-based Spinal Parsing
</sectionHeader>
<bodyText confidence="0.9999835">
In this section we describe an arc-eager transition
system that produces spinal trees. Figure 3 shows
a parsing example. In essence, the strategy we
propose builds the spine of a token by pieces, by
adding a piece of spine each time the parser pro-
duces a dependency involving such token.
We first describe a labeling of dependencies that
encodes a triplet of constituent labels, and it is the
basis for defining an arc-eager statistical model.
Then we describe a set of constraints that guaran-
</bodyText>
<footnote confidence="0.905876">
1http://sourceforge.net/projects/zpar/
</footnote>
<bodyText confidence="0.999929333333333">
tees that the arc-eager derivations we produce cor-
respond to spinal trees. Finally we discuss how to
map arc-eager derivations to spinal trees.
</bodyText>
<subsectionHeader confidence="0.999679">
3.1 Constituent Triplets
</subsectionHeader>
<bodyText confidence="0.922349">
We follow Collins (1996) and define a labeling for
dependencies based on constituent triplets.
Consider a spinal tree (V, E) for a sentence
x1:n. A constituent triplet of a spinal dependency
(h, d, p) E E is a tuple (a, b, c) where:
</bodyText>
<listItem confidence="0.998917666666667">
• a E N is the node at position p of Qh (parent
label)
• b E N U {*} is the node at position p − 1 of
Qh (head label)
• c E N U{*} is the top node of Qd (dependent
label)
</listItem>
<bodyText confidence="0.978531666666667">
For example, a dependency labeled with
(S, VP, NP) is a subject relation, while the triplet
(VP, *, NP) represents an object relation. Note
that a constituent triplet, in essence, corresponds
to a context-free production in a head-driven
PCFG (i.e. a → bc, where b is the head child of a).
</bodyText>
<page confidence="0.973175">
291
</page>
<bodyText confidence="0.739216">
Initial configuration Ci = ([ ], [xi ... xn], 0, )
Terminal configuration Cf E {C  |C = (E, [ ], A)}
</bodyText>
<equation confidence="0.976289777777778">
SHIFT (E, i|B, A) ==&gt;- (E|i, B, A)
REDUCE (E|i, B, A) ==&gt;- (E, B, A)
if∃j,t:{jt→i}∈A
LEFT-ARC((a, b, c)) (E|i, j|B, A) ==&gt;- (E, j|B, A U {j (a,b,c)
� i})
if ¬∃k, t: {i →t k} ∈ A
(1) if (c =6 *) ∨ ¬(∃k: {i t→ k} ∈ A)
ha0 ,b0,c0i
(3) if b = * ⇒ ∀{i → k} ∈ A, a = a0 ∧ b = b0
RIGHT-ARC((a, b, c)) (E|i, j|B, A) ==&gt;- (E|i|j, B, A U {i (a,b,c)
� j})
(1) if (c =6 *) ∨ ¬(∃k: {j →t k} ∈ A)
(2) if ¬(∃k, ha0, b0, c0i : {k ha0,b0,c0i
→ i} ∈ A ∧ c0 = * )
(4) if b = * ⇒ ∀{j ha0,b0,c0i
→ k} ∈ A such that j &lt; k, a = a0 ∧ b = b0
(5) if b = * ⇒ ∀{j ha0,b0,c0i
→ k} ∈ A such that k &lt; j ∧ b0 = *, a = a0
</equation>
<figureCaption confidence="0.902342">
Figure 2: Arc-eager transition system with spinal constraints. E represents the stack, B represents the
</figureCaption>
<bodyText confidence="0.940806285714286">
buffer, A represents the set of arcs, t represents a given triplet when its components are not relevant,
(a, b, c) represents a given triplet when its components are relevant and i, j and k represent tokens of the
sentence. The constraints labeled with (1) ... (5) are described in Section 3.2. The constraints that are
not labeled are standard constraints of the arc-eager parsing algorithm (Nivre, 2003).
In the literature, these triplets have been shown to
provide very rich parameterizations of statistical
models for parsing (Collins, 1996; Collins, 1999;
Carreras et al., 2008).
For our purposes, we associate with each spinal
dependency (h, d, p) E E a triplet dependency
(h, d, (a, b, c)), where the triplet is defined as
above. We then define a standard statistical model
for arc-eager parsing that uses constituent triplets
as dependency labels. An important advantage of
this model is that left-arc and right-arc transitions
can have feature descriptions that combine stan-
dard dependency features with phrase-structure in-
formation in the form of constituent triplets. As
shown by Carreras et al. (2008), this rich set of
features can obtain significant gains in parsing ac-
curacy.
</bodyText>
<subsectionHeader confidence="0.99991">
3.2 Spinal Arc-Eager Constraints
</subsectionHeader>
<bodyText confidence="0.9995075">
We now describe constraints that guarantee that
any derivation produced by a triplet-based arc-
eager model corresponds to a spinal structure.
Let us make explicit some properties that relate
a derivation D with a token i, the arcs in D involv-
ing i, and its spine σi:
</bodyText>
<listItem confidence="0.513932055555556">
• D has at most a single arc (h, i, (a, b, c))
where i is in the dependent position. The de-
pendent label c of this triplet defines the top
of σi. If c = * then σi = , and i can not
have dependants.
• Consider the subsequence of D of left arcs
with head i, of the form (i, j, (a, b, c)). In an
arc-eager derivation this subsequence follows
a head-outwards order. Each of these arcs has
in its triplet a pair of contiguous nodes b−a of
σi. We call such pairs spinal edges. The sub-
sequence of spinal edges is ordered bottom-
up, because arcs appear head-outwards. In
addition, sibling arcs may attach to the same
position in σi. Thus, the subsequence of left
spinal edges of i in D is a subsequence with
repeats of the sequence of edges of σi.
• Analogously, the subsequence of right spinal
</listItem>
<page confidence="0.983965">
292
</page>
<table confidence="0.970116">
Transition Stack Buffer Added Arc
SHIFT [Root] [This, market, has, been, very, badly, damaged,.] (NP,?,?)
L-A(hNP, *, *i) [Root, This] [market, has, been, very, badly, damaged,.] market −→ This
SHIFT [Root] [market, has, been, very, badly, damaged, .] (S,VP,NP)
L-A(hS, VP, NPi) [Root, market] [has, been, very, badly, damaged,.] has −→ market
R-A(hTOP, *, Si) [Root] [has, been, very, badly, damaged, .] Root (TOP,?,S)
R-A(hVP, *, VPi) [Root, has] [been, very, badly, damaged, .] −→ has
SHIFT [Root, has, been] [very, badly, damaged, .] has (VP,?,VP)
R-A(hADVP, *, *i) [Root, has,been, very] [badly, damaged, .] −→ been
REDUCE [Root, has, been, very, badly] [damaged, .] very (ADVP,?,?)
L-A(hVP, *, ADVPi) [Root, has, been, very] [damaged, .] −→ badly
R-A(hVP, *, VPi) [Root, has, been] [damaged, .] damaged (VP,?,ADVP)
REDUCE [Root, has, been, damaged] [.] −→ very
REDUCE [Root, has, been] [.] (VP,?,VP)
R-A(hS, VP, *i) [Root, has] [.] been −→ damaged
[Root, has, .] [ ] (S,VP,?)
has −→ .
</table>
<figureCaption confidence="0.996272">
Figure 3: Transition sequence for This market has been very badly damaged.
</figureCaption>
<bodyText confidence="0.998891">
edges of i in D is a subsequence with repeats
of the edges of QZ. In D, right spinal edges
appear after left spinal edges.
We constrain the arc-eager transition process
such that these properties hold. Recall that a well-
formed spine starts with a terminal node *, and so
does the first edge of the spine and only the first.
Let C be a configuration, i.e. a partial derivation.
The constraints are:
</bodyText>
<listItem confidence="0.953822">
(1) An arc (h, i, (a, b, *)) is not valid if i has de-
pendents in C.
(2) An arc (i, j, (a, b, c)) is not valid if
C contains a dependency of the form
(h, i, (a&apos;, b&apos;, *)).
(3) A left arc (i, j, (a, *, c)) is only valid if
all sibling left arcs in C are of the form
(i, j&apos;, (a, *, c&apos;)).
(4) Analogous to (3) for right arcs.
(5) If C has a left arc (i, j, (a, *, c)), then a right
arc (i, j&apos;, (a&apos;, *, c)) is not valid if a =� a&apos;.
</listItem>
<bodyText confidence="0.999794947368421">
In essence, constraints 1-2 relate the top of a
spine with the existence of descendants, while
constraints 3-5 enforce that the bottom of the
spine is well formed. We enforce no further con-
straints looking at edges in the middle of the spine.
This means that left and right arc operations can
add spinal edges in a free manner, without ex-
plicitly encoding how these edges relate to each
other. In other words, we rely on the statistical
model to correctly build a spine by adding left and
right spinal edges along the transition process in a
bottom-up fashion.
It is easy to see that these constraints do not pre-
vent the transition process from ending. Specif-
ically, even though the constraints invalidate arc
operations, the arc-eager process can always finish
by leaving tokens in the buffer without any head
assigned, in which case the resulting derivation is
a forest of several projective trees.
</bodyText>
<subsectionHeader confidence="0.998493">
3.3 Mapping Derivations to Spinal Trees
</subsectionHeader>
<bodyText confidence="0.99968147368421">
The constrained arc-eager derivations correspond
to spinal structures, but not necessarily to sin-
gle spinal trees, for two reasons. First, from the
derivation we can extract two subsequences of left
and right spinal edges, but the derivation does not
encode how these sequences should merge into a
spine. Second, as in the basic arc-eager process,
the derivation might be a forest rather than a single
tree. Next we describe processes to turn a spinal
arc-eager derivation into a tree.
Forming spines. For each token i we depart
from the top of the spine t, a sequence L of left
spinal edges, and a sequence R of right spinal
edges. The goal is to form a spine QZ, such that
its top is t, and that L and R are subsequences
with repeats of the edges of QZ. We look for the
shortest spine satisfying these properties. For ex-
ample, consider the derivation in Figure 3 and the
third token has:
</bodyText>
<listItem confidence="0.999955666666667">
• Top t: S
• Left edges L: VP − S
• Right edges R: *−VP, VP − S
</listItem>
<page confidence="0.995786">
293
</page>
<bodyText confidence="0.998816">
In this case the shortest spine that is consistent
with the edges and the top is *−V P − S. Our
method runs in two steps:
</bodyText>
<listItem confidence="0.990855785714286">
1. Collapse. Traverse each sequence of edges
and replace any contiguous subsequence of
identical edges by a single occurrence. The
assumption is that identical contiguous edges
correspond to sibling dependencies that at-
tach to the same node in the spine.2
2. Merge the left L and right R sequences of
edges overlapping them as much as possible,
i.e. looking for the shortest spine. We do this
in O(nm), where n and m are the lengths of
the two sequences. Whenever multiple short-
est spines are compatible with the left and
right edge sequences, we give preference to
the spine that places left edges to the bottom.
</listItem>
<bodyText confidence="0.999946375">
The result of this process is a spine ai with left and
right dependents attached to positions of the spine.
Note that this strategy has some limitations: (a)
it can not recover non-terminal spinal nodes that
do not participate in any triplet; and (b) it flattens
spinal structures that involve contiguous identical
spinal edges. 3
Rooting Forests. The arc-eager transition sys-
tem is not guaranteed to generate a single root
in a derivation (though see (Nivre and Fern´andez-
Gonz´alez, 2014) for a solution). Thus, after map-
ping a derivation to a spinal structure, we might
get a forest of projective spinal trees. In this case,
to produce a constituent tree from the spinal for-
est, we promote the last tree and place the rest of
trees as children of its top node.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9608126">
In this section we describe the performance of the
transition-based spinal parser by running it with
different sizes of the beam and by comparing it
2However, this is not always the case. For example, in the
Penn Treebank adjuncts create an additional constituent level
in the verb-phrase structure, and this can result in a series
of contiguous VP spinal nodes. The effect of flattening such
structures is mild, see below.
3These limitations have relatively mild effects on recov-
ering constituent trees in the style of the Penn Treebank. To
measure the effect, we took the correct spinal trees of the
development section and mapped them to the corresponding
arc-eager derivation. Then we mapped the derivation back to
a spinal tree using this process and recovered the constituent
tree. This process obtained 98.4% of bracketing recall, 99.5%
of bracketing precision, and 99.0 of Fl measure.
with the state-of-the-art. We used the ZPar imple-
mentation modified to incorporate the constraints
for spinal arc-eager parsing. We used the exact
same features as Zhang and Nivre (2011), which
extract a rich set of features that encode higher-
order interactions betwen the current action and
elements of the stack. Since our dependency la-
bels are constituent triplets, these features encode
a mix of constituent and dependency structure.
</bodyText>
<subsectionHeader confidence="0.985568">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999931428571429">
We use the WSJ portion of the Penn Treebank4,
augmented with head-dependant information us-
ing the rules of Yamada and Matsumoto (2003).
This results in a total of 974 different constituent
triplets, which we use as dependency labels in the
spinal arc-eager model. We use predicted part-of-
speech tags5.
</bodyText>
<subsectionHeader confidence="0.672478">
4.2 Results in the Development Set
</subsectionHeader>
<bodyText confidence="0.99998568">
In Table 1 we show the results of our parser for
the dependency trees, the table shows unlabeled
attachment score (UAS) , triplet accuracy (TA,
which would be label accuracy, LA) and triplet at-
tachment score (TAS), and spinal accuracy (SA)
(the spinal accuracy is the percentage of complete
spines that the parser correctly predicts). In or-
der to be fully comparable, for the dependency-
based metrics we report results including and ex-
cluding punctuation symbols for evaluation. The
table also shows the speed (sentences per second)
in standard hardware. We trained the parser with
different beam values, we run a number of itera-
tions until the model converges and we report the
results of the best iteration.
As it can be observed the best model is the one
trained with beam size 64, and greater sizes of the
beam help to improve the results. Nonetheless, it
also makes the parser slower. This result is ex-
pected since the number of dependency labels, i.e.
triplets, is 974 so a higher size of the beam al-
lows to test more of them when new actions are in-
cluded in the agenda. This model already provides
high results over 92.34% UAS and it can also pre-
dict most of the triplets that label the dependency
</bodyText>
<footnote confidence="0.986524714285714">
4We use the standard partition: sections 02-21 for train-
ing, section 22 for development, and section 23 for testing.
5We use the same setting as in (Carreras et al., 2008) by
training over a treebank with predicted part-of-speech tags
with mxpost (Ratnaparkhi, 1996) (accuracy: 96.5) and we
test on the development set and test set with predicted part-
of-speech tags of Collins (1997) (accuracy: 96.8).
</footnote>
<page confidence="0.992033">
294
</page>
<table confidence="0.998680333333333">
Dep. (incl punct) Dep. (excl punct) Const Speed
Beam-size UAS TA TAS SA UAS TA TAS LR LP F1 Sent/Sec
8 91.39 90.47 88.78 95.60 92.32 91.21 89.73 88.6 88.4 88.5 7.8
16 91.81 90.95 89.28 95.84 92.70 91.65 90.21 89.0 89.1 89.1 3.9
32 92.08 91.14 89.52 95.96 92.91 91.77 90.38 89.4 89.5 89.5 1.7
64 92.34 91.45 89.84 96.13 93.12 92.04 90.65 89.5 89.7 89.7 0.8
</table>
<tableCaption confidence="0.999242">
Table 1: UAS with predicted part-of-speech tags for the dev.set including and excluding punctuation
</tableCaption>
<bodyText confidence="0.980902">
symbols. Constituent results for the development set. Parsing speed in sentences per second (an estimate
that varies depending on the machine). TA and TAS refer to label accuracy and labeled attachment score
where the labels are the different constituent triplets described in Section 3. SA is the spinal accuracy.
arcs (91.45 TA and 89.84 TAS) (including punctu-
ation symbols for evaluation).
Table 1 also shows the results of the parser in
the development set after transforming the depen-
dency trees by following the method described in
Section 3. The result even surpasses 89.5% F1
which is a competitive accuracy. As we can see,
the parser also provides a good trade-off between
parsing speed and accuracy.6
In order to test whether the number of depen-
dency labels is an issue for the parser, we also
trained a model on dependency trees labeled with
Yamada and Matsumoto (2003) rules, and the re-
sults are comparable to ours. For a beam of size
64, the best model with dependency labels pro-
vides 92.3% UAS for the development set includ-
ing punctuation and 93.0% excluding punctuation,
while our spinal parser for the same beam size
provides 92.3% UAS including punctuation and
93.1% excluding punctuation. This means that the
beam-search arc-eager parser is capable of coping
with the dependency triplets, since it even pro-
vides slightly better results for unlabeled attach-
ment scores. However, unlike (Carreras et al.,
2008), the arc-eager parser does not substantially
benefit of using the triplets during training.
</bodyText>
<subsectionHeader confidence="0.990214">
4.3 Final Results and State-of-the-art
Comparison
</subsectionHeader>
<bodyText confidence="0.997477833333333">
Our best model (obtained with beam=64) provides
92.14 UAS, 90.91 TA and 89.32 TAS in the test
set including punctuation and 92.78 UAS, 91.53
6However, in absolute terms, our running times are slower
than typical shift-reduce parsers. Our purpose is to show a re-
lation between speed and accuracy, and we opted for a simple
implementation rather than an engineered one. As one exam-
ple, our parser considers all dependency triplets (974) in all
cases, which is somehow absurd since most of these can be
ruled out given the parts-of-speech of the candidate depen-
dency. Incorporating a filtering strategy of this kind would
result in a speedup factor constant to all beam sizes.
</bodyText>
<table confidence="0.995563928571429">
Parser UAS
McDonald et al. (2005) 90.9
McDonald and Pereira (2006) 91.5
Huang and Sagae (2010) 92.1
Zhang and Nivre (2011) 92.9
Koo and Collins (2010)* 93.0
Bohnet and Nivre (2012) 93.0
Koo et al. (2008) †* 93.2
Martins et al. (2010) 93.3
Ballesteros and Bohnet (2014) 93.5
Carreras et al. (2008) †* 93.5
Suzuki et al. (2009) †* 93.8
this work (beam 64) †* 92.1
this work (beam 64) † 92.8
</table>
<tableCaption confidence="0.611423">
Table 2: State-of-the-art comparison for unlabeled
attachment score for WSJ-PTB with Y&amp;M rules.
Results marked with † use other kind of infor-
mation, and are not directly comparable. Results
marked with * include punctuation for evaluation.
</tableCaption>
<bodyText confidence="0.9997847">
TA and 90.11 TAS excluding punctuation. Table
2 compares our results with the state-of-the-art.
Our model obtains comptetitive dependency accu-
racies when compared to other systems.
In terms of constituent structure, our best model
(beam=64) obtains 88.74 LR, 89.21 LP and 88.97
F1. Table 3 compares our model with other con-
stituent parsers, including shift-reduce parsers as
ours. Our best model is competitive compared
with the rest.
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999799333333333">
Collins (1996) defined a statistical model for
dependency parsing based on using constituent
triplets in the labels, which forms the basis of our
arc-eager model. In that work, a chart-based al-
gorithm was used for parsing, while here we use
greedy transition-based parsing.
</bodyText>
<page confidence="0.99492">
295
</page>
<table confidence="0.9999460625">
Beam-size LR LP F1
Sagae and Lavie (2005)* 86.1 86.0 86.0
Ratnaparkhi (1999) 86.3 87.5 86.9
Sagae and Lavie (2006)* 87.8 88.1 87.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.5 89.9 89.5
Zhang and Clark (2009)* 90.0 89.9 89.9
Petrov and Klein (2007) 90.1 90.2 90.1
Zhu et al. (2013)-1* 90.2 90.7 90.4
Carreras et al. (2008) 90.7 91.4 91.1
Zhu et al. (2013)-2†* 91.1 91.5 91.3
Huang (2008) 91.2 91.8 91.5
Charniak (2000) 91.2 91.8 91.5
Huang et al. (2010) 91.2 91.8 91.5
McClosky et al. (2006) 91.2 91.8 91.5
this work (beam 64)* 88.7 89.2 89.0
</table>
<tableCaption confidence="0.79358">
Table 3: State-of-the-art comparison in the test
set for phrase structure parsing. Results marked
</tableCaption>
<bodyText confidence="0.995744253164558">
with † use additional information, such as semi-
supervised models, and are not directly compara-
ble to the others. Results marked with * are shift-
reduce parsers.
Carreras et al. (2008) was the first to use spinal
representations to define an arc-factored depen-
dency parsing model based on the Eisner algo-
rithm, that parses in cubic time. Our work can
be seen as the transition-based counterpart of that,
with a greedy parsing strategy that runs in linear
time. Because of the extra complexity of spinal
structures, they used three probabilistic non-spinal
dependency models to prune the search space of
the spinal model. In our work, we show that a sin-
gle arc-eager model can obtain very competitive
results, even though the accuracies of our model
are lower than theirs.
In terms of parsing spinal structures, Rush et al.
(2010) introduced a dual decomposition method
that uses constituent and dependency parsing rou-
tines to parse a combined spinal structure.
In a similar style to our method Hall et al.
(2007), Hall and Nivre (2008) and Hall (2008)
introduced an approach for parsing Swedish and
German, in which MaltParser (Nivre et al., 2007)
is used to predict dependency trees, whose depen-
dency labels are enriched with constituency labels.
They used tuples that encode dependency labels,
constituent labels, head relations and the attach-
ment. The last step is to make the inverse transfor-
mation from a dependency graph to a constituent
structure.
Recently Kong et al. (2015) proposed a struc-
tured prediction model for mapping dependency
trees to constituent trees, using the CKY algo-
rithm. They assume a fixed dependency tree used
as a hard constraint. Also recently, Fern´andez-
Gonz´alez and Martins (2015) proposed an arc-
factored dependency model for constituent pars-
ing. In that work dependency labels encode the
constituent node where the dependency arises as
well as the position index of that node in the head
spine. In contrast, we use constituent triplets as
dependency labels.
Our method is based on constraining a shift-
reduce parser using the arc-eager strategy. Nivre
(2003) and Nivre (2004) establish the basis for
arc-eager algorithm and arc-standard parsing algo-
rithms, which are central to most recent transition-
based parsers (Zhang and Clark, 2011b; Zhang
and Nivre, 2011; Bohnet and Nivre, 2012). These
parsers are very fast, because the number of pars-
ing actions is linear in the length of the sentence,
and they obtain state-of-the-art-performance, as
shown in Section 4.3.
For shift-reduce constituent parsing, Sagae
and Lavie (2005; 2006) presented a shift-reduce
phrase structure parser. The main difference to
ours is that their models do not use lexical de-
pendencies. Zhang and Clark (2011a) presented
a shift-reduce parser based on CCG, and as such
is lexicalized. Both spinal and CCG represen-
tations are very expressive. One difference is
that spinal trees can be directly obtained from
constituent treebanks with head-child information,
while CCG derivations are harder to obtain.
More recently, Zhang and Clark (2009) and the
subsequent work of Zhu et al. (2013) described
a beam-search shift-reduce parsers obtaining very
high results. These models use dependency in-
formation via stacking, by running a dependency
parser as a preprocess. In the literature, stacking
is a common technique to improve accuracies by
combining dependency and constituent informa-
tion, in both ways (Wang and Zong, 2011; Farkas
and Bohnet, 2012). Our model differs from stack-
ing approaches in that it natively produces the two
structures jointly, in such a way that a rich set of
features is available.
</bodyText>
<page confidence="0.997896">
296
</page>
<sectionHeader confidence="0.986805" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987461538462">
There are several lessons to learn from this paper.
First, we show that a simple modification to the
arc-eager strategy results in a competitive greedy
spinal parser which is capable of predicting depen-
dency and constituent structure jointly. In order
to make it work, we introduce simple constraints
to the arc-eager strategy that ensure well-formed
spinal derivations. Second, by doing this, we are
providing a good trade-off between speed and ac-
curacy, while at the same time we are providing
a dependency structure which can be really useful
for downstream applications. Even if the depen-
dency model needs to cope with a huge amount
of dependency labels (in the form of constituent
triplets), the unlabeled attachment accuracy does
not drop and the labeling accuracy (for the triplets)
is good enough for getting a good phrase-structure
parse. Overall, our work shows that greedy strate-
gies to dependency parsing can be successfuly
augmented to include constituent structure.
In the future, we plan to explore spinal deriva-
tions in new transition-based dependency parsers
(Chen and Manning, 2014; Dyer et al., 2015;
Weiss et al., 2015; Zhou et al., 2015). This would
allow to explore the spinal derivations in new ways
and to test their potentialities.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9987165">
We thank Noah A. Smith and the anonymous re-
viewers for their very useful comments. Miguel
Ballesteros is supported by the European Com-
mission under the contract numbers FP7-ICT-
610411 (project MULTISENSOR) and H2020-
RIA-645012 (project KRISTINA).
</bodyText>
<sectionHeader confidence="0.998401" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998628485294117">
Miguel Ballesteros and Bernd Bohnet. 2014. Au-
tomatic feature selection for agenda-based depen-
dency parsing. In Proc. COLING.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455–1465, Jeju Island, Korea, July. Association for
Computational Linguistics.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, Dynamic Programming, and the Per-
ceptron for Efficient, Feature-Rich Parsing. In
CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning,
pages 9–16, Manchester, England, August. Coling
2008 Organizing Committee.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
132–139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. EMNLP.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 184–191,
Santa Cruz, California, USA, June. Association for
Computational Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
eighth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 16–23.
Association for Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of ACL.
Rich´ard Farkas and Bernd Bohnet. 2012. Stacking of
dependency and phrase structure parsers. In COL-
ING, pages 849–866.
Daniel Fern´andez-Gonz´alez and Andr´e F. T. Martins.
2015. Parsing as reduction. CoRR, abs/1503.00030.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for german dependency and con-
stituency representations. In Proceedings of the
Workshop on Parsing German, PaGe ’08, pages 47–
54, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2007.
A Hybrid Constituency-Dependency Parser for
Swedish. In Proceedings of the 16th Nordic Con-
ference of Computational Linguistics (NODALIDA).
Johan Hall. 2008. Transition-based natural language
parsing with dependency and constituency represen-
tations. Master’s thesis, V¨axj¨o University.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1077–1086.
</reference>
<page confidence="0.984395">
297
</page>
<reference confidence="0.998967740740741">
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent vari-
able grammars. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 12–22. Association for Computa-
tional Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586–
594.
Lingpeng Kong, Alexander M. Rush, and Noah A.
Smith. 2015. Transforming dependencies into
phrase structures. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics – Human Lan-
guage Technologies. Association for Computational
Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 1–11.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 595–603.
Sandra Kubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: De-
pendency parsing by approximate variational infer-
ence. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 34–44.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing.
In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ’06, pages 152–
159, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 91–98.
Joakim Nivre and Daniel Fern´andez-Gonz´alez. 2014.
Arc-eager parsing with the tree constraint. Compu-
tational Linguistics, 40(2):259–267.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Guls¸en Eryiˇgit, Sandra Kubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95–135.
Joakim Nivre, Yoav Goldberg, and Ryan T. McDon-
ald. 2014. Constrained arc-eager dependency pars-
ing. Computational Linguistics, 40(2):249–527.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149–160.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together (ACL), pages 50–57.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In HLT-NAACL, vol-
ume 7, pages 404–411.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151–175.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11. Association for Computa-
tional Linguistics.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, Parsing ’05, pages 125–132, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kenji Sagae and Alon Lavie. 2006. Parser combi-
nation by reparsing. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL-Short
’06, pages 129–132, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of EMNLP, pages
551–560.
Zhiguo Wang and Chengqing Zong. 2011. Parse
reranking based on higher-order lexical dependen-
cies. In IJCNLP, pages 1251–1259.
</reference>
<page confidence="0.968215">
298
</page>
<reference confidence="0.99974782051282">
David Weiss, Christopher Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neural
network transition-based parsing. In Proc. ACL.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195–206.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162–171. Association for Computational Linguis-
tics.
Yue Zhang and Stephen Clark. 2011a. Shift-reduce
ccg parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 683–692. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011b. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
188–193, Portland, Oregon, USA.
Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A Neural Probabilistic Structured-
Prediction Model for Transition-Based Dependency
Parsing. In ACL.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434–443. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.998653">
299
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934783">
<title confidence="0.999486">Transition-based Spinal Parsing</title>
<author confidence="0.987437">Xavier</author>
<affiliation confidence="0.9735">Group, Pompeu Fabra University Mellon Research Centre Europe</affiliation>
<email confidence="0.995921">miguel.ballesteros@upf.eduxavier.carreras@xrce.xerox.com</email>
<abstract confidence="0.999738823529412">We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and yields state of the art performance for both dependency and constituent parsing measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Bernd Bohnet</author>
</authors>
<title>Automatic feature selection for agenda-based dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="25496" citStr="Ballesteros and Bohnet (2014)" startWordPosition="4447" endWordPosition="4450">a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptetitive dependency accuracies when compared to other systems. In terms of constituent structure, our best model (beam=64</context>
</contexts>
<marker>Ballesteros, Bohnet, 2014</marker>
<rawString>Miguel Ballesteros and Bernd Bohnet. 2014. Automatic feature selection for agenda-based dependency parsing. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1455--1465</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="25408" citStr="Bohnet and Nivre (2012)" startWordPosition="4431" endWordPosition="4434">s. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptetitive dependency accuracies wh</context>
<context position="29549" citStr="Bohnet and Nivre, 2012" startWordPosition="5111" endWordPosition="5114">and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can be directly obtained from</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1455–1465, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-Rich Parsing.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>9--16</pages>
<location>Manchester, England,</location>
<contexts>
<context position="2373" citStr="Carreras et al. (2008)" startWordPosition="339" endWordPosition="342">jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic constituent-based PCFG that make a central use of lexical dependencies. An alternative approach is to view the combined representation as a dependency structure augmented with constituent information. This approach was first explored by Collins (1996), who defined a dependency-based probabilistic model that associates a triple of constituents with each dependency. In our case, we follow the representations proposed by Carreras et al. (2008), which we call spinal trees. In a spinal tree (see Figure 1 for an example), each token is associated with a spine of constituents, and head-modifier dependencies are attached to nodes in the spine, thus combining the two sources of information in a tight manner. Since spinal trees are inherently dependencybased, it is possible to extend dependency models for such representations, as shown by Carreras et al. (2008) using a so-called graph-based model. The main advantage of such models is that they allow a large family of rich features that include dependency features, constituent features and</context>
<context position="4870" citStr="Carreras et al. (2008)" startWordPosition="744" endWordPosition="747">l structure available in the stack of the parser, provide a very rich set of features. • We describe a set of conditions that an arceager strategy must guarantee in order to produce valid spinal structures. • In experiments using beam search we show that our method obtains a good tradeoff between speed and accuracy for both dependency-based attachment scores and constituent measures. 2 Background 2.1 Spinal Trees A spinal tree is a generalization of a dependency tree that adds constituent structure to the dependencies in the form of spines. In this section we describe the spinal trees used by Carreras et al. (2008). A spine is a sequence of constituent nodes associated with a word in the sentence. From a linguistic perspective, a spine corresponds to the projection of the word in the constituent tree. In other words, the spine of a word consists of the constituents whose head is the word. See Figure 1 for an example of a sentence and its constituent and spinal trees. In the example the spine of each token is the vertical sequence on top of it. Formally a spinal tree for a sentence x1:n is a pair (V, E), where V is a sequence of n spinal nodes and E is a set of n spinal dependencies. The i-th node in V i</context>
<context position="6871" citStr="Carreras et al., 2008" startWordPosition="1140" endWordPosition="1144">s the standard conditions of forming a rooted directed projected tree (K¨ubler et al., 2009). Plus, E satisfies that the dependencies are correctly nested with respect to the constituent structure that the spines represent. Formally, let (h, d1, p1) and (h, d2, p2) be two spinal dependencies associated with the same head h. For left dependencies, correct nesting means that if d1 &lt; d2 &lt; h then p1 &gt; p2. For right dependents, if h &lt; d1 &lt; d2 then p1 &lt; p2. In practice, it is straightforward to obtain spinal trees from a treebank of constituent trees with head-child annotations in each constituent (Carreras et al., 2008): starting from a token, its spine consists of the non-terminal labels of the constituents whose head is the token; the parent node of the top of the spine gives information about the lexical head (by following the head children of the parent) and the position where the spine attaches to. Given a spinal tree it is trivial to recover the constituent and dependency trees. 2.2 Arc-Eager Transition-Based Parsing The arc-eager transition-based parser (Nivre, 2003) parses a sentence from left to right in linear time. It makes use of a stack that stores tokens that are already processed (partially bu</context>
<context position="12242" citStr="Carreras et al., 2008" startWordPosition="2143" endWordPosition="2146">represents the stack, B represents the buffer, A represents the set of arcs, t represents a given triplet when its components are not relevant, (a, b, c) represents a given triplet when its components are relevant and i, j and k represent tokens of the sentence. The constraints labeled with (1) ... (5) are described in Section 3.2. The constraints that are not labeled are standard constraints of the arc-eager parsing algorithm (Nivre, 2003). In the literature, these triplets have been shown to provide very rich parameterizations of statistical models for parsing (Collins, 1996; Collins, 1999; Carreras et al., 2008). For our purposes, we associate with each spinal dependency (h, d, p) E E a triplet dependency (h, d, (a, b, c)), where the triplet is defined as above. We then define a standard statistical model for arc-eager parsing that uses constituent triplets as dependency labels. An important advantage of this model is that left-arc and right-arc transitions can have feature descriptions that combine standard dependency features with phrase-structure information in the form of constituent triplets. As shown by Carreras et al. (2008), this rich set of features can obtain significant gains in parsing ac</context>
<context position="22293" citStr="Carreras et al., 2008" startWordPosition="3916" endWordPosition="3919">one trained with beam size 64, and greater sizes of the beam help to improve the results. Nonetheless, it also makes the parser slower. This result is expected since the number of dependency labels, i.e. triplets, is 974 so a higher size of the beam allows to test more of them when new actions are included in the agenda. This model already provides high results over 92.34% UAS and it can also predict most of the triplets that label the dependency 4We use the standard partition: sections 02-21 for training, section 22 for development, and section 23 for testing. 5We use the same setting as in (Carreras et al., 2008) by training over a treebank with predicted part-of-speech tags with mxpost (Ratnaparkhi, 1996) (accuracy: 96.5) and we test on the development set and test set with predicted partof-speech tags of Collins (1997) (accuracy: 96.8). 294 Dep. (incl punct) Dep. (excl punct) Const Speed Beam-size UAS TA TAS SA UAS TA TAS LR LP F1 Sent/Sec 8 91.39 90.47 88.78 95.60 92.32 91.21 89.73 88.6 88.4 88.5 7.8 16 91.81 90.95 89.28 95.84 92.70 91.65 90.21 89.0 89.1 89.1 3.9 32 92.08 91.14 89.52 95.96 92.91 91.77 90.38 89.4 89.5 89.5 1.7 64 92.34 91.45 89.84 96.13 93.12 92.04 90.65 89.5 89.7 89.7 0.8 Table 1: </context>
<context position="24411" citStr="Carreras et al., 2008" startWordPosition="4268" endWordPosition="4271">rained a model on dependency trees labeled with Yamada and Matsumoto (2003) rules, and the results are comparable to ours. For a beam of size 64, the best model with dependency labels provides 92.3% UAS for the development set including punctuation and 93.0% excluding punctuation, while our spinal parser for the same beam size provides 92.3% UAS including punctuation and 93.1% excluding punctuation. This means that the beam-search arc-eager parser is capable of coping with the dependency triplets, since it even provides slightly better results for unlabeled attachment scores. However, unlike (Carreras et al., 2008), the arc-eager parser does not substantially benefit of using the triplets during training. 4.3 Final Results and State-of-the-art Comparison Our best model (obtained with beam=64) provides 92.14 UAS, 90.91 TA and 89.32 TAS in the test set including punctuation and 92.78 UAS, 91.53 6However, in absolute terms, our running times are slower than typical shift-reduce parsers. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is</context>
<context position="26913" citStr="Carreras et al. (2008)" startWordPosition="4680" endWordPosition="4683"> 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an arc-factored dependency parsing model bas</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-Rich Parsing. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 9–16, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26761" citStr="Charniak (2000)" startWordPosition="4654" endWordPosition="4655"> compares our model with other constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked w</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000, pages 132–139, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Cruz, California, USA,</location>
<contexts>
<context position="2180" citStr="Collins (1996)" startWordPosition="312" endWordPosition="313">te transition-based statistical models that produce spinal trees, a representation that combines dependency and constituent structures. Statistical models that use both representations jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic constituent-based PCFG that make a central use of lexical dependencies. An alternative approach is to view the combined representation as a dependency structure augmented with constituent information. This approach was first explored by Collins (1996), who defined a dependency-based probabilistic model that associates a triple of constituents with each dependency. In our case, we follow the representations proposed by Carreras et al. (2008), which we call spinal trees. In a spinal tree (see Figure 1 for an example), each token is associated with a spine of constituents, and head-modifier dependencies are attached to nodes in the spine, thus combining the two sources of information in a tight manner. Since spinal trees are inherently dependencybased, it is possible to extend dependency models for such representations, as shown by Carreras e</context>
<context position="4193" citStr="Collins (1996)" startWordPosition="631" endWordPosition="632">paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are 289 Proceedings of the 19th Conference on Computational Language Learning, pages 289–299, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics the following: • We define an arc-eager statistical model for spinal parsing that is based on the triplet relations by Collins (1996). Such relations, in conjunction with the partial spinal structure available in the stack of the parser, provide a very rich set of features. • We describe a set of conditions that an arceager strategy must guarantee in order to produce valid spinal structures. • In experiments using beam search we show that our method obtains a good tradeoff between speed and accuracy for both dependency-based attachment scores and constituent measures. 2 Background 2.1 Spinal Trees A spinal tree is a generalization of a dependency tree that adds constituent structure to the dependencies in the form of spines</context>
<context position="10192" citStr="Collins (1996)" startWordPosition="1713" endWordPosition="1714"> example. In essence, the strategy we propose builds the spine of a token by pieces, by adding a piece of spine each time the parser produces a dependency involving such token. We first describe a labeling of dependencies that encodes a triplet of constituent labels, and it is the basis for defining an arc-eager statistical model. Then we describe a set of constraints that guaran1http://sourceforge.net/projects/zpar/ tees that the arc-eager derivations we produce correspond to spinal trees. Finally we discuss how to map arc-eager derivations to spinal trees. 3.1 Constituent Triplets We follow Collins (1996) and define a labeling for dependencies based on constituent triplets. Consider a spinal tree (V, E) for a sentence x1:n. A constituent triplet of a spinal dependency (h, d, p) E E is a tuple (a, b, c) where: • a E N is the node at position p of Qh (parent label) • b E N U {*} is the node at position p − 1 of Qh (head label) • c E N U{*} is the top node of Qd (dependent label) For example, a dependency labeled with (S, VP, NP) is a subject relation, while the triplet (VP, *, NP) represents an object relation. Note that a constituent triplet, in essence, corresponds to a context-free production</context>
<context position="12203" citStr="Collins, 1996" startWordPosition="2139" endWordPosition="2140">em with spinal constraints. E represents the stack, B represents the buffer, A represents the set of arcs, t represents a given triplet when its components are not relevant, (a, b, c) represents a given triplet when its components are relevant and i, j and k represent tokens of the sentence. The constraints labeled with (1) ... (5) are described in Section 3.2. The constraints that are not labeled are standard constraints of the arc-eager parsing algorithm (Nivre, 2003). In the literature, these triplets have been shown to provide very rich parameterizations of statistical models for parsing (Collins, 1996; Collins, 1999; Carreras et al., 2008). For our purposes, we associate with each spinal dependency (h, d, p) E E a triplet dependency (h, d, (a, b, c)), where the triplet is defined as above. We then define a standard statistical model for arc-eager parsing that uses constituent triplets as dependency labels. An important advantage of this model is that left-arc and right-arc transitions can have feature descriptions that combine standard dependency features with phrase-structure information in the form of constituent triplets. As shown by Carreras et al. (2008), this rich set of features can</context>
<context position="26321" citStr="Collins (1996)" startWordPosition="4582" endWordPosition="4583">ules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptetitive dependency accuracies when compared to other systems. In terms of constituent structure, our best model (beam=64) obtains 88.74 LR, 89.21 LP and 88.97 F1. Table 3 compares our model with other constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 184–191, Santa Cruz, California, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22505" citStr="Collins (1997)" startWordPosition="3951" endWordPosition="3952"> so a higher size of the beam allows to test more of them when new actions are included in the agenda. This model already provides high results over 92.34% UAS and it can also predict most of the triplets that label the dependency 4We use the standard partition: sections 02-21 for training, section 22 for development, and section 23 for testing. 5We use the same setting as in (Carreras et al., 2008) by training over a treebank with predicted part-of-speech tags with mxpost (Ratnaparkhi, 1996) (accuracy: 96.5) and we test on the development set and test set with predicted partof-speech tags of Collins (1997) (accuracy: 96.8). 294 Dep. (incl punct) Dep. (excl punct) Const Speed Beam-size UAS TA TAS SA UAS TA TAS LR LP F1 Sent/Sec 8 91.39 90.47 88.78 95.60 92.32 91.21 89.73 88.6 88.4 88.5 7.8 16 91.81 90.95 89.28 95.84 92.70 91.65 90.21 89.0 89.1 89.1 3.9 32 92.08 91.14 89.52 95.96 92.91 91.77 90.38 89.4 89.5 89.5 1.7 64 92.34 91.45 89.84 96.13 93.12 92.04 90.65 89.5 89.7 89.7 0.8 Table 1: UAS with predicted part-of-speech tags for the dev.set including and excluding punctuation symbols. Constituent results for the development set. Parsing speed in sentences per second (an estimate that varies depe</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 16–23. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1362" citStr="Collins, 1999" startWordPosition="195" endWordPosition="196">de-off between speed and accuracy, and yields state of the art performance for both dependency and constituent parsing measures. 1 Introduction There are two main representations of the syntactic structure of sentences, namely constituent and dependency-based structures. In terms of statistical modeling, an advantage of dependency representations is that they are naturally lexicalized, and this allows the statistical model to capture a rich set of lexico-syntactic features. The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing (Collins, 1999; Nivre, 2003; McDonald et al., 2005). Constituent structure, on the other hand, might still provide valuable syntactic information that is not captured by standard dependencies. In this work we investigate transition-based statistical models that produce spinal trees, a representation that combines dependency and constituent structures. Statistical models that use both representations jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic constituent-based PCFG that make </context>
<context position="12218" citStr="Collins, 1999" startWordPosition="2141" endWordPosition="2142">constraints. E represents the stack, B represents the buffer, A represents the set of arcs, t represents a given triplet when its components are not relevant, (a, b, c) represents a given triplet when its components are relevant and i, j and k represent tokens of the sentence. The constraints labeled with (1) ... (5) are described in Section 3.2. The constraints that are not labeled are standard constraints of the arc-eager parsing algorithm (Nivre, 2003). In the literature, these triplets have been shown to provide very rich parameterizations of statistical models for parsing (Collins, 1996; Collins, 1999; Carreras et al., 2008). For our purposes, we associate with each spinal dependency (h, d, p) E E a triplet dependency (h, d, (a, b, c)), where the triplet is defined as above. We then define a standard statistical model for arc-eager parsing that uses constituent triplets as dependency labels. An important advantage of this model is that left-arc and right-arc transitions can have feature descriptions that combine standard dependency features with phrase-structure information in the form of constituent triplets. As shown by Carreras et al. (2008), this rich set of features can obtain signifi</context>
<context position="26730" citStr="Collins (1999)" startWordPosition="4649" endWordPosition="4650">89.21 LP and 88.97 F1. Table 3 compares our model with other constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transitionbased dependency parsing with stack long shortterm memory.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Bernd Bohnet</author>
</authors>
<title>Stacking of dependency and phrase structure parsers.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>849--866</pages>
<contexts>
<context position="30688" citStr="Farkas and Bohnet, 2012" startWordPosition="5287" endWordPosition="5290">re very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very high results. These models use dependency information via stacking, by running a dependency parser as a preprocess. In the literature, stacking is a common technique to improve accuracies by combining dependency and constituent information, in both ways (Wang and Zong, 2011; Farkas and Bohnet, 2012). Our model differs from stacking approaches in that it natively produces the two structures jointly, in such a way that a rich set of features is available. 296 6 Conclusions and Future Work There are several lessons to learn from this paper. First, we show that a simple modification to the arc-eager strategy results in a competitive greedy spinal parser which is capable of predicting dependency and constituent structure jointly. In order to make it work, we introduce simple constraints to the arc-eager strategy that ensure well-formed spinal derivations. Second, by doing this, we are providi</context>
</contexts>
<marker>Farkas, Bohnet, 2012</marker>
<rawString>Rich´ard Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In COLING, pages 849–866.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Fern´andez-Gonz´alez</author>
<author>Andr´e F T Martins</author>
</authors>
<title>Parsing as reduction.</title>
<date>2015</date>
<location>CoRR, abs/1503.00030.</location>
<marker>Fern´andez-Gonz´alez, Martins, 2015</marker>
<rawString>Daniel Fern´andez-Gonz´alez and Andr´e F. T. Martins. 2015. Parsing as reduction. CoRR, abs/1503.00030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>A dependencydriven parser for german dependency and constituency representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German, PaGe ’08,</booktitle>
<pages>47--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="28265" citStr="Hall and Nivre (2008)" startWordPosition="4908" endWordPosition="4911">edy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree u</context>
</contexts>
<marker>Hall, Nivre, 2008</marker>
<rawString>Johan Hall and Joakim Nivre. 2008. A dependencydriven parser for german dependency and constituency representations. In Proceedings of the Workshop on Parsing German, PaGe ’08, pages 47– 54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>A Hybrid Constituency-Dependency Parser for Swedish.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA).</booktitle>
<contexts>
<context position="28242" citStr="Hall et al. (2007)" startWordPosition="4904" endWordPosition="4907"> of that, with a greedy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a </context>
</contexts>
<marker>Hall, Nivre, Nilsson, 2007</marker>
<rawString>Johan Hall, Joakim Nivre, and Jens Nilsson. 2007. A Hybrid Constituency-Dependency Parser for Swedish. In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
</authors>
<title>Transition-based natural language parsing with dependency and constituency representations. Master’s thesis,</title>
<date>2008</date>
<institution>V¨axj¨o University.</institution>
<contexts>
<context position="28281" citStr="Hall (2008)" startWordPosition="4913" endWordPosition="4914">runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard co</context>
</contexts>
<marker>Hall, 2008</marker>
<rawString>Johan Hall. 2008. Transition-based natural language parsing with dependency and constituency representations. Master’s thesis, V¨axj¨o University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="25322" citStr="Huang and Sagae (2010)" startWordPosition="4416" endWordPosition="4419">ver, in absolute terms, our running times are slower than typical shift-reduce parsers. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our resu</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>12--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27044" citStr="Huang et al. (2010)" startWordPosition="4704" endWordPosition="4707">hich forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses in cubic time. Our work can be seen as the transition-based counterpart of that, with a gre</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable grammars. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12–22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="26978" citStr="Huang (2008)" startWordPosition="4694" endWordPosition="4695">arsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses in cubic time. Our work c</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL, pages 586– 594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Alexander M Rush</author>
<author>Noah A Smith</author>
</authors>
<title>Transforming dependencies into phrase structures.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="28712" citStr="Kong et al. (2015)" startWordPosition="4979" endWordPosition="4982">thod that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Ni</context>
</contexts>
<marker>Kong, Rush, Smith, 2015</marker>
<rawString>Lingpeng Kong, Alexander M. Rush, and Noah A. Smith. 2015. Transforming dependencies into phrase structures. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="25378" citStr="Koo and Collins (2010)" startWordPosition="4426" endWordPosition="4429">n typical shift-reduce parsers. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptet</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>595--603</pages>
<contexts>
<context position="25431" citStr="Koo et al. (2008)" startWordPosition="4436" endWordPosition="4439">elation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptetitive dependency accuracies when compared to other sy</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<publisher>Morgan</publisher>
<marker>Kubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra Kubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Morgan and Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
<author>Pedro Aguiar</author>
<author>Mario Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>34--44</pages>
<contexts>
<context position="25461" citStr="Martins et al. (2010)" startWordPosition="4442" endWordPosition="4445">accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptetitive dependency accuracies when compared to other systems. In terms of constituent</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="27082" citStr="McClosky et al. (2006)" startWordPosition="4711" endWordPosition="4714">er model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses in cubic time. Our work can be seen as the transition-based counterpart of that, with a greedy parsing strategy that runs in line</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 152– 159, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="25294" citStr="McDonald and Pereira (2006)" startWordPosition="4411" endWordPosition="4414">uation and 92.78 UAS, 91.53 6However, in absolute terms, our running times are slower than typical shift-reduce parsers. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuatio</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1399" citStr="McDonald et al., 2005" startWordPosition="199" endWordPosition="202">uracy, and yields state of the art performance for both dependency and constituent parsing measures. 1 Introduction There are two main representations of the syntactic structure of sentences, namely constituent and dependency-based structures. In terms of statistical modeling, an advantage of dependency representations is that they are naturally lexicalized, and this allows the statistical model to capture a rich set of lexico-syntactic features. The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing (Collins, 1999; Nivre, 2003; McDonald et al., 2005). Constituent structure, on the other hand, might still provide valuable syntactic information that is not captured by standard dependencies. In this work we investigate transition-based statistical models that produce spinal trees, a representation that combines dependency and constituent structures. Statistical models that use both representations jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic constituent-based PCFG that make a central use of lexical dependencies</context>
<context position="25261" citStr="McDonald et al. (2005)" startWordPosition="4406" endWordPosition="4409">the test set including punctuation and 92.78 UAS, 91.53 6However, in absolute terms, our running times are slower than typical shift-reduce parsers. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA a</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Daniel Fern´andez-Gonz´alez</author>
</authors>
<title>Arc-eager parsing with the tree constraint.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>2</issue>
<marker>Nivre, Fern´andez-Gonz´alez, 2014</marker>
<rawString>Joakim Nivre and Daniel Fern´andez-Gonz´alez. 2014. Arc-eager parsing with the tree constraint. Computational Linguistics, 40(2):259–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Guls¸en Eryiˇgit, Sandra Kubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<pages>13--95</pages>
<contexts>
<context position="28377" citStr="Nivre et al., 2007" startWordPosition="4926" endWordPosition="4929">ee probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependen</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Guls¸en Eryiˇgit, Sandra Kubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13:95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Yoav Goldberg</author>
<author>Ryan T McDonald</author>
</authors>
<title>Constrained arc-eager dependency parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="8458" citStr="Nivre et al., 2014" startWordPosition="1415" endWordPosition="1418">a configuration. The SHIFT transition removes the first node from the buffer and puts it on the stack. The REDUCE transition removes the top node from the stack. The LEFT-ARCt transition introduces a labeled dependency edge between the first element of the buffer and the top element of the stack with the label t. The top element is removed from the stack (reduce transition). The RIGHT-ARCt transition introduces a labeled dependency edge between the top element of the stack and the first element in the buffer with a label d, and it performs a shift transition. Each action can have constraints (Nivre et al., 2014), Figure 2 and Section 3.2 describe the constraints of the spinal parser. 290 (a) — Constituent Tree with head-children annotations NP VP . DT This NN market VBN has VP . VBN VP been very RB ADVP ADV badly VBN damaged (b) — Spinal Tree Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the underlined child annotates the head child of the constituent. (b) The corresponding spinal tree. * This * market * has * been * very * badly * damaged * . S ADVP VP NP VP VP In this paper, we took the already existent implementation of arc-eager from ZPar1 (Zh</context>
</contexts>
<marker>Nivre, Goldberg, McDonald, 2014</marker>
<rawString>Joakim Nivre, Yoav Goldberg, and Ryan T. McDonald. 2014. Constrained arc-eager dependency parsing. Computational Linguistics, 40(2):249–527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="1375" citStr="Nivre, 2003" startWordPosition="197" endWordPosition="198">speed and accuracy, and yields state of the art performance for both dependency and constituent parsing measures. 1 Introduction There are two main representations of the syntactic structure of sentences, namely constituent and dependency-based structures. In terms of statistical modeling, an advantage of dependency representations is that they are naturally lexicalized, and this allows the statistical model to capture a rich set of lexico-syntactic features. The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing (Collins, 1999; Nivre, 2003; McDonald et al., 2005). Constituent structure, on the other hand, might still provide valuable syntactic information that is not captured by standard dependencies. In this work we investigate transition-based statistical models that produce spinal trees, a representation that combines dependency and constituent structures. Statistical models that use both representations jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic constituent-based PCFG that make a central use</context>
<context position="3381" citStr="Nivre (2003)" startWordPosition="505" endWordPosition="506"> Carreras et al. (2008) using a so-called graph-based model. The main advantage of such models is that they allow a large family of rich features that include dependency features, constituent features and conjunctions of the two. However, the consequence is that the additional spinal structure greatly increases the number of dependency relations. Even though a graph-based model remains parseable in cubic time, it is impractical unless some pruning strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are 289 Proceedings of the 19th Conference on Computational Language Learning, pages 289–299, Beij</context>
<context position="7334" citStr="Nivre, 2003" startWordPosition="1218" endWordPosition="1219">s straightforward to obtain spinal trees from a treebank of constituent trees with head-child annotations in each constituent (Carreras et al., 2008): starting from a token, its spine consists of the non-terminal labels of the constituents whose head is the token; the parent node of the top of the spine gives information about the lexical head (by following the head children of the parent) and the position where the spine attaches to. Given a spinal tree it is trivial to recover the constituent and dependency trees. 2.2 Arc-Eager Transition-Based Parsing The arc-eager transition-based parser (Nivre, 2003) parses a sentence from left to right in linear time. It makes use of a stack that stores tokens that are already processed (partially built dependency structures) and it chooses the highest-scoring parsing action at each point. The arc-eager algorithm adds every arc at the earliest possible opportunity and it can only parse projective trees. The training process is performed with an oracle (a set of transitions to a parse for a given sentence, (see Figure 2)) and it learns the best transition given a configuration. The SHIFT transition removes the first node from the buffer and puts it on the</context>
<context position="12064" citStr="Nivre, 2003" startWordPosition="2119" endWordPosition="2120">ch that j &lt; k, a = a0 ∧ b = b0 (5) if b = * ⇒ ∀{j ha0,b0,c0i → k} ∈ A such that k &lt; j ∧ b0 = *, a = a0 Figure 2: Arc-eager transition system with spinal constraints. E represents the stack, B represents the buffer, A represents the set of arcs, t represents a given triplet when its components are not relevant, (a, b, c) represents a given triplet when its components are relevant and i, j and k represent tokens of the sentence. The constraints labeled with (1) ... (5) are described in Section 3.2. The constraints that are not labeled are standard constraints of the arc-eager parsing algorithm (Nivre, 2003). In the literature, these triplets have been shown to provide very rich parameterizations of statistical models for parsing (Collins, 1996; Collins, 1999; Carreras et al., 2008). For our purposes, we associate with each spinal dependency (h, d, p) E E a triplet dependency (h, d, (a, b, c)), where the triplet is defined as above. We then define a standard statistical model for arc-eager parsing that uses constituent triplets as dependency labels. An important advantage of this model is that left-arc and right-arc transitions can have feature descriptions that combine standard dependency featur</context>
<context position="29322" citStr="Nivre (2003)" startWordPosition="5078" endWordPosition="5079">5) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical depende</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL),</booktitle>
<pages>50--57</pages>
<contexts>
<context position="29339" citStr="Nivre (2004)" startWordPosition="5081" endWordPosition="5082">uctured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and </context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<volume>7</volume>
<pages>404--411</pages>
<contexts>
<context position="26839" citStr="Petrov and Klein (2007)" startWordPosition="4666" endWordPosition="4669">duce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spi</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, volume 7, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="22388" citStr="Ratnaparkhi, 1996" startWordPosition="3931" endWordPosition="3932">ss, it also makes the parser slower. This result is expected since the number of dependency labels, i.e. triplets, is 974 so a higher size of the beam allows to test more of them when new actions are included in the agenda. This model already provides high results over 92.34% UAS and it can also predict most of the triplets that label the dependency 4We use the standard partition: sections 02-21 for training, section 22 for development, and section 23 for testing. 5We use the same setting as in (Carreras et al., 2008) by training over a treebank with predicted part-of-speech tags with mxpost (Ratnaparkhi, 1996) (accuracy: 96.5) and we test on the development set and test set with predicted partof-speech tags of Collins (1997) (accuracy: 96.8). 294 Dep. (incl punct) Dep. (excl punct) Const Speed Beam-size UAS TA TAS SA UAS TA TAS LR LP F1 Sent/Sec 8 91.39 90.47 88.78 95.60 92.32 91.21 89.73 88.6 88.4 88.5 7.8 16 91.81 90.95 89.28 95.84 92.70 91.65 90.21 89.0 89.1 89.1 3.9 32 92.08 91.14 89.52 95.96 92.91 91.77 90.38 89.4 89.5 89.5 1.7 64 92.34 91.45 89.84 96.13 93.12 92.04 90.65 89.5 89.7 89.7 0.8 Table 1: UAS with predicted part-of-speech tags for the dev.set including and excluding punctuation symb</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="26661" citStr="Ratnaparkhi (1999)" startWordPosition="4637" endWordPosition="4638">rms of constituent structure, our best model (beam=64) obtains 88.74 LR, 89.21 LP and 88.97 F1. Table 3 compares our model with other constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional informa</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28059" citStr="Rush et al. (2010)" startWordPosition="4874" endWordPosition="4877">l representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses in cubic time. Our work can be seen as the transition-based counterpart of that, with a greedy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05,</booktitle>
<pages>125--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26626" citStr="Sagae and Lavie (2005)" startWordPosition="4630" endWordPosition="4633">s when compared to other systems. In terms of constituent structure, our best model (beam=64) obtains 88.74 LR, 89.21 LP and 88.97 F1. Table 3 compares our model with other constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results m</context>
<context position="29789" citStr="Sagae and Lavie (2005" startWordPosition="5148" endWordPosition="5151">ast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very h</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05, pages 125–132, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</booktitle>
<pages>129--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26699" citStr="Sagae and Lavie (2006)" startWordPosition="4642" endWordPosition="4645">best model (beam=64) obtains 88.74 LR, 89.21 LP and 88.97 F1. Table 3 compares our model with other constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, a</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06, pages 129–132, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semisupervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="25553" citStr="Suzuki et al. (2009)" startWordPosition="4458" endWordPosition="4461">ple, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-art. Our model obtains comptetitive dependency accuracies when compared to other systems. In terms of constituent structure, our best model (beam=64) obtains 88.74 LR, 89.21 LP and 88.97 F1. Table 3 compar</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semisupervised structured conditional models for dependency parsing. In Proceedings of EMNLP, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Parse reranking based on higher-order lexical dependencies.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1251--1259</pages>
<contexts>
<context position="30662" citStr="Wang and Zong, 2011" startWordPosition="5283" endWordPosition="5286">CCG representations are very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very high results. These models use dependency information via stacking, by running a dependency parser as a preprocess. In the literature, stacking is a common technique to improve accuracies by combining dependency and constituent information, in both ways (Wang and Zong, 2011; Farkas and Bohnet, 2012). Our model differs from stacking approaches in that it natively produces the two structures jointly, in such a way that a rich set of features is available. 296 6 Conclusions and Future Work There are several lessons to learn from this paper. First, we show that a simple modification to the arc-eager strategy results in a competitive greedy spinal parser which is capable of predicting dependency and constituent structure jointly. In order to make it work, we introduce simple constraints to the arc-eager strategy that ensure well-formed spinal derivations. Second, by </context>
</contexts>
<marker>Wang, Zong, 2011</marker>
<rawString>Zhiguo Wang and Chengqing Zong. 2011. Parse reranking based on higher-order lexical dependencies. In IJCNLP, pages 1251–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weiss</author>
<author>Christopher Alberti</author>
<author>Michael Collins</author>
<author>Slav Petrov</author>
</authors>
<title>Structured training for neural network transition-based parsing.</title>
<date>2015</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Weiss, Alberti, Collins, Petrov, 2015</marker>
<rawString>David Weiss, Christopher Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural network transition-based parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<contexts>
<context position="20715" citStr="Yamada and Matsumoto (2003)" startWordPosition="3640" endWordPosition="3643">bracketing precision, and 99.0 of Fl measure. with the state-of-the-art. We used the ZPar implementation modified to incorporate the constraints for spinal arc-eager parsing. We used the exact same features as Zhang and Nivre (2011), which extract a rich set of features that encode higherorder interactions betwen the current action and elements of the stack. Since our dependency labels are constituent triplets, these features encode a mix of constituent and dependency structure. 4.1 Data We use the WSJ portion of the Penn Treebank4, augmented with head-dependant information using the rules of Yamada and Matsumoto (2003). This results in a total of 974 different constituent triplets, which we use as dependency labels in the spinal arc-eager model. We use predicted part-ofspeech tags5. 4.2 Results in the Development Set In Table 1 we show the results of our parser for the dependency trees, the table shows unlabeled attachment score (UAS) , triplet accuracy (TA, which would be label accuracy, LA) and triplet attachment score (TAS), and spinal accuracy (SA) (the spinal accuracy is the percentage of complete spines that the parser correctly predicts). In order to be fully comparable, for the dependencybased metri</context>
<context position="23864" citStr="Yamada and Matsumoto (2003)" startWordPosition="4180" endWordPosition="4183">t triplets described in Section 3. SA is the spinal accuracy. arcs (91.45 TA and 89.84 TAS) (including punctuation symbols for evaluation). Table 1 also shows the results of the parser in the development set after transforming the dependency trees by following the method described in Section 3. The result even surpasses 89.5% F1 which is a competitive accuracy. As we can see, the parser also provides a good trade-off between parsing speed and accuracy.6 In order to test whether the number of dependency labels is an issue for the parser, we also trained a model on dependency trees labeled with Yamada and Matsumoto (2003) rules, and the results are comparable to ours. For a beam of size 64, the best model with dependency labels provides 92.3% UAS for the development set including punctuation and 93.0% excluding punctuation, while our spinal parser for the same beam size provides 92.3% UAS including punctuation and 93.1% excluding punctuation. This means that the beam-search arc-eager parser is capable of coping with the dependency triplets, since it even provides slightly better results for unlabeled attachment scores. However, unlike (Carreras et al., 2008), the arc-eager parser does not substantially benefit</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>162--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9078" citStr="Zhang and Clark, 2009" startWordPosition="1533" endWordPosition="1536">4), Figure 2 and Section 3.2 describe the constraints of the spinal parser. 290 (a) — Constituent Tree with head-children annotations NP VP . DT This NN market VBN has VP . VBN VP been very RB ADVP ADV badly VBN damaged (b) — Spinal Tree Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the underlined child annotates the head child of the constituent. (b) The corresponding spinal tree. * This * market * has * been * very * badly * damaged * . S ADVP VP NP VP VP In this paper, we took the already existent implementation of arc-eager from ZPar1 (Zhang and Clark, 2009) which is a beam-search parser implemented in C++ focused on efficiency. ZPar gives competitive accuracies, yielding state-of-the-art results, and very fast parsing speeds for dependency parsing. In the case of ZPar, the parsing process starts with a root node at the top of the stack (see Figure 3) and the buffer contains the words/tokens to be parsed. 3 Transition-based Spinal Parsing In this section we describe an arc-eager transition system that produces spinal trees. Figure 3 shows a parsing example. In essence, the strategy we propose builds the spine of a token by pieces, by adding a pie</context>
<context position="26799" citStr="Zhang and Clark (2009)" startWordPosition="4659" endWordPosition="4662">constituent parsers, including shift-reduce parsers as ours. Our best model is competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carrera</context>
<context position="30282" citStr="Zhang and Clark (2009)" startWordPosition="5224" endWordPosition="5227">nd they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very high results. These models use dependency information via stacking, by running a dependency parser as a preprocess. In the literature, stacking is a common technique to improve accuracies by combining dependency and constituent information, in both ways (Wang and Zong, 2011; Farkas and Bohnet, 2012). Our model differs from stacking approaches in that it natively produces the two structures jointly, in such a way that a rich set of features is available. 296 6 Conclusions and Future Work Th</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies, pages 162–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce ccg parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>683--692</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3842" citStr="Zhang and Clark, 2011" startWordPosition="577" endWordPosition="580">strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are 289 Proceedings of the 19th Conference on Computational Language Learning, pages 289–299, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics the following: • We define an arc-eager statistical model for spinal parsing that is based on the triplet relations by Collins (1996). Such relations, in conjunction with the partial spinal structure available in the stack of the parser, provide a very rich set of features. • We describe a set of conditions that an arceager strategy must guarantee in order to produce valid spinal</context>
<context position="29500" citStr="Zhang and Clark, 2011" startWordPosition="5103" endWordPosition="5106">constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference i</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011a. Shift-reduce ccg parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 683–692. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="3842" citStr="Zhang and Clark, 2011" startWordPosition="577" endWordPosition="580">strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are 289 Proceedings of the 19th Conference on Computational Language Learning, pages 289–299, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics the following: • We define an arc-eager statistical model for spinal parsing that is based on the triplet relations by Collins (1996). Such relations, in conjunction with the partial spinal structure available in the stack of the parser, provide a very rich set of features. • We describe a set of conditions that an arceager strategy must guarantee in order to produce valid spinal</context>
<context position="29500" citStr="Zhang and Clark, 2011" startWordPosition="5103" endWordPosition="5106">constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference i</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011b. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="20320" citStr="Zhang and Nivre (2011)" startWordPosition="3577" endWordPosition="3580">y mild effects on recovering constituent trees in the style of the Penn Treebank. To measure the effect, we took the correct spinal trees of the development section and mapped them to the corresponding arc-eager derivation. Then we mapped the derivation back to a spinal tree using this process and recovered the constituent tree. This process obtained 98.4% of bracketing recall, 99.5% of bracketing precision, and 99.0 of Fl measure. with the state-of-the-art. We used the ZPar implementation modified to incorporate the constraints for spinal arc-eager parsing. We used the exact same features as Zhang and Nivre (2011), which extract a rich set of features that encode higherorder interactions betwen the current action and elements of the stack. Since our dependency labels are constituent triplets, these features encode a mix of constituent and dependency structure. 4.1 Data We use the WSJ portion of the Penn Treebank4, augmented with head-dependant information using the rules of Yamada and Matsumoto (2003). This results in a total of 974 different constituent triplets, which we use as dependency labels in the spinal arc-eager model. We use predicted part-ofspeech tags5. 4.2 Results in the Development Set In</context>
<context position="25350" citStr="Zhang and Nivre (2011)" startWordPosition="4421" endWordPosition="4424">running times are slower than typical shift-reduce parsers. Our purpose is to show a relation between speed and accuracy, and we opted for a simple implementation rather than an engineered one. As one example, our parser considers all dependency triplets (974) in all cases, which is somehow absurd since most of these can be ruled out given the parts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Parser UAS McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Zhang and Nivre (2011) 92.9 Koo and Collins (2010)* 93.0 Bohnet and Nivre (2012) 93.0 Koo et al. (2008) †* 93.2 Martins et al. (2010) 93.3 Ballesteros and Bohnet (2014) 93.5 Carreras et al. (2008) †* 93.5 Suzuki et al. (2009) †* 93.8 this work (beam 64) †* 92.1 this work (beam 64) † 92.8 Table 2: State-of-the-art comparison for unlabeled attachment score for WSJ-PTB with Y&amp;M rules. Results marked with † use other kind of information, and are not directly comparable. Results marked with * include punctuation for evaluation. TA and 90.11 TAS excluding punctuation. Table 2 compares our results with the state-of-the-ar</context>
<context position="29524" citStr="Zhang and Nivre, 2011" startWordPosition="5107" endWordPosition="5110">y, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 188–193, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhou</author>
<author>Yue Zhang</author>
<author>Shujian Huang</author>
<author>Jiajun Chen</author>
</authors>
<title>A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<marker>Zhou, Zhang, Huang, Chen, 2015</marker>
<rawString>Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen. 2015. A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>434--443</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26872" citStr="Zhu et al. (2013)" startWordPosition="4673" endWordPosition="4676"> competitive compared with the rest. 5 Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size LR LP F1 Sagae and Lavie (2005)* 86.1 86.0 86.0 Ratnaparkhi (1999) 86.3 87.5 86.9 Sagae and Lavie (2006)* 87.8 88.1 87.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.5 89.9 89.5 Zhang and Clark (2009)* 90.0 89.9 89.9 Petrov and Klein (2007) 90.1 90.2 90.1 Zhu et al. (2013)-1* 90.2 90.7 90.4 Carreras et al. (2008) 90.7 91.4 91.1 Zhu et al. (2013)-2†* 91.1 91.5 91.3 Huang (2008) 91.2 91.8 91.5 Charniak (2000) 91.2 91.8 91.5 Huang et al. (2010) 91.2 91.8 91.5 McClosky et al. (2006) 91.2 91.8 91.5 this work (beam 64)* 88.7 89.2 89.0 Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an </context>
<context position="30327" citStr="Zhu et al. (2013)" startWordPosition="5233" endWordPosition="5236">own in Section 4.3. For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very high results. These models use dependency information via stacking, by running a dependency parser as a preprocess. In the literature, stacking is a common technique to improve accuracies by combining dependency and constituent information, in both ways (Wang and Zong, 2011; Farkas and Bohnet, 2012). Our model differs from stacking approaches in that it natively produces the two structures jointly, in such a way that a rich set of features is available. 296 6 Conclusions and Future Work There are several lessons to learn from this pa</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 434–443. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>