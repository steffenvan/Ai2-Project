<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992503">
A Constructive View of Discourse Operators *
</title>
<author confidence="0.967792">
Allan Ramsay
</author>
<affiliation confidence="0.959066">
Department of Computation
</affiliation>
<address confidence="0.8339405">
UMIST, PO Box 88,
Manchester M60 1QD, UK
</address>
<email confidence="0.841614">
Allan.RamsayOumist.ac.uk
</email>
<author confidence="0.945732">
Helen Gaylard
</author>
<affiliation confidence="0.997947">
Department of Computer Science
University of Exeter
</affiliation>
<address confidence="0.964104">
Exeter, EX4 4PT, UK
</address>
<email confidence="0.942659">
H.L.GaylardOexeter.ac.uk
</email>
<sectionHeader confidence="0.974083" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911375">
Dialogue systems have to consider not
just the propositional content of the
user&apos;s utterances, but also the user&apos;s atti-
tude to that content. It is common prac-
tice to treat these issues separately: we
will argue that they can, and should, be
dealt with at the same time and using the
same mechanisms.
</bodyText>
<sectionHeader confidence="0.7497145" genericHeader="method">
1 Interpretations, Proofs and
Meaning Postulates
</sectionHeader>
<bodyText confidence="0.994924235294118">
Language is used to convey ideas — to produce
in the hearer&apos;s mind a picture of the world that
corresponds to the picture which is already in the
speaker&apos;s. More than that, however, it is used to
convey attitudes — this bit of what I am saying is
interesting, this bit undermines what you just said,
this bit follows on from what you said, ...
Any system that is to process and respond to
user utterances will have to be sensitive to the way
that utterances encode attitudes. We suggest that
the best way to capture this information is to build
it into the basic meaning representation, and then
to elaborate the consequences of using one con-
struction rather than another in exactly the same
way that you elaborate the consequences of using
a specific lexical item.
Consider for instance (1) and (2):
</bodyText>
<listItem confidence="0.9997345">
(1) a. Mary and John got divorced in March.
b. In March Mary and John got divorced.
(2) a. Mary and John got divorced in March.
b. Mary and John got divorced in March.
</listItem>
<bodyText confidence="0.947089045454546">
These sentences all report the same event —
they all have the same PROPOSITIONAL CONTENT.
°This work was partially supported by EU grant
IST-2000-29452 &amp;quot;Dynamic Universal Mobility for
Adaptive Speech Interfaces&amp;quot;
Nonetheless, they differ in the way they express
the speaker&apos;s attitude to that content, or to parts
of it.
We can capture the propositional content itself
by constructing a LOGICAL FORM in the usual way.
The LF in Fig. 1 is fairly orthodox, and we will
use it to illustrate what you have to do in order to
include facts about the speaker&apos;s attitude. The de-
tails of how many 0-roles there should be&apos;, the de-
cision to deal with definite NPs by including REF-
ERENCE TERMS inside the LF rather than treat-
ing them separately as constraints (Barwise and
Perry, 1983; Kamp, 1984), the specific treatment
of aspect are all open to debate, but nothing much
hangs on these issues in the remainder of the paper
and we will simply assume that they are at least
defensible.
</bodyText>
<figure confidence="0.5096915">
faspect(simple, ref (ABpast(P)), A)}
0(A, object, ref (AE(name(E, Mary))))2
&amp;0(A, object, ref (AF(name(F, John))))
&amp;divorce(A)
&amp;A is event
&amp;in(A, ref (AG (name(G , March))))
</figure>
<figureCaption confidence="0.999519">
Figure 1: Logical form for (la)
</figureCaption>
<bodyText confidence="0.9947923125">
This LF was constructed COMPOSITIONALLY, i.e.
on the basis of the meanings of the parts and their
mode of combination. There is no alternative:
when you hear an utterance, or a read a text, you
have to base your understanding on what you hear
or see. The key claim in this paper is that when
you are trying to participate in a dialogue it is even
more important to extract all the information that
is encoded by the choice and arrangement of words
than when you are merely trying to extract the
propositional content.
There is no point, however, in constructing an
LF unless you link it to other things that you know.
When people say things to one another, they as-
&amp;quot;see (Dowty, 1989; Dowty, 1991) for a detailed dis-
cussion of how many thematic roles there are.
</bodyText>
<page confidence="0.99894">
103
</page>
<bodyText confidence="0.999832444444445">
sume that the other person will construct a rich
model of what was said, incorporating a combina-
tion of general background knowledge and a model
of what has already been said (by any participant
in the conversation). This model goes by various
names — the discourse model, the common ground,
... We follow fairly common practice in using the
term SLATE for this object.
The curious thing about the slate is that al-
though both the speaker and hearer rely on having
the same view of the slate, neither of them can di-
rectly inspect the other&apos;s version. Dialogues only
flow coherently if both parties have very similar
slates, but since neither of them can see directly
into the other person&apos;s head they cannot be sure
that they do.
We take it, then, that the basic steps in assimi-
lating an utterance are as follow:
</bodyText>
<listItem confidence="0.99542325">
1. construct a logical form that captures all and
only the information explicitly encoded by the
form of the utterance.
2. check that any claims that this logical form
makes about the current slate are indeed sup-
portable
3. update the slate to include the new informa-
tion in the utterance
</listItem>
<bodyText confidence="0.999805781818182">
If you get step 1 wrong, you&apos;ve got a problem.
If you fail to capture any of the information en-
coded by the utterance, no amount of subsequent
inference can retrieve it for you; and if you in-
clude anything erroneous you will have great trou-
ble spotting it and removing it. You may need to
note that there are alternative analyses, and either
follow up one choice, with the possibility of back-
tracking when it turn out to be wrong, or somehow
delay making a decision until it really matters (at
which point you may have the information required
for making an informed choice), but you do have
to look closely at the utterance and get what you
can from it.
Step 2 involves verifying any presuppositions
that are encoded in the utterance. (van der Sandt,
1992) argues that referential expressions are, in
fact, a form of presupposition, and that determin-
ing the item &apos;pointed to&apos; by such an expression is
just a by-product of the process of verifying the
presupposition. We argue that if you are the hearer
then this process involves checking that your ver-
sion of the slate EH supports the same proofs as
the speaker&apos;s Es. This is the closest you can get
to inspecting each other&apos;s views of the slate — if
they support the same proofs then they can&apos;t be
all that different. The uniqueness element of def-
inite NPs is particularly critical at this point — if
Es supports a proof of man(t) for exactly one t
and EH does likewise then S and H are probably
thinking about the same man, who can therefore
safely be referred to as &apos;the man&apos; (Ramsay, 1999;
Gaylard and Ramsay, 2002).
Step 3 involves adding the new information in-
cluded in the current utterance to the logical form,
producing Ezi/P1 from EiH. This step may just in-
volve adding the new elements of the logical form,
or it may involve adding other things that can be
inferred from the logical form in the current con-
text. We have argued that you can combine steps
2 and 3 by trying to build a model of the current
utterance (Ramsay and Seville, 2000a). Other au-
thors have taken a similar view (Wedekind, 1996;
Blackburn et al., 1997; Baumgartner and Kuhn,
2000; Gardent and Konrad, 2000).
No matter how you approach steps 2 and 3, there
is no doubt that you need substantial amounts of
background knowledge. To find out what someone
who utters (la) has in mind, you have to know that
divorce is an action that involves terminating an
earlier contract, and that usually when it happens
the two parties concerned no longer love each other,
and ... In other words, you need to have access to
the kind of information contained in Fig. 2, and you
need to be able to reason with this information.
</bodyText>
<equation confidence="0.987087285714286">
VA : {divorce(A)}
VB : {0(A, object, B)}
VC: {O(A, object, C) &amp;B C}
]Dmarry(D)
O(D, object, B)
&amp; O(D, object, C)
&amp;termination(A,D)
</equation>
<figureCaption confidence="0.8706895">
Figure 2: You can only get divorced if you were
married
</figureCaption>
<bodyText confidence="0.999979933333333">
We are thus working in a framework where we
construct logical forms and use these to build
models that support them, taking the model con-
structed at each stage to be the current version of
the slate. For a variety of reasons we assume that
it is convenient to use a fine-grained intensional
logic as the formal substrate of this activity. The
logic we use is a constructive version of (Turner,
1987)&apos;s PROPERTY THEORY. Other intensional log-
ics are available — (Bealer, 1989)&apos;s fine-grained in-
tensional logic, non-well-founded set theory (Aczel,
1988), higher-order unification(Pulman, 1993). We
choose property theory because it lends itself to a
simple extension of a standard first-order theorem
prover, which is what we use for constructing mod-
</bodyText>
<page confidence="0.997985">
104
</page>
<bodyText confidence="0.9124596">
els (Ramsay, 2001).
2 Discourse Operators as Attitude
Reports
Can we extend the update process described above
so that it deals with the differences between the
various versions of the report of John and Mary&apos;s
divorce in (1) and (2)? If we can, then we can make
the critical information about how these sentence
fit into the discourse as a whole available to our
dialogue manager.
</bodyText>
<subsectionHeader confidence="0.991776">
2.1 Theme &amp; Rheme
</subsectionHeader>
<bodyText confidence="0.9997855">
The first move is to see just what the differences
are. We reconsider (1):
</bodyText>
<listItem confidence="0.9531895">
(1) a. Mary and John got divorced in March.
b. In March Mary and John got divorced.
</listItem>
<bodyText confidence="0.999863">
The difference between them lies in the fact
that by putting the temporal modifier first in (lb)
we are drawing attention to what haappened in
March, rather than saying what John and Mary
did.
Informally we can say that (1a) is &apos;about&apos; John
and Mary, and (lb) is about what happened in
March. Formally we can say the same thing:
</bodyText>
<figure confidence="0.917656166666667">
about(AB(B.re f (AC (name(C, Mary)))
B .re f (AD(name(D, John)))),
.X.EEA : faspect(simple, ref (AFpast(F)), A)}
E . AI(&amp;(A, object, I))
Sidivorce(A)&amp;A is event
&amp;in(A, ref (AJ(naine(J, March)))))
</figure>
<figureCaption confidence="0.988599">
Figure 3: (la) is &apos;about&apos; what happened to John
and Mary
</figureCaption>
<construct confidence="0.7378894">
about(AC(AD(C . D &amp; in(D, ref (AE(name(E, March)))))),
ABa.4. : faspect(simple, ref (AFpast(F)), A)}
(B.AIO(I, object, ref (AJ(name(J, John))))
&amp;O(I, object, ref (Alf (name(K, Mary))))
Szclivorce(I)8zI is event).A)
</construct>
<figureCaption confidence="0.984683">
Figure 4: (lb) is &apos;about&apos; what happened in March
</figureCaption>
<bodyText confidence="0.999894391304348">
We do this by noting, following (Halliday, 1985),
that the leftmost phrasal daughter of an English
sentence — its THEME — seems to be particularly
significant. It is easy enough to spot which item is
the leftmost phrasal daughter, so we can mark this
item carefully when we are constructing our logical
form. In fact all we have to do is to delay combin-
ing the meaning of the theme and the meaning of
the rheme (everything else).
Having obtained such a logical form, however, we
have to specify the meaning of &apos;about&apos;. This comes
in two parts: (i) we have to recover the normal
propositional content — if what I tell you about
John and Mary is that they got divorced then they
did indeed get divorced — and (ii) we have to make
use of the fact that one part of the utterance is
marked as being more important.
The first part of this is easy, given that we are
using a highly intensional logic. Fig. 5 just says
that the rheme holds of the theme, which recov-
ers the standard meaning (in fact it just carries
out the A-reduction step that we delayed when we
constructed the logical form).
</bodyText>
<equation confidence="0.793539">
VXVYabout(X,Y) —&gt; Y.X
</equation>
<figureCaption confidence="0.964712">
Figure 5: If I say Y about X then Y is true of X
</figureCaption>
<bodyText confidence="0.998884166666667">
What do we do with the theme:rheme division
itself?
One obvious function is to help link the current
utterence to an earlier one. If, for instance, the
current utterance is an answer to an earlier ques-
tion then they should have the same rheme:
</bodyText>
<listItem confidence="0.9759434">
(3) Who got divorced in March? John and
Mary got divorced in March.
(4) Who got divorced in March? Peter and
Susan got divorced in April.
(4) doesn&apos;t make a sensible question:answer pair
</listItem>
<bodyText confidence="0.513007333333333">
because the rheme of the answer is different from
the rheme of the question. We can capture this
constraint with Fig. 63.
</bodyText>
<equation confidence="0.7319945">
VUiVUi+janswer(Ui,U)
VR(rheme(Ui, rheme(Ui+j, R))
</equation>
<figureCaption confidence="0.87233">
Figure 6: An answer should address the rheme of
the question
</figureCaption>
<bodyText confidence="0.7570192">
Similarly, if the current utterance is a narrative
continuation of some previous utterance then the
two are likely to share the same theme:
VUiVUi+jnarrative(Ui, Ui+j)
VT(therne(U. , theme(Ui+i , T))
</bodyText>
<figureCaption confidence="0.9485535">
Figure 7: The theme stays the same as a story
unfolds
</figureCaption>
<bodyText confidence="0.906270666666667">
Thus the theme:rheme distinction can be used to
look for connections between utterances of the kind
described in RST (Mann and Thompson, 1988;
Mann, 1999). It is also worth noting that objects
3Note that this rule can also be used to reconstruct
the theme of an elliptical answer.
</bodyText>
<page confidence="0.996773">
105
</page>
<bodyText confidence="0.9986275">
referred to within the theme are likely to be par-
ticularly salient in the discourse, so that they are
good candidates for when you are trying to deref-
erence pronouns4.
</bodyText>
<subsectionHeader confidence="0.988964">
2.2 Focus
</subsectionHeader>
<bodyText confidence="0.995811285714286">
The difference between (2a) and (2b) is in some
ways similar. Again we have individual items
picked out, the difference being that here we use
intonation or typography rather than dislocation.
Again the marked items are in some way interest-
ing, and again we have to be able to reconstruct
the basic propositional content.
</bodyText>
<listItem confidence="0.855489">
(2) a. Mary and John got divorced in March.
b. Mary and John got divorced in March.
</listItem>
<bodyText confidence="0.986085333333333">
Isolating the semantics of the focussed item is
a slightly more complicated process this time, but
the resulting analyses are fairly similar:
</bodyText>
<equation confidence="0.4806615">
f ocus(AB(name(B, March)),
ACA : {aspect(simple, ref (ADpast(D)), Ail
0(A, object, ref (AG(name(G, Mary))))
&amp;0(A, object, re f (AH(name(H, John))))
&amp;divorce(A)&amp;A is event
&amp;in(A, ref (Al (C . I))))
</equation>
<figureCaption confidence="0.962701">
Figure 8: They got divorced in March, not in April
</figureCaption>
<figure confidence="0.695212333333333">
f ocus(AB(divorce(B)&amp; B is event
&amp; in(B, ref (AC(name(C, March))))),
XDEA : {aspect(simple, ref (AEpast(E)), A)}
0(A, object, ref (AH(name(H, Mary))))
&amp;0(A, object, ref (AI(name(I, John))))
&amp;D . A)
</figure>
<figureCaption confidence="0.999494">
Figure 9: They got divorced, not married, in March
</figureCaption>
<bodyText confidence="0.9994554">
As before, we have a rule for reconstructing the
standard propositional content by combining the
two parts that have been left unreduced, so that
each of (2a) and (2b) entails that they did get di-
vorced in March:
</bodyText>
<equation confidence="0.7205175">
VXVYfocus(X, Y) Y.X
Figure 10: If I say Y about X then Y is true of X
</equation>
<bodyText confidence="0.994628333333333">
The extra information carried by the fact that
one item is put in focus has to be consumed by
some other operator. In cases like (2a) and (2b)
where there is no overt discourse operator, we as-
sume that the given sentence is being contrasted
4The relationship between the theme:rheme distinc-
tion and the phenomena investigated in CENTERING
THEORY (Joshi and Weinstein, 1998) is rather unclear:
what is clear is that devices for indicating which ele-
ments of an utterance are particularly prominent car-
ries a great deal of information that can be used for
organising an extended dialogue.
with some proposition which is already present in
the discourse, and which is now being denied. We
therefore have the rule in Fig. 11, which picks out,
via the referential term, some item which is cur-
rently believed to satisfy Q and claims that this is
not in fact the case. Note the explicit reference to
the slate E in this rule: the current utterance is be-
ing contrasted with some other salient proposition
which is entailed by the slate.
</bodyText>
<equation confidence="0.4095225">
VPVQ (f ocus(P, Q)
—i(Q.ref (AP&apos; (E (Q.P&apos;)))))
</equation>
<figureCaption confidence="0.951107">
Figure 11: Q does not hold of P&apos;
</figureCaption>
<bodyText confidence="0.999839">
The act of focussing one element of the sentence,
however, does not always indicate a direct contrast
with some proposition in the discourse. Its task is
to split the propositional content into two parts.
In many cases, such as (5) and (6), there is an
explicit lexical item which requires two such argu-
ments. The assumption that there is a contrast
should only be made when there is no such lexical
item.
</bodyText>
<listItem confidence="0.999238">
(5) a. I only borrowed your bike
b. I only borrowed it.
(6) a. Susan even kissed Peter.
b. She even hugged him.
</listItem>
<bodyText confidence="0.783929666666667">
The logical forms for (5a) and (6b) are given in
Fig. 12 and Fig. 13: the corresponding pair are
similar.
</bodyText>
<figure confidence="0.5354518">
only(AF(bike(F)),
AGEH {H is interval Sr past(11)}
EE : {ospect(simple, H, E)}
BlE,agent,ref(XJspeaker(J)))
8,borrow(E)
StE is event
&amp;0(E,
object,
re f (AL(G.L,
Rcof(L, ref (AM(hearer(M))))))))
</figure>
<figureCaption confidence="0.892206">
Figure 12: (5a): what I borrowed was your bike,
not something else of yours
</figureCaption>
<figure confidence="0.52158825">
even(AE(hug(E) &amp; E is event),
AFEG : {G is interval &amp; past(G)}
ED : {aspect(simple, G, D)}
0(D, agent, ref (Alf (I)))
&amp;F.D
&amp;O(D,
object,
re f (AKm(K))))
</figure>
<figureCaption confidence="0.7133675">
Figure 13: (6b): she hugged him, which was the
last thing anyone expected her to do
</figureCaption>
<bodyText confidence="0.997755">
As usual, we have to say what follows from these
operators. They are, as before, truth preserving, so
</bodyText>
<page confidence="0.993078">
106
</page>
<bodyText confidence="0.999021">
we need the rule in Fig. 14. &apos;only&apos; again contrasts
the current proposition with something which is
already present in the discourse, with the extra
constraint that this proposition should be in some
sense &apos;stronger&apos; 5.
</bodyText>
<equation confidence="0.9170535">
V AVB(only(A,B) B.A)
VAVB(even(A,B) B.A)
</equation>
<figureCaption confidence="0.968101">
Figure 14: &apos;even&apos; and &apos;only&apos; are truth preserving
</figureCaption>
<figure confidence="0.733655">
V AV B(only(A, B) .re f (AA&apos; (A&apos; &gt;&gt; A&amp;B.A&apos;))
</figure>
<figureCaption confidence="0.974604">
Figure 15: B doesn&apos;t hold of the &apos;stronger&apos; item A&apos;
which you thought it did
</figureCaption>
<bodyText confidence="0.99194025">
The effect of &apos;even&apos; is to emphasise how unlikely
the reported event is: we capture this with Fig. 16,
which notes that there is some similar but more
probable proposition:
</bodyText>
<equation confidence="0.756599">
VAVB(even(A,B)
3A&apos;(A&apos; A8zprob(B.A1)&gt; prob(B.A))
</equation>
<figureCaption confidence="0.949223">
Figure 16: There is some similar A&apos; for which B.A&apos;
is more likely than B.A
</figureCaption>
<bodyText confidence="0.999855111111111">
Note that &apos;even&apos; and &apos;only&apos; are components of
the actual utterance, and hence would naturally be
dealt with in its logical form, whereas the implied
contrastive stress in (2) is a relationship between
utterances. Since the device used to mark the fo-
cussed elements is the same in all these cases, it is
highly desirable to provide a uniform treatment by
including the constrastive stress in the logical form
as well.
</bodyText>
<subsectionHeader confidence="0.986591">
2.3 Mood
</subsectionHeader>
<bodyText confidence="0.997838">
The theme:rheme distinction and the use of
marked stress, then, enrich the propositional con-
tent of an utterance by linking it to the surround-
ing discourse and by expressing complex relations
to various components. By capturing these facets
of the meaning inside the logical form, we obtain
a smooth connection between the two.
</bodyText>
<footnote confidence="0.738681">
5You can reasonably say &apos;You think I stole it, but
actually I only borrowed it&apos;, whereas &apos;You think I bor-
rowed it, but actually I only stole it&apos; sounds strange:
</footnote>
<bodyText confidence="0.756382285714286">
the notion of relative strength here is similar to that
in (Gazdar, 1979)&apos;s use of &apos;expression alternatives&apos; for
dealing with implicature, and to (Kruijff-Korbayova
and Webber, 2001)&apos;s notion of &apos;alternative sets&apos;
It is also clear that choosing the way the words
are arranged allows you to express different atti-
tudes to the truth of the proposition as a whole:
</bodyText>
<listItem confidence="0.997671">
(7) Did John and Mary get divorced in March?
(8) Get divorced in March.
</listItem>
<bodyText confidence="0.999973222222222">
The basic event type depicted by (7) is the same
as the one depicted by (2) and (1), and the one de-
picted by (8) is clearly closely related. It is there-
fore worth seeing whether we can capture the at-
titudes underlying these two sentences within the
logical form as well.
As ever, we can easily include a term in the logi-
cal form which corresponds to the specified mood,
just on the basis of the surface form:
</bodyText>
<equation confidence="0.935101153846154">
clairn(3A : laspect(simple, ref (ABpast(B)), A)}
0(A, object, ref (AE(name(E, Mary))))
Sze (A, object, ref (AF(riame(F, John))))
&amp;clivorce(A)&amp;A is event
&amp;in(A, ref (AG(naine(G, March)))))
Figure 17: (la): I&apos;m telling you it&apos;s true
query(EA : {A is interval
&amp;past(A)}
EE : {aspect(simple, A, Ell
19(E, object, ref (XF(naine(F, Mary))))
&amp;O(E, object, ref (AG(name(G, John))))
&amp;divorce(E)&amp;E is event
&amp;irt(E, ref (AH(riarrie(H, March)))))
</equation>
<figureCaption confidence="0.820607333333333">
Figure 18: (7): I don&apos;t know whether it&apos;s true
use fulOGO(C, object, ref (ADhearer(D)))
&amp;divorce(C)
&amp;C is event
&amp;in(C, ref (AF(name(F, March)))))
Figure 19: (8): I&apos;d like you to make it true
</figureCaption>
<bodyText confidence="0.999891615384615">
And as ever, having included such terms we have
to devise rules to account for their effects.
Most treatments of mood follow (Austin, 1962;
Searle, 1969) in assuming that there is a connection
between the overt &apos;force&apos; of the utterance and the
speaker&apos;s underlying goals, with a substantial tra-
dition of AT work linking this to Al planning theory
(Cohen and Perrault, 1979; Cohen and Levesque,
1980; Cohen et al., 1990; Allen and Perrault, 1980;
Bunt and Black, 2000). We will argue that this
connection has to be made explicit in the logical
form, rather than being dealt with as a separate
phenomenon.
</bodyText>
<listItem confidence="0.848346666666667">
Consider (9):
(9) a. John fancies every woman he sees.
b. John fancies any woman he sees.
</listItem>
<page confidence="0.998395">
107
</page>
<bodyText confidence="0.933841652173913">
There doesn&apos;t seem to be much difference be-
tween (9a) and (9b). They both seem to claim
that if X is a woman and John sees X then John
fancies X. In other words, &apos;any&apos; and &apos;every&apos; both
seem to be universal quantifiers.
In (10), however, the effects of &apos;every&apos; and &apos;any&apos;
are very different:
(10) a. You can&apos;t invite John. He&apos;ll drink ev-
erything.
b. You can invite John. He&apos;ll drink any-
thing.
The difference between (10a) and (10b) is that
(10a) says that there will be a future state of affairs
where &apos;John drinks everything&apos; is true, whereas
(10b) says that for any drink D there is a future
state of affairs where &apos;John drinks D&apos; is true.
The fact that &apos;any&apos; has very wide scope is also
reflected in the interpretation of negative sentences
like &apos;I didn&apos;t get anything I wanted for Christmas&apos;,
which can be paraphrased as saying that if X is
something I wanted then I didn&apos;t get X for Christ-
mas. Just how wide its scope is, however, becomes
clear when we consider (11) and (12):
</bodyText>
<listItem confidence="0.90745175">
(11) a. Do you know everyone at the party?
b. Do you know anyone at the party?
(12) a. Take any lane for Macclesfield.
b. Take every lane for Macclesfield.
</listItem>
<bodyText confidence="0.983963041666667">
Fig. 20 says that there is something I would like
to do, but that I can&apos;t do it unless I have evidence
that you know someone at the party. What kind
of thing might I be able to do under those circum-
stances? If I&apos;m the host then I might go and look
for them so that you have someone to talk to, if
I&apos;m a fellow guest then I might ask you to intro-
duce them to me, ...In general, however, when
you ask a question it&apos;s because there is something
that you could do if you knew the answer. The key
point about Fig. 20 is that it says that it doesn&apos;t
matter which person you know: for every one who
is at the party, if you know them then I can do
whatever it is I have in mind.
We thus encode the notion that &apos;any&apos; gives you
a &apos;free choice&apos; (Vendler, 1967) within a perfectly
standard logic. It is still just a universal quanti-
fier, but it has very wide scope — wide enough to
be quantifying over questions that I would like the
answer to, rather than as part of the queried propo-
sition as would happen with (11a), as in Fig. 21.
Similar considerations apply to (12a) and (12b).
(12a) says that whichever lane you choose will
be appropriate for getting to Macclesfield. (12b)
</bodyText>
<equation confidence="0.9951518">
3C : lintend(speaker, do(speaker, C))1
VD : lone(D)
at(D, ref (AE(party(E))))1
SF : IF is interval Sz past(F)}
3H : {aspect(simple, F, H)}
B(H, agent, ref (AHhearer(1))))
&amp;know(H)
86H is event
860(H, object, D)
do(speaker, C)
</equation>
<figureCaption confidence="0.83214">
Figure 20: There&apos;s something I could do if I knew
that you knew someone at the party
</figureCaption>
<equation confidence="0.9621795">
3C :{intend(speaker, do(speaker, C))1
3D :{D is interval &amp; past(D)}
VF : lone(F) &amp; at(F, ref (AG(party(G))))1
SH : laspect(sirnple, D, H)}
8(H, agent, ref (AI (hearer(I))))
kzknow(H)SzH is event
SzO(H, object, F)
do(speaker, C)
</equation>
<figureCaption confidence="0.9667315">
Figure 21: There&apos;s something I could do if I knew
that you knew everyone at the party
</figureCaption>
<bodyText confidence="0.593443">
doesn&apos;t really make any sense, since it seems to
require you to drive in several lanes at once. We
therefore paraphrase (12a) as in Fig. 22.
</bodyText>
<equation confidence="0.425985444444444">
V B : {lane(B)}
use f ul(SAtake(A)
SzA is event
SzO(A, object, B)
Szf or(A,
ref (AC(name(C, Macc))))
Sth(A,
agent,
ref (ADspeaker(D))))
</equation>
<bodyText confidence="0.918878846153846">
Figure 22: For any lane, it makes sense to take it
if you want to go to Macclesfield
The wide scope of &apos;any&apos; allows us to interpret
(12a) as saying that there are several things that
would be useful in the current situation, namely all
the propositions that result from choosing a lane.
We have weakened the force of the imperative to
useful because, as is clear from the example, not
all commands actually relate to things the speaker
wants. As always, however, having introduced the
term useful into our logical form we are under an
obligation to provide an account of what it signi-
fies.
</bodyText>
<equation confidence="0.99679425">
V P (use f ul(P)
C:] {VS: {speaker(S)}want(S, G)
vVH : {hearer(H)}want(H,G)}
P
</equation>
<bodyText confidence="0.9160035">
Figure 23: Something is useful if it will help either
the hearer or speaker achieve some goal
G in Fig. 23 will usually be something that the
speaker wants, but in examples like (12a) it may be
</bodyText>
<page confidence="0.998184">
108
</page>
<bodyText confidence="0.999740173913043">
the hearer&apos;s underlying goal that will be satisfied
by the specified action. Note that the logical form
for (12a) said that certain kinds of situations would
be useful. Fig. 23 then says that if a situation that
is described by the proposition P is useful then it
must be the case that G will be achievable in any
situations of this general type.
It is very hard indeed to see how the interac-
tions between &apos;any&apos; and the various moods can be
captured other than by including the mood in the
logical form. (11b) involves quantifying over ques-
tions whose answer would satisfy my needs, (12a)
involves quantifying over useful/desirable actions.
If you try to separate the logical form and the
mood you simply cannot retrieve this interaction.
By including the mood inside the logical form, we
make it possible to account for the discourse effects
of &apos;any&apos;, and we continue to work with a unified
two-stage framework — anchor the logical form and
then think about its consequenes. This is particu-
larly significant when considering the interactions
between negation, mood and quantification. Con-
sider (13):
</bodyText>
<figure confidence="0.635413714285714">
(13) Don&apos;t touch anything.
We suggest the logical form in Fig. 24 for this.
VC: {thing(C)}
use f ulHalA(D, agent, re f (AE speaker(E)))
Sztouch(D)
&amp;D is event
809(D, object, C)))
</figure>
<figureCaption confidence="0.616097">
Figure 24: For each thing C it would be helpful if
you didn&apos;t touch C
</figureCaption>
<bodyText confidence="0.9999775">
Ensuring the right relations between the universal
quantifier, the negation and useful would be ex-
temely difficult if they were not dealt with at the
same time and in the same place.
</bodyText>
<sectionHeader confidence="0.999707" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.999990555555556">
We have show that it is possible to capture vari-
ous aspects of the speaker&apos;s attitude to what he is
saying inside the logical form. We believe that it is
also extremely convenient to do so. If you do not
deal with information structure and intonation in
the logical form, you will find it extremely difficult
to localise their effects and coordinate them with
the propositional content, since they dfeal with lo-
cal elements of the propositional content (and in
the case of focus, they deal with arbitrary elements,
so that you cannot rely on them to pick out dis-
course referents or other simple entities). If you
do not deal with mood in the logical form you will
simply find it impossible to cope with the interac-
tions between illocutionary force and the quantifier
&apos;any&apos; (so that you will not, for instance, be able
to react appropriately to a question such as &apos;Do I
have any messages about Viagra?&apos; oe a command
like &apos;Don&apos;t delete any mails from John&apos;). We deal
with these cases by constructing models which re-
veal the critical issues. The logical form contains
all the information encoded by the surface form.
The inference engine unpacks this information in
a way that makes it possible to plan appropriate
responses.
Computing logical forms of the kind shown
above can be done using the standard composi-
tional techniques. All the logical forms in this pa-
per were obtained in this way, using the parser de-
scribed in (Ramsay and Seville, 2000b). You have
to make use of standard rescoping algorithms (van
Eijck and Alshawi, 1992; Milward and Cooper,
1994) to ensure that the relations between the var-
ious discourse operators are handled correctly, but
you need this anyway for handling scopes.
Carrying out the required inference is harder.
In (Ramsay and Seville, 2000a) we showed how to
construct models by combining the literal content
of an utterance, as encoded in the logical form,
with a rich collection of background knowledge. To
do this with the meaning postulates given above for
discourse operators we need to be able to carry out
this activity with meaning postulates couched in a
higher-order logic. The theorem prover described
in (?) allows us to do exactly that.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789388888889">
P Aczel. 1988. Non-Well-Founded-Sets. CSLI Publi-
cations, Stanford.
J F Allen and C R Perrault. 1980. Analysing intention
in utterances. Artificial Intelligence, 15:148-178.
J Austin. 1962. How to Do Things with Words. Oxford
University Press, Oxford.
J Barwise and J Perry. 1983. Situations and Attitudes.
Bradford Books, Cambridge, MA.
P Baumgartner and M Kuhn. 2000. Abducing coref-
erence by model construction. Journal of Language
and Computation, 1(2).
G Bealer. 1989. Fine-grained type-free intensional-
ity. In G Chierchia, B H Partee, and R Turner,
editors, Properties, types and meaning: vol I, foun-
dational issues. Kluwer Academic Publishers, Dor-
drecht/Boston/London.
P Blackburn, J Bos, M Kohlhase, and H de Nivelle.
1997. Inference and computational semantics. In
</reference>
<page confidence="0.983707">
109
</page>
<reference confidence="0.999560147368422">
H Bunt and E Thijsse, editors, 3rd International
Workshop on Computational Semantics, pages 5-19,
University of Tilburg.
H Bunt and W J Black, editors. 2000. Abduction, Be-
liefs and Context: Studies in Computational Prag-
matics. John Benjamins, Amsterdam/Philadelphia.
P R Cohen and H Levesque. 1980. Speech acts and the
recognition of shared plans. In Proceedings, Cana-
dian Society for Computational Studies of Intelli-
gence, pages 263-270.
P R Cohen and C R Perrault. 1979. Elements of a
plan-based theory of speech acts. Cognitive Science,
7(2):171-190.
P R Cohen, J Morgan, and M E Pollack. 1990. In-
tentions in Communication. Bradford Books, Cam-
bridge, Mass.
D R Dowty. 1989. On the semantic content of the no-
tion of &apos;thematic role&apos;. In G Chierchia, B H Partee,
and R Turner, editors, Properties, Types and Mean-
ing II: Semantic Issues, pages 69-130, Dordrecht.
Kluwer Academic Press.
D R Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67:547-619.
C Gardent and K Konrad. 2000. Interpreting definites
using model generation. Journal of Language and
Computation, 1(2):215-230.
H Gaylard and A M Ramsay. 2002. A unified theory of
reference resolution. In 4th Discourse and Dialogue
Colloquium, Lisbon.
G Gazdar. 1979. Pragmatics: Implicature, Presupposi-
tion and Logical Form. Academic Press, New York.
M A K Halliday. 1985. An Introduction to Functional
Grammar. Arnold, London.
A Joshi and S Weinstein. 1998. Formal systems for
complexity and control of inference: A reprise and
some hints. In M. A. Walker, A. K. Joshi, and
E. F. Prince, editors, Centering Theory in Discourse,
pages 31-38. Oxford University Press, Oxford.
H Kamp. 1984. A theory of truth and semantic repre-
sentation. In J A G Groenendijk, T M V Janssen,
and M B J Stokhof, editors, Formal Methods in the
Study of Language, pages 277-322, Dordrecht. Foris
Publications.
I Kruijff-Korbayova and B L Webber. 2001. Conces-
sion, implicature and alternative sets. In H Bunt,
I van der Sluis, and E Thijsse, editors, 4th In-
ternational Workshop on Computational Semantics,
pages 227-248, University of Tilburg.
W C Mann and S A Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3) :243-281.
W C Mann. 1999. An introduction to rhetorical struc-
ture theory.
D Milward and R Cooper. 1994. Incremental inter-
pretation: Applications, theory and relationship to
dynamic semantics. In Proceedings of the 15th In-
ternational Conference on Computational Linguis-
tics (COLING-94), pages 88748-754, Kyoto.
S G Pulman. 1993. Higher order unification and the
interpretation of focus. Linguistics flu Philosophy,
20(1):73-115.
A M Ramsay and H Seville. 2000a. Models and dis-
course models. Journal of Language and Computa-
tion, 1(2):167-181.
A M Ramsay and H Seville. 2000b. Unscrambling
English word order. In M Kay, editor, Proceedings
of the 18th International Conference on Computa-
tional Linguistics (COLING-2000), pages 656-662,
Universitat des Saarlandes, July.
A M Ramsay. 1999. Does it make any sense? up-
dating = consistency checking. In K Turner, editor,
The Semantics-Pragmatics Interface from Different
Points of View, London and Amsterdam. Elsevier.
A M Ramsay. 2001. Theorem proving for untyped
constructive A-calculus: implementation and appli-
cation. Logic Journal of the Interest Group in Pure
and Applied Logics, 9(1):89-106.
J R Searle. 1969. Speech Acts: an Essay in the Phi-
losophy of Language. Cambridge University Press,
Cambridge.
R Turner. 1987. A theory of properties. Journal of
Symbolic Logic, 52(2):455-472.
R van der Sandt. 1992. Presupposition projection as
anaphora resolution. Journal of Semantics, 9:333-
377.
J van Eijck and H Alshawi. 1992. Logical forms.
In H Alshawi, editor, The Core Language En-
gine, pages 11-40, Cambridge, Mass. Bradford
Books/MIT Press.
Z. Vendler. 1967. Philosophy in Linguistics. Cornell
University Press, Ithaca.
J Wedekind. 1996. On inference-based procedures for
lexical disambiguation. In Proceedings of the 16th
International Conference on Computational Linguis-
tics (COLING-96), pages 980-985, Copenhagen.
</reference>
<page confidence="0.998356">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.999939">A Constructive View of Discourse Operators *</title>
<author confidence="0.998771">Allan</author>
<affiliation confidence="0.8740215">Department of UMIST, PO Box</affiliation>
<address confidence="0.995122">Manchester M60 1QD, UK</address>
<title confidence="0.502183">Allan.RamsayOumist.ac.uk</title>
<author confidence="0.993158">Helen</author>
<affiliation confidence="0.999875">Department of Computer University of</affiliation>
<address confidence="0.998523">Exeter, EX4 4PT, UK</address>
<email confidence="0.997554">H.L.GaylardOexeter.ac.uk</email>
<abstract confidence="0.958680430303032">Dialogue systems have to consider not just the propositional content of the user&apos;s utterances, but also the user&apos;s attitude to that content. It is common practice to treat these issues separately: we will argue that they can, and should, be dealt with at the same time and using the same mechanisms. 1 Interpretations, Proofs and Meaning Postulates Language is used to convey ideas — to produce in the hearer&apos;s mind a picture of the world that corresponds to the picture which is already in the speaker&apos;s. More than that, however, it is used to — bit of what I am saying is interesting, this bit undermines what you just said, this bit follows on from what you said, ... system that is to process respond to user utterances will have to be sensitive to the way that utterances encode attitudes. We suggest that the best way to capture this information is to build it into the basic meaning representation, and then to elaborate the consequences of using one construction rather than another in exactly the same way that you elaborate the consequences of using a specific lexical item. Consider for instance (1) and (2): (1) a. Mary and John got divorced in b. In March Mary and John got divorced. a. Mary and John got divorced in Mary and John got March. These sentences all report the same event — all have the same CONTENT. work was partially supported by EU grant IST-2000-29452 &amp;quot;Dynamic Universal Mobility for Adaptive Speech Interfaces&amp;quot; Nonetheless, they differ in the way they express the speaker&apos;s attitude to that content, or to parts of it. We can capture the propositional content itself constructing a FORM the usual way. The LF in Fig. 1 is fairly orthodox, and we will use it to illustrate what you have to do in order to include facts about the speaker&apos;s attitude. The details of how many 0-roles there should be&apos;, the deto deal with definite NPs by including REF- TERMS LF rather than treating them separately as constraints (Barwise and Perry, 1983; Kamp, 1984), the specific treatment of aspect are all open to debate, but nothing much hangs on these issues in the remainder of the paper and we will simply assume that they are at least defensible. faspect(simple, ref (ABpast(P)), A)} object, ref (AE(name(E, &amp;0(A, object, ref (AF(name(F, John)))) &amp;divorce(A) &amp;A is event &amp;in(A, ref (AG (name(G , March)))) Figure 1: Logical form for (la) LF on the basis of the meanings of the parts and their mode of combination. There is no alternative: when you hear an utterance, or a read a text, you have to base your understanding on what you hear key claim in this paper is that when you are trying to participate in a dialogue it is even more important to extract all the information that is encoded by the choice and arrangement of words than when you are merely trying to extract the propositional content. There is no point, however, in constructing an LF unless you link it to other things that you know. people to one another, they as- 1989; Dowty, 1991) for a detailed discussion of how many thematic roles there are. 103 sume that the other person will construct a rich model of what was said, incorporating a combination of general background knowledge and a model of what has already been said (by any participant in the conversation). This model goes by various names — the discourse model, the common ground, ... We follow fairly common practice in using the this object. The curious thing about the slate is that although both the speaker and hearer rely on having the same view of the slate, neither of them can directly inspect the other&apos;s version. Dialogues only flow coherently if both parties have very similar slates, but since neither of them can see directly into the other person&apos;s head they cannot be sure that they do. We take it, then, that the basic steps in assimilating an utterance are as follow: 1. construct a logical form that captures all and only the information explicitly encoded by the form of the utterance. 2. check that any claims that this logical form makes about the current slate are indeed supportable 3. update the slate to include the new information in the utterance If you get step 1 wrong, you&apos;ve got a problem. If you fail to capture any of the information encoded by the utterance, no amount of subsequent inference can retrieve it for you; and if you include anything erroneous you will have great trouble spotting it and removing it. You may need to note that there are alternative analyses, and either follow up one choice, with the possibility of backtracking when it turn out to be wrong, or somehow delay making a decision until it really matters (at which point you may have the information required for making an informed choice), but you do have to look closely at the utterance and get what you can from it. Step 2 involves verifying any presuppositions that are encoded in the utterance. (van der Sandt, 1992) argues that referential expressions are, in fact, a form of presupposition, and that determining the item &apos;pointed to&apos; by such an expression is just a by-product of the process of verifying the presupposition. We argue that if you are the hearer then this process involves checking that your version of the slate EH supports the same proofs as the speaker&apos;s Es. This is the closest you can get to inspecting each other&apos;s views of the slate — if they support the same proofs then they can&apos;t be all that different. The uniqueness element of definite NPs is particularly critical at this point — if supports a proof of exactly one EH does likewise then S and probably thinking about the same man, who can therefore be referred to as (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, from This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach steps 2 and 3, there is no doubt that you need substantial amounts of background knowledge. To find out what someone who utters (la) has in mind, you have to know that divorce is an action that involves terminating an earlier contract, and that usually when it happens the two parties concerned no longer love each other, and ... In other words, you need to have access to the kind of information contained in Fig. 2, and you need to be able to reason with this information. : VB : {0(A, object, B)} VC: {O(A, object, C) &amp;B C} ]Dmarry(D) O(D, object, B) &amp; O(D, object, C) &amp;termination(A,D) Figure 2: You can only get divorced if you were married We are thus working in a framework where we construct logical forms and use these to build models that support them, taking the model constructed at each stage to be the current version of the slate. For a variety of reasons we assume that it is convenient to use a fine-grained intensional logic as the formal substrate of this activity. The logic we use is a constructive version of (Turner, THEORY. intensional logics are available — (Bealer, 1989)&apos;s fine-grained intensional logic, non-well-founded set theory (Aczel, 1988), higher-order unification(Pulman, 1993). We choose property theory because it lends itself to a simple extension of a standard first-order theorem which is what we use for constructing mod- 104 els (Ramsay, 2001). 2 Discourse Operators as Attitude Reports Can we extend the update process described above so that it deals with the differences between the various versions of the report of John and Mary&apos;s in (2)? If we can, then we can make the critical information about how these sentence fit into the discourse as a whole available to our dialogue manager. Theme The first move is to see just what the differences are. We reconsider (1): (1) a. Mary and John got divorced in March. b. In March Mary and John got divorced. The difference between them lies in the fact that by putting the temporal modifier first in (lb) we are drawing attention to what haappened in March, rather than saying what John and Mary did. Informally we can say that (1a) is &apos;about&apos; John and Mary, and (lb) is about what happened in March. Formally we can say the same thing: about(AB(B.re f (AC (name(C, Mary))) B .re f (AD(name(D, John)))), : ref (AFpast(F)), E . AI(&amp;(A, object, I)) is (AJ(naine(J, March))))) Figure 3: (la) is &apos;about&apos; what happened to John and Mary about(AC(AD(C . D &amp; in(D, ref (AE(name(E, March)))))), : faspect(simple, ref (AFpast(F)), (B.AIO(I, object, ref (AJ(name(J, John)))) &amp;O(I, object, ref (Alf (name(K, Mary)))) is Figure 4: (lb) is &apos;about&apos; what happened in March We do this by noting, following (Halliday, 1985), that the leftmost phrasal daughter of an English — its — to be particularly significant. It is easy enough to spot which item is the leftmost phrasal daughter, so we can mark this item carefully when we are constructing our logical form. In fact all we have to do is to delay combining the meaning of the theme and the meaning of the rheme (everything else). Having obtained such a logical form, however, we have to specify the meaning of &apos;about&apos;. This comes in two parts: (i) we have to recover the normal content — if what you about John and Mary is that they got divorced then they did indeed get divorced — and (ii) we have to make use of the fact that one part of the utterance is marked as being more important. The first part of this is easy, given that we are using a highly intensional logic. Fig. 5 just says that the rheme holds of the theme, which recovers the standard meaning (in fact it just carries out the A-reduction step that we delayed when we constructed the logical form). VXVYabout(X,Y) —&gt; Y.X Figure 5: If I say Y about X then Y is true of X What do we do with the theme:rheme division itself? One obvious function is to help link the current utterence to an earlier one. If, for instance, the current utterance is an answer to an earlier question then they should have the same rheme: (3) Who got divorced in March? John Mary got divorced in March. (4) Who got divorced in March? Peter Susan got divorced in April. (4) doesn&apos;t make a sensible question:answer pair because the rheme of the answer is different from the rheme of the question. We can capture this with Fig. VUiVUi+janswer(Ui,U) VR(rheme(Ui, rheme(Ui+j, R)) 6: An answer should address the the question Similarly, if the current utterance is a narrative continuation of some previous utterance then the two are likely to share the same theme: VUiVUi+jnarrative(Ui, Ui+j) VT(therne(U. , theme(Ui+i , T)) Figure 7: The theme stays the same as a story unfolds Thus the theme:rheme distinction can be used to look for connections between utterances of the kind described in RST (Mann and Thompson, 1988; Mann, 1999). It is also worth noting that objects that this rule can also be used to the theme of an elliptical answer. 105 referred to within the theme are likely to be particularly salient in the discourse, so that they are good candidates for when you are trying to deref- 2.2 Focus The difference between (2a) and (2b) is in some ways similar. Again we have individual items picked out, the difference being that here we use intonation or typography rather than dislocation. Again the marked items are in some way interesting, and again we have to be able to reconstruct the basic propositional content. a. Mary and John got divorced in Mary and John got March. Isolating the semantics of the focussed item is a slightly more complicated process this time, but the resulting analyses are fairly similar: f ocus(AB(name(B, March)), : ref (ADpast(D)), 0(A, object, ref (AG(name(G, Mary)))) &amp;0(A, object, re f (AH(name(H, John)))) &amp;divorce(A)&amp;A is event &amp;in(A, ref (Al (C . I)))) Figure 8: They got divorced in March, not in April f ocus(AB(divorce(B)&amp; B is event &amp; in(B, ref (AC(name(C, March))))), : ref (AEpast(E)), A)} 0(A, object, ref (AH(name(H, Mary)))) &amp;0(A, object, ref (AI(name(I, John)))) . Figure 9: They got divorced, not married, in March As before, we have a rule for reconstructing the standard propositional content by combining the two parts that have been left unreduced, so that each of (2a) and (2b) entails that they did get divorced in March: VXVYfocus(X, Y) Y.X Figure 10: If I say Y about X then Y is true of X The extra information carried by the fact that one item is put in focus has to be consumed by some other operator. In cases like (2a) and (2b) where there is no overt discourse operator, we assume that the given sentence is being contrasted relationship between the theme:rheme distincand the phenomena investigated in and Weinstein, 1998) is rather unclear: what is clear is that devices for indicating which elements of an utterance are particularly prominent carries a great deal of information that can be used for organising an extended dialogue. with some proposition which is already present in the discourse, and which is now being denied. We therefore have the rule in Fig. 11, which picks out, via the referential term, some item which is curbelieved to satisfy claims that this is not in fact the case. Note the explicit reference to the slate E in this rule: the current utterance is being contrasted with some other salient proposition which is entailed by the slate. (f Q) (E 11: not hold of The act of focussing one element of the sentence, however, does not always indicate a direct contrast with some proposition in the discourse. Its task is to split the propositional content into two parts. In many cases, such as (5) and (6), there is an explicit lexical item which requires two such arguments. The assumption that there is a contrast should only be made when there is no such lexical item. a. I only borrowed your I only a. Susan even kissed She even The logical forms for (5a) and (6b) are given in Fig. 12 and Fig. 13: the corresponding pair are similar. only(AF(bike(F)), AGEH {H is interval Sr past(11)} EE : {ospect(simple, H, E)} BlE,agent,ref(XJspeaker(J))) 8,borrow(E) StE is event &amp;0(E, object, re f (AL(G.L, Figure 12: (5a): what I borrowed was your bike, not something else of yours even(AE(hug(E) &amp; E is event), AFEG : {G is interval &amp; past(G)} ED : {aspect(simple, G, D)} 0(D, agent, ref (Alf (I))) &amp;F.D &amp;O(D, object, re f (AKm(K)))) Figure 13: (6b): she hugged him, which was the last thing anyone expected her to do As usual, we have to say what follows from these operators. They are, as before, truth preserving, so 106 need the rule in Fig. 14. contrasts the current proposition with something which is already present in the discourse, with the extra constraint that this proposition should be in some &apos;stronger&apos; V AVB(only(A,B) B.A) 14: truth preserving V AV B(only(A, B) .re f (AA&apos; (A&apos; &gt;&gt; A&amp;B.A&apos;)) 15: hold of the &apos;stronger&apos; item A&apos; which you thought it did effect of is emphasise how unlikely the reported event is: we capture this with Fig. 16, which notes that there is some similar but more probable proposition: VAVB(even(A,B) prob(B.A)) 16: There is some similar A&apos; for which more likely than that components of the actual utterance, and hence would naturally be dealt with in its logical form, whereas the implied stress in (2) is a relationship utterances. Since the device used to mark the focussed elements is the same in all these cases, it is highly desirable to provide a uniform treatment by including the constrastive stress in the logical form as well. 2.3 Mood The theme:rheme distinction and the use of marked stress, then, enrich the propositional content of an utterance by linking it to the surrounding discourse and by expressing complex relations to various components. By capturing these facets of the meaning inside the logical form, we obtain a smooth connection between the two. can reasonably say think I stole it, but I only borrowed it&apos;, &apos;You I borit, but actually I only stole it&apos; strange: the notion of relative strength here is similar to that in (Gazdar, 1979)&apos;s use of &apos;expression alternatives&apos; for dealing with implicature, and to (Kruijff-Korbayova and Webber, 2001)&apos;s notion of &apos;alternative sets&apos; also that choosing the way the words are arranged allows you to express different attitudes to the truth of the proposition as a whole: (7) Did John and Mary get divorced in March? (8) Get divorced in March. The basic event type depicted by (7) is the same as the one depicted by (2) and (1), and the one depicted by (8) is clearly closely related. It is therefore worth seeing whether we can capture the attitudes underlying these two sentences within the logical form as well. As ever, we can easily include a term in the logical form which corresponds to the specified mood, just on the basis of the surface form: clairn(3A : laspect(simple, ref (ABpast(B)), A)} 0(A, object, ref (AE(name(E, Mary)))) Sze (A, object, ref (AF(riame(F, John)))) &amp;clivorce(A)&amp;A is event &amp;in(A, ref (AG(naine(G, March))))) Figure 17: (la): I&apos;m telling you it&apos;s true query(EA : {A is interval &amp;past(A)} EE : {aspect(simple, A, Ell 19(E, object, ref (XF(naine(F, Mary)))) &amp;O(E, object, ref (AG(name(G, John)))) &amp;divorce(E)&amp;E is event &amp;irt(E, ref (AH(riarrie(H, March))))) Figure 18: (7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference between (9a) and (9b). They both seem to claim if a woman and John sees John other words, &apos;any&apos; and seem to be universal quantifiers. In (10), however, the effects of &apos;every&apos; and &apos;any&apos; are very different: a. You can&apos;t invite John. He&apos;ll drink everything. b. You can invite John. He&apos;ll drink anything. The difference between (10a) and (10b) is that (10a) says that there will be a future state of affairs drinks everything&apos; true, whereas says that for any drink is a future of affairs where drinks D&apos; true. The fact that &apos;any&apos; has very wide scope is also reflected in the interpretation of negative sentences didn&apos;t get anything I wanted for Christmas&apos;, can be paraphrased as saying that if then I didn&apos;t get Christmas. Just how wide its scope is, however, becomes clear when we consider (11) and (12): (11) a. Do you know everyone at the party? b. Do you know anyone at the party? (12) a. Take any lane for Macclesfield. b. Take every lane for Macclesfield. 20 says that there is something like to do, but that I can&apos;t do it unless I have evidence that you know someone at the party. What kind of thing might I be able to do under those circumstances? If I&apos;m the host then I might go and look for them so that you have someone to talk to, if I&apos;m a fellow guest then I might ask you to introduce them to me, ...In general, however, when you ask a question it&apos;s because there is something that you could do if you knew the answer. The key point about Fig. 20 is that it says that it doesn&apos;t matter which person you know: for every one who at the party, if you know them then do whatever it is I have in mind. We thus encode the notion that &apos;any&apos; gives you a &apos;free choice&apos; (Vendler, 1967) within a perfectly standard logic. It is still just a universal quantifier, but it has very wide scope — wide enough to quantifying over questions that like the answer to, rather than as part of the queried proposition as would happen with (11a), as in Fig. 21. Similar considerations apply to (12a) and (12b). (12a) says that whichever lane you choose will be appropriate for getting to Macclesfield. (12b) 3C : lintend(speaker, do(speaker, C))1 VD : lone(D) at(D, ref (AE(party(E))))1 SF : IF is interval Sz past(F)} 3H : {aspect(simple, F, H)} B(H, agent, ref (AHhearer(1)))) &amp;know(H) 86H is event 860(H, object, D) do(speaker, C) Figure 20: There&apos;s something I could do if I knew that you knew someone at the party 3C :{intend(speaker, do(speaker, C))1 3D :{D is interval &amp; past(D)} VF : lone(F) &amp; at(F, ref (AG(party(G))))1 SH : laspect(sirnple, D, H)} 8(H, agent, ref (AI (hearer(I)))) kzknow(H)SzH is event SzO(H, object, F) do(speaker, C) Figure 21: There&apos;s something I could do if I knew that you knew everyone at the party doesn&apos;t really make any sense, since it seems to require you to drive in several lanes at once. We therefore paraphrase (12a) as in Fig. 22. V B : {lane(B)} use f ul(SAtake(A) SzA is event SzO(A, object, B) Szf or(A, ref (AC(name(C, Macc)))) Sth(A, agent, ref (ADspeaker(D)))) Figure 22: For any lane, it makes sense to take it if you want to go to Macclesfield The wide scope of &apos;any&apos; allows us to interpret (12a) as saying that there are several things that would be useful in the current situation, namely all the propositions that result from choosing a lane. We have weakened the force of the imperative to as is clear from the example, not all commands actually relate to things the speaker wants. As always, however, having introduced the our logical form we are under an obligation to provide an account of what it signifies. V P (use f ul(P) C:] {VS: {speaker(S)}want(S, : P Figure 23: Something is useful if it will help either the hearer or speaker achieve some goal Fig. 23 will usually be something that the speaker wants, but in examples like (12a) it may be 108 the hearer&apos;s underlying goal that will be satisfied by the specified action. Note that the logical form for (12a) said that certain kinds of situations would be useful. Fig. 23 then says that if a situation that described by the proposition useful then it be the case that be achievable in any situations of this general type. It is very hard indeed to see how the interactions between &apos;any&apos; and the various moods can be captured other than by including the mood in the logical form. (11b) involves quantifying over questions whose answer would satisfy my needs, (12a) involves quantifying over useful/desirable actions. If you try to separate the logical form and the mood you simply cannot retrieve this interaction. By including the mood inside the logical form, we make it possible to account for the discourse effects of &apos;any&apos;, and we continue to work with a unified two-stage framework — anchor the logical form and then think about its consequenes. This is particularly significant when considering the interactions between negation, mood and quantification. Consider (13): (13) Don&apos;t touch anything. We suggest the logical form in Fig. 24 for this. {thing(C)} use f ulHalA(D, agent, re f (AE speaker(E))) Sztouch(D) &amp;D is event 809(D, object, C))) Figure 24: For each thing C it would be helpful if you didn&apos;t touch C Ensuring the right relations between the universal the negation and be extemely difficult if they were not dealt with at the same time and in the same place. 3 Conclusions We have show that it is possible to capture various aspects of the speaker&apos;s attitude to what he is saying inside the logical form. We believe that it is also extremely convenient to do so. If you do not deal with information structure and intonation in the logical form, you will find it extremely difficult to localise their effects and coordinate them with the propositional content, since they dfeal with local elements of the propositional content (and in case of focus, they deal with so that you cannot rely on them to pick out discourse referents or other simple entities). If you do not deal with mood in the logical form you will simply find it impossible to cope with the interactions between illocutionary force and the quantifier &apos;any&apos; (so that you will not, for instance, be able react appropriately to a question such as I any messages about Viagra?&apos; a command delete any mails from John&apos;). deal with these cases by constructing models which reveal the critical issues. The logical form contains all the information encoded by the surface form. The inference engine unpacks this information in a way that makes it possible to plan appropriate responses. Computing logical forms of the kind shown above can be done using the standard compositional techniques. All the logical forms in this paper were obtained in this way, using the parser described in (Ramsay and Seville, 2000b). You have to make use of standard rescoping algorithms (van Eijck and Alshawi, 1992; Milward and Cooper, 1994) to ensure that the relations between the various discourse operators are handled correctly, but you need this anyway for handling scopes. Carrying out the required inference is harder. In (Ramsay and Seville, 2000a) we showed how to construct models by combining the literal content of an utterance, as encoded in the logical form, with a rich collection of background knowledge. To do this with the meaning postulates given above for discourse operators we need to be able to carry out this activity with meaning postulates couched in a higher-order logic. The theorem prover described in (?) allows us to do exactly that.</abstract>
<note confidence="0.847986666666666">References Aczel. 1988. Publications, Stanford. J F Allen and C R Perrault. 1980. Analysing intention utterances. Intelligence, Austin. 1962. to Do Things with Words. University Press, Oxford. Barwise and J Perry. 1983. and Attitudes. Bradford Books, Cambridge, MA. P Baumgartner and M Kuhn. 2000. Abducing corefby model construction. of Language Computation, G Bealer. 1989. Fine-grained type-free intensionality. In G Chierchia, B H Partee, and R Turner, types and meaning: vol I, founissues. Academic Publishers, Dordrecht/Boston/London. P Blackburn, J Bos, M Kohlhase, and H de Nivelle. 1997. Inference and computational semantics. In 109 Bunt and E Thijsse, editors, International on Computational Semantics, 5-19, University of Tilburg. Bunt and W J Black, editors. 2000. Beliefs and Context: Studies in Computational Prag- Benjamins, Amsterdam/Philadelphia. P R Cohen and H Levesque. 1980. Speech acts and the of shared plans. In Canadian Society for Computational Studies of Intelli- 263-270. P R Cohen and C R Perrault. 1979. Elements of a theory of speech acts. Science, 7(2):171-190. R Cohen, J Morgan, and M E Pollack. 1990. Inin Communication. Books, Cambridge, Mass. D R Dowty. 1989. On the semantic content of the notion of &apos;thematic role&apos;. In G Chierchia, B H Partee, R Turner, editors, Types and Mean- II: Semantic Issues, 69-130, Dordrecht. Kluwer Academic Press. D R Dowty. 1991. Thematic proto-roles and argument C Gardent and K Konrad. 2000. Interpreting definites model generation. of Language and H Gaylard and A M Ramsay. 2002. A unified theory of resolution. In Discourse and Dialogue Gazdar. 1979. Implicature, Presupposiand Logical Form. Press, New York.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Aczel</author>
</authors>
<title>Non-Well-Founded-Sets.</title>
<date>1988</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford.</location>
<contexts>
<context position="8013" citStr="Aczel, 1988" startWordPosition="1411" endWordPosition="1412">(A,D) Figure 2: You can only get divorced if you were married We are thus working in a framework where we construct logical forms and use these to build models that support them, taking the model constructed at each stage to be the current version of the slate. For a variety of reasons we assume that it is convenient to use a fine-grained intensional logic as the formal substrate of this activity. The logic we use is a constructive version of (Turner, 1987)&apos;s PROPERTY THEORY. Other intensional logics are available — (Bealer, 1989)&apos;s fine-grained intensional logic, non-well-founded set theory (Aczel, 1988), higher-order unification(Pulman, 1993). We choose property theory because it lends itself to a simple extension of a standard first-order theorem prover, which is what we use for constructing mod104 els (Ramsay, 2001). 2 Discourse Operators as Attitude Reports Can we extend the update process described above so that it deals with the differences between the various versions of the report of John and Mary&apos;s divorce in (1) and (2)? If we can, then we can make the critical information about how these sentence fit into the discourse as a whole available to our dialogue manager. 2.1 Theme &amp; Rheme</context>
</contexts>
<marker>Aczel, 1988</marker>
<rawString>P Aczel. 1988. Non-Well-Founded-Sets. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analysing intention in utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>15--148</pages>
<contexts>
<context position="19388" citStr="Allen and Perrault, 1980" startWordPosition="3366" endWordPosition="3369">(7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference between (9a) and (9b). They both seem to claim that if X is a woman and John sees X then John fancies X. In other words, &apos;any&apos; and &apos;every&apos; both seem to be universal quantifiers. In (10), however, the effects of &apos;every&apos; and &apos;any&apos; are very different: (10) a. You can&apos;t invite John. He&apos;ll drink everything. b</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>J F Allen and C R Perrault. 1980. Analysing intention in utterances. Artificial Intelligence, 15:148-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Austin</author>
</authors>
<title>How to Do Things with Words.</title>
<date>1962</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="19082" citStr="Austin, 1962" startWordPosition="3317" endWordPosition="3318"> &amp;in(A, ref (AG(naine(G, March))))) Figure 17: (la): I&apos;m telling you it&apos;s true query(EA : {A is interval &amp;past(A)} EE : {aspect(simple, A, Ell 19(E, object, ref (XF(naine(F, Mary)))) &amp;O(E, object, ref (AG(name(G, John)))) &amp;divorce(E)&amp;E is event &amp;irt(E, ref (AH(riarrie(H, March))))) Figure 18: (7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>J Austin. 1962. How to Do Things with Words. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>J Perry</author>
</authors>
<title>Situations and Attitudes.</title>
<date>1983</date>
<publisher>Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2359" citStr="Barwise and Perry, 1983" startWordPosition="401" endWordPosition="404">452 &amp;quot;Dynamic Universal Mobility for Adaptive Speech Interfaces&amp;quot; Nonetheless, they differ in the way they express the speaker&apos;s attitude to that content, or to parts of it. We can capture the propositional content itself by constructing a LOGICAL FORM in the usual way. The LF in Fig. 1 is fairly orthodox, and we will use it to illustrate what you have to do in order to include facts about the speaker&apos;s attitude. The details of how many 0-roles there should be&apos;, the decision to deal with definite NPs by including REFERENCE TERMS inside the LF rather than treating them separately as constraints (Barwise and Perry, 1983; Kamp, 1984), the specific treatment of aspect are all open to debate, but nothing much hangs on these issues in the remainder of the paper and we will simply assume that they are at least defensible. faspect(simple, ref (ABpast(P)), A)} 0(A, object, ref (AE(name(E, Mary))))2 &amp;0(A, object, ref (AF(name(F, John)))) &amp;divorce(A) &amp;A is event &amp;in(A, ref (AG (name(G , March)))) Figure 1: Logical form for (la) This LF was constructed COMPOSITIONALLY, i.e. on the basis of the meanings of the parts and their mode of combination. There is no alternative: when you hear an utterance, or a read a text, yo</context>
</contexts>
<marker>Barwise, Perry, 1983</marker>
<rawString>J Barwise and J Perry. 1983. Situations and Attitudes. Bradford Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Baumgartner</author>
<author>M Kuhn</author>
</authors>
<title>Abducing coreference by model construction.</title>
<date>2000</date>
<journal>Journal of Language and Computation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="6745" citStr="Baumgartner and Kuhn, 2000" startWordPosition="1187" endWordPosition="1190">herefore safely be referred to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach steps 2 and 3, there is no doubt that you need substantial amounts of background knowledge. To find out what someone who utters (la) has in mind, you have to know that divorce is an action that involves terminating an earlier contract, and that usually when it happens the two parties concerned no longer love each other, and ... In other words, you need to have access to the kind of information contained in Fig. 2, and you need to be able to reason with this information. VA : {divorce(A)} VB : {0(A, object, B)} VC: {O(A, object, C) &amp;B C} ]</context>
</contexts>
<marker>Baumgartner, Kuhn, 2000</marker>
<rawString>P Baumgartner and M Kuhn. 2000. Abducing coreference by model construction. Journal of Language and Computation, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bealer</author>
</authors>
<title>Fine-grained type-free intensionality. In</title>
<date>1989</date>
<editor>G Chierchia, B H Partee, and R Turner, editors,</editor>
<publisher>Kluwer Academic Publishers, Dordrecht/Boston/London.</publisher>
<contexts>
<context position="7937" citStr="Bealer, 1989" startWordPosition="1402" endWordPosition="1403">A, object, C) &amp;B C} ]Dmarry(D) O(D, object, B) &amp; O(D, object, C) &amp;termination(A,D) Figure 2: You can only get divorced if you were married We are thus working in a framework where we construct logical forms and use these to build models that support them, taking the model constructed at each stage to be the current version of the slate. For a variety of reasons we assume that it is convenient to use a fine-grained intensional logic as the formal substrate of this activity. The logic we use is a constructive version of (Turner, 1987)&apos;s PROPERTY THEORY. Other intensional logics are available — (Bealer, 1989)&apos;s fine-grained intensional logic, non-well-founded set theory (Aczel, 1988), higher-order unification(Pulman, 1993). We choose property theory because it lends itself to a simple extension of a standard first-order theorem prover, which is what we use for constructing mod104 els (Ramsay, 2001). 2 Discourse Operators as Attitude Reports Can we extend the update process described above so that it deals with the differences between the various versions of the report of John and Mary&apos;s divorce in (1) and (2)? If we can, then we can make the critical information about how these sentence fit into t</context>
</contexts>
<marker>Bealer, 1989</marker>
<rawString>G Bealer. 1989. Fine-grained type-free intensionality. In G Chierchia, B H Partee, and R Turner, editors, Properties, types and meaning: vol I, foundational issues. Kluwer Academic Publishers, Dordrecht/Boston/London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
<author>J Bos</author>
<author>M Kohlhase</author>
<author>H de Nivelle</author>
</authors>
<title>Inference and computational semantics.</title>
<date>1997</date>
<booktitle>In H Bunt and E Thijsse, editors, 3rd International Workshop on Computational Semantics,</booktitle>
<pages>5--19</pages>
<institution>University of Tilburg.</institution>
<marker>Blackburn, Bos, Kohlhase, de Nivelle, 1997</marker>
<rawString>P Blackburn, J Bos, M Kohlhase, and H de Nivelle. 1997. Inference and computational semantics. In H Bunt and E Thijsse, editors, 3rd International Workshop on Computational Semantics, pages 5-19, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<date>2000</date>
<booktitle>Abduction, Beliefs and Context: Studies in Computational Pragmatics. John Benjamins,</booktitle>
<editor>H Bunt and W J Black, editors.</editor>
<location>Amsterdam/Philadelphia.</location>
<marker>2000</marker>
<rawString>H Bunt and W J Black, editors. 2000. Abduction, Beliefs and Context: Studies in Computational Pragmatics. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>H Levesque</author>
</authors>
<title>Speech acts and the recognition of shared plans.</title>
<date>1980</date>
<booktitle>In Proceedings, Canadian Society for Computational Studies of Intelligence,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="19342" citStr="Cohen and Levesque, 1980" startWordPosition="3358" endWordPosition="3361">t(E, ref (AH(riarrie(H, March))))) Figure 18: (7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference between (9a) and (9b). They both seem to claim that if X is a woman and John sees X then John fancies X. In other words, &apos;any&apos; and &apos;every&apos; both seem to be universal quantifiers. In (10), however, the effects of &apos;every&apos; and &apos;any&apos; are very different: (10) a. Yo</context>
</contexts>
<marker>Cohen, Levesque, 1980</marker>
<rawString>P R Cohen and H Levesque. 1980. Speech acts and the recognition of shared plans. In Proceedings, Canadian Society for Computational Studies of Intelligence, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>C R Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>7--2</pages>
<contexts>
<context position="19316" citStr="Cohen and Perrault, 1979" startWordPosition="3354" endWordPosition="3357">&amp;divorce(E)&amp;E is event &amp;irt(E, ref (AH(riarrie(H, March))))) Figure 18: (7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference between (9a) and (9b). They both seem to claim that if X is a woman and John sees X then John fancies X. In other words, &apos;any&apos; and &apos;every&apos; both seem to be universal quantifiers. In (10), however, the effects of &apos;every&apos; and &apos;any&apos; are </context>
</contexts>
<marker>Cohen, Perrault, 1979</marker>
<rawString>P R Cohen and C R Perrault. 1979. Elements of a plan-based theory of speech acts. Cognitive Science, 7(2):171-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>J Morgan</author>
<author>M E Pollack</author>
</authors>
<title>Intentions in Communication.</title>
<date>1990</date>
<publisher>Bradford Books,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="19362" citStr="Cohen et al., 1990" startWordPosition="3362" endWordPosition="3365">rch))))) Figure 18: (7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference between (9a) and (9b). They both seem to claim that if X is a woman and John sees X then John fancies X. In other words, &apos;any&apos; and &apos;every&apos; both seem to be universal quantifiers. In (10), however, the effects of &apos;every&apos; and &apos;any&apos; are very different: (10) a. You can&apos;t invite John.</context>
</contexts>
<marker>Cohen, Morgan, Pollack, 1990</marker>
<rawString>P R Cohen, J Morgan, and M E Pollack. 1990. Intentions in Communication. Bradford Books, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>On the semantic content of the notion of &apos;thematic role&apos;.</title>
<date>1989</date>
<booktitle>Properties, Types and Meaning II: Semantic Issues,</booktitle>
<pages>69--130</pages>
<editor>In G Chierchia, B H Partee, and R Turner, editors,</editor>
<publisher>Kluwer Academic Press.</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3448" citStr="Dowty, 1989" startWordPosition="594" endWordPosition="595">ings of the parts and their mode of combination. There is no alternative: when you hear an utterance, or a read a text, you have to base your understanding on what you hear or see. The key claim in this paper is that when you are trying to participate in a dialogue it is even more important to extract all the information that is encoded by the choice and arrangement of words than when you are merely trying to extract the propositional content. There is no point, however, in constructing an LF unless you link it to other things that you know. When people say things to one another, they as&amp;quot;see (Dowty, 1989; Dowty, 1991) for a detailed discussion of how many thematic roles there are. 103 sume that the other person will construct a rich model of what was said, incorporating a combination of general background knowledge and a model of what has already been said (by any participant in the conversation). This model goes by various names — the discourse model, the common ground, ... We follow fairly common practice in using the term SLATE for this object. The curious thing about the slate is that although both the speaker and hearer rely on having the same view of the slate, neither of them can direc</context>
</contexts>
<marker>Dowty, 1989</marker>
<rawString>D R Dowty. 1989. On the semantic content of the notion of &apos;thematic role&apos;. In G Chierchia, B H Partee, and R Turner, editors, Properties, Types and Meaning II: Semantic Issues, pages 69-130, Dordrecht. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>Thematic proto-roles and argument selection.</title>
<date>1991</date>
<journal>Language,</journal>
<pages>67--547</pages>
<contexts>
<context position="3462" citStr="Dowty, 1991" startWordPosition="596" endWordPosition="597">arts and their mode of combination. There is no alternative: when you hear an utterance, or a read a text, you have to base your understanding on what you hear or see. The key claim in this paper is that when you are trying to participate in a dialogue it is even more important to extract all the information that is encoded by the choice and arrangement of words than when you are merely trying to extract the propositional content. There is no point, however, in constructing an LF unless you link it to other things that you know. When people say things to one another, they as&amp;quot;see (Dowty, 1989; Dowty, 1991) for a detailed discussion of how many thematic roles there are. 103 sume that the other person will construct a rich model of what was said, incorporating a combination of general background knowledge and a model of what has already been said (by any participant in the conversation). This model goes by various names — the discourse model, the common ground, ... We follow fairly common practice in using the term SLATE for this object. The curious thing about the slate is that although both the speaker and hearer rely on having the same view of the slate, neither of them can directly inspect th</context>
</contexts>
<marker>Dowty, 1991</marker>
<rawString>D R Dowty. 1991. Thematic proto-roles and argument selection. Language, 67:547-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
<author>K Konrad</author>
</authors>
<title>Interpreting definites using model generation.</title>
<date>2000</date>
<journal>Journal of Language and Computation,</journal>
<pages>1--2</pages>
<contexts>
<context position="6772" citStr="Gardent and Konrad, 2000" startWordPosition="1191" endWordPosition="1194">to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach steps 2 and 3, there is no doubt that you need substantial amounts of background knowledge. To find out what someone who utters (la) has in mind, you have to know that divorce is an action that involves terminating an earlier contract, and that usually when it happens the two parties concerned no longer love each other, and ... In other words, you need to have access to the kind of information contained in Fig. 2, and you need to be able to reason with this information. VA : {divorce(A)} VB : {0(A, object, B)} VC: {O(A, object, C) &amp;B C} ]Dmarry(D) O(D, object, B) &amp;</context>
</contexts>
<marker>Gardent, Konrad, 2000</marker>
<rawString>C Gardent and K Konrad. 2000. Interpreting definites using model generation. Journal of Language and Computation, 1(2):215-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaylard</author>
<author>A M Ramsay</author>
</authors>
<title>A unified theory of reference resolution.</title>
<date>2002</date>
<booktitle>In 4th Discourse and Dialogue Colloquium,</booktitle>
<location>Lisbon.</location>
<contexts>
<context position="6203" citStr="Gaylard and Ramsay, 2002" startWordPosition="1089" endWordPosition="1092">g the presupposition. We argue that if you are the hearer then this process involves checking that your version of the slate EH supports the same proofs as the speaker&apos;s Es. This is the closest you can get to inspecting each other&apos;s views of the slate — if they support the same proofs then they can&apos;t be all that different. The uniqueness element of definite NPs is particularly critical at this point — if Es supports a proof of man(t) for exactly one t and EH does likewise then S and H are probably thinking about the same man, who can therefore safely be referred to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach st</context>
</contexts>
<marker>Gaylard, Ramsay, 2002</marker>
<rawString>H Gaylard and A M Ramsay. 2002. A unified theory of reference resolution. In 4th Discourse and Dialogue Colloquium, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Pragmatics: Implicature, Presupposition and Logical Form.</title>
<date>1979</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="17548" citStr="Gazdar, 1979" startWordPosition="3062" endWordPosition="3063">stive stress in the logical form as well. 2.3 Mood The theme:rheme distinction and the use of marked stress, then, enrich the propositional content of an utterance by linking it to the surrounding discourse and by expressing complex relations to various components. By capturing these facets of the meaning inside the logical form, we obtain a smooth connection between the two. 5You can reasonably say &apos;You think I stole it, but actually I only borrowed it&apos;, whereas &apos;You think I borrowed it, but actually I only stole it&apos; sounds strange: the notion of relative strength here is similar to that in (Gazdar, 1979)&apos;s use of &apos;expression alternatives&apos; for dealing with implicature, and to (Kruijff-Korbayova and Webber, 2001)&apos;s notion of &apos;alternative sets&apos; It is also clear that choosing the way the words are arranged allows you to express different attitudes to the truth of the proposition as a whole: (7) Did John and Mary get divorced in March? (8) Get divorced in March. The basic event type depicted by (7) is the same as the one depicted by (2) and (1), and the one depicted by (8) is clearly closely related. It is therefore worth seeing whether we can capture the attitudes underlying these two sentences w</context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>G Gazdar. 1979. Pragmatics: Implicature, Presupposition and Logical Form. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar.</title>
<date>1985</date>
<location>Arnold, London.</location>
<contexts>
<context position="9667" citStr="Halliday, 1985" startWordPosition="1686" endWordPosition="1687">ned in March. Formally we can say the same thing: about(AB(B.re f (AC (name(C, Mary))) B .re f (AD(name(D, John)))), .X.EEA : faspect(simple, ref (AFpast(F)), A)} E . AI(&amp;(A, object, I)) Sidivorce(A)&amp;A is event &amp;in(A, ref (AJ(naine(J, March))))) Figure 3: (la) is &apos;about&apos; what happened to John and Mary about(AC(AD(C . D &amp; in(D, ref (AE(name(E, March)))))), ABa.4. : faspect(simple, ref (AFpast(F)), A)} (B.AIO(I, object, ref (AJ(name(J, John)))) &amp;O(I, object, ref (Alf (name(K, Mary)))) Szclivorce(I)8zI is event).A) Figure 4: (lb) is &apos;about&apos; what happened in March We do this by noting, following (Halliday, 1985), that the leftmost phrasal daughter of an English sentence — its THEME — seems to be particularly significant. It is easy enough to spot which item is the leftmost phrasal daughter, so we can mark this item carefully when we are constructing our logical form. In fact all we have to do is to delay combining the meaning of the theme and the meaning of the rheme (everything else). Having obtained such a logical form, however, we have to specify the meaning of &apos;about&apos;. This comes in two parts: (i) we have to recover the normal propositional content — if what I tell you about John and Mary is that</context>
</contexts>
<marker>Halliday, 1985</marker>
<rawString>M A K Halliday. 1985. An Introduction to Functional Grammar. Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Formal systems for complexity and control of inference: A reprise and some hints.</title>
<date>1998</date>
<booktitle>Centering Theory in Discourse,</booktitle>
<pages>31--38</pages>
<editor>In M. A. Walker, A. K. Joshi, and E. F. Prince, editors,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="13859" citStr="Joshi and Weinstein, 1998" startWordPosition="2419" endWordPosition="2422">reconstructing the standard propositional content by combining the two parts that have been left unreduced, so that each of (2a) and (2b) entails that they did get divorced in March: VXVYfocus(X, Y) Y.X Figure 10: If I say Y about X then Y is true of X The extra information carried by the fact that one item is put in focus has to be consumed by some other operator. In cases like (2a) and (2b) where there is no overt discourse operator, we assume that the given sentence is being contrasted 4The relationship between the theme:rheme distinction and the phenomena investigated in CENTERING THEORY (Joshi and Weinstein, 1998) is rather unclear: what is clear is that devices for indicating which elements of an utterance are particularly prominent carries a great deal of information that can be used for organising an extended dialogue. with some proposition which is already present in the discourse, and which is now being denied. We therefore have the rule in Fig. 11, which picks out, via the referential term, some item which is currently believed to satisfy Q and claims that this is not in fact the case. Note the explicit reference to the slate E in this rule: the current utterance is being contrasted with some oth</context>
</contexts>
<marker>Joshi, Weinstein, 1998</marker>
<rawString>A Joshi and S Weinstein. 1998. Formal systems for complexity and control of inference: A reprise and some hints. In M. A. Walker, A. K. Joshi, and E. F. Prince, editors, Centering Theory in Discourse, pages 31-38. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
</authors>
<title>A theory of truth and semantic representation.</title>
<date>1984</date>
<journal>In J A G Groenendijk, T M V Janssen, and M B</journal>
<booktitle>Formal Methods in the Study of Language,</booktitle>
<pages>277--322</pages>
<editor>J Stokhof, editors,</editor>
<publisher>Foris Publications.</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="2372" citStr="Kamp, 1984" startWordPosition="405" endWordPosition="406">bility for Adaptive Speech Interfaces&amp;quot; Nonetheless, they differ in the way they express the speaker&apos;s attitude to that content, or to parts of it. We can capture the propositional content itself by constructing a LOGICAL FORM in the usual way. The LF in Fig. 1 is fairly orthodox, and we will use it to illustrate what you have to do in order to include facts about the speaker&apos;s attitude. The details of how many 0-roles there should be&apos;, the decision to deal with definite NPs by including REFERENCE TERMS inside the LF rather than treating them separately as constraints (Barwise and Perry, 1983; Kamp, 1984), the specific treatment of aspect are all open to debate, but nothing much hangs on these issues in the remainder of the paper and we will simply assume that they are at least defensible. faspect(simple, ref (ABpast(P)), A)} 0(A, object, ref (AE(name(E, Mary))))2 &amp;0(A, object, ref (AF(name(F, John)))) &amp;divorce(A) &amp;A is event &amp;in(A, ref (AG (name(G , March)))) Figure 1: Logical form for (la) This LF was constructed COMPOSITIONALLY, i.e. on the basis of the meanings of the parts and their mode of combination. There is no alternative: when you hear an utterance, or a read a text, you have to bas</context>
</contexts>
<marker>Kamp, 1984</marker>
<rawString>H Kamp. 1984. A theory of truth and semantic representation. In J A G Groenendijk, T M V Janssen, and M B J Stokhof, editors, Formal Methods in the Study of Language, pages 277-322, Dordrecht. Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kruijff-Korbayova</author>
<author>B L Webber</author>
</authors>
<title>Concession, implicature and alternative sets.</title>
<date>2001</date>
<booktitle>4th International Workshop on Computational Semantics,</booktitle>
<pages>227--248</pages>
<editor>In H Bunt, I van der Sluis, and E Thijsse, editors,</editor>
<institution>University of Tilburg.</institution>
<contexts>
<context position="17657" citStr="Kruijff-Korbayova and Webber, 2001" startWordPosition="3074" endWordPosition="3077">use of marked stress, then, enrich the propositional content of an utterance by linking it to the surrounding discourse and by expressing complex relations to various components. By capturing these facets of the meaning inside the logical form, we obtain a smooth connection between the two. 5You can reasonably say &apos;You think I stole it, but actually I only borrowed it&apos;, whereas &apos;You think I borrowed it, but actually I only stole it&apos; sounds strange: the notion of relative strength here is similar to that in (Gazdar, 1979)&apos;s use of &apos;expression alternatives&apos; for dealing with implicature, and to (Kruijff-Korbayova and Webber, 2001)&apos;s notion of &apos;alternative sets&apos; It is also clear that choosing the way the words are arranged allows you to express different attitudes to the truth of the proposition as a whole: (7) Did John and Mary get divorced in March? (8) Get divorced in March. The basic event type depicted by (7) is the same as the one depicted by (2) and (1), and the one depicted by (8) is clearly closely related. It is therefore worth seeing whether we can capture the attitudes underlying these two sentences within the logical form as well. As ever, we can easily include a term in the logical form which corresponds t</context>
</contexts>
<marker>Kruijff-Korbayova, Webber, 2001</marker>
<rawString>I Kruijff-Korbayova and B L Webber. 2001. Concession, implicature and alternative sets. In H Bunt, I van der Sluis, and E Thijsse, editors, 4th International Workshop on Computational Semantics, pages 227-248, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<contexts>
<context position="11836" citStr="Mann and Thompson, 1988" startWordPosition="2073" endWordPosition="2076">se the rheme of the answer is different from the rheme of the question. We can capture this constraint with Fig. 63. VUiVUi+janswer(Ui,U) VR(rheme(Ui, rheme(Ui+j, R)) Figure 6: An answer should address the rheme of the question Similarly, if the current utterance is a narrative continuation of some previous utterance then the two are likely to share the same theme: VUiVUi+jnarrative(Ui, Ui+j) VT(therne(U. , theme(Ui+i , T)) Figure 7: The theme stays the same as a story unfolds Thus the theme:rheme distinction can be used to look for connections between utterances of the kind described in RST (Mann and Thompson, 1988; Mann, 1999). It is also worth noting that objects 3Note that this rule can also be used to reconstruct the theme of an elliptical answer. 105 referred to within the theme are likely to be particularly salient in the discourse, so that they are good candidates for when you are trying to dereference pronouns4. 2.2 Focus The difference between (2a) and (2b) is in some ways similar. Again we have individual items picked out, the difference being that here we use intonation or typography rather than dislocation. Again the marked items are in some way interesting, and again we have to be able to r</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W C Mann and S A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3) :243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
</authors>
<title>An introduction to rhetorical structure theory.</title>
<date>1999</date>
<contexts>
<context position="11849" citStr="Mann, 1999" startWordPosition="2077" endWordPosition="2078">r is different from the rheme of the question. We can capture this constraint with Fig. 63. VUiVUi+janswer(Ui,U) VR(rheme(Ui, rheme(Ui+j, R)) Figure 6: An answer should address the rheme of the question Similarly, if the current utterance is a narrative continuation of some previous utterance then the two are likely to share the same theme: VUiVUi+jnarrative(Ui, Ui+j) VT(therne(U. , theme(Ui+i , T)) Figure 7: The theme stays the same as a story unfolds Thus the theme:rheme distinction can be used to look for connections between utterances of the kind described in RST (Mann and Thompson, 1988; Mann, 1999). It is also worth noting that objects 3Note that this rule can also be used to reconstruct the theme of an elliptical answer. 105 referred to within the theme are likely to be particularly salient in the discourse, so that they are good candidates for when you are trying to dereference pronouns4. 2.2 Focus The difference between (2a) and (2b) is in some ways similar. Again we have individual items picked out, the difference being that here we use intonation or typography rather than dislocation. Again the marked items are in some way interesting, and again we have to be able to reconstruct th</context>
</contexts>
<marker>Mann, 1999</marker>
<rawString>W C Mann. 1999. An introduction to rhetorical structure theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
<author>R Cooper</author>
</authors>
<title>Incremental interpretation: Applications, theory and relationship to dynamic semantics.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94),</booktitle>
<pages>88748--754</pages>
<location>Kyoto.</location>
<contexts>
<context position="26822" citStr="Milward and Cooper, 1994" startWordPosition="4701" endWordPosition="4704">any mails from John&apos;). We deal with these cases by constructing models which reveal the critical issues. The logical form contains all the information encoded by the surface form. The inference engine unpacks this information in a way that makes it possible to plan appropriate responses. Computing logical forms of the kind shown above can be done using the standard compositional techniques. All the logical forms in this paper were obtained in this way, using the parser described in (Ramsay and Seville, 2000b). You have to make use of standard rescoping algorithms (van Eijck and Alshawi, 1992; Milward and Cooper, 1994) to ensure that the relations between the various discourse operators are handled correctly, but you need this anyway for handling scopes. Carrying out the required inference is harder. In (Ramsay and Seville, 2000a) we showed how to construct models by combining the literal content of an utterance, as encoded in the logical form, with a rich collection of background knowledge. To do this with the meaning postulates given above for discourse operators we need to be able to carry out this activity with meaning postulates couched in a higher-order logic. The theorem prover described in (?) allow</context>
</contexts>
<marker>Milward, Cooper, 1994</marker>
<rawString>D Milward and R Cooper. 1994. Incremental interpretation: Applications, theory and relationship to dynamic semantics. In Proceedings of the 15th International Conference on Computational Linguistics (COLING-94), pages 88748-754, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
</authors>
<title>Higher order unification and the interpretation of focus. Linguistics flu Philosophy,</title>
<date>1993</date>
<pages>20--1</pages>
<contexts>
<context position="8053" citStr="Pulman, 1993" startWordPosition="1414" endWordPosition="1415">ed if you were married We are thus working in a framework where we construct logical forms and use these to build models that support them, taking the model constructed at each stage to be the current version of the slate. For a variety of reasons we assume that it is convenient to use a fine-grained intensional logic as the formal substrate of this activity. The logic we use is a constructive version of (Turner, 1987)&apos;s PROPERTY THEORY. Other intensional logics are available — (Bealer, 1989)&apos;s fine-grained intensional logic, non-well-founded set theory (Aczel, 1988), higher-order unification(Pulman, 1993). We choose property theory because it lends itself to a simple extension of a standard first-order theorem prover, which is what we use for constructing mod104 els (Ramsay, 2001). 2 Discourse Operators as Attitude Reports Can we extend the update process described above so that it deals with the differences between the various versions of the report of John and Mary&apos;s divorce in (1) and (2)? If we can, then we can make the critical information about how these sentence fit into the discourse as a whole available to our dialogue manager. 2.1 Theme &amp; Rheme The first move is to see just what the </context>
</contexts>
<marker>Pulman, 1993</marker>
<rawString>S G Pulman. 1993. Higher order unification and the interpretation of focus. Linguistics flu Philosophy, 20(1):73-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
<author>H Seville</author>
</authors>
<title>Models and discourse models.</title>
<date>2000</date>
<journal>Journal of Language and Computation,</journal>
<pages>1--2</pages>
<contexts>
<context position="6634" citStr="Ramsay and Seville, 2000" startWordPosition="1169" endWordPosition="1172">an(t) for exactly one t and EH does likewise then S and H are probably thinking about the same man, who can therefore safely be referred to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach steps 2 and 3, there is no doubt that you need substantial amounts of background knowledge. To find out what someone who utters (la) has in mind, you have to know that divorce is an action that involves terminating an earlier contract, and that usually when it happens the two parties concerned no longer love each other, and ... In other words, you need to have access to the kind of information contained in Fig. 2, and you need to</context>
<context position="26709" citStr="Ramsay and Seville, 2000" startWordPosition="4683" endWordPosition="4686">eact appropriately to a question such as &apos;Do I have any messages about Viagra?&apos; oe a command like &apos;Don&apos;t delete any mails from John&apos;). We deal with these cases by constructing models which reveal the critical issues. The logical form contains all the information encoded by the surface form. The inference engine unpacks this information in a way that makes it possible to plan appropriate responses. Computing logical forms of the kind shown above can be done using the standard compositional techniques. All the logical forms in this paper were obtained in this way, using the parser described in (Ramsay and Seville, 2000b). You have to make use of standard rescoping algorithms (van Eijck and Alshawi, 1992; Milward and Cooper, 1994) to ensure that the relations between the various discourse operators are handled correctly, but you need this anyway for handling scopes. Carrying out the required inference is harder. In (Ramsay and Seville, 2000a) we showed how to construct models by combining the literal content of an utterance, as encoded in the logical form, with a rich collection of background knowledge. To do this with the meaning postulates given above for discourse operators we need to be able to carry out</context>
</contexts>
<marker>Ramsay, Seville, 2000</marker>
<rawString>A M Ramsay and H Seville. 2000a. Models and discourse models. Journal of Language and Computation, 1(2):167-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
<author>H Seville</author>
</authors>
<title>Unscrambling English word order.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000),</booktitle>
<pages>656--662</pages>
<editor>In M Kay, editor,</editor>
<contexts>
<context position="6634" citStr="Ramsay and Seville, 2000" startWordPosition="1169" endWordPosition="1172">an(t) for exactly one t and EH does likewise then S and H are probably thinking about the same man, who can therefore safely be referred to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach steps 2 and 3, there is no doubt that you need substantial amounts of background knowledge. To find out what someone who utters (la) has in mind, you have to know that divorce is an action that involves terminating an earlier contract, and that usually when it happens the two parties concerned no longer love each other, and ... In other words, you need to have access to the kind of information contained in Fig. 2, and you need to</context>
<context position="26709" citStr="Ramsay and Seville, 2000" startWordPosition="4683" endWordPosition="4686">eact appropriately to a question such as &apos;Do I have any messages about Viagra?&apos; oe a command like &apos;Don&apos;t delete any mails from John&apos;). We deal with these cases by constructing models which reveal the critical issues. The logical form contains all the information encoded by the surface form. The inference engine unpacks this information in a way that makes it possible to plan appropriate responses. Computing logical forms of the kind shown above can be done using the standard compositional techniques. All the logical forms in this paper were obtained in this way, using the parser described in (Ramsay and Seville, 2000b). You have to make use of standard rescoping algorithms (van Eijck and Alshawi, 1992; Milward and Cooper, 1994) to ensure that the relations between the various discourse operators are handled correctly, but you need this anyway for handling scopes. Carrying out the required inference is harder. In (Ramsay and Seville, 2000a) we showed how to construct models by combining the literal content of an utterance, as encoded in the logical form, with a rich collection of background knowledge. To do this with the meaning postulates given above for discourse operators we need to be able to carry out</context>
</contexts>
<marker>Ramsay, Seville, 2000</marker>
<rawString>A M Ramsay and H Seville. 2000b. Unscrambling English word order. In M Kay, editor, Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000), pages 656-662, Universitat des Saarlandes, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
</authors>
<title>Does it make any sense? updating = consistency checking. In</title>
<date>1999</date>
<booktitle>The Semantics-Pragmatics Interface from Different Points of View, London and Amsterdam.</booktitle>
<editor>K Turner, editor,</editor>
<publisher>Elsevier.</publisher>
<contexts>
<context position="6176" citStr="Ramsay, 1999" startWordPosition="1087" endWordPosition="1088">ss of verifying the presupposition. We argue that if you are the hearer then this process involves checking that your version of the slate EH supports the same proofs as the speaker&apos;s Es. This is the closest you can get to inspecting each other&apos;s views of the slate — if they support the same proofs then they can&apos;t be all that different. The uniqueness element of definite NPs is particularly critical at this point — if Es supports a proof of man(t) for exactly one t and EH does likewise then S and H are probably thinking about the same man, who can therefore safely be referred to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No</context>
</contexts>
<marker>Ramsay, 1999</marker>
<rawString>A M Ramsay. 1999. Does it make any sense? updating = consistency checking. In K Turner, editor, The Semantics-Pragmatics Interface from Different Points of View, London and Amsterdam. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Ramsay</author>
</authors>
<title>Theorem proving for untyped constructive A-calculus: implementation and application.</title>
<date>2001</date>
<booktitle>Logic Journal of the Interest Group in Pure and Applied Logics,</booktitle>
<pages>9--1</pages>
<contexts>
<context position="8232" citStr="Ramsay, 2001" startWordPosition="1444" endWordPosition="1445">age to be the current version of the slate. For a variety of reasons we assume that it is convenient to use a fine-grained intensional logic as the formal substrate of this activity. The logic we use is a constructive version of (Turner, 1987)&apos;s PROPERTY THEORY. Other intensional logics are available — (Bealer, 1989)&apos;s fine-grained intensional logic, non-well-founded set theory (Aczel, 1988), higher-order unification(Pulman, 1993). We choose property theory because it lends itself to a simple extension of a standard first-order theorem prover, which is what we use for constructing mod104 els (Ramsay, 2001). 2 Discourse Operators as Attitude Reports Can we extend the update process described above so that it deals with the differences between the various versions of the report of John and Mary&apos;s divorce in (1) and (2)? If we can, then we can make the critical information about how these sentence fit into the discourse as a whole available to our dialogue manager. 2.1 Theme &amp; Rheme The first move is to see just what the differences are. We reconsider (1): (1) a. Mary and John got divorced in March. b. In March Mary and John got divorced. The difference between them lies in the fact that by puttin</context>
</contexts>
<marker>Ramsay, 2001</marker>
<rawString>A M Ramsay. 2001. Theorem proving for untyped constructive A-calculus: implementation and application. Logic Journal of the Interest Group in Pure and Applied Logics, 9(1):89-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech Acts: an Essay in the Philosophy of Language.</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="19097" citStr="Searle, 1969" startWordPosition="3319" endWordPosition="3320">G(naine(G, March))))) Figure 17: (la): I&apos;m telling you it&apos;s true query(EA : {A is interval &amp;past(A)} EE : {aspect(simple, A, Ell 19(E, object, ref (XF(naine(F, Mary)))) &amp;O(E, object, ref (AG(name(G, John)))) &amp;divorce(E)&amp;E is event &amp;irt(E, ref (AH(riarrie(H, March))))) Figure 18: (7): I don&apos;t know whether it&apos;s true use fulOGO(C, object, ref (ADhearer(D))) &amp;divorce(C) &amp;C is event &amp;in(C, ref (AF(name(F, March))))) Figure 19: (8): I&apos;d like you to make it true And as ever, having included such terms we have to devise rules to account for their effects. Most treatments of mood follow (Austin, 1962; Searle, 1969) in assuming that there is a connection between the overt &apos;force&apos; of the utterance and the speaker&apos;s underlying goals, with a substantial tradition of AT work linking this to Al planning theory (Cohen and Perrault, 1979; Cohen and Levesque, 1980; Cohen et al., 1990; Allen and Perrault, 1980; Bunt and Black, 2000). We will argue that this connection has to be made explicit in the logical form, rather than being dealt with as a separate phenomenon. Consider (9): (9) a. John fancies every woman he sees. b. John fancies any woman he sees. 107 There doesn&apos;t seem to be much difference between (9a) a</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>J R Searle. 1969. Speech Acts: an Essay in the Philosophy of Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Turner</author>
</authors>
<title>A theory of properties.</title>
<date>1987</date>
<journal>Journal of Symbolic Logic,</journal>
<pages>52--2</pages>
<contexts>
<context position="7862" citStr="Turner, 1987" startWordPosition="1391" endWordPosition="1392">son with this information. VA : {divorce(A)} VB : {0(A, object, B)} VC: {O(A, object, C) &amp;B C} ]Dmarry(D) O(D, object, B) &amp; O(D, object, C) &amp;termination(A,D) Figure 2: You can only get divorced if you were married We are thus working in a framework where we construct logical forms and use these to build models that support them, taking the model constructed at each stage to be the current version of the slate. For a variety of reasons we assume that it is convenient to use a fine-grained intensional logic as the formal substrate of this activity. The logic we use is a constructive version of (Turner, 1987)&apos;s PROPERTY THEORY. Other intensional logics are available — (Bealer, 1989)&apos;s fine-grained intensional logic, non-well-founded set theory (Aczel, 1988), higher-order unification(Pulman, 1993). We choose property theory because it lends itself to a simple extension of a standard first-order theorem prover, which is what we use for constructing mod104 els (Ramsay, 2001). 2 Discourse Operators as Attitude Reports Can we extend the update process described above so that it deals with the differences between the various versions of the report of John and Mary&apos;s divorce in (1) and (2)? If we can, th</context>
</contexts>
<marker>Turner, 1987</marker>
<rawString>R Turner. 1987. A theory of properties. Journal of Symbolic Logic, 52(2):455-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R van der Sandt</author>
</authors>
<title>Presupposition projection as anaphora resolution.</title>
<date>1992</date>
<journal>Journal of Semantics,</journal>
<pages>9--333</pages>
<marker>van der Sandt, 1992</marker>
<rawString>R van der Sandt. 1992. Presupposition projection as anaphora resolution. Journal of Semantics, 9:333-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Eijck</author>
<author>H Alshawi</author>
</authors>
<title>Logical forms.</title>
<date>1992</date>
<booktitle>The Core Language Engine,</booktitle>
<pages>11--40</pages>
<editor>In H Alshawi, editor,</editor>
<publisher>Bradford Books/MIT Press.</publisher>
<location>Cambridge, Mass.</location>
<marker>van Eijck, Alshawi, 1992</marker>
<rawString>J van Eijck and H Alshawi. 1992. Logical forms. In H Alshawi, editor, The Core Language Engine, pages 11-40, Cambridge, Mass. Bradford Books/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Vendler</author>
</authors>
<title>Philosophy in Linguistics.</title>
<date>1967</date>
<publisher>Cornell University Press,</publisher>
<location>Ithaca.</location>
<contexts>
<context position="21527" citStr="Vendler, 1967" startWordPosition="3782" endWordPosition="3783">ty. What kind of thing might I be able to do under those circumstances? If I&apos;m the host then I might go and look for them so that you have someone to talk to, if I&apos;m a fellow guest then I might ask you to introduce them to me, ...In general, however, when you ask a question it&apos;s because there is something that you could do if you knew the answer. The key point about Fig. 20 is that it says that it doesn&apos;t matter which person you know: for every one who is at the party, if you know them then I can do whatever it is I have in mind. We thus encode the notion that &apos;any&apos; gives you a &apos;free choice&apos; (Vendler, 1967) within a perfectly standard logic. It is still just a universal quantifier, but it has very wide scope — wide enough to be quantifying over questions that I would like the answer to, rather than as part of the queried proposition as would happen with (11a), as in Fig. 21. Similar considerations apply to (12a) and (12b). (12a) says that whichever lane you choose will be appropriate for getting to Macclesfield. (12b) 3C : lintend(speaker, do(speaker, C))1 VD : lone(D) at(D, ref (AE(party(E))))1 SF : IF is interval Sz past(F)} 3H : {aspect(simple, F, H)} B(H, agent, ref (AHhearer(1)))) &amp;know(H) </context>
</contexts>
<marker>Vendler, 1967</marker>
<rawString>Z. Vendler. 1967. Philosophy in Linguistics. Cornell University Press, Ithaca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wedekind</author>
</authors>
<title>On inference-based procedures for lexical disambiguation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>980--985</pages>
<location>Copenhagen.</location>
<contexts>
<context position="6693" citStr="Wedekind, 1996" startWordPosition="1181" endWordPosition="1182">y thinking about the same man, who can therefore safely be referred to as &apos;the man&apos; (Ramsay, 1999; Gaylard and Ramsay, 2002). Step 3 involves adding the new information included in the current utterance to the logical form, producing Ezi/P1 from EiH. This step may just involve adding the new elements of the logical form, or it may involve adding other things that can be inferred from the logical form in the current context. We have argued that you can combine steps 2 and 3 by trying to build a model of the current utterance (Ramsay and Seville, 2000a). Other authors have taken a similar view (Wedekind, 1996; Blackburn et al., 1997; Baumgartner and Kuhn, 2000; Gardent and Konrad, 2000). No matter how you approach steps 2 and 3, there is no doubt that you need substantial amounts of background knowledge. To find out what someone who utters (la) has in mind, you have to know that divorce is an action that involves terminating an earlier contract, and that usually when it happens the two parties concerned no longer love each other, and ... In other words, you need to have access to the kind of information contained in Fig. 2, and you need to be able to reason with this information. VA : {divorce(A)}</context>
</contexts>
<marker>Wedekind, 1996</marker>
<rawString>J Wedekind. 1996. On inference-based procedures for lexical disambiguation. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 980-985, Copenhagen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>