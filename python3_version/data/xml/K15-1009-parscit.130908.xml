<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001581">
<title confidence="0.989535">
Big Data Small Data, In Domain Out-of Domain, Known Word Unknown
Word: The Impact of Word Representations on Sequence Labelling Tasks
</title>
<author confidence="0.992802">
Lizhen Qu1 2, Gabriela Ferraro1 2, Liyuan Zhou1, Weiwei Hou1,
Nathan Schneider3 and Timothy Baldwin1 4
</author>
<affiliation confidence="0.977382">
1 NICTA, Australia
2 The Australian National University
3 University of Edinburgh
4 The University of Melbourne
</affiliation>
<email confidence="0.984845333333333">
{lizhen.qu,gabriela.ferraro,liyuan.zho,weiwei.hou}@nicta.com.au
nschneid@cs.cmu.edu
tb@ldwin.net
</email>
<sectionHeader confidence="0.993401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99961896">
Word embeddings — distributed word
representations that can be learned from
unlabelled data — have been shown to
have high utility in many natural language
processing applications. In this paper, we
perform an extrinsic evaluation of four
popular word embedding methods in the
context of four sequence labelling tasks:
part-of-speech tagging, syntactic chunk-
ing, named entity recognition, and multi-
word expression identification. A particu-
lar focus of the paper is analysing the ef-
fects of task-based updating of word rep-
resentations. We show that when using
word embeddings as features, as few as
several hundred training instances are suf-
ficient to achieve competitive results, and
that word embeddings lead to improve-
ments over out-of-vocabulary words and
also out of domain. Perhaps more sur-
prisingly, our results indicate there is little
difference between the different word em-
bedding methods, and that simple Brown
clusters are often competitive with word
embeddings across all tasks we consider.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932770833333">
Recently, distributed word representations have
grown to become a mainstay of natural language
processing (NLP), and have been shown to have
empirical utility in a myriad of tasks (Collobert
and Weston, 2008; Turian et al., 2010; Baroni et
al., 2014; Andreas and Klein, 2014). The un-
derlying idea behind distributed word representa-
tions is simple: to map each word w in vocabu-
lary V onto a continuous-valued vector of dimen-
sionality d « |V J. Words that are similar (e.g.,
with respect to syntax or lexical semantics) will
ideally be mapped to similar regions of the vec-
tor space, implicitly supporting both generalisa-
tion across in-vocabulary (IV) items, and counter-
ing the effects of data sparsity for low-frequency
and out-of-vocabulary (OOV) items.
Without some means of automatically deriv-
ing the vector representations without reliance on
labelled data, however, word embeddings would
have little practical utility. Fortunately, it has
been shown that they can be “pre-trained” from
unlabelled text data using various algorithms to
model the distributional hypothesis (i.e., that
words which occur in similar contexts tend to be
semantically similar). Pre-training methods have
been refined considerably in recent years, and
scaled up to increasingly large corpora.
As with other machine learning methods, it is
well known that the quality of the pre-trained word
embeddings depends heavily on factors including
parameter optimisation, the size of the training
data, and the fit with the target application. For
example, Turian et al. (2010) showed that the op-
timal dimensionality for word embeddings is task-
specific. One factor which has received relatively
little attention in NLP is the effect of “updating”
the pre-trained word embeddings as part of the
task-specific training, based on self-taught learn-
ing (Raina et al., 2007). Updating leads to word
representations that are task-specific, but often at
the cost of over-fitting low-frequency and OOV
words.
In this paper, we perform an extensive evalu-
ation of four recently proposed word embedding
approaches under fixed experimental conditions,
applied to four sequence labelling tasks: part-of-
speech (POS) tagging, full-text chunking, named
entity recognition (NER), and multiword expres-
</bodyText>
<page confidence="0.989922">
83
</page>
<note confidence="0.980033">
Proceedings of the 19th Conference on Computational Language Learning, pages 83–93,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9352722">
sion (MWE) identification.1 We build on previous
empirical studies (Collobert et al., 2011; Turian et
al., 2010; Pennington et al., 2014) in considering
a broader range of word embedding approaches
and evaluating them over more sequence labelling
tasks. In addition, we explore the following re-
search questions:
RQ1: are word embeddings better than baseline
approaches of one-hot unigram2 features
and Brown clusters?
</bodyText>
<listItem confidence="0.996865666666667">
RQ2: do word embeddings require less training
data (i.e., generalise better) than one-hot
unigram features? If so, to what degree can
word embeddings reduce the amount of la-
belled data?
RQ3: what is the impact of updating word em-
beddings in sequence labelling tasks, both
empirically over the target task and geo-
metrically over the vectors?
RQ4: what is the impact of these word embed-
dings (with and without updating) on both
OOV items (relative to the training data)
and out-of-domain data?
RQ5: overall, are some word embeddings better
than others in a sequence labelling context?
</listItem>
<sectionHeader confidence="0.965921" genericHeader="method">
2 Word Representations
</sectionHeader>
<subsectionHeader confidence="0.99832">
2.1 Types of Word Representations
</subsectionHeader>
<bodyText confidence="0.99947925">
Turian et al. (2010) identifies three varieties
of word representations: distributional, cluster-
based, and distributed.
Distributional representation methods map
each word w to a context word vector Cw,
which is constructed directly from co-occurrence
counts between w and its context words. The
learning methods either store the co-occurrence
counts between two words w and i directly
in Cwi (Sahlgren, 2006; Turney and Pantel,
2010; Honkela, 1997) or project the concur-
rence counts between words into a lower dimen-
sional space (ˇReh˚uˇrek and Sojka, 2010; Lund and
Burgess, 1996), using dimensionality reduction
techniques such as SVD (Dumais et al., 1988) or
LDA (Blei et al., 2003).
</bodyText>
<footnote confidence="0.99317875">
1MWEs are lexicalized combinations of two or more sim-
plex words that are exceptional enough to be considered as
single units in the lexicon (Baldwin and Kim, 2010; Schnei-
der et al., 2014a), e.g., pick up or part of speech.
2Word vectors with one-hot representation are binary vec-
tors with a single dimension per word in the vocabulary (i.e.,
d = IV ), with the single dimension corresponding to the
target word set to 1 and all other dimensions set to 0.
</footnote>
<bodyText confidence="0.996641166666667">
Cluster-based representation methods build
clusters of words by applying either soft or hard
clustering algorithms (Lin and Wu, 2009; Li and
McCallum, 2005). Some of them also rely on
a co-occurrence matrix of words (Pereira et al.,
1993). The Brown clustering algorithm (Brown
et al., 1992) is the best-known method in this cat-
egory.
Distributed representation methods usu-
ally map words into dense, low-dimensional,
continuous-valued vectors, with x ∈ Rd, where d
is referred to as the word dimension.
</bodyText>
<subsectionHeader confidence="0.998743">
2.2 Selected Word Representations
</subsectionHeader>
<bodyText confidence="0.9963791875">
Over a range of sequence labelling tasks, we eval-
uate four methods for inducing word represen-
tations: Brown clustering (Brown et al., 1992)
(“BROWN”), the continuous bag-of-words model
(“CBOW”) (Mikolov et al., 2013a), the continu-
ous skip-gram model (“SKIP-GRAM”) (Mikolov et
al., 2013b), and Global vectors (“GLOVE”) (Pen-
nington et al., 2014). All have been shown to be
at or near state-of-the-art in recent empirical stud-
ies (Turian et al., 2010; Pennington et al., 2014).3
The training of these word representations is un-
supervised: the common underlying idea is to pre-
dict the occurrence of words in the neighbour-
ing context. Their training objectives share the
same form, which is a sum of local training fac-
tors J(w, ctx(w)),
</bodyText>
<equation confidence="0.9857605">
L =1: J(w, ctx(w))
w∈T
</equation>
<bodyText confidence="0.969907461538462">
where T is the set of tokens in a given corpus, and
ctx(w) denotes the local context of word w. The
local context of a word is conventionally its pre-
ceding m words, or alternatively the m words sur-
rounding it. Local training factors are designed
to capture the relationship between w and its lo-
cal contexts of use, either by predicting w based
on its local context, or using w to predict the con-
text words. Other than BROWN, which utilises a
cluster-based representation, all the other methods
employ a distributed representation.
The starting point for CBOW and SKIP-GRAM
is to employ softmax to predict word occurrence:
</bodyText>
<equation confidence="0.952534333333333">
�
exp(vTwvctx(w))
�+j∈V exp(vTj vctx(w))
</equation>
<footnote confidence="0.97422475">
3The word embedding approach proposed in Collobert et
al. (2011) is not considered because it was found to be inferior
to our four target word embedding approaches in previous
work.
</footnote>
<equation confidence="0.455075">
J(w, ctx(w)) = −log
</equation>
<page confidence="0.988464">
84
</page>
<bodyText confidence="0.998432">
where vctx(w) denotes the distributed representa-
tion of the local context of word w, and V is the
vocabulary of a given corpus. CBOW derives
vctx(w) based on averaging over the context words.
That is, it estimates the probability of each w given
its local context. In contrast, SKIP-GRAM applies
softmax to each context word of a given occur-
rence of word w. In this case, vctx(w) corresponds
to the representation of one of its context words.
This model can be characterised as predicting con-
text words based on w. In practice, softmax is
too expensive to compute over large corpora, and
thus Mikolov et al. (2013b) use hierarchical soft-
max and negative sampling to scale up the train-
ing.
GLOVE assumes the dot product of two word
embeddings should be similar to the logarithm of
the co-occurrence count Xij of the two words. As
such, the local factor J(w, ctx(w)) becomes:
g(Xij)(vTi vj + bi + bj − log(Xij))2
where bi and bj are the bias terms of words i and
j, respectively, and g(Xij) is a weighting function
based on the co-occurrence count. This weight-
ing function controls the degree of agreement be-
tween the parametric function vTi vj + bi + bj and
log(Xij). Frequently co-occurring word pairs will
have larger weight than infrequent pairs, up to a
threshold.
BROWN partitions words into a finite set of
word classes V . The conditional probability of
seeing the next word is defined to be:
</bodyText>
<equation confidence="0.995734333333333">
p(wk|wk−1
k−m) = p(wk|hk)p(hk|hk−1
k−m)
</equation>
<bodyText confidence="0.9329982">
where hk denotes the word class of the word
wk, wk−1
k−m are the previous m words, and
hk−1 are their respective word classes. Then
km
</bodyText>
<equation confidence="0.861106">
J(w,ctx(w)) = −log p(wk|wk−1
</equation>
<bodyText confidence="0.986955333333333">
k−m). Since there
is no tractable method to find an optimal parti-
tion of word classes, the method uses only a bi-
gram class model, and utilises hierarchical clus-
tering as an approximation method to find a suffi-
ciently good partition of words.
</bodyText>
<subsectionHeader confidence="0.999612">
2.3 Building Word Representations
</subsectionHeader>
<bodyText confidence="0.95822475">
To ensure the comparison of different word rep-
resentations is fair, we train BROWN, CBOW,
SKIP-GRAM, and GLOVE on a fixed corpus, com-
prised of freely available corpora, as detailed in
</bodyText>
<tableCaption confidence="0.971857">
Table 1. The joint corpus was preprocessed with
</tableCaption>
<table confidence="0.96312175">
Data set Size Words
UMBC (Han et al., 2013) 48.1GB 3G
One Billion (Chelba et al., 2013) 4.1GB 1G
English Wikipedia 49.6GB 3G
</table>
<tableCaption confidence="0.91494">
Table 1: Corpora used to pre-train the word em-
beddings
</tableCaption>
<figureCaption confidence="0.998214">
Figure 1: Linear-chain graph transformer
</figureCaption>
<bodyText confidence="0.946516272727273">
the Stanford CoreNLP sentence splitter and to-
keniser. All consecutive digit substrings were
replaced by NUMf, where f is the length of
the digit substring (e.g., 10.20 is replaced by
NUM2.NUM2.
The dimensionality of the word embeddings
and the size of the context window are the key hy-
perparameters when learning distributed represen-
tations. We use all combinations of the following
values to train word embeddings on the combined
corpus:
</bodyText>
<listItem confidence="0.999889">
• Embedding dim. d E 125, 50,100, 200}
• Context window size m E 11, 5,10}
</listItem>
<bodyText confidence="0.929886">
BROWN requires only the number of clusters
as a hyperparameter. Here, we perform clustering
with b E 1250, 500, 1000, 2000, 4000} clusters.
</bodyText>
<sectionHeader confidence="0.915669" genericHeader="method">
3 Sequence Labelling Tasks
</sectionHeader>
<bodyText confidence="0.9999895">
We evaluate the different word representations
over four sequence labelling tasks: POS tagging
(POS tagging), full-text chunking (Chunking),
NER (NER), and MWE identification (MWE).
For each task, we fed features into a first-order
linear-chain graph transformer (Collobert et al.,
2011) made up of two layers: the upper layer is
identical to a linear-chain CRF (Lafferty et al.,
2001), and the lower layer consists of word rep-
resentation and hand-crafted features. If we treat
word representations as fixed, the graph trans-
former is a simple linear-chain CRF. On the other
hand, if we can treat the word representations as
model parameters, the model is equivalent to a
neural network with word embeddings as the input
layer, as shown in Figure 1. We trained all models
using AdaGrad (Duchi et al., 2011).
As in Turian et al. (2010), at each word position,
we construct word representation features from
the words in a context window of size two to either
</bodyText>
<page confidence="0.998653">
85
</page>
<bodyText confidence="0.999646608695652">
side of the target word, based on the pre-trained
representation of each word type. For BROWN,
the features are the prefix features extracted from
word clusters in the same way as Turian et al.
(2010). As a baseline (and to test RQ1), we in-
clude a one-hot representation (which is equiva-
lent to a linear-chain CRF with only lexical con-
text features).
Our hand-crafted features for POS tagging,
Chunking and MWE, are those used by Collobert
et al. (2011), Turian et al. (2010) and Schneider
et al. (2014b), respectively. For NER, we use the
same feature space as Turian et al. (2010), except
for the previous two predictions, because we want
to evaluate all word representations with the same
type of model — a first-order graph transformer.
In training the distributed word representations,
we consider two settings: (1) the word represen-
tations are fixed during sequence model training;
and (2) the graph transformer updated the token-
level word representations during training.
As outlined in Table 2, for each sequence la-
belling task, we experiment over the de facto cor-
pus, based on pre-existing training–dev–test splits
where available:4
POS tagging: the Wall Street Journal portion
of the Penn Treebank (Marcus et al. (1993):
“WSJ”) with Penn POS tags
Chunking: the Wall Street Journal portion of the
Penn Treebank (“WSJ”), converted into IOB-
style full-text chunks using the CoNLL con-
version scripts for training and dev, and the
WSJ-derived CoNLL-2000 full text chunk-
ing test data for testing (Tjong Kim Sang and
Buchholz, 2000)
NER: the English portion of the CoNLL-2003
English Named Entity Recognition data set,
for which the source data was taken from
Reuters newswire articles (Tjong Kim Sang
and De Meulder (2003): “Reuters”)
MWE: the MWE dataset of Schneider et al.
(2014b), over a portion of text from the En-
glish Web Treebank5 (“EWT”)
For all tasks other than MWE,6 we additionally
have an out-of-domain test set, in order to eval-
uate the out-of-domain robustness of the different
</bodyText>
<footnote confidence="0.990898142857143">
4For the MWE dataset, no such split pre-existed, so we
constructed our own.
5https://catalog.ldc.upenn.edu/
LDC2012T13
6Unfortunately, there is no second domain which has been
hand-tagged with MWEs using the method of Schneider et al.
(2014b) to use as an out-of-domain test corpus.
</footnote>
<bodyText confidence="0.999470857142857">
word representations, with and without updating.
These datasets are as follows:
POS tagging: the English Web Treebank with
Penn POS tags (“EWT”)
Chunking: the Brown Corpus portion of the
Penn Treebank (“Brown”), converted into
IOB-style full-text chunks using the CoNLL
conversion scripts
NER: the MUC-7 named entity recognition cor-
pus7 (“MUC7”)
For reproducibility, we tuned the hyperparam-
eters with random search over the development
data for each task (Bergstra and Bengio, 2012).
In this, we randomly sampled 50 distinct hyper-
parameter sets with the same random seed for the
non-updating models (i.e., the models that don’t
update the word representation), and sampled 100
distinct hyperparameter sets for the updating mod-
els (i.e., the models that do). For each set of hy-
perparameters and task, we train a model over its
training set and choose the best one based on its
performance on development data (Turian et al.,
2010). We also tune the word representation hy-
perparameters — namely, the word vector size d
and context window size m (distributed represen-
tations), and in the case of Brown, the number of
clusters.
For the updating models, we found that the re-
sults over the test data were always inferior to
those that do not update the word representations,
due to the higher number of hyperparameters and
small sample size (i.e., 100). Since the two-layer
model of the graph transformer contains a distinct
set of hyperparameters for each layer, we reuse the
best-performing hyperparameter settings from the
non-updating models, and only tune the hyperpa-
rameters of AdaGrad for the word representation
layer. This method requires only 32 additional
runs and achieves consistently better results than
100 random draws.
In order to test the impact of the volume of
training data on the different models (RQ2), we
split the training set into 10 partitions based on
a base-2 log scale (i.e., the second smallest par-
tition will be twice the size of the smallest parti-
tion), and created 10 successively larger training
sets by merging these partitions from the smallest
one to the largest one, and used each of these to
train a model. From these, we construct learning
</bodyText>
<footnote confidence="0.986644">
7https://catalog.ldc.upenn.edu/
LDC2001T02
</footnote>
<page confidence="0.982769">
86
</page>
<table confidence="0.993304">
Training Development In-domain Test Out-of-domain Test
POS tagging WSJ Sec. 0-18 WSJ Sec. 19–21 WSJ Sec. 22–24 EWT
Chunking WSJ WSJ (1K sentences) WSJ (CoNLL-00 test) Brown
NER Reuters (CoNLL-03 train) Reuters (CoNLL-03 dev) Reuters (CoNLL-03 test) MUC7
MWE EWT (500 docs) EWT (100 docs) EWT (123 docs) —
</table>
<tableCaption confidence="0.999098">
Table 2: Training, development and test (in- and out-of-domain) data for each sequence labelling task.
</tableCaption>
<bodyText confidence="0.996535142857143">
curves over each task.
For ease of comparison with previous results,
we evaluate both in- and out-of-domain using
chunk/entity/expression-level F1-measure (“F1”)
for all tasks except POS tagging, for which we
use token-level accuracy (“ACC”). To test perfor-
mance over OOV (unknown) tokens — i.e., the
words that do not occur in the training set — we
use token-level accuracy for all tasks (e.g., for
Chunking, we evaluate whether the full IOB tag
is correct or not), because chunks/NEs/MWEs can
consist of a mixture of in-vocabulary and OOV to-
kens, which makes the use of chunk-based evalu-
ation measures inappropriate.
</bodyText>
<sectionHeader confidence="0.983328" genericHeader="method">
4 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.98637398630137">
We structure our evaluation by stepping through
each of our five research questions (RQ1–5) from
the start of the paper. In this, we make reference
to: (1) the best-performing method both in- and
out-of-domain vs. the state-of-the-art (Table 3);
(2) a heat map for each task indicating the con-
vergence rate for each word representation, with
and without updating (Figure 2); (3) OOV accu-
racy both in-domain and out-of-domain for each
task (Figure 3); and (4) visualisation of the impact
of updating on word embeddings, based on t-SNE
(Figure 4).
RQ1: Are the selected word embeddings better
than one-hot unigram features and Brown clus-
ters? As shown in Table 3, the best-performing
method for every task except in-domain Chunk-
ing is a word embedding method, although the
precise method varies greatly. Figure 2, on the
other hand, tells a more subtle story: the difference
between UNIGRAM and the other word represen-
tations is relatively modest, esp. as the amount of
training data increases. Additionally, the differ-
ence between BROWN and the word embedding
methods is modest across all tasks. So, the over-
all answer would appear to be: yes, word embed-
dings are better than unigrams when there is little
training data, but they are not markedly better than
Brown clusters.
RQ2: Do word embedding features require
less training data? Figure 2 shows that for
POS tagging and NER, with only several hun-
dred training instances, word embedding fea-
tures achieve superior results to UNIGRAM. For
example, when trained with 561 instances, the
POS tagging model using SKIP-GRAM+UP em-
beddings is 5.3% above UNIGRAM; and when
trained with 932 instances, the NER model us-
ing SKIP-GRAM is 11.7% above UNIGRAM. Sim-
ilar improvements are also found for other types
of word embeddings and BROWN, when the train-
ing set is small. However, all word representa-
tions perform similarly for Chunking regardless
of training data size. For MWE, BROWN performs
slightly better than the other methods when trained
with approximately 25% of the training instances.
Therefore, we conjecture that the POS tagging
and NER tasks benefit more from distributional
similarity than Chunking and MWE.
RQ3: Does task-specific updating improve all
word embeddings across all tasks? Based on
Figure 2, updating of word representations can
equally correct poorly-learned word representa-
tions, and harm pre-trained representations, due to
overfitting. For example, GLOVE performs sub-
stantially worse than SKIP-GRAM for both POS
tagging and NER without updating, but with up-
dating, the relative empirical gap between the best
performing method becomes smaller. In contrast,
SKIP-GRAM performs worse over the test data
with updating, despite the results on the develop-
ment set improving by 1%.
To further investigate the effects of updating,
we sampled 60 words and plotted the changes in
their word embeddings under updating, using 2-
d vector fields generated using matplotlib and t-
SNE (van der Maaten and Hinton, 2008). Half
of the words were chosen manually to include
known word clusters such as days of the week and
names of countries; the other half were selected
randomly. Additional plots with 100 randomly-
sampled words and the top-100 most frequent
words, for all the methods and all the tasks, can
be found in the supplementary material and at
</bodyText>
<page confidence="0.996257">
87
</page>
<table confidence="0.9989942">
Task Benchmark In-domain Test set Out-of-domain Test set
POS tagging (ACC) 0.972 (Toutanova et al., 2003) 0.959 (SKIP-GRAM+UP) 0.910 (SKIP-GRAM)
Chunking (F1) 0.942 (Sha and Pereira, 2003) 0.938 (BROWNb=2000) 0.676 (GLOVE)
NER (F1) 0.893 (Ando and Zhang, 2005) 0.868 (SKIP-GRAM) 0.736 (SKIP-GRAM)
MWE (F1) 0.625 (Schneider et al., 2014a) 0.654 (CBOW+UP) —
</table>
<tableCaption confidence="0.999373">
Table 3: State-of-the-art results vs. our best results for in-domain and out-of-domain test sets.
</tableCaption>
<figure confidence="0.9962945">
(a) POS tagging (ACC) (b) Chunking (F1)
(c) NER (F1) (d) MWE (F1)
</figure>
<figureCaption confidence="0.949419">
Figure 2: Results for each type of word representation over POS tagging, Chunking, NER and MWE,
optionally with updating (“+UP”). The y-axis indicates the training data sizes (on a log scale). Green
</figureCaption>
<bodyText confidence="0.987410466666667">
= high performance, and red = low performance, based on a linear scale of the best- to worst-result for
each task.
https://goo.gl/Y8bk2w. In each plot, a
single arrow signifies one word, pointing from the
position of the original word embedding to the up-
dated representation.
In Figure 4, we show vector fields plots for
Chunking and NER using SKIP-GRAM embed-
dings. For Chunking, most of the vectors were
changed with similar magnitude, but in very dif-
ferent directions, including within the clusters of
days of the week and country names. In contrast,
for NER, there was more homogeneous change in
word vectors belonging to the same cluster. This
greater consistency is further evidence that seman-
tic homogeneity appears to be more beneficial for
NER than Chunking.
RQ4: What is the impact of word embeddings
cross-domain and for OOV words? As shown
in Table 3, results predictably drop when we eval-
uate out of domain. The difference is most pro-
nounced for Chunking, where there is an absolute
drop in F1 of around 30% for all methods, indi-
cating that word embeddings and unigram features
provide similar information for Chunking.
Another interesting observation is that updating
often hurts out-of-domain performance because
the distribution between domains is different. This
suggests that, if the objective is to optimise per-
formance across domains, it is best not to perform
</bodyText>
<page confidence="0.994944">
88
89
</page>
<figure confidence="0.999256888888889">
POS accuracy for in-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 5000 10000 15000 20000 25000 30000 35000 40000
Training size
NER accuracy for in-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 2000 4000 6000 8000 10000 12000 14000 16000
Training size
Chunking accuracy for in-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 2000 4000 6000 8000 10000
Training size
MWE accuracy for in-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 500 1000 1500 2000 2500 3000 3500
Training size
POS accuracy for out-of-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 5000 10000 15000 20000 25000 30000 35000 40000
Training size
NER accuracy for out-of-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 2000 4000 6000 8000 10000 12000 14000 16000
Training size
Chunking accuracy for out-of-domain OOV
1.0
0.9
0.8
0.7
0.6
0.5
0 2000 4000 6000 8000 10000
Training size
</figure>
<figureCaption confidence="0.989182">
Figure 3: ACC over out-of-vocabulary (OOV) words for in-domain and out-of-domain test sets.
</figureCaption>
<figure confidence="0.99915516">
200
100
days of
the week
0
100
names of
countries
200
300
200 100 0 100 200
(a) Chunking
names of
countries
days of
the week
30
20
10
0
10
20
30
30 20 10 0 10 20 30
(b) NER
</figure>
<figureCaption confidence="0.7839585">
Figure 4: A t-SNE plot of the impact of updating on SKIP-GRAM
updating.
</figureCaption>
<bodyText confidence="0.98659165">
We also analyze performance on OOV words
both in-domain and out-of-domain in Figure 3.
As expected, word embeddings and BROWN excel
in out-of-domain OOV performance. Consistent
with our overall observations about cross-domain
brown_cluster
cbow_noup
cbow_up
glove_noup
glove_up
skip_gram_negsam_noup
skip_gram_negsam_up
unigram
generalisation, the OOV results are better when
updating is not performed.
RQ5 Overall, are some word embeddings bet-
ter than others? Comparing the different word
embedding techniques over our four sequence la-
belling tasks, for the different evaluations (overall,
out-of-domain and OOV), there is no clear winner
among the word embeddings — for POS tagging,
SKIP-GRAM appears to have a slight advantage,
but this does not generalise to other tasks.
While the aim of this paper was not to achieve
the state of the art over the respective tasks, it is
important to concede that our best (in-domain) re-
sults for NER, POS tagging and Chunking are
slightly worse than the state of the art (Table 3).
The 2.7% difference between our NER system
and the best performing system is due to the fact
that we use a first-order instead of a second-order
CRF (Ando and Zhang, 2005), and for the other
tasks, there are similarly differences in the learner
and the complexity of the features used. Another
difference is that we tuned the hyperparameters
with random search, to enable replication using
the same random seed. In contrast, the hyperpa-
rameters for the state-of-the-art methods are tuned
more extensively by experts, making them more
difficult to reproduce.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999977545454545">
Collobert et al. (2011) proposed a unified neural
network framework that learns word embeddings
and applied it to POS tagging, Chunking, NER
and semantic role labelling. When they combined
word embeddings with hand-crafted features (e.g.,
word suffixes for POS tagging; gazetteers for
NER) and applied other tricks like cascading and
classifier combination, they achieved state-of-the-
art performance. Similarly, Turian et al. (2010)
evaluated three different word representations on
NER and Chunking, and concluded that unsu-
pervised word representations improved NER and
Chunking. They also found that combining dif-
ferent word representations can further improve
performance. Guo et al. (2014) also explored dif-
ferent ways of using word embeddings for NER.
Owoputi et al. (2013) and Schneider et al. (2014a)
found that BROWN clustering enhances Twitter
POS tagging and MWE, respectively. Compared
to previous work, we consider more word rep-
resentations including the most recent work and
evaluate them on more sequence labelling tasks,
wherein the models are trained with training sets
of varying size.
Bansal et al. (2014) reported that direct use of
word embeddings in dependency parsing did not
show improvement. They achieved an improve-
ment only when they performed hierarchical clus-
tering of the word embeddings, and used features
extracted from the cluster hierarchy. In a simi-
lar vein, Andreas and Klein (2014) explored the
use of word embeddings for constituency pars-
ing and concluded that the information contained
in word embeddings might duplicate the one ac-
quired by a syntactic parser, unless the training set
is extremely small. Other syntactic parsing studies
that reported improvements by using word embed-
dings include Koo et al. (2008), Koo et al. (2010),
Haffari et al. (2011), Tratz and Hovy (2011) and
Chen and Manning (2014).
Word embeddings have also been applied to
other (non-sequential NLP) tasks like grammar in-
duction (Spitkovsky et al., 2011), and semantic
tasks such as semantic relatedness, synonymy de-
tection, concept categorisation, selectional prefer-
ence learning and analogy (Baroni et al., 2014;
Levy and Goldberg, 2014; Levy et al., 2015).
Huang and Yates (2009) demonstrated that us-
ing distributional word representations methods
(like TF-IDF and LSA) as features, improves the
labelling of OOV, when test for POS tagging and
Chunking. In our study, we evaluate the labelling
performance of OOV words for updated vs. non-
updated word embedding representations, relative
to the training set and with out-of-domain data.
</bodyText>
<sectionHeader confidence="0.997465" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999861125">
We have performed an extensive extrinsic evalua-
tion of four word embedding methods under fixed
experimental conditions, and evaluated their ap-
plicability to four sequence labelling tasks: POS
tagging, Chunking, NER and MWE identifica-
tion. We found that word embedding features re-
liably outperformed unigram features, especially
with limited training data, but that there was rela-
tively little difference over Brown clusters, and no
one embedding method was consistently superior
across the different tasks and settings. Word em-
beddings and Brown clusters were also found to
improve out-of-domain performance and for OOV
words. We expected a performance gap between
the fixed and task-updated embeddings, but the ob-
served difference was marginal. Indeed, we found
</bodyText>
<page confidence="0.995152">
90
</page>
<bodyText confidence="0.9999165">
that updating can result in overfitting. We also car-
ried out preliminary analysis of the impact of up-
dating on the vectors, a direction which we intend
to pursue further.
</bodyText>
<sectionHeader confidence="0.987867" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998362">
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program.
</bodyText>
<sectionHeader confidence="0.997835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983272568421053">
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817–1853.
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 822–827, Baltimore, USA.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 809–
815, Baltimore, USA.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 238–247, Baltimore, USA.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(1):281–305.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. Tech-
nical report, Google.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), pages 740–750, Doha, Qatar.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167, Helsinki, Fin-
land.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.
Susan T Dumais, George W Furnas, Thomas K Lan-
dauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve ac-
cess to textual information. In Proceedings of the
SIGCHI conference on Human Factors in Comput-
ing Systems, pages 281–285.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2014), pages 110–
120, Doha, Qatar.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In ACL 2011 (Short Papers),
pages 710–714, Portland, USA.
Lushan Han, Abhay L. Kashyap, Tim Finin, James
Mayfield, and Johnathan Weese. 2013. UMBC
EBIQUITY CORE: Semantic textual similarity sys-
tems. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics,
pages 44–52, Atlanta, USA.
Timo Honkela. 1997. Self-organizing maps of words
for natural language processing applications. In
Proceedings of the International ICSC Symposium
on Soft Computing, pages 401–407, Nimes, France.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1 - Vol-
ume 1, pages 495–503, Suntec, Singapore.
</reference>
<page confidence="0.993946">
91
</page>
<reference confidence="0.999524293577981">
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595–603,
Columbus, USA.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP 2010), pages 1288–1298, Cambridge,
USA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289, Williamstown, USA.
Omer Levy and Yoav Goldberg. 2014. Neural
word embedding as implicit matrix factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N.D.
Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
2177–2185. Curran Associates, Inc.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3(1):211–225.
Wei Li and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In Proceedings of the National Conference
on Artificial Intelligence, Pittsburgh, USA.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1030–1038, Suntec, Singapore.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, &amp; Computers, 28(2):203–208.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL HLT 2013), pages 380–390, Atlanta, USA.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vec-
tors for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2014), pages 1532–
1543, Doha, Qatar.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 183–
190, Columbus, USA.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y Ng. 2007. Self-taught learn-
ing: transfer learning from unlabeled data. In Pro-
ceedings of the 24th International Conference on
Machine Learning, pages 759–766, Corvallis, USA.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 51–55, Valetta,
Malta.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Institutio-
nen f¨or lingvistik.
Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014a. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association of Computa-
tional Linguistics, 2(1):193–206.
Nathan Schneider, Spencer Onuffer, Nora Kazour,
Emily Danchik, Michael T. Mordowanec, Henrietta
Conrad, and Noah A. Smith. 2014b. Comprehen-
sive annotation of multiword expressions in a social
web corpus. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation, pages 455–461, Reykjavik, Iceland.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology - Volume 1, pages
134–141, Edmonton, Canada.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
</reference>
<page confidence="0.932318">
92
</page>
<reference confidence="0.99988805">
tags. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1281–1290, Edinburgh, UK.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proceedings of the 4th Confer-
ence on Computational Natural Language Learning
(CoNLL-2000), pages 127–132, Lisbon, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the 7th Conference on Natural Lan-
guage Learning (CoNLL-2003), pages 142–147, Ed-
monton, Canada.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, pages 173–180, Edmonton, Canada.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
1257–1268, Edinburgh, UK.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394, Uppsala, Swe-
den.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
Laurens J.P. van der Maaten and Geoffrey Hinton.
2008. Visualizing high-dimensional data using
t-sne. Journal of Machine Learning Research,
9:2579–2605.
</reference>
<page confidence="0.999169">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.230482">
<title confidence="0.9972545">Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representations on Sequence Labelling Tasks</title>
<author confidence="0.770344">Gabriela Liyuan Weiwei Australian National</author>
<affiliation confidence="0.696265">of University of Melbourne</affiliation>
<email confidence="0.926074">nschneid@cs.cmu.edutb@ldwin.net</email>
<abstract confidence="0.9990735">Word embeddings — distributed word representations that can be learned from unlabelled data — have been shown to have high utility in many natural language processing applications. In this paper, we perform an extrinsic evaluation of four popular word embedding methods in the context of four sequence labelling tasks: part-of-speech tagging, syntactic chunking, named entity recognition, and multiword expression identification. A particular focus of the paper is analysing the effects of task-based updating of word representations. We show that when using word embeddings as features, as few as several hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="21368" citStr="Ando and Zhang, 2005" startWordPosition="3433" endWordPosition="3436">Maaten and Hinton, 2008). Half of the words were chosen manually to include known word clusters such as days of the week and names of countries; the other half were selected randomly. Additional plots with 100 randomlysampled words and the top-100 most frequent words, for all the methods and all the tasks, can be found in the supplementary material and at 87 Task Benchmark In-domain Test set Out-of-domain Test set POS tagging (ACC) 0.972 (Toutanova et al., 2003) 0.959 (SKIP-GRAM+UP) 0.910 (SKIP-GRAM) Chunking (F1) 0.942 (Sha and Pereira, 2003) 0.938 (BROWNb=2000) 0.676 (GLOVE) NER (F1) 0.893 (Ando and Zhang, 2005) 0.868 (SKIP-GRAM) 0.736 (SKIP-GRAM) MWE (F1) 0.625 (Schneider et al., 2014a) 0.654 (CBOW+UP) — Table 3: State-of-the-art results vs. our best results for in-domain and out-of-domain test sets. (a) POS tagging (ACC) (b) Chunking (F1) (c) NER (F1) (d) MWE (F1) Figure 2: Results for each type of word representation over POS tagging, Chunking, NER and MWE, optionally with updating (“+UP”). The y-axis indicates the training data sizes (on a log scale). Green = high performance, and red = low performance, based on a linear scale of the best- to worst-result for each task. https://goo.gl/Y8bk2w. In </context>
<context position="25530" citStr="Ando and Zhang, 2005" startWordPosition="4137" endWordPosition="4140">luations (overall, out-of-domain and OOV), there is no clear winner among the word embeddings — for POS tagging, SKIP-GRAM appears to have a slight advantage, but this does not generalise to other tasks. While the aim of this paper was not to achieve the state of the art over the respective tasks, it is important to concede that our best (in-domain) results for NER, POS tagging and Chunking are slightly worse than the state of the art (Table 3). The 2.7% difference between our NER system and the best performing system is due to the fact that we use a first-order instead of a second-order CRF (Ando and Zhang, 2005), and for the other tasks, there are similarly differences in the learner and the complexity of the features used. Another difference is that we tuned the hyperparameters with random search, to enable replication using the same random seed. In contrast, the hyperparameters for the state-of-the-art methods are tuned more extensively by experts, making them more difficult to reproduce. 5 Related Work Collobert et al. (2011) proposed a unified neural network framework that learns word embeddings and applied it to POS tagging, Chunking, NER and semantic role labelling. When they combined word embe</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax?</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>822--827</pages>
<location>Baltimore, USA.</location>
<contexts>
<context position="1749" citStr="Andreas and Klein, 2014" startWordPosition="254" endWordPosition="257"> results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d « |V J. Words that are similar (e.g., with respect to syntax or lexical semantics) will ideally be mapped to similar regions of the vector space, implicitly supporting both generalisation across in-vocabulary (IV) items, and countering the effects of data sparsity for low-frequency and out-of-vocabulary (OOV) items. Without some means of automatically deriving the vector representations without reliance on labelled data, however, word e</context>
<context position="27349" citStr="Andreas and Klein (2014)" startWordPosition="4414" endWordPosition="4417">et al. (2014a) found that BROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, wherein the models are trained with training sets of varying size. Bansal et al. (2014) reported that direct use of word embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedne</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 822–827, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Su Nam Kim</author>
</authors>
<title>Multiword expressions.</title>
<date>2010</date>
<booktitle>In Nitin Indurkhya</booktitle>
<editor>and Fred J. Damerau, editors,</editor>
<publisher>CRC Press,</publisher>
<location>Boca Raton, USA,</location>
<note>2nd edition.</note>
<contexts>
<context position="5803" citStr="Baldwin and Kim, 2010" startWordPosition="875" endWordPosition="878">ctly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-kno</context>
</contexts>
<marker>Baldwin, Kim, 2010</marker>
<rawString>Timothy Baldwin and Su Nam Kim. 2010. Multiword expressions. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing. CRC Press, Boca Raton, USA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>809--815</pages>
<location>Baltimore, USA.</location>
<contexts>
<context position="27055" citStr="Bansal et al. (2014)" startWordPosition="4367" endWordPosition="4370">d concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that BROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, wherein the models are trained with training sets of varying size. Bansal et al. (2014) reported that direct use of word embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddi</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 809– 815, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>238--247</pages>
<location>Baltimore, USA.</location>
<contexts>
<context position="1723" citStr="Baroni et al., 2014" startWordPosition="250" endWordPosition="253">o achieve competitive results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d « |V J. Words that are similar (e.g., with respect to syntax or lexical semantics) will ideally be mapped to similar regions of the vector space, implicitly supporting both generalisation across in-vocabulary (IV) items, and countering the effects of data sparsity for low-frequency and out-of-vocabulary (OOV) items. Without some means of automatically deriving the vector representations without reliance on labe</context>
<context position="28061" citStr="Baroni et al., 2014" startWordPosition="4525" endWordPosition="4528">ion contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling performance of OOV words for updated vs. nonupdated word embedding representations, relative to the training set and with out-of-domain data. 6 Conclusions We have performed an extensive extrinsic evaluation of four word embedding methods under fixed experimental conditions, and evaluated their applicability to fou</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 238–247, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Yoshua Bengio</author>
</authors>
<title>Random search for hyper-parameter optimization.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="14990" citStr="Bergstra and Bengio, 2012" startWordPosition="2403" endWordPosition="2406">e is no second domain which has been hand-tagged with MWEs using the method of Schneider et al. (2014b) to use as an out-of-domain test corpus. word representations, with and without updating. These datasets are as follows: POS tagging: the English Web Treebank with Penn POS tags (“EWT”) Chunking: the Brown Corpus portion of the Penn Treebank (“Brown”), converted into IOB-style full-text chunks using the CoNLL conversion scripts NER: the MUC-7 named entity recognition corpus7 (“MUC7”) For reproducibility, we tuned the hyperparameters with random search over the development data for each task (Bergstra and Bengio, 2012). In this, we randomly sampled 50 distinct hyperparameter sets with the same random seed for the non-updating models (i.e., the models that don’t update the word representation), and sampled 100 distinct hyperparameter sets for the updating models (i.e., the models that do). For each set of hyperparameters and task, we train a model over its training set and choose the best one based on its performance on development data (Turian et al., 2010). We also tune the word representation hyperparameters — namely, the word vector size d and context window size m (distributed representations), and in t</context>
</contexts>
<marker>Bergstra, Bengio, 2012</marker>
<rawString>James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(1):281–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5639" citStr="Blei et al., 2003" startWordPosition="847" endWordPosition="850">ions: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and Mc</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="6387" citStr="Brown et al., 1992" startWordPosition="975" endWordPosition="978"> the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“SKIP-GRAM”) (Mikolov et al., 2013b), and Global vectors (“GLOVE”) (Pennington et al., 2014). All have </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Tomas Mikolov</author>
<author>Mike Schuster</author>
<author>Qi Ge</author>
<author>Thorsten Brants</author>
<author>Phillipp Koehn</author>
<author>Tony Robinson</author>
</authors>
<title>One billion word benchmark for measuring progress in statistical language modeling.</title>
<date>2013</date>
<tech>Technical report, Google.</tech>
<contexts>
<context position="10454" citStr="Chelba et al., 2013" startWordPosition="1668" endWordPosition="1671">ctx(w)) = −log p(wk|wk−1 k−m). Since there is no tractable method to find an optimal partition of word classes, the method uses only a bigram class model, and utilises hierarchical clustering as an approximation method to find a sufficiently good partition of words. 2.3 Building Word Representations To ensure the comparison of different word representations is fair, we train BROWN, CBOW, SKIP-GRAM, and GLOVE on a fixed corpus, comprised of freely available corpora, as detailed in Table 1. The joint corpus was preprocessed with Data set Size Words UMBC (Han et al., 2013) 48.1GB 3G One Billion (Chelba et al., 2013) 4.1GB 1G English Wikipedia 49.6GB 3G Table 1: Corpora used to pre-train the word embeddings Figure 1: Linear-chain graph transformer the Stanford CoreNLP sentence splitter and tokeniser. All consecutive digit substrings were replaced by NUMf, where f is the length of the digit substring (e.g., 10.20 is replaced by NUM2.NUM2. The dimensionality of the word embeddings and the size of the context window are the key hyperparameters when learning distributed representations. We use all combinations of the following values to train word embeddings on the combined corpus: • Embedding dim. d E 125, 5</context>
</contexts>
<marker>Chelba, Mikolov, Schuster, Ge, Brants, Koehn, Robinson, 2013</marker>
<rawString>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. Technical report, Google.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>740--750</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="27777" citStr="Chen and Manning (2014)" startWordPosition="4484" endWordPosition="4487">chieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling performance of OOV words for upd</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 740–750, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="1681" citStr="Collobert and Weston, 2008" startWordPosition="242" endWordPosition="245">veral hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d « |V J. Words that are similar (e.g., with respect to syntax or lexical semantics) will ideally be mapped to similar regions of the vector space, implicitly supporting both generalisation across in-vocabulary (IV) items, and countering the effects of data sparsity for low-frequency and out-of-vocabulary (OOV) items. Without some means of automatically deriving the vecto</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="3986" citStr="Collobert et al., 2011" startWordPosition="589" endWordPosition="592">sk-specific, but often at the cost of over-fitting low-frequency and OOV words. In this paper, we perform an extensive evaluation of four recently proposed word embedding approaches under fixed experimental conditions, applied to four sequence labelling tasks: part-ofspeech (POS) tagging, full-text chunking, named entity recognition (NER), and multiword expres83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and evaluating them over more sequence labelling tasks. In addition, we explore the following research questions: RQ1: are word embeddings better than baseline approaches of one-hot unigram2 features and Brown clusters? RQ2: do word embeddings require less training data (i.e., generalise better) than one-hot unigram features? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both </context>
<context position="8115" citStr="Collobert et al. (2011)" startWordPosition="1259" endWordPosition="1262">l context of a word is conventionally its preceding m words, or alternatively the m words surrounding it. Local training factors are designed to capture the relationship between w and its local contexts of use, either by predicting w based on its local context, or using w to predict the context words. Other than BROWN, which utilises a cluster-based representation, all the other methods employ a distributed representation. The starting point for CBOW and SKIP-GRAM is to employ softmax to predict word occurrence: � exp(vTwvctx(w)) �+j∈V exp(vTj vctx(w)) 3The word embedding approach proposed in Collobert et al. (2011) is not considered because it was found to be inferior to our four target word embedding approaches in previous work. J(w, ctx(w)) = −log 84 where vctx(w) denotes the distributed representation of the local context of word w, and V is the vocabulary of a given corpus. CBOW derives vctx(w) based on averaging over the context words. That is, it estimates the probability of each w given its local context. In contrast, SKIP-GRAM applies softmax to each context word of a given occurrence of word w. In this case, vctx(w) corresponds to the representation of one of its context words. This model can b</context>
<context position="11556" citStr="Collobert et al., 2011" startWordPosition="1842" endWordPosition="1845">use all combinations of the following values to train word embeddings on the combined corpus: • Embedding dim. d E 125, 50,100, 200} • Context window size m E 11, 5,10} BROWN requires only the number of clusters as a hyperparameter. Here, we perform clustering with b E 1250, 500, 1000, 2000, 4000} clusters. 3 Sequence Labelling Tasks We evaluate the different word representations over four sequence labelling tasks: POS tagging (POS tagging), full-text chunking (Chunking), NER (NER), and MWE identification (MWE). For each task, we fed features into a first-order linear-chain graph transformer (Collobert et al., 2011) made up of two layers: the upper layer is identical to a linear-chain CRF (Lafferty et al., 2001), and the lower layer consists of word representation and hand-crafted features. If we treat word representations as fixed, the graph transformer is a simple linear-chain CRF. On the other hand, if we can treat the word representations as model parameters, the model is equivalent to a neural network with word embeddings as the input layer, as shown in Figure 1. We trained all models using AdaGrad (Duchi et al., 2011). As in Turian et al. (2010), at each word position, we construct word representat</context>
<context position="25955" citStr="Collobert et al. (2011)" startWordPosition="4203" endWordPosition="4206">tate of the art (Table 3). The 2.7% difference between our NER system and the best performing system is due to the fact that we use a first-order instead of a second-order CRF (Ando and Zhang, 2005), and for the other tasks, there are similarly differences in the learner and the complexity of the features used. Another difference is that we tuned the hyperparameters with random search, to enable replication using the same random seed. In contrast, the hyperparameters for the state-of-the-art methods are tuned more extensively by experts, making them more difficult to reproduce. 5 Related Work Collobert et al. (2011) proposed a unified neural network framework that learns word embeddings and applied it to POS tagging, Chunking, NER and semantic role labelling. When they combined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different w</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="12074" citStr="Duchi et al., 2011" startWordPosition="1932" endWordPosition="1935">each task, we fed features into a first-order linear-chain graph transformer (Collobert et al., 2011) made up of two layers: the upper layer is identical to a linear-chain CRF (Lafferty et al., 2001), and the lower layer consists of word representation and hand-crafted features. If we treat word representations as fixed, the graph transformer is a simple linear-chain CRF. On the other hand, if we can treat the word representations as model parameters, the model is equivalent to a neural network with word embeddings as the input layer, as shown in Figure 1. We trained all models using AdaGrad (Duchi et al., 2011). As in Turian et al. (2010), at each word position, we construct word representation features from the words in a context window of size two to either 85 side of the target word, based on the pre-trained representation of each word type. For BROWN, the features are the prefix features extracted from word clusters in the same way as Turian et al. (2010). As a baseline (and to test RQ1), we include a one-hot representation (which is equivalent to a linear-chain CRF with only lexical context features). Our hand-crafted features for POS tagging, Chunking and MWE, are those used by Collobert et al</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Scott Deerwester</author>
<author>Richard Harshman</author>
</authors>
<title>Using latent semantic analysis to improve access to textual information.</title>
<date>1988</date>
<booktitle>In Proceedings of the SIGCHI conference on Human Factors in Computing Systems,</booktitle>
<pages>281--285</pages>
<contexts>
<context position="5612" citStr="Dumais et al., 1988" startWordPosition="841" endWordPosition="844">varieties of word representations: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (</context>
</contexts>
<marker>Dumais, Furnas, Landauer, Deerwester, Harshman, 1988</marker>
<rawString>Susan T Dumais, George W Furnas, Thomas K Landauer, Scott Deerwester, and Richard Harshman. 1988. Using latent semantic analysis to improve access to textual information. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 281–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>110--120</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="26625" citStr="Guo et al. (2014)" startWordPosition="4298" endWordPosition="4301">rns word embeddings and applied it to POS tagging, Chunking, NER and semantic role labelling. When they combined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that BROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, wherein the models are trained with training sets of varying size. Bansal et al. (2014) reported that direct use of word embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 110– 120, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Marzieh Razavi</author>
<author>Anoop Sarkar</author>
</authors>
<title>An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing.</title>
<date>2011</date>
<booktitle>In ACL 2011 (Short Papers),</booktitle>
<pages>710--714</pages>
<location>Portland, USA.</location>
<contexts>
<context position="27726" citStr="Haffari et al. (2011)" startWordPosition="4475" endWordPosition="4478">pendency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we eval</context>
</contexts>
<marker>Haffari, Razavi, Sarkar, 2011</marker>
<rawString>Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar. 2011. An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing. In ACL 2011 (Short Papers), pages 710–714, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<title>UMBC EBIQUITY CORE: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>44--52</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="10410" citStr="Han et al., 2013" startWordPosition="1660" endWordPosition="1663">eir respective word classes. Then km J(w,ctx(w)) = −log p(wk|wk−1 k−m). Since there is no tractable method to find an optimal partition of word classes, the method uses only a bigram class model, and utilises hierarchical clustering as an approximation method to find a sufficiently good partition of words. 2.3 Building Word Representations To ensure the comparison of different word representations is fair, we train BROWN, CBOW, SKIP-GRAM, and GLOVE on a fixed corpus, comprised of freely available corpora, as detailed in Table 1. The joint corpus was preprocessed with Data set Size Words UMBC (Han et al., 2013) 48.1GB 3G One Billion (Chelba et al., 2013) 4.1GB 1G English Wikipedia 49.6GB 3G Table 1: Corpora used to pre-train the word embeddings Figure 1: Linear-chain graph transformer the Stanford CoreNLP sentence splitter and tokeniser. All consecutive digit substrings were replaced by NUMf, where f is the length of the digit substring (e.g., 10.20 is replaced by NUM2.NUM2. The dimensionality of the word embeddings and the size of the context window are the key hyperparameters when learning distributed representations. We use all combinations of the following values to train word embeddings on the </context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013. UMBC EBIQUITY CORE: Semantic textual similarity systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, pages 44–52, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Honkela</author>
</authors>
<title>Self-organizing maps of words for natural language processing applications.</title>
<date>1997</date>
<booktitle>In Proceedings of the International ICSC Symposium on Soft Computing,</booktitle>
<pages>401--407</pages>
<location>Nimes, France.</location>
<contexts>
<context position="5403" citStr="Honkela, 1997" startWordPosition="810" endWordPosition="811">-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 Word Representations 2.1 Types of Word Representations Turian et al. (2010) identifies three varieties of word representations: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the s</context>
</contexts>
<marker>Honkela, 1997</marker>
<rawString>Timo Honkela. 1997. Self-organizing maps of words for natural language processing applications. In Proceedings of the International ICSC Symposium on Soft Computing, pages 401–407, Nimes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence-labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>495--503</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="28130" citStr="Huang and Yates (2009)" startWordPosition="4537" endWordPosition="4540">by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling performance of OOV words for updated vs. nonupdated word embedding representations, relative to the training set and with out-of-domain data. 6 Conclusions We have performed an extensive extrinsic evaluation of four word embedding methods under fixed experimental conditions, and evaluated their applicability to four sequence labelling tasks: POS tagging, Chunking, NER and MWE identi</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, pages 495–503, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="27684" citStr="Koo et al. (2008)" startWordPosition="4467" endWordPosition="4470">at direct use of word embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS t</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT, pages 595–603, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>1288--1298</pages>
<location>Cambridge, USA.</location>
<contexts>
<context position="27703" citStr="Koo et al. (2010)" startWordPosition="4471" endWordPosition="4474">rd embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 1288–1298, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Williamstown, USA.</location>
<contexts>
<context position="11654" citStr="Lafferty et al., 2001" startWordPosition="1860" endWordPosition="1863">edding dim. d E 125, 50,100, 200} • Context window size m E 11, 5,10} BROWN requires only the number of clusters as a hyperparameter. Here, we perform clustering with b E 1250, 500, 1000, 2000, 4000} clusters. 3 Sequence Labelling Tasks We evaluate the different word representations over four sequence labelling tasks: POS tagging (POS tagging), full-text chunking (Chunking), NER (NER), and MWE identification (MWE). For each task, we fed features into a first-order linear-chain graph transformer (Collobert et al., 2011) made up of two layers: the upper layer is identical to a linear-chain CRF (Lafferty et al., 2001), and the lower layer consists of word representation and hand-crafted features. If we treat word representations as fixed, the graph transformer is a simple linear-chain CRF. On the other hand, if we can treat the word representations as model parameters, the model is equivalent to a neural network with word embeddings as the input layer, as shown in Figure 1. We trained all models using AdaGrad (Duchi et al., 2011). As in Turian et al. (2010), at each word position, we construct word representation features from the words in a context window of size two to either 85 side of the target word, </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282– 289, Williamstown, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization. In</title>
<date>2014</date>
<booktitle>Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2177--2185</pages>
<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="28086" citStr="Levy and Goldberg, 2014" startWordPosition="4529" endWordPosition="4532"> embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling performance of OOV words for updated vs. nonupdated word embedding representations, relative to the training set and with out-of-domain data. 6 Conclusions We have performed an extensive extrinsic evaluation of four word embedding methods under fixed experimental conditions, and evaluated their applicability to four sequence labelling task</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2177–2185. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="28106" citStr="Levy et al., 2015" startWordPosition="4533" endWordPosition="4536">te the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling performance of OOV words for updated vs. nonupdated word embedding representations, relative to the training set and with out-of-domain data. 6 Conclusions We have performed an extensive extrinsic evaluation of four word embedding methods under fixed experimental conditions, and evaluated their applicability to four sequence labelling tasks: POS tagging, Chun</context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3(1):211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Semisupervised sequence modeling with syntactic topic models.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="6252" citStr="Li and McCallum, 2005" startWordPosition="952" endWordPosition="955">l., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), </context>
</contexts>
<marker>Li, McCallum, 2005</marker>
<rawString>Wei Li and Andrew McCallum. 2005. Semisupervised sequence modeling with syntactic topic models. In Proceedings of the National Conference on Artificial Intelligence, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1030--1038</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6228" citStr="Lin and Wu, 2009" startWordPosition="948" endWordPosition="951"> or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1030–1038, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="5535" citStr="Lund and Burgess, 1996" startWordPosition="830" endWordPosition="833">tations 2.1 Types of Word Representations Turian et al. (2010) identifies three varieties of word representations: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods bu</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, &amp; Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13458" citStr="Marcus et al. (1993)" startWordPosition="2162" endWordPosition="2165">wo predictions, because we want to evaluate all word representations with the same type of model — a first-order graph transformer. In training the distributed word representations, we consider two settings: (1) the word representations are fixed during sequence model training; and (2) the graph transformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing training–dev–test splits where available:4 POS tagging: the Wall Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English W</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="6848" citStr="Mikolov et al., 2013" startWordPosition="1045" endWordPosition="1048">; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“SKIP-GRAM”) (Mikolov et al., 2013b), and Global vectors (“GLOVE”) (Pennington et al., 2014). All have been shown to be at or near state-of-the-art in recent empirical studies (Turian et al., 2010; Pennington et al., 2014).3 The training of these word representations is unsupervised: the common underlying idea is to predict the occurrence of words in the neighbouring context. Their training objectives share the same form, which is a sum of local training factors J(w, ctx(w)), L =1: J(w, ctx(w)) w∈T where T is the set of tokens in a given corpus, and ctx(w) d</context>
<context position="8869" citStr="Mikolov et al. (2013" startWordPosition="1391" endWordPosition="1394">log 84 where vctx(w) denotes the distributed representation of the local context of word w, and V is the vocabulary of a given corpus. CBOW derives vctx(w) based on averaging over the context words. That is, it estimates the probability of each w given its local context. In contrast, SKIP-GRAM applies softmax to each context word of a given occurrence of word w. In this case, vctx(w) corresponds to the representation of one of its context words. This model can be characterised as predicting context words based on w. In practice, softmax is too expensive to compute over large corpora, and thus Mikolov et al. (2013b) use hierarchical softmax and negative sampling to scale up the training. GLOVE assumes the dot product of two word embeddings should be similar to the logarithm of the co-occurrence count Xij of the two words. As such, the local factor J(w, ctx(w)) becomes: g(Xij)(vTi vj + bi + bj − log(Xij))2 where bi and bj are the bias terms of words i and j, respectively, and g(Xij) is a weighting function based on the co-occurrence count. This weighting function controls the degree of agreement between the parametric function vTi vj + bi + bj and log(Xij). Frequently co-occurring word pairs will have l</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="6848" citStr="Mikolov et al., 2013" startWordPosition="1045" endWordPosition="1048">; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“SKIP-GRAM”) (Mikolov et al., 2013b), and Global vectors (“GLOVE”) (Pennington et al., 2014). All have been shown to be at or near state-of-the-art in recent empirical studies (Turian et al., 2010; Pennington et al., 2014).3 The training of these word representations is unsupervised: the common underlying idea is to predict the occurrence of words in the neighbouring context. Their training objectives share the same form, which is a sum of local training factors J(w, ctx(w)), L =1: J(w, ctx(w)) w∈T where T is the set of tokens in a given corpus, and ctx(w) d</context>
<context position="8869" citStr="Mikolov et al. (2013" startWordPosition="1391" endWordPosition="1394">log 84 where vctx(w) denotes the distributed representation of the local context of word w, and V is the vocabulary of a given corpus. CBOW derives vctx(w) based on averaging over the context words. That is, it estimates the probability of each w given its local context. In contrast, SKIP-GRAM applies softmax to each context word of a given occurrence of word w. In this case, vctx(w) corresponds to the representation of one of its context words. This model can be characterised as predicting context words based on w. In practice, softmax is too expensive to compute over large corpora, and thus Mikolov et al. (2013b) use hierarchical softmax and negative sampling to scale up the training. GLOVE assumes the dot product of two word embeddings should be similar to the logarithm of the co-occurrence count Xij of the two words. As such, the local factor J(w, ctx(w)) becomes: g(Xij)(vTi vj + bi + bj − log(Xij))2 where bi and bj are the bias terms of words i and j, respectively, and g(Xij) is a weighting function based on the co-occurrence count. This weighting function controls the degree of agreement between the parametric function vTi vj + bi + bj and log(Xij). Frequently co-occurring word pairs will have l</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013),</booktitle>
<pages>380--390</pages>
<location>Atlanta, USA.</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013), pages 380–390, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>1532--1543</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="4033" citStr="Pennington et al., 2014" startWordPosition="597" endWordPosition="600">tting low-frequency and OOV words. In this paper, we perform an extensive evaluation of four recently proposed word embedding approaches under fixed experimental conditions, applied to four sequence labelling tasks: part-ofspeech (POS) tagging, full-text chunking, named entity recognition (NER), and multiword expres83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and evaluating them over more sequence labelling tasks. In addition, we explore the following research questions: RQ1: are word embeddings better than baseline approaches of one-hot unigram2 features and Brown clusters? RQ2: do word embeddings require less training data (i.e., generalise better) than one-hot unigram features? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrica</context>
<context position="6976" citStr="Pennington et al., 2014" startWordPosition="1063" endWordPosition="1067">ring algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“SKIP-GRAM”) (Mikolov et al., 2013b), and Global vectors (“GLOVE”) (Pennington et al., 2014). All have been shown to be at or near state-of-the-art in recent empirical studies (Turian et al., 2010; Pennington et al., 2014).3 The training of these word representations is unsupervised: the common underlying idea is to predict the occurrence of words in the neighbouring context. Their training objectives share the same form, which is a sum of local training factors J(w, ctx(w)), L =1: J(w, ctx(w)) w∈T where T is the set of tokens in a given corpus, and ctx(w) denotes the local context of word w. The local context of a word is conventionally its preceding m words, or alternatively the m </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 1532– 1543, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="6334" citStr="Pereira et al., 1993" startWordPosition="967" endWordPosition="970"> exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“SKIP-GRAM”) (Mikolov et al., 2013b), and Global v</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183– 190, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Alexis Battle</author>
<author>Honglak Lee</author>
<author>Benjamin Packer</author>
<author>Andrew Y Ng</author>
</authors>
<title>Self-taught learning: transfer learning from unlabeled data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>759--766</pages>
<location>Corvallis, USA.</location>
<contexts>
<context position="3312" citStr="Raina et al., 2007" startWordPosition="496" endWordPosition="499"> years, and scaled up to increasingly large corpora. As with other machine learning methods, it is well known that the quality of the pre-trained word embeddings depends heavily on factors including parameter optimisation, the size of the training data, and the fit with the target application. For example, Turian et al. (2010) showed that the optimal dimensionality for word embeddings is taskspecific. One factor which has received relatively little attention in NLP is the effect of “updating” the pre-trained word embeddings as part of the task-specific training, based on self-taught learning (Raina et al., 2007). Updating leads to word representations that are task-specific, but often at the cost of over-fitting low-frequency and OOV words. In this paper, we perform an extensive evaluation of four recently proposed word embedding approaches under fixed experimental conditions, applied to four sequence labelling tasks: part-ofspeech (POS) tagging, full-text chunking, named entity recognition (NER), and multiword expres83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics sion (MWE) ident</context>
</contexts>
<marker>Raina, Battle, Lee, Packer, Ng, 2007</marker>
<rawString>Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y Ng. 2007. Self-taught learning: transfer learning from unlabeled data. In Proceedings of the 24th International Conference on Machine Learning, pages 759–766, Corvallis, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software framework for topic modelling with large corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>51--55</pages>
<location>Valetta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 51–55, Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Institutionen f¨or lingvistik.</institution>
<contexts>
<context position="5362" citStr="Sahlgren, 2006" startWordPosition="804" endWordPosition="805">s (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 Word Representations 2.1 Types of Word Representations Turian et al. (2010) identifies three varieties of word representations: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in t</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. thesis, Institutionen f¨or lingvistik.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Emily Danchik</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Discriminative lexical semantic segmentation with gaps: Running the MWE gamut.</title>
<date>2014</date>
<journal>Transactions of the Association of Computational Linguistics,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5827" citStr="Schneider et al., 2014" startWordPosition="879" endWordPosition="883"> counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this catego</context>
<context position="12731" citStr="Schneider et al. (2014" startWordPosition="2048" endWordPosition="2051">ch word position, we construct word representation features from the words in a context window of size two to either 85 side of the target word, based on the pre-trained representation of each word type. For BROWN, the features are the prefix features extracted from word clusters in the same way as Turian et al. (2010). As a baseline (and to test RQ1), we include a one-hot representation (which is equivalent to a linear-chain CRF with only lexical context features). Our hand-crafted features for POS tagging, Chunking and MWE, are those used by Collobert et al. (2011), Turian et al. (2010) and Schneider et al. (2014b), respectively. For NER, we use the same feature space as Turian et al. (2010), except for the previous two predictions, because we want to evaluate all word representations with the same type of model — a first-order graph transformer. In training the distributed word representations, we consider two settings: (1) the word representations are fixed during sequence model training; and (2) the graph transformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing tra</context>
<context position="14013" citStr="Schneider et al. (2014" startWordPosition="2253" endWordPosition="2256">l Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English Web Treebank5 (“EWT”) For all tasks other than MWE,6 we additionally have an out-of-domain test set, in order to evaluate the out-of-domain robustness of the different 4For the MWE dataset, no such split pre-existed, so we constructed our own. 5https://catalog.ldc.upenn.edu/ LDC2012T13 6Unfortunately, there is no second domain which has been hand-tagged with MWEs using the method of Schneider et al. (2014b) to use as an out-of-domain test corpus. word representations, with and without updating. These datasets are as follows: POS tagging: the English </context>
<context position="21443" citStr="Schneider et al., 2014" startWordPosition="3444" endWordPosition="3447">e known word clusters such as days of the week and names of countries; the other half were selected randomly. Additional plots with 100 randomlysampled words and the top-100 most frequent words, for all the methods and all the tasks, can be found in the supplementary material and at 87 Task Benchmark In-domain Test set Out-of-domain Test set POS tagging (ACC) 0.972 (Toutanova et al., 2003) 0.959 (SKIP-GRAM+UP) 0.910 (SKIP-GRAM) Chunking (F1) 0.942 (Sha and Pereira, 2003) 0.938 (BROWNb=2000) 0.676 (GLOVE) NER (F1) 0.893 (Ando and Zhang, 2005) 0.868 (SKIP-GRAM) 0.736 (SKIP-GRAM) MWE (F1) 0.625 (Schneider et al., 2014a) 0.654 (CBOW+UP) — Table 3: State-of-the-art results vs. our best results for in-domain and out-of-domain test sets. (a) POS tagging (ACC) (b) Chunking (F1) (c) NER (F1) (d) MWE (F1) Figure 2: Results for each type of word representation over POS tagging, Chunking, NER and MWE, optionally with updating (“+UP”). The y-axis indicates the training data sizes (on a log scale). Green = high performance, and red = low performance, based on a linear scale of the best- to worst-result for each task. https://goo.gl/Y8bk2w. In each plot, a single arrow signifies one word, pointing from the position of</context>
<context position="26737" citStr="Schneider et al. (2014" startWordPosition="4318" endWordPosition="4321">bined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that BROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, wherein the models are trained with training sets of varying size. Bansal et al. (2014) reported that direct use of word embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and </context>
</contexts>
<marker>Schneider, Danchik, Dyer, Smith, 2014</marker>
<rawString>Nathan Schneider, Emily Danchik, Chris Dyer, and Noah A. Smith. 2014a. Discriminative lexical semantic segmentation with gaps: Running the MWE gamut. Transactions of the Association of Computational Linguistics, 2(1):193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Spencer Onuffer</author>
<author>Nora Kazour</author>
<author>Emily Danchik</author>
<author>Michael T Mordowanec</author>
<author>Henrietta Conrad</author>
<author>Noah A Smith</author>
</authors>
<title>Comprehensive annotation of multiword expressions in a social web corpus.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation,</booktitle>
<pages>455--461</pages>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="5827" citStr="Schneider et al., 2014" startWordPosition="879" endWordPosition="883"> counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = IV ), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this catego</context>
<context position="12731" citStr="Schneider et al. (2014" startWordPosition="2048" endWordPosition="2051">ch word position, we construct word representation features from the words in a context window of size two to either 85 side of the target word, based on the pre-trained representation of each word type. For BROWN, the features are the prefix features extracted from word clusters in the same way as Turian et al. (2010). As a baseline (and to test RQ1), we include a one-hot representation (which is equivalent to a linear-chain CRF with only lexical context features). Our hand-crafted features for POS tagging, Chunking and MWE, are those used by Collobert et al. (2011), Turian et al. (2010) and Schneider et al. (2014b), respectively. For NER, we use the same feature space as Turian et al. (2010), except for the previous two predictions, because we want to evaluate all word representations with the same type of model — a first-order graph transformer. In training the distributed word representations, we consider two settings: (1) the word representations are fixed during sequence model training; and (2) the graph transformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing tra</context>
<context position="14013" citStr="Schneider et al. (2014" startWordPosition="2253" endWordPosition="2256">l Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English Web Treebank5 (“EWT”) For all tasks other than MWE,6 we additionally have an out-of-domain test set, in order to evaluate the out-of-domain robustness of the different 4For the MWE dataset, no such split pre-existed, so we constructed our own. 5https://catalog.ldc.upenn.edu/ LDC2012T13 6Unfortunately, there is no second domain which has been hand-tagged with MWEs using the method of Schneider et al. (2014b) to use as an out-of-domain test corpus. word representations, with and without updating. These datasets are as follows: POS tagging: the English </context>
<context position="21443" citStr="Schneider et al., 2014" startWordPosition="3444" endWordPosition="3447">e known word clusters such as days of the week and names of countries; the other half were selected randomly. Additional plots with 100 randomlysampled words and the top-100 most frequent words, for all the methods and all the tasks, can be found in the supplementary material and at 87 Task Benchmark In-domain Test set Out-of-domain Test set POS tagging (ACC) 0.972 (Toutanova et al., 2003) 0.959 (SKIP-GRAM+UP) 0.910 (SKIP-GRAM) Chunking (F1) 0.942 (Sha and Pereira, 2003) 0.938 (BROWNb=2000) 0.676 (GLOVE) NER (F1) 0.893 (Ando and Zhang, 2005) 0.868 (SKIP-GRAM) 0.736 (SKIP-GRAM) MWE (F1) 0.625 (Schneider et al., 2014a) 0.654 (CBOW+UP) — Table 3: State-of-the-art results vs. our best results for in-domain and out-of-domain test sets. (a) POS tagging (ACC) (b) Chunking (F1) (c) NER (F1) (d) MWE (F1) Figure 2: Results for each type of word representation over POS tagging, Chunking, NER and MWE, optionally with updating (“+UP”). The y-axis indicates the training data sizes (on a log scale). Green = high performance, and red = low performance, based on a linear scale of the best- to worst-result for each task. https://goo.gl/Y8bk2w. In each plot, a single arrow signifies one word, pointing from the position of</context>
<context position="26737" citStr="Schneider et al. (2014" startWordPosition="4318" endWordPosition="4321">bined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that BROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, wherein the models are trained with training sets of varying size. Bansal et al. (2014) reported that direct use of word embeddings in dependency parsing did not show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and </context>
</contexts>
<marker>Schneider, Onuffer, Kazour, Danchik, Mordowanec, Conrad, Smith, 2014</marker>
<rawString>Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily Danchik, Michael T. Mordowanec, Henrietta Conrad, and Noah A. Smith. 2014b. Comprehensive annotation of multiword expressions in a social web corpus. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, pages 455–461, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>134--141</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="21296" citStr="Sha and Pereira, 2003" startWordPosition="3422" endWordPosition="3425">g, using 2- d vector fields generated using matplotlib and tSNE (van der Maaten and Hinton, 2008). Half of the words were chosen manually to include known word clusters such as days of the week and names of countries; the other half were selected randomly. Additional plots with 100 randomlysampled words and the top-100 most frequent words, for all the methods and all the tasks, can be found in the supplementary material and at 87 Task Benchmark In-domain Test set Out-of-domain Test set POS tagging (ACC) 0.972 (Toutanova et al., 2003) 0.959 (SKIP-GRAM+UP) 0.910 (SKIP-GRAM) Chunking (F1) 0.942 (Sha and Pereira, 2003) 0.938 (BROWNb=2000) 0.676 (GLOVE) NER (F1) 0.893 (Ando and Zhang, 2005) 0.868 (SKIP-GRAM) 0.736 (SKIP-GRAM) MWE (F1) 0.625 (Schneider et al., 2014a) 0.654 (CBOW+UP) — Table 3: State-of-the-art results vs. our best results for in-domain and out-of-domain test sets. (a) POS tagging (ACC) (b) Chunking (F1) (c) NER (F1) (d) MWE (F1) Figure 2: Results for each type of word representation over POS tagging, Chunking, NER and MWE, optionally with updating (“+UP”). The y-axis indicates the training data sizes (on a log scale). Green = high performance, and red = low performance, based on a linear scal</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 134–141, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1281--1290</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="27902" citStr="Spitkovsky et al., 2011" startWordPosition="4503" endWordPosition="4506">from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling performance of OOV words for updated vs. nonupdated word embedding representations, relative to the training set and with out-of-domain data. 6 Conclusions W</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and Daniel Jurafsky. 2011. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1281–1290, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Conference on Computational Natural Language Learning (CoNLL-2000),</booktitle>
<pages>127--132</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="13763" citStr="Sang and Buchholz, 2000" startWordPosition="2212" endWordPosition="2215">sformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing training–dev–test splits where available:4 POS tagging: the Wall Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English Web Treebank5 (“EWT”) For all tasks other than MWE,6 we additionally have an out-of-domain test set, in order to evaluate the out-of-domain robustness of the different 4For the MWE dataset, no such split pre-existed, so we constructed our own. 5https://catalog.ldc.upenn.edu/ LDC2012T13 6Unfortunately, the</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the 4th Conference on Computational Natural Language Learning (CoNLL-2000), pages 127–132, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning (CoNLL-2003),</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the 7th Conference on Natural Language Learning (CoNLL-2003), pages 142–147, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="21213" citStr="Toutanova et al., 2003" startWordPosition="3411" endWordPosition="3414">, we sampled 60 words and plotted the changes in their word embeddings under updating, using 2- d vector fields generated using matplotlib and tSNE (van der Maaten and Hinton, 2008). Half of the words were chosen manually to include known word clusters such as days of the week and names of countries; the other half were selected randomly. Additional plots with 100 randomlysampled words and the top-100 most frequent words, for all the methods and all the tasks, can be found in the supplementary material and at 87 Task Benchmark In-domain Test set Out-of-domain Test set POS tagging (ACC) 0.972 (Toutanova et al., 2003) 0.959 (SKIP-GRAM+UP) 0.910 (SKIP-GRAM) Chunking (F1) 0.942 (Sha and Pereira, 2003) 0.938 (BROWNb=2000) 0.676 (GLOVE) NER (F1) 0.893 (Ando and Zhang, 2005) 0.868 (SKIP-GRAM) 0.736 (SKIP-GRAM) MWE (F1) 0.625 (Schneider et al., 2014a) 0.654 (CBOW+UP) — Table 3: State-of-the-art results vs. our best results for in-domain and out-of-domain test sets. (a) POS tagging (ACC) (b) Chunking (F1) (c) NER (F1) (d) MWE (F1) Figure 2: Results for each type of word representation over POS tagging, Chunking, NER and MWE, optionally with updating (“+UP”). The y-axis indicates the training data sizes (on a log </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 173–180, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A fast, accurate, non-projective, semantically-enriched parser.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1257--1268</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="27749" citStr="Tratz and Hovy (2011)" startWordPosition="4479" endWordPosition="4482">t show improvement. They achieved an improvement only when they performed hierarchical clustering of the word embeddings, and used features extracted from the cluster hierarchy. In a similar vein, Andreas and Klein (2014) explored the use of word embeddings for constituency parsing and concluded that the information contained in word embeddings might duplicate the one acquired by a syntactic parser, unless the training set is extremely small. Other syntactic parsing studies that reported improvements by using word embeddings include Koo et al. (2008), Koo et al. (2010), Haffari et al. (2011), Tratz and Hovy (2011) and Chen and Manning (2014). Word embeddings have also been applied to other (non-sequential NLP) tasks like grammar induction (Spitkovsky et al., 2011), and semantic tasks such as semantic relatedness, synonymy detection, concept categorisation, selectional preference learning and analogy (Baroni et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). Huang and Yates (2009) demonstrated that using distributional word representations methods (like TF-IDF and LSA) as features, improves the labelling of OOV, when test for POS tagging and Chunking. In our study, we evaluate the labelling perf</context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1257–1268, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<location>Uppsala,</location>
<contexts>
<context position="1702" citStr="Turian et al., 2010" startWordPosition="246" endWordPosition="249">nces are sufficient to achieve competitive results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d « |V J. Words that are similar (e.g., with respect to syntax or lexical semantics) will ideally be mapped to similar regions of the vector space, implicitly supporting both generalisation across in-vocabulary (IV) items, and countering the effects of data sparsity for low-frequency and out-of-vocabulary (OOV) items. Without some means of automatically deriving the vector representations wit</context>
<context position="3021" citStr="Turian et al. (2010)" startWordPosition="450" endWordPosition="453">nately, it has been shown that they can be “pre-trained” from unlabelled text data using various algorithms to model the distributional hypothesis (i.e., that words which occur in similar contexts tend to be semantically similar). Pre-training methods have been refined considerably in recent years, and scaled up to increasingly large corpora. As with other machine learning methods, it is well known that the quality of the pre-trained word embeddings depends heavily on factors including parameter optimisation, the size of the training data, and the fit with the target application. For example, Turian et al. (2010) showed that the optimal dimensionality for word embeddings is taskspecific. One factor which has received relatively little attention in NLP is the effect of “updating” the pre-trained word embeddings as part of the task-specific training, based on self-taught learning (Raina et al., 2007). Updating leads to word representations that are task-specific, but often at the cost of over-fitting low-frequency and OOV words. In this paper, we perform an extensive evaluation of four recently proposed word embedding approaches under fixed experimental conditions, applied to four sequence labelling tas</context>
<context position="4974" citStr="Turian et al. (2010)" startWordPosition="746" endWordPosition="749">ining data (i.e., generalise better) than one-hot unigram features? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 Word Representations 2.1 Types of Word Representations Turian et al. (2010) identifies three varieties of word representations: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techni</context>
<context position="7080" citStr="Turian et al., 2010" startWordPosition="1083" endWordPosition="1086">hods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd, where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“BROWN”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“SKIP-GRAM”) (Mikolov et al., 2013b), and Global vectors (“GLOVE”) (Pennington et al., 2014). All have been shown to be at or near state-of-the-art in recent empirical studies (Turian et al., 2010; Pennington et al., 2014).3 The training of these word representations is unsupervised: the common underlying idea is to predict the occurrence of words in the neighbouring context. Their training objectives share the same form, which is a sum of local training factors J(w, ctx(w)), L =1: J(w, ctx(w)) w∈T where T is the set of tokens in a given corpus, and ctx(w) denotes the local context of word w. The local context of a word is conventionally its preceding m words, or alternatively the m words surrounding it. Local training factors are designed to capture the relationship between w and its </context>
<context position="12102" citStr="Turian et al. (2010)" startWordPosition="1938" endWordPosition="1941">into a first-order linear-chain graph transformer (Collobert et al., 2011) made up of two layers: the upper layer is identical to a linear-chain CRF (Lafferty et al., 2001), and the lower layer consists of word representation and hand-crafted features. If we treat word representations as fixed, the graph transformer is a simple linear-chain CRF. On the other hand, if we can treat the word representations as model parameters, the model is equivalent to a neural network with word embeddings as the input layer, as shown in Figure 1. We trained all models using AdaGrad (Duchi et al., 2011). As in Turian et al. (2010), at each word position, we construct word representation features from the words in a context window of size two to either 85 side of the target word, based on the pre-trained representation of each word type. For BROWN, the features are the prefix features extracted from word clusters in the same way as Turian et al. (2010). As a baseline (and to test RQ1), we include a one-hot representation (which is equivalent to a linear-chain CRF with only lexical context features). Our hand-crafted features for POS tagging, Chunking and MWE, are those used by Collobert et al. (2011), Turian et al. (201</context>
<context position="15437" citStr="Turian et al., 2010" startWordPosition="2479" endWordPosition="2482"> entity recognition corpus7 (“MUC7”) For reproducibility, we tuned the hyperparameters with random search over the development data for each task (Bergstra and Bengio, 2012). In this, we randomly sampled 50 distinct hyperparameter sets with the same random seed for the non-updating models (i.e., the models that don’t update the word representation), and sampled 100 distinct hyperparameter sets for the updating models (i.e., the models that do). For each set of hyperparameters and task, we train a model over its training set and choose the best one based on its performance on development data (Turian et al., 2010). We also tune the word representation hyperparameters — namely, the word vector size d and context window size m (distributed representations), and in the case of Brown, the number of clusters. For the updating models, we found that the results over the test data were always inferior to those that do not update the word representations, due to the higher number of hyperparameters and small sample size (i.e., 100). Since the two-layer model of the graph transformer contains a distinct set of hyperparameters for each layer, we reuse the best-performing hyperparameter settings from the non-updat</context>
<context position="26364" citStr="Turian et al. (2010)" startWordPosition="4261" endWordPosition="4264">sing the same random seed. In contrast, the hyperparameters for the state-of-the-art methods are tuned more extensively by experts, making them more difficult to reproduce. 5 Related Work Collobert et al. (2011) proposed a unified neural network framework that learns word embeddings and applied it to POS tagging, Chunking, NER and semantic role labelling. When they combined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that BROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tas</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="5387" citStr="Turney and Pantel, 2010" startWordPosition="806" endWordPosition="809">he training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 Word Representations 2.1 Types of Word Representations Turian et al. (2010) identifies three varieties of word representations: distributional, clusterbased, and distributed. Distributional representation methods map each word w to a context word vector Cw, which is constructed directly from co-occurrence counts between w and its context words. The learning methods either store the co-occurrence counts between two words w and i directly in Cwi (Sahlgren, 2006; Turney and Pantel, 2010; Honkela, 1997) or project the concurrence counts between words into a lower dimensional space (ˇReh˚uˇrek and Sojka, 2010; Lund and Burgess, 1996), using dimensionality reduction techniques such as SVD (Dumais et al., 1988) or LDA (Blei et al., 2003). 1MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens J P van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing high-dimensional data using t-sne.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--2579</pages>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>Laurens J.P. van der Maaten and Geoffrey Hinton. 2008. Visualizing high-dimensional data using t-sne. Journal of Machine Learning Research, 9:2579–2605.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>