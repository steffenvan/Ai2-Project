<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9965315">
Entity Linking Korean Text: An Unsupervised Learning Approach using
Semantic Relations
</title>
<author confidence="0.916685">
Youngsik Kim
</author>
<affiliation confidence="0.6856">
KAIST / Korea, The Republic of
</affiliation>
<email confidence="0.94106">
twilight@kaist.ac.kr
</email>
<note confidence="0.622923">
Key-Sun Choi
KAIST / Korea, The Republic of
</note>
<email confidence="0.983207">
kschoi@kaist.edu
</email>
<sectionHeader confidence="0.993535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897761904762">
Although entity linking is a widely re-
searched topic, the same cannot be said for
entity linking geared for languages other
than English. Several limitations includ-
ing syntactic features and the relative lack
of resources prevent typical approaches to
entity linking to be used as effectively for
other languages in general. We describe
an entity linking system that leverage se-
mantic relations between entities within an
existing knowledge base to learn and per-
form entity linking using a minimal en-
vironment consisting of a part-of-speech
tagger. We measure the performance of
our system against Korean Wikipedia ab-
stract snippets, using the Korean DBpe-
dia knowledge base for training. Based on
these results, we argue both the feasibil-
ity of our system and the possibility of ex-
tending to other domains and languages in
general.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989875">
A crucial step in creating the Web of Data is
the process of extracting structured data, or RDF
(Adida et al., 2012) from unstructured text. This
step enables machines to read and understand
unstructured Web pages that consist the major-
ity of the Web. Three tasks play a part in ex-
tracting RDF from unstructured text: Named en-
tity recognition(NER), where strings representing
named entities are extracted from the given text;
entity linking(EL), where each named entity rec-
ognized from NER is mapped to a appropriate re-
source from a knowledge base; and relation ex-
traction(Usbeck et al., 2014). Although entity
linking is an extensively researched field, most
research done is aimed primarily for the English
language. Research about entity linking for lan-
guages other than English have also been per-
formed (Jakob et al., 2013), but most state-of-art
entity linking systems are not fully language inde-
pendent.
The reason for this is two-fold. Firstly, most
entity linking systems depend on an existing sys-
tem to perform named entity recognition before-
hand. For instance, the system proposed by Us-
beck (2014) uses FOX (Speck et al., 2014) to
perform named entity recognition as the starting
point. The problem with this approach is that
an existing named entity recognition system is re-
quired, and thus the performance of entity linking
is bound by the performance of the named entity
recognition system. Named entity recognition sys-
tems for English achieve high performance even
by utilizing a simple dictionary-based approach
augmented by part-of-speech annotations, but this
approach does not work in all languages in gen-
eral. CJK1 languages in particular are difficult to
perform named entity recognition on because lan-
guage traits such as capitalization and strict token
separation by white-space do not exist. Other ap-
proaches to named entity recognition such as sta-
tistical models or projection models respectively
require a large amount of annotated training data
and an extensive parallel corpus to work, but the
cost of creating these resources is also non-trivial.
Some approaches to entity linking utilizing su-
pervised and semi-supervised learning also suffer
from the lack of manually annotated training data
for some languages. Currently, there is no proper
golden standard dataset for entity linking for the
Korean language.
In this paper, we present an entity linking sys-
tem for Korean that overcomes these obstacles
with an unsupervised learning approach utilizing
semantic relations between entities obtained from
a given knowledge base. Our system uses these
semantic relations as hints to learn feature values
</bodyText>
<note confidence="0.435833">
1Chinese, Japanese, Korean
</note>
<page confidence="0.924197">
132
</page>
<note confidence="0.9807335">
Proceedings of the 19th Conference on Computational Language Learning, pages 132–141,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99621675">
for both named entity recognition and entity link-
ing. In Section 3, we describe the requirements
of our system, and present the architecture of both
the training and actual entity linking processes. In
Section 4, we compare the performance of our sys-
tem against both a rule-based baseline and the cur-
rent state-of-art system based on Kim’s (2014) re-
search.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999967326086957">
The current state-of-art entity linking system for
Korean handles both named entity recognition and
entity linking as two separate steps (Kim et al.,
2014). This system uses hyperlinks within the Ko-
rean Wikipedia and a small amount of text man-
ually annotated with entity information as train-
ing data. It employs a SVM model trained with
character-based features to perform named entity
recognition, and uses the TF*ICF (Mendes et al.,
2011) and LDA metrics to disambiguate between
entity resources during entity linking. Kim (2014)
reports an F1-score of 75.66% for a simplified task
in which only surface forms and entities which ap-
pear as at least one hyperlink within the Korean
Wikipedia are recognized as potential entity can-
didates.
Our system utilizes relations between entities,
which can be said to be a graph-based approach
to entity linking. There have been recent research
about entity linking that exploit the graph struc-
ture of both the named entities within text and
RDF knowledge bases. Han (2011) uses a graph-
based collective method which can model and ex-
ploit the global interdependence between different
entity linking decisions. Alhelbawy (2014) uses
graph ranking combined with clique partitioning.
Moro (2014) introduces Babelfy, a unified graph-
based approach to entity linking and word sense
disambiguation based on a loose identification of
candidate meanings coupled with a densest sub-
graph heuristic which selects high-coherence se-
mantic interpretations. Usbeck (2014) combines
the Hypertext-Induced Topic Search (HITS) algo-
rithm with label expansion strategies and string
similarity measures.
There also has been research about named en-
tity recognition for Korean. Kim (2012) pro-
poses a method to automatically label multi-
lingual data with named entity tags, combin-
ing Wikipedia meta-data with information ob-
tained through English-foreign language parallel
Wikipedia sentences. We do not use this approach
in our system because our scope of entities is
wider than the named entity scope defined in the
MUC-7 annotation guidelines, which is the scope
of Kim’s research.
</bodyText>
<sectionHeader confidence="0.958365" genericHeader="method">
3 The System
</sectionHeader>
<bodyText confidence="0.8535099">
Due to the limitations of performing entity linking
for the Korean language described in Section 1,
our system is designed with some requirements in
mind. The requirements are:
• The system should be able to be trained and
ran within a minimal environment, which we
define as an existing RDF knowledge base
containing semantic relations between enti-
ties, and a part-of-speech tagger. In this pa-
per, we use the 2014 Korean DBpedia RDF
knowledge base and the ETRI Korean part-
of-speech tagger.
• The system should be able to perform en-
tity linking without using external informa-
tion not derived from the knowledge base.
We define the task of our system as follows:
Given a list of entity uniform resource identi-
fiers(URI) derived from the knowledge base, an-
notate any given text with the appropriate entity
URIs and their positions within the text.
</bodyText>
<subsectionHeader confidence="0.999156">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999839375">
The preprocessing step of our system consists
of querying the knowledge base to build a dic-
tionary of entity URIs and their respective sur-
face forms. As we are using the Korean DB-
pedia as our knowledge base, we define all re-
sources with a URI starting with the names-
pace ‘http://ko.dbpedia.org/resource/’ and which
are not mapped to disambiguation nor redirection
Wikipedia pages as valid entities. For each entity,
we define all literal strings connected via the prop-
erty ‘rdfs:label’ to the entity and all entities that
disambiguate or redirect to the entity as possible
surface forms.
The dictionary that results from preprocessing
contains 303,779 distinct entity URIs and 569,908
entity URI-surface form pairs.
</bodyText>
<subsectionHeader confidence="0.998125">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.9650665">
After the preprocessing step, our system performs
training by adjusting feature values based on data
</bodyText>
<page confidence="0.998528">
133
</page>
<bodyText confidence="0.999824333333334">
from a large amount of unannotated text docu-
ments. As the given text is not annotated with en-
tity data, we use the following assumption to help
distinguish potential entities within the text:
Assumption. Entity candidates which have a high
degree of semantic relations with nearby en-
tity candidates are likely actual entities. We
define an ‘entity candidate’ of a document as
a substring-entity URI pair in which the sub-
string appears at a specific position within the
document and the pair exists in the dictionary
created during preprocessing, and a ‘seman-
tic relation’ between two entity candidates c1,
c2 (RelPred(c1, c2)) as an undirected relation
consisting of all predicates in the knowledge
base that connect the entity URIs of the entity
candidates.
The basis for this assumption is that some men-
tions of ‘popular’(having a high degree within
the knowledge base RDF graph) entity URIs will
be accompanied with related terms, and that the
knowledge base will have RDF triples connecting
the entity to the other entity URIs representing the
related terms. Thus we assume that by selecting all
entity candidates with a high degree of semantic
relations, these candidates display features more
representative of actual entities than the remaining
entity candidates do.
For each document in the training set, we first
perform part-of-speech tagging to split the text
into individual morphemes. Because the concate-
nation of the morphemes of a Korean word is not
always identical to the word itself, we transform
the morphemes into ‘atomic’ sub-strings which
represent minimal building blocks for entity can-
didates and can have multiple POS tags.
After part-of-speech tagging is complete, we
then gather a set of entity candidates from the
document. We first find all non-overlapping sub-
strings within the document that correspond to
entity surface forms. It is possible for these
sub-strings to overlap with each other([[Chicago]
Bulls]); and because the average length of entity
surface forms in Korean is very short(between 2
to 3 characters), we opt to reduce the problem size
of the training process by choosing only the sub-
string with the longest length when multiple sub-
strings overlap with each other.
As to further reduce the number of entity candi-
dates we consider for training, we only use the en-
tity candidate with the most ‘popular’ entity URI
</bodyText>
<figureCaption confidence="0.895626">
Figure 1: All possible entity candidates within the
text fragment ‘Gyeongjong of Goryeo is’ (Kim et
al., 2014)
</figureCaption>
<bodyText confidence="0.9994054">
per group of candidates that share the same sub-
string within the document. We define the ‘popu-
larity’ of an entity URI in terms of the RDF triples
in the knowledge base that has the URI as the ob-
ject. More formally, we define UriPop(c) for an
entity candidate c with the entity URI cu with the
equations below. A larger UriPop value means a
more ‘popular’ entity URI, and Pred is meant to
prevent overly frequent predicates in the knowl-
edge base from dominating UriPop.
</bodyText>
<equation confidence="0.976725166666667">
�
UriPop(c) = log(
pEKBpredicates
X |ls|(s, p,cu) E KB}|)
Pred(p)=1 I {(s, o) |(s, p, o) E KB} |2
— |l(s, p, o) E KB} |( )
</equation>
<bodyText confidence="0.999867333333333">
At this stage of the training process, we have
a set of non-overlapping entity candidates. We
now classify these candidates into two classes:
eT(entity) and eF(non-entity) according to our
previous assumption. We measure the degree
of semantic relations of an entity candidate c,
S emRel(c), with the following equation where Nc
is the set of entity candidates which are within 20
words from the target candidate in the document:
</bodyText>
<equation confidence="0.991954">
SemRel(c) = Ec&apos;ENc EpERelPred(c,c&apos;) Pred(p)
|Nc|
</equation>
<bodyText confidence="0.999966571428571">
Since we do not have enough evidence at this
point to distinguish entities from non-entities,
we use semantic relations from all nearby en-
tity candidates to calculate S emRel. We order
the entity candidates into a list in decreasing or-
der of S emRel, and classify entity candidates
from the start of the list into eT until either 1)
</bodyText>
<equation confidence="0.3660105">
eT
document word # &gt; lu, or 2) S emRel(c) &lt; ls are
</equation>
<bodyText confidence="0.99986725">
satisfied, where both lu, and ls are constants that
continuously get adjusted during the training pro-
cess. The remaining entity candidates all get clas-
sified into eF.
</bodyText>
<equation confidence="0.9783795">
Pred(p) (1)
(3)
</equation>
<page confidence="0.970639">
134
</page>
<bodyText confidence="0.99995">
We now update the current feature values based
on the entity candidates in eT and eF. For each
feature f, we first define two sub-classes efT =
{e|e has f ∧ e E eT} and efF = {e|e has f ∧ e E eF}.
We then update the feature value of f from βα to
</bodyText>
<equation confidence="0.664482">
/1 α+|efT |, which represents the probability of an
Y+IefT |+|efF|
</equation>
<bodyText confidence="0.965856666666667">
entity candidate having feature f to be classified
into eT(is an entity). The full list of features we
used is shown below:
</bodyText>
<listItem confidence="0.936261954545454">
f1: String length The length (in characters) of
the sub-string of the candidate.
f2: POS tag The POS tag(s) of the sub-string of
the candidate. If multiple POS tags exist, we
take the average of the f2 values for each tag
as the representive f2 value.
f3: Head POS tag The first POS tag of the sub-
string of the candidate.
f4: Tail POS tag The last POS tag of the sub-
string of the candidate.
f5: Previous POS tag The POS tag of the sub-
string right before the candidate. If the can-
didate is preceded by white-space, we use the
special tag ‘BLANK’.
f6: Next POS tag The POS tag of the sub-string
right after the candidate. If the candidate is
followed by white-space, we use the special
tag ‘BLANK’.
f7: UriPop The UriPop score of the entity URI
of the candidate. Since UriPop has a con-
tinuous range, we keep separate features for
UriPop score intervals of 0.2.
</listItem>
<bodyText confidence="0.999326666666667">
Given these features, we define IndS core(c) of
an entity candidate c, which represents the overall
probability c would be classified in eT indepen-
dently of its surrounding context, as the average
of the feature values for c.
We also define WeightedS emRel(c) as the
amount of evidence via semantic relations c has
of being classified in eT. We define this score in
terms of semantic relations relative to the UriPop
score of c in order to positively consider entity
candidates which have more semantic relations
than their entity URIs would normally have.
Finally, we define EntityS core(c) representing
the overall evidence for c to be in eT. The respec-
tive equations for these scores are shown below.
</bodyText>
<equation confidence="0.991180428571429">
WeightedS emRel(c)
� � pERelPred(c,c&apos;) Pred(p) x UriPop(c&apos;)
c&apos;ENc
=
UriPop(c)
EntityS core(c) = IndS core(c)
+ WeightedSemRel(c)
</equation>
<bodyText confidence="0.999981307692308">
As we want to assign stronger evidence for se-
mantic relations with entity candidates that are
likely actual entities (as opposed to relations with
non-entities), we define WeightedS emRel to be
have a recursive relation with EntityS core.
We end the training process for a single doc-
ument by computing the EntityS core for each
entity candidate in eT and eF, and adding these
scores respectively into the lists of scores distT
and distF. As the EntityScore for the same en-
tity candidate will change as training proceeds, we
only maintain the 10,000 most recent scores for
both lists.
</bodyText>
<subsectionHeader confidence="0.997017">
3.3 Entity Linking
</subsectionHeader>
<bodyText confidence="0.99999572">
Since the actual entity linking process is also about
selecting entity candidates from the given docu-
ment, the process itself is similar to the training
process. We list the differences of the entity link-
ing process compared to training below.
When we choose the initial entity candidates,
we do not remove candidates with overlapping
sub-strings. Although we do limit the number of
candidates that map to the same sub-string, we
choose the top 3 candidates based on UriPop in-
stead of just 1.
We compute the EntityS core for each entity
candidate without performing candidate classifica-
tion nor feature value updates. Although the value
of EntityS core(c) is intended to be proportional
to the possibility the candidate c is actually an en-
tity, we need a way to define a threshold constant
to determine which candidates to actually classify
as entities. Thus, we then normalize any given
EntityS core(c) score of an entity candidate c into
a confidence score Conf(c), which represents the
relative probability of c being a member of eT
against being a member of eF. This is computed
by comparing the score against the lists distT and
distF, as shown in the following equations:
</bodyText>
<equation confidence="0.9729818">
|{x|x E distT ∧ x &lt; EntityScore(c)}|
ConfT(c) =
|distT|
135
�{x�x E distF ∧ x &gt; EntityScore(c)��
ConfF(c) =
IdistFJ
ConfT(c)
Con f (c) =
ConfT(c) + Conf F(c)
</equation>
<bodyText confidence="0.871875833333333">
Conf is a normalized score which satisfies 0 &lt;
Con f (c) &lt; 1 for any entity candidate c. This gives
us the flexibility to define a threshold 0 &lt; γ &lt; 1 so
that only entity candidates satisfying Conf (c) &gt;— γ
are classified as entities.
Algorithm 1 The entity selection algorithm
</bodyText>
<equation confidence="0.4508238">
E (-- []
for all c E C do
if Conf(c) &gt;— γ then
unsetE (-- []
valid = true
</equation>
<bodyText confidence="0.483867">
for all e E E do
if sub-string of c contains e then
</bodyText>
<equation confidence="0.546395466666667">
unsetE (-- unsetE + e
else if sub-string of c overlaps with e
then
valid = false
end if
end for
if valid is true then
E (-- E + c
for all e E unsetE do
E (-- E − e
end for
end if
end if
end for
return E
</equation>
<bodyText confidence="0.999835583333333">
Algorithm 1 shows the entity selection process,
where C is initialized as a list of all entity candi-
dates ordered by decreasing score of Conf. We
only select entity candidates which have a confi-
dence score of at least γ. When the sub-strings of
multiple entity candidates overlap with each other,
we prioritize the candidate with the highest con-
fidence with the exception of candidates that con-
tain(one is completely covered by the other, with-
out the two being identical) other candidates in
which we choose the candidate that contains the
other candidates.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.975678">
4.1 Entity Linking for Korean
4.1.1 The Dataset
</subsectionHeader>
<bodyText confidence="0.999991777777778">
Although the dataset used by Kim (2014) exists,
we do not use this dataset because it is for a sim-
plified version of the entity linking problem as dis-
cussed in Section 2. We instead have created a
new dataset intended to serve as answer data for
the entity linking for Korean task, based on guide-
lines that were derived from the TAC KBP 20142
guidelines. The guidelines used to annotate text
for our dataset are shown below:
</bodyText>
<listItem confidence="0.977493625">
• All entities must be tagged with an entity
URI that exists in the 2014 Korean DBpedia
knowledge base.
• All entities must be tagged with an entity URI
which is correct to appear within the context
of the entity.
• All entity URIs must identify a single thing,
not a group of things.
• Verbs and adjectives that have an equiva-
lent noun that is identified by an entity URI
should not be tagged as entities.
• Only words that directly identify the entity
should be tagged.
• If several possible entities are contained
within each other, the entity that contains all
other entities should be tagged.
• If several consecutive words each can be rep-
resented by the same entity URI, these words
must be tagged as a single entity, as opposed
to tagging each separate word.
• Indirect mentions of entity URIs (ex: pro-
nouns) should not be tagged even if co-
resolution can be performed in order to iden-
tify the entity URI the mention stands for.
</listItem>
<bodyText confidence="0.892944">
Our dataset consists of 60 Korean Wikipedia ab-
stract documents annotated by 3 different annota-
tors.
</bodyText>
<footnote confidence="0.962622">
2http://nlp.cs.rpi.edu/kbp/2014/elquery.pdf
</footnote>
<page confidence="0.997496">
136
</page>
<sectionHeader confidence="0.509059" genericHeader="method">
4.1.2 Evaluation
</sectionHeader>
<bodyText confidence="0.999968">
We evaluate our system against the work of Kim
(2014) in terms of precision, recall, and F1-score
metrics. As Kim’s (2014) system(which we will
refer to as KEL2014) was originally trained with
the Korean Wikipedia, it required only slight ad-
justments to properly operate for our dataset.
We first train our system using 4,000 documents
randomly gathered from the Korean Wikipedia.
We then evaluate our system with our dataset,
while adjusting the confidence threshold γ from
0.01 to 0.99 in increments of 0.01. We compare
our results with those of the best-performing set-
tings of KEL2014.
</bodyText>
<sectionHeader confidence="0.913419" genericHeader="method">
4.1.3 Results
</sectionHeader>
<figureCaption confidence="0.99138925">
Figure 2: The distribution of distT and distF after
training with 4,000 documents
The results of the training process is shown in
Figure 2. We can see that the EntityS core values
</figureCaption>
<bodyText confidence="0.955246380952381">
of entity candidates classified in eT are generally
higher than those in eF. This can be seen as evi-
dence to our claim that the features of entities with
a high degree of semantic relations can be used to
distinguish entities from non-entities in general.
We show additional evidence to this claim with
Table 1, which lists the feature values for the fea-
ture f1(sub-string length) after the training process
is complete. We observe that this feature gives rel-
atively little weight to entity candidates with a sub-
string of length 1, which is consistent with our ini-
tial observation that most of these candidates are
not entities.
Figure 3 shows the performance of our sys-
tem compared to the performance of KEL2014
against our dataset. As we increase the confidence
threshold, the recall decreases and the precision
increases. The maximum F1-score of our system
using our dataset, 0.630, was obtained with the
confidence threshold γ = 0.79. This is an im-
provement over KEL2014 which scored an F1-
</bodyText>
<table confidence="0.999655523809524">
Length eT eT + eF Value
1 37629 312543 0.120
2 41355 168672 0.245
3 20164 41098 0.490
4 12542 19619 0.637
5 13953 19722 0.704
6 5299 7503 0.698
7 3223 4187 0.753
8 2686 4282 0.616
9 2346 3034 0.751
10 922 1099 0.774
11 917 1011 0.830
12 477 546 0.750
13 249 276 0.687
14 218 274 0.615
15 155 170 0.618
16 109 116 0.567
17 99 118 0.523
18 48 52 0.434
19 49 55 0.433
20 34 40 0.384
</table>
<tableCaption confidence="0.8829845">
Table 1: Feature values of the feature f1 after train-
ing with 4,000 documents
</tableCaption>
<bodyText confidence="0.999938266666667">
score of 0.598. Although the performance dif-
ference itself is small, this shows that our system
trained with unannotated text documents performs
better than KEL2014, which is trained with both
partially annotated documents (Wikipedia articles)
and a small number of documents completely an-
notated with entity data. This also shows that
KEL2014 does not perform as well outside the
scope of the simplified version of entity linking it
was designed for.
Our system currently is implemented as a sim-
ple RESTful service where both training and ac-
tual entity linking are initiated via POST requests.
Our system can be deployed to a server at its initial
state or at a pre-trained state.
</bodyText>
<subsectionHeader confidence="0.9983685">
4.2 Alternative Methods and Deviations
4.2.1 Using Feature Subsets
</subsectionHeader>
<bodyText confidence="0.999944">
In order to test the effectiveness of each feature we
use in training, we compare the results obtained in
Section 4.1.3 against the performance of our sys-
tem trained with smaller subsets of the features
shown in Section 3.2. Using the same training data
as in Section 4.1.2, we evaluate the performance of
our system using two different subsets of features
as shown below:
</bodyText>
<page confidence="0.995948">
137
</page>
<figureCaption confidence="0.962566444444445">
Figure 3: The performance of our system against
KEL2014. The dots represent the results of our
system, and the cross represents the results of
KEL2014.
Figure 5: The performance of our system using the
feature subset Dev1.
Figure 4: A temporary web interface for our sys-
tem showing entity linking results for a sample
text snippet
</figureCaption>
<figure confidence="0.84536525">
Dev1 All features except POS-related ones: f1,
f7.
Dev2 All POS-related features only: f2, f3, f4, f5,
f6.
</figure>
<bodyText confidence="0.997778363636364">
Figures 5 and 6 shows the performance of our
system using the feature subsets Dev1 and Dev2.
For the feature subset Dev1, our system displays
a maximum F1-score of 0.433 with γ = 0.99; for
the features subset Dev2, our system performs best
with γ = 0.98 for an F1-score of 0.568.
As expected, both deviations perform worse
than our system trained with the full set of fea-
tures. We observe that the performance of Dev1
is significantly worse than that of Dev2. This sug-
gests that POS-based features are more important
</bodyText>
<figureCaption confidence="0.8613265">
Figure 6: The performance of our system using the
feature subset Dev2.
</figureCaption>
<bodyText confidence="0.9282535">
in distinguishing entities from non-entities than
the other features.
</bodyText>
<subsectionHeader confidence="0.940849">
4.2.2 Using SVM Models
</subsectionHeader>
<bodyText confidence="0.999734833333333">
Kim (2014) effectively utilized trained SVM mod-
els to detect entities within text. Based on Kim’s
experiments, we investigate whether SVM can be
also used to raise performance of our system.
Although we now need to base entity predic-
tions with a trained SVM model instead of the
confidence metric Conf, the training process de-
scribed in Section 3.2 is mostly unaltered because
it the classification of entity candidate into eT and
eF results in the training data required to train a
SVM binary classification model. The differences
in the training process are shown below:
</bodyText>
<listItem confidence="0.852482">
• After we process each document, a list of fea-
</listItem>
<bodyText confidence="0.736941285714286">
tures is produced for each entity candidate
classified as eT or eF. Instead of appending
the EntityS core values of these entity candi-
dates to distT and distF, we append the fea-
ture lists of each entity candidate themselves
into two lists of lists, listT and listF. These
lists still contain only the data of the newest
</bodyText>
<page confidence="0.990131">
138
</page>
<listItem confidence="0.723891">
10,000 entity candidates.
• After the entire training set is processed, we
</listItem>
<bodyText confidence="0.9851346">
use the list of feature lists in listT and listF
to train the SVM model. As the original fea-
tures are not all numbers, we transform each
list of features into 7-dimensional vectors by
replacing each feature key into the feature
value of the respective feature.
We use the same training data as in Section
4.1.2, and train a SVM model with a 3-degree
polynomial kernel. We choose this kernel accord-
ing to Kim’s (2014) work, where Kim compares
the performance of SVM for the task of entity link-
ing for Korean using multiple kernels.
Our system, using a trained SVM model results
in a F1-score of 0.569, which is about 0.07 lower
than the best performance of our system using the
confidence model. One possible reason our sys-
tem performed worse using SVM classification is
that the training data that we feed to the classi-
fier is not correctly classified, but rather based on
the semantic relations assumption in Section 3.2.
As this assumption does not cover any character-
istics of non-entities, the effectiveness of SVM de-
creases as many entity candidates which are actual
entities get classified as eF due to them not having
enough semantic relations.
</bodyText>
<subsectionHeader confidence="0.99927">
4.3 Application on Other Languages
</subsectionHeader>
<bodyText confidence="0.999979368421053">
As our system does not explicitly exploit any char-
acteristics of the Korean language, it theoretically
is a language-independent entity linking system.
We investigate this point using Japanese as a sam-
ple language, and MeCab3 as the part-of-speech
tagger.
As no proper dataset for the entity linking for
Korean task exists, we have created a new dataset
in order to measure the performance of our sys-
tem. As we believe many other languages will
also lack a proper dataset, we must devise an al-
ternative method to measure the performance of
our system for other languages in general.
We use Wikipedia documents and the links an-
notated within them as the dataset to use for mea-
suring performance of our system for languages
other than Korean. Although the manually anno-
tated links within Wikipedia documents do rep-
resent actual entities and their surface forms, we
</bodyText>
<footnote confidence="0.788257">
3http://taku910.github.io/mecab/
</footnote>
<bodyText confidence="0.984185172413793">
cannot completely rely on these links as answer
data because of the following reasons:
• The majority of entities within a Wikipedia
document are not actually tagged as links.
This includes self-references (entities that ap-
pear within the document describing that en-
tity), frequently appearing entities that were
tagged as links for their first few occurrences
but were not tagged afterwards, and entities
that were not considered important enough to
tag as links by the annotators. Due to the
existence of so many untagged entities, we
effectively cannot use precision as a perfor-
mance measure.
• Some links represent entities that are out-
side the scope of our research. This includes
links that point to non-existent Wikipedia
pages (‘red links’), and links that require co-
resolution to resolve.
Due to these problems, we only use the recall
metric to measure the approximate performance
of our system when using Wikipedia documents
as answer data. We compare the performance of
our system for Korean and Japanese by first train-
ing our system with 3,000 Wikipedia docuem-
nts, and measuring the recall of our system for
100 Wikipedia documents for both respective lan-
guages while adjusting the confidence threshold γ
from 0.01 to 0.99 in increments of 0.01.
</bodyText>
<figureCaption confidence="0.996084">
Figure 7: The recall of our system against Korean
and Japanese Wikipedia documents
Figure 7 shows the recall of our system against
</figureCaption>
<bodyText confidence="0.92756825">
Korean and Japanese Wikipedia documents. For
the confidence threshold optimized via the exper-
iments performed in Section 4.1 (γ = 0.79), our
system shows a large difference of recall between
</bodyText>
<page confidence="0.996584">
139
</page>
<bodyText confidence="0.9998032">
Korean and Japanese. Although this does not ac-
curately represent the actual performance of our
system, we leave the task of improving our system
to display consistent performance across multiple
languages as future work.
</bodyText>
<sectionHeader confidence="0.999213" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.999994">
As our system currently only uses labels of en-
tity URIs to determine surface forms of entities, it
can not detect entities with irregular surface forms.
For instance, the entity ‘Steve&apos;Jobs’ has the labels
‘Steve Jobs’ and ‘Jobs’ within the Korean DBpe-
dia knowledge base, but does not have the label
‘Steve’. This results in the inability of our system
to detect certain entities within our dataset, regard-
less of training. We plan to improve our system
to handle derivative surface forms such as ‘Steve’
for ‘Steve&apos;Jobs’, without relying on external dic-
tionaries if possible.
Kim (2014) shows that a SVM classifier using
character-based features trained with Wikipedia
articles achieves better named entity recogni-
tion performance than a rule-based classifier us-
ing part-of-speech tags. Although our system
uses trained features based on part-of-speech tags
rather than a rule-based method, we may be able to
remove even the part-of-speech tagger from the re-
quirements of our system by substituting these fea-
tures with the character-based features suggested
in Kim’s (2014) work.
Finally, future work must be performed about
replacing the part-of-speech tagger currently used
in our system with a chunking algorithm that does
not utilize supervised training. Our system cur-
rently utilizes the list of morphemes and their re-
spective POS tags that are produced from the part-
of-speech tagger. Since our system does not re-
quire this information to be completely accurate,
a dictionary-based approach to chunking might be
applicable as well.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985619047619">
In this paper, we present an entity linking system
for Korean that utilizes several features trained
with plain text documents. By taking an unsu-
pervised learning approach, our system is able to
perform entity linking with a minimal environ-
ment consisting of an RDF knowledge base and
a part-of-speech tagger. We compare the perfor-
mance of our system against the state-of-art sys-
tem KEL2014, and show that our system outper-
forms KEL2014 in terms of F1-score. We also
briefly describe variations to our system training
process, such as using feature subsets and utilizing
SVM models instead of our confidence metric.
Many languages including Korean are not as
rich in resources as the English language, and the
lack of resources might prohibit the state-of-art
systems for entity linking for English from per-
forming as well in other languages. By utilizing
a minimal amount of resources, our system may
provide a firm starting point for research about en-
tity linking for these languages.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999885166666667">
This work was supported by Institute for Infor-
mation &amp; communications Technology Promo-
tion(IITP) grant funded by the Korea govern-
ment(MSIP) (No. R0101-15-0054, WiseKB: Big
data based self-evolving knowledge base and rea-
soning platform)
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853066666667">
B. Adida, I. Herman, M. Sporny, and M. Birbeck.
RDFa 1.1 Primer. Technical report, World Wide Web
Consortium, http://www.w3.org/TR/2012/NOTE-
rdfa-primer- 20120607/, June 2012.
Usbeck, R., Ngomo, A. C. N., Rder, M., Gerber,
D., Coelho, S. A., Auer, S., and Both, A. (2014).
AGDISTIS-graph-based disambiguation of named
entities using linked data. In The Semantic We-
bISWC 2014 (pp. 457-471). Springer International
Publishing.
Daiber, J., Jakob, M., Hokamp, C., and Mendes, P. N.
(2013, September). Improving efficiency and accu-
racy in multilingual entity extraction. In Proceed-
ings of the 9th International Conference on Semantic
Systems (pp. 121-124). ACM.
Speck, R., and Ngomo, A. C. N. (2014). Ensemble
learning for named entity recognition. In The Se-
mantic WebISWC 2014 (pp. 519-534). Springer In-
ternational Publishing.
Y. Kim, Y. Hamn, J. Kim, D. Hwang, and K.-S. Choi,
A Non-morphological Approach for DBpedia URI
Spotting within Korean Text, Proc. of HCLT 2014,
Chuncheon, 2014.
P. N. Mendes, M. Jakob, A. Garca-Silve, and C. Bizer,
”DBpedia spotlight: shedding light on the web of
documents,” Proc. of i-SEMANTICS 2011 (7th Int.
Conf. on Semantic Systems, 2011.
Han, X., Sun, L., and Zhao, J. (2011, July). Collective
entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
</reference>
<page confidence="0.972158">
140
</page>
<reference confidence="0.999351">
conference on Research and development in Infor-
mation Retrieval (pp. 765-774). ACM.
Alhelbawy, Ayman, and Robert Gaizauskas. ”Col-
lective Named Entity Disambiguation using Graph
Ranking and Clique Partitioning Approaches.”
Moro, Andrea, Alessandro Raganato, and Roberto
Navigli. ”Entity linking meets word sense disam-
biguation: a unified approach.” Transactions of the
Association for Computational Linguistics 2 (2014):
231-244.
Kim, S., Toutanova, K., and Yu, H. (2012, July). Multi-
lingual named entity recognition using parallel data
and metadata from wikipedia. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1 (pp.
694-702). Association for Computational Linguis-
tics.
</reference>
<page confidence="0.998255">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.306098">
<title confidence="0.9994625">Entity Linking Korean Text: An Unsupervised Learning Approach using Semantic Relations</title>
<author confidence="0.980132">Youngsik Kim</author>
<title confidence="0.4432835">The Republic of twilight@kaist.ac.kr</title>
<author confidence="0.985804">Key-Sun Choi</author>
<affiliation confidence="0.933958">The Republic of</affiliation>
<email confidence="0.998421">kschoi@kaist.edu</email>
<abstract confidence="0.995360545454545">Although entity linking is a widely researched topic, the same cannot be said for entity linking geared for languages other than English. Several limitations including syntactic features and the relative lack of resources prevent typical approaches to linking to be used as for other languages in general. We describe an entity linking system that leverage semantic relations between entities within an existing knowledge base to learn and perform entity linking using a minimal environment consisting of a part-of-speech tagger. We measure the performance of our system against Korean Wikipedia abstract snippets, using the Korean DBpedia knowledge base for training. Based on these results, we argue both the feasibility of our system and the possibility of extending to other domains and languages in general.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Adida</author>
<author>I Herman</author>
<author>M Sporny</author>
<author>M Birbeck</author>
</authors>
<title>RDFa 1.1 Primer.</title>
<date>2012</date>
<journal>World Wide Web Consortium,</journal>
<tech>Technical report,</tech>
<volume>3</volume>
<pages>20120607</pages>
<contexts>
<context position="1185" citStr="Adida et al., 2012" startWordPosition="184" endWordPosition="187">ty linking system that leverage semantic relations between entities within an existing knowledge base to learn and perform entity linking using a minimal environment consisting of a part-of-speech tagger. We measure the performance of our system against Korean Wikipedia abstract snippets, using the Korean DBpedia knowledge base for training. Based on these results, we argue both the feasibility of our system and the possibility of extending to other domains and languages in general. 1 Introduction A crucial step in creating the Web of Data is the process of extracting structured data, or RDF (Adida et al., 2012) from unstructured text. This step enables machines to read and understand unstructured Web pages that consist the majority of the Web. Three tasks play a part in extracting RDF from unstructured text: Named entity recognition(NER), where strings representing named entities are extracted from the given text; entity linking(EL), where each named entity recognized from NER is mapped to a appropriate resource from a knowledge base; and relation extraction(Usbeck et al., 2014). Although entity linking is an extensively researched field, most research done is aimed primarily for the English languag</context>
</contexts>
<marker>Adida, Herman, Sporny, Birbeck, 2012</marker>
<rawString>B. Adida, I. Herman, M. Sporny, and M. Birbeck. RDFa 1.1 Primer. Technical report, World Wide Web Consortium, http://www.w3.org/TR/2012/NOTErdfa-primer- 20120607/, June 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Usbeck</author>
<author>A C N Ngomo</author>
<author>M Rder</author>
<author>D Gerber</author>
<author>S A Coelho</author>
<author>S Auer</author>
<author>A Both</author>
</authors>
<title>AGDISTIS-graph-based disambiguation of named entities using linked data.</title>
<date>2014</date>
<booktitle>In The Semantic WebISWC</booktitle>
<pages>457--471</pages>
<publisher>Springer International Publishing.</publisher>
<contexts>
<context position="1662" citStr="Usbeck et al., 2014" startWordPosition="260" endWordPosition="264">in general. 1 Introduction A crucial step in creating the Web of Data is the process of extracting structured data, or RDF (Adida et al., 2012) from unstructured text. This step enables machines to read and understand unstructured Web pages that consist the majority of the Web. Three tasks play a part in extracting RDF from unstructured text: Named entity recognition(NER), where strings representing named entities are extracted from the given text; entity linking(EL), where each named entity recognized from NER is mapped to a appropriate resource from a knowledge base; and relation extraction(Usbeck et al., 2014). Although entity linking is an extensively researched field, most research done is aimed primarily for the English language. Research about entity linking for languages other than English have also been performed (Jakob et al., 2013), but most state-of-art entity linking systems are not fully language independent. The reason for this is two-fold. Firstly, most entity linking systems depend on an existing system to perform named entity recognition beforehand. For instance, the system proposed by Usbeck (2014) uses FOX (Speck et al., 2014) to perform named entity recognition as the starting poi</context>
</contexts>
<marker>Usbeck, Ngomo, Rder, Gerber, Coelho, Auer, Both, 2014</marker>
<rawString>Usbeck, R., Ngomo, A. C. N., Rder, M., Gerber, D., Coelho, S. A., Auer, S., and Both, A. (2014). AGDISTIS-graph-based disambiguation of named entities using linked data. In The Semantic WebISWC 2014 (pp. 457-471). Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daiber</author>
<author>M Jakob</author>
<author>C Hokamp</author>
<author>P N Mendes</author>
</authors>
<title>Improving efficiency and accuracy in multilingual entity extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 9th International Conference on Semantic Systems</booktitle>
<pages>121--124</pages>
<publisher>ACM.</publisher>
<marker>Daiber, Jakob, Hokamp, Mendes, 2013</marker>
<rawString>Daiber, J., Jakob, M., Hokamp, C., and Mendes, P. N. (2013, September). Improving efficiency and accuracy in multilingual entity extraction. In Proceedings of the 9th International Conference on Semantic Systems (pp. 121-124). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Speck</author>
<author>A C N Ngomo</author>
</authors>
<title>Ensemble learning for named entity recognition.</title>
<date>2014</date>
<booktitle>In The Semantic WebISWC</booktitle>
<pages>519--534</pages>
<publisher>Springer International Publishing.</publisher>
<marker>Speck, Ngomo, 2014</marker>
<rawString>Speck, R., and Ngomo, A. C. N. (2014). Ensemble learning for named entity recognition. In The Semantic WebISWC 2014 (pp. 519-534). Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kim</author>
<author>Y Hamn</author>
<author>J Kim</author>
<author>D Hwang</author>
<author>K-S Choi</author>
</authors>
<title>A Non-morphological Approach for DBpedia URI Spotting within Korean Text,</title>
<date>2014</date>
<booktitle>Proc. of HCLT 2014, Chuncheon,</booktitle>
<contexts>
<context position="4425" citStr="Kim et al., 2014" startWordPosition="689" endWordPosition="692">2–141, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics for both named entity recognition and entity linking. In Section 3, we describe the requirements of our system, and present the architecture of both the training and actual entity linking processes. In Section 4, we compare the performance of our system against both a rule-based baseline and the current state-of-art system based on Kim’s (2014) research. 2 Related Work The current state-of-art entity linking system for Korean handles both named entity recognition and entity linking as two separate steps (Kim et al., 2014). This system uses hyperlinks within the Korean Wikipedia and a small amount of text manually annotated with entity information as training data. It employs a SVM model trained with character-based features to perform named entity recognition, and uses the TF*ICF (Mendes et al., 2011) and LDA metrics to disambiguate between entity resources during entity linking. Kim (2014) reports an F1-score of 75.66% for a simplified task in which only surface forms and entities which appear as at least one hyperlink within the Korean Wikipedia are recognized as potential entity candidates. Our system utili</context>
<context position="10546" citStr="Kim et al., 2014" startWordPosition="1669" endWordPosition="1672">orms. It is possible for these sub-strings to overlap with each other([[Chicago] Bulls]); and because the average length of entity surface forms in Korean is very short(between 2 to 3 characters), we opt to reduce the problem size of the training process by choosing only the substring with the longest length when multiple substrings overlap with each other. As to further reduce the number of entity candidates we consider for training, we only use the entity candidate with the most ‘popular’ entity URI Figure 1: All possible entity candidates within the text fragment ‘Gyeongjong of Goryeo is’ (Kim et al., 2014) per group of candidates that share the same substring within the document. We define the ‘popularity’ of an entity URI in terms of the RDF triples in the knowledge base that has the URI as the object. More formally, we define UriPop(c) for an entity candidate c with the entity URI cu with the equations below. A larger UriPop value means a more ‘popular’ entity URI, and Pred is meant to prevent overly frequent predicates in the knowledge base from dominating UriPop. � UriPop(c) = log( pEKBpredicates X |ls|(s, p,cu) E KB}|) Pred(p)=1 I {(s, o) |(s, p, o) E KB} |2 — |l(s, p, o) E KB} |( ) At thi</context>
</contexts>
<marker>Kim, Hamn, Kim, Hwang, Choi, 2014</marker>
<rawString>Y. Kim, Y. Hamn, J. Kim, D. Hwang, and K.-S. Choi, A Non-morphological Approach for DBpedia URI Spotting within Korean Text, Proc. of HCLT 2014, Chuncheon, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P N Mendes</author>
<author>M Jakob</author>
<author>A Garca-Silve</author>
<author>C Bizer</author>
</authors>
<title>DBpedia spotlight: shedding light on the web of documents,”</title>
<date>2011</date>
<booktitle>Proc. of i-SEMANTICS 2011 (7th Int. Conf. on Semantic Systems,</booktitle>
<contexts>
<context position="4710" citStr="Mendes et al., 2011" startWordPosition="736" endWordPosition="739">esses. In Section 4, we compare the performance of our system against both a rule-based baseline and the current state-of-art system based on Kim’s (2014) research. 2 Related Work The current state-of-art entity linking system for Korean handles both named entity recognition and entity linking as two separate steps (Kim et al., 2014). This system uses hyperlinks within the Korean Wikipedia and a small amount of text manually annotated with entity information as training data. It employs a SVM model trained with character-based features to perform named entity recognition, and uses the TF*ICF (Mendes et al., 2011) and LDA metrics to disambiguate between entity resources during entity linking. Kim (2014) reports an F1-score of 75.66% for a simplified task in which only surface forms and entities which appear as at least one hyperlink within the Korean Wikipedia are recognized as potential entity candidates. Our system utilizes relations between entities, which can be said to be a graph-based approach to entity linking. There have been recent research about entity linking that exploit the graph structure of both the named entities within text and RDF knowledge bases. Han (2011) uses a graphbased collecti</context>
</contexts>
<marker>Mendes, Jakob, Garca-Silve, Bizer, 2011</marker>
<rawString>P. N. Mendes, M. Jakob, A. Garca-Silve, and C. Bizer, ”DBpedia spotlight: shedding light on the web of documents,” Proc. of i-SEMANTICS 2011 (7th Int. Conf. on Semantic Systems, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>L Sun</author>
<author>J Zhao</author>
</authors>
<title>Collective entity linking in web text: a graph-based method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</booktitle>
<pages>765--774</pages>
<publisher>ACM.</publisher>
<marker>Han, Sun, Zhao, 2011</marker>
<rawString>Han, X., Sun, L., and Zhao, J. (2011, July). Collective entity linking in web text: a graph-based method. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval (pp. 765-774). ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ayman Alhelbawy</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Collective Named Entity Disambiguation using Graph Ranking and Clique Partitioning Approaches.”</title>
<marker>Alhelbawy, Gaizauskas, </marker>
<rawString>Alhelbawy, Ayman, and Robert Gaizauskas. ”Collective Named Entity Disambiguation using Graph Ranking and Clique Partitioning Approaches.”</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity linking meets word sense disambiguation: a unified approach.”</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics</journal>
<volume>2</volume>
<pages>231--244</pages>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Moro, Andrea, Alessandro Raganato, and Roberto Navigli. ”Entity linking meets word sense disambiguation: a unified approach.” Transactions of the Association for Computational Linguistics 2 (2014): 231-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>K Toutanova</author>
<author>H Yu</author>
</authors>
<title>Multilingual named entity recognition using parallel data and metadata from wikipedia.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>694--702</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Kim, Toutanova, Yu, 2012</marker>
<rawString>Kim, S., Toutanova, K., and Yu, H. (2012, July). Multilingual named entity recognition using parallel data and metadata from wikipedia. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1 (pp. 694-702). Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>