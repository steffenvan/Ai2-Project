<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000378">
<title confidence="0.999022">
Better Hypothesis Testing for Statistical Machine Translation:
Controlling for Optimizer Instability
</title>
<author confidence="0.997602">
Jonathan H. Clark Chris Dyer Alon Lavie Noah A. Smith
</author>
<affiliation confidence="0.889448333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998867">
{jhclark,cdyer,alavie,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998645">
In statistical machine translation, a researcher
seeks to determine whether some innovation
(e.g., a new feature, model, or inference al-
gorithm) improves translation quality in com-
parison to a baseline system. To answer this
question, he runs an experiment to evaluate the
behavior of the two systems on held-out data.
In this paper, we consider how to make such
experiments more statistically reliable. We
provide a systematic analysis of the effects of
optimizer instability—an extraneous variable
that is seldom controlled for—on experimen-
tal outcomes, and make recommendations for
reporting results more accurately.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975769230769">
The need for statistical hypothesis testing for ma-
chine translation (MT) has been acknowledged since
at least Och (2003). In that work, the proposed
method was based on bootstrap resampling and was
designed to improve the statistical reliability of re-
sults by controlling for randomness across test sets.
However, there is no consistently used strategy that
controls for the effects of unstable estimates of
model parameters.1 While the existence of opti-
mizer instability is an acknowledged problem, it is
only infrequently discussed in relation to the relia-
bility of experimental results, and, to our knowledge,
there has yet to be a systematic study of its effects on
</bodyText>
<footnote confidence="0.496842">
1We hypothesize that the convention of “trusting” BLEU
score improvements of, e.g., &gt; 1, is not merely due to an ap-
preciation of what qualitative difference a particular quantita-
tive improvement will have, but also an implicit awareness that
current methodology leads to results that are not consistently
reproducible.
</footnote>
<bodyText confidence="0.9997882">
hypothesis testing. In this paper, we present a series
of experiments demonstrating that optimizer insta-
bility can account for substantial amount of variation
in translation quality,2 which, if not controlled for,
could lead to incorrect conclusions. We then show
that it is possible to control for this variable with a
high degree of confidence with only a few replica-
tions of the experiment and conclude by suggesting
new best practices for significance testing for ma-
chine translation.
</bodyText>
<sectionHeader confidence="0.965071" genericHeader="introduction">
2 Nondeterminism and Other
Optimization Pitfalls
</sectionHeader>
<bodyText confidence="0.999530117647059">
Statistical machine translation systems consist of a
model whose parameters are estimated to maximize
some objective function on a set of development
data. Because the standard objectives (e.g., 1-best
BLEU, expected BLEU, marginal likelihood) are
not convex, only approximate solutions to the op-
timization problem are available, and the parame-
ters learned are typically only locally optimal and
may strongly depend on parameter initialization and
search hyperparameters. Additionally, stochastic
optimization and search techniques, such as mini-
mum error rate training (Och, 2003) and Markov
chain Monte Carlo methods (Arun et al., 2010),3
constitute a second, more obvious source of noise
in the optimization procedure.
This variation in the parameter vector affects the
quality of the model measured on both development
</bodyText>
<footnote confidence="0.994307333333333">
2This variation directly affects the output translations, and
so it will propagate to both automated metrics as well as human
evaluators.
3Online subgradient techniques such as MIZA (Crammer et
al., 2006; Chiang et al., 2008) have an implicit stochastic com-
ponent as well based on the order of the training examples.
</footnote>
<page confidence="0.944282">
176
</page>
<note confidence="0.5984665">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9855285625">
data and held-out test data, independently of any ex- explored regularization of MERT to improve gener-
perimental manipulation. Thus, when trying to de- alization on test sets. Moore and Quirk (2008) ex-
termine whether the difference between two mea- plored strategies for selecting better random “restart
surements is significant, it is necessary to control for points” in optimization. Cer et al. (2010) analyzed
variance due to noisy parameter estimates. This can the standard deviation over 5 MERT runs when each
be done by replication of the optimization procedure of several metrics was used as the objective function.
with different starting conditions (e.g., by running 4 Experiments
MERT many times). In our experiments, we ran the MERT optimizer to
Unfortunately, common practice in reporting ma- optimize BLEU on a held-out development set many
chine translation results is to run the optimizer once times to obtain a set of optimizer samples on two dif-
per system configuration and to draw conclusions ferent pairs of systems (4 configurations total). Each
about the experimental manipulation from this sin- pair consists of a baseline system (System A) and an
gle sample. However, it could be that a particu- “experimental” system (System B), which previous
lar sample is on the “low” side of the distribution research has suggested will perform better.
over optimizer outcomes (i.e., it results in relatively The first system pair contrasts a baseline phrase-
poorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchi-
The danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were con-
paired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus
useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with
identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007;
how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con-
3 Related Work trasts two German-English Hiero/cdec systems con-
The use of statistical hypothesis testing has grown structed from the WMT11 parallel training data
apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on
natural language processing. Bootstrap techniques unsegmented words, and the experimental system
(Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmenta-
in many problem areas, including for confidence tion of the German text according to the CRF word
estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese-
2004), and to determine the significance of MT re- English systems were optimized 300 times, and the
sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times.
Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation
tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two de-
nique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20
gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly
a significant difference where none exists; Riezler from the default ranges for each feature, and, at each
and Maxwell, 2005). Other uses in NLP include iteration, 200-best lists were extracted with the cur-
the MUC-6 evaluation (Chinchor, 1993) and pars- rent weight vector (Bertoldi et al., 2009). The cdec
ing (Cahill et al., 2008). However, these previous MERT implementation performs inference over the
methods assume model parameters are elements of decoder search space which is structured as a hyper-
the system rather than extraneous variables. graph (Kumar et al., 2009). Rather than using restart
Prior work on optimizer noise in MT has fo- points, in addition to optimizing each feature inde-
cused primarily on reducing optimizer instability pendently, it optimizes in 5 random directions per it-
(whereas our concern is how to deal with optimizer eration by constructing a search vector by uniformly
noise, when it exists). Foster and Kuhn (2009) mea- sampling each element of the vector from (−1, 1)
sured the instability of held-out BLEU scores across and then renormalizing so it has length 1. For all
10 MERT runs to improve tune/test set correlation. systems, the initial weight vector was manually ini-
However, they only briefly mention the implications tialized so as to yield reasonable translations.
of the instability on significance. Cer et al. (2008)
</bodyText>
<table confidence="0.979364823529412">
177
4http://statmt.org/wmt11/
Metric System Avg ssel sdev stest
BTEC Chinese-English (n = 300)
BLEU ↑ System A 48.4 1.6 0.2 0.5
System B 49.9 1.5 0.1 0.4
MET ↑ System A 63.3 0.9 - 0.4
System B 63.8 0.9 - 0.5
TER ↓ System A 30.2 1.1 - 0.6
System B 28.7 1.0 - 0.2
WMT German-English (n = 50)
BLEU ↑ System A 18.5 0.3 0.0 0.1
System B 18.7 0.3 0.0 0.2
MET ↑ System A 49.0 0.2 - 0.2
System B 50.0 0.2 - 0.1
TER ↓ System A 65.5 0.4 - 0.3
System B 64.9 0.4 - 0.4
</table>
<tableCaption confidence="0.92284125">
Table 1: Measured standard deviations of different au-
tomatic metrics due to test-set and optimizer variability.
sdev is reported only for the tuning objective function
BLEU.
</tableCaption>
<bodyText confidence="0.62037975">
Results are reported using BLEU (Papineni et
al., 2002), METEOR5 (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010), and TER (Snover et
al., 2006).
</bodyText>
<subsectionHeader confidence="0.997686">
4.1 Extraneous variables in one system
</subsectionHeader>
<bodyText confidence="0.985932621212121">
In this section, we describe and measure (on the ex-
ample systems just described) three extraneous vari-
ables that should be considered when evaluating a
translation system. We quantify these variables in
terms of standard deviation s, since it is expressed
in the same units as the original metric. Refer to
Table 1 for the statistics.
Local optima effects sdev The first extraneous
variable we discuss is the stochasticity of the opti-
mizer. As discussed above, different optimization
runs find different local maxima. The noise due to
this variable can depend on many number of fac-
tors, including the number of random restarts used
(in MERT), the number of features in a model, the
number of references, the language pair, the portion
of the search space visible to the optimizer (e.g. 10-
best, 100-best, a lattice, a hypergraph), and the size
of the tuning set. Unfortunately, there is no proxy to
estimate this effect as with bootstrap resampling. To
control for this variable, we must run the optimizer
multiple times to estimate the spread it induces on
the development set. Using the n optimizer samples,
with mi as the translation quality measurement of
5METEOR version 1.2 with English ranking parameters and
all modules.
the development set for the ith optimization run, and
m is the average of all mis, we report the standard
deviation over the tuning set as sdev:
(mi − m)2
n − 1
A high sdev value may indicate that the optimizer is
struggling with local optima and changing hyperpa-
rameters (e.g. more random restarts in MERT) could
improve system performance.
Overfitting effects stest As with any optimizer,
there is a danger that the optimal weights for a tuning
set may not generalize well to unseen data (i.e., we
overfit). For a randomized optimizer, this means that
parameters can generalize to different degrees over
multiple optimizer runs. We measure the spread in-
duced by optimizer randomness on the test set met-
ric score stest, as opposed to the overfitting effect in
isolation. The computation of stest is identical to sdev
except that the mis are the translation metrics cal-
culated on the test set. In Table 1, we observe that
stest &gt; sdev, indicating that optimized parameters are
likely not generalizing well.
Test set selection ssel The final extraneous vari-
able we consider is the selection of the test set it-
self. A good test set should be representative of
the domain or language for which experimental ev-
idence is being considered. However, with only a
single test corpus, we may have unreliable results
because of idiosyncrasies in the test set. This can
be mitigated in two ways. First, replication of ex-
periments by testing on multiple, non-overlapping
test sets can eliminate it directly. Since this is not
always practical (more test data may not be avail-
abile), the widely-used bootstrap resampling method
(§3) also controls for test set effects by resampling
multiple “virtual” test sets from a single set, making
it possible to infer distributional parameters such as
the standard deviation of the translation metric over
(very similar) test sets.6 Furthermore, this can be
done for each of our optimizer samples. By averag-
ing the bootstrap-estimated standard deviations over
</bodyText>
<footnote confidence="0.8880826">
6Unlike actually using multiple test sets, bootstrap resam-
pling does not help to re-estimate the mean metric score due to
test set spread (unlike actually using multiple test sets) since the
mean over bootstrap replicates is approximately the aggregate
metric score.
</footnote>
<equation confidence="0.833770833333333">
sdev =
n
i=1
178
d k
j=1
1
n
si =
ssel =
n
i=1
</equation>
<bodyText confidence="0.999903590909091">
optimizer samples, we have a statistic that jointly
quantifies the impact of test set effects and optimizer
instability on a test set. We call this statistic ssel.
Different values of this statistic can suggest method-
ological improvements. For example, a large ssel in-
dicates that more replications will be necessary to
draw reliable inferences from experiments on this
test set, so a larger test set may be helpful.
To compute ssel, assume we have n indepen-
dent optimization runs which produced weight vec-
tors that were used to translate a test set n times.
The test set has ` segments with references R =
(R1, R2, ... , Rt). Let X = (X1, X2, ... , Xn)
where each Xi = (Xi1, Xi2, . . . , Xit) is the list of
translated segments from the ith optimization run
list of the ` translated segments of the test set. For
each hypothesis output Xi, we construct k bootstrap
replicates by drawing ` segments uniformly, with re-
placement, from Xi, together with its corresponding
reference. This produces k virtual test sets for each
optimization run i. We designate the score of the jth
virtual test set of the ith optimization run with mij.
</bodyText>
<equation confidence="0.932698">
If mi = 1 Ek j=1 mij, then we have:
k
(mij − mi)2
k − 1
</equation>
<bodyText confidence="0.809065">
si
</bodyText>
<subsectionHeader confidence="0.997001">
4.2 Comparing Two Systems
</subsectionHeader>
<bodyText confidence="0.999318388888889">
In the previous section, we gave statistics about
the distribution of evaluation metrics across a large
number of experimental samples (Table 1). Because
of the large number of trials we carried out, we can
be extremely confident in concluding that for both
pairs of systems, the experimental manipulation ac-
counts for the observed metric improvements, and
furthermore, that we have a good estimate of the
magnitude of that improvement. However, it is not
generally feasible to perform as many replications
as we did, so here we turn to the question of how
to compare two systems, accounting for optimizer
noise, but without running 300 replications.
We begin with a visual illustration how opti-
mizer instability affects test set scores when com-
paring two systems. Figure 1 plots the histogram
of the 300 optimizer samples each from the two
BTEC Chinese-English systems. The phrase-based
</bodyText>
<figure confidence="0.975742454545455">
40
35
30
25
20
15
10
5
0
46 47 48 49 50 51
BLEU
</figure>
<figureCaption confidence="0.999062">
Figure 1: Histogram of test set BLEU scores for the
BTEC phrase-based system (left) and BTEC hierarchical
system (right). While the difference between the systems
is 1.5 BLEU in expectation, there is a non-trivial region
of overlap indicating that some random outcomes will re-
sult in little to no difference being observed.
</figureCaption>
<figure confidence="0.999049">
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.6 0.3 0.0 0.3 0.6 0.9
BLEU difference
</figure>
<figureCaption confidence="0.574354428571429">
Figure 2: Relative frequencies of obtaining differences
in BLEU scores on the WMT system as a function of the
number of optimizer samples. The expected difference
is 0.2 BLEU. While there is a reasonably high chance of
observing a non-trivial improvement (or even a decline)
for 1 sample, the distribution quickly peaks around the
expected value given just a few more samples.
</figureCaption>
<bodyText confidence="0.999600875">
system’s distribution is centered at the sample
mean 48.4, and the hierarchical system is centered
at 49.9, a difference of 1.5 BLEU, correspond-
ing to the widely replicated result that hierarchi-
cal phrase-based systems outperform conventional
phrase-based systems in Chinese-English transla-
tion. Crucially, although the distributions are dis-
tinct, there is a non-trivial region of overlap, and
experimental samples from the overlapping region
could suggest the opposite conclusion!
To further underscore the risks posed by this over-
lap, Figure 2 plots the relative frequencies with
which different BLEU score deltas will occur, as a
function of the number of optimizer samples used.
When is a difference significant? To determine
whether an experimental manipulation results in a
</bodyText>
<figure confidence="0.997246571428571">
Observation Count
Probability of observation
1 sample
3 samples
5 samples
10 samples
50 samples
</figure>
<page confidence="0.995459">
179
</page>
<bodyText confidence="0.999911785714286">
statistically reliable difference for an evaluation met-
ric, we use a stratified approximate randomization
(AR) test. This is a nonparametric test that approxi-
mates a paired permutation test by sampling permu-
tations (Noreen, 1989). AR estimates the probability
(p-value) that a measured difference in metric scores
arose by chance by randomly exchanging sentences
between the two systems. If there is no significant
difference between the systems (i.e., the null hypoth-
esis is true), then this shuffling should not change
the computed metric score. Crucially, this assumes
that the samples being analyzed are representative
of all extraneous variables that could affect the out-
come of the experiment. Therefore, we must include
multiple optimizer replications. Also, since metric
scores (such as BLEU) are in general not compa-
rable across test sets, we stratify, exchanging only
hypotheses that correspond to the same sentence.
Table 2 shows the p-values computed by AR, test-
ing the significance of the differences between the
two systems in each pair. The first three rows illus-
trate “single sample” testing practice. Depending on
luck with MERT, the results can vary widely from
insignificant (at p &gt; .05) to highly significant.
The last two lines summarize the results of the test
when a small number of replications are performed,
as ought to be reasonable in a research setting. In
this simulation, we randomly selected n optimizer
outputs from our large pool and ran the AR test to
determine the significance; we repeated this proce-
dure 250 times. The p-values reported are the p-
values at the edges of the 95% confidence interval
(CI) according to AR seen in the 250 simulated com-
parison scenarios. These indicate that we are very
likely to observe a significant difference for BTEC
at n = 5, and a very significant difference by n = 50
(Table 2). Similarly, we see this trend in the WMT
system: more replications leads to more significant
results, which will be easier to reproduce. Based on
the average performance of the systems reported in
Table 1, we expect significance over a large enough
number of independent trials.
</bodyText>
<sectionHeader confidence="0.995362" genericHeader="discussions">
5 Discussion and Recommendations
</sectionHeader>
<bodyText confidence="0.9993896">
No experiment can completely control for all pos-
sible confounding variables. Nor are metric scores
(even if they are statistically reliable) a substitute
for thorough human analysis. However, we believe
that the impact of optimizer instability has been ne-
</bodyText>
<table confidence="0.99874275">
n System A System B p-value WMT
BTEC
1 high low 0.25 0.95
1 median median 0.15 0.13
1 low high 0.0003 0.003
p-value (95% CI)
5 random random 0.001–0.034 0.001–0.38
50 random random 0.001–0.001 0.001–0.33
</table>
<tableCaption confidence="0.834183375">
Table 2: Two-system analysis: AR p-values for three
different “single sample” scenarios that illustrate differ-
ent pathological scenarios that can result when the sam-
pled weight vectors are “low” or “high.” For “random,”
we simulate an experiments with n optimization replica-
tions by drawing n optimized system outputs from our
pool and performing AR; this simulation was repeated
250 times and the 95% CI of the AR p-values is reported.
</tableCaption>
<bodyText confidence="0.999281375">
glected by standard experimental methodology in
MT research, where single-sample measurements
are too often used to assess system differences. In
this paper, we have provided evidence that optimizer
instability can have a substantial impact on results.
However, we have also shown that it is possible to
control for it with very few replications (Table 2).
We therefore suggest:
</bodyText>
<listItem confidence="0.99643">
• Replication be adopted as standard practice in
MT experimental methodology, especially in
reporting results;7
• Replication of optimization (MERT) and test
</listItem>
<bodyText confidence="0.93117375">
set evaluation be performed at least three times;
more replications may be necessary for experi-
mental manipulations with more subtle effects;
• Use of the median system according to a trusted
metric when manually analyzing system out-
put; preferably, the median should be deter-
mined based on one test set and a second test
set should be manually analyzed.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8182347">
We thank Michael Denkowski, Kevin Gimpel, Kenneth
Heafield, Michael Heilman, and Brendan O’Connor for
insightful feedback. This research was supported in part
by the National Science Foundation through TeraGrid re-
sources provided by Pittsburgh Supercomputing Center
under TG-DBS110003; the National Science Foundation
under IIS-0713402, IIS-0844507, IIS-0915187, and IIS-
0915327; the DARPA GALE program, the U. S. Army
Research Laboratory, and the U. S. Army Research Of-
fice under contract/grant number W911NF-10-1-0533.
</bodyText>
<footnote confidence="0.965688666666667">
7Source code to carry out the AR test for multiple optimizer
samples on the three metrics in this paper is available from
http://github.com/jhclark/multeval.
</footnote>
<page confidence="0.996163">
180
</page>
<sectionHeader confidence="0.996238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998249186813187">
A. Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer,
and P. Blunsom. 2010. Monte Carlo techniques
for phrase-based translation. Machine Translation,
24:103–121.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In Proc. of ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
N. Bertoldi, B. Haddow, and J.-B. Fouet. 2009. Im-
proved minimum error rate training in Moses. Prague
Bulletin of Mathematical Linguistics, No. 91:7–16.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluation.
In Proc. of ICASSP.
A. Cahill, M. Burke, R. O’Donovan, S. Riezler, J. van
Genabith, and A. Way. 2008. Wide-coverage deep
statistical parsing using automatic dependency struc-
ture annotation. Computational Linguistics, 34(1):81–
124.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical mt system
optimization. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 555–563. Proc. of ACL, June.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
N. Chinchor. 1993. The statistical significance of the
MUC-5 results. Proc. of MUC.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.
M. Denkowski and A. Lavie. 2010. Extending the
METEOR machine translation evaluation metric to the
phrase level. In Proc. of NAACL.
C. Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom,
and P. Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proc. ofACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
B. Efron. 1979. Bootstrap methods: Another look at the
jackknife. The Annals of Statistics, 7(1):1–26.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
P. Koehn, A. Birch, C. Callison-burch, M. Federico,
N. Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of COLING, Manchester, UK.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-j. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the Workshop on Intrinsic and Extrin-
sic Evaluation Methods for Machine Translation and
Summarization.
M. Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
L. Wasserman. 2003. All of Statistics: A Concise Course
in Statistical Inference. Springer.
Y. Zhang and S. Vogel. 2010. Significance tests of auto-
matic machine translation metrics. Machine Transla-
tion, 24:51–65.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
BLEU/NIST scores: How much improvement do we
need to have a better system? In Proc. of LREC.
</reference>
<page confidence="0.99831">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.781023">
<title confidence="0.99334">Better Hypothesis Testing for Statistical Machine Controlling for Optimizer Instability</title>
<author confidence="0.998827">Jonathan H Clark Chris Dyer Alon Lavie Noah A</author>
<affiliation confidence="0.8980335">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992786">Pittsburgh, PA 15213,</address>
<abstract confidence="0.9996758">In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>A Lopez</author>
<author>C Dyer</author>
<author>P Blunsom</author>
</authors>
<title>Monte Carlo techniques for phrase-based translation.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<pages>24--103</pages>
<contexts>
<context position="3090" citStr="Arun et al., 2010" startWordPosition="454" endWordPosition="457"> translation systems consist of a model whose parameters are estimated to maximize some objective function on a set of development data. Because the standard objectives (e.g., 1-best BLEU, expected BLEU, marginal likelihood) are not convex, only approximate solutions to the optimization problem are available, and the parameters learned are typically only locally optimal and may strongly depend on parameter initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3Online subgradient techniques such as MIZA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortp</context>
</contexts>
<marker>Arun, Haddow, Koehn, Lopez, Dyer, Blunsom, 2010</marker>
<rawString>A. Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer, and P. Blunsom. 2010. Monte Carlo techniques for phrase-based translation. Machine Translation, 24:103–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. of ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="9205" citStr="Banerjee and Lavie, 2005" startWordPosition="1439" endWordPosition="1442">EU ↑ System A 48.4 1.6 0.2 0.5 System B 49.9 1.5 0.1 0.4 MET ↑ System A 63.3 0.9 - 0.4 System B 63.8 0.9 - 0.5 TER ↓ System A 30.2 1.1 - 0.6 System B 28.7 1.0 - 0.2 WMT German-English (n = 50) BLEU ↑ System A 18.5 0.3 0.0 0.1 System B 18.7 0.3 0.0 0.2 MET ↑ System A 49.0 0.2 - 0.2 System B 50.0 0.2 - 0.1 TER ↓ System A 65.5 0.4 - 0.3 System B 64.9 0.4 - 0.4 Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find differ</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proc. of ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>B Haddow</author>
<author>J-B Fouet</author>
</authors>
<title>Improved minimum error rate training in Moses.</title>
<date>2009</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>No.</volume>
<pages>91--7</pages>
<contexts>
<context position="7412" citStr="Bertoldi et al., 2009" startWordPosition="1126" endWordPosition="1129">fault implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Other uses in NLP include iteration, 200-best lists were extracted with the curthe MUC-6 evaluation (Chinchor, 1993) and pars- rent weight vector (Bertoldi et al., 2009). The cdec ing (Cahill et al., 2008). However, these previous MERT implementation performs inference over the methods assume model parameters are elements of decoder search space which is structured as a hyperthe system rather than extraneous variables. graph (Kumar et al., 2009). Rather than using restart Prior work on optimizer noise in MT has fo- points, in addition to optimizing each feature indecused primarily on reducing optimizer instability pendently, it optimizes in 5 random directions per it(whereas our concern is how to deal with optimizer eration by constructing a search vector by </context>
</contexts>
<marker>Bertoldi, Haddow, Fouet, 2009</marker>
<rawString>N. Bertoldi, B. Haddow, and J.-B. Fouet. 2009. Improved minimum error rate training in Moses. Prague Bulletin of Mathematical Linguistics, No. 91:7–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Bootstrap estimates for confidence intervals in ASR performance evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<marker>Bisani, Ney, 2004</marker>
<rawString>M. Bisani and H. Ney. 2004. Bootstrap estimates for confidence intervals in ASR performance evaluation. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R O’Donovan</author>
<author>S Riezler</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Wide-coverage deep statistical parsing using automatic dependency structure annotation.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>124</pages>
<marker>Cahill, Burke, O’Donovan, Riezler, van Genabith, Way, 2008</marker>
<rawString>A. Cahill, M. Burke, R. O’Donovan, S. Riezler, J. van Genabith, and A. Way. 2008. Wide-coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics, 34(1):81– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Regularization and search for minimum error rate training.</title>
<date>2008</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="8483" citStr="Cer et al. (2008)" startWordPosition="1297" endWordPosition="1300">endently, it optimizes in 5 random directions per it(whereas our concern is how to deal with optimizer eration by constructing a search vector by uniformly noise, when it exists). Foster and Kuhn (2009) mea- sampling each element of the vector from (−1, 1) sured the instability of held-out BLEU scores across and then renormalizing so it has length 1. For all 10 MERT runs to improve tune/test set correlation. systems, the initial weight vector was manually iniHowever, they only briefly mention the implications tialized so as to yield reasonable translations. of the instability on significance. Cer et al. (2008) 177 4http://statmt.org/wmt11/ Metric System Avg ssel sdev stest BTEC Chinese-English (n = 300) BLEU ↑ System A 48.4 1.6 0.2 0.5 System B 49.9 1.5 0.1 0.4 MET ↑ System A 63.3 0.9 - 0.4 System B 63.8 0.9 - 0.5 TER ↓ System A 30.2 1.1 - 0.6 System B 28.7 1.0 - 0.2 WMT German-English (n = 50) BLEU ↑ System A 18.5 0.3 0.0 0.1 System B 18.7 0.3 0.0 0.2 MET ↑ System A 49.0 0.2 - 0.2 System B 50.0 0.2 - 0.1 TER ↓ System A 65.5 0.4 - 0.3 System B 64.9 0.4 - 0.4 Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the </context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regularization and search for minimum error rate training. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>C D Manning</author>
<author>D Jurafsky</author>
</authors>
<title>The best lexical metric for phrase-based statistical mt system optimization.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>555--563</pages>
<contexts>
<context position="4200" citStr="Cer et al. (2010)" startWordPosition="620" endWordPosition="623">xamples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics data and held-out test data, independently of any ex- explored regularization of MERT to improve generperimental manipulation. Thus, when trying to de- alization on test sets. Moore and Quirk (2008) extermine whether the difference between two mea- plored strategies for selecting better random “restart surements is significant, it is necessary to control for points” in optimization. Cer et al. (2010) analyzed variance due to noisy parameter estimates. This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times). In our experiments, we ran the MERT optimizer to Unfortunately, common practice in reporting ma- optimize BLEU on a held-out development set many chine translation results is to run the optimizer once times to obtain a set of optimizer samples on two difper system configuration and to draw conc</context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2010</marker>
<rawString>D. Cer, C. D. Manning, and D. Jurafsky. 2010. The best lexical metric for phrase-based statistical mt system optimization. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 555–563. Proc. of ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online largemargin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3500" citStr="Chiang et al., 2008" startWordPosition="517" endWordPosition="520">r initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3Online subgradient techniques such as MIZA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics data and held-out test data, independently of any ex- explored regularization of MERT to improve generperimental manipulation. Thus, when trying to de- alization on test sets. Moore and Quirk (2008) extermine whether the difference between two mea- plored strategies for selecting better random “restart</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>D. Chiang, Y. Marton, and P. Resnik. 2008. Online largemargin training of syntactic and structural translation features. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5770" citStr="Chiang, 2007" startWordPosition="873" endWordPosition="874">r. over optimizer outcomes (i.e., it results in relatively The first system pair contrasts a baseline phrasepoorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchiThe danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were conpaired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con3 Related Work trasts two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<title>The statistical significance of the MUC-5 results.</title>
<date>1993</date>
<booktitle>Proc. of MUC.</booktitle>
<contexts>
<context position="7359" citStr="Chinchor, 1993" startWordPosition="1119" endWordPosition="1120">oximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Other uses in NLP include iteration, 200-best lists were extracted with the curthe MUC-6 evaluation (Chinchor, 1993) and pars- rent weight vector (Bertoldi et al., 2009). The cdec ing (Cahill et al., 2008). However, these previous MERT implementation performs inference over the methods assume model parameters are elements of decoder search space which is structured as a hyperthe system rather than extraneous variables. graph (Kumar et al., 2009). Rather than using restart Prior work on optimizer noise in MT has fo- points, in addition to optimizing each feature indecused primarily on reducing optimizer instability pendently, it optimizes in 5 random directions per it(whereas our concern is how to deal with </context>
</contexts>
<marker>Chinchor, 1993</marker>
<rawString>N. Chinchor. 1993. The statistical significance of the MUC-5 results. Proc. of MUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="3478" citStr="Crammer et al., 2006" startWordPosition="513" endWordPosition="516">gly depend on parameter initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3Online subgradient techniques such as MIZA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics data and held-out test data, independently of any ex- explored regularization of MERT to improve generperimental manipulation. Thus, when trying to de- alization on test sets. Moore and Quirk (2008) extermine whether the difference between two mea- plored strategies for selecting </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
<author>A Lavie</author>
</authors>
<title>Extending the METEOR machine translation evaluation metric to the phrase level.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="9233" citStr="Denkowski and Lavie, 2010" startWordPosition="1443" endWordPosition="1446"> 0.5 System B 49.9 1.5 0.1 0.4 MET ↑ System A 63.3 0.9 - 0.4 System B 63.8 0.9 - 0.5 TER ↓ System A 30.2 1.1 - 0.6 System B 28.7 1.0 - 0.2 WMT German-English (n = 50) BLEU ↑ System A 18.5 0.3 0.0 0.1 System B 18.7 0.3 0.0 0.2 MET ↑ System A 49.0 0.2 - 0.2 System B 50.0 0.2 - 0.1 TER ↓ System A 65.5 0.4 - 0.3 System B 64.9 0.4 - 0.4 Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise </context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>M. Denkowski and A. Lavie. 2010. Extending the METEOR machine translation evaluation metric to the phrase level. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>J Weese</author>
<author>A Lopez</author>
<author>V Eidelman</author>
<author>P Blunsom</author>
<author>P Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models.</title>
<date>2010</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="5844" citStr="Dyer et al., 2010" startWordPosition="884" endWordPosition="887"> system pair contrasts a baseline phrasepoorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchiThe danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were conpaired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con3 Related Work trasts two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech rec</context>
</contexts>
<marker>Dyer, Weese, Lopez, Eidelman, Blunsom, Resnik, 2010</marker>
<rawString>C. Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom, and P. Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="6503" citStr="Dyer (2009)" startWordPosition="984" endWordPosition="985">s two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, dra</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>C. Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
</authors>
<title>Bootstrap methods: Another look at the jackknife.</title>
<date>1979</date>
<journal>The Annals of Statistics,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="6240" citStr="Efron, 1979" startWordPosition="944" endWordPosition="945"> the later of which was decoded with identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con3 Related Work trasts two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed a</context>
</contexts>
<marker>Efron, 1979</marker>
<rawString>B. Efron. 1979. Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 7(1):1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Stabilizing minimum error rate training.</title>
<date>2009</date>
<booktitle>Proc. of WMT.</booktitle>
<contexts>
<context position="8068" citStr="Foster and Kuhn (2009)" startWordPosition="1230" endWordPosition="1233">8). However, these previous MERT implementation performs inference over the methods assume model parameters are elements of decoder search space which is structured as a hyperthe system rather than extraneous variables. graph (Kumar et al., 2009). Rather than using restart Prior work on optimizer noise in MT has fo- points, in addition to optimizing each feature indecused primarily on reducing optimizer instability pendently, it optimizes in 5 random directions per it(whereas our concern is how to deal with optimizer eration by constructing a search vector by uniformly noise, when it exists). Foster and Kuhn (2009) mea- sampling each element of the vector from (−1, 1) sured the instability of held-out BLEU scores across and then renormalizing so it has length 1. For all 10 MERT runs to improve tune/test set correlation. systems, the initial weight vector was manually iniHowever, they only briefly mention the implications tialized so as to yield reasonable translations. of the instability on significance. Cer et al. (2008) 177 4http://statmt.org/wmt11/ Metric System Avg ssel sdev stest BTEC Chinese-English (n = 300) BLEU ↑ System A 48.4 1.6 0.2 0.5 System B 49.9 1.5 0.1 0.4 MET ↑ System A 63.3 0.9 - 0.4 </context>
</contexts>
<marker>Foster, Kuhn, 2009</marker>
<rawString>G. Foster and R. Kuhn. 2009. Stabilizing minimum error rate training. Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Birch</author>
<author>C Callison-burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>C Moran</author>
<author>C Dyer</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5756" citStr="Koehn et al., 2007" startWordPosition="869" endWordPosition="872">d will perform better. over optimizer outcomes (i.e., it results in relatively The first system pair contrasts a baseline phrasepoorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchiThe danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were conpaired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con3 Related Work trasts two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including f</context>
</contexts>
<marker>Koehn, Birch, Callison-burch, Federico, Bertoldi, Cowan, Moran, Dyer, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, A. Birch, C. Callison-burch, M. Federico, N. Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6646" citStr="Koehn, 2004" startWordPosition="1008" endWordPosition="1009">pace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Oth</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Macherey</author>
<author>C Dyer</author>
<author>F Och</author>
</authors>
<title>Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="7692" citStr="Kumar et al., 2009" startWordPosition="1169" endWordPosition="1172">tart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Other uses in NLP include iteration, 200-best lists were extracted with the curthe MUC-6 evaluation (Chinchor, 1993) and pars- rent weight vector (Bertoldi et al., 2009). The cdec ing (Cahill et al., 2008). However, these previous MERT implementation performs inference over the methods assume model parameters are elements of decoder search space which is structured as a hyperthe system rather than extraneous variables. graph (Kumar et al., 2009). Rather than using restart Prior work on optimizer noise in MT has fo- points, in addition to optimizing each feature indecused primarily on reducing optimizer instability pendently, it optimizes in 5 random directions per it(whereas our concern is how to deal with optimizer eration by constructing a search vector by uniformly noise, when it exists). Foster and Kuhn (2009) mea- sampling each element of the vector from (−1, 1) sured the instability of held-out BLEU scores across and then renormalizing so it has length 1. For all 10 MERT runs to improve tune/test set correlation. systems, the i</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>C Quirk</author>
</authors>
<title>Random restarts in minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of COLING,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="3995" citStr="Moore and Quirk (2008)" startWordPosition="589" endWordPosition="592">ed metrics as well as human evaluators. 3Online subgradient techniques such as MIZA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics data and held-out test data, independently of any ex- explored regularization of MERT to improve generperimental manipulation. Thus, when trying to de- alization on test sets. Moore and Quirk (2008) extermine whether the difference between two mea- plored strategies for selecting better random “restart surements is significant, it is necessary to control for points” in optimization. Cer et al. (2010) analyzed variance due to noisy parameter estimates. This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times). In our experiments, we ran the MERT optimizer to Unfortunately, common practice in reporti</context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>R. C. Moore and C. Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proc. of COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="16866" citStr="Noreen, 1989" startWordPosition="2725" endWordPosition="2726">erscore the risks posed by this overlap, Figure 2 plots the relative frequencies with which different BLEU score deltas will occur, as a function of the number of optimizer samples used. When is a difference significant? To determine whether an experimental manipulation results in a Observation Count Probability of observation 1 sample 3 samples 5 samples 10 samples 50 samples 179 statistically reliable difference for an evaluation metric, we use a stratified approximate randomization (AR) test. This is a nonparametric test that approximates a paired permutation test by sampling permutations (Noreen, 1989). AR estimates the probability (p-value) that a measured difference in metric scores arose by chance by randomly exchanging sentences between the two systems. If there is no significant difference between the systems (i.e., the null hypothesis is true), then this shuffling should not change the computed metric score. Crucially, this assumes that the samples being analyzed are representative of all extraneous variables that could affect the outcome of the experiment. Therefore, we must include multiple optimizer replications. Also, since metric scores (such as BLEU) are in general not comparabl</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1046" citStr="Och (2003)" startWordPosition="144" endWordPosition="145">improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. 1 Introduction The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003). In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets. However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on 1We hypothesize that the convention of “trusting” </context>
<context position="3033" citStr="Och, 2003" startWordPosition="446" endWordPosition="447">d Other Optimization Pitfalls Statistical machine translation systems consist of a model whose parameters are estimated to maximize some objective function on a set of development data. Because the standard objectives (e.g., 1-best BLEU, expected BLEU, marginal likelihood) are not convex, only approximate solutions to the optimization problem are available, and the parameters learned are typically only locally optimal and may strongly depend on parameter initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3Online subgradient techniques such as MIZA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meetin</context>
<context position="6633" citStr="Och, 2003" startWordPosition="1006" endWordPosition="1007">ning data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwel</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-j Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9170" citStr="Papineni et al., 2002" startWordPosition="1434" endWordPosition="1437">BTEC Chinese-English (n = 300) BLEU ↑ System A 48.4 1.6 0.2 0.5 System B 49.9 1.5 0.1 0.4 MET ↑ System A 63.3 0.9 - 0.4 System B 63.8 0.9 - 0.5 TER ↓ System A 30.2 1.1 - 0.6 System B 28.7 1.0 - 0.2 WMT German-English (n = 50) BLEU ↑ System A 18.5 0.3 0.0 0.1 System B 18.7 0.3 0.0 0.2 MET ↑ System A 49.0 0.2 - 0.2 System B 50.0 0.2 - 0.1 TER ↓ System A 65.5 0.4 - 0.3 System B 64.9 0.4 - 0.4 Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, diff</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-j. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proc. of the Workshop on Intrinsic and Extrinsic Evaluation Methods for Machine Translation and Summarization.</booktitle>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proc. of the Workshop on Intrinsic and Extrinsic Evaluation Methods for Machine Translation and Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>C Park</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="9264" citStr="Snover et al., 2006" startWordPosition="1449" endWordPosition="1452">System A 63.3 0.9 - 0.4 System B 63.8 0.9 - 0.5 TER ↓ System A 30.2 1.1 - 0.6 System B 28.7 1.0 - 0.2 WMT German-English (n = 50) BLEU ↑ System A 18.5 0.3 0.0 0.1 System B 18.7 0.3 0.0 0.2 MET ↑ System A 49.0 0.2 - 0.2 System B 50.0 0.2 - 0.1 TER ↓ System A 65.5 0.4 - 0.3 System B 64.9 0.4 - 0.4 Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise due to this variable can depend</context>
</contexts>
<marker>Snover, Dorr, Park, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wasserman</author>
</authors>
<title>All of Statistics: A Concise Course in Statistical Inference.</title>
<date>2003</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6258" citStr="Wasserman, 2003" startWordPosition="946" endWordPosition="947"> which was decoded with identified as useless. We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap. Dyer et al., 2010). The second system pair con3 Related Work trasts two German-English Hiero/cdec systems conThe use of statistical hypothesis testing has grown structed from the WMT11 parallel training data apace with the adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable </context>
</contexts>
<marker>Wasserman, 2003</marker>
<rawString>L. Wasserman. 2003. All of Statistics: A Concise Course in Statistical Inference. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>Significance tests of automatic machine translation metrics.</title>
<date>2010</date>
<booktitle>Machine Translation,</booktitle>
<pages>24--51</pages>
<contexts>
<context position="6738" citStr="Zhang and Vogel, 2010" startWordPosition="1020" endWordPosition="1023">was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Other uses in NLP include iteration, 200-best lists were extracted with the curthe MUC-6 evalua</context>
</contexts>
<marker>Zhang, Vogel, 2010</marker>
<rawString>Y. Zhang and S. Vogel. 2010. Significance tests of automatic machine translation metrics. Machine Translation, 24:51–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="6666" citStr="Zhang et al., 2004" startWordPosition="1010" endWordPosition="1013"> adoption of empirical methods in (98M words).4 The baseline system was trained on natural language processing. Bootstrap techniques unsegmented words, and the experimental system (Efron, 1979; Wasserman, 2003) are widespread was constructed using the most probable segmentain many problem areas, including for confidence tion of the German text according to the CRF word estimation in speech recognition (Bisani and Ney, segmentation model of Dyer (2009). The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times. Zhang and Vogel, 2010). Approximate randomiza- Our experiments used the default implementation tion (AR) has been proposed as a more reliable tech- of MERT that accompanies each of the two denique for MT significance testing, and evidence sug- coders. The Moses MERT implementation uses 20 gests that it yields fewer type I errors (i.e., claiming random restart points per iteration, drawn uniformly a significant difference where none exists; Riezler from the default ranges for each feature, and, at each and Maxwell, 2005). Other uses in NLP inclu</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proc. of LREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>