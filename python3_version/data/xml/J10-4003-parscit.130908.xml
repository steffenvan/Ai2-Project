<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000770">
<title confidence="0.742867">
Squibs
An Asymptotic Model for the English
Hapax/Vocabulary Ratio
</title>
<author confidence="0.994417">
Fan Fengxiang*
</author>
<affiliation confidence="0.998289">
Dalian Maritime University
</affiliation>
<bodyText confidence="0.989328714285714">
In the known literature, hapax legomena in an English text or a collection of texts roughly
account for about 50% of the vocabulary. This sort of constancy is baffling. The 100-million-
word British National Corpus was used to study this phenomenon. The result reveals that the
hapax/vocabulary ratio follows a U-shaped pattern. Initially, as the size of text increases, the
hapax/vocabulary ratio decreases; however, after the text size reaches about 3,000,000 words,
the hapax/vocabulary ratio starts to increase steadily. A computer simulation shows that as the
text size continues to increase, the hapax/vocabulary ratio would approach 1.
</bodyText>
<sectionHeader confidence="0.978994" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998684">
Words in English texts have a very peculiar distribution. On the one hand, between
50–100 top frequency words typically account for about 50% of the words in any text
(Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a
text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are
referred to as hapax legomenon (plural form: hapax legomena), hapax for short.
Hapaxes play a very important role in language studies. For example, the ratio
between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is
widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabu-
lary richness and author identification (Holmes 1991), language typology (Popescu and
Altmann 2008), the degree of analytism (Popescu, Maˇcutek, and Altmann 2009), and
so on.
The high percentage of the top 50–100 words within a text is understandable, as
only two of them, the and a(n) would account for over 5% of the total word tokens of
a text. However, the seemingly constant and high HVR of a text or collection of texts
is baffling. Intuitively, as the length of a text or collection of texts increases, the HVR
would decrease; as text length approaches infinity, all the words in the language would
have occurred, and the number of hapaxes would approach zero. But the known facts
so far do not seem to corroborate this intuition. For example, in Lewis Carroll’s 26,505-
word Alice’s Adventures in Wonderland, 44% of the vocabulary are hapaxes (Baayen 2001);
in Mark Twain’s 71,370-word The Adventures of Tom Sawyer, the percentage is 49.8%
(Manning and Sch¨utze 2001); in the 43-million-word Merc Corpus, this percentage is
</bodyText>
<note confidence="0.934026">
* School of Foreign Languages, Dalian Maritime University, Dalian, China.
</note>
<email confidence="0.918588">
E-mail: fanfengxiang@yahoo.com.
</email>
<note confidence="0.624071">
Submission received: 5 February 2010; accepted for publication: 2 May 2010.
</note>
<footnote confidence="0.4119765">
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
</footnote>
<bodyText confidence="0.9970245">
56.6% (Kornai 2002). There seems to be no explanation for this strange behavior of
hapax legomena in the literature.
The high HVR even in a mega-corpus poses problems for natural language process-
ing; it would suggest at least sparseness of lexical, semantic, syntactic, discoursal, and
pragmatic information on roughly half of the vocabulary. The following questions
ensue: What are the factors behind this enigmatic distribution of HVR? Is it possible
to substantially reduce HVR by increasing the size of a corpus? If so, how large should
such a corpus be? These questions are the focus of this article.
In this study, the dynamic relationship among the vocabulary size V, number of
hapaxes H, and text length N was examined in the 100-million-word British National
Corpus (BNC). In the study, the orthographic word concept is adopted as a working def-
inition, that is, a word (also called word token) is a string of contiguous alphanumeric
characters with a space on either side (Kuˇcera and Francis 1967; Biber et al. 1999). The
alphanumeric character set E is defined as
</bodyText>
<equation confidence="0.957357">
E _ {a,b,c ... z;A,B,C ... Z;0,1,2 ... 9}
The word ω is defined as
ω G E+
</equation>
<bodyText confidence="0.999425166666667">
There are 62 characters in E; however, in this study, all words are case-insensitive (i.e.,
words such as Language, LANGUAGE, and language are regarded as the same). So there
are actually 36 characters in E. Another concept used is lemma, which refers to the set of
words having the same stem, the same major part-of-speech, and the same word-sense
(Jurafsky and Martin 2000). In this study, the vocabulary of a text or a corpus is the set
of different lemmas within the text or corpus.
</bodyText>
<sectionHeader confidence="0.513604" genericHeader="categories and subject descriptors">
2. Data and Analysis
</sectionHeader>
<bodyText confidence="0.999993">
To study the dynamic relationship between N, V, and H, the entire BNC was divided
into equi-sized text blocks automatically by the computer. The size of each of the text
blocks was initially set to 4,500 words, a size suitable for this study because in comput-
ing the growth curves of V, H, and HVR of a large corpus, text blocks much smaller
than this (such as the average size of the text blocks of the one-million-word Brown and
LOB corpora, which is about 2,000 words) would considerably increase the number of
text blocks and would therefore result in a longer computing time. However, the actual
size of each of the text blocks is N ≈ 4,200 words because of the removal of textual
structure codes in the text blocks such as sn=“10”, /head, /p, and so on; and punctuation
tags such as cPUN. The total number of such text blocks is 23,709. These text chunks
were subsequently tokenized and lemmatized; characters that are not included in E
were ignored, except for word-linking hyphens, which were replaced with white spaces.
These processed text blocks were then recombined into 10 test sets, each having 2,371
text blocks, totaling about 10,000,000 word tokens, with the exception of the tenth set,
which has 2,370 text blocks. The formation of each set was done by random sampling
without replacement from the 23,709 text blocks. During the formation of each set, as
the text blocks were continuously sampled and pooled one by one to form a set, the
growth of V, H, and the corresponding HVR were computed along the way. The V, H,
and HVR of each of the ten sets are close to the means, which are 103,588.9, 42,384.6,
and 0.4091, respectively.
However, when the HVR curves were plotted, an interesting pattern was revealed.
(See the right panel, Figure 1.) The HVR curves drop sharply initially, then stop drop-
</bodyText>
<page confidence="0.982373">
632
</page>
<note confidence="0.48166">
Fengxiang An Asymptotic Model for the English Hapax/Vocabulary Ratio
</note>
<figureCaption confidence="0.995191">
Figure 1
</figureCaption>
<bodyText confidence="0.990869368421052">
Growth curves of V and H (the left panel), and the HVR curves (the right panel), along with the
increase in set sizes. The V growth curves are in the upper curve cluster, and the H growth
curves are in the lower curve cluster.
ping at some point far from the end, and start to rise, slowly but persistently, all the way
to the end. The number of word tokens at which HVR is the lowest and from which it
displays a general upward trend until the end is referred to as POR (Point Of Return).
Table 1 shows the initial HVRs (N ≈ 4, 200), the minimum HVRs, the final HVRs, V and
H at minimum HVRs, and PORs. The minimum HVRs of the ten sets are fairly close,
around 0.3928, and so are the final HVRs, around 0.4091, although the PORs have a wide
dispersion, from 1,515,629 to 4,382,688, averaging 2,957,179.5. It seems to defy common
sense that PORs should exist in all the ten sets, and are much smaller than the sizes of
the sets.
To see what would happen to HVR and POR when N is much larger than 10,000,000,
the growth of V, H, and HVR of the entire BNC were computed at an interval of
21 text blocks, about 89,000 word tokens. There are 1,129 such intervals, each formed
by random sampling without replacement from the 23,709 text blocks. The purpose of
reforming the 23,709 text blocks into 1,129 larger text chunks was to reduce computing
time. The result is shown in Figure 2. The vocabulary size of the entire BNC is 346,578,
and the number of hapaxes is 154,403. The initial HVR is 0.4583, the minimum HVR is
</bodyText>
<tableCaption confidence="0.981221">
Table 1
</tableCaption>
<table confidence="0.958588071428571">
The initial HVRs (HVRi), the minimum HVRs (HVRm), the final HVRs (HVRf), H at HVRm
(Hhvrm), V at HVRm (Vhvrm), and the PORs.
Set No. HVRi HVRm HVRf Hhvrm Vhvrm POR
Set 1 0.5605 0.3896 0.4064 23,727 60,900 3,509,249
Set 2 0.4950 0.3927 0.4086 18,056 45,978 2,015,669
Set 3 0.6078 0.3964 0.4115 24,936 62,902 3,734,768
Set 4 0.5420 0.3880 0.4063 15,088 38,882 1,515,629
Set 5 0.6119 0.3962 0.4124 21,906 55,295 2,834,215
Set 6 0.5780 0.3937 0.4134 16,211 41,174 1,624,036
Set 7 0.6080 0.3930 0.4089 22,627 57,568 3,031,637
Set 8 0.5732 0.3948 0.4078 26,998 68,379 4,382,688
Set 9 0.5710 0.3885 0.4044 19,476 50,130 2,591,759
Set 10 0.5662 0.3949 0.4117 26,618 67,396 4,332,145
Average 0.5714 0.3928 0.4091 21,564.3 54,860.4 2,957,179.5
</table>
<page confidence="0.914637">
Q1
633
</page>
<figure confidence="0.813362">
Computational Linguistics Volume 36, Number 4
</figure>
<figureCaption confidence="0.995103">
Figure 2
</figureCaption>
<bodyText confidence="0.995234619047619">
Growth curves of V and H (left panel), and the HVR curve (right panel), along with the increase
in set size.
0.3899, and the final HVR is 0.4455. The POR is 3,820,340. The HVR curve quickly reaches
POR, and then goes up steeply. The rise gradually slows down but still continues till the
end.
To find out the general composition of the 154,403 BNC hapaxes, 1,000 of these
hapaxes were randomly sampled and examined. These sampled hapaxes can be roughly
divided into three major types: within-dictionary words, out-of-dictionary words, and
typos. The total number of within-dictionary words is 232. They consist of 137 general
words (niffy, superreader, theonomous, etc.); 93 techno-scientific words (metavolcanosed-
imentary, bitstream, etc.); and two Early-modern English words (auncyent, howshold).
The total number of out-of-dictionary words is 718. They consist of 416 personal,
organizational, and place and brand names of various national origins (Khachaturyan,
Nimmanhemin, Canham, etc.); 161 alphanumeric strings (hhfd, dlgs, 90kyr, etc.); 123 Arabic
numerals (0431, 2562, 817200, etc.); 16 foreign words (mkukumkuku, gevald, gesprach,
etc.); and two interjections (aagghhh, khrrrrr). There are 50 typos (appication, disolv,
challeneger, etc.).
The word length distribution of the entire BNC hapaxes was also analyzed. The
average length of the hapaxes is 7.74 characters, compared with the average length of
7.29 characters of the non-hapaxes. The longest hapax has 104 characters (it is the longest
word in the BNC as well), and the shortest has two. The following are some examples:
</bodyText>
<listItem confidence="0.9983168">
1. krumfettmahanamanahulamoranosecanosemahanaritiraoosalalarama
lapalapapopapopobaalacataltootoochepereelong (104 characters)
2. kahahahahahahahahahahahahehehehehehehehehehehehehahahahaha
hahah (63 characters)
3. llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch
(58 characters)
4. 5cggaggggccctagagggccctagagggccccccaaaaacccccaaaaacccccc3
(57 characters)
5. 01001101010101110101010010110101110010 (38 characters)
6. ywtfghikmccowlxpmtxwkirlmhjdb (29 characters)
</listItem>
<page confidence="0.99289">
634
</page>
<figure confidence="0.9922284">
Fengxiang An Asymptotic Model for the English Hapax/Vocabulary Ratio
7. antidisestablishmentarianism (28 characters)
8. tetramethylindocarbocyanine (27 characters)
9. whothinkstorideanangeldown (26 characters)
10. verfassungsschutzbericht (24 characters)
11. yyyyyyyyyyyyyyyyyyy (20 characters)
12. actggaagggttagtttg (18 characters)
13. yslrwqieiifkvwksl (17 characters)
14. 5dddddddddddddd6 (16 characters)
15. chippingdale (12 characters)
16. reporeted (9 characters)
17. kwaouvi (7 characters)
18. baaba (5 characters)
19. akfu (4 characters)
20. xw (2 characters)
</figure>
<bodyText confidence="0.967123590909091">
As seen in this list, many of the hapaxes are pure random alphanumeric strings;
many of the names, personal or non-personal, and foreign words appear to be random
strings to the reader because of their diversified linguistic origins. Assuming the longest
possible word in the English language has 104 characters, because there are 36 charac-
ters in Σ (ignoring case), the maximum possible vocabulary size Vmax of the English
language would be
Vmax = 1 04 36l
�
l=1
Vmax tends to infinity. The number of within-dictionary words of the English lan-
guage plus those formed by compounding, derivation, and higher-probability alphanu-
meric strings would account for only a very tiny fraction of Vmax. As more and more text
blocks were sampled from the BNC, many of these words would have occurred more
than once, but extremely low probability within-dictionary words and out-of-dictionary
alphanumeric strings would accumulate. When N reached POR, the accumulation was
large enough to reverse the downward trend of HVR, and the HVR curve started to go
upwards. This suggests that as N approaches infinity, almost all the within-dictionary
words would have appeared more than once, and the vocabulary growth would mainly
be the growth of hapaxes formed by random alphanumeric strings, and the HVR curve
would gradually approach its horizontal asymptote 1. That is,
lim H = 1
n→∞ V
</bodyText>
<sectionHeader confidence="0.658415" genericHeader="general terms">
3. Computer Simulation
</sectionHeader>
<bodyText confidence="0.862248">
A computer simulation was designed and performed to test this hypothesis. In the
simulation, Lewis Carroll’s Through the Looking-Glass and What Alice Found There was
</bodyText>
<page confidence="0.995292">
635
</page>
<note confidence="0.592078">
Computational Linguistics Volume 36, Number 4
</note>
<bodyText confidence="0.999924388888889">
used as a miniature language source that contains all the within-dictionary-words of
the miniature language. The novel has 30,566 word tokens and 2,754 word types. A
simulation corpus was built by repeated sampling with replacement from the miniature
language source. The size of the sample was 100, each consisting of 94 words randomly
drawn from the source, with six low-probability random alphanumeric strings added.
These strings were automatically generated by the computer with lengths between
six and nine characters, and would roughly simulate the number of low-probability
hapaxes in a natural English text of about 4,000 words, as there are about six low-
probability hapaxes in a 4,200-word text chunk of the BNC. The following are some
of these computer-generated random alphanumeric strings: sygtxue, ungwba, aruvfyr9,
layyieubk. As the size of the simulation corpus increased, V, H, and HVR were computed.
The simulation corpus finally consisted of 639,506 such samples, with N = 63,950,600,
V = 3,838,940, H = 3,835,368, the initial HVR = 0.8554, POR = 20,300, the HVR at POR =
0.6004, and the final HVR = 0.9991. The result is shown in Figure 3. At N = 192,000 the
HVR curve looks similar to that of the BNC; after the HVR curve reaches POR, it starts
to go up and gradually approaches its asymptote. If we were able to build a corpus with
N hundreds of times larger than the BNC, its HVR curve would be similar to that of the
simulation corpus.
</bodyText>
<sectionHeader confidence="0.990069" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999971083333333">
This study reveals that HVR is not constant at all; it follows a U-shaped pattern. In
the case of the BNC, initially, as N increases, HVR decreases; after N reaches POR,
HVR starts to increase. This HVR distribution is due to the reoccurrence of hapaxes
consisting of within-dictionary words and high probability alphanumeric strings, and
the accumulation of extremely low probability within-dictionary words and out-of-
dictionary alphanumeric strings. If N approaches infinity, HVR would approach its
horizontal asymptote 1.
The asymptotic property of HVR indicates that, at least for a general balanced
corpus like the BNC, if it does not have a distinct POR, this would suggest that it has not
reached the stage where hapaxes of extremely low probability within-dictionary words
and out-of-dictionary alphanumeric strings substantially contribute to the vocabulary
growth, and this may imply insufficient coverage of the core vocabulary of the text
</bodyText>
<figureCaption confidence="0.486351">
Figure 3
</figureCaption>
<bodyText confidence="0.989438">
The HVR curve of the simulation corpus. Left panel: HVR between N = 100–192,000. Right panel:
HVR between 100–63,950,600; the dotted line is the horizontal asymptote.
</bodyText>
<page confidence="0.996016">
636
</page>
<note confidence="0.727045">
Fengxiang An Asymptotic Model for the English Hapax/Vocabulary Ratio
</note>
<bodyText confidence="0.999745">
population from which it is built. To display a clear POR, N should be at least 10,000,000
for such a corpus, as shown in Section 2. On the other hand, a corpus with a size much
larger than that of the BNC would have higher HVR; such a corpus would have more
lexical noise, with the within-dictionary words far outnumbered by out-of-dictionary
alphanumeric strings, leaving sparseness of lexical, semantic, syntactic, discoursal, and
pragmatic information on more than half of the vocabulary of the corpus practically
unsolved.
</bodyText>
<sectionHeader confidence="0.996979" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999042822222222">
Baayen, Harald. 1996. The effects of lexical
specialization on the growth curve of the
vocabulary. Computational Linguistics,
22(4):455–480.
Baayen, Harald. 2001. Word Frequency
Distributions. Kluwer Academic
Publishers, Dordrecht.
Biber, Douglas, Stig Johansson, Geoffrey
Leech, Susan Conrad, and Edward
Finegan. 1999. Longman Grammar of
Spoken and Written English. Pearson
Education Limited, Harlow, Essex.
Holmes, David. 1991. Vocabulary richness
and the prophetic voice. Literary and
Linguistic Computing, 6(4):259–268.
Jurafsky, Daniel and James Martin. 2000.
Speech and Language Processing, an
Introduction to Natural Language
Processing, Computational Linguistics,
and Speech Recognition. Prentice-Hall,
Upper Saddle River.
Kennedy, Graeme. 1998. An Introduction
to Corpus Linguistics. Addison Wesley,
London.
Kornai, A. 2002. How many words are
there? Glottometrics, 4:61–86.
Kuˇcera, H. and W. Francis. 1967.
Computational Analysis of Present-Day
American English. Brown University
Press, Providence, RI.
Manning, Christopher and Hinrich Sch¨utze.
2001. Foundations of Statistical Natural
Language Processing. The MIT Press,
Cambridge, MA.
Popescu, I. I. and G. Altmann. 2008. Hapax
legomena and language typology. Journal
of Quantitative Linguistics, 15(4):370–378.
Popescu, I. I., J. Maˇcutek, and G. Altmann.
2009. Aspects of word frequencies. RAM,
L¨udenscheid.
Tweedie, Fiona and Harald Baayen. 1998.
How variable may a constant be?
Measure of lexical richness in perspective.
Journal of Quantitative Linguistics,
32(5):323–352.
</reference>
<page confidence="0.997792">
637
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486914">
<title confidence="0.955456">Squibs An Asymptotic Model for the English</title>
<author confidence="0.589361">HapaxVocabulary Ratio</author>
<affiliation confidence="0.995465">Dalian Maritime University</affiliation>
<abstract confidence="0.978258428571429">In the known literature, hapax legomena in an English text or a collection of texts roughly account for about 50% of the vocabulary. This sort of constancy is baffling. The 100-millionword British National Corpus was used to study this phenomenon. The result reveals that the hapax/vocabulary ratio follows a U-shaped pattern. Initially, as the size of text increases, the hapax/vocabulary ratio decreases; however, after the text size reaches about 3,000,000 words, the hapax/vocabulary ratio starts to increase steadily. A computer simulation shows that as the text size continues to increase, the hapax/vocabulary ratio would approach 1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>The effects of lexical specialization on the growth curve of the vocabulary.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="1065" citStr="Baayen 1996" startWordPosition="165" endWordPosition="166">ize of text increases, the hapax/vocabulary ratio decreases; however, after the text size reaches about 3,000,000 words, the hapax/vocabulary ratio starts to increase steadily. A computer simulation shows that as the text size continues to increase, the hapax/vocabulary ratio would approach 1. 1. Introduction Words in English texts have a very peculiar distribution. On the one hand, between 50–100 top frequency words typically account for about 50% of the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. Hapaxes play a very important role in language studies. For example, the ratio between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabulary richness and author identification (Holmes 1991), language typology (Popescu and Altmann 2008), the degree of analytism (Popescu, Maˇcutek, and Altmann 2009), and so on. The high percentage of the top 50–100 words within a t</context>
</contexts>
<marker>Baayen, 1996</marker>
<rawString>Baayen, Harald. 1996. The effects of lexical specialization on the growth curve of the vocabulary. Computational Linguistics, 22(4):455–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>Word Frequency Distributions.</title>
<date>2001</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="2300" citStr="Baayen 2001" startWordPosition="368" endWordPosition="369">as only two of them, the and a(n) would account for over 5% of the total word tokens of a text. However, the seemingly constant and high HVR of a text or collection of texts is baffling. Intuitively, as the length of a text or collection of texts increases, the HVR would decrease; as text length approaches infinity, all the words in the language would have occurred, and the number of hapaxes would approach zero. But the known facts so far do not seem to corroborate this intuition. For example, in Lewis Carroll’s 26,505- word Alice’s Adventures in Wonderland, 44% of the vocabulary are hapaxes (Baayen 2001); in Mark Twain’s 71,370-word The Adventures of Tom Sawyer, the percentage is 49.8% (Manning and Sch¨utze 2001); in the 43-million-word Merc Corpus, this percentage is * School of Foreign Languages, Dalian Maritime University, Dalian, China. E-mail: fanfengxiang@yahoo.com. Submission received: 5 February 2010; accepted for publication: 2 May 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 56.6% (Kornai 2002). There seems to be no explanation for this strange behavior of hapax legomena in the literature. The high HVR even in a mega-corpus pos</context>
</contexts>
<marker>Baayen, 2001</marker>
<rawString>Baayen, Harald. 2001. Word Frequency Distributions. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
<author>Stig Johansson</author>
<author>Geoffrey Leech</author>
<author>Susan Conrad</author>
<author>Edward Finegan</author>
</authors>
<title>Longman Grammar of Spoken and Written English.</title>
<date>1999</date>
<publisher>Pearson Education Limited,</publisher>
<location>Harlow, Essex.</location>
<contexts>
<context position="3771" citStr="Biber et al. 1999" startWordPosition="595" endWordPosition="598">nigmatic distribution of HVR? Is it possible to substantially reduce HVR by increasing the size of a corpus? If so, how large should such a corpus be? These questions are the focus of this article. In this study, the dynamic relationship among the vocabulary size V, number of hapaxes H, and text length N was examined in the 100-million-word British National Corpus (BNC). In the study, the orthographic word concept is adopted as a working definition, that is, a word (also called word token) is a string of contiguous alphanumeric characters with a space on either side (Kuˇcera and Francis 1967; Biber et al. 1999). The alphanumeric character set E is defined as E _ {a,b,c ... z;A,B,C ... Z;0,1,2 ... 9} The word ω is defined as ω G E+ There are 62 characters in E; however, in this study, all words are case-insensitive (i.e., words such as Language, LANGUAGE, and language are regarded as the same). So there are actually 36 characters in E. Another concept used is lemma, which refers to the set of words having the same stem, the same major part-of-speech, and the same word-sense (Jurafsky and Martin 2000). In this study, the vocabulary of a text or a corpus is the set of different lemmas within the text o</context>
</contexts>
<marker>Biber, Johansson, Leech, Conrad, Finegan, 1999</marker>
<rawString>Biber, Douglas, Stig Johansson, Geoffrey Leech, Susan Conrad, and Edward Finegan. 1999. Longman Grammar of Spoken and Written English. Pearson Education Limited, Harlow, Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Holmes</author>
</authors>
<title>Vocabulary richness and the prophetic voice.</title>
<date>1991</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>6--4</pages>
<contexts>
<context position="1489" citStr="Holmes 1991" startWordPosition="230" endWordPosition="231">pically account for about 50% of the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. Hapaxes play a very important role in language studies. For example, the ratio between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabulary richness and author identification (Holmes 1991), language typology (Popescu and Altmann 2008), the degree of analytism (Popescu, Maˇcutek, and Altmann 2009), and so on. The high percentage of the top 50–100 words within a text is understandable, as only two of them, the and a(n) would account for over 5% of the total word tokens of a text. However, the seemingly constant and high HVR of a text or collection of texts is baffling. Intuitively, as the length of a text or collection of texts increases, the HVR would decrease; as text length approaches infinity, all the words in the language would have occurred, and the number of hapaxes would </context>
</contexts>
<marker>Holmes, 1991</marker>
<rawString>Holmes, David. 1991. Vocabulary richness and the prophetic voice. Literary and Linguistic Computing, 6(4):259–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James Martin</author>
</authors>
<title>Speech and Language Processing, an Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, Upper Saddle River.</title>
<date>2000</date>
<contexts>
<context position="4269" citStr="Jurafsky and Martin 2000" startWordPosition="684" endWordPosition="687">en) is a string of contiguous alphanumeric characters with a space on either side (Kuˇcera and Francis 1967; Biber et al. 1999). The alphanumeric character set E is defined as E _ {a,b,c ... z;A,B,C ... Z;0,1,2 ... 9} The word ω is defined as ω G E+ There are 62 characters in E; however, in this study, all words are case-insensitive (i.e., words such as Language, LANGUAGE, and language are regarded as the same). So there are actually 36 characters in E. Another concept used is lemma, which refers to the set of words having the same stem, the same major part-of-speech, and the same word-sense (Jurafsky and Martin 2000). In this study, the vocabulary of a text or a corpus is the set of different lemmas within the text or corpus. 2. Data and Analysis To study the dynamic relationship between N, V, and H, the entire BNC was divided into equi-sized text blocks automatically by the computer. The size of each of the text blocks was initially set to 4,500 words, a size suitable for this study because in computing the growth curves of V, H, and HVR of a large corpus, text blocks much smaller than this (such as the average size of the text blocks of the one-million-word Brown and LOB corpora, which is about 2,000 wo</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, Daniel and James Martin. 2000. Speech and Language Processing, an Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, Upper Saddle River.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Kennedy</author>
</authors>
<title>An Introduction to Corpus Linguistics.</title>
<date>1998</date>
<publisher>Addison Wesley,</publisher>
<location>London.</location>
<contexts>
<context position="946" citStr="Kennedy 1998" startWordPosition="142" endWordPosition="143">study this phenomenon. The result reveals that the hapax/vocabulary ratio follows a U-shaped pattern. Initially, as the size of text increases, the hapax/vocabulary ratio decreases; however, after the text size reaches about 3,000,000 words, the hapax/vocabulary ratio starts to increase steadily. A computer simulation shows that as the text size continues to increase, the hapax/vocabulary ratio would approach 1. 1. Introduction Words in English texts have a very peculiar distribution. On the one hand, between 50–100 top frequency words typically account for about 50% of the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. Hapaxes play a very important role in language studies. For example, the ratio between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabulary richness and author identification (Holmes 1991), language typology (Popescu and Altmann 2008), the degre</context>
</contexts>
<marker>Kennedy, 1998</marker>
<rawString>Kennedy, Graeme. 1998. An Introduction to Corpus Linguistics. Addison Wesley, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kornai</author>
</authors>
<title>How many words are there?</title>
<date>2002</date>
<journal>Glottometrics,</journal>
<pages>4--61</pages>
<contexts>
<context position="1079" citStr="Kornai 2002" startWordPosition="167" endWordPosition="168">ncreases, the hapax/vocabulary ratio decreases; however, after the text size reaches about 3,000,000 words, the hapax/vocabulary ratio starts to increase steadily. A computer simulation shows that as the text size continues to increase, the hapax/vocabulary ratio would approach 1. 1. Introduction Words in English texts have a very peculiar distribution. On the one hand, between 50–100 top frequency words typically account for about 50% of the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. Hapaxes play a very important role in language studies. For example, the ratio between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabulary richness and author identification (Holmes 1991), language typology (Popescu and Altmann 2008), the degree of analytism (Popescu, Maˇcutek, and Altmann 2009), and so on. The high percentage of the top 50–100 words within a text is underst</context>
<context position="2764" citStr="Kornai 2002" startWordPosition="431" endWordPosition="432">orate this intuition. For example, in Lewis Carroll’s 26,505- word Alice’s Adventures in Wonderland, 44% of the vocabulary are hapaxes (Baayen 2001); in Mark Twain’s 71,370-word The Adventures of Tom Sawyer, the percentage is 49.8% (Manning and Sch¨utze 2001); in the 43-million-word Merc Corpus, this percentage is * School of Foreign Languages, Dalian Maritime University, Dalian, China. E-mail: fanfengxiang@yahoo.com. Submission received: 5 February 2010; accepted for publication: 2 May 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 56.6% (Kornai 2002). There seems to be no explanation for this strange behavior of hapax legomena in the literature. The high HVR even in a mega-corpus poses problems for natural language processing; it would suggest at least sparseness of lexical, semantic, syntactic, discoursal, and pragmatic information on roughly half of the vocabulary. The following questions ensue: What are the factors behind this enigmatic distribution of HVR? Is it possible to substantially reduce HVR by increasing the size of a corpus? If so, how large should such a corpus be? These questions are the focus of this article. In this study</context>
</contexts>
<marker>Kornai, 2002</marker>
<rawString>Kornai, A. 2002. How many words are there? Glottometrics, 4:61–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kuˇcera</author>
<author>W Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<marker>Kuˇcera, Francis, 1967</marker>
<rawString>Kuˇcera, H. and W. Francis. 1967. Computational Analysis of Present-Day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>2001</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 2001</marker>
<rawString>Manning, Christopher and Hinrich Sch¨utze. 2001. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I I Popescu</author>
<author>G Altmann</author>
</authors>
<title>Hapax legomena and language typology.</title>
<date>2008</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="1535" citStr="Popescu and Altmann 2008" startWordPosition="234" endWordPosition="237">the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. Hapaxes play a very important role in language studies. For example, the ratio between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabulary richness and author identification (Holmes 1991), language typology (Popescu and Altmann 2008), the degree of analytism (Popescu, Maˇcutek, and Altmann 2009), and so on. The high percentage of the top 50–100 words within a text is understandable, as only two of them, the and a(n) would account for over 5% of the total word tokens of a text. However, the seemingly constant and high HVR of a text or collection of texts is baffling. Intuitively, as the length of a text or collection of texts increases, the HVR would decrease; as text length approaches infinity, all the words in the language would have occurred, and the number of hapaxes would approach zero. But the known facts so far do n</context>
</contexts>
<marker>Popescu, Altmann, 2008</marker>
<rawString>Popescu, I. I. and G. Altmann. 2008. Hapax legomena and language typology. Journal of Quantitative Linguistics, 15(4):370–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I I Popescu</author>
<author>J Maˇcutek</author>
<author>G Altmann</author>
</authors>
<title>Aspects of word frequencies.</title>
<date>2009</date>
<publisher>RAM, L¨udenscheid.</publisher>
<marker>Popescu, Maˇcutek, Altmann, 2009</marker>
<rawString>Popescu, I. I., J. Maˇcutek, and G. Altmann. 2009. Aspects of word frequencies. RAM, L¨udenscheid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fiona Tweedie</author>
<author>Harald Baayen</author>
</authors>
<title>How variable may a constant be? Measure of lexical richness in perspective.</title>
<date>1998</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="1428" citStr="Tweedie and Baayen 1998" startWordPosition="220" endWordPosition="223">liar distribution. On the one hand, between 50–100 top frequency words typically account for about 50% of the words in any text (Kennedy 1998); on the other, generally, about half of the words of the vocabulary of a text occur only once in the text (Baayen 1996; Kornai 2002). These lexical singletons are referred to as hapax legomenon (plural form: hapax legomena), hapax for short. Hapaxes play a very important role in language studies. For example, the ratio between the number of hapaxes and vocabulary size (hereinafter referred to as HVR) is widely used in studies such as vocabulary growth (Tweedie and Baayen 1998), vocabulary richness and author identification (Holmes 1991), language typology (Popescu and Altmann 2008), the degree of analytism (Popescu, Maˇcutek, and Altmann 2009), and so on. The high percentage of the top 50–100 words within a text is understandable, as only two of them, the and a(n) would account for over 5% of the total word tokens of a text. However, the seemingly constant and high HVR of a text or collection of texts is baffling. Intuitively, as the length of a text or collection of texts increases, the HVR would decrease; as text length approaches infinity, all the words in the l</context>
</contexts>
<marker>Tweedie, Baayen, 1998</marker>
<rawString>Tweedie, Fiona and Harald Baayen. 1998. How variable may a constant be? Measure of lexical richness in perspective. Journal of Quantitative Linguistics, 32(5):323–352.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>