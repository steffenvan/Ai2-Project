<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000994">
<title confidence="0.995697">
Minimal-length linearizations for mildly context-sensitive dependency trees
</title>
<author confidence="0.996725">
Y. Albert Park
</author>
<affiliation confidence="0.999708">
Department of Computer Science and Engineering
</affiliation>
<address confidence="0.893997">
9500 Gilman Drive
La Jolla, CA 92037-404, USA
</address>
<email confidence="0.998689">
yapark@ucsd.edu
</email>
<author confidence="0.98649">
Roger Levy
</author>
<affiliation confidence="0.992604">
Department of Linguistics
</affiliation>
<address confidence="0.891338">
9500 Gilman Drive
La Jolla, CA 92037-108, USA
</address>
<email confidence="0.999157">
rlevy@ling.ucsd.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869230769231">
The extent to which the organization of nat-
ural language grammars reflects a drive to
minimize dependency length remains little
explored. We present the first algorithm
polynomial-time in sentence length for obtain-
ing the minimal-length linearization of a de-
pendency tree subject to constraints of mild
context sensitivity. For the minimally context-
sensitive case of gap-degree 1 dependency
trees, we prove several properties of minimal-
length linearizations which allow us to im-
prove the efficiency of our algorithm to the
point that it can be used on most naturally-
occurring sentences. We use the algorithm
to compare optimal, observed, and random
sentence dependency length for both surface
and deep dependencies in English and Ger-
man. We find in both languages that anal-
yses of surface and deep dependencies yield
highly similar results, and that mild context-
sensitivity affords very little reduction in min-
imal dependency length over fully projective
linearizations; but that observed linearizations
in German are much closer to random and far-
ther from minimal-length linearizations than
in English.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965351351351">
This paper takes up the relationship between two
hallmarks of natural language dependency structure.
First, there seem to be qualitative constraints on the
relationship between the dependency structure of the
words in a sentence and their linear ordering. In par-
ticular, this relationship seems to be such that any
natural language sentence, together with its depen-
dency structure, should be generable by a mildly
context-sensitivity formalism (Joshi, 1985), in par-
ticular a linear context-free rewrite system in which
the right-hand side of each rule has a distinguished
head (Pollard, 1984; Vijay-Shanker et al., 1987;
Kuhlmann, 2007). This condition places strong con-
straints on the linear contiguity of word-word de-
pendency relations, such that only limited classes of
crossing context-free dependency structures may be
admitted.
The second constraint is a softer preference for
words in a dependency relation to occur in close
proximity to one another. This constraint is perhaps
best documented in psycholinguistic work suggest-
ing that large distances between governors and de-
pendents induce processing difficulty in both com-
prehension and production (Hawkins, 1994, 2004;
Gibson, 1998; Jaeger, 2006). Intuitively there is
a relationship between these two constraints: con-
sistently large dependency distances in a sentence
would require many crossing dependencies. How-
ever, it is not the case that crossing dependencies
always mean longer dependency distances. For ex-
ample, (1) below has no crossing dependencies, but
the distance between arrived and its dependent Yes-
terday is large. The overall dependency length of the
sentence can be reduced by extraposing the relative
clause who was wearing a hat, resulting in (2), in
which the dependency Yesterday→arrived crosses
the dependency woman←who.
</bodyText>
<listItem confidence="0.8622395">
(1) Yesterday a woman who was wearing a hat arrived.
(2) Yesterday a woman arrived who was wearing a hat.
</listItem>
<page confidence="0.975007">
335
</page>
<note confidence="0.890822">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 335–343,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999750230769231">
There has been some recent work on dependency
length minimization in natural language sentences
(Gildea and Temperley, 2007), but the relationship
between the precise constraints on available lin-
earizations and dependency length minimization re-
mains little explored. In this paper, we introduce
the first efficient algorithm for obtaining lineariza-
tions of dependency trees that minimize overall de-
pendency lengths subject to the constraint of mild
context-sensitivity, and use it to investigate the rela-
tionship between this constraint and the distribution
of dependency length actually observed in natural
languages.
</bodyText>
<sectionHeader confidence="0.9932665" genericHeader="introduction">
2 Projective and mildly non-projective
dependency-tree linearizations
</sectionHeader>
<bodyText confidence="0.998502096774194">
In the last few years there has been a resurgence
of interest in computation on dependency-tree struc-
tures for natural language sentences, spurred by
work such as McDonald et al. (2005a,b) show-
ing that working with dependency-tree syntactic
representations in which each word in the sen-
tence corresponds to a node in the dependency tree
(and vice versa) can lead to algorithmic benefits
over constituency-structure representations. The lin-
earization of a dependency tree is simply the linear
order in which the nodes of the tree occur in a sur-
face string. There is a broad division between two
classes of linearizations: projective linearizations
that do not lead to any crossing dependencies in the
tree, and non-projective linearizations that involve
at least one crossing dependency pair. Example (1),
for example, is projective, whereas Example (2) is
non-projective due to the crossing between the Yes-
terday→arrived and woman←who dependencies.
Beyond this dichotomy, however, the homomor-
phism from headed tree structures to dependency
structures (Miller, 2000) can be used together with
work on the mildly context-sensitive formalism lin-
ear context-free rewrite systems (LCFRSs) (Vijay-
Shanker et al., 1987) to characterize various classes
of mildly non-projective dependency-tree lineariza-
tions (Kuhlmann and Nivre, 2006). The LCFRSs are
an infinite sequence of classes of formalism for gen-
erating surface strings through derivation trees in a
rule-based context-free rewriting system. The i-th
LCFRS class (for i = 0, 1, 2,... ) imposes the con-
</bodyText>
<figureCaption confidence="0.997822">
Figure 1: Sample dependency subtree for Figure 2
</figureCaption>
<bodyText confidence="0.999926666666667">
straint that every node in the derivation tree maps to
to a collection of at most i+1 contiguous substrings.
The 0-th class of LCFRS, for example, corresponds
to the context-free grammars, since each node in the
derivation tree must map to a single contiguous sub-
string; the 1st class of LCFRS corresponds to Tree-
Adjoining Grammars (Joshi et al., 1975), in which
each node in the derivation tree must map to at most
a pair of contiguous substrings; and so forth. The
dependency trees induced when each rewrite rule in
an i-th order LCFRS distinguish a unique head can
similarly be characterized by being of gap-degree i,
so that i is the maximum number of gaps that may
appear between contiguous substrings of any subtree
in the dependency tree (Kuhlmann and M¨ohl, 2007).
The dependency tree for Example (2), for example,
is of gap-degree 1. Although there are numerous
documented cases in which projectivity is violated
in natural language, there are exceedingly few doc-
umented cases in which the documented gap degree
exceeds 1 (though see, for example, Kobele, 2006).
</bodyText>
<sectionHeader confidence="0.9670145" genericHeader="method">
3 Finding minimal dependency-length
linearizations
</sectionHeader>
<bodyText confidence="0.9999275">
Even under the strongest constraint of projectivity,
the number of possible linearizations of a depen-
dency tree is exponential in both sentence length
and arity (the maximum number of dependencies
for any word). As pointed out by Gildea and Tem-
perley (2007), however, finding the unconstrained
minimal-length linearization is a well-studied prob-
lem with an O(n1.6) solution (Chung, 1984). How-
ever, this approach does not take into account con-
straints of projectivity or mild context-sensitivity.
Gildea and Temperley themselves introduced a
novel efficient algorithm for finding the minimized
dependency length of a sentence subject to the con-
straint that the linearization is projective. Their al-
gorithm can perhaps be most simply understood by
making three observations. First, the total depen-
</bodyText>
<page confidence="0.983782">
336
</page>
<figure confidence="0.52578575">
d12
d11
h d21 d32 d31 d22
|c1 |kh |c2|
</figure>
<figureCaption confidence="0.999997">
Figure 2: Dependency length factorization for efficient
projective linearization, using the dependency subtree of
Figure 1
</figureCaption>
<bodyText confidence="0.923935">
dency length of a projective linearization can be
</bodyText>
<equation confidence="0.846483">
written as
⎤
D(wi, Ej) ⎦ ⎥ (1)
</equation>
<bodyText confidence="0.999969129032258">
where Ei is the boundary of the contiguous substring
corresponding to the dependency subtree rooted at
wi which stands between wi and its governor, and
D(wi, Ej) is the distance from wi to Ej, with the
special case of D(wroot, Eroot) = 0 (Figures 1
and 2). Writing the total dependency length this
way makes it clear that each term in the outer sum
can be optimized independently, and thus one can
use dynamic programming to recursively find op-
timal subtree orderings from the bottom up. Sec-
ond, for each subtree, the optimal ordering can be
obtained by placing dependent subtrees on alternat-
ing sides of w from inside out in order of increas-
ing length. Third, the total dependency lengths be-
tween any words withing an ordering stays the same
when the ordering is reversed, letting us assume that
D(wi, Ei) will be the length to the closest edge.
These three observations lead to an algorithm with
worst-case complexity of O(n log m) time, where
n is sentence length and m is sentence arity. (The
log m term arises from the need to sort the daugh-
ters of each node into descending order of length.)
When limited subclasses of nonprojectivity are
admitted, however, the problem becomes more diffi-
cult because total dependency length can no longer
be written in such a simple form as in Equation (1).
Intuitively, the size of the effect on dependency
length of a decision to order a given subtree discon-
tiguously, as in a woman... who was wearing a hat
in Example (2), cannot be calculated without con-
sulting the length of the string that the discontiguous
</bodyText>
<figureCaption confidence="0.9933152">
Figure 3: Factorizing dependency length at node wi of
a mildly context-sensitive dependency tree. This partial
linearization of head with dependent components makes
c1 the head component and leads to l = 2 links crossing
between c1 and c2.
</figureCaption>
<bodyText confidence="0.9993834">
subtree would be wrapped around. Nevertheless, for
any limited gap degree, it is possible to use a dif-
ferent factorization of dependency length that keeps
computation polynomial in sentence length. We in-
troduce this factorization in the next section.
</bodyText>
<sectionHeader confidence="0.906422" genericHeader="method">
4 Minimization with limited gap degree
</sectionHeader>
<bodyText confidence="0.955359227272727">
We begin by defining some terms. We use the word
component to refer to a full linearization of a sub-
tree in the case where it is realized as a single con-
tiguous string, or to refer to any of of the contigu-
ous substrings produced when a subtree is realized
discontiguously. We illustrate the factorization for
gap-degree 1, so that any subtree has at most two
components. We refer to the component contain-
ing the head of the subtree as the head component,
the remaining component as the dependent compo-
nent, and for any given (head component, depen-
dent component) pair, we use pair component to re-
fer to the other component in the pair. We refer to
the two components of dependent dj as dj1 and dj2
respectively, and assume that dj1 is the head com-
ponent. When dependencies can cross, total depen-
dency length cannot be factorized as simply as in
Equation (1) for the projective case. However, we
can still make use of a more complex factorization
of the total dependency length as follows:
⎤
⎥
</bodyText>
<equation confidence="0.9330475">
[D(wi, Ej) + ljkj� ⎦
(2)
</equation>
<bodyText confidence="0.99995325">
where lj is the number of links crossing between the
two components of dj, and kj is the distance added
between these two components by the partial lin-
earization at wi. Figure 3 illustrates an example of
</bodyText>
<equation confidence="0.709490166666667">
�
D(wi, Ei) +
dep
wj →
wi
⎡
⎢ ⎣
�
wi
�
D(wi, Ei) +
dep
wj →
wi
⎡
⎢ ⎣
E
wi
</equation>
<page confidence="0.973879">
337
</page>
<bodyText confidence="0.999953833333333">
such a partial linearization, where k2 is |d31 |+ |d32|
due to the fact that the links between d21 and d22
have to cross both components of d3. The factor-
ization in Equation (2) allows us to use dynamic
programming to find minimal-length linearizations,
so that worst-case complexity is polynomial rather
than exponential in sentence length. However, the
additional term in the factorization means that we
need to track the number of links l crossing between
the two components of the subtree Si headed by wi
and the component lengths |c1 |and |c2|. Addition-
ally, the presence of crossing dependencies means
that Gildea and Temperley’s proof that ordering de-
pendent components from the inside out in order
of increasing length no longer goes through. This
means that at each node wi we need to hold on to the
minimal-length partial linearization for each combi-
nation of the following quantities:
</bodyText>
<listItem confidence="0.996354">
• |c2 |(which also determines |c1|);
• the number of links l between c1 and c2;
• and the direction of the link between wi and its
governor.
</listItem>
<bodyText confidence="0.999859714285714">
We shall refer to a combination of these factors
as a status set. The remainder of this section de-
scribes a dynamic-programming algorithm for find-
ing optimal linearizations based on the factorization
in Equation (2), and continues with several further
findings leading to optimizations that make the al-
gorithm tractable for naturally occurring sentences.
</bodyText>
<subsectionHeader confidence="0.994285">
4.1 Algorithm 1
</subsectionHeader>
<bodyText confidence="0.999985558823529">
Our first algorithm takes a tree and recursively finds
the optimal orderings for each possible status set of
each of its child subtrees, which it then uses to cal-
culate the optimal ordering of the tree. To calcu-
late the optimal orderings for each possible status
set of a subtree S, we use the brute-force method
of choosing all combinations of one status set from
each child subtree, and for each combination, we try
all possible orderings of the components of the child
subtrees, calculate all possible status sets for S, and
store the minimal dependency value for each appear-
ing status set of S. The number of possible length
pairings |c1|, |c2 |and number of crossing links l
are each bounded above by the sentence length n,
so that the maximum number of status sets at each
node is bounded above by n2. Since the sum of the
status sets of all child subtrees is also bounded by
n2, the maximum number of status set combinations
is bounded by (n2 m )m (obtainable from the inequal-
ity of arithmetic and geometric means). There are
(2m+1)!m possible arrangements of head word and
dependent components into two components. Since
there are n nodes in the tree and each possible com-
bination of status sets from each dependent sub tree
must be tried, this algorithm has worst-case com-
plexity of O((2m + 1)!mn(n2 m )m). This algorithm
could be generalized for mildly context-sensitive
linearizations polynomial in sentence length for any
gap degree desired, by introducing additional l terms
denoting the number of links between pairs of com-
ponents. However, even for gap degree 1 this bound
is incredibly large, and as we show in Figure 7, al-
gorithm 1 is not computationally feasible for batch
processing sentences of arity greater than 5.
</bodyText>
<subsectionHeader confidence="0.856682">
4.2 Algorithm 2
</subsectionHeader>
<bodyText confidence="0.999991423076923">
We now show how to speed up our algorithm by
proving by contradiction that for any optimal or-
dering which minimizes the total dependency length
with the two-cluster constraint, for any given sub-
tree S and its child subtree C, the pair components
c1 and c2 of a child subtree C must be placed on
opposite sides of the head h of subtree S.
Let us assume that for some dependency tree
structure, there exists an optimal ordering where c1
and c2 are on the same side of h. Let us refer to the
ordered set of words between c1 and c2 as v. None of
the words in v will have dependency links to any of
the words in c1 and c2, since the dependencies of the
words in c1 and c2 are either between themselves or
the one link to h, which is not between the two com-
ponents by our assumption. There will be j1 &gt; 0
links from v going over c1, j2 &gt; 0 dependency links
from v going over c2, and l &gt; 1 links between c1 and
c2. Without loss of generality, let us assume that h is
on the right side of c2. Let us consider the effect on
total dependency length of swapping c1 with v, so
that the linear ordering is v c1 c2 � h. The total de-
pendency length of the new word ordering changes
by −j1|c1|−l|v|+j2|c1 |if c2 is the head component,
and decreases by another |v |if c1 is the head com-
ponent. Thus the total change in dependency length
</bodyText>
<page confidence="0.991384">
338
</page>
<bodyText confidence="0.9324166">
is less than or equal to
(j2 − j1)|c1 |− l × |v |&lt; (j2 − j1)|c1 |(3)
If instead we swap places of v with c2 instead of c1
so that we have c1 c2 v ≺ h, we find that the total
change in dependency length is less than or equal to
</bodyText>
<equation confidence="0.787099">
(j1 − j2)|c2 |− (l − 1)|v |≤ (j1 − j2)|c2 |(4)
</equation>
<bodyText confidence="0.999881777777778">
It is impossible for the right-hand sides of (3) and (4)
to be positive at the same time, so swapping v with
either c1 or c2 must lead to a linearization with lower
overall dependency length. But this is a contradic-
tion to our original assumption, so we see that for
any optimal ordering, all split child subtree compo-
nents c1 and c2 of the child subtree of S must be
placed on opposite sides of the head h.
This constraint allows us to simplify our algo-
rithm for finding the minimal-length linearization.
Instead of going through all logically possible or-
derings of components of the child subtrees, we can
now decide on which side the head component will
be on, and go through all possible orderings for each
side. This changes the factorial part of our algorithm
run time from (2m + 1)!m to 2m(m!)2m, giving us
O(2m(m!)2mn(n2 m )m), greatly reducing actual pro-
cessing time.
</bodyText>
<subsectionHeader confidence="0.991196">
4.3 Algorithm 3
</subsectionHeader>
<bodyText confidence="0.999930789473684">
We now present two more findings for further in-
creasing the efficiency of the algorithm. First, we
look at the status sets which need to be stored for the
dynamic programming algorithm. In the straightfor-
ward approach we first presented, we stored the op-
timal dependency lengths for all cases of possible
status sets. We now know that we only need to con-
sider cases where the pair components are on op-
posite sides. This means the direction of the link
from the head to the parent will always be toward
the inside direction of the pair components, so we
can re-define the status set as (p, l) where p is again
the length of the dependent component, and l is the
number of links between the two pair components.
If the p values for sets s1 and s2 are equal, s1 has
a smaller number of links than s2 (ls1 ≤ ls2) and
s1 has a smaller or equal total dependency length
to s2, then replacing the components of s2 with s1
will always give us the same or more optimal total
</bodyText>
<figureCaption confidence="0.956718">
Figure 4: Initial setup for latter part of optimization proof
in section 4.4. To the far left is the head h of subtree S.
The component pair C1 and C2 makes up S, and g is the
governor of h. The length of the substring v between C1
and C2 is k. ci and ci+1 are child subtree components.
</figureCaption>
<bodyText confidence="0.989699173913044">
dependency length. Thus, we do not have to store
instances of these cases for our algorithm.
Next, we prove by contradiction that for any two
status sets s1 and s2, if ps1 &gt; ps2 &gt; 0, ls1 = ls2, and
the TOTAL INTERNAL DEPENDENCY LENGTH t1 of
s1—defined as the sum in Equation (2) over only
those words inside the subtree headed by h—is less
than or equal to t2 of s2, then using s1 will be at least
as good as s2, so we can ignore s2. Let us suppose
that the optimal linearization can use s2 but not s1.
Then in the optimal linearization, the two pair com-
ponents cs2,1 and cs2,2 of s2 are on opposite sides
of the parent head h. WLOG, let us assume that
components cs1,1 and cs2,1 are the dependent com-
ponents. Let us denote the total number of links go-
ing over cs2,1 as j1 and the words between cs2,1 and
cs2,2 as v (note that v must contain h). If we swap
cs2,1 with v, so that cs2,1 lies adjacent to cs2,2, then
there would be j2+1 links going over cs2,1. By mov-
ing cs2,1 from opposite sides of the head to be right
next to cs2,2, the total dependency length of the sen-
tence changes by −j1|cs2,1|−ls2|v|+(j2 +1)|cs2,1|.
Since the ordering was optimal, we know that
</bodyText>
<equation confidence="0.980253">
(j2 − j1 + 1)|cs2,1 |− ls2|v |≥ 0
</equation>
<bodyText confidence="0.9028038">
Since l &gt; 0, we can see that j1 − j2 ≤ 0. Now, in-
stead of swapping v with cs2,1, let us try substituting
the components from s1 instead of s2. The change
of the total dependency length of the sentence will
be:
</bodyText>
<equation confidence="0.999331666666667">
j1 × (|cs1,1 |− |cs2,1|) + j2 × (|cs1,2|
−|cs2,2|) + t1 − t2
= (j1 − j2) × (ps1 − ps2) + (t1 − t2)
</equation>
<bodyText confidence="0.882974333333333">
Since j1 − j2 ≤ 0 and ps1 &gt; ps2, the first term
is less than or equal to 0 and since t1 − t2 ≤ 0, the
total dependency length will have been be equal or
</bodyText>
<page confidence="0.998776">
339
</page>
<figureCaption confidence="0.9999955">
Figure 5: Moving ci+1 to C1
Figure 6: Moving ci to C2
</figureCaption>
<bodyText confidence="0.999910857142857">
have decreased. But this contradicts our assumption
that only s2 can be part of an optimal ordering.
This finding greatly reduces the number of sta-
tus sets we need to store and check higher up in
the algorithm. The worst-case complexity remains
O(2mm!2mn(n2 m )m), but the actual runtime is re-
duced by several orders of magnitude.
</bodyText>
<subsectionHeader confidence="0.99132">
4.4 Algorithm 4
</subsectionHeader>
<bodyText confidence="0.975356478260869">
Our last optimization is on the ordering among the
child subtree components on each side of the sub-
tree head h. The initially proposed algorithm went
through all combinations of possible orderings to
find the optimal dependency length for each status
set. By the first optimization in section 4.2 we have
shown that we only need to consider the orderings
in which the components are on opposite sides of
the head. We now look into the ordering of the com-
ponents on each side of the head. We first define the
rank value r for each component c as follows:
|c|
# links between c and its pair component+I(c)
where I(c) is the indicator function having value 1 if
c is a head component and 0 otherwise . Using this
definition, we prove by contradiction that the order-
ing of the components from the head outward must
be in order of increasing rank value.
Let us suppose that at some subtree 5 headed by
h and with head component C1 and dependent com-
ponent C2, there is an optimal linearization in which
there exist two components ci and ci+1 of immedi-
ate subtrees of 5 such that ci is closer to h, the com-
</bodyText>
<figure confidence="0.814076">
1 2 3 4 5 6 7
maximum number of dependencies per head
</figure>
<figureCaption confidence="0.997513">
Figure 7: Timing comparison of first and fully optimized
algorithms
</figureCaption>
<bodyText confidence="0.999964428571429">
ponents have rank values ri and ri+1 respectively,
ri &gt; ri+1, and no other component of the imme-
diate subtrees of 5 intervenes between ci and ci+1.
We shall denote the number of links between each
component and its pair component as li, li+1. Let
l�i = li + I(ci) and lz+1 = li+1 + I(ci+1). There
are two cases to consider: either (1) ci and ci+1 are
within the same component of 5, or (2) ci is at the
edge of C1 nearest C2 and ci+1 is at the edge of C2
neareast C1.
Consider case 1, and let us swap ci with ci+1; this
affects only the lengths of links involving connec-
tions to ci or ci+1. The total dependency length of
the new linearization will change by
</bodyText>
<equation confidence="0.98966">
−lz+1|ci |+ ll|ci+1 |= −lZlz+1(ri − ri+1) &lt; 0
</equation>
<bodyText confidence="0.9985381">
This is a contradiction to the assumption that we had
an optimal ordering.
Now consider case 2, which is illustrated in Fig-
ure 4. We denote the number of links going over
ci and ci+1, excluding links to ci, ci+1 as α1 and
α2 respectively, and the length of words between
the edges of C1 and C2 as k. Let us move ci+1
to the outermost position of C1, as shown in Fig-
ure 5. Since the original linearization was optimal,
we have:
</bodyText>
<equation confidence="0.999706666666667">
−α2|ci+1 |+ α1|ci+1 |− lz+1k ≥ 0
(α1 − α2)|ci+1 |≥ lz+1k
(α1 − α2)ri+1 ≥ k
</equation>
<bodyText confidence="0.995157">
Let us also consider the opposite case of mov-
ing ci to the inner edge of C2, as shown in Fig-
ure 6. Once again due to optimality of the original
linearization, we have
</bodyText>
<figure confidence="0.9889045">
Execution times for algorithms 1 &amp; 4
Algorithm 1
Algorithm 4
106
104
102
100
time(ms)
</figure>
<page confidence="0.982079">
340
</page>
<table confidence="0.995546375">
DLA English Deep German Deep
Surface Surface
Optimal with one crossing dependency 32.7 33.0 24.5 23.3
Optimal with projectivity constraint 34.1 34.4 25.5 24.2
Observed 46.6 48.0 43.6 43.1
Random with projectivity constraint 82.4 82.8 50.6 49.2
Random with two-cluster constraint 84.0 84.3 50.7 49.5
Random ordering with no constraint 183.2 184.2 106.9 101.1
</table>
<tableCaption confidence="0.999915">
Table 1: Average sentence dependency lengths(with max arity of 10)
</tableCaption>
<equation confidence="0.998990333333333">
−α1|ci |+ α2|ci |+ l&apos;k &gt; 0
(α2 − α1)|ci |&gt; −lzk
(α1 − α2)ri G k
</equation>
<bodyText confidence="0.998902">
But this is a contradiction, since ri &gt; ri+1. Com-
bining the two cases, we can see that regardless of
where the components may be split, in an optimal
ordering the components going outwards from the
head must have an increasing rank value.
This result allows us to simplify our algorithm
greatly, because we no longer need to go through
all combinations of orderings. Once it has been de-
cided which components will come on each side of
the head, we can sort the components by rank value
and place them from the head out. This reduces the
factorial component of the algorithm’s complexity
to m log m, and the overall worst-case complexity
to O(nm2 log m(2n2 m )m). Although this is still ex-
ponential in the arity of the tree, nearly all sentences
encountered in treebanks have an arity low enough
to make the algorithm tractable and even very effi-
cient, as we show in the following section.
</bodyText>
<sectionHeader confidence="0.978242" genericHeader="method">
5 Empirical results
</sectionHeader>
<bodyText confidence="0.99997342">
Using the above algorithm, we calculated minimal
dependency lengths for English sentences from the
WSJ portion of the Penn Treebank, and for German
sentences from the NEGRA corpus. The English-
German comparison is of interest because word or-
der is freer, and crossing dependencies more com-
mon, in German than in English (Kruijff and Va-
sishth, 2003). We extracted dependency trees from
these corpora using the head rules of Collins (1999)
for English, and the head rules of Levy and Man-
ning (2004) for German. Two dependency trees
were extracted from each sentence, the surface tree
extracted by using the head rules on the context-
free tree representation (i.e. no crossing dependen-
cies), and the deep tree extracted by first return-
ing discontinuous dependents (marked by *T* and
*ICH* in WSJ, and by *T* in the Penn-format ver-
sion of NEGRA) before applying head rules. Fig-
ure 7 shows the average time it takes to calculate
the minimal dependency length with crossing depen-
dencies for WSJ sentences using the unoptimized al-
gorithm of Section 4.1 and the fully optimized al-
gorithm of Section 4.4. Timing tests were imple-
mented and performed using Java 1.6.0 10 on a sys-
tem running Linux 2.6.18-6-amd64 with a 2.0 GHz
Intel Xeon processor and 16 gigs of memory, run on
a single core. We can see from Figure 7 that the
straight-forward dynamic programming algorithm
takes many more magnitudes of time than our op-
timized algorithm, making it infeasible to calculate
the minimal dependency length for larger sentences.
The results we present below were obtained with the
fully optimized algorithm from the sentences with
a maximum arity of 10, using 49,176 of the 49,208
WSJ sentences and 20,563 of the 20,602 NEGRA
sentences.
Summary results over all sentences from each cor-
pus are shown in Table 1. We can see that for both
corpora, the oberved dependency length is smaller
than the dependency length of random orderings,
even when the random ordering is subject to the
projectivity constraint. Relaxing the projectivity
constraint by allowing crossing dependencies intro-
duces a slightly lower optimal dependency length.
The average sentence dependency lengths for the
three random orderings are significantly higher than
the observed values. It is interesting to note that the
random orderings given the projectivity constraint
and the two-cluster constraint have very similar de-
pendency lengths, where as a total random ordering
</bodyText>
<page confidence="0.989865">
341
</page>
<figure confidence="0.9983854">
English/Surface English/Deep German/Surface German/Deep
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Sentence length Sentence length Sentence length Sentence length
</figure>
<figureCaption confidence="0.991744">
Figure 8: Average sentence DL as a function of sentence length. Legend is ordered top curve to bottom curve.
</figureCaption>
<figure confidence="0.999919157894737">
English/Surface English/Deep
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Sentence Arity Sentence Arity
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
German/Surface German/Deep
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Sentence Arity Sentence Arity
Average sentence DL
0 100 200 300 400
Average sentence DL
0 100 200 300 400
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
Unconstrained Random
2−component Random
Projective Random
Observed
Projective Optimal
2−component Optimal
</figure>
<figureCaption confidence="0.99995">
Figure 9: Average sentence DL as a function of sentence arity. Legend is ordered top curve to bottom curve.
</figureCaption>
<bodyText confidence="0.999157066666667">
increases the dependency length significantly.
NEGRA generally has shorter sentences than
WSJ, so we need a more detailed picture of depen-
dency length as a function of sentence length; this
is shown in Figure 8. As in Table 1, we see that
English, which has less crossing dependency struc-
tures than German, has observed DL closer to opti-
mal DL and farther from random DL. We also see
that the random and observed DLs behave very sim-
ilarly across different sentence lengths in English
and German, but observed DL grows faster in Ger-
man. Perhaps surprisingly, optimal projective DL
and gap-degree 1 DL tend to be very similar even
for longer sentences. The picture as a function of
sentence arity is largely the same (Figure 9).
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987151515151">
In this paper, we have presented an efficient dynamic
programming algorithm which finds minimum-
length dependency-tree linearizations subject to
constraints of mild context-sensitivity. For the gap-
degree 1 case, we have proven several properties of
these linearizations, and have used these properties
to optimize our algorithm. This made it possible to
find minimal dependency lengths for sentences from
the English Penn Treebank WSJ and German NE-
GRA corpora. The results show that for both lan-
guages, using surface dependencies and deep de-
pendencies lead to generally similar conclusions,
but that minimal lengths for deep dependencies are
consistently slightly higher for English and slightly
lower for German. This may be because German
has many more crossing dependencies than English.
Another finding is that the difference between aver-
age sentence DL does not change much between op-
timizing for the projectivity constraint and the two-
cluster constraint: projectivity seems to give nat-
ural language almost all the flexibility it needs to
minimize DL. For both languages, the observed lin-
earization is much closer in DL to optimal lineariza-
tions than to random linearizations; but crucially, we
see that English is closer to the optimal linearization
and farther from random linearization than German.
This finding is resonant with the fact that German
has richer morphology and overall greater variability
in observed word order, and with psycholinguistic
results suggesting that dependencies of greater lin-
ear distance do not always pose the same increased
processing load in German sentence comprehension
as they do in English (Konieczny, 2000).
</bodyText>
<page confidence="0.9973">
342
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999715">
Chung, F. R. K. (1984). On optimal linear arrange-
ments of trees. Computers and Mathematics with
Applications, 10:43–60.
Collins, M. (1999). Head-Driven Statistical Models
for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania.
Gibson, E. (1998). Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1–76.
Gildea, D. and Temperley, D. (2007). Optimizing
grammars for minimum dependency length. In
Proceedings ofACL.
Hawkins, J. A. (1994). A Performance Theory of
Order and Constituency. Cambridge.
Hawkins, J. A. (2004). Efficiency and Complexity in
Grammars. Oxford University Press.
Jaeger, T. F. (2006). Redundancy and Syntactic Re-
duction in Spontaneous Speech. PhD thesis, Stan-
ford University, Stanford, CA.
Joshi, A. K. (1985). How much context-sensitivity
is necessary for characterizing structural descrip-
tions – Tree Adjoining Grammars. In Dowty,
D., Karttunen, L., and Zwicky, A., editors, Nat-
ural Language Processing – Theoretical, Com-
putational, and Psychological Perspectives. Cam-
bridge.
Joshi, A. K., Levy, L. S., and Takahashi, M. (1975).
Tree adjunct grammars. Journal of Computer and
System Sciences, 10(1).
Kobele, G. M. (2006). Generating Copies: An inves-
tigation into Structural Identity in Language and
Grammar. PhD thesis, UCLA.
Konieczny, L. (2000). Locality and parsing com-
plexity. Journal of Psycholinguistic Research,
29(6):627–645.
Kruijff, G.-J. M. and Vasishth, S. (2003). Quantify-
ing word order freedom in natural language: Im-
plications for sentence processing. Proceedings of
the Architectures and Mechanisms for Language
Processing conference.
Kuhlmann, M. (2007). Dependency Structures and
Lexicalized Grammars. PhD thesis, Saarland Uni-
versity.
Kuhlmann, M. and M¨ohl, M. (2007). Mildly
context-sensitive dependency languages. In Pro-
ceedings ofACL.
Kuhlmann, M. and Nivre, J. (2006). Mildly non-
projective dependency structures. In Proceedings
of COLING/ACL.
Levy, R. and Manning, C. (2004). Deep depen-
dencies from context-free statistical parsers: cor-
recting the surface dependency approximation. In
Proceedings ofACL.
McDonald, R., Crammer, K., and Pereira, F.
(2005a). Online large-margin training of depen-
dency parsers. In Proceedings ofACL.
McDonald, R., Pereira, F., Ribarov, K., and Hajiˇc,
J. (2005b). Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of
ACL.
Miller, P. (2000). Strong Generative Capacity: The
Semantics of Linguistic Formalism. Cambridge.
Pollard, C. (1984). Generalized Phrase Structure
Grammars, Head Grammars, and Natural Lan-
guages. PhD thesis, Stanford.
Vijay-Shanker, K., Weir, D. J., and Joshi, A. K.
(1987). Characterizing structural descriptions
produced by various grammatical formalisms. In
Proceedings ofACL.
</reference>
<page confidence="0.999369">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.853957">
<title confidence="0.999758">Minimal-length linearizations for mildly context-sensitive dependency trees</title>
<author confidence="0.999742">Y Albert</author>
<affiliation confidence="0.999949">Department of Computer Science and</affiliation>
<address confidence="0.9813335">9500 Gilman La Jolla, CA 92037-404,</address>
<email confidence="0.99983">yapark@ucsd.edu</email>
<author confidence="0.989936">Roger</author>
<affiliation confidence="0.991367">Department of</affiliation>
<address confidence="0.983805">9500 Gilman La Jolla, CA 92037-108,</address>
<email confidence="0.99987">rlevy@ling.ucsd.edu</email>
<abstract confidence="0.99735537037037">The extent to which the organization of natural language grammars reflects a drive to minimize dependency length remains little explored. We present the first algorithm polynomial-time in sentence length for obtaining the minimal-length linearization of a dependency tree subject to constraints of mild context sensitivity. For the minimally contextsensitive case of gap-degree 1 dependency trees, we prove several properties of minimallength linearizations which allow us to improve the efficiency of our algorithm to the point that it can be used on most naturallyoccurring sentences. We use the algorithm to compare optimal, observed, and random sentence dependency length for both surface and deep dependencies in English and German. We find in both languages that analyses of surface and deep dependencies yield highly similar results, and that mild contextsensitivity affords very little reduction in minimal dependency length over fully projective linearizations; but that observed linearizations in German are much closer to random and farther from minimal-length linearizations than in English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F R K Chung</author>
</authors>
<title>On optimal linear arrangements of trees.</title>
<date>1984</date>
<journal>Computers and Mathematics with Applications,</journal>
<pages>10--43</pages>
<contexts>
<context position="7335" citStr="Chung, 1984" startWordPosition="1111" endWordPosition="1112">rojectivity is violated in natural language, there are exceedingly few documented cases in which the documented gap degree exceeds 1 (though see, for example, Kobele, 2006). 3 Finding minimal dependency-length linearizations Even under the strongest constraint of projectivity, the number of possible linearizations of a dependency tree is exponential in both sentence length and arity (the maximum number of dependencies for any word). As pointed out by Gildea and Temperley (2007), however, finding the unconstrained minimal-length linearization is a well-studied problem with an O(n1.6) solution (Chung, 1984). However, this approach does not take into account constraints of projectivity or mild context-sensitivity. Gildea and Temperley themselves introduced a novel efficient algorithm for finding the minimized dependency length of a sentence subject to the constraint that the linearization is projective. Their algorithm can perhaps be most simply understood by making three observations. First, the total depen336 d12 d11 h d21 d32 d31 d22 |c1 |kh |c2| Figure 2: Dependency length factorization for efficient projective linearization, using the dependency subtree of Figure 1 dency length of a projecti</context>
</contexts>
<marker>Chung, 1984</marker>
<rawString>Chung, F. R. K. (1984). On optimal linear arrangements of trees. Computers and Mathematics with Applications, 10:43–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="24865" citStr="Collins (1999)" startWordPosition="4338" endWordPosition="4339">all sentences encountered in treebanks have an arity low enough to make the algorithm tractable and even very efficient, as we show in the following section. 5 Empirical results Using the above algorithm, we calculated minimal dependency lengths for English sentences from the WSJ portion of the Penn Treebank, and for German sentences from the NEGRA corpus. The EnglishGerman comparison is of interest because word order is freer, and crossing dependencies more common, in German than in English (Kruijff and Vasishth, 2003). We extracted dependency trees from these corpora using the head rules of Collins (1999) for English, and the head rules of Levy and Manning (2004) for German. Two dependency trees were extracted from each sentence, the surface tree extracted by using the head rules on the contextfree tree representation (i.e. no crossing dependencies), and the deep tree extracted by first returning discontinuous dependents (marked by *T* and *ICH* in WSJ, and by *T* in the Penn-format version of NEGRA) before applying head rules. Figure 7 shows the average time it takes to calculate the minimal dependency length with crossing dependencies for WSJ sentences using the unoptimized algorithm of Sect</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>Linguistic complexity: Locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>68--1</pages>
<contexts>
<context position="2622" citStr="Gibson, 1998" startWordPosition="388" endWordPosition="389">ollard, 1984; Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances between governors and dependents induce processing difficulty in both comprehension and production (Hawkins, 1994, 2004; Gibson, 1998; Jaeger, 2006). Intuitively there is a relationship between these two constraints: consistently large dependency distances in a sentence would require many crossing dependencies. However, it is not the case that crossing dependencies always mean longer dependency distances. For example, (1) below has no crossing dependencies, but the distance between arrived and its dependent Yesterday is large. The overall dependency length of the sentence can be reduced by extraposing the relative clause who was wearing a hat, resulting in (2), in which the dependency Yesterday→arrived crosses the dependenc</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Gibson, E. (1998). Linguistic complexity: Locality of syntactic dependencies. Cognition, 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Temperley</author>
</authors>
<title>Optimizing grammars for minimum dependency length.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3661" citStr="Gildea and Temperley, 2007" startWordPosition="543" endWordPosition="546">dependency length of the sentence can be reduced by extraposing the relative clause who was wearing a hat, resulting in (2), in which the dependency Yesterday→arrived crosses the dependency woman←who. (1) Yesterday a woman who was wearing a hat arrived. (2) Yesterday a woman arrived who was wearing a hat. 335 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 335–343, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics There has been some recent work on dependency length minimization in natural language sentences (Gildea and Temperley, 2007), but the relationship between the precise constraints on available linearizations and dependency length minimization remains little explored. In this paper, we introduce the first efficient algorithm for obtaining linearizations of dependency trees that minimize overall dependency lengths subject to the constraint of mild context-sensitivity, and use it to investigate the relationship between this constraint and the distribution of dependency length actually observed in natural languages. 2 Projective and mildly non-projective dependency-tree linearizations In the last few years there has bee</context>
<context position="7205" citStr="Gildea and Temperley (2007)" startWordPosition="1091" endWordPosition="1095">n and M¨ohl, 2007). The dependency tree for Example (2), for example, is of gap-degree 1. Although there are numerous documented cases in which projectivity is violated in natural language, there are exceedingly few documented cases in which the documented gap degree exceeds 1 (though see, for example, Kobele, 2006). 3 Finding minimal dependency-length linearizations Even under the strongest constraint of projectivity, the number of possible linearizations of a dependency tree is exponential in both sentence length and arity (the maximum number of dependencies for any word). As pointed out by Gildea and Temperley (2007), however, finding the unconstrained minimal-length linearization is a well-studied problem with an O(n1.6) solution (Chung, 1984). However, this approach does not take into account constraints of projectivity or mild context-sensitivity. Gildea and Temperley themselves introduced a novel efficient algorithm for finding the minimized dependency length of a sentence subject to the constraint that the linearization is projective. Their algorithm can perhaps be most simply understood by making three observations. First, the total depen336 d12 d11 h d21 d32 d31 d22 |c1 |kh |c2| Figure 2: Dependenc</context>
</contexts>
<marker>Gildea, Temperley, 2007</marker>
<rawString>Gildea, D. and Temperley, D. (2007). Optimizing grammars for minimum dependency length. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hawkins</author>
</authors>
<title>A Performance Theory of Order and Constituency.</title>
<date>1994</date>
<location>Cambridge.</location>
<contexts>
<context position="2602" citStr="Hawkins, 1994" startWordPosition="385" endWordPosition="386">distinguished head (Pollard, 1984; Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances between governors and dependents induce processing difficulty in both comprehension and production (Hawkins, 1994, 2004; Gibson, 1998; Jaeger, 2006). Intuitively there is a relationship between these two constraints: consistently large dependency distances in a sentence would require many crossing dependencies. However, it is not the case that crossing dependencies always mean longer dependency distances. For example, (1) below has no crossing dependencies, but the distance between arrived and its dependent Yesterday is large. The overall dependency length of the sentence can be reduced by extraposing the relative clause who was wearing a hat, resulting in (2), in which the dependency Yesterday→arrived c</context>
</contexts>
<marker>Hawkins, 1994</marker>
<rawString>Hawkins, J. A. (1994). A Performance Theory of Order and Constituency. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hawkins</author>
</authors>
<title>Efficiency and Complexity in Grammars.</title>
<date>2004</date>
<publisher>Oxford University Press.</publisher>
<marker>Hawkins, 2004</marker>
<rawString>Hawkins, J. A. (2004). Efficiency and Complexity in Grammars. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T F Jaeger</author>
</authors>
<title>Redundancy and Syntactic Reduction in Spontaneous Speech.</title>
<date>2006</date>
<tech>PhD thesis,</tech>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="2637" citStr="Jaeger, 2006" startWordPosition="390" endWordPosition="391">Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances between governors and dependents induce processing difficulty in both comprehension and production (Hawkins, 1994, 2004; Gibson, 1998; Jaeger, 2006). Intuitively there is a relationship between these two constraints: consistently large dependency distances in a sentence would require many crossing dependencies. However, it is not the case that crossing dependencies always mean longer dependency distances. For example, (1) below has no crossing dependencies, but the distance between arrived and its dependent Yesterday is large. The overall dependency length of the sentence can be reduced by extraposing the relative clause who was wearing a hat, resulting in (2), in which the dependency Yesterday→arrived crosses the dependency woman←who. (1</context>
</contexts>
<marker>Jaeger, 2006</marker>
<rawString>Jaeger, T. F. (2006). Redundancy and Syntactic Reduction in Spontaneous Speech. PhD thesis, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>How much context-sensitivity is necessary for characterizing structural descriptions – Tree Adjoining Grammars.</title>
<date>1985</date>
<booktitle>Natural Language Processing – Theoretical, Computational, and Psychological Perspectives.</booktitle>
<editor>In Dowty, D., Karttunen, L., and Zwicky, A., editors,</editor>
<location>Cambridge.</location>
<contexts>
<context position="1888" citStr="Joshi, 1985" startWordPosition="278" endWordPosition="279">tions; but that observed linearizations in German are much closer to random and farther from minimal-length linearizations than in English. 1 Introduction This paper takes up the relationship between two hallmarks of natural language dependency structure. First, there seem to be qualitative constraints on the relationship between the dependency structure of the words in a sentence and their linear ordering. In particular, this relationship seems to be such that any natural language sentence, together with its dependency structure, should be generable by a mildly context-sensitivity formalism (Joshi, 1985), in particular a linear context-free rewrite system in which the right-hand side of each rule has a distinguished head (Pollard, 1984; Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, A. K. (1985). How much context-sensitivity is necessary for characterizing structural descriptions – Tree Adjoining Grammars. In Dowty, D., Karttunen, L., and Zwicky, A., editors, Natural Language Processing – Theoretical, Computational, and Psychological Perspectives. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="6177" citStr="Joshi et al., 1975" startWordPosition="925" endWordPosition="928">LCFRSs are an infinite sequence of classes of formalism for generating surface strings through derivation trees in a rule-based context-free rewriting system. The i-th LCFRS class (for i = 0, 1, 2,... ) imposes the conFigure 1: Sample dependency subtree for Figure 2 straint that every node in the derivation tree maps to to a collection of at most i+1 contiguous substrings. The 0-th class of LCFRS, for example, corresponds to the context-free grammars, since each node in the derivation tree must map to a single contiguous substring; the 1st class of LCFRS corresponds to TreeAdjoining Grammars (Joshi et al., 1975), in which each node in the derivation tree must map to at most a pair of contiguous substrings; and so forth. The dependency trees induced when each rewrite rule in an i-th order LCFRS distinguish a unique head can similarly be characterized by being of gap-degree i, so that i is the maximum number of gaps that may appear between contiguous substrings of any subtree in the dependency tree (Kuhlmann and M¨ohl, 2007). The dependency tree for Example (2), for example, is of gap-degree 1. Although there are numerous documented cases in which projectivity is violated in natural language, there are</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, A. K., Levy, L. S., and Takahashi, M. (1975). Tree adjunct grammars. Journal of Computer and System Sciences, 10(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G M Kobele</author>
</authors>
<title>Generating Copies: An investigation into Structural Identity in Language and Grammar.</title>
<date>2006</date>
<tech>PhD thesis, UCLA.</tech>
<contexts>
<context position="6895" citStr="Kobele, 2006" startWordPosition="1047" endWordPosition="1048">orth. The dependency trees induced when each rewrite rule in an i-th order LCFRS distinguish a unique head can similarly be characterized by being of gap-degree i, so that i is the maximum number of gaps that may appear between contiguous substrings of any subtree in the dependency tree (Kuhlmann and M¨ohl, 2007). The dependency tree for Example (2), for example, is of gap-degree 1. Although there are numerous documented cases in which projectivity is violated in natural language, there are exceedingly few documented cases in which the documented gap degree exceeds 1 (though see, for example, Kobele, 2006). 3 Finding minimal dependency-length linearizations Even under the strongest constraint of projectivity, the number of possible linearizations of a dependency tree is exponential in both sentence length and arity (the maximum number of dependencies for any word). As pointed out by Gildea and Temperley (2007), however, finding the unconstrained minimal-length linearization is a well-studied problem with an O(n1.6) solution (Chung, 1984). However, this approach does not take into account constraints of projectivity or mild context-sensitivity. Gildea and Temperley themselves introduced a novel </context>
</contexts>
<marker>Kobele, 2006</marker>
<rawString>Kobele, G. M. (2006). Generating Copies: An investigation into Structural Identity in Language and Grammar. PhD thesis, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Konieczny</author>
</authors>
<title>Locality and parsing complexity.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>6</issue>
<marker>Konieczny, 2000</marker>
<rawString>Konieczny, L. (2000). Locality and parsing complexity. Journal of Psycholinguistic Research, 29(6):627–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-J M Kruijff</author>
<author>S Vasishth</author>
</authors>
<title>Quantifying word order freedom in natural language: Implications for sentence processing.</title>
<date>2003</date>
<booktitle>Proceedings of the Architectures and Mechanisms for Language Processing conference.</booktitle>
<contexts>
<context position="24776" citStr="Kruijff and Vasishth, 2003" startWordPosition="4321" endWordPosition="4325">plexity to O(nm2 log m(2n2 m )m). Although this is still exponential in the arity of the tree, nearly all sentences encountered in treebanks have an arity low enough to make the algorithm tractable and even very efficient, as we show in the following section. 5 Empirical results Using the above algorithm, we calculated minimal dependency lengths for English sentences from the WSJ portion of the Penn Treebank, and for German sentences from the NEGRA corpus. The EnglishGerman comparison is of interest because word order is freer, and crossing dependencies more common, in German than in English (Kruijff and Vasishth, 2003). We extracted dependency trees from these corpora using the head rules of Collins (1999) for English, and the head rules of Levy and Manning (2004) for German. Two dependency trees were extracted from each sentence, the surface tree extracted by using the head rules on the contextfree tree representation (i.e. no crossing dependencies), and the deep tree extracted by first returning discontinuous dependents (marked by *T* and *ICH* in WSJ, and by *T* in the Penn-format version of NEGRA) before applying head rules. Figure 7 shows the average time it takes to calculate the minimal dependency le</context>
</contexts>
<marker>Kruijff, Vasishth, 2003</marker>
<rawString>Kruijff, G.-J. M. and Vasishth, S. (2003). Quantifying word order freedom in natural language: Implications for sentence processing. Proceedings of the Architectures and Mechanisms for Language Processing conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
</authors>
<title>Dependency Structures and Lexicalized Grammars.</title>
<date>2007</date>
<tech>PhD thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="2067" citStr="Kuhlmann, 2007" startWordPosition="306" endWordPosition="307">he relationship between two hallmarks of natural language dependency structure. First, there seem to be qualitative constraints on the relationship between the dependency structure of the words in a sentence and their linear ordering. In particular, this relationship seems to be such that any natural language sentence, together with its dependency structure, should be generable by a mildly context-sensitivity formalism (Joshi, 1985), in particular a linear context-free rewrite system in which the right-hand side of each rule has a distinguished head (Pollard, 1984; Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances between governors and dependents induce processing difficulty in both comprehension and production (Hawkins, 1994, 2004; Gibson, 1998; Jaeger, 2006). Intuitively there is a relat</context>
</contexts>
<marker>Kuhlmann, 2007</marker>
<rawString>Kuhlmann, M. (2007). Dependency Structures and Lexicalized Grammars. PhD thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
<author>M M¨ohl</author>
</authors>
<title>Mildly context-sensitive dependency languages.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Kuhlmann, M¨ohl, 2007</marker>
<rawString>Kuhlmann, M. and M¨ohl, M. (2007). Mildly context-sensitive dependency languages. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
<author>J Nivre</author>
</authors>
<title>Mildly nonprojective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="5552" citStr="Kuhlmann and Nivre, 2006" startWordPosition="819" endWordPosition="822">ree, and non-projective linearizations that involve at least one crossing dependency pair. Example (1), for example, is projective, whereas Example (2) is non-projective due to the crossing between the Yesterday→arrived and woman←who dependencies. Beyond this dichotomy, however, the homomorphism from headed tree structures to dependency structures (Miller, 2000) can be used together with work on the mildly context-sensitive formalism linear context-free rewrite systems (LCFRSs) (VijayShanker et al., 1987) to characterize various classes of mildly non-projective dependency-tree linearizations (Kuhlmann and Nivre, 2006). The LCFRSs are an infinite sequence of classes of formalism for generating surface strings through derivation trees in a rule-based context-free rewriting system. The i-th LCFRS class (for i = 0, 1, 2,... ) imposes the conFigure 1: Sample dependency subtree for Figure 2 straint that every node in the derivation tree maps to to a collection of at most i+1 contiguous substrings. The 0-th class of LCFRS, for example, corresponds to the context-free grammars, since each node in the derivation tree must map to a single contiguous substring; the 1st class of LCFRS corresponds to TreeAdjoining Gram</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Kuhlmann, M. and Nivre, J. (2006). Mildly nonprojective dependency structures. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>C Manning</author>
</authors>
<title>Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="24924" citStr="Levy and Manning (2004)" startWordPosition="4347" endWordPosition="4351">ty low enough to make the algorithm tractable and even very efficient, as we show in the following section. 5 Empirical results Using the above algorithm, we calculated minimal dependency lengths for English sentences from the WSJ portion of the Penn Treebank, and for German sentences from the NEGRA corpus. The EnglishGerman comparison is of interest because word order is freer, and crossing dependencies more common, in German than in English (Kruijff and Vasishth, 2003). We extracted dependency trees from these corpora using the head rules of Collins (1999) for English, and the head rules of Levy and Manning (2004) for German. Two dependency trees were extracted from each sentence, the surface tree extracted by using the head rules on the contextfree tree representation (i.e. no crossing dependencies), and the deep tree extracted by first returning discontinuous dependents (marked by *T* and *ICH* in WSJ, and by *T* in the Penn-format version of NEGRA) before applying head rules. Figure 7 shows the average time it takes to calculate the minimal dependency length with crossing dependencies for WSJ sentences using the unoptimized algorithm of Section 4.1 and the fully optimized algorithm of Section 4.4. T</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Levy, R. and Manning, C. (2004). Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4410" citStr="McDonald et al. (2005" startWordPosition="651" endWordPosition="654">ttle explored. In this paper, we introduce the first efficient algorithm for obtaining linearizations of dependency trees that minimize overall dependency lengths subject to the constraint of mild context-sensitivity, and use it to investigate the relationship between this constraint and the distribution of dependency length actually observed in natural languages. 2 Projective and mildly non-projective dependency-tree linearizations In the last few years there has been a resurgence of interest in computation on dependency-tree structures for natural language sentences, spurred by work such as McDonald et al. (2005a,b) showing that working with dependency-tree syntactic representations in which each word in the sentence corresponds to a node in the dependency tree (and vice versa) can lead to algorithmic benefits over constituency-structure representations. The linearization of a dependency tree is simply the linear order in which the nodes of the tree occur in a surface string. There is a broad division between two classes of linearizations: projective linearizations that do not lead to any crossing dependencies in the tree, and non-projective linearizations that involve at least one crossing dependenc</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R., Crammer, K., and Pereira, F. (2005a). Online large-margin training of dependency parsers. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>McDonald, R., Pereira, F., Ribarov, K., and Hajiˇc, J. (2005b). Non-projective dependency parsing using spanning tree algorithms. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Miller</author>
</authors>
<title>Strong Generative Capacity: The Semantics of Linguistic Formalism.</title>
<date>2000</date>
<location>Cambridge.</location>
<contexts>
<context position="5291" citStr="Miller, 2000" startWordPosition="785" endWordPosition="786"> a dependency tree is simply the linear order in which the nodes of the tree occur in a surface string. There is a broad division between two classes of linearizations: projective linearizations that do not lead to any crossing dependencies in the tree, and non-projective linearizations that involve at least one crossing dependency pair. Example (1), for example, is projective, whereas Example (2) is non-projective due to the crossing between the Yesterday→arrived and woman←who dependencies. Beyond this dichotomy, however, the homomorphism from headed tree structures to dependency structures (Miller, 2000) can be used together with work on the mildly context-sensitive formalism linear context-free rewrite systems (LCFRSs) (VijayShanker et al., 1987) to characterize various classes of mildly non-projective dependency-tree linearizations (Kuhlmann and Nivre, 2006). The LCFRSs are an infinite sequence of classes of formalism for generating surface strings through derivation trees in a rule-based context-free rewriting system. The i-th LCFRS class (for i = 0, 1, 2,... ) imposes the conFigure 1: Sample dependency subtree for Figure 2 straint that every node in the derivation tree maps to to a collec</context>
</contexts>
<marker>Miller, 2000</marker>
<rawString>Miller, P. (2000). Strong Generative Capacity: The Semantics of Linguistic Formalism. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
</authors>
<title>Generalized Phrase Structure Grammars, Head Grammars, and Natural Languages. PhD thesis,</title>
<date>1984</date>
<location>Stanford.</location>
<contexts>
<context position="2022" citStr="Pollard, 1984" startWordPosition="300" endWordPosition="301">glish. 1 Introduction This paper takes up the relationship between two hallmarks of natural language dependency structure. First, there seem to be qualitative constraints on the relationship between the dependency structure of the words in a sentence and their linear ordering. In particular, this relationship seems to be such that any natural language sentence, together with its dependency structure, should be generable by a mildly context-sensitivity formalism (Joshi, 1985), in particular a linear context-free rewrite system in which the right-hand side of each rule has a distinguished head (Pollard, 1984; Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances between governors and dependents induce processing difficulty in both comprehension and production (Hawkins, 1994, 2004; Gibson, 1998</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Pollard, C. (1984). Generalized Phrase Structure Grammars, Head Grammars, and Natural Languages. PhD thesis, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2050" citStr="Vijay-Shanker et al., 1987" startWordPosition="302" endWordPosition="305">uction This paper takes up the relationship between two hallmarks of natural language dependency structure. First, there seem to be qualitative constraints on the relationship between the dependency structure of the words in a sentence and their linear ordering. In particular, this relationship seems to be such that any natural language sentence, together with its dependency structure, should be generable by a mildly context-sensitivity formalism (Joshi, 1985), in particular a linear context-free rewrite system in which the right-hand side of each rule has a distinguished head (Pollard, 1984; Vijay-Shanker et al., 1987; Kuhlmann, 2007). This condition places strong constraints on the linear contiguity of word-word dependency relations, such that only limited classes of crossing context-free dependency structures may be admitted. The second constraint is a softer preference for words in a dependency relation to occur in close proximity to one another. This constraint is perhaps best documented in psycholinguistic work suggesting that large distances between governors and dependents induce processing difficulty in both comprehension and production (Hawkins, 1994, 2004; Gibson, 1998; Jaeger, 2006). Intuitively</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>Vijay-Shanker, K., Weir, D. J., and Joshi, A. K. (1987). Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>