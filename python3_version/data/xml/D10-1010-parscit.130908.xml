<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988497">
Learning the Relative Usefulness of Questions in Community QA
</title>
<author confidence="0.917196">
Razvan Bunescu
</author>
<affiliation confidence="0.939005">
School of EECS
Ohio University
</affiliation>
<address confidence="0.876779">
Athens, OH 43201, USA
</address>
<email confidence="0.999357">
bunescu@ohio.edu
</email>
<author confidence="0.997534">
Yunfeng Huang
</author>
<affiliation confidence="0.9979395">
School of EECS
Ohio University
</affiliation>
<address confidence="0.966327">
Athens, OH 43201, USA
</address>
<email confidence="0.999298">
yh324906@ohio.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998395">
We present a machine learning approach for
the task of ranking previously answered ques-
tions in a question repository with respect to
their relevance to a new, unanswered refer-
ence question. The ranking model is trained
on a collection of question groups manually
annotated with a partial order relation reflect-
ing the relative utility of questions inside each
group. Based on a set of meaning and struc-
ture aware features, the new ranking model is
able to substantially outperform more straight-
forward, unsupervised similarity measures.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992464490566038">
Open domain Question Answering (QA) is one of
the most complex and challenging tasks in natural
language processing. In general, a question answer-
ing system may need to integrate knowledge coming
from a wide variety of linguistic processing tasks
such as syntactic parsing, semantic role labeling,
named entity recognition, and anaphora resolution
(Prager, 2006). State of the art implementations of
these linguistic analysis tasks are still limited in their
performance, with errors that compound and prop-
agate into the final performance of the QA system
(Moldovan et al., 2002). Consequently, the perfor-
mance of open domain QA systems has yet to ar-
rive at a level at which it would become a feasible
alternative to the current paradigms for information
access based on keyword searches.
Recently, community-driven QA sites such as Ya-
hoo! Answers and WikiAnswers 1 have established
1answers.yahoo.com, wiki.answers.com
a new approach to question answering that shifts
the inherent complexity of open domain QA from
the computer system to volunteer contributors. The
computer is no longer required to perform a deep
linguistic analysis of questions and generate corre-
sponding answers, and instead acts as a mediator
between users submitting questions and volunteers
providing the answers.
An important objective in community QA is to
minimize the time elapsed between the submission
of questions by users and the subsequent posting
of answers by volunteer contributors. One useful
strategy for minimizing the response latency is to
search the QA repository for similar questions that
have already been answered, and provide the cor-
responding ranked list of answers, if such a ques-
tion is found. The success of this approach de-
pends on the definition and implementation of the
question-to-question similarity function. In the sim-
plest solution, the system searches for previously
answered questions based on exact string match-
ing with the reference question. Alternatively, sites
such as WikiAnswers allow the users to mark ques-
tions they think are rephrasings (“alternate word-
ings”, or paraphrases) of existing questions. These
question clusters are then taken into account when
performing exact string matching, therefore increas-
ing the likelihood of finding previously answered
questions that are semantically equivalent to the ref-
erence question.
In order to lessen the amount of work required
from the contributors, an alternative approach is to
build a system that automatically finds rephrasings
of questions, especially since question rephrasing
</bodyText>
<page confidence="0.995861">
97
</page>
<note confidence="0.821162">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 97–107,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.941660047619048">
seems to be computationally less demanding than
question answering. According to previous work in
this domain, a question is considered a rephrasing of
a reference question Q0 if it uses an alternate word-
ing to express an identical information need. For
example, Q0 and Q1 below are rephrasings of each
other, and consequently they are expected to have
the same answer.
Q0 What should I feed my turtle?
Q1 What do I feed my pet turtle?
Paraphrasings of a new question cannot always be
found in the community QA repository. We believe
that computing a ranked list of existing questions
that at least partially address the original informa-
tion need could also be useful to the user, at least
until other users volunteer to give an exact answer
to the original, unanswered reference question. For
example, in the absence of any additional informa-
tion about the reference question Q0, the expected
answers to questions Q2 and Q3 below may be seen
as partially overlapping in information content with
the expected answer for the reference question Q0.
An answer to question Q4, on the other hand, is less
likely to benefit the user, even though it has a signif-
icant lexical overlap with the reference question.
Q2 What kind of fish should I feed my turtle?
Q3 What do you feed a turtle that is the size of a
quarter?
Q4 What kind of food should I feed a turtle dove?
In this paper, we propose a supervised learning
approach to the question ranking problem, a gen-
eralization of the question paraphrasing problem in
which questions are ranked in a partial order based
on the relative information overlap between their ex-
pected answers and the expected answer of the refer-
ence question. Underlying the question ranking task
is the expectation that the user who submits a ref-
erence question will find the answers of the highly
ranked questions to be more useful than the answers
associated with the lower ranked questions. For the
reference question Q0 above, the learned ranking
model is expected to produce a partial order in which
</bodyText>
<equation confidence="0.286294">
Q1 is ranked higher than Q2, Q3 and Q4, whereas
Q2 and Q3 are ranked higher than Q4.
</equation>
<sectionHeader confidence="0.9243885" genericHeader="introduction">
2 Partially Ordered Datasets for Question
Ranking
</sectionHeader>
<bodyText confidence="0.994206133333334">
In order to enable the evaluation of question rank-
ing approaches, we have previously created a dataset
of 60 groups of questions (Bunescu and Huang,
2010b). Each group consists of a reference question
(e.g. Q0 above) that is associated with a partially or-
dered set of questions (e.g. Q1 to Q4 above). For
each reference questions, its corresponding partially
ordered set is created from questions in Yahoo! An-
swers and other online repositories that have a high
cosine similarity with the reference question. Out
of the 26 top categories in Yahoo! Answers, the 60
reference questions span a diverse set of categories.
Figure 1 lists the 20 categories covered, where each
category is shown with the number of corresponding
reference questions between parentheses.
</bodyText>
<table confidence="0.789733181818182">
Travel (10), Computers &amp; Internet (6),
Beauty &amp; Style (5), Entertainment &amp;
Music (5), Food &amp; Drink (5), Health (5),
Arts &amp; Humanities (3), Cars &amp;
Transportation (3), Consumer Electronics
(3), Pets (3), Family &amp; Relationships
(2), Science &amp; Mathematics (2),
Education &amp; Reference (1), Environment
(1), Local Businesses (1), Pregnancy &amp;
Parenting (1), Society &amp; Culture (1),
Sports (1), Yahoo! Products (1)
</table>
<figureCaption confidence="0.999209">
Figure 1: The 20 categories represented in the dataset.
</figureCaption>
<bodyText confidence="0.967771705882353">
Inside each group, the questions are manually an-
notated with a partial order relation, according to
their utility with respect to the reference question.
We use the notation (QZ &gt;- Qj|Qr) to encode the
fact that question QZ is more useful than question Qj
with respect to the reference question Qr. Similarly,
(QZ = Qj) will be used to express the fact that ques-
tions QZ and Qj are reformulations of each other
(the reformulation relation is independent of the ref-
erence question). The partial ordering among the
questions Q0 to Q4 above can therefore be expressed
concisely as follows: (Q0 = Q1), (Q1 &gt;- Q2|Q0),
(Q1 &gt;- Q3|Q0), (Q2 &gt;- Q4|Q0), (Q3 &gt;- Q4|Q0).
Note that we do not explicitly annotate the rela-
tion (Q1 &gt;- Q4|Q0), since it can be inferred based
on the transitivity of the more useful than relation:
(Q1 &gt;- Q2|Q0)n(Q2 &gt;- Q4|Q0) ==&gt; (Q1 &gt;- Q4|Q0).
</bodyText>
<page confidence="0.996863">
98
</page>
<table confidence="0.82334845">
REFERENCE QUESTION (Qr)
Q5 What’s a nice summer camp to go to in Florida?
PARAPHRASING QUESTIONS (P)
Q6 What camps are good for a vacation during the summer in FL?
Q7 What summer camps in FL do you recommend?
USEFUL QUESTIONS (U)
Q8 Does anyone know a good art summer camp to go to in FL?
Q9 Are there any good artsy camps for girls in FL?
Q10 What are some summer camps for like singing in Florida?
Q11 What is a good cooking summer camp in FL?
Q12 Do you know of any summer camps in Tampa, FL?
Q13 What is a good summer camp in Sarasota FL for a 12 year old?
Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?
Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?
Q16 Does anyone know any volleyball camps in Miramar, FL?
Q17 Does anyone know about any cool science camps in Miami?
Q18 What’s a good summer camp you’ve ever been to?
NEUTRAL QUESTIONS (N)
Q19 What’s a good summer camp in Canada?
Q20 What’s the summer like in Florida?
</table>
<tableCaption confidence="0.995971">
Table 1: A question group.
</tableCaption>
<bodyText confidence="0.999875142857143">
Also note that no relation is specified between Q2
and Q3, and similarly no relation can be inferred be-
tween these two questions. This reflects our belief
that, in the absence of any additional information re-
garding the user or the “turtle” referenced in Q0, we
cannot compare questions Q2 and Q3 in terms of
their usefulness with respect to Q0.
Table 1 shows another reference question Q5 from
our dataset, together with its annotated group of
questions Q6 to Q20. In order to make the anno-
tation process easier and reproducible, we have di-
vided it into two levels of annotation. During the
first annotation stage, each question group is parti-
tioned manually into 3 subgroups of questions:
</bodyText>
<listItem confidence="0.999725333333333">
• P is the set of paraphrasing questions.
• U is the set of useful questions.
• N is the set of neutral questions.
</listItem>
<bodyText confidence="0.9934899">
A question is deemed useful if its expected answer
may overlap in information content with the ex-
pected answer of the reference question. The ex-
pected answer of a neutral question, on the other
hand, should be irrelevant with respect to the ref-
erence question. Let Qr be the reference question,
Qp E P a paraphrasing question, Qu E U a useful
question, and Qn E N a neutral question. Then the
following relations are assumed to hold among these
questions:
</bodyText>
<listItem confidence="0.98813925">
1. (Qp ≻ Qu|Qr): a paraphrasing question is
more useful than a useful question.
2. (Qu ≻ Qn|Qr): a useful question is more use-
ful than a neutral question.
</listItem>
<bodyText confidence="0.995682333333333">
Note that as long as these relations hold between
the 3 types of questions, the names of the sub-
groups and their definitions are irrelevant with re-
spect to the implied set of more useful than rela-
tions, since only the implied ternary relations will
be used for training and evaluating question rank-
ing approaches. We also assume that, by tran-
sitivity, the following ternary relations also hold:
(Qp ≻ Qn|Qr), i.e. a paraphrasing question is
more useful than a neutral question. Furthermore, if
Qp,, Qp, E P are two paraphrasing questions, this
implies (QpM = Qp�l|Qr).
</bodyText>
<page confidence="0.992877">
99
</page>
<bodyText confidence="0.999952583333333">
For the vast majority of questions, the first annota-
tion stage is straightforward and non-controversial.
In the second annotation stage, we perform a finer
annotation of relations between questions in the
middle group U. Table 1 shows two such relations
(using indentation): (Q8 ≻ Q9|Q5) and (Q8 ≻
Q10|Q5). Question Q8 would have been a rephras-
ing of the reference question, were it not for the
noun “art” modifying the focus noun phrase “sum-
mer camp”. Therefore, the information content of
the answer to Q8 is strictly subsumed in the infor-
mation content associated with the answer to Q5.
Similarly, in Q9 the focus noun phrase is further spe-
cialized through the prepositional phrase “for girls”.
Therefore, (an answer to) Q9 is less useful to Q5
than (an answer to) Q8, i.e. (Q8 ≻ Q9|Q5). Fur-
thermore, the focus “art summer camp” in Q8 con-
ceptually subsumes the focus “summer camps for
singing” in Q10, therefore (Q8 ≻ Q10|Q5).
We call this dataset simple since most of the ref-
erence questions are shorter than the other questions
in their group. We have also created a complex ver-
sion of the same dataset, by selecting as the refer-
ence question in each group a longer question from
the same group. For example, if Q0 were a reference
question, it would be replaced with a more complex
question, such as Q2, or Q3. The annotation is re-
done to reflect the relative usefulness relations with
respect to the new reference questions. We believe
that the new complex dataset is closer to the actual
distribution of questions in community QA reposi-
tories: unanswered questions tend to be more spe-
cific (longer), whereas general questions (shorter)
are more likely to have been answered already. Each
dataset is annotated by two annotators, leading to
a total of 4 datasets: Simple1, Simple2, Complex1,
and Complex2.
Table 2 presents the following statistics on the two
types of datasets (Simple, Complex) for each anno-
tator (1, 2): the total number of paraphrasings (P),
the total number of useful questions (U), the total
number of neutral questions (N), the total number
of more useful than ordered pairs encoded in the
dataset, either explicitly or through transitivity, and
the Inter-Annotator Agreement (ITA). We compute
the ITA as the precision (P) and recall (R) with re-
spect to the more useful than ordered pairs encoded
in one annotation (Pairs1) relative to the ordered
</bodyText>
<table confidence="0.9993268">
Dataset P U N Pairs ITA
Simple1 164 775 594 11015 P: 76.6
Simple2 134 778 621 10436 R: 81.6
Complex1 103 766 664 10654 P: 71.3
Complex2 89 730 714 9979 R: 81.3
</table>
<tableCaption confidence="0.996561">
Table 2: Dataset statistics.
</tableCaption>
<equation confidence="0.956992333333333">
pairs encoded in the other annotation (Pairs2).
P = |Pairs1 n Pairs2 |R = |Pairs1 n Pairs2|
Pairs1 Pairs2
</equation>
<bodyText confidence="0.999690333333333">
The statistics in Table 2 indicate that the second
annotator was in general more conservative in tag-
ging questions as paraphrases or useful questions.
</bodyText>
<sectionHeader confidence="0.983538" genericHeader="method">
3 Unsupervised Methods for Question
Ranking
</sectionHeader>
<bodyText confidence="0.999843">
An ideal question ranking method would take an ar-
bitrary triplet of questions Qr, Qi and Qj as input,
and output an ordering between Qi and Qj with re-
spect to the reference question Qr, i.e. one of (Qi ≻
Qj|Qr), (Qi = Qj|Qr), or (Qj ≻ Qi|Qr). One ap-
proach is to design a usefulness function u(Qi, Qr)
that measures how useful question Qi is for the ref-
erence question Qr, and define the more useful than
(≻) relation as follows:
</bodyText>
<equation confidence="0.520711">
(Qi ≻ Qj|Qr) ⇔ u(Qi, Qr) &gt; u(Qj, Qr)
</equation>
<bodyText confidence="0.9999705">
If we define I(Q) to be the information need asso-
ciated with question Q, then u(Qi, Qr) could be de-
fined as a measure of the relative overlap between
I(Qi) and I(Qr). Unfortunately, the information
need is a concept that, in general, is defined only
intensionally and therefore it is difficult to measure.
For lack of an operational definition of the informa-
tion need, we will approximate u(Qi, Qr) directly
as a measure of the similarity between Qi and Qr.
The similarity between two questions can be seen as
a special case of text-to-text similarity, consequently
one possibility is to use a general text-to-text simi-
larity function such as cosine similarity in the vector
space model (Baeza-Yates and Ribeiro-Neto, 1999):
</bodyText>
<equation confidence="0.807878">
COs(Qi, Qr) = QTi Qr
IIQiIIIIQrII
</equation>
<page confidence="0.892254">
100
</page>
<bodyText confidence="0.99991025">
Here, Qi and Qr denote the corresponding tfxidf
vectors.
As a measure of question similarity, one major
drawback of cosine similarity is that it is oblivious
of the meanings of words in each question. This par-
ticular problem is illustrated by the three questions
below. Q22 and Q23 have the same cosine similar-
ity with Q21, they are therefore indistinguishable in
terms of their usefulness to the reference question
Q21, even though we expect Q22 to be more use-
ful than Q23 (a place that sells hydrangea often sells
other types of plants too, possibly including cacti).
</bodyText>
<equation confidence="0.885760666666667">
Q21 Where can I buy a hydrangea?
Q22 Where can I buy a cactus?
Q23 Where can I buy an iPad?
</equation>
<bodyText confidence="0.972419666666667">
To alleviate the lexical chasm, we can redefine
u(Qi, Qr) to be the similarity measure proposed by
(Mihalcea et al., 2006) as follows:
</bodyText>
<equation confidence="0.996793625">
E maxSim(w, Qr) * idf(w)
wE{Q;}
E idf(w)
wE{Q;}
E maxSim(w, Qi) * idf(w)
wE{Q,.}
E idf(w)
wE{Q,.}
</equation>
<bodyText confidence="0.999844928571429">
Since scaling factors are immaterial for ranking, we
have ignored the normalization constant contained
in the original measure. For each word w E Qi,
maxSim(w, Qr) computes the maximum semantic
similarity between w and any word wr E Qr. The
similarity scores are weighted by the correspond-
ing idf’s, and normalized. A similar score is com-
puted for each word w E Qr. The score computed
by maxSim depends on the actual function used
to compute the word-to-word semantic similarity.
In this paper, we evaluated four of the knowledge-
based measures explored in (Mihalcea et al., 2006):
wup (Wu and Palmer, 1994), res (Resnik, 1995), lin
(Lin, 1998), and jcn (Jiang and Conrath, 1997).
</bodyText>
<sectionHeader confidence="0.98413" genericHeader="method">
4 Supervised Learning for Question
Ranking
</sectionHeader>
<bodyText confidence="0.999901">
Cosine similarity, henceforth referred as cos, treats
questions as bags-of-words. The meta-measure pro-
posed in (Mihalcea et al., 2006), henceforth called
mcs, treats questions as bags-of-concepts. Both cos
and mcs ignore the syntactic relations between the
words in a question, and therefore may miss impor-
tant structural information. In the next three sec-
tions we describe a set of structural features that we
believe are relevant for judging question similarity.
These and other types of features will be integrated
in an SVM model for ranking, as described later in
Section 4.4.
</bodyText>
<subsectionHeader confidence="0.99422">
4.1 Matching the Focus Words
</subsectionHeader>
<bodyText confidence="0.999963625">
If we consider the question Q24 below as reference,
question Q26 will be deemed more useful than Q25
when using cos or mcs because of the higher rela-
tive lexical and conceptual overlap with Q24. How-
ever, this is contrary to the actual ordering (Q25 &gt;-
Q26|Q24), which reflects the fact that Q25, which
expects the same answer type as Q24, should be
deemed more useful than Q26, which has a differ-
ent answer type.
The analysis above shows the importance of us-
ing the answer type when computing the similar-
ity between two questions. However, instead of re-
lying exclusively on a predefined hierarchy of an-
swer types, we identify the question focus of a ques-
tion, defined as the set of maximal noun phrases in
the question that corefer with the expected answer
(Bunescu and Huang, 2010a). Focus nouns such as
movies and songs provide more discriminative in-
formation than general answer types such as prod-
ucts. We use answer types only for questions such
as Q27 or Q28 below that lack an explicit question
focus. In such cases, an artificial question focus is
created from the answer type (e.g. location for Q27,
or method for Q28).
</bodyText>
<figure confidence="0.534456">
mcs(Qi, Qr) =
+ Q24 What are some good thriller movies?
Q25 What are some thriller movies with happy end-
ing?
Q26 What are some good songs from a thriller
movie?
</figure>
<page confidence="0.844543">
101
</page>
<note confidence="0.529089">
Q27 Where can I buy a good coffee maker?
Q28 How do I make a pizza?
</note>
<bodyText confidence="0.99348625">
Let fi and fr be the focus words corresponding to
questions Qi and Qr. We introduce a focus feature
φf, and set its value to be equal with the similarity
between the focus words:
</bodyText>
<equation confidence="0.899633">
φf(Qi, Qr) = wsim(fi, fr) (1)
</equation>
<bodyText confidence="0.998422">
We use wsim to denote a generic word meaning sim-
ilarity measure (e.g. wup, res, lin or jcn). When
computing the focus feature, the non-focus word
“movie” in Q26 will not be compared with the fo-
cus word “movies” in Q24, and therefore Q26 will
have a lower value for this feature than Q25, i.e.
φf(Q26, Q24) &lt; φf(Q25, Q24).
</bodyText>
<subsectionHeader confidence="0.996452">
4.2 Matching the Main Verbs
</subsectionHeader>
<bodyText confidence="0.990444833333333">
In addition to the question focus, the main verb of
a question can also provide key information in es-
timating question-to-question similarity. We define
the main verb to be the content verb that is highest
in the dependency tree of the question, e.g. buy for
Q27, or make for Q28. If the question does not con-
tain a content verb, the main verb is defined to be the
highest verb in the dependency tree, as for example
are in Q24 to Q26. The utility of a question’s main
verb in judging its similarity to other questions can
be seen more clearly in the questions below, where
Q29 is the reference:
</bodyText>
<equation confidence="0.847403">
Q29 How can I transfer music from iTunes to my
iPod?
Q30 How can I upload music to my iPod?
Q31 How can I play music in iTunes?
</equation>
<bodyText confidence="0.999879285714286">
The fact that upload, as the main verb of Q30, is
more semantically related to transfer is essential in
deciding that (Q30 &gt;- Q31|Q29), i.e. Q30 is more
useful than Q31 to Q29.
Let vi and vr be the main verbs corresponding to
questions Qi and Qr. We introduce a main verb fea-
ture φv as follows:
</bodyText>
<equation confidence="0.862487">
φv(Qi, Qr) = wsim(vi, vr) (2)
</equation>
<bodyText confidence="0.987192">
If Q29 is considered as reference question, it is ex-
pected that the main verb feature for question Q30
will have a higher value than the main verb feature
for Q31, i.e. φf(Q31, Q29) &lt; φf(Q30, Q29).
</bodyText>
<figureCaption confidence="0.998671">
Figure 2: Matched dependency trees.
</figureCaption>
<subsectionHeader confidence="0.998447">
4.3 Matching the Dependency Trees
</subsectionHeader>
<bodyText confidence="0.99916392">
The question focus and the main verb are only two
of the nodes in the syntactic dependency tree of a
question. In general, all the words in a question are
important when judging its semantic similarity with
another question. We therefore propose a more gen-
eral feature that exploits the dependency structure
of the question and, in doing so, it also considers
all the words in the question, like cos and mcs. For
any given question we initially ignore the direction
of the dependency arcs and change the question de-
pendency tree to be rooted at the focus word, as il-
lustrated in Figure 2 for questions Q5 and Q9. In-
terrogative patterns such as “What is” or “Are there
any” are automatically eliminated from the depen-
dency trees. We define the dependency tree similar-
ity between two questions Qi and Qr to be a func-
tion of similarities wsim(vi, vr) computed between
aligned nodes vi E Qi and vr E Qr. The nodes
of two dependency trees are aligned through a func-
tion MaxMatch(ui.C, ur.C) that takes two sets of
children nodes as arguments, one from Qi and one
from Qr, and finds the maximum weighted bipartite
matching between ui.C and ur.C. Given two chil-
dren nodes vi E ui.C and vr E ur.C, the weight of
a potential matching between vi and vr is defined
</bodyText>
<page confidence="0.997822">
102
</page>
<bodyText confidence="0.9996575">
simply as wsim(vi, vr). MaxMatch(ui.C, ur.C) is
furthermore constrained to match only nodes that
have compatible part-of-speech tags (e.g. nouns
are matched to nouns, verbs are matched to verbs),
and children nodes that have the same head-modifier
relationship with their parents (i.e. they are both
heads, or they are both dependents of their par-
ents). Table 3 shows the recursive algorithm used
</bodyText>
<figure confidence="0.828494571428571">
TreeMatch(ui, ur)
[In]: Two dependency tree nodes ui, ur.
[Out]: A set of node pairs M.
1. set M ← {(ui, ur)}
2. for each (vi, vr) ∈ MaxMatch(ui.C, ur.C):
3. set M ← M ∪ TreeMatch(vi, vr)
4. return M
</figure>
<tableCaption confidence="0.977641">
Table 3: Dependency Tree Matching.
</tableCaption>
<bodyText confidence="0.9995580625">
for finding a matching between two question depen-
dency trees rooted at the focus words. The initial
arguments of the algorithm are the two focus words
ui = fi and ur = fr. Thus, the pair (fi, fr) is
the first pair of nodes to be added to the matching
M in step 1. In the next step, we compute the maxi-
mum weighted matching between the children nodes
ui.C and ur.C, and recursively call the matching al-
gorithm on pairs of matched nodes (vi, vr) from M.
The algorithm stops when MaxMatch returns an
empty matching, which may happen when reach-
ing leaf nodes, or when no pair of children nodes
has compatible POS tags, or child-parent dependen-
cies. Figure 2 shows the results of applying the
tree matching algorithm on questions Q5 and Q9.
Matched nodes share the same index and are shown
in circles, whereas unmatched nodes are shown in
italics.
We introduce a new feature φt(Qi, Qr) whose
value is defined as the dependency tree similarity
between questions Qi and Qr. Once the optimum
matching M(Qi, Qr) between dependency trees has
been found, φt(Qi, Qr) is computed as the nor-
malized sum of the similarities between pairs of
matched nodes vi and vr, as shown in Equations 3
and 4 below. When computing the similarity be-
tween two matched nodes, we factor in the similar-
ities between corresponding pairs of words on the
paths fi ❀ vi, fr ❀ vr between the focus words fi,
fr and the nodes vi, vr, as shown in Equation 5. This
has the effect of reducing the importance of words
that are farther away from the focus word in the de-
</bodyText>
<equation confidence="0.954164666666667">
pendency tree.
sim(Qi, Qr)
φt(Qi, Qr) = (3)
V/sim(Qi, Qi)sim(Qr, Qr)
sim(Qi, Qr) = E sim(fi ❀ vi, fr ❀ vr) (4)
(vi,vr)EM(Qi,Qr)
n
sim(u1 ❀ un, v1 ❀ vn) = wsim(ui, vi) (5)
i=1
</equation>
<bodyText confidence="0.873732">
If the word similarity function is normalized and
defined to return 1 for identical words, the nor-
malizer in Equation 3 becomes equivalent with
V/
|Qi||Qr|. Thus, words that are left unmatched im-
plicitly decrease the dependency tree similarity.
</bodyText>
<subsectionHeader confidence="0.987568">
4.4 An SVM Model for Ranking Questions
</subsectionHeader>
<bodyText confidence="0.999085">
We consider learning a usefulness function
u(Qi, Qr) of the following general, linear form:
</bodyText>
<equation confidence="0.934852">
u(Qi, Qr) = WT φ(Qi, Qr) (6)
</equation>
<bodyText confidence="0.7249175">
The vector φ(Qi, Qr) is defined to contain the fol-
lowing generic features:
</bodyText>
<listItem confidence="0.9832163">
1. φf(Qi, Qr) = the semantic similarity between
focus words, as described in Section 4.1.
2. φv(Qi, Qr) = the semantic similarity between
main verbs, as described in Section 4.2.
3. φt(Qi, Qr) = the semantic similarity between
the dependency trees, as described in Sec-
tion 4.3.
4. cos(Qi, Qr) = the cosine similarity between the
two questions, as described in Section 3.
5. mcs(Qi, Qr) = the bag-of-concepts similarity
</listItem>
<bodyText confidence="0.897529857142857">
between the two questions, as described in Sec-
tion 3.
Each of the generic features φf, φv, φt, and mcs cor-
responds to four actual features, one for each possi-
ble choice of the word similarity function wsim (i.e.
wup, res, lin or jcn). An additional pair of features
is targeted at questions containing locations:
</bodyText>
<page confidence="0.997165">
103
</page>
<listItem confidence="0.9662846">
6. φl(Qi, Qr) = 1 if both questions contain loca-
tions, 0 otherwise.
7. φd(Qi, Qr) = the normalized geographical dis-
tance between the locations in Qi and Qr, 0 if
φl(Qi, Qr) = 0.
</listItem>
<bodyText confidence="0.9988281">
Given two location names, we first find their latitude
and longitude using Google Maps, and then com-
pute the spherical distance between them using the
haversine formula.
The corresponding parameters w will be trained
on pairs from one of the partially ordered datasets
described in Section 2. We use the kernel version of
the large-margin ranking approach from (Joachims,
2002) which solves the optimization problem in Fig-
ure 3 below. The aim of this formulation is to find a
</bodyText>
<equation confidence="0.927282">
minimize:
J(w,ξ) = 12I IwI I2 + C E ξrij
subject to:
wTφ(Qi, Qr) − wT φ(Qj, Qr) &gt; 1 − ξrij
ξrij &gt; 0
VQr, Qi, Qj E D, (Qi &gt;- Qj|Qr)
</equation>
<figureCaption confidence="0.996633">
Figure 3: SVM ranking optimization problem.
</figureCaption>
<bodyText confidence="0.991784857142857">
weight vector w such that 1) the number of ranking
constraints u(Qi, Qr) &gt; u(Qj, Qr) from the train-
ing data D that are violated is minimized, and 2) the
ranking function u(Qi, Qr) generalizes well beyond
the training data. The learned w is a linear combina-
tion of the feature vectors φ(Qi, Qr), which makes
it possible to use kernels.
</bodyText>
<sectionHeader confidence="0.992859" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999313625">
We use the four question ranking datasets described
in Section 2 to evaluate the three similarity mea-
sures cos, mcs, and φt, as well as the SVM rank-
ing model. We report one set of results for each of
the four word similarity measures wup, res, lin or
jcn. Each question similarity measure is evaluated
in terms of its accuracy on the set of ordered pairs,
and the performance is averaged between the two
annotators for the Simple and Complex datasets. If
(Qi &gt;- Qj|Qr) is a relation specified in the anno-
tation, we consider the tuple (Qi, Qj, Qr) correctly
classified if and only if u(Qi, Qr) &gt; u(Qj, Qr),
where u is the question similarity measure. We used
the SVMlight 2 implementation of ranking SVMs,
with a cubic kernel and the standard parameters. The
SVM ranking model was trained and tested using
10-fold cross-validation, and the overall accuracy
was computed by averaging over the 10 folds.
We used the NLTK 3 implementation of the four
similarity measures wup, res, lin or jcn. The idf val-
ues for each word were computed from frequency
counts over the entire Wikipedia. For each ques-
tion, the focus is identified automatically by an SVM
tagger trained on a separate corpus of 2,000 ques-
tions manually annotated with focus information
(Bunescu and Huang, 2010a). The SVM tagger
uses a combination of lexico-syntactic features and
a quadratic kernel to achieve a 93.5% accuracy in
a 10-fold cross validation evaluation on the 2,000
questions. The head-modifier dependencies were
derived automatically from the syntactic parse tree
using the head finding rules from (Collins, 1999).
The syntactic tree is obtained using Spear 4, a syn-
tactic parser which comes pre-trained on an addi-
tional treebank of questions. The main verb of
a question is identified deterministically using a
breadth first traversal of the dependency tree.
The overall accuracy results presented in Table 4
show that the SVM ranking model obtains by far the
best performance on both datasets, a substantial 10%
higher than cos, which is the best performing unsu-
pervised method. The random baseline – assigning
a random similarity value to each pair of questions –
results in 50% accuracy. Even though its use of word
senses was expected to lead to superior results, mcs
does not perform better than cos on this dataset. Our
implementation of mcs did however perform better
than cos on the Microsoft paraphrase corpus (Dolan
et al., 2004). One possible reason for this behav-
ior is that mcs seems to be less resilient than cos
to differences in question length. Whereas the Mi-
crosoft paraphrase corpus was specifically designed
such that “the length of the shorter of the two sen-
tences, in words, is at least 66% that of the longer”
(Dolan and Brockett, 2005), the question ranking
datasets place no constraints on the lengths of the
</bodyText>
<footnote confidence="0.999779">
2svmlight.joachims.org
3www.nltk.org
4www.surdeanu.name/mihai/spear
</footnote>
<page confidence="0.987634">
104
</page>
<table confidence="0.9995404">
Question cos wup res lin jcn SVM
Dataset
mcs Ot mcs Ot mcs Ot mcs Ot
Simple 73.7 69.1 69.4 71.3 71.8 70.8 69.8 71.9 71.7 82.1
Complex 72.6 64.1 69.6 66.0 71.5 66.9 69.1 69.4 71.0 82.5
</table>
<tableCaption confidence="0.972977">
Table 4: Pairwise accuracy results.
</tableCaption>
<table confidence="0.999723333333333">
Dataset all −Of −O„ −Ot −Ol,d −cos −mcs −Of,t
Simple 82.1 79.3 82.0 80.2 81.5 80.3 81.4 78.5
Complex 82.5 81.3 81.3 78.7 81.8 79.2 81.8 77.4
</table>
<tableCaption confidence="0.999788">
Table 5: Ablation results.
</tableCaption>
<bodyText confidence="0.99992294117647">
questions. However, even though by themselves the
meaning aware mcs and the structure-and-meaning
aware Ot do not outperform the bag-of-words cos,
they do help in increasing the performance of the
SVM ranking model, as can be inferred from the cor-
responding columns in Table 5. The table shows the
results of ablation experiments in which all but one
type of features are used. The results indicate that
all types of features are useful, with significant con-
tributions being brought especially by cos and the
focus related features Of,t.
The measures investigated in this paper are all
compositional and reduce the similarity computa-
tions to word level. The following question patterns
illustrate the need to design more complex similarity
measures that take into account the context of every
word in the question:
</bodyText>
<equation confidence="0.505356">
P1 Where can I find a job around (City )?
P2 What are some famous people from (City)?
P3 What is the population of (City)?
</equation>
<bodyText confidence="0.924164411764706">
Below are three instantiations of the first question
pattern:
Q32 Where can I find a job around Anaheim, CA?
Q33 Where can I find a job around Los Angeles?
Q34 Where can I find a job around Vista, CA?
If we take Q32 as reference question, the fact that
the distance between Los Angeles and Anaheim is
smaller than the distance between Vista and Ana-
heim leads the ranking system to rank Q33 as more
useful than Q34 with respect to Q32, which is the
expected result. The preposition “around” from the
city context in the first pattern is a good indica-
tor that proximity relations are relevant in this case.
When the same three cities are used for instantiating
the other two patterns, it can be seen that the prox-
imity relations are no longer as relevant for judging
the relative usefulness of questions.
</bodyText>
<sectionHeader confidence="0.999825" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.9998996">
We plan to integrate context dependent word sim-
ilarity measures into a more robust question util-
ity function. We also plan to make the dependency
tree matching more flexible in order to account for
paraphrase patterns that may differ in their syntactic
structure. The questions that are posted on commu-
nity QA sites often contain spelling or grammatical
errors. Consequently, we will work on interfacing
the question ranking system with a separate module
aimed at fixing orthographic and grammatical errors.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999934769230769">
The question rephrasing subtask has spawned a di-
verse set of approaches. (Hermjakob et al., 2002)
derive a set of phrasal patterns for question reformu-
lation by generalizing surface patterns acquired au-
tomatically from a large corpus of web documents.
The focus of the work in (Tomuro, 2003) is on deriv-
ing reformulation patterns for the interrogative part
of a question. In (Jeon et al., 2005), word trans-
lation probabilities are trained on pairs of seman-
tically similar questions that are automatically ex-
tracted from an FAQ archive, and then used in a
language model that retrieves question reformula-
tions. (Jijkoun and de Rijke, 2005) describe an FAQ
</bodyText>
<page confidence="0.997619">
105
</page>
<bodyText confidence="0.999983583333333">
question retrieval system in which weighted com-
binations of similarity functions corresponding to
questions, existing answers, FAQ titles and pages
are computed using a vector space model. (Zhao et
al., 2007) exploit the Encarta logs to automatically
extract clusters containing question paraphrases and
further train a perceptron to recognize question para-
phrases inside each cluster based on a combination
of lexical, syntactic and semantic similarity features.
More recently, (Bernhard and Gurevych, 2008) eval-
uated various string similarity measures and vec-
tor space based similarity measures on the task of
retrieving question paraphrases from the WikiAn-
swers repository. The aim of the question search
task presented in (Duan et al., 2008) is to return
questions that are semantically equivalent or close
to the queried question, and is therefore similar to
our question ranking task. Their approach is eval-
uated on a dataset in which questions are catego-
rized either as relevant or irrelevant. Our formula-
tion of question ranking is more general, and in par-
ticular subsumes the annotation of binary question
categories such as relevant vs. irrelevant, or para-
phrases vs. non-paraphrases. Moreover, we are able
to exploit the annotated utility relations as super-
vision in a learning for ranking approach, whereas
(Duan et al., 2008) use the annotated dataset to tune
the 3 parameters of a mostly unsupervised approach.
The question ranking task was first formulated in
(Bunescu and Huang, 2010b), where an initial ver-
sion of the dataset was also described. In this pa-
per, we introduce 4 versions of the dataset, a more
general meaning and structure aware similarity mea-
sure, and a supervised model for ranking that sub-
stantially outperforms the previously proposed util-
ity measures.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999736">
We presented a supervised learning approach to the
question ranking task in which previously known
questions are ordered based on their relative util-
ity with respect to a new, reference question. We
created four versions of a dataset of 60 groups of
questions 5, each annotated with a partial order rela-
tion reflecting the relative utility of questions inside
each group. An SVM ranking model was trained
</bodyText>
<footnote confidence="0.667366">
5The dataset will be made publicly available.
</footnote>
<bodyText confidence="0.999923666666667">
on the dataset and evaluated together with a set of
simpler, unsupervised question-to-question similar-
ity models. Experimental results demonstrate the
importance of using structure and meaning aware
features when computing the relative usefulness of
questions.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998431">
We would like to thank the anonymous reviewers for
their insightful comments.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876846153846">
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press, New
York.
Delphine Bernhard and Iryna Gurevych. 2008. Answer-
ing learners’ questions by retrieving question para-
phrases from social Q&amp;A sites. In EANL ’08: Pro-
ceedings of the Third Workshop on Innovative Use of
NLP for Building Educational Applications, pages 44–
52, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Razvan Bunescu and Yunfeng Huang. 2010a. Towards a
general model of answer typing: Question focus iden-
tification. In Proceedings of The 11th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing 2010), RCS Volume,
pages 231–242.
Razvan Bunescu and Yunfeng Huang. 2010b. A utility-
driven approach to question ranking in social QA.
In Proceedings of The 23rd International Conference
on Computational Linguistics (COLING 2010), pages
125–133.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing (IWP2005), pages 9–16.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting assively parallel news sources. In Proceed-
ings of The 20th International Conference on Compu-
tational Linguistics (COLING’04), page 350.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying question
topic and question focus. In Proceedings of ACL-08:
HLT, pages 156–164, Columbus, Ohio, June.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
</reference>
<page confidence="0.936321">
106
</page>
<reference confidence="0.999248920634921">
resource and web exploitation for question answering.
In Proceedings of TREC-2002.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management (CIKM’05), pages 84–90, New York, NY,
USA. ACM.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, pages 19–33.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
Web. In Proceedings of the 14th ACM international
conference on Information and knowledge manage-
ment (CIKM’05), pages 76–83, New York, NY, USA.
ACM.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD-2002), Ed-
monton, Canada.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning (ICML
’98), pages 296–304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the 21st national conference on Artificial intelligence
(AAAI’06), pages 775–780. AAAI Press.
Dan I. Moldovan, Marius Pasca, Sanda M. Harabagiu,
and Mihai Surdeanu. 2002. Performance issues and
error analysis in an open-domain question answering
system. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
33–40, Philadelphia, PA, July.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91–231.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In IJCAI’95:
Proceedings of the 14th international joint conference
on Artificial intelligence, pages 448–453, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Noriko Tomuro. 2003. Interrogative reformulation pat-
terns and acquisition of question paraphrases. In Pro-
ceedings of the Second International Workshop on
Paraphrasing, pages 33–40, Morristown, NJ, USA.
Association for Computational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics, pages 133–138, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learn-
ing question paraphrases for QA from Encarta logs. In
Proceedings of the 20th international joint conference
on Artifical intelligence (IJCAI’07), pages 1795–1800,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
</reference>
<page confidence="0.998699">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697274">
<title confidence="0.999979">Learning the Relative Usefulness of Questions in Community QA</title>
<author confidence="0.998802">Razvan Bunescu</author>
<affiliation confidence="0.951278">School of EECS Ohio</affiliation>
<address confidence="0.983375">Athens, OH 43201,</address>
<email confidence="0.999796">bunescu@ohio.edu</email>
<author confidence="0.820477">Yunfeng</author>
<affiliation confidence="0.998758">School of Ohio University</affiliation>
<address confidence="0.999544">Athens, OH 43201, USA</address>
<email confidence="0.998035">yh324906@ohio.edu</email>
<abstract confidence="0.996918923076923">We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new, unanswered reference question. The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group. Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>ACM Press,</publisher>
<location>New York.</location>
<contexts>
<context position="14864" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="2503" endWordPosition="2506">could be defined as a measure of the relative overlap between I(Qi) and I(Qr). Unfortunately, the information need is a concept that, in general, is defined only intensionally and therefore it is difficult to measure. For lack of an operational definition of the information need, we will approximate u(Qi, Qr) directly as a measure of the similarity between Qi and Qr. The similarity between two questions can be seen as a special case of text-to-text similarity, consequently one possibility is to use a general text-to-text similarity function such as cosine similarity in the vector space model (Baeza-Yates and Ribeiro-Neto, 1999): COs(Qi, Qr) = QTi Qr IIQiIIIIQrII 100 Here, Qi and Qr denote the corresponding tfxidf vectors. As a measure of question similarity, one major drawback of cosine similarity is that it is oblivious of the meanings of words in each question. This particular problem is illustrated by the three questions below. Q22 and Q23 have the same cosine similarity with Q21, they are therefore indistinguishable in terms of their usefulness to the reference question Q21, even though we expect Q22 to be more useful than Q23 (a place that sells hydrangea often sells other types of plants too, possibly includin</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>Answering learners’ questions by retrieving question paraphrases from social Q&amp;A sites.</title>
<date>2008</date>
<booktitle>In EANL ’08: Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>44--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="33083" citStr="Bernhard and Gurevych, 2008" startWordPosition="5682" endWordPosition="5685"> then used in a language model that retrieves question reformulations. (Jijkoun and de Rijke, 2005) describe an FAQ 105 question retrieval system in which weighted combinations of similarity functions corresponding to questions, existing answers, FAQ titles and pages are computed using a vector space model. (Zhao et al., 2007) exploit the Encarta logs to automatically extract clusters containing question paraphrases and further train a perceptron to recognize question paraphrases inside each cluster based on a combination of lexical, syntactic and semantic similarity features. More recently, (Bernhard and Gurevych, 2008) evaluated various string similarity measures and vector space based similarity measures on the task of retrieving question paraphrases from the WikiAnswers repository. The aim of the question search task presented in (Duan et al., 2008) is to return questions that are semantically equivalent or close to the queried question, and is therefore similar to our question ranking task. Their approach is evaluated on a dataset in which questions are categorized either as relevant or irrelevant. Our formulation of question ranking is more general, and in particular subsumes the annotation of binary qu</context>
</contexts>
<marker>Bernhard, Gurevych, 2008</marker>
<rawString>Delphine Bernhard and Iryna Gurevych. 2008. Answering learners’ questions by retrieving question paraphrases from social Q&amp;A sites. In EANL ’08: Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications, pages 44– 52, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Yunfeng Huang</author>
</authors>
<title>Towards a general model of answer typing: Question focus identification.</title>
<date>2010</date>
<booktitle>In Proceedings of The 11th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2010), RCS Volume,</booktitle>
<pages>231--242</pages>
<contexts>
<context position="5858" citStr="Bunescu and Huang, 2010" startWordPosition="931" endWordPosition="934">the question ranking task is the expectation that the user who submits a reference question will find the answers of the highly ranked questions to be more useful than the answers associated with the lower ranked questions. For the reference question Q0 above, the learned ranking model is expected to produce a partial order in which Q1 is ranked higher than Q2, Q3 and Q4, whereas Q2 and Q3 are ranked higher than Q4. 2 Partially Ordered Datasets for Question Ranking In order to enable the evaluation of question ranking approaches, we have previously created a dataset of 60 groups of questions (Bunescu and Huang, 2010b). Each group consists of a reference question (e.g. Q0 above) that is associated with a partially ordered set of questions (e.g. Q1 to Q4 above). For each reference questions, its corresponding partially ordered set is created from questions in Yahoo! Answers and other online repositories that have a high cosine similarity with the reference question. Out of the 26 top categories in Yahoo! Answers, the 60 reference questions span a diverse set of categories. Figure 1 lists the 20 categories covered, where each category is shown with the number of corresponding reference questions between par</context>
<context position="17914" citStr="Bunescu and Huang, 2010" startWordPosition="3024" endWordPosition="3027">cal and conceptual overlap with Q24. However, this is contrary to the actual ordering (Q25 &gt;- Q26|Q24), which reflects the fact that Q25, which expects the same answer type as Q24, should be deemed more useful than Q26, which has a different answer type. The analysis above shows the importance of using the answer type when computing the similarity between two questions. However, instead of relying exclusively on a predefined hierarchy of answer types, we identify the question focus of a question, defined as the set of maximal noun phrases in the question that corefer with the expected answer (Bunescu and Huang, 2010a). Focus nouns such as movies and songs provide more discriminative information than general answer types such as products. We use answer types only for questions such as Q27 or Q28 below that lack an explicit question focus. In such cases, an artificial question focus is created from the answer type (e.g. location for Q27, or method for Q28). mcs(Qi, Qr) = + Q24 What are some good thriller movies? Q25 What are some thriller movies with happy ending? Q26 What are some good songs from a thriller movie? 101 Q27 Where can I buy a good coffee maker? Q28 How do I make a pizza? Let fi and fr be the</context>
<context position="27636" citStr="Bunescu and Huang, 2010" startWordPosition="4781" endWordPosition="4784">re. We used the SVMlight 2 implementation of ranking SVMs, with a cubic kernel and the standard parameters. The SVM ranking model was trained and tested using 10-fold cross-validation, and the overall accuracy was computed by averaging over the 10 folds. We used the NLTK 3 implementation of the four similarity measures wup, res, lin or jcn. The idf values for each word were computed from frequency counts over the entire Wikipedia. For each question, the focus is identified automatically by an SVM tagger trained on a separate corpus of 2,000 questions manually annotated with focus information (Bunescu and Huang, 2010a). The SVM tagger uses a combination of lexico-syntactic features and a quadratic kernel to achieve a 93.5% accuracy in a 10-fold cross validation evaluation on the 2,000 questions. The head-modifier dependencies were derived automatically from the syntactic parse tree using the head finding rules from (Collins, 1999). The syntactic tree is obtained using Spear 4, a syntactic parser which comes pre-trained on an additional treebank of questions. The main verb of a question is identified deterministically using a breadth first traversal of the dependency tree. The overall accuracy results pres</context>
<context position="34074" citStr="Bunescu and Huang, 2010" startWordPosition="5842" endWordPosition="5845"> ranking task. Their approach is evaluated on a dataset in which questions are categorized either as relevant or irrelevant. Our formulation of question ranking is more general, and in particular subsumes the annotation of binary question categories such as relevant vs. irrelevant, or paraphrases vs. non-paraphrases. Moreover, we are able to exploit the annotated utility relations as supervision in a learning for ranking approach, whereas (Duan et al., 2008) use the annotated dataset to tune the 3 parameters of a mostly unsupervised approach. The question ranking task was first formulated in (Bunescu and Huang, 2010b), where an initial version of the dataset was also described. In this paper, we introduce 4 versions of the dataset, a more general meaning and structure aware similarity measure, and a supervised model for ranking that substantially outperforms the previously proposed utility measures. 8 Conclusion We presented a supervised learning approach to the question ranking task in which previously known questions are ordered based on their relative utility with respect to a new, reference question. We created four versions of a dataset of 60 groups of questions 5, each annotated with a partial orde</context>
</contexts>
<marker>Bunescu, Huang, 2010</marker>
<rawString>Razvan Bunescu and Yunfeng Huang. 2010a. Towards a general model of answer typing: Question focus identification. In Proceedings of The 11th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2010), RCS Volume, pages 231–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Yunfeng Huang</author>
</authors>
<title>A utilitydriven approach to question ranking in social QA.</title>
<date>2010</date>
<booktitle>In Proceedings of The 23rd International Conference on Computational Linguistics (COLING</booktitle>
<pages>125--133</pages>
<contexts>
<context position="5858" citStr="Bunescu and Huang, 2010" startWordPosition="931" endWordPosition="934">the question ranking task is the expectation that the user who submits a reference question will find the answers of the highly ranked questions to be more useful than the answers associated with the lower ranked questions. For the reference question Q0 above, the learned ranking model is expected to produce a partial order in which Q1 is ranked higher than Q2, Q3 and Q4, whereas Q2 and Q3 are ranked higher than Q4. 2 Partially Ordered Datasets for Question Ranking In order to enable the evaluation of question ranking approaches, we have previously created a dataset of 60 groups of questions (Bunescu and Huang, 2010b). Each group consists of a reference question (e.g. Q0 above) that is associated with a partially ordered set of questions (e.g. Q1 to Q4 above). For each reference questions, its corresponding partially ordered set is created from questions in Yahoo! Answers and other online repositories that have a high cosine similarity with the reference question. Out of the 26 top categories in Yahoo! Answers, the 60 reference questions span a diverse set of categories. Figure 1 lists the 20 categories covered, where each category is shown with the number of corresponding reference questions between par</context>
<context position="17914" citStr="Bunescu and Huang, 2010" startWordPosition="3024" endWordPosition="3027">cal and conceptual overlap with Q24. However, this is contrary to the actual ordering (Q25 &gt;- Q26|Q24), which reflects the fact that Q25, which expects the same answer type as Q24, should be deemed more useful than Q26, which has a different answer type. The analysis above shows the importance of using the answer type when computing the similarity between two questions. However, instead of relying exclusively on a predefined hierarchy of answer types, we identify the question focus of a question, defined as the set of maximal noun phrases in the question that corefer with the expected answer (Bunescu and Huang, 2010a). Focus nouns such as movies and songs provide more discriminative information than general answer types such as products. We use answer types only for questions such as Q27 or Q28 below that lack an explicit question focus. In such cases, an artificial question focus is created from the answer type (e.g. location for Q27, or method for Q28). mcs(Qi, Qr) = + Q24 What are some good thriller movies? Q25 What are some thriller movies with happy ending? Q26 What are some good songs from a thriller movie? 101 Q27 Where can I buy a good coffee maker? Q28 How do I make a pizza? Let fi and fr be the</context>
<context position="27636" citStr="Bunescu and Huang, 2010" startWordPosition="4781" endWordPosition="4784">re. We used the SVMlight 2 implementation of ranking SVMs, with a cubic kernel and the standard parameters. The SVM ranking model was trained and tested using 10-fold cross-validation, and the overall accuracy was computed by averaging over the 10 folds. We used the NLTK 3 implementation of the four similarity measures wup, res, lin or jcn. The idf values for each word were computed from frequency counts over the entire Wikipedia. For each question, the focus is identified automatically by an SVM tagger trained on a separate corpus of 2,000 questions manually annotated with focus information (Bunescu and Huang, 2010a). The SVM tagger uses a combination of lexico-syntactic features and a quadratic kernel to achieve a 93.5% accuracy in a 10-fold cross validation evaluation on the 2,000 questions. The head-modifier dependencies were derived automatically from the syntactic parse tree using the head finding rules from (Collins, 1999). The syntactic tree is obtained using Spear 4, a syntactic parser which comes pre-trained on an additional treebank of questions. The main verb of a question is identified deterministically using a breadth first traversal of the dependency tree. The overall accuracy results pres</context>
<context position="34074" citStr="Bunescu and Huang, 2010" startWordPosition="5842" endWordPosition="5845"> ranking task. Their approach is evaluated on a dataset in which questions are categorized either as relevant or irrelevant. Our formulation of question ranking is more general, and in particular subsumes the annotation of binary question categories such as relevant vs. irrelevant, or paraphrases vs. non-paraphrases. Moreover, we are able to exploit the annotated utility relations as supervision in a learning for ranking approach, whereas (Duan et al., 2008) use the annotated dataset to tune the 3 parameters of a mostly unsupervised approach. The question ranking task was first formulated in (Bunescu and Huang, 2010b), where an initial version of the dataset was also described. In this paper, we introduce 4 versions of the dataset, a more general meaning and structure aware similarity measure, and a supervised model for ranking that substantially outperforms the previously proposed utility measures. 8 Conclusion We presented a supervised learning approach to the question ranking task in which previously known questions are ordered based on their relative utility with respect to a new, reference question. We created four versions of a dataset of 60 groups of questions 5, each annotated with a partial orde</context>
</contexts>
<marker>Bunescu, Huang, 2010</marker>
<rawString>Razvan Bunescu and Yunfeng Huang. 2010b. A utilitydriven approach to question ranking in social QA. In Proceedings of The 23rd International Conference on Computational Linguistics (COLING 2010), pages 125–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="27956" citStr="Collins, 1999" startWordPosition="4830" endWordPosition="4831">, lin or jcn. The idf values for each word were computed from frequency counts over the entire Wikipedia. For each question, the focus is identified automatically by an SVM tagger trained on a separate corpus of 2,000 questions manually annotated with focus information (Bunescu and Huang, 2010a). The SVM tagger uses a combination of lexico-syntactic features and a quadratic kernel to achieve a 93.5% accuracy in a 10-fold cross validation evaluation on the 2,000 questions. The head-modifier dependencies were derived automatically from the syntactic parse tree using the head finding rules from (Collins, 1999). The syntactic tree is obtained using Spear 4, a syntactic parser which comes pre-trained on an additional treebank of questions. The main verb of a question is identified deterministically using a breadth first traversal of the dependency tree. The overall accuracy results presented in Table 4 show that the SVM ranking model obtains by far the best performance on both datasets, a substantial 10% higher than cos, which is the best performing unsupervised method. The random baseline – assigning a random similarity value to each pair of questions – results in 50% accuracy. Even though its use o</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="29103" citStr="Dolan and Brockett, 2005" startWordPosition="5024" endWordPosition="5027">lue to each pair of questions – results in 50% accuracy. Even though its use of word senses was expected to lead to superior results, mcs does not perform better than cos on this dataset. Our implementation of mcs did however perform better than cos on the Microsoft paraphrase corpus (Dolan et al., 2004). One possible reason for this behavior is that mcs seems to be less resilient than cos to differences in question length. Whereas the Microsoft paraphrase corpus was specifically designed such that “the length of the shorter of the two sentences, in words, is at least 66% that of the longer” (Dolan and Brockett, 2005), the question ranking datasets place no constraints on the lengths of the 2svmlight.joachims.org 3www.nltk.org 4www.surdeanu.name/mihai/spear 104 Question cos wup res lin jcn SVM Dataset mcs Ot mcs Ot mcs Ot mcs Ot Simple 73.7 69.1 69.4 71.3 71.8 70.8 69.8 71.9 71.7 82.1 Complex 72.6 64.1 69.6 66.0 71.5 66.9 69.1 69.4 71.0 82.5 Table 4: Pairwise accuracy results. Dataset all −Of −O„ −Ot −Ol,d −cos −mcs −Of,t Simple 82.1 79.3 82.0 80.2 81.5 80.3 81.4 78.5 Complex 82.5 81.3 81.3 78.7 81.8 79.2 81.8 77.4 Table 5: Ablation results. questions. However, even though by themselves the meaning aware m</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting assively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of The 20th International Conference on Computational Linguistics (COLING’04),</booktitle>
<pages>350</pages>
<contexts>
<context position="28783" citStr="Dolan et al., 2004" startWordPosition="4967" endWordPosition="4970">irst traversal of the dependency tree. The overall accuracy results presented in Table 4 show that the SVM ranking model obtains by far the best performance on both datasets, a substantial 10% higher than cos, which is the best performing unsupervised method. The random baseline – assigning a random similarity value to each pair of questions – results in 50% accuracy. Even though its use of word senses was expected to lead to superior results, mcs does not perform better than cos on this dataset. Our implementation of mcs did however perform better than cos on the Microsoft paraphrase corpus (Dolan et al., 2004). One possible reason for this behavior is that mcs seems to be less resilient than cos to differences in question length. Whereas the Microsoft paraphrase corpus was specifically designed such that “the length of the shorter of the two sentences, in words, is at least 66% that of the longer” (Dolan and Brockett, 2005), the question ranking datasets place no constraints on the lengths of the 2svmlight.joachims.org 3www.nltk.org 4www.surdeanu.name/mihai/spear 104 Question cos wup res lin jcn SVM Dataset mcs Ot mcs Ot mcs Ot mcs Ot Simple 73.7 69.1 69.4 71.3 71.8 70.8 69.8 71.9 71.7 82.1 Complex</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting assively parallel news sources. In Proceedings of The 20th International Conference on Computational Linguistics (COLING’04), page 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yong Yu</author>
</authors>
<title>Searching questions by identifying question topic and question focus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>156--164</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="33320" citStr="Duan et al., 2008" startWordPosition="5720" endWordPosition="5723">FAQ titles and pages are computed using a vector space model. (Zhao et al., 2007) exploit the Encarta logs to automatically extract clusters containing question paraphrases and further train a perceptron to recognize question paraphrases inside each cluster based on a combination of lexical, syntactic and semantic similarity features. More recently, (Bernhard and Gurevych, 2008) evaluated various string similarity measures and vector space based similarity measures on the task of retrieving question paraphrases from the WikiAnswers repository. The aim of the question search task presented in (Duan et al., 2008) is to return questions that are semantically equivalent or close to the queried question, and is therefore similar to our question ranking task. Their approach is evaluated on a dataset in which questions are categorized either as relevant or irrelevant. Our formulation of question ranking is more general, and in particular subsumes the annotation of binary question categories such as relevant vs. irrelevant, or paraphrases vs. non-paraphrases. Moreover, we are able to exploit the annotated utility relations as supervision in a learning for ranking approach, whereas (Duan et al., 2008) use th</context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong Yu. 2008. Searching questions by identifying question topic and question focus. In Proceedings of ACL-08: HLT, pages 156–164, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>Natural language based reformulation resource and web exploitation for question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of TREC-2002.</booktitle>
<contexts>
<context position="32013" citStr="Hermjakob et al., 2002" startWordPosition="5517" endWordPosition="5520">rk We plan to integrate context dependent word similarity measures into a more robust question utility function. We also plan to make the dependency tree matching more flexible in order to account for paraphrase patterns that may differ in their syntactic structure. The questions that are posted on community QA sites often contain spelling or grammatical errors. Consequently, we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors. 7 Related Work The question rephrasing subtask has spawned a diverse set of approaches. (Hermjakob et al., 2002) derive a set of phrasal patterns for question reformulation by generalizing surface patterns acquired automatically from a large corpus of web documents. The focus of the work in (Tomuro, 2003) is on deriving reformulation patterns for the interrogative part of a question. In (Jeon et al., 2005), word translation probabilities are trained on pairs of semantically similar questions that are automatically extracted from an FAQ archive, and then used in a language model that retrieves question reformulations. (Jijkoun and de Rijke, 2005) describe an FAQ 105 question retrieval system in which wei</context>
</contexts>
<marker>Hermjakob, Echihabi, Marcu, 2002</marker>
<rawString>Ulf Hermjakob, Abdessamad Echihabi, and Daniel Marcu. 2002. Natural language based reformulation resource and web exploitation for question answering. In Proceedings of TREC-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM’05),</booktitle>
<pages>84--90</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="32310" citStr="Jeon et al., 2005" startWordPosition="5567" endWordPosition="5570">mmunity QA sites often contain spelling or grammatical errors. Consequently, we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors. 7 Related Work The question rephrasing subtask has spawned a diverse set of approaches. (Hermjakob et al., 2002) derive a set of phrasal patterns for question reformulation by generalizing surface patterns acquired automatically from a large corpus of web documents. The focus of the work in (Tomuro, 2003) is on deriving reformulation patterns for the interrogative part of a question. In (Jeon et al., 2005), word translation probabilities are trained on pairs of semantically similar questions that are automatically extracted from an FAQ archive, and then used in a language model that retrieves question reformulations. (Jijkoun and de Rijke, 2005) describe an FAQ 105 question retrieval system in which weighted combinations of similarity functions corresponding to questions, existing answers, FAQ titles and pages are computed using a vector space model. (Zhao et al., 2007) exploit the Encarta logs to automatically extract clusters containing question paraphrases and further train a perceptron to r</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM’05), pages 84–90, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="16477" citStr="Jiang and Conrath, 1997" startWordPosition="2783" endWordPosition="2786">ave ignored the normalization constant contained in the original measure. For each word w E Qi, maxSim(w, Qr) computes the maximum semantic similarity between w and any word wr E Qr. The similarity scores are weighted by the corresponding idf’s, and normalized. A similar score is computed for each word w E Qr. The score computed by maxSim depends on the actual function used to compute the word-to-word semantic similarity. In this paper, we evaluated four of the knowledgebased measures explored in (Mihalcea et al., 2006): wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997). 4 Supervised Learning for Question Ranking Cosine similarity, henceforth referred as cos, treats questions as bags-of-words. The meta-measure proposed in (Mihalcea et al., 2006), henceforth called mcs, treats questions as bags-of-concepts. Both cos and mcs ignore the syntactic relations between the words in a question, and therefore may miss important structural information. In the next three sections we describe a set of structural features that we believe are relevant for judging question similarity. These and other types of features will be integrated in an SVM model for ranking, as descr</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the Web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM’05),</booktitle>
<pages>76--83</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving answers from frequently asked questions pages on the Web. In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM’05), pages 76–83, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2002),</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="25732" citStr="Joachims, 2002" startWordPosition="4440" endWordPosition="4441">features is targeted at questions containing locations: 103 6. φl(Qi, Qr) = 1 if both questions contain locations, 0 otherwise. 7. φd(Qi, Qr) = the normalized geographical distance between the locations in Qi and Qr, 0 if φl(Qi, Qr) = 0. Given two location names, we first find their latitude and longitude using Google Maps, and then compute the spherical distance between them using the haversine formula. The corresponding parameters w will be trained on pairs from one of the partially ordered datasets described in Section 2. We use the kernel version of the large-margin ranking approach from (Joachims, 2002) which solves the optimization problem in Figure 3 below. The aim of this formulation is to find a minimize: J(w,ξ) = 12I IwI I2 + C E ξrij subject to: wTφ(Qi, Qr) − wT φ(Qj, Qr) &gt; 1 − ξrij ξrij &gt; 0 VQr, Qi, Qj E D, (Qi &gt;- Qj|Qr) Figure 3: SVM ranking optimization problem. weight vector w such that 1) the number of ranking constraints u(Qi, Qr) &gt; u(Qj, Qr) from the training data D that are violated is minimized, and 2) the ranking function u(Qi, Qr) generalizes well beyond the training data. The learned w is a linear combination of the feature vectors φ(Qi, Qr), which makes it possible to use </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2002), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning (ICML ’98),</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="16442" citStr="Lin, 1998" startWordPosition="2779" endWordPosition="2780">ial for ranking, we have ignored the normalization constant contained in the original measure. For each word w E Qi, maxSim(w, Qr) computes the maximum semantic similarity between w and any word wr E Qr. The similarity scores are weighted by the corresponding idf’s, and normalized. A similar score is computed for each word w E Qr. The score computed by maxSim depends on the actual function used to compute the word-to-word semantic similarity. In this paper, we evaluated four of the knowledgebased measures explored in (Mihalcea et al., 2006): wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997). 4 Supervised Learning for Question Ranking Cosine similarity, henceforth referred as cos, treats questions as bags-of-words. The meta-measure proposed in (Mihalcea et al., 2006), henceforth called mcs, treats questions as bags-of-concepts. Both cos and mcs ignore the syntactic relations between the words in a question, and therefore may miss important structural information. In the next three sections we describe a set of structural features that we believe are relevant for judging question similarity. These and other types of features will be integrated in</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML ’98), pages 296–304, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence (AAAI’06),</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="15688" citStr="Mihalcea et al., 2006" startWordPosition="2649" endWordPosition="2652">he meanings of words in each question. This particular problem is illustrated by the three questions below. Q22 and Q23 have the same cosine similarity with Q21, they are therefore indistinguishable in terms of their usefulness to the reference question Q21, even though we expect Q22 to be more useful than Q23 (a place that sells hydrangea often sells other types of plants too, possibly including cacti). Q21 Where can I buy a hydrangea? Q22 Where can I buy a cactus? Q23 Where can I buy an iPad? To alleviate the lexical chasm, we can redefine u(Qi, Qr) to be the similarity measure proposed by (Mihalcea et al., 2006) as follows: E maxSim(w, Qr) * idf(w) wE{Q;} E idf(w) wE{Q;} E maxSim(w, Qi) * idf(w) wE{Q,.} E idf(w) wE{Q,.} Since scaling factors are immaterial for ranking, we have ignored the normalization constant contained in the original measure. For each word w E Qi, maxSim(w, Qr) computes the maximum semantic similarity between w and any word wr E Qr. The similarity scores are weighted by the corresponding idf’s, and normalized. A similar score is computed for each word w E Qr. The score computed by maxSim depends on the actual function used to compute the word-to-word semantic similarity. In this p</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence (AAAI’06), pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Moldovan</author>
<author>Marius Pasca</author>
<author>Sanda M Harabagiu</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Performance issues and error analysis in an open-domain question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1373" citStr="Moldovan et al., 2002" startWordPosition="206" endWordPosition="209">rvised similarity measures. 1 Introduction Open domain Question Answering (QA) is one of the most complex and challenging tasks in natural language processing. In general, a question answering system may need to integrate knowledge coming from a wide variety of linguistic processing tasks such as syntactic parsing, semantic role labeling, named entity recognition, and anaphora resolution (Prager, 2006). State of the art implementations of these linguistic analysis tasks are still limited in their performance, with errors that compound and propagate into the final performance of the QA system (Moldovan et al., 2002). Consequently, the performance of open domain QA systems has yet to arrive at a level at which it would become a feasible alternative to the current paradigms for information access based on keyword searches. Recently, community-driven QA sites such as Yahoo! Answers and WikiAnswers 1 have established 1answers.yahoo.com, wiki.answers.com a new approach to question answering that shifts the inherent complexity of open domain QA from the computer system to volunteer contributors. The computer is no longer required to perform a deep linguistic analysis of questions and generate corresponding ans</context>
</contexts>
<marker>Moldovan, Pasca, Harabagiu, Surdeanu, 2002</marker>
<rawString>Dan I. Moldovan, Marius Pasca, Sanda M. Harabagiu, and Mihai Surdeanu. 2002. Performance issues and error analysis in an open-domain question answering system. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 33–40, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
</authors>
<date>2006</date>
<booktitle>Open-domain questionanswering. Foundations and Trends in Information Retrieval,</booktitle>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="1156" citStr="Prager, 2006" startWordPosition="173" endWordPosition="174">n reflecting the relative utility of questions inside each group. Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures. 1 Introduction Open domain Question Answering (QA) is one of the most complex and challenging tasks in natural language processing. In general, a question answering system may need to integrate knowledge coming from a wide variety of linguistic processing tasks such as syntactic parsing, semantic role labeling, named entity recognition, and anaphora resolution (Prager, 2006). State of the art implementations of these linguistic analysis tasks are still limited in their performance, with errors that compound and propagate into the final performance of the QA system (Moldovan et al., 2002). Consequently, the performance of open domain QA systems has yet to arrive at a level at which it would become a feasible alternative to the current paradigms for information access based on keyword searches. Recently, community-driven QA sites such as Yahoo! Answers and WikiAnswers 1 have established 1answers.yahoo.com, wiki.answers.com a new approach to question answering that </context>
</contexts>
<marker>Prager, 2006</marker>
<rawString>John M. Prager. 2006. Open-domain questionanswering. Foundations and Trends in Information Retrieval, 1(2):91–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In IJCAI’95: Proceedings of the 14th international joint conference on Artificial intelligence,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="16425" citStr="Resnik, 1995" startWordPosition="2776" endWordPosition="2777"> factors are immaterial for ranking, we have ignored the normalization constant contained in the original measure. For each word w E Qi, maxSim(w, Qr) computes the maximum semantic similarity between w and any word wr E Qr. The similarity scores are weighted by the corresponding idf’s, and normalized. A similar score is computed for each word w E Qr. The score computed by maxSim depends on the actual function used to compute the word-to-word semantic similarity. In this paper, we evaluated four of the knowledgebased measures explored in (Mihalcea et al., 2006): wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997). 4 Supervised Learning for Question Ranking Cosine similarity, henceforth referred as cos, treats questions as bags-of-words. The meta-measure proposed in (Mihalcea et al., 2006), henceforth called mcs, treats questions as bags-of-concepts. Both cos and mcs ignore the syntactic relations between the words in a question, and therefore may miss important structural information. In the next three sections we describe a set of structural features that we believe are relevant for judging question similarity. These and other types of features will</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In IJCAI’95: Proceedings of the 14th international joint conference on Artificial intelligence, pages 448–453, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
</authors>
<title>Interrogative reformulation patterns and acquisition of question paraphrases.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Workshop on Paraphrasing,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="32207" citStr="Tomuro, 2003" startWordPosition="5551" endWordPosition="5552">aphrase patterns that may differ in their syntactic structure. The questions that are posted on community QA sites often contain spelling or grammatical errors. Consequently, we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors. 7 Related Work The question rephrasing subtask has spawned a diverse set of approaches. (Hermjakob et al., 2002) derive a set of phrasal patterns for question reformulation by generalizing surface patterns acquired automatically from a large corpus of web documents. The focus of the work in (Tomuro, 2003) is on deriving reformulation patterns for the interrogative part of a question. In (Jeon et al., 2005), word translation probabilities are trained on pairs of semantically similar questions that are automatically extracted from an FAQ archive, and then used in a language model that retrieves question reformulations. (Jijkoun and de Rijke, 2005) describe an FAQ 105 question retrieval system in which weighted combinations of similarity functions corresponding to questions, existing answers, FAQ titles and pages are computed using a vector space model. (Zhao et al., 2007) exploit the Encarta log</context>
</contexts>
<marker>Tomuro, 2003</marker>
<rawString>Noriko Tomuro. 2003. Interrogative reformulation patterns and acquisition of question paraphrases. In Proceedings of the Second International Workshop on Paraphrasing, pages 33–40, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16405" citStr="Wu and Palmer, 1994" startWordPosition="2771" endWordPosition="2774">df(w) wE{Q,.} Since scaling factors are immaterial for ranking, we have ignored the normalization constant contained in the original measure. For each word w E Qi, maxSim(w, Qr) computes the maximum semantic similarity between w and any word wr E Qr. The similarity scores are weighted by the corresponding idf’s, and normalized. A similar score is computed for each word w E Qr. The score computed by maxSim depends on the actual function used to compute the word-to-word semantic similarity. In this paper, we evaluated four of the knowledgebased measures explored in (Mihalcea et al., 2006): wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997). 4 Supervised Learning for Question Ranking Cosine similarity, henceforth referred as cos, treats questions as bags-of-words. The meta-measure proposed in (Mihalcea et al., 2006), henceforth called mcs, treats questions as bags-of-concepts. Both cos and mcs ignore the syntactic relations between the words in a question, and therefore may miss important structural information. In the next three sections we describe a set of structural features that we believe are relevant for judging question similarity. These and other ty</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
</authors>
<title>Learning question paraphrases for QA from Encarta logs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI’07),</booktitle>
<pages>1795--1800</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="32783" citStr="Zhao et al., 2007" startWordPosition="5641" endWordPosition="5644">s. The focus of the work in (Tomuro, 2003) is on deriving reformulation patterns for the interrogative part of a question. In (Jeon et al., 2005), word translation probabilities are trained on pairs of semantically similar questions that are automatically extracted from an FAQ archive, and then used in a language model that retrieves question reformulations. (Jijkoun and de Rijke, 2005) describe an FAQ 105 question retrieval system in which weighted combinations of similarity functions corresponding to questions, existing answers, FAQ titles and pages are computed using a vector space model. (Zhao et al., 2007) exploit the Encarta logs to automatically extract clusters containing question paraphrases and further train a perceptron to recognize question paraphrases inside each cluster based on a combination of lexical, syntactic and semantic similarity features. More recently, (Bernhard and Gurevych, 2008) evaluated various string similarity measures and vector space based similarity measures on the task of retrieving question paraphrases from the WikiAnswers repository. The aim of the question search task presented in (Duan et al., 2008) is to return questions that are semantically equivalent or clo</context>
</contexts>
<marker>Zhao, Zhou, Liu, 2007</marker>
<rawString>Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning question paraphrases for QA from Encarta logs. In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI’07), pages 1795–1800, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>