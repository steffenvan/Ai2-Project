<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000223">
<title confidence="0.996564">
Dialogue Act Tagging for Instant Messaging Chat Sessions
</title>
<author confidence="0.998216">
Edward Ivanovic
</author>
<affiliation confidence="0.9982255">
Department of Computer Science and Software Engineering
University of Melbourne
</affiliation>
<address confidence="0.506802">
Victoria 3010, Australia
</address>
<email confidence="0.998288">
edwardi@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999625866666667">
Instant Messaging chat sessions are real-
time text-based conversations which can
be analyzed using dialogue-act models.
We describe a statistical approach for
modelling and detecting dialogue acts in
Instant Messaging dialogue. This in-
volved the collection of a small set of
task-based dialogues and annotating them
with a revised tag set. We then dealt with
segmentation and synchronisation issues
which do not arise in spoken dialogue.
The model we developed combines naive
Bayes and dialogue-act n-grams to obtain
better than 80% accuracy in our tagging
experiment.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863">
Instant Messaging (IM) dialogue has received rel-
atively little attention in discourse modelling. The
novelty and popularity of IM dialogue and the
significant differences between written and spoken
English warrant specific research on IM dialogue.
We show that IM dialogue has some unique prob-
lems and attributes not found in transcribed spoken
dialogue, which has been the focus of most work in
discourse modelling. The present study addresses
the problems presented by these differences when
modelling dialogue acts in IM dialogue.
Stolcke et al. (2000) point out that the use of
dialogue acts is a useful first level of analysis for
describing discourse structure. Dialogue acts are
based on the illocutionary force of an utterance from
speech act theory, and represent acts such as asser-
tions and declarations (Austin, 1962; Searle, 1979).
</bodyText>
<page confidence="0.977881">
79
</page>
<bodyText confidence="0.999977833333333">
This theory has been extended in dialogue acts to
model the conversational functions that utterances
can perform. Dialogue acts have been used to ben-
efit tasks such as machine translation (Tanaka and
Yokoo, 1999) and the automatic detection of dia-
logue games (Levin et al., 1999). This deeper level
of discourse understanding may help replace or as-
sist a support representative using IM dialogue by
suggesting responses that are more sophisticated and
realistic to a human dialogue participant.
The unique problems and attributes exhibited by
IM dialogue prohibit existing dialogue act classi-
fication methods from being applied directly. We
present solutions to some of these problems along
with methods to obtain high accuracy in automated
dialogue act classification. A statistical discourse
model is trained and then used to classify dialogue
acts based on the observed words in an utterance.
The training data are online conversations between
two people: a customer and a shopping assistant,
which we collected and manually annotated. Table 1
shows a sample of the type of dialogue and discourse
structure used in this study.
We begin by considering the preliminary issues
that arise in IM dialogue, why they are problematic
when modelling dialogue acts, and present their so-
lutions in §2. With the preliminary problems solved,
we investigate the dialogue act labelling task with a
description of our data in §3. The remainder of the
paper describes our experiment involving the train-
ing of a naive Bayes model combined with a n-gram
discourse model (§4). The results of this model and
evaluation statistics are presented in §5. §6 contains
a discussion of the approach we used including its
strengths, areas of improvement, and issues for fu-
ture research followed by the conclusion in §7.
</bodyText>
<note confidence="0.503173">
Proceedings of the ACL Student Research Workshop, pages 79–84,
</note>
<footnote confidence="0.241967">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</footnote>
<table confidence="0.650206">
Turn Msg Sec Speaker Message
5 8 18 Customer [i was talking to mike and my browser crashed]Ug:STATEMENT - [can you transfer me to him
again?]Ug:YES-NO-QUESTION
5 9 7 Customer [he found a gift i wanted]U10:STATEMENT
6 10 35 Sally [I will try my best to help you find the gift,]U11:STATEMENT [please let me know the
request]U12:REQUEST
6 11 9 Sally [Mike is not available at this point of time]U13:STATEMENT
7 12 1 Customer [but mike already found it]U14:STATEMENT [isn’t he there?]U15:YES-NO-QUESTION
8 13 8 Customer [it was a remote control car]U16:STATEMENT
9 14 2 Sally [Mike is not available right now.]U1,:NO-ANSWER [I am here to assist you.]U1g:STATEMENT
10 15 28 Sally [Sure Customer,]U1g:RESPONSE-ACK [I will search for the remote control car.]U20:STATEMENT
</table>
<tableCaption confidence="0.970924">
Table 1: An example of unsynchronised messages occurring when a user prematurely assumes a turn is
finished. Here, message (“Msg”) 12 is actually in response to 10, not 11 since turn 6 was sent as 2 messages:
</tableCaption>
<bodyText confidence="0.3850605">
10 and 11. We use the seconds elapsed (“Sec”) since the previous message as part of a method to re-
synchronise messages. Utterance boundaries and their respective dialogue acts are denoted by U,
</bodyText>
<sectionHeader confidence="0.889756" genericHeader="method">
2 Issues in Instant Messaging Dialogue
</sectionHeader>
<bodyText confidence="0.999541375">
There are several differences between IM and tran-
scribed spoken dialogue. The dialogue act classifier
described in this paper is dependent on preprocess-
ing tasks to resolve the issues discussed in this sec-
tion.
Sequences of words in textual dialogue are
grouped into three levels. The first level is a Turn,
consisting of at least one Message, which consists
of at least one Utterance, defined as follows:
Turn: Dialogue participants normally take turns
writing.
Message: A message is defined as a group of words
that are sent from one dialogue participant to the
other as a single unit. A single turn can span multi-
ple messages, which sometimes leads to accidental
interruptions as discussed in §2.2.
Utterance: This is the shortest unit we deal with and
can be thought of as one complete semantic unit—
something that has a meaning. This can be a com-
plete sentence or as short as an emoticon (e.g. “:-)”
to smile).
Several lines from one of the dialogues in our cor-
pus are shown as an example denoted with Turn,
Message, and Utterance boundaries in Table 1.
</bodyText>
<subsectionHeader confidence="0.994072">
2.1 Utterance Segmentation
</subsectionHeader>
<bodyText confidence="0.958609111111111">
Because dialogue acts work at the utterance level
and users send messages which may contain more
than one utterance, we first need to segment the mes-
sages by detecting utterance boundaries. Messages
in our data were manually labelled with one or more
dialogue act depending on the number of utterances
each message contained. Labelling in this fashion
had the effect of also segmenting messages into ut-
terances based on the dialogue act boundaries.
</bodyText>
<subsectionHeader confidence="0.999854">
2.2 Synchronising Messages in IM Dialogue
</subsectionHeader>
<bodyText confidence="0.999976791666667">
The end of a turn is not always obvious in typed
dialogue. Users often divide turns into multiple
messages, usually at clause or utterance boundaries,
which can result in the end of a message being mis-
taken as the end of that turn. This ambiguity can lead
to accidental turn interruptions which cause mes-
sages to become unsynchronised. In these cases
each participant tends to respond to an earlier mes-
sage than the immediately previous one, making the
conversation seem somewhat incoherent when read
as a transcript. An example of such a case is shown
in Table 1 in which Customer replied to message 10
with message 12 while Sally was still completing
turn 6 with message 11. If the resulting discourse is
read sequentially it would seem that the customer ig-
nored the information provided in message 11. The
time between messages shows that only 1 second
elapsed between messages 11 and 12, so message
12 must in fact be in response to message 10.
Message Mi is defined to be dependent on mes-
sage Md if the user wrote Mi having already seen
and presumably considered Md. The importance
of unsynchronised messages is that they result in
the dialogue acts also being out of order, which is
</bodyText>
<page confidence="0.981772">
80
</page>
<bodyText confidence="0.99993625">
problematic when using bigram or higher-order n-
gram language models. Therefore, messages are
re-synchronised as described in §3.2 before training
and classification.
</bodyText>
<sectionHeader confidence="0.930854" genericHeader="method">
3 The Dialogue Act Labelling Task
</sectionHeader>
<bodyText confidence="0.999976">
The domain being modelled is the online shopping
assistance provided as part of the MSN Shopping
site. People are employed to provide live assistance
via an IM medium to potential customers who need
help in finding items for purchase. Several dialogues
were collected using this service, which were then
manually labelled with dialogue acts and used to
train our statistical models.
There were 3 aims of this task: 1) to obtain a re-
alistic corpus; 2) to define a suitable set of dialogue
act tags; and 3) to manually label the corpus using
the dialogue act tag set, which is then used for train-
ing the statistical models for automatic dialogue act
classification.
</bodyText>
<subsectionHeader confidence="0.999658">
3.1 Tag Set
</subsectionHeader>
<bodyText confidence="0.999931043478261">
We chose 12 tags by manually labelling the dialogue
corpus using tags that seemed appropriate from the
42 tags used by Stolcke et al. (2000) based on the
Dialog Act Markup in Several Layers (DAMSL) tag
set (Core and Allen, 1997). Some tags, such as UN-
INTERPRETABLE and SELF-TALK, were eliminated
as they are not relevant for typed dialogue. Tags that
were difficult to distinguish, given the types of ut-
terances in our corpus, were collapsed into one tag.
For example, NO ANSWERS, REJECT, and NEGA-
TIVE NON-NO ANSWERS are all represented by NO-
ANSWER in our tag set.
The Kappa statistic was used to compare inter-
annotator agreement normalised for chance (Siegel
and Castellan, 1988). Labelling was carried out
by three computational linguistics graduate students
with 89% agreement resulting in a Kappa statistic of
0.87, which is a satisfactory indication that our cor-
pus can be labelled with high reliability using our
tag set (Carletta, 1996).
A complete list of the 12 dialogue acts we used is
shown in Table 2 along with examples and the fre-
quency of each dialogue act in our corpus.
</bodyText>
<table confidence="0.999914444444445">
Tag Example %
STATEMENT I am sending you the page now 36.0
THANKING Thank you for contacting us 14.7
YES-NO- Did you receive the page? 13.9
QUESTION
RESPONSE-ACK Sure 7.2
REQUEST Please let me know how I can 5.9
assist
OPEN- how do I use the international 5.3
QUESTION version?
YES-ANSWER yes, yeah 5.1
CONVENTIONAL- Bye Bye 2.9
CLOSING
NO-ANSWER no, nope 2.5
CONVENTIONAL- Hello Customer 2.3
OPENING
EXPRESSIVE haha, :-), grr 2.3
DOWNPLAYER my pleasure 1.9
</table>
<tableCaption confidence="0.987388">
Table 2: The 12 dialogue act labels with examples
</tableCaption>
<bodyText confidence="0.7959605">
and frequencies given as percentages of the total
number of utterances in our corpus.
</bodyText>
<subsectionHeader confidence="0.992069">
3.2 Re-synchronising Messages
</subsectionHeader>
<bodyText confidence="0.838603428571429">
The typing rate is used to determine message
dependencies. We calculate the typing rate by
time(Mi)−time(Md) , which is the elapsed time be-
length(Mi)
tween two messages divided by the number of char-
acters in Mi. The dependent message Md may be
the immediately preceding message such that d =
i − 1 or any earlier message where 0 &lt; d &lt; i with
the first message being M1. This algorithm is shown
in Algorithm 1.
Algorithm 1 Calculate message dependency for
message i
d ← i
repeat
</bodyText>
<equation confidence="0.3138328">
d ← d − 1
typing rate ← time(Mi)−time(Md)
length(Mi)
until typing rate &lt; typing threshold or d = 1
or speaker(Mi) = speaker(Md)
</equation>
<bodyText confidence="0.999872833333333">
The typing threshold in Algorithm 1 was calcu-
lated by taking the 90th percentile of all observed
typing rates from approximately 300 messages that
had their dependent messages manually labelled re-
sulting in a value of 5 characters per second. We
found that 20% of our messages were unsynchro-
</bodyText>
<page confidence="0.993477">
81
</page>
<bodyText confidence="0.9994884">
nised, giving a baseline accuracy of automatically
detecting message dependencies of 80% assuming
that Md = Mz_1. Using the method described, we
achieved a correct dependency detection accuracy of
94.2%.
</bodyText>
<sectionHeader confidence="0.901738" genericHeader="method">
4 Training on Speech Acts
</sectionHeader>
<bodyText confidence="0.9998604">
Our goal is to perform automatic dialogue act clas-
sification of the current utterance given any previous
utterances and their tags. Given all available evi-
dence E about a dialogue, the goal is to find the
dialogue act sequence U with the highest posterior
probability P(U|E) given that evidence. To achieve
this goal, we implemented a naive Bayes classifier
using bag-of-words feature representation such that
the most probable dialogue act d given a bag-of-
words input vector v� is taken to be:
</bodyText>
<equation confidence="0.682925181818182">
d = P(�v|d)P(d)
P(v|d) �
d =
argmax
dED P(v)
n
H P(vj|d)
j=1
n
argmax P(d) H P(vj|d)
dED j=1
</equation>
<bodyText confidence="0.999459058823529">
where vj is the jth element in v, D denotes the set of
all dialogue acts and P(v) is constant for all d E D.
The use of P(d) in Equation 3 assumes that dia-
logue acts are independent of one another. However,
we intuitively know that if someone asks a YES-NO-
QUESTION then the response is more likely to be a
YES-ANSWER rather than, say, CONVENTIONAL-
CLOSING. This intuition is reflected in the bigram
transition probabilities obtained from our corpus.1
To capture this dialogue act relationship we
trained standard n-gram models of dialogue act his-
tory with add-one smoothing for the calculation
of P(vj|d). The bigram model uses the posterior
probability P(d|H) rather than the prior probability
P(d) in Equation 3, where H is the n-gram context
vector containing the previous dialogue act or previ-
ous 2 dialogue acts in the case of the trigram model.
</bodyText>
<footnote confidence="0.982701333333333">
1Due to space constraints, the dialogue act transition ta-
ble has been omitted from this paper and is made available at
http://www.cs.mu.oz.au/∼edwardi/papers/da transitions.html
</footnote>
<table confidence="0.954806666666667">
Model Min Max Mean Hit % Px
Baseline — — 36.0% — —
Likelihood 72.3% 90.5% 80.1% — —
Unigram 74.7% 90.5% 80.6% 100 7.7
Bigram 75.0% 92.4% 81.6% 97 4.7
Trigram 69.5% 94.1% 80.9% 88 3.3
</table>
<tableCaption confidence="0.931345">
Table 3: Mean accuracy of labelling utterances with
dialogue acts using n-gram models. Shown with hit-
rate results and perplexities (“Px”)
</tableCaption>
<sectionHeader confidence="0.995076" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999952542857143">
Evaluation of the results was conducted via 9-fold
cross-validation across the 9 dialogues in our cor-
pus using 8 dialogues for training and 1 for testing.
Table 3 shows the results of running the experiment
with various models replacing the prior probability,
P(d), in Equation 3. The Min, Max, and Mean
columns are obtained from the cross-validation tech-
nique used for evaluation. The baseline used for this
task was to assign the most frequently observed dia-
logue act to each utterance, namely, STATEMENT.
Omitting P(d) from Equation 3 such that only
the likelihood (Equation 2) of the naive Bayes for-
mula is used resulted in a mean accuracy of 80.1%.
The high accuracy obtained with only the likelihood
reflects the high dependency between dialogue acts
and the actual words used in utterances. This de-
pendency is represented well by the bag-of-words
approach. Using P(d) to arrive at Equation 3 yields
a slight increase in accuracy to 80.6%.
The bigram model obtains the best result with
81.6% accuracy. This result is due to more accurate
predictions with P(d|H). The trigram model pro-
duced a slightly lower accuracy rate, partly due to a
lack of training data and to dialogue act adjacency
pairs not being dependent on dialogue acts further
removed as discussed in §4.
In order to gauge the effectiveness of the bigram
and trigram models in view of the small amount of
training data, hit-rate statistics were collected during
testing. These statistics, presented in Table 3, show
the percentage of conditions that existed in the var-
ious models. Conditions that did not exist were not
counted in the accuracy measure during evaluation.
The perplexities (Cover and Thomas, 1991) for
the various n-gram models we used are shown in
</bodyText>
<page confidence="0.999004">
82
</page>
<tableCaption confidence="0.831906">
Table 3. The biggest improvement, indicated by a
</tableCaption>
<bodyText confidence="0.948994571428571">
decreased perplexity, comes when moving from the
unigram to bigram models as expected. However,
the large difference between the bigram and trigram
models is somewhat unexpected given the theory of
adjacency pairs. This may be a result of insufficient
training data as would be suggested by the lower tri-
gram hit rate.
</bodyText>
<sectionHeader confidence="0.994117" genericHeader="evaluation">
6 Discussion and Future Research
</sectionHeader>
<bodyText confidence="0.999927551724138">
As indicated by the Kappa statistics in §3.1, la-
belling utterances with dialogue acts can sometimes
be a subjective task. Moreover, there are many pos-
sible tag sets to choose from. These two factors
make it difficult to accurately compare various tag-
ging methods and is one reason why Kappa statistics
and perplexity measures are useful. The work pre-
sented in this paper shows that using even the rel-
atively simple bag-of-words approach with a naive
Bayes classifier can produce very good results.
One important area not tackled by this experiment
was that of utterance boundary detection. Multiple
utterances are often sent in one message, sometimes
in one sentence, and each utterance must be tagged.
Approximately 40% of the messages in our corpus
have more than one utterance per message. Utter-
ances were manually marked in this experiment as
the study was focussed only on dialogue act classi-
fication given a sequence of utterances. It is rare,
however, to be given text that is already segmented
into utterances, so some work will be required to
accomplish this segmentation before automated di-
alogue act tagging can commence. Therefore, ut-
terance boundary detection is an important area for
further research.
The methods used to detect dialogue acts pre-
sented here do not take into account sentential struc-
ture. The sentences in (1) would thus be treated
equally with the bag-of-words approach.
</bodyText>
<listItem confidence="0.9677515">
(1) a. john has been to london
b. has john been to london
</listItem>
<bodyText confidence="0.9998624">
Without the punctuation (as is often the case with in-
formal typed dialogue) the bag-of-words approach
will not differentiate the sentences, whereas if we
look at the ordering of even the first two words we
can see that “john has ...” is likely to be a STATE-
MENT whereas “has john ...” would be a question. It
would be interesting to research other types of fea-
tures such as phrase structure or even looking at the
order of the first x words and the parts of speech of
an utterance to determine its dialogue act.
Aspects of dialogue macrogame theory (DMT)
(Mann, 2002) may help to increase tagging accu-
racy. In DMT, sets of utterances are grouped to-
gether to form a game. Games may be nested as
in the following example:
</bodyText>
<figure confidence="0.94273125">
A: May I know the price range please?
B: In which currency?
A: $US please
B: 200–300
</figure>
<bodyText confidence="0.999810393939394">
Here, B has nested a clarification question which
was required before providing the price range. The
bigram model presented in this paper will incor-
rectly capture this interaction as the sequence YES-
NO-QUESTION, OPEN-QUESTION, STATEMENT,
STATEMENT, whereas DMT would be able to ex-
tract the nested question resulting in the correct pairs
of question and answer sequences.
Although other studies have attempted to auto-
matically tag utterances with dialogue acts (Stolcke
et al., 2000; Jurafsky et al., 1997; Kita et al., 1996) it
is difficult to fairly compare results because the data
was significantly different (transcribed spoken dia-
logue versus typed dialogue) and the dialogue acts
were also different ranging from a set of 9 (Kita et
al., 1996) to 42 (Stolcke et al., 2000). It may be pos-
sible to use a standard set of dialogue acts for a par-
ticular domain, but inventing a set that could be used
for all domains seems unlikely. This is primarily due
to differing needs in various applications. A super-
set of dialogue acts that covers all domains would
necessarily be a large number of tags (at least the 42
identified by Stolcke et al. (2000)) with many tags
not being appropriate for other domains.
The best result from our dialogue act classifier
was obtained using a bigram discourse model result-
ing in an average tagging accuracy of 81.6% (see Ta-
ble 3). Although this is higher than the results from
13 recent studies presented by Stolcke et al. (2000)
with accuracy ranging from Pz� 40% to 81.2%, the
tasks, data, and tag sets used were all quite different,
so any comparison should be used as only a guide-
line.
</bodyText>
<page confidence="0.998511">
83
</page>
<sectionHeader confidence="0.99797" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991363636364">
In this paper, we have highlighted some unique char-
acteristics in IM dialogue that are not found in tran-
scribed spoken dialogue or other forms of written
dialogue such as e-mail; namely, utterance segmen-
tation and message synchronisation. We showed the
problem of unsynchronised messages can be readily
solved using a simple technique utilising the typing-
rate and time stamps of messages. We described
a method for high-accuracy dialogue act classifica-
tion, which is an essential part for a deeper under-
standing of dialogue. In our experiments, the bi-
gram model performed with the highest tagging ac-
curacy which indicates that dialogue acts often oc-
cur as adjacency pairs. We also saw that the high
tagging accuracy results obtained by the likelihood
from the naive Bayes model indicated the high cor-
relation between the actual words and dialogue acts.
The Kappa statistics we calculated indicate that our
tag set can be used reliably for annotation tasks.
The increasing popularity of IM and automated
agent-based support services is ripe with new chal-
lenges for research and development. For example,
IM provides the ability for an automated agent to ask
clarification questions. Appropriate dialogue mod-
elling will enable the automated agent to reliably
distinguish questions from statements. More gener-
ally, the rapidly expanding scope of online support
services provides the impetus for IM dialogue sys-
tems and discourse models to be developed further.
Our findings have demonstrated the potential for di-
alogue modelling for IM chat sessions, and opens
the way for a comprehensive investigation of this
new application area.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974">
We thank Steven Bird, Timothy Baldwin, Trevor
Cohn, and the anonymous reviewers for their help-
ful and constructive comments on this paper. We
also thank Vanessa Smith, Patrick Ye, and Jeremy
Nicholson for annotating the data.
</bodyText>
<sectionHeader confidence="0.999268" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767163265306">
John L. Austin. 1962. How to do Things with Words.
Clarendon Press, Oxford.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguis-
tics, 22(2):249–254.
Mark Core and James Allen. 1997. Coding dialogs with
the DAMSL annotation scheme. Working Notes of
the AAAI Fall Symposium on Communicative Action
in Humans and Machines, pages 28–35.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
ofInformation Theory. Wiley, New York.
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1997. Automatic detection of discourse
structure for speech recognition and understanding.
Proceedings of the 1997 IEEE Workshop on Speech
Recognition and Understanding, pages 88–95.
Kenji Kita, Yoshikazu Fukui, Masaaki Nagata, and
Tsuyoshi Morimoto. 1996. Automatic acquisition
of probabilistic dialogue models. Proceedings of the
Fourth International Conference on Spoken Language,
1:196–199.
Lori Levin, Klaus Ries, Ann Thyme-Gobbel, and Alon
Lavie. 1999. Tagging of speech acts and dialogue
games in spanish call home. Towards Standards and
Tools for Discourse Tagging (Proceedings of the ACL
Workshop at ACL’99), pages 42–47.
William Mann. 2002. Dialogue macrogame theory. Pro-
ceedings of the 3rd SIGdial Workshop on Discourse
and Dialogue, pages 129–141.
John R. Searle. 1979. Expression and Meaning: Studies
in the Theory of Speech Acts. Cambridge University
Press, Cambridge, UK.
Sidney Siegel and N. John Castellan, Jr. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, second edition.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
Hideki Tanaka and Akio Yokoo. 1999. An efficient
statistical speech act type tagging system for speech
translation systems. In Proceedings of the 37th con-
ference on Association for Computational Linguistics,
pages 381–388. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999235">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928258">
<title confidence="0.999692">Dialogue Act Tagging for Instant Messaging Chat Sessions</title>
<author confidence="0.99964">Edward Ivanovic</author>
<affiliation confidence="0.999894">Department of Computer Science and Software Engineering University of Melbourne</affiliation>
<address confidence="0.99908">Victoria 3010, Australia</address>
<email confidence="0.999131">edwardi@csse.unimelb.edu.au</email>
<abstract confidence="0.99560975">Instant Messaging chat sessions are realtime text-based conversations which can be analyzed using dialogue-act models. We describe a statistical approach for modelling and detecting dialogue acts in Instant Messaging dialogue. This involved the collection of a small set of task-based dialogues and annotating them with a revised tag set. We then dealt with segmentation and synchronisation issues which do not arise in spoken dialogue. The model we developed combines naive and dialogue-act to obtain better than 80% accuracy in our tagging experiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John L Austin</author>
</authors>
<title>How to do Things with Words.</title>
<date>1962</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1625" citStr="Austin, 1962" startWordPosition="241" endWordPosition="242">pecific research on IM dialogue. We show that IM dialogue has some unique problems and attributes not found in transcribed spoken dialogue, which has been the focus of most work in discourse modelling. The present study addresses the problems presented by these differences when modelling dialogue acts in IM dialogue. Stolcke et al. (2000) point out that the use of dialogue acts is a useful first level of analysis for describing discourse structure. Dialogue acts are based on the illocutionary force of an utterance from speech act theory, and represent acts such as assertions and declarations (Austin, 1962; Searle, 1979). 79 This theory has been extended in dialogue acts to model the conversational functions that utterances can perform. Dialogue acts have been used to benefit tasks such as machine translation (Tanaka and Yokoo, 1999) and the automatic detection of dialogue games (Levin et al., 1999). This deeper level of discourse understanding may help replace or assist a support representative using IM dialogue by suggesting responses that are more sophisticated and realistic to a human dialogue participant. The unique problems and attributes exhibited by IM dialogue prohibit existing dialogu</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>John L. Austin. 1962. How to do Things with Words. Clarendon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="9379" citStr="Carletta, 1996" startWordPosition="1524" endWordPosition="1525">ed dialogue. Tags that were difficult to distinguish, given the types of utterances in our corpus, were collapsed into one tag. For example, NO ANSWERS, REJECT, and NEGATIVE NON-NO ANSWERS are all represented by NOANSWER in our tag set. The Kappa statistic was used to compare interannotator agreement normalised for chance (Siegel and Castellan, 1988). Labelling was carried out by three computational linguistics graduate students with 89% agreement resulting in a Kappa statistic of 0.87, which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set (Carletta, 1996). A complete list of the 12 dialogue acts we used is shown in Table 2 along with examples and the frequency of each dialogue act in our corpus. Tag Example % STATEMENT I am sending you the page now 36.0 THANKING Thank you for contacting us 14.7 YES-NO- Did you receive the page? 13.9 QUESTION RESPONSE-ACK Sure 7.2 REQUEST Please let me know how I can 5.9 assist OPEN- how do I use the international 5.3 QUESTION version? YES-ANSWER yes, yeah 5.1 CONVENTIONAL- Bye Bye 2.9 CLOSING NO-ANSWER no, nope 2.5 CONVENTIONAL- Hello Customer 2.3 OPENING EXPRESSIVE haha, :-), grr 2.3 DOWNPLAYER my pleasure 1.</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Core</author>
<author>James Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<contexts>
<context position="8664" citStr="Core and Allen, 1997" startWordPosition="1406" endWordPosition="1409">ice, which were then manually labelled with dialogue acts and used to train our statistical models. There were 3 aims of this task: 1) to obtain a realistic corpus; 2) to define a suitable set of dialogue act tags; and 3) to manually label the corpus using the dialogue act tag set, which is then used for training the statistical models for automatic dialogue act classification. 3.1 Tag Set We chose 12 tags by manually labelling the dialogue corpus using tags that seemed appropriate from the 42 tags used by Stolcke et al. (2000) based on the Dialog Act Markup in Several Layers (DAMSL) tag set (Core and Allen, 1997). Some tags, such as UNINTERPRETABLE and SELF-TALK, were eliminated as they are not relevant for typed dialogue. Tags that were difficult to distinguish, given the types of utterances in our corpus, were collapsed into one tag. For example, NO ANSWERS, REJECT, and NEGATIVE NON-NO ANSWERS are all represented by NOANSWER in our tag set. The Kappa statistic was used to compare interannotator agreement normalised for chance (Siegel and Castellan, 1988). Labelling was carried out by three computational linguistics graduate students with 89% agreement resulting in a Kappa statistic of 0.87, which is</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark Core and James Allen. 1997. Coding dialogs with the DAMSL annotation scheme. Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines, pages 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="14919" citStr="Cover and Thomas, 1991" startWordPosition="2463" endWordPosition="2466">h P(d|H). The trigram model produced a slightly lower accuracy rate, partly due to a lack of training data and to dialogue act adjacency pairs not being dependent on dialogue acts further removed as discussed in §4. In order to gauge the effectiveness of the bigram and trigram models in view of the small amount of training data, hit-rate statistics were collected during testing. These statistics, presented in Table 3, show the percentage of conditions that existed in the various models. Conditions that did not exist were not counted in the accuracy measure during evaluation. The perplexities (Cover and Thomas, 1991) for the various n-gram models we used are shown in 82 Table 3. The biggest improvement, indicated by a decreased perplexity, comes when moving from the unigram to bigram models as expected. However, the large difference between the bigram and trigram models is somewhat unexpected given the theory of adjacency pairs. This may be a result of insufficient training data as would be suggested by the lower trigram hit rate. 6 Discussion and Future Research As indicated by the Kappa statistics in §3.1, labelling utterances with dialogue acts can sometimes be a subjective task. Moreover, there are ma</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements ofInformation Theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Rebecca Bates</author>
<author>Noah Coccaro</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Klaus Ries</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Paul Taylor</author>
<author>Carol Van EssDykema</author>
</authors>
<title>Automatic detection of discourse structure for speech recognition and understanding.</title>
<date>1997</date>
<booktitle>Proceedings of the 1997 IEEE Workshop on Speech Recognition and Understanding,</booktitle>
<pages>88--95</pages>
<marker>Jurafsky, Bates, Coccaro, Martin, Meteer, Ries, Shriberg, Stolcke, Taylor, Van EssDykema, 1997</marker>
<rawString>Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van EssDykema. 1997. Automatic detection of discourse structure for speech recognition and understanding. Proceedings of the 1997 IEEE Workshop on Speech Recognition and Understanding, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Kita</author>
<author>Yoshikazu Fukui</author>
<author>Masaaki Nagata</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>Automatic acquisition of probabilistic dialogue models.</title>
<date>1996</date>
<booktitle>Proceedings of the Fourth International Conference on Spoken Language,</booktitle>
<pages>1--196</pages>
<contexts>
<context position="18164" citStr="Kita et al., 1996" startWordPosition="3012" endWordPosition="3015">example: A: May I know the price range please? B: In which currency? A: $US please B: 200–300 Here, B has nested a clarification question which was required before providing the price range. The bigram model presented in this paper will incorrectly capture this interaction as the sequence YESNO-QUESTION, OPEN-QUESTION, STATEMENT, STATEMENT, whereas DMT would be able to extract the nested question resulting in the correct pairs of question and answer sequences. Although other studies have attempted to automatically tag utterances with dialogue acts (Stolcke et al., 2000; Jurafsky et al., 1997; Kita et al., 1996) it is difficult to fairly compare results because the data was significantly different (transcribed spoken dialogue versus typed dialogue) and the dialogue acts were also different ranging from a set of 9 (Kita et al., 1996) to 42 (Stolcke et al., 2000). It may be possible to use a standard set of dialogue acts for a particular domain, but inventing a set that could be used for all domains seems unlikely. This is primarily due to differing needs in various applications. A superset of dialogue acts that covers all domains would necessarily be a large number of tags (at least the 42 identified </context>
</contexts>
<marker>Kita, Fukui, Nagata, Morimoto, 1996</marker>
<rawString>Kenji Kita, Yoshikazu Fukui, Masaaki Nagata, and Tsuyoshi Morimoto. 1996. Automatic acquisition of probabilistic dialogue models. Proceedings of the Fourth International Conference on Spoken Language, 1:196–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Levin</author>
<author>Klaus Ries</author>
<author>Ann Thyme-Gobbel</author>
<author>Alon Lavie</author>
</authors>
<title>Tagging of speech acts and dialogue games in spanish call home.</title>
<date>1999</date>
<booktitle>Towards Standards and Tools for Discourse Tagging (Proceedings of the ACL Workshop at ACL’99),</booktitle>
<pages>42--47</pages>
<contexts>
<context position="1924" citStr="Levin et al., 1999" startWordPosition="288" endWordPosition="291">logue acts in IM dialogue. Stolcke et al. (2000) point out that the use of dialogue acts is a useful first level of analysis for describing discourse structure. Dialogue acts are based on the illocutionary force of an utterance from speech act theory, and represent acts such as assertions and declarations (Austin, 1962; Searle, 1979). 79 This theory has been extended in dialogue acts to model the conversational functions that utterances can perform. Dialogue acts have been used to benefit tasks such as machine translation (Tanaka and Yokoo, 1999) and the automatic detection of dialogue games (Levin et al., 1999). This deeper level of discourse understanding may help replace or assist a support representative using IM dialogue by suggesting responses that are more sophisticated and realistic to a human dialogue participant. The unique problems and attributes exhibited by IM dialogue prohibit existing dialogue act classification methods from being applied directly. We present solutions to some of these problems along with methods to obtain high accuracy in automated dialogue act classification. A statistical discourse model is trained and then used to classify dialogue acts based on the observed words </context>
</contexts>
<marker>Levin, Ries, Thyme-Gobbel, Lavie, 1999</marker>
<rawString>Lori Levin, Klaus Ries, Ann Thyme-Gobbel, and Alon Lavie. 1999. Tagging of speech acts and dialogue games in spanish call home. Towards Standards and Tools for Discourse Tagging (Proceedings of the ACL Workshop at ACL’99), pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
</authors>
<title>Dialogue macrogame theory.</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>129--141</pages>
<contexts>
<context position="17402" citStr="Mann, 2002" startWordPosition="2886" endWordPosition="2887">s been to london b. has john been to london Without the punctuation (as is often the case with informal typed dialogue) the bag-of-words approach will not differentiate the sentences, whereas if we look at the ordering of even the first two words we can see that “john has ...” is likely to be a STATEMENT whereas “has john ...” would be a question. It would be interesting to research other types of features such as phrase structure or even looking at the order of the first x words and the parts of speech of an utterance to determine its dialogue act. Aspects of dialogue macrogame theory (DMT) (Mann, 2002) may help to increase tagging accuracy. In DMT, sets of utterances are grouped together to form a game. Games may be nested as in the following example: A: May I know the price range please? B: In which currency? A: $US please B: 200–300 Here, B has nested a clarification question which was required before providing the price range. The bigram model presented in this paper will incorrectly capture this interaction as the sequence YESNO-QUESTION, OPEN-QUESTION, STATEMENT, STATEMENT, whereas DMT would be able to extract the nested question resulting in the correct pairs of question and answer se</context>
</contexts>
<marker>Mann, 2002</marker>
<rawString>William Mann. 2002. Dialogue macrogame theory. Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue, pages 129–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Expression and Meaning: Studies in the Theory of Speech Acts.</title>
<date>1979</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="1640" citStr="Searle, 1979" startWordPosition="243" endWordPosition="244">ch on IM dialogue. We show that IM dialogue has some unique problems and attributes not found in transcribed spoken dialogue, which has been the focus of most work in discourse modelling. The present study addresses the problems presented by these differences when modelling dialogue acts in IM dialogue. Stolcke et al. (2000) point out that the use of dialogue acts is a useful first level of analysis for describing discourse structure. Dialogue acts are based on the illocutionary force of an utterance from speech act theory, and represent acts such as assertions and declarations (Austin, 1962; Searle, 1979). 79 This theory has been extended in dialogue acts to model the conversational functions that utterances can perform. Dialogue acts have been used to benefit tasks such as machine translation (Tanaka and Yokoo, 1999) and the automatic detection of dialogue games (Levin et al., 1999). This deeper level of discourse understanding may help replace or assist a support representative using IM dialogue by suggesting responses that are more sophisticated and realistic to a human dialogue participant. The unique problems and attributes exhibited by IM dialogue prohibit existing dialogue act classific</context>
</contexts>
<marker>Searle, 1979</marker>
<rawString>John R. Searle. 1979. Expression and Meaning: Studies in the Theory of Speech Acts. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1988</date>
<note>McGrawHill, second edition.</note>
<contexts>
<context position="9116" citStr="Siegel and Castellan, 1988" startWordPosition="1481" endWordPosition="1484">orpus using tags that seemed appropriate from the 42 tags used by Stolcke et al. (2000) based on the Dialog Act Markup in Several Layers (DAMSL) tag set (Core and Allen, 1997). Some tags, such as UNINTERPRETABLE and SELF-TALK, were eliminated as they are not relevant for typed dialogue. Tags that were difficult to distinguish, given the types of utterances in our corpus, were collapsed into one tag. For example, NO ANSWERS, REJECT, and NEGATIVE NON-NO ANSWERS are all represented by NOANSWER in our tag set. The Kappa statistic was used to compare interannotator agreement normalised for chance (Siegel and Castellan, 1988). Labelling was carried out by three computational linguistics graduate students with 89% agreement resulting in a Kappa statistic of 0.87, which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set (Carletta, 1996). A complete list of the 12 dialogue acts we used is shown in Table 2 along with examples and the frequency of each dialogue act in our corpus. Tag Example % STATEMENT I am sending you the page now 36.0 THANKING Thank you for contacting us 14.7 YES-NO- Did you receive the page? 13.9 QUESTION RESPONSE-ACK Sure 7.2 REQUEST Please let me </context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and N. John Castellan, Jr. 1988. Nonparametric statistics for the behavioral sciences. McGrawHill, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Noah Coccaro</author>
<author>Rebecca Bates</author>
<author>Paul Taylor</author>
<author>Carol Van Ess-Dykema</author>
<author>Klaus Ries</author>
<author>Elizabeth Shriberg</author>
<author>Daniel Jurafsky</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Coccaro, Bates, Taylor, Van Ess-Dykema, Ries, Shriberg, Jurafsky, Martin, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul Taylor, Carol Van Ess-Dykema, Klaus Ries, Elizabeth Shriberg, Daniel Jurafsky, Rachel Martin, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Tanaka</author>
<author>Akio Yokoo</author>
</authors>
<title>An efficient statistical speech act type tagging system for speech translation systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th conference on Association for Computational Linguistics,</booktitle>
<pages>381--388</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1857" citStr="Tanaka and Yokoo, 1999" startWordPosition="276" endWordPosition="279">ddresses the problems presented by these differences when modelling dialogue acts in IM dialogue. Stolcke et al. (2000) point out that the use of dialogue acts is a useful first level of analysis for describing discourse structure. Dialogue acts are based on the illocutionary force of an utterance from speech act theory, and represent acts such as assertions and declarations (Austin, 1962; Searle, 1979). 79 This theory has been extended in dialogue acts to model the conversational functions that utterances can perform. Dialogue acts have been used to benefit tasks such as machine translation (Tanaka and Yokoo, 1999) and the automatic detection of dialogue games (Levin et al., 1999). This deeper level of discourse understanding may help replace or assist a support representative using IM dialogue by suggesting responses that are more sophisticated and realistic to a human dialogue participant. The unique problems and attributes exhibited by IM dialogue prohibit existing dialogue act classification methods from being applied directly. We present solutions to some of these problems along with methods to obtain high accuracy in automated dialogue act classification. A statistical discourse model is trained a</context>
</contexts>
<marker>Tanaka, Yokoo, 1999</marker>
<rawString>Hideki Tanaka and Akio Yokoo. 1999. An efficient statistical speech act type tagging system for speech translation systems. In Proceedings of the 37th conference on Association for Computational Linguistics, pages 381–388. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>