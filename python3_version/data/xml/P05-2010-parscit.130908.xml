<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003288">
<title confidence="0.989811">
Using Readers to Identify Lexical Cohesive Structures in Texts
</title>
<author confidence="0.978365">
Beata Beigman Klebanov
</author>
<affiliation confidence="0.843925">
School of Computer Science and Engineering
The Hebrew University of Jerusalem
Jerusalem, 91904, Israel
</affiliation>
<email confidence="0.99716">
beata@cs.huji.ac.il
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935">
This paper describes a reader-based exper-
iment on lexical cohesion, detailing the
task given to readers and the analysis of
the experimental data. We conclude with
discussion of the usefulness of the data in
future research on lexical cohesion.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936695652174">
The quest for finding what it is that makes an ordered
list of linguistic forms into a text that is fluently read-
able by people dates back at least to Halliday and
Hasan’s (1976) seminal work on textual cohesion.
They identified a number of cohesive constructions:
repetition (using the same words, or via repeated
reference, substitution and ellipsis), conjunction and
lexical cohesion.
Some of those structures - for example, cohesion
achieved through repeated reference - have been
subjected to reader based tests, often while trying to
produce gold standard data for testing computational
models, a task requiring sufficient inter-annotator
agreement (Hirschman et al., 1998; Mitkov et al.,
2000; Poesio and Vieira, 1998).
Experimental investigation of lexical cohesion is
an emerging enterprise (Morris and Hirst, 2005) to
which the current study contributes. We present our
version of the question to the reader to which lexi-
cal cohesion patterns are an answer (section 2), de-
scribe an experiment on 22 readers using this ques-
tion (section 3), and analyze the experimental data
(section 4).
</bodyText>
<page confidence="0.991396">
55
</page>
<sectionHeader confidence="0.949834" genericHeader="method">
2 From Lexical Cohesion to Anchoring
</sectionHeader>
<bodyText confidence="0.999900322580645">
Cohesive ties between items in a text draw on the
resources of a language to build up the text’s unity
(Halliday and Hasan, 1976). Lexical cohesive ties
draw on the lexicon, i.e. word meanings.
Sometimes the relation between the members of
a tie is easy to identify, like near-synonymy (dis-
ease/illness), complementarity (boy/girl), whole-to-
part (box/lid), but the bulk of lexical cohesive tex-
ture is created by relations that are difficult to clas-
sify (Morris and Hirst, 2004). Halliday and Hasan
(1976) exemplify those with pairs like dig/garden,
ill/doctor, laugh/joke, which are reminiscent of the
idea of scripts (Schank and Abelson, 1977) or
schemata (Rumelhart, 1984): certain things are ex-
pected in certain situations, the paradigm example
being menu, tables, waiters and food in a restaurant.
However, texts sometimes start with descriptions
of situations where many possible scripts could ap-
ply. Consider a text starting with Mother died to-
day.1 What are the generated expectations? A de-
scription of an accident that led to the death, or of
a long illness? A story about what happened to the
rest of the family afterwards? Or emotional reac-
tion of the speaker - like the sense of loneliness in
the world? Or something more ”technical” - about
the funeral, or the will? Or something about the
mother’s last wish and its fulfillment? Many direc-
tions are easily thinkable at this point.
We suggest that rather than generating predic-
tions, scripts/schemata could provide a basis for
abduction. Once any ”normal” direction is ac-
</bodyText>
<footnote confidence="0.983539">
1the opening sentence of A. Camus’ The Stranger
</footnote>
<note confidence="0.657104">
Proceedings of the ACL Student Research Workshop, pages 55–60,
</note>
<page confidence="0.459864">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<bodyText confidence="0.999820909090909">
tually taken up by the following text, there is a
connection back to whatever makes this a normal
direction, according to the reader’s commonsense
knowledge (possibly coached in terms of scripts or
schemata). Thus, had the text developed the ill-
ness line, one would have known that it can be
best explained-by/blamed-upon/abduced-to the pre-
viously mentioned lethal outcome. We say in this
case that illness is anchored by died, and mark it
illness died; we aim to elicit such anchoring rela-
tions from the readers.
</bodyText>
<sectionHeader confidence="0.997636" genericHeader="method">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.961005915254237">
We chose 10 texts for the experiment: 3 news ar-
ticles, 4 items of journalistic writing, and 3 fiction
pieces. All news and one fiction story were taken in
full; others were cut at a meaningful break to stay
within 1000 word limit. The texts were in English -
original language for all but two texts.
Our subjects were 22 students at the Hebrew Uni-
versity of Jerusalem, Israel; 19 undergraduates and
3 graduates, all aged 21-29 years, studying various
subjects - Engineering, Cognitive Science, Biology,
History, Linguistics, Psychology, etc. Three of the
participants named English their mother tongue; the
rest claimed very high proficiency in English. Peo-
ple were paid for participation.
All participants were first asked to read the guide-
lines that contained an extensive example of an an-
notation done by us on a 4-paragraph text (a small
extract is shown in table 1), and short paragraphs
highlighting various issues, like the possibility of
multiple anchors per item (see table 1) and of multi-
word anchors (Scientific or American alone do not
anchor editor, but taken together they do).
In addition, the guidelines stressed the importance
of separation between general and personal knowl-
edge, and between general and instantial relations.
For the latter case, an example was given of a story
about children who went out in a boat with their fa-
ther who was an experienced sailor, with an explana-
tion that whereas fatherchildren and sailorboat
are based on general commonsense knowedge, the
connection between sailor and father is not some-
thing general but is created in the particular case be-
cause the two descriptions apply to the same person;
people were asked not to mark such relations.
Afterwards, the participants performed a trial an-
notation on a short news story, after which meetings
in small groups were held for them to bring up any
questions and comments2.
The Federal Aviation Administration underestimated
the number of aircraft flying over the Pantex Weapons Plant
outside Amarillo, Texas, where much of the nation’s surplus
plutonium is stored, according to computerized studies
under way by the Energy Department.
the whereamarillo texas outside
federal much
aviation nationfederal
administrationfederal surplus
underestimated plutoniumweapons
numberunderestimated is
of storedsurplus
aircraftaviation according
flyingaircraft aviation to
overflying computerized
pantex studiesunderestimated
weapons under
plant way
outside by
amarillo energyplutonium
texasfederal departmentadministration
</bodyText>
<tableCaption confidence="0.987457">
Table 1: Example Annotation from the Guidelines
</tableCaption>
<bodyText confidence="0.984390222222222">
(extract). x c d means each of c and d is an
anchor for x.
The experiment then started. For each of the 10
texts, each person was given the text to read, and
a separate wordlist on which to write down annota-
tions. The wordlist contained words from the text,
in their appearance order, excluding verbatim and
inflectional repetitions3. People were instructed to
read the text first, and then go through the wordlist
and ask themselves, for every item on the list, which
previously mentioned items help the easy accommo-
dation of this concept into the evolving story, if in-
deed it is easily accommodated, based on the com-
monsense knowledge as it is perceived by the anno-
tator. People were encouraged to use a dictionary if
they were not sure about some nuance of meaning.
Wordlist length per text ranged from 175 to 339
items; annotation of one text took a person 70 min-
2The guidelines and all the correspondence with the partici-
pants is archived and can be provided upon request.
3The exclusion was done mainly to keep the lists to reason-
able length while including as many newly mentioned items as
possible. We conjectured that repetitions are usually anchored
by the previous mention; this assumption is a simplification,
since sometimes the same form is used in a somewhat different
sense and may get anchored separately from the previous use of
this form. This issue needs further experimental investigation.
</bodyText>
<page confidence="0.981185">
56
</page>
<bodyText confidence="0.978106">
utes on average (each annotator was timed on two
texts; every text was timed for 2-4 annotators).
</bodyText>
<sectionHeader confidence="0.688381" genericHeader="method">
4 Analysis of Experimental Data
</sectionHeader>
<bodyText confidence="0.999901705882353">
Most of the existing research in computational lin-
guistics that uses human annotators is within the
framework of classification, where an annotator de-
cides, for every test item, on an appropriate tag out
of the pre-specified set of tags (Poesio and Vieira,
1998; Webber and Byron, 2004; Hearst, 1997; Mar-
cus et al., 1993).
Although our task is not that of classification, we
start from a classification sub-task, and use agree-
ment figures to guide subsequent analysis. We use
the by now standard statistic (Di Eugenio and
Glass, 2004; Carletta, 1996; Marcu et al., 1999;
Webber and Byron, 2004) to quantify the degree of
above-chance agreement between multiple annota-
tors, and the statistic for analysis of sources of
unreliability (Krippendorff, 1980). The formulas for
the two statistics are given in appendix A.
</bodyText>
<subsectionHeader confidence="0.979849">
4.1 Classification Sub-Task
</subsectionHeader>
<bodyText confidence="0.999971095238095">
Classifying items into anchored/unanchored can be
viewed as a sub-task of our experiment: before writ-
ing any particular item as an anchor, the annotator
asked himself whether the concept at hand is easy
to accommodate at all. Getting reliable data on this
task is therefore a pre-condition for asking any ques-
tions about the anchors. Agreement on this task av-
erages for the 10 texts. These reliability
figures do not reach the area which is the
accepted threshold for deciding that annotators were
working under similar enough internalized theories4
of the phenomenon; however, the figures are high
enough to suggest considerable overlaps.
Seeking more detailed insight into the degree of
similarity of the annotators’ ideas of the task, we
follow the procedure described in (Krippendorff,
1980) to find outliers. We calculate the category-
by-category co-markup matrix for all annotators5;
then for all but one annotators, and by subtraction
find the portion that is due to this one annotator.
We then regard the data as two-annotator data (one
</bodyText>
<footnote confidence="0.992562333333333">
4whatever annotators think the phenomenon is after having
read the guidelines
5See formula 7 in appendix A.
</footnote>
<bodyText confidence="0.999201142857143">
vs. everybody else), and calculate agreement coef-
ficients. We rank annotators (1 to 22) according to
the degree of agreement with the rest, separately for
each text, and average over the texts to obtain the
conformity rank of an annotator. The lower the rank,
the less compliant the annotator.
Annotators’ conformity ranks cluster into 3
groups described in table 2. The two members of
group A are consistent outliers - their average rank
for the 10 texts is below 2. The second group (B)
is, on average, in the bottom half of the annota-
tors with respect to agreement with the common,
whereas members of group C display relatively high
conformity.
</bodyText>
<table confidence="0.979514">
Gr Size Ranks Agr. within group ()
A 2 1.7 - 1.9 0.55
B 9 5.8 - 10.4 0.41
C 11 13.6 - 18.3 0.54
</table>
<tableCaption confidence="0.998241">
Table 2: Groups of annotators, by conformity ranks.
</tableCaption>
<bodyText confidence="0.999954346153846">
It is possible that annotators in groups A, B and C
have alternative interpretations of the guidelines, but
our idea of the ”common” (and thus the conformity
ranks) is dominated by the largest group, C. Within-
group agreement rates shown in table 2 suggest that
two annotators in group A do indeed have an alter-
native understanding of the task, being much better
correlated between each other than with the rest.
The figures for the other two groups could sup-
port two scenarios: (1) each group settled on a dif-
ferent theory of the phenomenon, where group C is
in better agreement on its version that group B on
its own; (2) people in groups B and C have basically
the same theory, but members of C are more sys-
tematic in carrying it through. It is crucial for our
analysis to tell those apart - in the case of multiple
stable interpretations it is difficult to talk about the
anchoring phenomenon; in the core-periphery case,
there is hope to identify the core emerging from 20
out of 22 annotations.
Let us call the set of majority opinions on a list of
items an interpretation of the group, and let us call
the average majority percentage consistency. Thus,
if all decisions of a 9 member group were almost
unanimous, the consistency of the group is 8/9 =
89%, whereas if every time there was a one vote
</bodyText>
<page confidence="0.992916">
57
</page>
<bodyText confidence="0.999991181818182">
edge to the winning decision, the consistency was
5/9=56%. The more consistent the interpretation
given by a group, the higher its agreement coeffi-
cient.
If groups B and C have different interpretations,
adding a person p from group C to group B would
usually not improve the consistency of the target
group (B), since p is likely to represent majority
opinion of a group with a different interpretation.
On the other hand, if the two groups settled on
basically the same interpretation, the difference in
ranks reflects difference in consistency. Then mov-
ing p from C to B would usually improve the con-
sistency in B, since, coming from a more consistent
group, p’s agreement with the interpretation is ex-
pected to be better than that of an average member
of group B, so the addition strengthens the majority
opinion in B6.
We performed this analysis on groups A and C
with respect to group B. Adding members of group
A to group B improved the agreement in group B
only for 1 out of the 10 texts. Thus, the relation-
ship between the two groups seems to be that of dif-
ferent interpretations. Adding members of group C
to group B resulted in improvement in agreement in
at least 7 out of 10 texts for every added member.
Thus, the difference between groups B and C is that
of consistency, not of interpretation; we may now
search for the well-agreed-upon core of this inter-
pretation. We exclude members of group A from
subsequent analysis; the remaining group of 20 an-
notators exhibits an average agreement of
on anchored/unanchored classification.
</bodyText>
<subsectionHeader confidence="0.997703">
4.2 Finding the Common Core
</subsectionHeader>
<bodyText confidence="0.999761875">
The next step is finding a reliably classified subset of
the data. We start with the most agreed upon items -
those classified as anchored or non-anchored by all
the 20 people, then by 19, 18, etc., testing, for ev-
ery such inclusion, that the chances of taking in in-
stances of chance agreement are small enough. This
means performing a statistical hypothesis test: with
how much confidence can we reject the hypothesis
</bodyText>
<footnote confidence="0.8967105">
6Experiments with synthetic data confirm this analysis: with
20 annotations split into 2 sets of sizes 9 and 11, it is possible
</footnote>
<figureCaption confidence="0.57760225">
to get an overall agreement of about either with 75%
and 90% consistency on the same interpretation, or with 90%
and 95% consistency on two interpretations with induced (i.e.
non-random) overlap of just 20%.
</figureCaption>
<bodyText confidence="0.99924">
that certain agreement level7 is due to chance. Con-
fidence level of is achieved including items
marked by at least 13 out of 20 people and items
unanimously left unmarked.8
The next step is identifying trustworthy anchors
for the reliably anchored items. We calculated av-
erage anchor strength for every text: the number of
people who wrote the same anchor for a given item,
averaged on all reliably anchored items in a text. Av-
erage anchor strength ranges between 5 and 7 in dif-
ferent texts. Taking only strong anchors (anchors of
at least the average strength), we retain about 25%
of all anchors assigned to anchored items in the reli-
able subset. In total, there are 1261 pairs of reliably
anchored items with their strong anchors, between
54 and 205 per text.
Strength cut-off is a heuristic procedure; some of
those anchors were marked by as few as 6 or 7 out
of 20 people, so it is not clear whether they can be
trusted as embodiments of the core of the anchoring
phenomenon in the analyzed texts. Consequently, an
anchor validation procedure is needed.
</bodyText>
<subsectionHeader confidence="0.999862">
4.3 Validating the Common Core
</subsectionHeader>
<bodyText confidence="0.931152730769231">
We observe that although people were asked to mark
all anchors for every item they thought was an-
chored, they actually produced only 1.86 anchors
per anchored item. Thus, people were most con-
cerned with finding an anchor, i.e. making sure that
something they think is easily accommodatable is
given at least one preceding item to blame for that;
they were less diligent in marking up all such items.
This is also understandable processing-wise; after a
scrupulous read of the text, coming up with one or
two anchors can be done from memory, only occa-
sionally going back to the text; putting down all an-
chors would require systematic scanning of the pre-
vious stretch of text for every item on the list; the
latter task is hardly doable in 70 minutes.
7A random variable ranging between 0 and 20 says how
many “random” people marked an item as anchored. We model
“random” versions of annotators by taking the proportions
of items marked as anchored by annotator in the whole of the
dataset, and assuming that for every word, the person was toss-
ing a coin with P(heads) = , independently for every word.
8Confidence level of allows augmenting the set
of reliably unanchored items with those marked by 1 or 2 peo-
ple, retaining the same cutoff for anchoredness. This cut covers
more than 60% of the data, and contains 1504 items, 538 of
which are anchored.
</bodyText>
<page confidence="0.996632">
58
</page>
<bodyText confidence="0.9999854375">
Having in mind the difficulty of producing an ex-
haustive list of anchors for every item, we conducted
a follow-up experiment to see whether people would
accept anchors when those are presented to them, as
opposed to generating ones. We used 6 out of the
10 texts and 17 out of 20 annotators for the follow-
up experiment. Each person did 3 text, each texts
received 7-9 annotations of this kind.
For each text, the reader was presented with the
same list of words as in the first part, only now each
word was accompanied by a list of anchors. For each
item, every anchor generated by at least one person
was included; the order of the anchors had no corre-
spondence with the number of people who generated
it. A small number of items also received a random
anchor – a randomly chosen word from the preced-
ing part of the wordlist. The task was crossing over
anchors that the person does not agree with.
Ideally, i.e. if lack of markup is merely a dif-
ference in attention but not in judgment, all non-
random anchors should be accepted. To see the dis-
tance of the actual results from this scenario, we cal-
culate the total mass of votes as number of anchored-
anchor pairs times number of people, and check
how many are accept votes. For all non-random
pairs, 62% were accept votes; for the core annota-
tions (pairs of reliably anchored items with strong
anchors) 94% were accept votes, texts ranging be-
tween 90% and 96%; for pairs with a random an-
chor, only 15% were accept votes. Thus, agreement
based analysis of anchor generation data allowed us
to identify a highly valid portion of the annotations.
</bodyText>
<sectionHeader confidence="0.993859" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999975">
This paper presented a reader-based experiment on
finding lexical cohesive patterns in texts. As it often
happens with tasks related to semantics/pragmatics
(Poesio and Vieira, 1998; Morris and Hirst, 2005),
the inter-reader agreement levels did not reach the
accepted reliability thresholds. We showed, how-
ever, that statistical analysis of the data, in conjunc-
tion with a subsequent validation experiment, allow
identification of a reliably annotated core of the phe-
nomenon.
The core data may now be used in various ways.
First, it can seed psycholinguistic experimentation
of lexical cohesion: are anchored items processed
quicker than unanchored ones? When asked to re-
call the content of a text, would people remember
prolific anchors of this text? Such experiments will
further our understanding of the nature of text-reader
interaction and help improve applications like text
generation and summarization.
Second, it can serve as a minimal test data for
computational models of lexical cohesion: any good
model should at least get the core part right. Much
of the existing applied research on lexical cohesion
uses WordNet-based (Miller, 1990) lexical chains to
identify the cohesive texture for a larger text pro-
cessing application (Barzilay and Elhadad, 1997;
Stokes et al., 2004; Moldovan and Novischi, 2002;
Al-Halimi and Kazman, 1998). We can now subject
these putative chains to a direct test; in fact, this is
the immediate future research direction.
In addition, analysis techniques discussed in the
paper – separating interpretation disagreement from
difference in consistency, using statistical hypoth-
esis testing to find reliable parts of the annota-
tions and validating them experimentally – may be
applied to data resulting from other kinds of ex-
ploratory experiments to gain insights about the phe-
nomena at hand.
</bodyText>
<sectionHeader confidence="0.953331" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999743">
I would like to thank Prof. Eli Shamir for guidance
and numerous discussions.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.9978919375">
Reem Al-Halimi and Rick Kazman. 1998. Temporal in-
dexing through lexical chaining. In C. Fellbaum, ed-
itor, WordNet: An Electronic Lexical Database, pages
333–351. MIT Press, Cambridge, MA.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings
of the ACL Intelligent Scalable Text Summarization
Workshop, pages 86–90.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguis-
tics, 22(2):249–254.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Computational Linguistics,
30(1):95–101.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman Group Ltd.
</reference>
<page confidence="0.993083">
59
</page>
<reference confidence="0.999470966101695">
Marti Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Lynette Hirschman, Patricia Robinson, John D. Burger,
and Marc Vilain. 1998. Automating coreference:
The role of annotated training data. CoRR, cmp-
lg/9803001.
Klaus Krippendorff. 1980. Content Analysis. Sage Pub-
lications.
Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a corpus
of discourse trees. In Proceedings of ACL’99 Work-
shop on Standards and Tools for Discourse Tagging,
pages 48–57.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313 – 330.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal ofLexicography, 3(4):235–312.
Ruslan Mitkov, Richard Evans, Constantin Orasan,
Catalina Barbu, Lisa Jones, and Violeta Sotirova.
2000. Coreference and anaphora: developing anno-
tating tools, annotated resources and annotation strate-
gies. In Proceedings of the Discourse Anaphora and
Anaphora Resolution Colloquium (DAARC’2000),
pages 49–58.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING 2002.
Jane Morris and Graeme Hirst. 2004. Non-classical lexi-
cal semantic relations. In Proceedings ofHLT-NAACL
Workshop on Computational Lexical Semantics.
Jane Morris and Graeme Hirst. 2005. The subjectivity
of lexical cohesion in text. In James C. Chanahan,
Yan Qu, and Janyce Wiebe, editors, Computing atti-
tude and affect in text. Springer, Dodrecht, The Nether-
lands.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183–216.
David E. Rumelhart. 1984. Understanding under-
standing. In J. Flood, editor, Understanding Reading
Comprehension, pages 1–20. Delaware: International
Reading Association.
Roger Schank and Robert Abelson. 1977. Scripts, plans,
goals, and understanding: An inquiry into human
knowledge structures. Hillsdale, NJ: Lawrence Erl-
baum.
Sidney Siegel and John N. Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw
Hill, Boston, MA.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004.
Select: A lexical cohesion based news story segmenta-
tion system. Journal ofAI Communications, 17(1):3–
12.
Bonny Webber and Donna Byron, editors. 2004. Pro-
ceedings of the ACL-2004 Workshop on Discourse An-
notation, Barcelona, Spain, July.
</reference>
<sectionHeader confidence="0.441667" genericHeader="method">
A Measures of Agreement
</sectionHeader>
<bodyText confidence="0.999922769230769">
Letbe the number of items to be classified;
- the number of categories to classify into;- the
number of raters;is the number of annotators
who assigned the i-th item to j-th category. We
use Siegel and Castellan’s (1988) version of ; al-
though it assumes similar distributions of categories
across coders in that it uses the average to estimate
the expected agreement (see equation 2), the cur-
rent experiment employs 22 coders, so averaging is a
much better justified enterprise than in studies with
very few coders (2-4), typical in discourse annota-
tion work (Di Eugenio and Glass, 2004). The calcu-
lation of thestatistic follows (Krippendorff, 1980).
</bodyText>
<subsectionHeader confidence="0.494026">
The Statistic
</subsectionHeader>
<bodyText confidence="0.746005">
The Statistic
</bodyText>
<page confidence="0.977066">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930966">
<title confidence="0.999988">Using Readers to Identify Lexical Cohesive Structures in Texts</title>
<author confidence="0.998386">Beata Beigman Klebanov</author>
<affiliation confidence="0.9944505">School of Computer Science and Engineering The Hebrew University of Jerusalem</affiliation>
<address confidence="0.999917">Jerusalem, 91904, Israel</address>
<email confidence="0.99922">beata@cs.huji.ac.il</email>
<abstract confidence="0.991847857142857">This paper describes a reader-based experiment on lexical cohesion, detailing the task given to readers and the analysis of the experimental data. We conclude with discussion of the usefulness of the data in future research on lexical cohesion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Reem Al-Halimi</author>
<author>Rick Kazman</author>
</authors>
<title>Temporal indexing through lexical chaining.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database,</booktitle>
<pages>333--351</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Al-Halimi, Kazman, 1998</marker>
<rawString>Reem Al-Halimi and Rick Kazman. 1998. Temporal indexing through lexical chaining. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 333–351. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Intelligent Scalable Text Summarization Workshop,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="19725" citStr="Barzilay and Elhadad, 1997" startWordPosition="3285" endWordPosition="3288">nanchored ones? When asked to recall the content of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of the nature of text-reader interaction and help improve applications like text generation and summarization. Second, it can serve as a minimal test data for computational models of lexical cohesion: any good model should at least get the core part right. Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998). We can now subject these putative chains to a direct test; in fact, this is the immediate future research direction. In addition, analysis techniques discussed in the paper – separating interpretation disagreement from difference in consistency, using statistical hypothesis testing to find reliable parts of the annotations and validating them experimentally – may be applied to data resulting from other kinds of exploratory experiments to gain insights about the phenomena at hand. Acknowledgment I would like to tha</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the ACL Intelligent Scalable Text Summarization Workshop, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="8528" citStr="Carletta, 1996" startWordPosition="1358" endWordPosition="1359">t was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of our experiment: before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Getting reliable data on this task is therefore a pre-condition for asking any q</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: a second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Barbara Di Eugenio and Michael Glass. 2004. The kappa statistic: a second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group Ltd.</publisher>
<contexts>
<context position="1744" citStr="Halliday and Hasan, 1976" startWordPosition="268" endWordPosition="271">tor agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998). Experimental investigation of lexical cohesion is an emerging enterprise (Morris and Hirst, 2005) to which the current study contributes. We present our version of the question to the reader to which lexical cohesion patterns are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). 55 2 From Lexical Cohesion to Anchoring Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (disease/illness), complementarity (boy/girl), whole-topart (box/lid), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris and Hirst, 2004). Halliday and Hasan (1976) exemplify those with pairs like dig/garden, ill/doctor, laugh/joke, which are reminiscent of the idea of scripts (Schank and Abelson, 1977) or schemata (Rumelhart, 1984): certain things are expected in certain situations, th</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman Group Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="8278" citStr="Hearst, 1997" startWordPosition="1316" endWordPosition="1317">etimes the same form is used in a somewhat different sense and may get anchored separately from the previous use of this form. This issue needs further experimental investigation. 56 utes on average (each annotator was timed on two texts; every text was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Patricia Robinson</author>
<author>John D Burger</author>
<author>Marc Vilain</author>
</authors>
<title>Automating coreference: The role of annotated training data. CoRR,</title>
<date>1998</date>
<contexts>
<context position="1156" citStr="Hirschman et al., 1998" startWordPosition="169" endWordPosition="172">ist of linguistic forms into a text that is fluently readable by people dates back at least to Halliday and Hasan’s (1976) seminal work on textual cohesion. They identified a number of cohesive constructions: repetition (using the same words, or via repeated reference, substitution and ellipsis), conjunction and lexical cohesion. Some of those structures - for example, cohesion achieved through repeated reference - have been subjected to reader based tests, often while trying to produce gold standard data for testing computational models, a task requiring sufficient inter-annotator agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998). Experimental investigation of lexical cohesion is an emerging enterprise (Morris and Hirst, 2005) to which the current study contributes. We present our version of the question to the reader to which lexical cohesion patterns are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). 55 2 From Lexical Cohesion to Anchoring Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical co</context>
</contexts>
<marker>Hirschman, Robinson, Burger, Vilain, 1998</marker>
<rawString>Lynette Hirschman, Patricia Robinson, John D. Burger, and Marc Vilain. 1998. Automating coreference: The role of annotated training data. CoRR, cmplg/9803001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis.</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="8731" citStr="Krippendorff, 1980" startWordPosition="1388" endWordPosition="1389">e an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of our experiment: before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Getting reliable data on this task is therefore a pre-condition for asking any questions about the anchors. Agreement on this task averages for the 10 texts. These reliability figures do not reach the area which is the accepted threshold for deciding that annotators were working und</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis. Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Estibaliz Amorrortu</author>
<author>Magdalena Romera</author>
</authors>
<title>Experiments in constructing a corpus of discourse trees.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL’99 Workshop on Standards and Tools for Discourse Tagging,</booktitle>
<pages>48--57</pages>
<contexts>
<context position="8548" citStr="Marcu et al., 1999" startWordPosition="1360" endWordPosition="1363">2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of our experiment: before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Getting reliable data on this task is therefore a pre-condition for asking any questions about the a</context>
</contexts>
<marker>Marcu, Amorrortu, Romera, 1999</marker>
<rawString>Daniel Marcu, Estibaliz Amorrortu, and Magdalena Romera. 1999. Experiments in constructing a corpus of discourse trees. In Proceedings of ACL’99 Workshop on Standards and Tools for Discourse Tagging, pages 48–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>330</pages>
<contexts>
<context position="8300" citStr="Marcus et al., 1993" startWordPosition="1318" endWordPosition="1322">e form is used in a somewhat different sense and may get anchored separately from the previous use of this form. This issue needs further experimental investigation. 56 utes on average (each annotator was timed on two texts; every text was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items into anchored/unanchored can be viewed as a sub-task of our </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313 – 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal ofLexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="19608" citStr="Miller, 1990" startWordPosition="3269" endWordPosition="3270">n seed psycholinguistic experimentation of lexical cohesion: are anchored items processed quicker than unanchored ones? When asked to recall the content of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of the nature of text-reader interaction and help improve applications like text generation and summarization. Second, it can serve as a minimal test data for computational models of lexical cohesion: any good model should at least get the core part right. Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998). We can now subject these putative chains to a direct test; in fact, this is the immediate future research direction. In addition, analysis techniques discussed in the paper – separating interpretation disagreement from difference in consistency, using statistical hypothesis testing to find reliable parts of the annotations and validating them experimentally – may be applied to data resulting from ot</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An on-line lexical database. International Journal ofLexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Richard Evans</author>
<author>Constantin Orasan</author>
<author>Catalina Barbu</author>
<author>Lisa Jones</author>
<author>Violeta Sotirova</author>
</authors>
<title>Coreference and anaphora: developing annotating tools, annotated resources and annotation strategies.</title>
<date>2000</date>
<booktitle>In Proceedings of the Discourse Anaphora and Anaphora Resolution Colloquium (DAARC’2000),</booktitle>
<pages>49--58</pages>
<contexts>
<context position="1177" citStr="Mitkov et al., 2000" startWordPosition="173" endWordPosition="176">into a text that is fluently readable by people dates back at least to Halliday and Hasan’s (1976) seminal work on textual cohesion. They identified a number of cohesive constructions: repetition (using the same words, or via repeated reference, substitution and ellipsis), conjunction and lexical cohesion. Some of those structures - for example, cohesion achieved through repeated reference - have been subjected to reader based tests, often while trying to produce gold standard data for testing computational models, a task requiring sufficient inter-annotator agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998). Experimental investigation of lexical cohesion is an emerging enterprise (Morris and Hirst, 2005) to which the current study contributes. We present our version of the question to the reader to which lexical cohesion patterns are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). 55 2 From Lexical Cohesion to Anchoring Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on t</context>
</contexts>
<marker>Mitkov, Evans, Orasan, Barbu, Jones, Sotirova, 2000</marker>
<rawString>Ruslan Mitkov, Richard Evans, Constantin Orasan, Catalina Barbu, Lisa Jones, and Violeta Sotirova. 2000. Coreference and anaphora: developing annotating tools, annotated resources and annotation strategies. In Proceedings of the Discourse Anaphora and Anaphora Resolution Colloquium (DAARC’2000), pages 49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adrian Novischi</author>
</authors>
<title>Lexical chains for question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="19775" citStr="Moldovan and Novischi, 2002" startWordPosition="3293" endWordPosition="3296">of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of the nature of text-reader interaction and help improve applications like text generation and summarization. Second, it can serve as a minimal test data for computational models of lexical cohesion: any good model should at least get the core part right. Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998). We can now subject these putative chains to a direct test; in fact, this is the immediate future research direction. In addition, analysis techniques discussed in the paper – separating interpretation disagreement from difference in consistency, using statistical hypothesis testing to find reliable parts of the annotations and validating them experimentally – may be applied to data resulting from other kinds of exploratory experiments to gain insights about the phenomena at hand. Acknowledgment I would like to thank Prof. Eli Shamir for guidance and numerous disc</context>
</contexts>
<marker>Moldovan, Novischi, 2002</marker>
<rawString>Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Non-classical lexical semantic relations.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL Workshop on Computational Lexical Semantics.</booktitle>
<contexts>
<context position="2092" citStr="Morris and Hirst, 2004" startWordPosition="322" endWordPosition="325"> experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). 55 2 From Lexical Cohesion to Anchoring Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (disease/illness), complementarity (boy/girl), whole-topart (box/lid), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris and Hirst, 2004). Halliday and Hasan (1976) exemplify those with pairs like dig/garden, ill/doctor, laugh/joke, which are reminiscent of the idea of scripts (Schank and Abelson, 1977) or schemata (Rumelhart, 1984): certain things are expected in certain situations, the paradigm example being menu, tables, waiters and food in a restaurant. However, texts sometimes start with descriptions of situations where many possible scripts could apply. Consider a text starting with Mother died today.1 What are the generated expectations? A description of an accident that led to the death, or of a long illness? A story ab</context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>Jane Morris and Graeme Hirst. 2004. Non-classical lexical semantic relations. In Proceedings ofHLT-NAACL Workshop on Computational Lexical Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>The subjectivity of lexical cohesion in text.</title>
<date>2005</date>
<editor>In James C. Chanahan, Yan Qu, and Janyce Wiebe, editors,</editor>
<publisher>Springer, Dodrecht, The Netherlands.</publisher>
<contexts>
<context position="1302" citStr="Morris and Hirst, 2005" startWordPosition="190" endWordPosition="193">l cohesion. They identified a number of cohesive constructions: repetition (using the same words, or via repeated reference, substitution and ellipsis), conjunction and lexical cohesion. Some of those structures - for example, cohesion achieved through repeated reference - have been subjected to reader based tests, often while trying to produce gold standard data for testing computational models, a task requiring sufficient inter-annotator agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998). Experimental investigation of lexical cohesion is an emerging enterprise (Morris and Hirst, 2005) to which the current study contributes. We present our version of the question to the reader to which lexical cohesion patterns are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). 55 2 From Lexical Cohesion to Anchoring Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (</context>
<context position="18665" citStr="Morris and Hirst, 2005" startWordPosition="3122" endWordPosition="3125"> check how many are accept votes. For all non-random pairs, 62% were accept votes; for the core annotations (pairs of reliably anchored items with strong anchors) 94% were accept votes, texts ranging between 90% and 96%; for pairs with a random anchor, only 15% were accept votes. Thus, agreement based analysis of anchor generation data allowed us to identify a highly valid portion of the annotations. 5 Conclusion This paper presented a reader-based experiment on finding lexical cohesive patterns in texts. As it often happens with tasks related to semantics/pragmatics (Poesio and Vieira, 1998; Morris and Hirst, 2005), the inter-reader agreement levels did not reach the accepted reliability thresholds. We showed, however, that statistical analysis of the data, in conjunction with a subsequent validation experiment, allow identification of a reliably annotated core of the phenomenon. The core data may now be used in various ways. First, it can seed psycholinguistic experimentation of lexical cohesion: are anchored items processed quicker than unanchored ones? When asked to recall the content of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of th</context>
</contexts>
<marker>Morris, Hirst, 2005</marker>
<rawString>Jane Morris and Graeme Hirst. 2005. The subjectivity of lexical cohesion in text. In James C. Chanahan, Yan Qu, and Janyce Wiebe, editors, Computing attitude and affect in text. Springer, Dodrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Renata Vieira</author>
</authors>
<title>A corpusbased investigation of definite description use.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="1203" citStr="Poesio and Vieira, 1998" startWordPosition="177" endWordPosition="180">luently readable by people dates back at least to Halliday and Hasan’s (1976) seminal work on textual cohesion. They identified a number of cohesive constructions: repetition (using the same words, or via repeated reference, substitution and ellipsis), conjunction and lexical cohesion. Some of those structures - for example, cohesion achieved through repeated reference - have been subjected to reader based tests, often while trying to produce gold standard data for testing computational models, a task requiring sufficient inter-annotator agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998). Experimental investigation of lexical cohesion is an emerging enterprise (Morris and Hirst, 2005) to which the current study contributes. We present our version of the question to the reader to which lexical cohesion patterns are an answer (section 2), describe an experiment on 22 readers using this question (section 3), and analyze the experimental data (section 4). 55 2 From Lexical Cohesion to Anchoring Cohesive ties between items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word mean</context>
<context position="8240" citStr="Poesio and Vieira, 1998" startWordPosition="1308" endWordPosition="1311">n; this assumption is a simplification, since sometimes the same form is used in a somewhat different sense and may get anchored separately from the previous use of this form. This issue needs further experimental investigation. 56 utes on average (each annotator was timed on two texts; every text was timed for 2-4 annotators). 4 Analysis of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993). Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the statistic for analysis of sources of unreliability (Krippendorff, 1980). The formulas for the two statistics are given in appendix A. 4.1 Classification Sub-Task Classifying items </context>
<context position="18640" citStr="Poesio and Vieira, 1998" startWordPosition="3118" endWordPosition="3121">mes number of people, and check how many are accept votes. For all non-random pairs, 62% were accept votes; for the core annotations (pairs of reliably anchored items with strong anchors) 94% were accept votes, texts ranging between 90% and 96%; for pairs with a random anchor, only 15% were accept votes. Thus, agreement based analysis of anchor generation data allowed us to identify a highly valid portion of the annotations. 5 Conclusion This paper presented a reader-based experiment on finding lexical cohesive patterns in texts. As it often happens with tasks related to semantics/pragmatics (Poesio and Vieira, 1998; Morris and Hirst, 2005), the inter-reader agreement levels did not reach the accepted reliability thresholds. We showed, however, that statistical analysis of the data, in conjunction with a subsequent validation experiment, allow identification of a reliably annotated core of the phenomenon. The core data may now be used in various ways. First, it can seed psycholinguistic experimentation of lexical cohesion: are anchored items processed quicker than unanchored ones? When asked to recall the content of a text, would people remember prolific anchors of this text? Such experiments will furthe</context>
</contexts>
<marker>Poesio, Vieira, 1998</marker>
<rawString>Massimo Poesio and Renata Vieira. 1998. A corpusbased investigation of definite description use. Computational Linguistics, 24(2):183–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
</authors>
<title>Understanding understanding.</title>
<date>1984</date>
<booktitle>Understanding Reading Comprehension,</booktitle>
<pages>1--20</pages>
<editor>In J. Flood, editor,</editor>
<publisher>Delaware: International Reading Association.</publisher>
<contexts>
<context position="2289" citStr="Rumelhart, 1984" startWordPosition="352" endWordPosition="353">s of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (disease/illness), complementarity (boy/girl), whole-topart (box/lid), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris and Hirst, 2004). Halliday and Hasan (1976) exemplify those with pairs like dig/garden, ill/doctor, laugh/joke, which are reminiscent of the idea of scripts (Schank and Abelson, 1977) or schemata (Rumelhart, 1984): certain things are expected in certain situations, the paradigm example being menu, tables, waiters and food in a restaurant. However, texts sometimes start with descriptions of situations where many possible scripts could apply. Consider a text starting with Mother died today.1 What are the generated expectations? A description of an accident that led to the death, or of a long illness? A story about what happened to the rest of the family afterwards? Or emotional reaction of the speaker - like the sense of loneliness in the world? Or something more ”technical” - about the funeral, or the w</context>
</contexts>
<marker>Rumelhart, 1984</marker>
<rawString>David E. Rumelhart. 1984. Understanding understanding. In J. Flood, editor, Understanding Reading Comprehension, pages 1–20. Delaware: International Reading Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
<author>Robert Abelson</author>
</authors>
<title>Scripts, plans, goals, and understanding: An inquiry into human knowledge structures.</title>
<date>1977</date>
<location>Hillsdale, NJ: Lawrence Erlbaum.</location>
<contexts>
<context position="2259" citStr="Schank and Abelson, 1977" startWordPosition="346" endWordPosition="349">en items in a text draw on the resources of a language to build up the text’s unity (Halliday and Hasan, 1976). Lexical cohesive ties draw on the lexicon, i.e. word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (disease/illness), complementarity (boy/girl), whole-topart (box/lid), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris and Hirst, 2004). Halliday and Hasan (1976) exemplify those with pairs like dig/garden, ill/doctor, laugh/joke, which are reminiscent of the idea of scripts (Schank and Abelson, 1977) or schemata (Rumelhart, 1984): certain things are expected in certain situations, the paradigm example being menu, tables, waiters and food in a restaurant. However, texts sometimes start with descriptions of situations where many possible scripts could apply. Consider a text starting with Mother died today.1 What are the generated expectations? A description of an accident that led to the death, or of a long illness? A story about what happened to the rest of the family afterwards? Or emotional reaction of the speaker - like the sense of loneliness in the world? Or something more ”technical”</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger Schank and Robert Abelson. 1977. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Hillsdale, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>John N Castellan</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1988</date>
<publisher>McGraw Hill,</publisher>
<location>Boston, MA.</location>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and John N. Castellan. 1988. Nonparametric statistics for the behavioral sciences. McGraw Hill, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Stokes</author>
<author>Joe Carthy</author>
<author>Alan F Smeaton</author>
</authors>
<title>Select: A lexical cohesion based news story segmentation system.</title>
<date>2004</date>
<journal>Journal ofAI Communications,</journal>
<volume>17</volume>
<issue>1</issue>
<pages>12</pages>
<contexts>
<context position="19746" citStr="Stokes et al., 2004" startWordPosition="3289" endWordPosition="3292">o recall the content of a text, would people remember prolific anchors of this text? Such experiments will further our understanding of the nature of text-reader interaction and help improve applications like text generation and summarization. Second, it can serve as a minimal test data for computational models of lexical cohesion: any good model should at least get the core part right. Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998). We can now subject these putative chains to a direct test; in fact, this is the immediate future research direction. In addition, analysis techniques discussed in the paper – separating interpretation disagreement from difference in consistency, using statistical hypothesis testing to find reliable parts of the annotations and validating them experimentally – may be applied to data resulting from other kinds of exploratory experiments to gain insights about the phenomena at hand. Acknowledgment I would like to thank Prof. Eli Shamir f</context>
</contexts>
<marker>Stokes, Carthy, Smeaton, 2004</marker>
<rawString>Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004. Select: A lexical cohesion based news story segmentation system. Journal ofAI Communications, 17(1):3– 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonny Webber</author>
<author>Donna Byron</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of the ACL-2004 Workshop on Discourse Annotation,</booktitle>
<location>Barcelona, Spain,</location>
<marker>Webber, Byron, editors, 2004</marker>
<rawString>Bonny Webber and Donna Byron, editors. 2004. Proceedings of the ACL-2004 Workshop on Discourse Annotation, Barcelona, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>