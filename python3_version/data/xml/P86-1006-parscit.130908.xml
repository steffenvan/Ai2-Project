<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.8666984">
COMPUTATIONAL COMPLEXITY OF CURRENT GPSG THEORY
Eric Sven Ristad
MIT Artificial Intelligence Lab Thinking Machines Corporation
545 Technology Square and 245 First Street
Cambridge, MA 02139 Cambridge, MA 02142
</note>
<sectionHeader confidence="0.802739" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999249352941176">
An important goal of computational linguistics has been to use
linguistic theory to guide the construction of computationally
efficient real-world natural language processing systems. At first
glance, generalized phrase structure grammar (GPSG) appears
to be a blessing on two counts. First, the precise formalisms of
GPSG might be a direct and transparent guide for parser design
and implementation. Second, since GPSG has weak context-free
generative power and context-free languages can be parsed in
0(n3) by a wide range of algorithms, GPSG parsers would ap-
pear to run in polynomial time. This widely-assumed GPSG
&amp;quot;efficient parsability&amp;quot; result is misleading: here we prove that
the universal recognition problem for current GPSG theory is
exponential-polynomial time hard, and assuredly intractable.
The paper pinpoints sources of complexity (e.g. metarules and
the theory of syntactic features) in the current GPSG theory
and concludes with some linguistically and computationally mo-
tivated restrictions on GPSG.
</bodyText>
<sectionHeader confidence="0.996263" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976335">
An important goal of computational linguistics has been to use
linguistic theory to guide the construction of computationally
efficient real-world natural language processing systems. Gen-
eralized Phrase Structure Grammar (GPSG) linguistic theory
holds out considerable promise as an aid in this task. The pre-
cise formalisms of GPSG offer the prospect of a direct and trans-
parent guide for parser design and implementation. Further-
more, and more importantly, GPSG&apos;s weak context-free gener-
ative power suggests an efficiency advantage for GPSG-based
parsers. Since context-free languages can be parsed in polyno-
mial time, it seems plausible that GPSGs can also be parsed in
polynomial time. This would in turn seem to provide &amp;quot;the be-
ginnings of an explanation for the obvious, but largely ignored,
fact that/humans process the utterances they hear very rapidly
(G azdar ,198,1 :155) .&amp;quot; 1
In this paper I argue that the expectations of the informal
complexity argument from weak context-free generative power
are not in fact met. I begin by examining the computational
complexity of metarules and the feature system of GPSG and
show that these systems can lead to computational intractabil-
</bodyText>
<note confidence="0.58894225">
&apos;See also Joshi, &amp;quot;Tree Adjoining Grammars&amp;quot; p.226, in Natural Language
Parsing (1985) ed. by D. Dowty, L. Karttunen, and A. Zwicky, Cambridge
University Press: Cambridge, and &amp;quot;Exceptions to the Rule,&amp;quot; Science News
128: 314-315.
</note>
<bodyText confidence="0.999385333333333">
ity. Next I prove that the universal recognition problem for cur-
rent GPSG theory is Exp-Poly hard, and assuredly intractable.2
That is, the problem of determining for an arbitrary GPSG G
and input string x whether x is in the language L(G) gener-
ated by G, is exponential polynomial time hard. This result
puts GPSG-Recognition in a complexity class occupied by few
natural problems: GPSG-Recognition is harder than the trav-
eling salesman problem, context-sensitive language recognition,
or winning the game of Chess on an n x n board. The complex-
ity classification shows that the fastest recognition algorithm for
GPSGs must take exponential time or worse. One role of a com-
putational analysis is to provide formal insights into linguistic
theory. To this end, this paper pinpoints sources of complexity
in the current GPSG theory and concludes with some linguisti-
cally and computationally motivated restrictions.
</bodyText>
<sectionHeader confidence="0.772497" genericHeader="method">
2 Complexity of GPSG Components
</sectionHeader>
<bodyText confidence="0.9176934">
A generalized phrase structure grammar contains five language-
particular components — immediate dominance (ID) rules, meta-
rules, linear precedence (LP) statements, feature co-occurrence
restrictions (FCRs), and feature specification defaults (FSDs)
— and four universal components — a theory of syntactic fea-
tures, principles of universal feature instantiation, principles of
semantic interpretation, and formal relationships among various
components of the grammar.3
Syntactic categories are partial functions from features to
atomic feature values and syntactic categories. They encode
subcategorization, agreement, unbounded dependency, and other
significant syntactic information. The set K of syntactic cate-
gories is inductively specified by listing the set F of features, the
set A of atomic feature values, the function p° that defines the
range of each atomic-valued feature, and a set R of restrictive
predicates on categories (FCRs).
The set of ID rules obtained by taking the finite closure of
the metarules on the ID rules is mapped into local phrase struc-
ture trees, subject to principles of universal feature instantia-
tion, FSDs, FCRs, and LP statements. Finally, local trees are
2We use the universal problem to more accurately explore the power
of a grammatical formalism (see section 3.1 below for support). Ris-
tad(1985) has previously proven that the universal recognition problem for
the GPSG&apos;s of Gazdar(1681) is NP-hard and likely to be intractable, even
under severe rnetarule restrictions.
</bodyText>
<footnote confidence="0.959976666666667">
3This work is based on current GPSG theory as presented in Gazdar et.
al. (1985), hereafter GKPS. The reader is urged to consult that work for a
formal presentation and thorough exposition of current GPSG theory.
</footnote>
<page confidence="0.996696">
30
</page>
<bodyText confidence="0.999213028571429">
assembled to form phrase structure trees, which are terminated
by lexical elements.
To identify sources of complexity in GPSG theory, we con-
sider the isolated complexity of the finite metarule closure op-
eration and the rule to tree mapping, using the finite closure
membership and category membership problems, respectively.
Informally, the finite closure membership problem is to deter-
mine if an ID rule is in the finite closure of a set of metarules M
on a set of ID rules R. The category membership problem is to
determine if a category or C or a legal extension of C is in the
set K of all categories based the function p and the sets A, F
and R. Note that both problems must be solved by any GPSG-
based parsing system when computing the ID rule to local tree
mapping.
The major results are that finite closure membership is NP-
hard and category membership is PSPACE-hard. Barton(1985)
has previously shown that the recognition problem for ID/LP
grammars is NP-hard. The components of GPSG theory are
computationally complex, as is the theory as a whole.
Assumptions. In the following problem definitions, we allow
syntactic categories to be based on arbitrary sets of features
and feature values. In actuality, GPSG syntactic categories are
based on fixed sets and a fixed function p. As such, the set K of
permissible categories is finite, and a large table containing K
could, in principle, be given.4 We (uncontroversially) generalize
to arbitrary sets and an arbitrary function p to prevent such a
solution while preserving GPSG&apos;s theory of syntactic features.5
No other modifications to the theory are made.
An ambiguity in GKPS is how the FCRs actually apply to
embedded categories.6 Following Ivan Sag (personal communi-
cation), I make the natural assumption here that FCRs apply
top-level and to embedded categories equally.
*This suggestion is of no practical significance, because the actual num-
ber of GPSG syntactic categories is extremely large. The total number of
categories, given the 25 atomic features and 4 category-valued features, is:
</bodyText>
<equation confidence="0.9934785">
IK = K4 I = 32*((l + 325)((l + 322)((l + 322)(1 + 322)1)2)2)4
= 322(1 + 322)44 &gt; 31222 &gt; 10174
</equation>
<bodyText confidence="0.602702714285714">
See page 10 for details. Many of these categories will be linguistically
meaningless, but all GPSGs will generate all of them and then filter some
out in consideration of FCRs, PSDB, universal feature instantiation, and
the other admissible local trees and lexical entries in the GPSG. While
the FCRs in some grammars may reduce the number of categories, FCRs
are a language-particular component of the grammar. The vast number of
categories cited above is inherent in the GPSG framework.
</bodyText>
<construct confidence="0.742228777777778">
*Our goal is to identify sources of complexity in GPSG theory. The gen-
eralization to arbitrary sets allows a fine-grained study of one component
of GPSG theory (the theory of syntactic features) with the tools of compu-
tational complexity theory. Similarly, the chess board is uncontroversially
generalized to size n x n in order to study the computational complexity of
chess.
6A category C that is defined for a feature f, f E (F — Atom) n DOM(C)
(e.g. f = SLASH ), contains an embedded category C., where C(f) = C.
GKPS does not explain whether FCR&apos;s must be true of C. as well as C.
</construct>
<subsectionHeader confidence="0.987074">
2.1 Met arules
</subsectionHeader>
<bodyText confidence="0.999814444444444">
The complete set of ID rules in a GPSG is the maximal set that
can be arrived at by taking each metarule and applying it to
the set of rules that have not themselves arisen as a result of
the application of that metarule. This maximal set is called the
finite closure (FC) of a set R of lexical ID rules under a set M
of metarules.
The cleanest possible complexity proof for metarule finite
closure would fix the GPSG (with the exception of metarules)
for a given problem, and then construct metarules dependent
on the problem instance that is being reduced. Unfortunately,
metarules cannot be cleanly removed from the GPSG system.
Metarules take ID rules as input, and produce other ID rules as
their output. If we were to separate metarules from their inputs
and outputs, there would be nothing left to study.
The best complexity proof for metarules, then, would fix
the GPSG modulo the metarules and their input. We ensure
the input is not inadvertently performing some computation by
requiring the one ID rule R allowed in the reduction to be fully
specified, with only one 0-level category on the left-hand side
and one unanalyzable terminal symbol on the right-hand side.
Furthermore, no FCRs, FSDs, or principles of universal feature
instantiation are allowed to apply. These are exceedingly severe
constraints. The ID rules generated by this formal system will
be the finite closure of the lone ID rule R under the set M of
metarules.
The (strict, resp.) finite closure membership problem for
GPSG metarules is: Given an ID rule r and sets of metarules
</bodyText>
<listItem confidence="0.419179833333333">
M and ID rules R, determine if 3r&apos; such that r&apos; r (r&apos; = r,
resp.) and r1 E FC(M, R).
Theorem 1: Finite Closure Membership is NP-hard
Proof: On input 3-CNF formula F of length n using the m vari-
ables x1 ... xm., reduce 3-SAT, a known NP-complete problem,
to Metarule-Membership in polynomial time.
</listItem>
<bodyText confidence="0.999732111111111">
The set of ID rules consists of the one ID rule R, whose
mother category represents the formula variables and clauses,
and a set of metarules M s.t. an extension of the ID rule A is in
the finite closure of M over R if F is satisfiable. The metarules
generate possible truth assignments for the formula variables,
and then compute the truth value of F in the context of those
truth assignments.
Let w be the string of formula literals in F, and let wi denote
the ith symbol in the string w.
</bodyText>
<footnote confidence="0.473165">
1. The ID rules R, A
</footnote>
<page confidence="0.998072">
31
</page>
<equation confidence="0.687024125">
R: F &lt;satisfiability&gt;
A: [[STAGE 3]] —■ &lt;satisfiable&gt;
where
&lt;satisf iable&gt; is a terminal symbol
&lt;satisf lability&gt; is a terminal symbol
F = {[yi 0]:1 m}
U {[ci 0] : 1 &lt;i &lt; 111}
U {[STAGE 1]}
</equation>
<listItem confidence="0.963365666666667">
2. Construct the metarules
(a) in metarules to generate all possible assignments to
the variables
</listItem>
<equation confidence="0.969352785714286">
Vi, 1 &lt;1 &lt; in
{[ys 01, [STAGE 1[}—+W (1)
{[yi 1], [STAGE 1]} --■ W
(b) one metarule to stop the assignment generation pro-
cess
{[STAGE 1]} --■ w
(2)
{[STAGE 2[}—W
(c) 1w 1 metarules to verify assignments
Vi, j, k 1 &lt; 1&lt; 41, 1 &lt; j &lt; in, 0 &lt; k &lt; 2,
if w3;_k = x„ then construct the metarule
fly., 1],[c 0], [STAGE 2]} W
Cy, 1], [; 1], [STAGE 2]} W
Vi,j,k 1&lt;i&lt;11,1&lt;i&lt;m,0&lt;k&lt; 2,
</equation>
<bodyText confidence="0.7806496">
if w31_k = then construct the metarule
{[yi O[, [c 0], [STAGE 2]} W
{[yi 0[,[c 1], [STAGE 2]} W
(d) Let the category C = {[c, 1] : 1 &lt; i &lt; 411. Con-
struct the metarule
</bodyText>
<equation confidence="0.983191">
C[STAGE 2] —■ W
4 (5)
{[STAGE 3]} &lt;satisf iable&gt;
</equation>
<bodyText confidence="0.883306555555556">
The reduction constructs 0(1w 1) metarules of size toga WI),
and clearly may be performed in polynomial time: the reduc-
tion time is essentially the number of symbols needed to write
the GPSG down. Note that the strict finite closure membership
problem is also NP-hard. One need only add a polynomial num-
ber of metarules to &amp;quot;change&amp;quot; the feature values of the mother
node C to some canonical value when C(STAGE ) = 3 — all 0,
for example, with the exception of STAGE . Let F = {[yi 0] :
1 &lt; i &lt; in} U {[c, 0] : 1 &lt; i &lt;ty, . Then A would be
</bodyText>
<equation confidence="0.9959885">
A : F]STAGE 3] &lt;satisfiable&gt;
Q.E .D
</equation>
<bodyText confidence="0.9999832">
The major source of intractability is the finite closure opera-
tion itself. Informally, each metarule can more than double the
number of ID rules, hence by chaining metarules (i.e. by apply-
ing the output of a metarule to the input of the next metarule)
finite closure can increase the number of ID rules exponentially.7
</bodyText>
<subsectionHeader confidence="0.999827">
2.2 A Theory of Syntactic Features
</subsectionHeader>
<bodyText confidence="0.992113256410256">
Here we show that the complex feature system employed by
GPSG leads to computational intractability. The underlying
insight for the following complexity proof is the almost direct
equivalence between Alternating Turing Machines (ATMs) and
syntactic categories in GPSG. The nodes of an ATM compu-
tation correspond to 0-level syntactic categories, and the ATM
computation tree corresponds to a full, n-level syntactic cate-
gory. The finite feature closure restriction on categories, which
limits the depth of category nesting, will limit the depth of
the corresponding ATM computation tree. Finite feature clo-
sure constrains us to specifying (at most) a polynomially deep,
polynomially branching tree in polynomial time. This is ex-
actly equivalent to a polynomial time ATM computation, and
by Chandra and Stockmeyer(1976), also equivalent to a deter-
ministic polynomial space-bounded Turing Machine computa-
tion.
As a consequence of the above insight, one would expect
the GPSG Category-Membership problem to be PSPACE-hard.
The actual proof is considerably simpler when framed as a re-
duction from the Quantified Boolean Formula (QBF) problem,
a known PSPACE-complete problem.
Let a specification of K be the arbitrary sets of features F,
atomic features Atom, atomic feature values A, and feature co-
occurrence restrictions R and let p be an arbitrary function, all
equivalent to those defined in chapter 2 of GKPS.
The category membership problem is: Given a category C
and a specification of a set K of syntactic categories, determine
if 3C&apos; s.t. C&apos; C and C&apos; E K.
The QBF problem is {QiyiQ2y2 • • •QmymF(Yi, Y2, • • • , Ym) I
E {V, 3), where the yi are boolean variables, F is a boolean
formula of length n in conjunctive normal form with exactly
7More precisely, the metarule finite closure operation can increase the
size of a GPSG G worse than exponentially: from Cl&apos; to 0(IG 120). Given
a set of ID rules R of symbol size n, and a set M of m metarule, each of
size p, the symbol size of FC(M,R) is 0(n2&amp;quot;) = 0(1012). Each metarule
can match the productions in R 0(n) different ways, inducing 0(n + p)
new symbols per match: each metarule can therefore square the ID rule
grammar size. There are in metarules, so finite closure can create an ID
rule grammar with 0(n2&amp;quot;) symbols.
</bodyText>
<page confidence="0.996275">
32
</page>
<bodyText confidence="0.7457615">
three variables per clause (3-CNF), and the quantified formula
is true).
</bodyText>
<equation confidence="0.655650333333333">
Theorem 2: GPSG Category-Membership is PSPACE-hard
Proof: By reduction from QBF. On input formula
n = chyvQ2y2 •• • QtnYmF(1/1, Y2, • • • ,Ym)
</equation>
<bodyText confidence="0.9555645">
we construct an instance P of the Category-Membership
problem in polynomial time, such that 12 E QBF if and only
if P is true.
Consider the QBF as a strictly balanced binary tree, where
the ith quantifier Q, represents pairs of subtrees &lt; 71,Tf &gt; such
that (1) Ti and T1 each immediately dominate pairs of subtrees
representing the quantifiers Q1+1 • • • Qm, and (2) the ith variable
yi is true in Tt and false in Tf. All nodes at level i in the whole
tree correspond to the quantifier Q. The leaves of the tree are
different instantiations of the formula F, corresponding to the
quantifier-determined truth assignments to the m variables. A
leaf node is labeled true if the instantiated formula F that it
represents is true. An internal node in the tree at level i is
labeled true if
</bodyText>
<listItem confidence="0.990806">
1. Q, = &amp;quot;s&amp;quot; and either daughter is labeled true, or
2. Q, = &amp;quot;V&amp;quot; and both daughters are labeled true.
</listItem>
<bodyText confidence="0.997406083333333">
Otherwise, the node is labeled false.
Similarly, categories can be.understood as trees, where the
features in the domain of a category constitute a node in the
tree, and a category C immediately dominates all categories C&apos;
such that 3f E ((F — Atom) n Dom(C))[C(f)
In the QBF reduction, the atomic-valued features are used
to represent the m variables, the clauses of F, the quantifier
the category represents, and the truth label of the category.
The category-valued features represent the quantifiers — two
category-valued features qk, ek represent the subtree pairs &lt;
Tt,Tf &gt; for the quantifier Qk. FCRs maintain quantifier-imposed
variable truth assignments &amp;quot;down the tree&amp;quot; and calculate the
truth labeling of all leaves, according to F, and internal nodes,
according to quantifier meaning.
Details. Let w be the string of formula literals in F, and iv;
denote the ith symbol in the string w. We specify a set K of
permissible categories based on A, F, p, and the set of FCRs R
s.t. the category [[LABEL I]] or an extension of it is an element
of K if 12 is true.
First we define the set of possible 0-level categories, which
encode the formula F and truth assignments to the formula
variables. The feature ws represents the formula literal tut in w,
yi represents the variable y, in f2, and ci represents the truth
value of the ith clause in F.
</bodyText>
<equation confidence="0.995277857142857">
Atom= {LEVEL ,LABEL }
U {wi : 1 _&lt;
U {ya : 1 &lt; j &lt; m}
U {ci : 1 &lt;1 &lt; 4&apos;1}
F — Atom = {qk,ek: 1 k &lt; m}
P°(LEVEL) = {k :1 &lt; k &lt; m+ 1}
e(f) = {0,1} V f E Atom — {LEVEL }
</equation>
<figureCaption confidence="0.491233">
FCR&apos;s are included to constrain both the form and content of
the guesses:
</figureCaption>
<figure confidence="0.987786666666667">
1. FCR&apos;s to create strictly balanced binary trees:
Vk, 1 &lt; k
[LEVEL lc] [qk [[yk 11[LEVEL k 1[1]&amp;
lek [fvk CLEvEL k l[]]
2. FCR&apos;s to ensure all 0-level categories are fully specified:
VI, 1 &lt; i &lt; ,
[cd [w3,21az[w3i-Oz[w31]
[LABEL ] a [ci]
Vk, 1 &lt; k
[LABEL ] -a- [yk]
3. FCR&apos;s to label internal nodes with truth values deter-
mined by quantifier meaning:
</figure>
<equation confidence="0.749360909090909">
Vk, 1 &lt; k &lt; m ,
if Q k = &amp;quot;V&amp;quot;, then include:
[LEVEL k[&amp; [LABEL 1] a [qk [[LABEL 11]]&amp;[ql [[LABEL 1]]]
[LEVEL k]&amp;[LABEL 0] a- [qk ([LABEL 0]]] V [[LABEL 0]]]
otherwise Q k = &amp;quot;3&amp;quot;, and include:
[LEVEL le] &amp;[LABEL 1] a [qk [[LABEL 1]]] V [ek [(LABEL 1]]]
[LEVEL k]&amp; [LABEL 0] [qk [[LABEL 0]]]&amp;[q&apos;k [[LABEL O][]
The category-valued features qk and q&apos;k represent the quan-
tifier Q. In the category value of qk, the formula vari-
able yk = 1 everywhere, while in the category value of q&apos;k,
Ilk = 0 everywhere.
</equation>
<listItem confidence="0.4109365">
4. one FCR to guarantee that only satisfiable assignments
are permitted:
</listItem>
<figure confidence="0.791775">
[LEVEL 1] D [LABEL 1]
5. FCR&apos;s to ensure that quantifier assignments are preserved
&amp;quot;down the tree&amp;quot;:
Vi,k 1&lt;i&lt;k&lt;m,
[yi 1] D Eqk llyi 1[]]&amp;[(iik Usti 1]]]
[ys 0] [qk [[yi O]]iSz[ek [[yi 0]]]
33
6. FCR&apos;s to instantiate variable assignments into the formula 3.1 Defining the Recognition Problem
F:
Vi,k 1 &lt; &lt;Itol and I &lt; k &lt; m ,
</figure>
<construct confidence="0.668183">
if iv; = yk, then include:
</construct>
<table confidence="0.911662083333333">
[Yr: [.1 1]
[yk 0] D [wi 0]
else if wi = 97„ then include:
[yk 1] D [wi 0]
[Yk 0[ D [wi 11
7. FCR&apos;s to verify the guessed variable assignments in leaf
nodes:
Vi 1 &lt; &lt;
[c, 0] -a- [w32-2 0[Siltu3i-1 018t[tc3i 0]
[ci [tc31-2 1[ V [W31-2 1] V [tugi
[LEVEL en I]St[ci 0] D [LABEL 0]
[LEVEL en 118zIci 1]&amp;[c2 118z Si[c1/31.1 D [LABEL 1]
</table>
<bodyText confidence="0.991272">
The reduction constructs 0(1w1) features and 0(m2) FCRs
of size 0(log m) in a simple manner, and consequently may be
seen to be polynomial time. Q.( .D
The primary source of intractability in the theory of syn-
tactic features is the large number of possible syntactic cate-
gories (arising from finite feature closure) in combination with
the computational power of feature co-occurrence restrictions.8
FCRs of the &amp;quot;disjunctive consequence&amp;quot; form [f v] hi v1] V
...V [f,-, vnj compute the direct analogue of Satisfiability: when
used in conjunction with other FCRs, the GPSG effectively
must try all n feature-value combinations.
</bodyText>
<sectionHeader confidence="0.920447" genericHeader="method">
3 Complexity of GPSG-Recognition
</sectionHeader>
<bodyText confidence="0.987091466666667">
Two isolated membership problems for GPSG&apos;s component for-
mal devices were considered above in an attempt to isolate
sources of complexity in GPSG theory. In this section the recog-
nition problem (RP) for GPSG theory as a whole is considered.
I begin by arguing that the linguistically and computationally
relevant recognition problem is the universal recognition prob-
lem, as opposed to the fixed language recognition problem. I
then show that the former problem is exponential-polynomial
(Exp-Poly) time-hard.
&apos;Finite feature closure admits a surprisingly large number of possible
categories. Given a specification (F,Atom, A, R, p) of K, let a =I Atom I and
=1.P — Atom&apos;. Assume that all atomic features are binary: a feature may
be or undefined and there are 3 0-level categories. The b category-
valued features may each assume 0(3) possible values in a 1-level category,
so I K&apos; I= 0(3&amp;quot;(3&amp;quot;)°). More generally,
</bodyText>
<equation confidence="0.964832">
1K = Kb 1= 0(3(&apos;EI:=0 (b)r)) = 0(3(*.blEb—. *1) = 0(3-.)= 0(3&amp;quot;t)
</equation>
<bodyText confidence="0.996954508474576">
where Eb.,=0 converges to e en 2.7 very rapidly and a, b = 0(1G I); a
25, b = 4 in GKPS. The smallest category in K will be 1 symbol (null
set), and the largest, maximally-specified, category will be of symbol-size
log I K I= 0(a • b!).
The universal recognition problem is: given a grammar G and
input string x, is x E L(G)?. Alternately, the recognition prob-
lem for a class of grammars may be defined as the family of
questions in one unkown. This fixed language recognition prob-
lem is: given an input string x, is x E L for some fixed language
L?. For the fixed language RP, it does not matter which gram-
mar is chosen to generate L — typically, the fastest grammar is
picked.
It seems reasonable clear that the universal RP is of greater
linguistic and engineering interest than the fixed language RP.
The grammars licensed by linguistic theory assign structural
descriptions to utterances, which are used to query and update
databases, be interpreted semantically, translated into other hu-
man languages, and so on. The universal recognition problem
— unlike the fixed language problem — determines membership
with respect to a grammar, and therefore more accurately mod-
els the parsing problem, which must use a grammar to assign
structural descriptions.
The universal RP also bears most directly on issues of nat-
ural language acquisition. The language learner evidently pos-
sesses a mechanism for selecting grammmars from the class of
learnable natural language grammars LG on the basis of linguis-
tic inputs. The more fundamental question for linguistic theory,
then, is &amp;quot;what is the recognition complexity of the class La?&amp;quot;
If this problem should prove computationally intractable, then
the (potential) tractability of the problem for each language
generated by a G in the class is only a partial answer to the
linguistic questions raised.
Finally, complexity considerations favor the universal RP.
The goal of a complexity analysis is to characterize the amount
of computational resources (e.g. time, space) needed to solve the
problem in terms of all computationally relevera inputs on some
standard machine model (typically, a multi-tape deterministic
Turing machine). We know that both input string length and
grammar size and structure affect the complexity of the recog-
nition problem. Hence, excluding either input from complexity
consideration would not advance our understanding.8
Linguistics and computer science are primarily interested in
the universal recognition problem because both disciplines are
concerned with the formal power of a family of grammars. Lin-
guistic competence and performance must be considered in the
larger context of efficient language acquisition, while computa-
tional considerations demand that the recognition problem be
characterized in terms of both input string and grammar size.
Excluding grammar size from complexity consideration in order
°This &amp;quot;consider all relevant inputs&amp;quot; methodology is universally assumed
in the formal language and computational complexity literature. For ex-
ample, Hoperaft and Ullman(1979:139) define the context-free grammar
recognition problem as: &amp;quot;Given a CFG G = (V,T,P,S) and a string z in
T&apos;, is z in L(G)?&amp;quot;. Garey and Johnson(1979) is a standard reference work
in the field of computational complexity. All 10 automata and language
recognition problems covered in the book (pp. 265-271) are universal, i.e.
of the form &amp;quot;Given an instance of a machine/grammar and an input, does
the machine/grammar accept the input?* The complexity of these recog-
nition problems is always calculated in terms of grammar and input size.
</bodyText>
<page confidence="0.997929">
34
</page>
<bodyText confidence="0.9976774">
to argue that the recognition problem for a family of grammars
is tractable is akin to fixing the size of the chess board in order
to argue that winning the game of chess is tractable: neither
claim advances our scientific understanding of chess or natural
language.
</bodyText>
<subsectionHeader confidence="0.606237">
3.2 GPSG-Recognition is Exp-Poly hard
</subsectionHeader>
<tableCaption confidence="0.405935333333333">
Theorem 3: GPSG-Recognition is Exp-Poly time-hard
Proof 3: By direct simulation of a polynomial space bounded
alternating Turing Machine M on input w.
</tableCaption>
<bodyText confidence="0.7080828">
Let S(n) be a polynomial in n. Then, on input M, a S(n)
space-bounded one tape alternating Turing Machine (ATM),
and string w, we construct a GPSG G in polynomial time such
that WE L(M) if Owl 1w22. . .wnn$n-- 1 E L(G).
By Chandra and Stockmeyer(1976),
</bodyText>
<equation confidence="0.765175">
ASPACE(S(n))= U DTIMgcs(n))
e&gt;o
</equation>
<bodyText confidence="0.99781">
where ASPACE(S(n)) is the class of problems solvable in
space 5(n) on an ATM, and DTIME(F(n)) is the class of prob-
lems solvable in time F(n) on a deterministic Turing Machine.
As a consequence of this result and our following proof, we have
the immediate result that GPSG-Recognition is DT/ME(cs(n))-
hard, for all constants c, or Exp-Poly time-hard.
An alternating Turing Machine is like a nondeterministic
TM, except that some subset of its states will be referred to
as universal states, and the remainder as existential states. A
nondeterministic TM is an alternating TM with no universal
states.1°
The nodes of the ATM computation tree are represented by
syntactic categories in K° — one feature for every tape square,
plus three features to encode the ATM tape head positions and
the current state. The reduction is limited to specifying a poly-
nomial number of features in polynomial time; since these fea-
tures are used to encode the ATM tape, the reduction may only
specify polynomial space bounded ATM computations.
The ID rules encode the ATM Nextm() relation, i.e. C —*
Nextm(C) for a universal configuration C. The reduction con-
structs an ID rule for every combination of possible head po-
sition, machine state, and symbol on the scanned tape square.
Principles of universal feature instantiation transfer the rest of
the instantaneous description (i.e. contents of the tape) from
mother to daughters in ID rules.
&apos;°Our ATM definition is taken from Chandra and Stockmeyer(1976), with
the restriction that the work tapes are one-way infinite, instead of two-way
infinite. Without loss of generality, we use a 1-tape ATM, so
</bodyText>
<equation confidence="0.980057">
6c x x x rk x{L, R} x {L, R})
Let Nextm(C) = {Co, • .. , If C is a universal con-
figuration, then we construct an ID rule of the form
C CO,C1, • (6)
Otherwise, C is an existential configuration and we construct
the k 1 ID rules
C Vi, 0 &lt; i &lt; k (7)
</equation>
<bodyText confidence="0.5976508125">
A universal ATM configuration is labeled accepting if and
only if it has halted and accepted, or if all of its daughters are
labeled accepting. We reproduce this with the ID rules in 6
(or 8), which will be admissible only if all subtrees rooted by
the RHS nodes are also admissible.
An existential ATM configuration is labeled accepting if and
only if it has halted and accepted, or if one of its daughters is
labeled accepting. We reproduce this with the ID rules in 7
(or 9), which will be admissible only if one subtree rooted by a
RHS node is admissible.
All features that represent tape squares are declared to be
in the HEAD feature set, and all daughter categories in the
constructed ID rules are head daughters, thus ensuring that the
Head Feature Convention (HFC) will transfer the tape contents
of the mother to the daughter(s), modulo the tape writing ac-
tivity specified by the next move relation.
</bodyText>
<equation confidence="0.874753444444444">
Details.
Let
Re sultOm(i, a, d) =
[[HEADO + 1], [i a], [A 1]] if d = R
[[HEADO I — 11,11 a], [A I]] if d = L
Result lm(j, c,p, d) =
[[HE AD 1 j + i[,[1c][STATE p]] if d = R
HHEAD1 j — 1], [r; c][STATE p]] if d = L
Transm(q, a, 6) = {(p, c, d2) : ((q, a, b) , (p, c , d2)) ES)
</equation>
<bodyText confidence="0.632438666666667">
where
a is the read-only (R/O) tape symbol currently
being scanned
</bodyText>
<footnote confidence="0.840879833333333">
b is the read-write (R/W) tape symbol cur-
rently being scanned
d1 is the R/O tape direction
d2 is the R/W tape direction
The GPSG G contains:
1. Feature definitions
</footnote>
<page confidence="0.999376">
35
</page>
<bodyText confidence="0.92968675">
A category in K° represents a node of an ATM compu-
tation tree, where the features in Atom encode the ATM
configuration. Labeling is performed by ID rules.
(a) definition of F, Atom, A
</bodyText>
<equation confidence="0.919419307692308">
F = Atom = {STATE ,HEADO ,HEAD1 ,A}
U {i:0&lt;i5_1w1+1}
U {Pi : 1 j S(11.0}
A = QuEur ; as defined earlier
(b) definition of p°
p°(A) = {1,2,3}
p°(STATE ) = Q ; the ATM state set
p°(HEADO ) = {j:1 &lt;j 51w1}
p° (HE AD1 ) = {i :15i5Saw
V f E 0 51w1 +1}
p°(f) = E u {$} ; the ATM input alphabet
Vf E {F.; : 1 j &lt; S(Itv1)}
e(f) = r ; the ATM tape alphabet
</equation>
<listItem confidence="0.728202166666667">
(c) definition of HEAD feature set
HEAD = {i :0 &lt; I &lt;11.vi +1}U{1&apos;1 : 1 &lt;j &lt; S(Itp1)}
(d) FCRs to ensure full specification of all categories ex-
cept null ones.
Vf; I E Atom, [STATE D [f]
2. Grammatical rules
</listItem>
<equation confidence="0.948098285714286">
Vi,j,q,a,b :1 &lt; i &lt;frol, 1 &lt;j &lt; SOO, q E Q, a E E, b E
if Transm(q, a, b) 0 0, construct the following ID rules.
(a) if q E U (universal state)
CHEADO 1], [i a], [HEAD1 j], [r, b], [STATE q], [A 1]}
(HesultOm(i , a, d1k) UResultlm(i, ck,Pk,d2k)
(Pk,Ck,Clik,d2k) E Transm(q, a, b)}
(8)
</equation>
<bodyText confidence="0.992612">
where all categories on the RHS are heads.
</bodyText>
<equation confidence="0.872781">
(b) otherwise qEQ—U (existential state)
ck,dik,d2k) E TraaSm(q, a, b),
{[HEADO I], [I a], [HEAD1 j], [I&apos;, b], [STATE q], [A 1]}
ResultOm(i, a, du) u Resultlm(j, ck,Pk,d2k)
(9)
</equation>
<bodyText confidence="0.996873">
where all categories on the RHS are heads.
</bodyText>
<listItem confidence="0.983072">
(c) One ID rule to terminate accepting states, using null-
transitions.
{[STATE h], [1 YI) (10)
(d) Two ID rules to read input strings and begin the
ATM simulation. The A feature is used to separate
functionally distinct components of the grammar. [A
1] categories participate in the direct ATM simula-
tion, [A 2] categories are involved in reading the in-
put string, and the [A 3] category connects the read
input string with the ATM simulation start state.
</listItem>
<equation confidence="0.4544005">
START —■ {[A 1]},{[A 21}
{[A 2]} {[A 2]},{[A 2]}
</equation>
<bodyText confidence="0.860114">
where all daughters are head daughters, and where
</bodyText>
<equation confidence="0.967256">
START = {[HEADO 1], [HEAD1 1], [STATE s], [A 3]}
U {[r, #]:1 saw1)}
(e) the lexical rules,
Va,i E E, 1 &lt; &lt;lurk
Vi 0&lt; 1 &lt;Ito&apos; +1,
&lt; oi,{[A 2],[i a]} &gt; (12)
&lt; Si , {[A 2[,[i &gt;
</equation>
<bodyText confidence="0.9999678">
The reduction plainly may be performed in polynomial time
in the size of the simulated ATM, by inspection.
No metarules or LP statements are needed, although meta-
rules could have been used instead of the Head Feature Conven-
tion. Both devices are capable of transferring the contents of the
ATM tape from the mother to the daughter(s). One metarule
would be needed for each tape square/tape symbol combination
in the ATM.
GKPS Definition 5.14 of Admissibility guarantees that ad-
missible trees must be terminated. 11 By the construction above
— see especially the ID rule 10 — an [A 1] node can be termi-
nated only if it is an accepting configuration (i.e. it has halted
and printed Y on its first square). This means the only admis-
sible trees are accepting ones whose yield is the input string
followed by a very long empty string. Q .e
</bodyText>
<subsectionHeader confidence="0.672076">
&amp;quot;The admissibility of nonlocal trees is defined as follows (GKPS, p.104):
Definition: Admissibility
</subsectionHeader>
<bodyText confidence="0.370866">
Let R be a set of ID rules. Then a tree t is admissible from R
if and only if
</bodyText>
<listItem confidence="0.979521">
1. t is terminated, and
2. every local subtree in t is either terminated or locally
admissible from some r E R.
</listItem>
<page confidence="0.994115">
36
</page>
<subsectionHeader confidence="0.988113">
3.3 Sources of Intractability
</subsectionHeader>
<bodyText confidence="0.999870142857143">
The two sources of intractability in GPSG theory spotlighted
by this reduction are null-transitions in ID rules (see the ID
rule 10 above), and universal feature instantiation (in this case,
the Head Feature Convention).
Grammars with unrestricted null-transitions can assign elab-
orate phrase structure to the empty string, which is linguisti-
cally undesirable and computationally costly. The reduction
must construct a GPSG G and input string x in polynomial
time such that x E L(G) if w E L(M), where M is a PSPACE-
bounded ATM with input w. The &apos;polynomial time&apos; constraint
prevents us from making either x or G too big. Null-transitions
allow the grammar to simulate the PSPACE ATM computation
(and an Exp-Poly TM computation indirectly) with an enor-
mously long derivation string and then erase the string. If the
GPSG G were unable to erase the derivation string, G would
only accept strings which were exponentially larger than M and
w, i.e. too big to write down in polynomial time.
The Head Feature Condition transfers HEAD feature val-
ues from the mother to the head daughters just in case they
don&apos;t conflict. In the reduction we use HEAD &apos;features to en-
code the ATM tape, and thereby use the HFC to transfer the
tape contents from one ATM configuration C (represented by
the mother) to its immediate successors CO3.. • , Cu (the head
daughters). The configurations C, Co, . ,C,, have identical tapes,
with the critical exception of one tape square. If the HFC en-
forced absolute agreement between the HEAD features of the
mother and head daughters, we would be unable to simulate the
PSPACE ATM computation in this manner.
</bodyText>
<sectionHeader confidence="0.997647" genericHeader="method">
4 Interpreting the Result
</sectionHeader>
<subsectionHeader confidence="0.8487845">
4.1 Generative Power and Computational Com-
plexity
</subsectionHeader>
<bodyText confidence="0.999976363636364">
At first glance, a proof that GPSG-Recognition is Exp-Poly hard
appears to contradict the fact that context-free languages can
be recognized in 0(n3) time by a wide range of algorithms. To
see why there is no contradiction, we must first explicitly state
the argument from weak context-free generative power, which
we dub the efficient parsability (EP) argument.
The EP argument states that any GPSG can be converted
into a weakly equivalent context-free grammar (CFG), and that
CFG-Recognition is polynomial time; therefore, GPSG-Recognition
must also be polynomial time. The EP argument continues: if
the conversion is fast, then GPSG-Recognition is fast, but even
if the conversion is slow, recognition using the &amp;quot;compiled&amp;quot; CFG
will still be fast, and we may justifiably lose interest in recogni-
tion using the original, slow, GPSG.
The EP argument is misleading because it ignores both the
effect conversion has on grammar size, and the effect grammar
size has on recognition speed. Crucially, grammar size affects
recognition time in all known algorithms, and the only gram-
mars directly usable by context-free parsers, i.e. with the same
complexity as a CFG, are those composed of context-free pro-
ductions with atomic nonterminal symbols. For GPSG, this is
the set of admissible local trees, and this set is astronomical:
</bodyText>
<equation confidence="0.969155">
0pmlin2-1-1) (13)
</equation>
<bodyText confidence="0.9706892">
in a GPSG G of size M.12
Context-free parsers like the Earley algorithm run in time
0(1G&apos; 12 .n3) where j G&apos; 1 is the size of the CFG G&apos; and n the
input string length, so a GPSG G of size m will be recognized
in time
</bodyText>
<equation confidence="0.947476">
o(32 713) (14)
</equation>
<bodyText confidence="0.92918">
The hyper-exponential term will dominate the Earley algo-
rithm complexity in the reduction above because en is a function
of the size of the ATM we are simulating. Even if the GPSG is
held constant, the stunning derived grammar size in formula 13
turns up as an equally stunning &apos;constant&apos; multiplicative factor
in 14, which in turn will dominate the real-world performance of
the Earley algorithm for all expected inputs (i.e. any that can
be written down in the universe), every time we use the derived
grammar.13
Pullum(1985) has suggested that &amp;quot;examination of a suitable
&apos;typical&apos; GPSG description reveals a ratio of only 4 to 1 between
expanded and unexpanded grammar statements,&amp;quot; strongly im-
plying that GPSG is efficiently processable as a consequence.14
But this &amp;quot;expanded grammar&amp;quot; is not adequately expanded, i.e.
it is not composed of context-free productions with unanalyz-
&apos;As we saw above, the metarule finite closure operation can increase
the ID rule grammar size from I R G I) to 0(m2&apos;°) in a GPSG
G of size m. We ignore the effects of ID/LP format on the number of
admissible local trees here, and note that if we expanded out all admissible
linear precedence possibilities in FC(M,R), the resultant `ordered&apos; ID rule
grammar would be of size 0(m2&amp;quot;9). In the worst case, every symbol in
FC(M,R) is underspecified, and every category in K extends every symbol
in the FC(M,R) grammar. Since there are
0(3&amp;quot;&amp;quot;)
possible syntactic categories, and 0(m2&apos;) symbols in FC(M,R), the number
of admissible local trees (= atomic context-free productions) in G is
0((3m ) = 0(3&amp;quot;&apos;)
i.e. astronomical. Ristad(1986) argues that the minimal set of admissible
local trees in GKPS&apos; GPSG for English is considerably smaller, yet still
contains more than 1020 local trees.
&amp;quot;The compiled grammar recognition problem is at least as intractable
as the uncompiled one. Even worse, Barton(1985) shows how the grammar
expansion increases both the space and time costs of recognition, when
compared to the cost of using the grammar directly.
&amp;quot;This substantive argument is somewhat strange coming from a co-author
of a book which advocates the purely formal investigation of linguistics:
&amp;quot;The universalism [of natural language] is, ultimately, intended to be en-
tirely embodied in the formal system, not expressed by statements made in
it.&amp;quot;GKPS(4). It is difficult to respond precisely to the claims made in Pul-
lum(1985), since the abstract is (necessarily) brief and consists of assertions
unsupported by factual documentation or clarifying assumptions.
</bodyText>
<page confidence="0.998725">
37
</page>
<bodyText confidence="0.9995">
able nonterminal symbols.15 These informal tractability argu-
ments are a particular instance of the more general EP argument
and are equally misleading.
The preceding discussion of how intractability arises when
converting a GPSG into a weakly equivalent CFG does not in
principle preclude the existence of an efficient compilation step.
If the compiled grammar is truly fast and assigns the same struc-
tural descriptions as the uncompiled GPSG, and it is possible to
compile the GPSG in practice, then the complexity of the uni-
versal recognition problem would not accurately reflect the real
cost of parsing.16 But until such a suggestion is forthcoming,
we must assume that it does not exist.17.15
&amp;quot;&amp;quot;Expanded grammar&amp;quot; appears to refer to the output of metarule finite
closure (i.e. ID rules), and this expanded grammar is trw-table only if
the grammar is directly usable by the Earley algorithm exactly as context-
free productions are: all nonterminals in the context-free productions must
be unanalyzable. But the categories and ID rules of the metarule finite
closure grammar do not have this property. Nonterminals in GPSG are
decomposable into a complex set of feature specifications and cannot be
made atomic, in part because not all extensions of ID rule categories are
legal. For example, the categories —0001VP[4tINDRM PAS] and VP [.INV.
VFORM FIN] are not legal extensions of VP in English, while VP [+INV, +AUX,
VFORM FIN] is. FCRs, FSDa, LP statements, and principles of universal
feature instantiation — all of which contribute to GPSG&apos;s intractability —
must all still apply to the rules of this expanded grammar.
Even if we ignore the significant computational complexity introduced by
the machinery mentioned in the previous paragraph (i.e. theory of syntac-
tic features, PCBs, FSDs, ID/LP format, null-transitions, and metarules),
GPSG will still not obtain an efficient parsalnlity result. This is because the
Head Feature Convention alone ensures that the universal recognition prob-
lem for GPSGs will be NP-hard and likely to be intractable. Ristad(1986)
contains a proof. This result should not be surprising, given that (1) prin-
ciples of universal feature instantiation in current GPSG theory replace the
metarules of earlier versions of GPSG theory, and (2) metarules are known
to cause intractability in GPSG.
&amp;quot;The existence or nonexistence of efficient compilation functions does
not affect either our scientific interest in the universal grammar recognition
problem or the power and relevance of a complexity analysis. If complexity
theory classifies a problem as intractable, we learn that something more
must be said to obtain tractability, and that any efficient compilation step,
if it exists at all, must itself be costly.
°Note that the GPSG we constructed in the preceding reduction will
actually accept any input x of length less than or equal to Itul if and only
if the ATM M accepts it using S(I wl) space. We prepare an input string
x for the GPSG by converting it to the string $0x1 1x22 x„n$n+1 e.g.
abadee is accepted by the ATM if and only if the string $0a1b2a3d4e5e6$7
is accepted by the GPSG. Trivial changes in the grammar allows us to per-
mute and &amp;quot;spread&amp;quot; the characters of a across an infinite class of strings
in an unbounded number of ways, e.g. $011x0:72 • • • &amp;quot;t.zi 1-th +1
where each is a string over an alphabet which is distinct from the cri
alphabet. Although the flexibility of this construction results in a more
complicated GPSG, it argues powerfully against the existence of any effi-
cient compilation procedure for GPSGs. Any efficient compilation proce-
dure must perform more than an exponential polynomial amount of work
(GPSG-Recognition takes at least Exp-Poly time) on at least an exponen-
tial number of inputs (all inputs that fit in the I w space of the ATM&apos;s
read-only tape). More importantly, the required compilation procedure will
convert any exponential-polynomial time bounded Turing Machine into a
polynomial-time TM for the class inputs whose membership can be deter-
mined within a arbitrary (fixed) exp-poly time bound. Simply listing the
accepted inputs will not work because both the GPSG and TM may ac-
cept an infinite class of inputs. Such a compilation procedure would be
extremely powerful.
&apos;Note that compilation illegitimately assumes that the compilation step
</bodyText>
<subsectionHeader confidence="0.951349">
4.2 Complexity and Succinctness
</subsectionHeader>
<bodyText confidence="0.996466054545455">
The major complexity result of this paper proves that the fastest
algorithm for GPSG-Recognition must take more than exponen-
tial time. The immediately preceding section demonstrates ex-
actly how a particular algorithm for GPSG-Recognition (the EP
argument) comes to grief: weak context-free generative power
does not ensure efficient parsability because a GPSG G is weakly
equivalent to a very large CFG C&apos;, and CFG size affects recog-
nition time. The rebuttal does not suggest that computational
complexity arises from representational succinctness, either here
or in general.
Complexity results characterize the amount of resources needed
to solve instances of a problem, while succinctness results mea-
sure the space reduction gained by one representation over an-
other, equivalent, representation.
There is no casual connection between computational com-
plexity and representational succinctness, either in practice or
principle. In practice, converting one grammar into a more suc-
cinct one can either increase or decrease the recognition cost.
For example, converting an instance of context-free recognition
(known to be polynomial time) into an instance of context-
sensitive recognition (known to be PSPACE-complete and likely
to be intractable) can significantly speed the recognition prob-
lem if the conversion decreases the size of the CFG logarithmi-
cally or better. Even more strangely, increasing ambiguity in
a CFG can speed recognition time if the succinctness gain is
large enough, or slow it down otherwise — unambiguous CFGs
can be recognized in linear time, while ambiguous ones require
cubic time.
In principle, tractable problems may involve succinct rep-
resentations. For example, the iterating coordination schema
(ICS) of GPSG is an unbeatably succinct encoding of an infi-
nite set of context-free rules; from a computational complexity
viewpoint, the ICS is utterly trivial using a slightly modified
Earley algorithm.&apos; Tractable problems may also be verbosely
represented: consider a random finite language, which may be
recognized in essentially constant time on a typical computer
(using a hash table), yet whose elements must be individually
listed. Similarly, intractable problems may be represented both
succinctly and nonsuccinctly. As is well known, the Turing ma-
chine for any arbitrary r.e. set may be either extremely small
or monstrously big. Winning the game of chess when played on
an n x n board is likely to be computationally intractable, yet
the chess board is not intended to be an encoding of another
representation, succinct or otherwise.
is free. There is one theory of primitive language learning and use: conjec-
ture a grammar and use it. For this procedure to work, grammars should
be easy to test on small inputs. The overall complexity of learning, testing,
and speech must be considered. Compilation speeds up the speech com-
ponent at the expense of greater complexity in the other two components.
For this linguistic reason the compilation argument is suspect.
IDA more extreme example of the unrelatedness of succinctness and com-
plexity is the absolute succinctness with which the dense language E• may
be represented — whether by a regular expression, CFG, or even Turing
machine — yet members of E• may be recognized in constant time (i.e.
always accept).
</bodyText>
<page confidence="0.996651">
38
</page>
<bodyText confidence="0.999967857142857">
Tractable problems may involve succinct or nonsuccinct rep-
resentations, as may intractable problems. The reductions in
this paper show that GPSGs are not merely succinct encod-
ings of some context-free grammars; they are inherently com-
plex grammars for some context-free languages. The heart of
the matter is that GPSG&apos;s formal devices are computationally
complex and can encode provably intractable problems.
</bodyText>
<subsectionHeader confidence="0.983663">
4.3 Relevance of the Result
</subsectionHeader>
<bodyText confidence="0.998650333333333">
In this paper, we argued that there is nothing in the GPSG for-
mal framework that guarantees computational tractability: pro-
ponents of GPSG must look elsewhere for an explanation of
efficient parsability, if one is to be given at all. The crux of
the matter is that the complex components of GPSG theory
interact in intractable ways, and that weak context-free gener-
ative power does not guarantee tractability when grammar size
is taken into account. A faithful implementation of the GPSG
formalisms of GKPS will provably be intractable; expectations
computational linguistics might have held in this regard are not
fulfilled by current GPSG theory.
This formal property of GPSGs is straightforwardly inter-
esting to GPSG linguists. As outlined by GKPS, &amp;quot;an important
goal of the GPSG approach to linguistics [isl, the construction
of theories of the structure of sentences under which significant
properties of grammars and languages fall out as theorems as
opposed to being stipulated as axioms (p.4).&amp;quot;
The role of a computational analysis of the sort provided
here is fundamentally positive: it can offer significant formal
insights into linguistic theory and human language, and sug-
gest improvements in linguistic theory and real-world parsers.
The insights gained may be used to revise the linguistic theory
so that it is both stronger linguistically and weaker formally.
Work on revising GPSG is in progress. Briefly, some proposed
changes suggested by the preceding reductions are: unit feature
closure, no FCRs or FSDs, no null-transitions in ID rules, meta-
rule unit closure, and no problematic feature specifications in
the principles of universal feature instantiation. Not only do
these restrictions alleviate most of GPSG&apos;s computational in-
tractability, but they increase the theory&apos;s linguistic constraint
and reduce the number of nonnatural language grammars li-
censed by the theory. Unfortunately, there is insufficient space
to discuss these proposed revisions here — the reader is referred
to Ristad(1986) for a complete discussion.
Acknowledgments. Robert Berwick, Jim Higginbotham, and
Richard Larson greatly assisted the author in writing this paper.
The author is also indebted to Sandiway Fong and David Waltz
for their help, and to the MIT Artificial Intelligence Lab and
Thinking Machines Corporation for supporting this research.
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="method">
5 References
</sectionHeader>
<reference confidence="0.92529348">
Barton, G.E. (1985). &amp;quot;On the Complexity of ID/LP Parsing,&amp;quot;
Computational Linguistics, 11(4): 205-218.
Chandra, A. and L. Stockmeyer (1976). &amp;quot;Alternation,&amp;quot; 17th
Annual Symposium on Foundations of Computer Science,:
98-108.
Gazdar, G. (1981). &amp;quot;Unbounded Dependencies and Coordinate
Structure,&amp;quot; Linguistic Inquiry 12: 155-184.
Gazdar, G., E. Klein, G. Pullum, and I. Sag (1985). Gener-
alized Phrase Structure Grammar. Oxford, England: Basil
Blackwell.
Garey, M, and D. Johnson (1979). Computers and Intractabil-
ity. San Francisco: W.H. Freeman and Co.
Hoperoft, J.E., and J.D. Ullman (1979). Introduction to Au-
tomata Theory, Languages, and Computation. Reading,
MA: Addison-Wesley.
Pullum, G.K. (1985). &amp;quot;The Computational Tractability of GPSG,&amp;quot;
Abstracts of the 60th Annual Meeting of the Linguistics So-
ciety of America, Seattle, WA: 36.
Ristad, E.S. (1985). &amp;quot;GPSG-Recognition is NP-hard,&amp;quot; A.I.
Memo No. 837, Cambridge, MA: M.I.T. Artificial Intelli-
gence Laboratory.
Ristad, E.S. (1986). &amp;quot;Complexity of Linguistic Models: A Com-
putational Analysis and Reconstruction of Generalized Phrase
Structure Grammar,&amp;quot; S.M. Thesis, MIT Department of Elec-
trical Engineering and Computer Science. (In progress).
</reference>
<page confidence="0.999429">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591629">
<title confidence="0.997416">COMPUTATIONAL COMPLEXITY OF CURRENT GPSG THEORY</title>
<author confidence="0.999883">Eric Sven Ristad</author>
<affiliation confidence="0.997651">MIT Artificial Intelligence Lab Thinking Machines Corporation</affiliation>
<address confidence="0.815966">Technology Square First Street Cambridge, MA 02139 Cambridge, MA 02142</address>
<abstract confidence="0.996524222222222">An important goal of computational linguistics has been to use linguistic theory to guide the construction of computationally efficient real-world natural language processing systems. At first glance, generalized phrase structure grammar (GPSG) appears to be a blessing on two counts. First, the precise formalisms of GPSG might be a direct and transparent guide for parser design and implementation. Second, since GPSG has weak context-free generative power and context-free languages can be parsed in by a wide range of algorithms, GPSG parsers would appear to run in polynomial time. This widely-assumed GPSG &amp;quot;efficient parsability&amp;quot; result is misleading: here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard, and assuredly intractable. The paper pinpoints sources of complexity (e.g. metarules and the theory of syntactic features) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G E Barton</author>
</authors>
<title>On the Complexity of ID/LP Parsing,&amp;quot;</title>
<date>1985</date>
<journal>Computational Linguistics,</journal>
<volume>11</volume>
<issue>4</issue>
<pages>205--218</pages>
<marker>Barton, 1985</marker>
<rawString>Barton, G.E. (1985). &amp;quot;On the Complexity of ID/LP Parsing,&amp;quot; Computational Linguistics, 11(4): 205-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chandra</author>
<author>L Stockmeyer</author>
</authors>
<date>1976</date>
<booktitle>Alternation,&amp;quot; 17th Annual Symposium on Foundations of Computer Science,:</booktitle>
<pages>98--108</pages>
<marker>Chandra, Stockmeyer, 1976</marker>
<rawString>Chandra, A. and L. Stockmeyer (1976). &amp;quot;Alternation,&amp;quot; 17th Annual Symposium on Foundations of Computer Science,: 98-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Unbounded Dependencies and Coordinate Structure,&amp;quot;</title>
<date>1981</date>
<journal>Linguistic Inquiry</journal>
<volume>12</volume>
<pages>155--184</pages>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G. (1981). &amp;quot;Unbounded Dependencies and Coordinate Structure,&amp;quot; Linguistic Inquiry 12: 155-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<date>1985</date>
<booktitle>Generalized Phrase Structure Grammar.</booktitle>
<publisher>Basil Blackwell.</publisher>
<location>Oxford, England:</location>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., E. Klein, G. Pullum, and I. Sag (1985). Generalized Phrase Structure Grammar. Oxford, England: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garey</author>
<author>D Johnson</author>
</authors>
<date>1979</date>
<journal>Computers</journal>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M, and D. Johnson (1979). Computers and Intractability. San Francisco: W.H. Freeman and Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>Hoperoft, J.E., and J.D. Ullman (1979). Introduction to Automata Theory, Languages, and Computation. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Pullum</author>
</authors>
<title>The Computational Tractability of GPSG,&amp;quot;</title>
<date>1985</date>
<booktitle>Abstracts of the 60th Annual Meeting of the Linguistics Society of America,</booktitle>
<pages>36</pages>
<location>Seattle, WA:</location>
<marker>Pullum, 1985</marker>
<rawString>Pullum, G.K. (1985). &amp;quot;The Computational Tractability of GPSG,&amp;quot; Abstracts of the 60th Annual Meeting of the Linguistics Society of America, Seattle, WA: 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
</authors>
<title>GPSG-Recognition is NP-hard,&amp;quot;</title>
<date>1985</date>
<journal>A.I. Memo</journal>
<volume>837</volume>
<institution>M.I.T. Artificial Intelligence Laboratory.</institution>
<location>Cambridge, MA:</location>
<marker>Ristad, 1985</marker>
<rawString>Ristad, E.S. (1985). &amp;quot;GPSG-Recognition is NP-hard,&amp;quot; A.I. Memo No. 837, Cambridge, MA: M.I.T. Artificial Intelligence Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
</authors>
<title>Complexity of Linguistic Models: A Computational Analysis and Reconstruction of Generalized Phrase Structure Grammar,&amp;quot;</title>
<date>1986</date>
<tech>S.M. Thesis,</tech>
<institution>MIT Department of Electrical Engineering and Computer Science. (In progress).</institution>
<marker>Ristad, 1986</marker>
<rawString>Ristad, E.S. (1986). &amp;quot;Complexity of Linguistic Models: A Computational Analysis and Reconstruction of Generalized Phrase Structure Grammar,&amp;quot; S.M. Thesis, MIT Department of Electrical Engineering and Computer Science. (In progress).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>