<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.901986">
A Bayesian Model For Morpheme and Paradigm Identification
</title>
<author confidence="0.737015">
Matthew G. Snover and Michael R. Brent
</author>
<affiliation confidence="0.728077666666667">
Department of Computer Science
Campus Box 1045
Washington University
</affiliation>
<address confidence="0.860097">
St. Louis, MO 63130-4899
</address>
<email confidence="0.804133">
{ms9, brent}Acs.wustl.edu
</email>
<sectionHeader confidence="0.988333" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988665">
This paper describes a system for un-
supervised learning of morphological af-
fixes from texts or word lists. The system
is composed of a generative probability
model and a search algorithm. Experi-
ments on the Wall Street Journal and the
Hansard Corpus (French and English)
demonstrate the effectiveness of this ap-
proach. The results suggest that more
integrated systems for learning both af-
fixes and morphographemic adjustment
rules may be feasible. In addition, sev-
eral definitions and a theorem are devel-
oped so that our search algorithm can be
formalized in terms of the lattice formed
by subsets of suffixes under inclusion.
This formalism is expected to be use-
ful for investigating alternative search
strategies over the same morphological
hypothesis space.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984933884298">
Systems for morphological analysis were one of the
early successes in computational linguistics (Kart-
tunen and Wittenburg, 1983; Karttunen, 1983).
Morphological analyzers are now a key component
of most natural language applications and they
continue to be a focus of research interest (Sproat,
1992). These systems are knowledge-intensive, re-
quiring a stem lexicon, a list of affixes, morpho-
tactic rules specifying the types of words to which
each affix can apply, and spelling adjustment rules
such as the English rule that inserts an e between
a stem-final sybillant and the suffix -s in words like
churches. Adaptation of a morphological analysis
system to a new language requires considerable
expertise with both the language and the mor-
phological analyzer.
In the 1990&apos;s, the focus of research in syn-
tactic parsing shifted from brittle, knowledge-
intensive systems to systems based on probabilis-
tic inference and unsupervised learning. Perhaps
surprisingly, given its established place in com-
putational linguistics, computational morphology
participated in this shift in only a very limited
way. Brent (1993) and Brent et al. (1995) de-
scribed systems for suffix-discovery based on Min-
imum Description Length (MDL), an approach
that is closely related to Bayesian probabilistic
modeling. One system attempted to find the set of
suffixes in the language using only the spellings of
the words; another used the syntactic categories
from a tagged corpus as well. Goldsmith (2000)
describes another MDL system based on the
spellings of words and the set of suffixes that ap-
pears with each stem, which Goldsmith calls the
signature of the word. Gaussier (1999) reports on
an explicitly probabilistic system that is based
primarily on spellings, while Dejean (1998) de-
scribes a heuristic approach. Unlike the meth-
ods described above, the memory-based algo-
rithm of van den Bosch and Daelemans (1999)
requires supervised training. However, this sys-
tem has the advantage that its output is a
full morphological analysis, with syntactic cate-
gory labels, not merely a splitting of the word
into stem and suffix. Schone and Jurafsky (2000)
take a completely different approach based on
latent semantic analysis; words that lie near
one another in the latent semantic space are
treated as more likely to be morphologically re-
lated. Yarowsky and Wicentowski (2000) have
developed a statistical system that takes a set of
regular suffixes as input and learns spelling change
rules and irregular morphology. This system uses
four different statistical models for identifying
words that are morphologically related. One of
these models is similar to the latent semantic anal-
ysis method of Schone and Jurafsky (2000).
In this paper, we describe an unsupervised
learning system that is designed to identify a small
number of highly productive suffixes with a very
low false positive rate. The system is intended to
work cross-linguistically without the need to ad-
just any free parameters. In particular, we were
concerned that the number of suffixes returned
by many systems grows as the number of distinct
words in the input grows. Thus, input size may
serve as a kind of hidden free parameter good
performance may depend on picking exactly the
right input size (Brent et al., 1995). Thus, we
set out to develop a system that yields very few
false positives over a wide range of input sizes in
multiple languages. Experiments on French and
English suggest that we have made substantial
progress toward this goal.
We emphasize a low false positive rate be-
cause the suffix-discovery system reported here
is intended as a first step toward a system for
learning a complete morphological analysis, in-
cluding irregular morphology, morphosyntax, and
spelling adjustment rules. For example, the
suffixes discovered by the system described be-
low could be used as one of the inputs to the
Yarowsky and Wicentowski (2000) system. We
believe that the best strategy for learning a com-
plete morphological analysis is to start with a
small set of highly productive suffixes containing
very few false positives. For example, a spelling
adjustment rule like deletion of stem-final e be-
fore suffixes beginning in vowels (rise+ing—&gt;-rising,
rise+er—&gt;Tiser) can be learned on the basis of
any suffix beginning in a vowel. Once the rule
is learned, it can help identify other suffixes be-
ginning in vowels. Thus, it may be more effective
to be conservative at first and count on step-by-
step learning to reveal linguistic regularities that
are missed early on.
The system described in this paper is based on
an explicit probability model and a search algo-
rithm. The model assigns probability to any pos-
sible hypothesis about the correct morphological
analysis of the input. A hypothesis consists of a
division of every word in the input into a stem and
suffix, where the suffix may be the empty string.
The model described below assigns probability to
any given hypothesis based on (1) the hypothe-
sized number of stems and suffixes, (2) the lengths
of the hypothesized stems and suffixes, (3) the
number of suffixes hypothesized to occur with each
stem, and (4) the frequency of each letter in the
hypothesized stem and suffix sets. The search al-
gorithm explores a series of hypotheses, replacing
its current hypothesis whenever a more probable
one is found.
The remainder of this paper is organized as fol-
lows. The next section describes the generative
probability model. The third section describes
the search algorithm. The fourth section describes
an experiment on words extracted from the Wall
Street Journal and the Hansard French and En-
glish corpora. The final section discusses the re-
sults and their implications.
</bodyText>
<sectionHeader confidence="0.932127" genericHeader="method">
2 Probability Model
</sectionHeader>
<bodyText confidence="0.9993037">
This section introduces a language-independant
probability distribution on all possible hypothe-
ses, where a hypothesis is a division of every word
in the input into a stem and a (possibly empty)
suffix. The distribution is based on a five-step
model for the generation of hypotheses. The steps
are presented below, along with their probability
distributions. What we describe in this section is
a mathematical model, not an algorithm that is
intended to be run.
</bodyText>
<listItem confidence="0.813863">
1. Choose the number of stems, M, according
to the distribution:
</listItem>
<equation confidence="0.9986245">
Pr(M) = —m
6 ( 1 )2
</equation>
<bodyText confidence="0.82736625">
The 6/72 term normalizes the inverse-
squared distribution on the positive integers.
Choose the number of suffixes, X, according
to the same distribution. The symbols M for
steMs and X for suffiXes are used throughout
this paper.
2. For each stem i, choose its length in letters,
Pin, according to the inverse squared distribu-
tion on the positive integers. Assuming that
the stem lengths are chosen independently
and multiplying together their probabilities,
we have:
</bodyText>
<equation confidence="0.9878765">
Pr(Pn M) = ()114- fl ( 1 ) 2 (2)
7F Dri
</equation>
<bodyText confidence="0.9900038">
For each suffix i, choose its length in letters,
LT, according to the same distribution. As-
suming that the suffix lengths are chosen in-
dependently, the joint distribution on suffix
lengths has the same form as (2).
</bodyText>
<listItem confidence="0.999777272727273">
3. Let E be the alphabet and let {pi ...vii}
be a probability distribution on E. For each
i from 1 to M, generate stem i by choos-
ing Lrin letters at random, according to the
probabilities {pi ...piEi }. Call the resulting
stem set STEM. Similarly, for each i from
1 to X, generate suffix i by choosing LT let-
ters at random, according to the probabili-
ties {pi ...piEl }. Call the resulting suffix set
SUFF. To compute the joint probability of
hypothesized stem and suffix sets under this
</listItem>
<equation confidence="0.719928">
(1)
</equation>
<bodyText confidence="0.8299185">
model we use the maximum likelihood esti- • The lengths of the stems (Pm) and suffixes
mates of the letter probabilities: (Lx)
</bodyText>
<listItem confidence="0.9401715">
• The number of suffixes that occur with each
stem (Freq)
</listItem>
<equation confidence="0.976757666666667">
Cl
= —
S
</equation>
<bodyText confidence="0.935821666666667">
where c1 is the frequency count of letter 1
among all the hypothesized stems and suf-
fixes, and S = El el. Thus,
Pr(STEM, SUFF I M, n, X, Lx) (3)
= M!X! 1-1 (ei )c&apos;
/EE
The factorial terms reflect the fact that the
stems in the set STEM can be generated in
any order, as can the suffixes in the set SUFF.
</bodyText>
<listItem confidence="0.9922288">
4. For each stem i in STEM, choose the number
of suffixes, Freq, which i will be paired with
in order to generate the lexicon. Since Freqi
is an integer between one and X we can use
a uniform distribution:
</listItem>
<equation confidence="0.948144">
1
Pr(Freqi I M, X) =
</equation>
<bodyText confidence="0.841986">
Assuming all these choices are made indepen-
dently and multiplying together their proba-
bilities yields:
</bodyText>
<equation confidence="0.912354333333333">
Pr(Freq I M, X) =
1 ) M
(4)
</equation>
<bodyText confidence="0.934993">
5. For each stem i in STEM, choose a set of suf-
fixes, Di, of size Freq, that i will be paired
with in order to generate the lexicon. The
number of subsets of a given size is finite,
so we can again use the uniform distribution.
This implies that the probability of each in-
dividual subset of size Freqi is the inverse of
the total number of such subsets:
Ix)-1
Vreqd
Assuming that all these choices are indepen-
dent yields:
</bodyText>
<equation confidence="0.451112">
Pr(D I M, X, Freq) =
</equation>
<bodyText confidence="0.9999166">
The probability of a hypothesis about how to
split every word in the input lexicon can be com-
puted by evaluating equations (1)-(5) and multi-
plying the results. These equations depend only
on the following characteristics of the hypothesis:
</bodyText>
<listItem confidence="0.969002666666667">
• The number of stems (M) and suffixes (X)
• The count of each letter in the combined stem
and suffix sets ({Cl}).
</listItem>
<bodyText confidence="0.998971588235294">
Intuitively, the primary determinants of the
probability of a hypothesis are the total number
of characters in the stem and suffix sets (E/ Cl)
and the number of suffixes X. Every decision
about whether to add a suffix to the current hy-
pothesis reflects a tradeoff between these oppos-
ing forces. Adding a frequent suffix, such as -Mg,
eliminates many copies of -mg from many differ-
ent stems, but it only adds one copy to the suffix
set. This reduces the total number of characters
greatly, and therefore increases the probability in
(3). However, it also increases the number of suf-
fixes, greatly decreasing the probabilities in (4)
and (5). Other determinants of probability of a
hypothesis, such as the number of stems and the
lengths of the stems, have a lesser impact on the
overall probability of a hypothesis.
</bodyText>
<sectionHeader confidence="0.978188" genericHeader="method">
3 Search
</sectionHeader>
<bodyText confidence="0.999982333333333">
This section describes the search algorithm we
used to find the most likely segmentations of all
the words in the input lexicon L. We first define
the hypothesis space in terms of a lattice of sets of
suffixes. This provides a very general framework
within which a variety of search algorithms can be
stated. Next, we define the initial hypothesis for
our search. Finally, we describe our algorithm for
finding a local maximum in the hypothesis space.
</bodyText>
<subsectionHeader confidence="0.998622">
3.1 The Hypothesis Space
</subsectionHeader>
<bodyText confidence="0.9800764">
The input to our morphology induction system
is a set of words, or lexicon, L. We define the
hypothesis space for this system in terms of the
set of all possible stems in L, pStem, and the set
of all possible suffixes in L, pSuff. The empty
string is considered to be a possible suffix but not
a possible stem. A hypothesis h is a function from
the set of possible stems to sets of suffixes:
h : pStem 2PSuff
where h(m) is interpreted as the set of suffixes that
occur with stem m in the input. The set of words
generated by a stem m under hypothesis h is sim-
ply the concatenation of m with all the suffixes in
h(m). For example, if h(walk) = {E, s, ing} then
the stem walk generates the words walk, walks,
</bodyText>
<equation confidence="0.7644029">
Pr(D I M, X, Freq) =
_1
x
Freq,i)
(5)
Level 2
3
s = {&amp;quot;&amp;quot;, &amp;quot;s&amp;quot;}
m = {&amp;quot;build&amp;quot;}
Level 1
1
2
s = {&amp;quot;&amp;quot;}
m = {&amp;quot;building&amp;quot;}
s = {&amp;quot;s&amp;quot;}
m = {}
Level 0
0
s = {}
m = {&amp;quot;builds&amp;quot;}
</equation>
<listItem confidence="0.974426666666667">
1. If x E h(mi) or mix L, Prom(mi, x, h) =
h.
2. Otherwise, Prom(mi, x, h) (m) =
</listItem>
<equation confidence="0.923731666666667">
h(m) U {x}
if m = mi
h(m) — {CompX(mi, x, h)}
if m = CompM(mi, x, h)
h(m)
otherwise
</equation>
<bodyText confidence="0.998390875">
Unlike promotion, demotion of a stem mi from
a node containing suffix x to an adjacent node
not containing x is always possible. The result is
that mi no longer generates word mix, so some
other stem must be moved up to a node in which
it generates mx. There is always at least one stem
that can be moved up so that it generates mx
namely, the stem that is equal to mx can be moved
up one level from its current node to the node that
also contains the empty suffix. (Its current node
cannot contain the empty suffix because otherwise
the word mx would have been generated twice).
Definition 3 Let L be an input lexicon, mi a
stem in pStem, x a suffix in pSuff, and h a hy-
pothesis. Then Dem(mi , x, h) is the hypothesis
such that:
</bodyText>
<listItem confidence="0.997104">
1. If x h(mi), Dem(mi, x, h) = h.
2. Otherwise, Dem(mi, x, h) (m) =
</listItem>
<equation confidence="0.9957064">
h(m) — {x} if m = mi
h(m) U fel if m = mix
lh(m) otherwise
Remark. Dem(mi, x, h) is equivalent to
Prom(mi x , E, h). Thus, Dem is a special
</equation>
<bodyText confidence="0.990385344827586">
case of Prom and all properties that hold for
Prom(mi, x, h) regardless of mi, x, or h also hold
for Dem.
The search algorithm used in the experiments
reported below applies the promotion and demo-
tion operators sequentially to all stems that map
to a given node. It is convenient to define this
operation by overloading the functions Prom and
Dem as follows:
Definition 4 Let L be an input lexicon, n, a node
in the subset lattice of pSuff, x a suffix in pSuff,
and h a hypothesis. Let {ml, , mk} be the set
of stems that map to node n, under hypothesis h.
Then Prom(n,, x, h) is the result of composing the
promotion operators for the k stems:
Prom(mi , x, Prom(m2, x, Prom(mk , x, h)))
Similarly, Dem(n,, x, h) is defined as the compo-
sition of demotion operators for all the stems in
node r.
Theorem 1 guarantees that the order of compo-
sition does not affect the result, and hence that
these functions are well defined.
Theorem 1 Let x a suffix in pSuff, h a hypoth-
esis, and {mi, ,m} a set of stems in pStem.
The hypothesis
P r OM( 7111, x, Prom(m2, x, . . . Prom(mk , x, h)))
is invariant under permutation of the subscripts.
A sketch of the proof can be found in Ap-
pendix A.
</bodyText>
<subsectionHeader confidence="0.979694">
3.4 Search Algorithm
</subsectionHeader>
<bodyText confidence="0.959619">
The search used in the experiments reported be-
low alternates between a promotion phase and de-
motion phase until neither phase can improve the
score further.
Search
</bodyText>
<listItem confidence="0.988327">
1: h = InitialHypothesis
2: While (Probability increases)
3: PromotionPhase
4: DemotionPhase
</listItem>
<bodyText confidence="0.999816076923077">
The promotion phase loops through all possible
suffixes. For each suffix, it loops through all nodes
that contain at least one stem and one suffix (the
node for the empty set of suffixes is not included).
For each suffix/node combination, it evaluates the
hypothesis that would result from applying the
promotion operator to that node and that suffix.
If the probability of the hypothesis resulting from
promotion is greater than the probability of the
current hypothesis then the promotion is carried
out. For each suffix x, the loop through all nodes
is restarted whenever a promotion is carried out.
PromotionPhase
</bodyText>
<listItem confidence="0.990595">
1: For each xE pSuff
2: For each nE lattice(pSuff)
3: if Pr(Prom(n, x, h)) &gt; Pr(h)
4: h = (Prom(n, x, h))
5: Goto 2
</listItem>
<bodyText confidence="0.996815">
The demotion phase is exactly analogous.
</bodyText>
<subsectionHeader confidence="0.964306">
3.5 Suffix Ordering Heuristic
</subsectionHeader>
<bodyText confidence="0.8285082">
The order in which PromotionPhase and
DemotionPhase iterate through possible suffixes
is determined by the relative frequency of the
suffix, as a string, divided by the relative frequen-
cies of its component letters. For example if c
is the total number of characters in the lexicon,
ffingv(c-2)
the rank of -ing would be This
f (1) f (n) f (g) / c3
formula reflects the degree to which the observed
</bodyText>
<table confidence="0.997094125">
Size WSJ Hansard English
500 €5 €5
1,000 E S E s d ed ing ly
2,000 E S E s d ed ing ly
4,000 E s d ed ing s ly E s d ed ing ly
8,000 E s d ed ing s ly E s d ed ing ly
16,000 E s d ed ing s ly E s d ed ing ly
32,000 N/A E s d ed ing ly
</table>
<tableCaption confidence="0.996268">
Table 1: English
</tableCaption>
<table confidence="0.99929475">
Size Hansard French
500 €5
1,000 E s
2,000 E s r
4,000 E s r
8,000 E s r nt ment
16,000 E s r nt ment e es it ient
32,000 E s r nt ment e es it ient ront
</table>
<tableCaption confidence="0.998516">
Table 2: French
</tableCaption>
<bodyText confidence="0.9997295">
relative frequency of a suffix exceeds what would
be expected under a null model in which letters
are chosen independently. Suffixes are processed
in order from highest rank to lowest in order to
give priority to those that are most likely to be
productive.
</bodyText>
<sectionHeader confidence="0.997973" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.9999716">
We tested a suffix discovery system consisting
of the probability model described in Section 2
and the search algorithm described in Section 3.
The input consisted of word lists extracted from
the TreeBank Wall Street Journal corpus and
the French and English versions of set A of the
Hansard Corpus. The Hansard corpora are pro-
ceedings of the Canadian Parliament, transcribed
into French or English and translated into the
other language.
</bodyText>
<subsectionHeader confidence="0.829604">
4.1 Input
</subsectionHeader>
<bodyText confidence="0.999994777777778">
We extracted input lexicons from each corpus, ex-
cluding words containing capital letters or non-
alphabetic characters. The 100 most common
words in each corpus were also excluded, since
these words tend not to have regular inflection.
The system was run on the 500, 1,000, 2,000,
4,000, 8,000, 16,000, and 32,000 most common re-
maining words. The Treebank corpus only had
enough words to run through 16,000.
</bodyText>
<sectionHeader confidence="0.729928" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.998888581818182">
Table 1 shows the suffixes found in lexicons ex-
tracted from each of the English corpora at each
lexicon size. All productive inflectional suffixes,
as well as the adjective-generating suffix /y, are
found by the time the input size reaches 1,000
words from the Hansard English corpus and 4,000
words from the WSJ corpus. Two variants of the
past tense and past participle suffix are found:
ed, the general form, and d. In reality, these
should be treated as single suffix, ed, together with
a spelling-change rule that deletes the &amp;quot;e&amp;quot; after
words ending in a vowel.
Table 2 shows the suffixes found in the lexicon
extracted from the Hansard French corpus at each
lexicon size. The suffixes E and s are found at all
lexicon sizes, reflecting the regular plural inflec-
tion of nouns and adjectives. At 2,000 words the
suffixes r is hypothesized, reflecting the fact that
verbal infinitives end in &amp;quot;r&amp;quot;. Consider, for exam-
ple, the present indicative paradigm of the verb
whose infinitive is parler:
je parle nous parlons
tu parles vous parlez
il parle ils parlent
The system takes park as the stem and r as the
suffix on the infinitive. All the errors, which are
truncations of true suffixes, can be understood as
resulting from the failure to include the &amp;quot;e&amp;quot; in such
cases as part of the stem. Errors are underlined in
Table 2. However, it should be emphasized that
these are relatively benign errors they are not
fictitious suffixes pulled out of thin air, but gen-
uine suffixes that were split from the stem at the
wrong place. At 8,000 words nt is split off of third
person plural verbs and ment is split off of ad-
verbs. At 16,000 words two changes occur. First,
e is split of of feminine singular adjectives and
past participles, and es is split off of feminine plu-
ral adjectives and past participles. (The suffix e
is not split off of tensed verbs, since the system
treats it as part of the stem.) Second, the sys-
tem splits it and ient of off imperfects, and nt off
both third person plural indicatives and present
participles. Here, it is treating an &amp;quot;a&amp;quot; as part of
the stem when the normal analysis puts it in the
suffixes ait, aient, and ant. Thus, it posits a sec-
ond stem ending in &amp;quot;a&amp;quot;, such as parla, for many
of the verb stems that it treats as ending in &amp;quot;e&amp;quot;,
such as parle. At 32,000 words ront is split off
of third person plural verbs in the future tense,
such as parleront. At no point is a single stem,
such as pan, identified for all forms of the verb.
Instead, the system eventually posits two stems,
parle and parla. This behavior is explained in the
next sections.
</bodyText>
<subsectionHeader confidence="0.977499">
4.3 Discussion of the Experiment
</subsectionHeader>
<bodyText confidence="0.999979064516129">
Our goal was to develop a system capable of iden-
tifying a small number of highly productive suf-
fixes, with very few false positives, across a wide
range of input sizes. The low false positive rate
was emphasized because this system is intended
as the first stage of a much more ambitious system
for learning irregular morphology, morphosyntax,
and spelling adjustment rules as well as suffixes. It
is best be conservative at the first stage, since an
early mis-step could be magnified at later stages
of processing.
In light of this goal, the results on both of the
English corpora were excellent. All of the produc-
tive regular inflectional suffixes were discovered by
1,000-4,000 words and remained stable over sev-
eral binary orders of magnitude. In additon to
the standard forms of the regular inflections, d
was hypothesized to be a suffix and was split off
of words like closed and used the past forms
of stems ending in &amp;quot;e&amp;quot;. The correct analysis in
this case requires a spelling change: the suffix is
ed, but when the stem ends in &amp;quot;e&amp;quot; one of the two
&amp;quot;e&amp;quot; &apos;s is deleted. A hypothesized suffix set includ-
ing both ed and d provides a good starting point
for a possible future system aimed at discovering
spelling change rules. There are a few moderately
productive derivational suffixes, such as er, which
the sytem did not choose. This is presumably be-
cause the probability model favors only the most
productive suffixes. The model could easily be
changed to identify suffixes such as -it -er, but it
is not clear whether this can be done without also
outputting other common word-endings that are
not morphemes.
The results in French are also good, but they
suffer from one persistent error: the first vowel of
the suffix is often misanalyzed as part of the stem.
This results in hypothesizing two stems, such as
parle and parla, for words that should share a
single stem, such as parl. In fact, with the cur-
rent search procedure and initial hypothesis, it is
only possible to hypothesize stems that appear as
words in the input lexicon. Thus, parle and parla,
both of which appear in the input, can be hypthe-
sized, but parl, which is not a word, cannot. To
see this, recall that the initial hypothesis maps all
stems that occur as words to the lattice node con-
taining the empty suffix; it maps all other stems to
the lattice node containing no suffixes (the unique
node at level 0). The search never applies the
Prom operator to this node directly. Further, Def-
inition 3 specifies that the compensating stem for
a demotion always occurs as a word in the lexicon.
This limitation to stems that occur as words only
became apparent when the system was tested on
French, since it has no ill effect on the English re-
sults. The next section discusses overcoming this
limitation by changing the initial hypothesis.
It worth noting that, although the exact divi-
sion between stem and suffix may not be right,
every suffix the system hypothesized corresponds
to an actual productive morpheme.
</bodyText>
<sectionHeader confidence="0.993753" genericHeader="conclusions">
5 General Discussion
</sectionHeader>
<bodyText confidence="0.999597">
The lattice of suffix sets and the invariants and
definitions presented above provide a useful for-
malism for describing alternative search algo-
rithms. Some of the improvements we hope to
make are sketched briefly in this section.
Improving the initial hypothesis Our immedi-
ate goal is to enable the system to hypothesize
stems that do not appear in the input, such as the
French parl. The approach will be to construct a
good starting hypothesis in a preprocessing phase
by heuristically evaluating individual nodes in the
suffix lattice and greedily filling the best ones with
as many stems as possible. The invariants will be
satisfied only when the initial hypothesis has been
completed. Because it is greedy, this process will
not find an optimal hypothesis, but it is expected
to improve on the current initial hypothesis. This
has been implemented and preliminary results are
very promising.
Pulling stems to good nodes The current search
carries out promotions and demotions without any
particular goal node toward which stems are be-
ing moved. The only goal is to improve the over-
all probability of the entire hypothesis. Using
a heuristic evaluation of individual lattice nodes,
however, it should be possible to develop a more
directed search that attempts to move stems to-
ward a highly valued lattice node. We believe that
it will be possible to efficiently determine whether
a given stem can move to a given node without vio-
lating the invariants, and if so, what other changes
must be made to maintain the invariants. This
would make it possible to implement a new search
operator, which might be called Pull(n, h). This
operator would check each stem to see (1) whether
it can be moved to node n, (2) if so, what com-
pensating actions are necessary to maintain the
invariants, and (3) whether such a move would
improve the overall probability of the hypothesis.
It would move each stem that can be moved to
good effect.
Relaxing the invariants Invariant 1 ensures that
a hypothesis generates each word in the input in
exactly one way and does not generate any other
words. This invariant could be relaxed somewhat,
allowing the system to hypothesize that a given
form of a word exists, but was not seen in the
input. One reason a given form might not be in
the input is sampling error. We plan to modify
the probability model so that it can use word fre-
quency to estimate the probability that a given
form was missed due to sampling error.
This is just a sampling of the interesting al-
gorithms that can be investigated using the for-
malisms developed here. These avenues of re-
search are likely to lead to better, more com-
prehensive systems for automated morphological
analysis.
</bodyText>
<sectionHeader confidence="0.992518" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.82629884">
M. R. Brent, S. K. Murthy, and A. Lundberg.
1995. Discovering morphemic suffixes: A case
study in minimum description length induction.
In Proceedings of the Fifth International Work-
shop on Artificial Intelligence and Statistics, Ft.
Laudersdale, FL.
M. R. Brent. 1993. Minimal generative models:
A middle ground between neurons and triggers.
In Proceedings of the 15th Annual Conference
of the Cognitive Science Society, pages 28-36,
Hillsdale, NJ. Erlbaum.
H. Dejean. 1998. Morphemes as necessary con-
cepts for structures: Discovery from untagged
corpora. http://www.info.unicaen.fr/ De-
Jean/travail/articles/pg11.htm.
E. Gaussier. 1999. Unsupervised learning of
derivational morphology from inflectional lexi-
cons. In ACL &apos;99 Workshop Proceedings: Un-
supervised Learning in Natural Language Pro-
cessing. ACL.
J. Goldsmith. 2000. Unsupervised learn-
ing of the morpholgy of a natural language.
http://humanities.uchicago.edu/faculty/goldsmith
L. Karttunen and K. Wittenburg. 1983. A two-
level morphological analysis of english. Texas
</bodyText>
<subsubsectionHeader confidence="0.318209">
Linguistics Forum 22, 22:217-223.
</subsubsectionHeader>
<reference confidence="0.8243410625">
L. Karttunen. 1983. KIMMO: a general morpho-
logical processor. Texas Linguistics Forum 22,
22:165-186.
P. Schone and D. Jurafsky. 2000. Knowledge-
free induction of morphology using latent se-
mantic analysis. In Proceedings of the Con-
ference on Computational Natural Language
Learning. Conference on Computational Natu-
ral Language Learning.
Richard Sproat. 1992. Morphology and Computa-
tion. MIT Press, Cambridge, MA.
Lemma 1 If mi m2 then
CompM(mi, x, h) = CompM(mi, x, Prom(m2, x, h)),
CompX(mi, x, h) = CompX(mi, x ,Prom(m2, x, h)).
Proof. By Definition 1,
CompM(mi, x, h)CompX(mi, x, h) = mix,
</reference>
<bodyText confidence="0.7693574">
so CompM(mi, x ,h) and CompX(mi, x, h) sat-
isfy the first condition of the definition of
CompM(mi, x , hi) and CompM(mi, x, hi) for any
hypothesis h&apos;. To show that they also satisfy the
second condition, we must show that
</bodyText>
<equation confidence="0.9107885">
CompX(mi, x, h)
E Prom(m2, x, h) (CompM(mi , x, h))
</equation>
<bodyText confidence="0.766328277777778">
By definition 1, we know
CompX(mi, x, h) E h(CompM(mi, x, h)), (6)
so if we are in a case where Prom(m2, x, h) = h
then the lemma is established. Otherwise, defini-
tion 2 gives us:
Prom(m2, x, h) (CompM(mi , x, h)) =
h(CompM(mi, x, h)) U {x} (a)
if CompM(mi, x, h) — rn2
h(CompM(mi, x, h)) — {CompX(m2, x, h)} (b)
if CompM(mi, x, h) = CompM(m2, x, h)
h(CompM(mi, x, h)) otherwise (c)
A. Van den Bosch and W. Daelemans. 1999.
Memory-based morphilogical analysis. In Proc.
of the 37th Annual Meeting of the ACL. ACL.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of ACL-
2000, pages 207-216. ACL.
</bodyText>
<figure confidence="0.7735072">
A Proof of Theorem 1
&apos;Thus, if CompM(mi, x, h) CompM(m2, x, h)
then
h(CompM(mi, x, h))
C Prom(m2, x, h) (CompM(mi , x, h))
</figure>
<bodyText confidence="0.940747090909091">
and the lemma is established. Finally, suppose
CompM(mi, x, h) = CompM(m2, x , h) and hence
case (b) applies. Then
CompX(mi, x, h) CompX(m2, x, h); (7)
otherwise we would have:
mix = CompM(mi, x, h)CompX(mi, x, h)
= CompM(m2, x, h)CompX(m2, x, h)
m2x,
and hence m1 = m2, contradicting the hypothesis
of the lemma. Since we are in case (b), (7) and
(6) imply that
</bodyText>
<equation confidence="0.962917">
CompX(mi, x, h)
E Prom(m2, x, h)(CompM(mi , x, h))
</equation>
<bodyText confidence="0.999485636363636">
and the lemma is established for all cases.
It is sufficient to show that
Prom(mi , x, Prom(m2, x, h)) (a)
is invariant under swapping of the subscripts.
Space limitations permit only a sketch of the
proof. Expanding (a) by two applications of Def-
inition 2 yields 9 cases. If mi = m2 it obvious
that (a) is invariant under subscript swapping, so
we assume ml m2. This allows us to drop the
first case and apply Lemma 1 to simplify the 8
remaining cases to:
</bodyText>
<equation confidence="0.9451282">
h(m) — {CompX(m2, x, h)} U {x} (2)
if m = m1 = CompM(m2, x, h)
h(m) U {x} (3)
if m = ml m2, m CompM(m2, x, h)
(h(m) U {x}) — {CompX(mi, x, h)} (4)
if m = CompM(mi, x, h) = m2
h(m) — {CompX(m2, x , h)} (5)
— {CompX(mi, x, h)}
if m = CompM(mi, x, h) = CompM(m2, x, h)
h(m) — {CompX(mi, x, h)} (6)
if m = CompM(mi, x, h),
m CompM(m2, x , h), and m rn2
h(m) U {x} (7)
if m CompM(mi, x, h), m mt, m = m2
h(m) — {CompX(m2, x , h)} (8)
</equation>
<bodyText confidence="0.923171041666667">
if m ml, m = CompM(m2, x, h),
and m CompM(mi, x, h)
h(m) otherwise (9)
For cases (5), and (9), swapping the subscripts
does not change either the condition or the con-
sequent. For cases (3) and (7), swapping sub-
scripts swaps the conditions, but the consequents
are identical. For cases (6) and (8), swapping sub-
scripts swaps both the conditions and the conse-
quents, leaving the overall definition unchanged.
For cases (2) and (4), swapping the subscripts
clearly swaps the conditions; it also swaps the con-
sequents, so long as x CompX(mi, x, h) and
x CompX(m2, x, h). Now suppose one of those
two conditions does not hold.
Without loss of generality, suppose x =
CompX(m2, x, h). By Lemma 2 (see below),
Prom(m2, x, h) = h. Thus,
Prom(mi , x, Prom(m2, x, h)) = Prom(mi , x, h)
We now establish the theorem by showing that
Prom(mi , x, h) = Prom(m2, x, Prom(mi , x, h))
By Lemma 1,
CompM(m2, x, h) = CompM(m2, x, Prom(mi, x, h))
Thus, we have
</bodyText>
<equation confidence="0.778434666666667">
x = CompM(m2, x, Prom(mi, x, h)).
Now Lemma 2 can be applied, substituting
Prom(mi, x, h) for h&apos;. This yields
Prom(m2, x, Prom(mi, x, h)) = Prom(mi , x, h)
Lemma 2 If x = CompX(m2, x, h1) then
Prom(m2, x, h&apos;) = h&apos;. Proof omitted.
</equation>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981446">
<title confidence="0.999949">A Bayesian Model For Morpheme and Paradigm Identification</title>
<author confidence="0.999974">Matthew G Snover</author>
<author confidence="0.999974">Michael R Brent</author>
<affiliation confidence="0.999766">Department of Computer Science</affiliation>
<address confidence="0.999213">Campus Box 1045</address>
<affiliation confidence="0.998161">Washington University</affiliation>
<address confidence="0.999738">St. Louis, MO 63130-4899</address>
<email confidence="0.999229">ms9Acs.wustl.edu</email>
<email confidence="0.999229">brentAcs.wustl.edu</email>
<abstract confidence="0.999266619047619">This paper describes a system for unsupervised learning of morphological affixes from texts or word lists. The system is composed of a generative probability model and a search algorithm. Experiments on the Wall Street Journal and the Hansard Corpus (French and English) demonstrate the effectiveness of this approach. The results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible. In addition, several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion. This formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>KIMMO: a general morphological processor.</title>
<date>1983</date>
<journal>Texas Linguistics Forum</journal>
<volume>22</volume>
<pages>22--165</pages>
<contexts>
<context position="1147" citStr="Karttunen, 1983" startWordPosition="172" endWordPosition="173">his approach. The results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible. In addition, several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion. This formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space. 1 Introduction Systems for morphological analysis were one of the early successes in computational linguistics (Karttunen and Wittenburg, 1983; Karttunen, 1983). Morphological analyzers are now a key component of most natural language applications and they continue to be a focus of research interest (Sproat, 1992). These systems are knowledge-intensive, requiring a stem lexicon, a list of affixes, morphotactic rules specifying the types of words to which each affix can apply, and spelling adjustment rules such as the English rule that inserts an e between a stem-final sybillant and the suffix -s in words like churches. Adaptation of a morphological analysis system to a new language requires considerable expertise with both the language and the morpho</context>
</contexts>
<marker>Karttunen, 1983</marker>
<rawString>L. Karttunen. 1983. KIMMO: a general morphological processor. Texas Linguistics Forum 22, 22:165-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Knowledgefree induction of morphology using latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning. Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="3133" citStr="Schone and Jurafsky (2000)" startWordPosition="484" endWordPosition="487">stem based on the spellings of words and the set of suffixes that appears with each stem, which Goldsmith calls the signature of the word. Gaussier (1999) reports on an explicitly probabilistic system that is based primarily on spellings, while Dejean (1998) describes a heuristic approach. Unlike the methods described above, the memory-based algorithm of van den Bosch and Daelemans (1999) requires supervised training. However, this system has the advantage that its output is a full morphological analysis, with syntactic category labels, not merely a splitting of the word into stem and suffix. Schone and Jurafsky (2000) take a completely different approach based on latent semantic analysis; words that lie near one another in the latent semantic space are treated as more likely to be morphologically related. Yarowsky and Wicentowski (2000) have developed a statistical system that takes a set of regular suffixes as input and learns spelling change rules and irregular morphology. This system uses four different statistical models for identifying words that are morphologically related. One of these models is similar to the latent semantic analysis method of Schone and Jurafsky (2000). In this paper, we describe </context>
</contexts>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>P. Schone and D. Jurafsky. 2000. Knowledgefree induction of morphology using latent semantic analysis. In Proceedings of the Conference on Computational Natural Language Learning. Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<title>Morphology and Computation.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1302" citStr="Sproat, 1992" startWordPosition="196" endWordPosition="197">veral definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion. This formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space. 1 Introduction Systems for morphological analysis were one of the early successes in computational linguistics (Karttunen and Wittenburg, 1983; Karttunen, 1983). Morphological analyzers are now a key component of most natural language applications and they continue to be a focus of research interest (Sproat, 1992). These systems are knowledge-intensive, requiring a stem lexicon, a list of affixes, morphotactic rules specifying the types of words to which each affix can apply, and spelling adjustment rules such as the English rule that inserts an e between a stem-final sybillant and the suffix -s in words like churches. Adaptation of a morphological analysis system to a new language requires considerable expertise with both the language and the morphological analyzer. In the 1990&apos;s, the focus of research in syntactic parsing shifted from brittle, knowledgeintensive systems to systems based on probabilis</context>
</contexts>
<marker>Sproat, 1992</marker>
<rawString>Richard Sproat. 1992. Morphology and Computation. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<booktitle>Lemma 1 If mi m2 then CompM(mi, x, h) = CompM(mi, x, Prom(m2, x, h)), CompX(mi, x, h) = CompX(mi, x ,Prom(m2, x, h)). Proof. By Definition 1,</booktitle>
<marker></marker>
<rawString>Lemma 1 If mi m2 then CompM(mi, x, h) = CompM(mi, x, Prom(m2, x, h)), CompX(mi, x, h) = CompX(mi, x ,Prom(m2, x, h)). Proof. By Definition 1,</rawString>
</citation>
<citation valid="false">
<authors>
<author>CompM</author>
</authors>
<note>h)CompX(mi, x, h) = mix,</note>
<marker>CompM, </marker>
<rawString>CompM(mi, x, h)CompX(mi, x, h) = mix,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>