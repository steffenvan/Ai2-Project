<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001516">
<title confidence="0.964635">
Baby SRL: Modeling Early Language Acquisition.
</title>
<author confidence="0.994642">
Michael Connor Yael Gertner
</author>
<affiliation confidence="0.9999">
Department of Computer Science Beckman Institute
University of Illinois University of Illinois
</affiliation>
<email confidence="0.994244">
connor2@uiuc.edu ygertner@cyrus.psych.uiuc.edu
</email>
<author confidence="0.991381">
Cynthia Fisher
</author>
<affiliation confidence="0.9989705">
Department of Psychology
University of Illinois
</affiliation>
<email confidence="0.995389">
cfisher@cyrus.psych.uiuc.edu
</email>
<sectionHeader confidence="0.982907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985357142857">
A fundamental task in sentence compre-
hension is to assign semantic roles to sen-
tence constituents. The structure-mapping
account proposes that children start with
a shallow structural analysis of sentences:
children treat the number of nouns in the
sentence as a cue to its semantic predicate-
argument structure, and represent language
experience in an abstract format that per-
mits rapid generalization to new verbs. In
this paper, we tested the consequences of
these representational assumptions via ex-
periments with a system for automatic se-
mantic role labeling (SRL), trained on a
sample of child-directed speech. When
the SRL was presented with representa-
tions of sentence structure consisting sim-
ply of an ordered set of nouns, it mim-
icked experimental findings with toddlers,
including a striking error found in children.
Adding features representing the position
of the verb increased accuracy and elim-
inated the error. We show the SRL sys-
tem can use incremental knowledge gain
to switch from error-prone noun order fea-
tures to a more accurate representation,
demonstrating a possible mechanism for
this process in child development.
</bodyText>
<sectionHeader confidence="0.995157" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917666666667">
How does the child get started in learning to in-
terpret sentences? The structure-mapping view
of early verb and syntax acquisition proposes that
</bodyText>
<note confidence="0.87396675">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</note>
<author confidence="0.963987">
Dan Roth
</author>
<affiliation confidence="0.9993105">
Department of Computer Science
University of Illinois
</affiliation>
<email confidence="0.97239">
danr@uiuc.edu
</email>
<bodyText confidence="0.999979789473684">
children start with a shallow structural analysis of
sentences: children treat the number of nouns in
the sentence as a cue to its semantic predicate-
argument structure (Fisher, 1996), and represent
language experience in an abstract format that per-
mits rapid generalization to new verbs (Gertner et
al., 2006).
The structure-mapping account makes strong
predictions. First, as soon as children can identify
some nouns, they should interpret transitive and in-
transitive sentences differently, simply by assign-
ing a distinct semantic role to each noun in the sen-
tence. Second, language-specific syntactic learn-
ing should transfer rapidly to new verbs. Third,
some striking errors of interpretation can occur.
In “Fred and Ginger danced”, an intransitive verb
is presented with two nouns. If children interpret
any two-noun sentence as if it were transitive, they
should be fooled into interpreting the order of two
nouns in such conjoined-subject intransitive sen-
tences as conveying agent-patient role information.
Experiments with young children support these
predictions. First, 21-month-olds use the number
of nouns to understand sentences containing new
verbs (Yuan et al., 2007). Second, 21-month-olds
generalize what they have learned about English
transitive word-order to sentences containing new
verbs: Children who heard ”The girl is gorping the
boy” interpreted the girl as an agent and the boy as
a patient (Gertner et al., 2006). Third, 21-month-
olds make the predicted error, treating intransitive
sentences containing two nouns as if they were
transitive: they interpret the first noun in “The girl
and the boy are gorping” as an agent and the sec-
ond as a patient (Gertner and Fisher, 2006). This
error is short-lived. By 25 months, children add
new features to their representations of sentences,
and interpret conjoined-subject intransitives differ-
</bodyText>
<page confidence="0.520444">
81
</page>
<note confidence="0.9685195">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 81–88
Manchester, August 2008
</note>
<bodyText confidence="0.998542131578947">
ently from transitives (Naigles, 1990).
These experimental results shed light on what
syntactic information children might have avail-
able for early sentence comprehension, but do not
rule out the possibility that children’s early per-
formance is based on a more complex underlying
system. In this paper, we tested the consequences
of our representational assumptions by perform-
ing experiments with a system for automatic se-
mantic role labeling (SRL), whose knowledge of
sentence structure is under our control. Com-
putational models of semantic role labeling learn
to identify, for each verb in a sentence, all con-
stituents that fill a semantic role, and to determine
their roles. We adopt the architecture proposed
by Roth and colleagues (Punyakanok et al., 2005),
limiting the classifier’s features to a set of lexical
features and shallow structural features suggested
by the structure-mapping account. Learning abil-
ity is measured by the level of SRL accuracy and,
more importantly, the types of errors made by the
system on sentences containing novel verbs. Test-
ing these predictions on the automatic SRL pro-
vides us with a demonstration that it is possible to
learn how to correctly assign semantic roles based
only on these very simple cues.
From an NLP perspective this feature study pro-
vides evidence for the efficacy of alternative, sim-
pler syntactic representations in gaining an initial
foothold on sentence interpretation. It is clear that
human learners do not begin interpreting sentences
in possession of full part-of-speech tagging, or full
parse trees. By building a model that uses shal-
low representations of sentences and mimics fea-
tures of language development in children, we can
explore the nature of initial representations of syn-
tactic structure and build more complex features
from there, further mimicking child development.
</bodyText>
<sectionHeader confidence="0.982428" genericHeader="method">
2 Learning Model
</sectionHeader>
<bodyText confidence="0.999563945454545">
We trained a simplified SRL classifier (Baby SRL)
with sets of features derived from the structure-
mapping account. Our test used novel verbs to
mimic sentences presented in experiments with
children. Our learning task is similar to the full
SRL task (Carreras and M`arquez, 2004), except
that we classify the roles of individual words rather
than full phrases. A full automatic SRL system
(e.g. (Punyakanok et al., 2005)) typically involves
multiple stages to 1) parse the input, 2) identify ar-
guments, 3) classify those arguments, and then 4)
run inference to make sure the final labeling for the
full sentence does not violate any linguistic con-
straints. Our simplified SRL architecture (Baby
SRL) essentially replaces the first two steps with
heuristics. Rather than identifying arguments via
a learned classifier with access to a full syntac-
tic parse, the Baby SRL treats each noun in the
sentence as a candidate argument and assigns a
semantic role to it. A simple heuristic collapsed
compound or sequential nouns to their final noun:
an approximation of the head noun of the noun
phrase. For example, ’Mr. Smith’ was treated
as the single noun ’Smith’. Other complex noun
phrases were not simplified in this way. Thus,
a phrase such as ’the toy on the floor’ would be
treated as two separate nouns, ’toy’ and ’floor’.
This represents the assumption that young children
know ’Mr. Smith’ is a single name, but they do not
know all the predicating terms that may link mul-
tiple nouns into a single noun phrase. The simpli-
fied learning task of the Baby SRL implements a
key assumption of the structure-mapping account:
that at the start of multiword sentence comprehen-
sion children can tell which words in a sentence are
nouns (Waxman and Booth, 2001), and treat each
noun as a candidate argument.
Feedback is provided based on annotation in
Propbank style: in training, each noun receives the
role label of the phrase that noun is part of. Feed-
back is given at the level of the macro-role (agent,
patient, etc., labeled A0-A4 for core arguments,
and AM-* adjuncts). We also introduced a NO la-
bel for nouns that are not part of any argument.
For argument classification we use a linear clas-
sifier trained with a regularized perceptron update
rule (Grove and Roth, 2001). This learning algo-
rithm provides a simple and general linear clas-
sifier that has been demonstrated to work well in
other text classification tasks, and allows us to in-
spect the weights of key features to determine their
importance for classification. The Baby SRL does
not use inference for the final classification. In-
stead it classifies every argument independently;
thus multiple nouns can have the same role.
</bodyText>
<subsectionHeader confidence="0.990217">
2.1 Training
</subsectionHeader>
<bodyText confidence="0.999562">
The training data were samples of parental speech
to one child (’Eve’; (Brown, 1973), available
via Childes (MacWhinney, 2000)). We trained
on parental utterances in samples 9 through 20,
recorded at child age 21-27 months. All verb-
</bodyText>
<page confidence="0.847357">
82
</page>
<bodyText confidence="0.999954533333333">
containing utterances without symbols indicating
long pauses or unintelligible words were automat-
ically parsed with the Charniak parser (Charniak,
1997) and annotated using an existing SRL sys-
tem (Punyakanok et al., 2005). In this initial pass,
sentences with parsing errors that misidentified ar-
gument boundaries were excluded. Final role la-
bels were hand-corrected using the Propbank an-
notation scheme (Kingsbury and Palmer, 2002).
The child-directed speech (CDS) training set con-
sisted of about 2200 sentences, of which a majority
had a single verb and two nouns to be labeled1. We
used the annotated CDS training data to train our
Baby SRL, converting labeled phrases to labeled
nouns in the manner described above.
</bodyText>
<sectionHeader confidence="0.992476" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.997532813953488">
To evaluate the Baby SRL we tested it with sen-
tences like those used for the experiments with
children described above. All test sentences con-
tained a novel verb (’gorp’). We constructed two
test sentence templates: ’A gorps B’ and ’A and B
gorp’, where A and B were replaced with nouns
that appeared more than twice in training. We
filled the A and B slots by sampling nouns that
occurred roughly equally as the first and second
of two nouns in the training data. This procedure
was adopted to avoid ’building in’ the predicted er-
ror by choosing A and B nouns biased toward an
agent-patient interpretation. For each test sentence
template we built a test set of 100 sentences by ran-
domly sampling nouns in this fashion.
The test sentences with novel verbs ask whether
the classifier transfers its learning about argument
role assignment to unseen verbs. Does it as-
sume the first of two nouns in a simple transi-
tive sentence (’A gorps B’) is the agent (A0) and
the second is the patient (A1)? Does it over-
generalize this rule to two-noun intransitives (’A
and B gorp’), mimicking children’s behavior? We
used two measures of success, one to assess clas-
sification accuracy, and the other to assess the
predicted error. We used a per argument F1 for
classification accuracy, with F1 based on correct
identification of individual nouns rather than full
phrases. Here precision is defined as the propor-
tion of nouns that were given the correct label
based on the argument they belong to, and recall
is the proportion of complete arguments for which
1Corpus available at http://L2R.cs.uiuc.edu/
—cogcomp/data.php
some noun in that argument was correctly labeled.
The desired labeling for ’A gorps B’ is A0 for the
first argument and A1 for the second; for ’A and
B gorp’ both arguments should be A0. To mea-
sure predicted errors we also report the proportion
of test sentences classified with A0 first and A1
second (%A0A1). This labeling is a correct gener-
alization for the novel ’A gorps B’ sentences, but
is an overgeneralization for ’A and B gorp.’
</bodyText>
<subsectionHeader confidence="0.999198">
3.1 Noun Pattern
</subsectionHeader>
<bodyText confidence="0.9999887">
The basic feature we propose is the noun pattern
feature. We hypothesize that children use the num-
ber and order of nouns to represent argument struc-
ture. To encode this we created a feature (NPat-
tern) that indicates how many nouns there are in
the sentence and which noun the target is. For ex-
ample, in our two-noun test sentences noun A has
the feature ’ N’ active indicating that it is the first
noun of two. Likewise for B the feature ’N ’ is ac-
tive, indicating that it is the second of two nouns.
This feature is easy to compute once nouns are
identified, and does not require fine-grained dis-
tinctions between types of nouns or any other part
of speech. Table 1 shows the initial feature pro-
gression that involves this feature. The baseline
system (feature set 1) uses lexical features only:
the target noun and the root form of the predicate.
We first tested the hypothesis that children use
the NPattern features to distinguish different noun
arguments, but only for specific verbs. The NPat-
tern&amp;V features are conjunctions of the target verb
and the noun pattern, and these are added to the
word features to form feature set 2. Now every
example has three features active: target noun, tar-
get predicate, and a NPattern&amp;V feature indicating
’the target is the first of two nouns and the verb
is X.’ This feature does not improve results on the
novel ’A gorps B’ test set, or generate the predicted
error with the ’A and B gorp’ test set, because the
verb-specific NPattern&amp;V features provide no way
to generalize to unseen verbs.
We next tested the NPattern feature alone, with-
out making it verb-specific (feature set 3). The
noun pattern feature was added to the word fea-
tures and again each example had three features ac-
tive: target noun, target predicate, and the target’s
noun-pattern feature (first of two, second of three,
etc.). The abstract NPattern feature allows the
Baby SRL to generalize to new verbs: it increases
the system’s tendency to predict that the first of two
</bodyText>
<page confidence="0.670529">
83
</page>
<table confidence="0.986505333333333">
CHILDES WSJ
Unbiased Noun Choice Biased Noun Choice Biased Noun Choice
A gorps B A and B gorp A gorps B A and B gorp A gorps B A and B gorp
Features F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1
1. Words 0.59 0.38 0.46 0.38 0.80 0.65 0.53 0.65 0.57 0.31 0.37 0.31
2. NPattern&amp;V 0.53 0.28 0.54 0.28 0.81 0.67 0.53 0.67 0.56 0.31 0.39 0.31
3. NPattern 0.83 0.65 0.33 0.65 0.96 0.92 0.46 0.92 0.67 0.44 0.37 0.44
4. NPattern + NPattern&amp;V 0.83 0.65 0.33 0.65 0.95 0.90 0.45 0.90 0.73 0.53 0.44 0.53
5. + VPosition 0.99 0.96 0.98 0.00 1.00 1.00 0.99 0.01 0.94 0.88 0.69 0.39
</table>
<tableCaption confidence="0.995842">
Table 1: Experiments showing the efficacy of Noun Pattern features for determining agent/patient roles in
</tableCaption>
<bodyText confidence="0.9826665">
simple two-noun sentences. The novel verb test sets assess whether the Baby SRL generalizes transitive
argument prediction to unseen verbs in the case of ‘A gorps B’ (increasing %A0A1 and thus F1), and
overgeneralizes in the case of ‘A and B gorp’ (increasing %A0A1, which is an error). By varying the
sampling method for creating the test sentences we can start with a biased or unbiased lexical baseline,
demonstrating that the noun pattern features still improve over knowledge that can be contained in
typical noun usage. The simple noun pattern features are still effective at learning this pattern when
trained with more complex Wall Street Journal training data.
nouns is A0 and the second of two nouns is A1 for
verbs not seen in training. Feature set 4 includes
both the abstract, non-verb-specific NPattern fea-
ture and the verb-specific version. This feature set
preserves the ability to generalize to unseen verbs;
thus the availability of the verb-specific NPattern
features during training did not prevent the abstract
NPattern features from gathering useful informa-
tion.
</bodyText>
<subsectionHeader confidence="0.99885">
3.2 Lexical Cues for Role-Labeling
</subsectionHeader>
<bodyText confidence="0.999986">
Thus far, the target nouns’ lexical features pro-
vided little help in role labeling, allowing us to
clearly see the contribution of the proposed sim-
ple structural features. Would our structural fea-
tures produce any improvement above a more re-
alistic lexical baseline? We created a new set of
test sentences, sampling the A nouns based on the
distribution of nouns seen as the first of two nouns
in training, and the B nouns based on the distri-
bution of nouns seen as the second of two nouns.
Given this revised sampling of nouns, the words-
only baseline is strongly biased toward A0A1 (bi-
ased results for feature set 1 in table 1). This high
baseline reflects a general property of conversa-
tion: Lexical choices provide considerable infor-
mation about semantic roles. For example, the 6
most common nouns in the Eve corpus are pro-
nouns that are strongly biased in their positions
and in their semantic roles (e.g., ’you’, ’it’). De-
spite this high baseline, however, we see the same
pattern in the unbiased and biased experiments in
table 1. The addition of the NPattern features (fea-
ture set 3) substantially improves performance on
’A gorps B’ test sentences, and promotes over-
generalization errors on ’A and B gorp’ sentences.
</bodyText>
<subsectionHeader confidence="0.997171">
3.3 More Complex Training Data
</subsectionHeader>
<bodyText confidence="0.999992676470588">
For comparison purposes we also trained the Baby
SRL on a subset of the Propbank training data
of Wall Street Journal (WSJ) text (Kingsbury and
Palmer, 2002). To approximate the simpler sen-
tences of child-directed speech we selected only
those sentences with 8 or fewer words. This
provided a training set of about 2500 sentences,
most with a single verb and two nouns to be la-
beled. The CDS and WSJ data pose similar prob-
lems for learning abstract and verb-specific knowl-
edge. However, newspaper text differs from ca-
sual speech to children in many ways, including
vocabulary and sentence complexity. One could
argue that the WSJ corpus presents a worst-case
scenario for learning based on shallow representa-
tions of sentence structure: Full passive sentences
are more common in written corpora such as the
WSJ than in samples of conversational speech, for
example (Roland et al., 2007). As a result of such
differences, two-noun sequences are less likely to
display an A0-A1 sequence in the WSJ (0.42 A0-
A1 in 2-noun sentences) than in the CDS training
data (0.67 A0-A1). The WSJ data provides a more
demanding test of the Baby SRL.
We trained the Baby SRL on the WSJ data, and
tested it using the biased lexical choices as de-
scribed above, sampling A and B nouns for novel-
verb test sentences based on the distribution of
nouns seen as the first of two nouns in training, and
as the second of two nouns, respectively. The WSJ
training produced performance strikingly similar
to the performance resulting from CDS training
(last 4 columns of Table 1). Even in this more
complex training set, the addition of the NPattern
</bodyText>
<page confidence="0.670745">
84
</page>
<bodyText confidence="0.87248">
features (feature set 3) improves performance on
’A gorps B’ test sentences, and promotes over-
generalization errors on ’A and B gorp’ sentences.
</bodyText>
<subsectionHeader confidence="0.546503">
3.4 Tests with Familiar Verbs
</subsectionHeader>
<table confidence="0.993584666666667">
Features Total A0 A1 A2 A4
1. Words 0.64 0.83 0.74 0.33 0.00
2. NPattern&amp;V 0.67 0.86 0.77 0.45 0.44
3. NPattern 0.66 0.87 0.76 0.37 0.22
4. NPattern + NPattern&amp;V 0.68 0.87 0.80 0.47 0.44
5. + VPosition 0.70 0.88 0.83 0.50 0.50
</table>
<tableCaption confidence="0.991201">
Table 2: Testing NPattern features on full SRL task
</tableCaption>
<bodyText confidence="0.981404256410257">
of heldout section 8 of Eve when trained on sec-
tions 9 through 20. Each result column reflects a
per argument F1.
Learning to interpret sentences depends on bal-
ancing abstract and verb-specific structural knowl-
edge. Natural linguistic corpora, including our
CDS training data, have few verbs of very high fre-
quency and a long tail of rare verbs. Frequent verbs
occur with differing argument patterns. For exam-
ple, ’have’ and ’put’ are frequent in the CDS data.
’Have’ nearly always occurs in simple transitive
sentences that display the canonical word order of
English (e.g., ’I have cookies’). ’Put’, in contrast,
tends to appear in non-canonical sentences that do
not display an agent-patient ordering, including
imperatives (’Put it on the floor’). To probe the
Baby SRL’s ability to learn the argument-structure
preferences of familiar verbs, we tested it on a
held-out sample of CDS from the same source
(Eve sample 8, approximately 234 labeled sen-
tences). Table 2 shows the same feature progres-
sion shown previously, with the full SRL test set.
The words-only baseline (feature set 1 in Table 2)
yields fairly accurate performance, showing that
considerable success in role assignment in these
simple sentences can be achieved based on the
argument-role biases of the target nouns and the
familiar verbs. Despite this high baseline, how-
ever, we still see the benefit of simple structural
features. Adding verb-specific (feature set 2) or
abstract NPattern features (feature set 3) improves
classification performance, and the combination of
both verb-specific and abstract NPattern features
(feature set 4) yields higher performance than ei-
ther alone. The combination of abstract NPattern
features with the verb-specific versions allows the
Baby SRL both to generalize to unseen verbs, as
seen in earlier sections, and to learn the idiosyn-
crasies of known verbs.
</bodyText>
<subsectionHeader confidence="0.924498">
3.5 Verb Position
</subsectionHeader>
<bodyText confidence="0.999236375">
The noun pattern feature results show that the
Baby SRL can learn helpful rules for argument-
role assignment using only information about the
number and order of nouns. It also makes the error
predicted by the structure-mapping account, and
documented in children, because it has no way to
represent the difference between the ’A gorps B’
and ’A and B gorp’ test sentences. At some point
the learner must develop more sophisticated syn-
tactic representations that could differentiate these
two. These could include many aspects of the sen-
tence, including noun-phrase and verb-phrase mor-
phological features, and word-order features. As a
first step in examining recovery from the predicted
error, we focused on word-order features. We did
this by adding a verb position feature (VPosition)
that specifies whether the target noun is before or
after the verb. Now simple transitive sentences in
training should support the generalization that pre-
verbal nouns tend to be agents, and post-verbal
nouns tend to be patients. In testing, the Baby
SRL’s classification of the ’A gorps B’ and ’A and
B gorp’ sentences should diverge.
When we add verb position information (fea-
ture set 5 in table 1 and 2), performance improves
still further for transitive sentences, both with bi-
ased and unbiased test sentences. Also, for the first
time, the A0A1 pattern is predicted less often for
’A and B gorp’ sentences. This error diminished
because the classifier was able to use the verb po-
sition features to distinguish these from ’A gorps
B’ sentences.
</bodyText>
<table confidence="0.997132333333333">
Unbiased Lexical
A gorps B A and B gorp
Features F1 %A0A1 F1 %A0A1
1. Words 0.59 0.38 0.46 0.38
3. NPattern 0.83 0.65 0.33 0.65
6. VPosition 0.99 0.95 0.97 0.00
</table>
<tableCaption confidence="0.995828">
Table 3: Verb Position vs. Noun Pattern features
</tableCaption>
<bodyText confidence="0.940626727272727">
alone. Verb position features yield better overall
performance, but do not replicate the error on ‘A
and B gorp’ sentences seen with children.
Verb position alone provides another simple ab-
stract representation of sentence structure, so it
might be proposed as an equally natural initial
representation for human learners, rather than the
noun pattern features we proposed. The VPo-
sition features should also support learning and
generalization of word-order rules for interpret-
ing transitive sentences, thus reproducing some of
</bodyText>
<page confidence="0.958147">
85
</page>
<bodyText confidence="0.999969769230769">
the data from children that we reviewed above.
In table 3 we compared the words-only baseline
(set 1), words and NPattern features (set 3), and a
new feature set, words and VPosition (set 6). In
terms of correct performance on novel transitive
verbs (’A gorps B’), the VPosition features out-
perform the NPattern features. This may be partly
because the same VPosition features are used in
all sentences during training, while the NPattern
features partition sentences by number of nouns,
but is also due to the fact that the verb position
features provide a more sophisticated representa-
tion of English sentence structure. Verb position
features can distinguish transitive sentences from
imperatives containing multiple post-verbal nouns,
for example. Although verb position is ultimately
a more powerful representation of word order for
English sentences, it does not accurately reproduce
a 21-month-old’s performance on all aspects of
this task. In particular, the VPosition feature does
not support the overgeneralization of the A0A1
pattern to the ’A and B gorp’ test sentences. This
suggests that children’s very early sentence com-
prehension is dominated by less sophisticated rep-
resentations of word order, akin to the NPattern
features we proposed.
</bodyText>
<subsectionHeader confidence="0.968971">
3.6 Informativeness vs. Availability
</subsectionHeader>
<bodyText confidence="0.999698038461538">
In the preceding sections, we modeled increases
in syntactic knowledge by building in more so-
phisticated features. The Baby SRL escaped the
predicted error on two-noun intransitive sentences
when given access to features reflecting the posi-
tion of the target noun relative to the verb. This
imposed sequence of features is useful as a starting
point, but a more satisfying approach would be to
use the Baby SRL to explore possible reasons why
NPattern features might dominate early in acquisi-
tion, even though VPosition features are ultimately
more useful for English.
In theory, a feature might be unavailable early in
acquisition because of its computational complex-
ity. For example, lexical features are presumably
less complex than relative position features such as
NPattern and VPosition. In practice, features can
also be unavailable at first because of an informa-
tional lack. Here we suggest that NPattern features
might dominate VPosition features early in acqui-
sition because the early lexicon is dominated by
nouns, and it is easier to compute position relative
to a known word than to an unknown word. Many
studies have shown that children’s early vocabu-
lary is dominated by names for objects and peo-
ple (Gentner and Boroditsky, 2001).
</bodyText>
<figure confidence="0.989161666666666">
Examples
(a) Verb threshold = 5
Examples
(b) Verb threshold = 20
Examples
(c) Verb threshold = 20, +verb-specific features
</figure>
<figureCaption confidence="0.999493">
Figure 1: Testing the consequences of the assump-
</figureCaption>
<bodyText confidence="0.912074916666667">
tion that Verb Position features are only active for
familiar verbs. The figure plots the bias of the fea-
tures ’ N’ and ’ V’ to predict A0 over A1, as the
difference between the weights of these connec-
tions in the learned network. Verb position fea-
tures win out over noun pattern features as the
verb vocabulary grows. Varying the verb familiar-
ity threshold ((a) vs. (b)) and the presence versus
absence of verb-specific versions of the structural
features ((b) vs. (c)) affects how quickly the verb
position features become dominant.
To test the consequences of this proposed infor-
</bodyText>
<figure confidence="0.994235850746269">
0 5000 10000 15000 20000
-0.5
3.5
2.5
0.5
1.5
3
2
0
1
Known Verbs
N
_V
90
30
0
180
150
120
5
60
#
0 5000 10000 15000 20000
-0.5
3.5
2.5
0.5
1.5
3
2
0
1
Known Verbs
_N
V
90
30
0
180
150
120
5
60
#
0 5000 10000 15000 20000
-0.5
3.5
2.5
0.5
1.5
3
2
0
1
Known Verbs
_N
V
90
30
0
180
150
120
5
#
60
86
</figure>
<bodyText confidence="0.999821333333334">
mational bottleneck on the relative weighting of
NPattern and VPosition features during training,
we modified the Baby SRL’s training procedure
such that NPattern features were always active, but
VPosition features were active during training only
when the verb in the current example had been en-
countered a critical number of times. This repre-
sents the assumption that the child can recognize
which words in the sentence are nouns, based on
lexical familiarity or morphological context (Wax-
man and Booth, 2001), but is less likely to be able
to represent position relative to the verb without
knowing the verb well.
Figure 1 shows the tendency of the NPattern fea-
ture ’ N’ (first of two nouns) and the VPosition
feature ’ V’ (pre-verbal noun) to predict the role
A0 as opposed to A1 as the difference between
the weights of these connections in the learned net-
work. Figure 1(a) shows the results when VPosi-
tion features were active whenever the target verb
had occurred at least 5 times; in Figure 1(b) the
threshold for verb familiarity was 20. In both fig-
ures we see that the VPosition features win out
over the NPattern features as the verb vocabulary
grows. Varying the degree of verb familiarity re-
quired to accurately represent VPosition features
affects how quickly the VPosition features win
out (compare Figures 1(a) and 1(b)). Figure 1(c)
shows the same analysis with a threshold of 20,
but with verb-specific as well as abstract versions
of the NPattern and the VPosition features. In this
procedure, every example started with three fea-
tures: target noun, target predicate, NPattern, and
if the verb was known, added NPattern&amp;V, VPo-
sition, and VPosition&amp;V. Comparing Figures 1(b)
and 1(c), we see that the addition of verb-specific
versions of the structural features also affects the
rate at which the VPosition features come to dom-
inate the NPattern features.
Thus, in training the VPosition features become
dominant as the SRL learns to recognize more
verbs. However, the VPosition features are inac-
tive when the Baby SRL encounters the novel-verb
test sentences. Since the NPattern features are ac-
tive in test, the system generates the predicted error
until the bias of the NPattern features reaches 0.
Note in figure 1(c) that when verb-specific struc-
tural features were added, the Baby SRL never
learned to entirely discount the NPattern features
within the range of training provided. This result
is reminiscent of suggestions in the psycholinguis-
</bodyText>
<figure confidence="0.96681">
0 0.2 0.4 0.6 0.8 1
Noise
</figure>
<figureCaption confidence="0.987939">
Figure 2: Testing the ability of simple features
</figureCaption>
<bodyText confidence="0.9301965">
to cope with varying amounts of noisy feedback.
Even with noisy feedback, the noun pattern fea-
tures support learning and generalization to new
verbs of a simple agent-patient template for un-
derstanding transitive sentences. These results are
lower than those found in table 1 due to slightly
different training assumptions.
tics literature that shallow representations of syn-
tax persist in the adult parser, alongside more so-
phisticated representations (e.g., (Ferreira, 2003)).
</bodyText>
<subsectionHeader confidence="0.982592">
3.7 Noisy Training
</subsectionHeader>
<bodyText confidence="0.9999754">
So far, the Baby SRL has only been trained with
perfect feedback. Theories of human language ac-
quisition assume that learning to understand sen-
tences is naturally a partially-supervised task: the
child uses existing knowledge of words and syntax
to assign a meaning to a sentence; the appropriate-
ness of this meaning for the referential context pro-
vides the feedback (e.g., (Pinker, 1989)). But this
feedback must be noisy. Referential scenes pro-
vide useful but often ambiguous information about
the semantic roles of sentence participants. For ex-
ample, a participant could be construed as an agent
of fleeing or as a patient being chased. In a final
set of experiments, we examined the generaliza-
tion abilities of the Baby SRL as a function of the
integrity of semantic feedback.
We provided noisy semantic-role feedback dur-
ing training by giving a randomly-selected argu-
ment label on 0 to 100% of examples. Following
this training, we tested with the ’A gorps B’ test
sentences, using the unbiased noun choices.
As shown in Figure 2, feature sets including
NPattern or VPosition features yield reasonable
performance on the novel verb test sentences up to
50% noise, and promote an A0-A1 sequence over
</bodyText>
<figure confidence="0.997874888888889">
0.8
0.6
0.4
0.2
0
1
Words
+NPattern+NPattern&amp;V
+VPosition
</figure>
<page confidence="0.507008">
87
</page>
<bodyText confidence="0.998526875">
the words-only baseline even at higher noise lev-
els. Thus the proposed simple structural features
are robust to noisy feedback.
Gentner, D. and L. Boroditsky. 2001. Individuation,
relativity and early word learning. In Bowerman, M.
and S. C. Levinson, editors, Language acquisition
and conceptual development, pages 215–256. Cam-
bridge University Press, New York.
</bodyText>
<sectionHeader confidence="0.981607" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999996727272727">
The simplified SRL classifier mimicked experi-
mental results with toddlers. We structured the
learning task to ask whether shallow representa-
tions of sentence structure provided a useful ini-
tial representation for learning to interpret sen-
tences. Given representations of the number and
order of nouns in the sentence (noun pattern fea-
tures), the Baby SRL learned to classify the first
of two nouns as an agent and the second as a pa-
tient. When provided with both verb-general and
verb-specific noun pattern features, the Baby SRL
learned to balance verb-specific and abstract syn-
tactic knowledge. By treating each noun as an
argument, it also reproduced the errors children
make. Crucially, verb-position features improved
performance when added to the noun-pattern fea-
ture, but when presented alone failed to produce
the error found with toddlers. We believe that
our model can be naturally extended to support
the case in which the arguments are noun phrases
rather than single noun words and this extension is
one of the first steps we will explore next.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99992325">
We would like to thank our annotators, espe-
cially Yuancheng Tu. This research is supported
by NSF grant BCS-0620257 and NIH grant R01-
HD054448.
</bodyText>
<sectionHeader confidence="0.996448" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977518">
Brown, R. 1973. A First Language. Harvard Univer-
sity Press, Cambridge, MA.
Carreras, X. and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared tasks: Semantic role label-
ing. In Proceedings of CoNLL-2004, pages 89–97.
Boston, MA, USA.
Charniak, E. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
Ferreira, F. 2003. The misinterpretation of noncanoni-
cal sentences. Cognitive Psychology, 47:164–203.
Fisher, C. 1996. Structural limits on verb mapping:
The role of analogy in children’s interpretation of
sentences. Cognitive Psychology, 31:41–81.
Gertner, Y. and C. Fisher. 2006. Predicted errors in
early verb learning. In 31st Annual Boston Univer-
sity Conference on Language Development.
Gertner, Y., C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word order
in early sentence comprehension. Psychological Sci-
ence, 17:684–691.
Grove, A. and D. Roth. 2001. Linear concepts and
hidden variables. Machine Learning, 42(1/2):123–
141.
Kingsbury, P. and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC-2002, Spain.
MacWhinney, B. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elrbaum
Associates, Mahwah, NJ.
Naigles, L. R. 1990. Children use syntax to learn verb
meanings. Journal of Child Language, 17:357–374.
Pinker, S. 1989. Learnability and Cognition. Cam-
bridge: MIT Press.
Punyakanok, V., D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role label-
ing. In Proc. of the International Joint Conference
on Artificial Intelligence (IJCAI), pages 1117–1123.
Roland, D., F. Dick, and J. L. Elman. 2007. Fre-
quency of basic english grammatical structures: A
corpus analysis. Journal of Memory and Language,
57:348–379.
Waxman, S. R. and A. Booth. 2001. Seeing pink
elephants: Fourteen-month-olds’s interpretations of
novel nouns and adjectives. Cognitive Psychology,
43:217–242.
Yuan, S., C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-
month-olds assign relational meaning to novel tran-
sitive verbs. In Biennial Meeting of the Society for
Research in Child Development, Boston, MA.
</reference>
<page confidence="0.959815">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915140">
<title confidence="0.991322">Baby SRL: Modeling Early Language Acquisition.</title>
<author confidence="0.999842">Michael Connor Yael Gertner</author>
<affiliation confidence="0.9999455">Department of Computer Science Beckman Institute University of Illinois University of Illinois</affiliation>
<email confidence="0.99238">connor2@uiuc.eduygertner@cyrus.psych.uiuc.edu</email>
<author confidence="0.962547">Cynthia</author>
<affiliation confidence="0.99934">Department of University of</affiliation>
<email confidence="0.998246">cfisher@cyrus.psych.uiuc.edu</email>
<abstract confidence="0.998899965517241">A fundamental task in sentence comprehension is to assign semantic roles to sentence constituents. The structure-mapping account proposes that children start with a shallow structural analysis of sentences: children treat the number of nouns in the sentence as a cue to its semantic predicateargument structure, and represent language experience in an abstract format that permits rapid generalization to new verbs. In this paper, we tested the consequences of these representational assumptions via experiments with a system for automatic semantic role labeling (SRL), trained on a sample of child-directed speech. When the SRL was presented with representations of sentence structure consisting simply of an ordered set of nouns, it mimicked experimental findings with toddlers, including a striking error found in children. Adding features representing the position of the verb increased accuracy and eliminated the error. We show the SRL system can use incremental knowledge gain to switch from error-prone noun order features to a more accurate representation, demonstrating a possible mechanism for this process in child development.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Brown</author>
</authors>
<title>A First Language.</title>
<date>1973</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8514" citStr="Brown, 1973" startWordPosition="1323" endWordPosition="1324"> we use a linear classifier trained with a regularized perceptron update rule (Grove and Roth, 2001). This learning algorithm provides a simple and general linear classifier that has been demonstrated to work well in other text classification tasks, and allows us to inspect the weights of key features to determine their importance for classification. The Baby SRL does not use inference for the final classification. Instead it classifies every argument independently; thus multiple nouns can have the same role. 2.1 Training The training data were samples of parental speech to one child (’Eve’; (Brown, 1973), available via Childes (MacWhinney, 2000)). We trained on parental utterances in samples 9 through 20, recorded at child age 21-27 months. All verb82 containing utterances without symbols indicating long pauses or unintelligible words were automatically parsed with the Charniak parser (Charniak, 1997) and annotated using an existing SRL system (Punyakanok et al., 2005). In this initial pass, sentences with parsing errors that misidentified argument boundaries were excluded. Final role labels were hand-corrected using the Propbank annotation scheme (Kingsbury and Palmer, 2002). The child-direc</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Brown, R. 1973. A First Language. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared tasks: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>89--97</pages>
<location>Boston, MA, USA.</location>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>Carreras, X. and L. M`arquez. 2004. Introduction to the CoNLL-2004 shared tasks: Semantic role labeling. In Proceedings of CoNLL-2004, pages 89–97. Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8817" citStr="Charniak, 1997" startWordPosition="1367" endWordPosition="1368">es to determine their importance for classification. The Baby SRL does not use inference for the final classification. Instead it classifies every argument independently; thus multiple nouns can have the same role. 2.1 Training The training data were samples of parental speech to one child (’Eve’; (Brown, 1973), available via Childes (MacWhinney, 2000)). We trained on parental utterances in samples 9 through 20, recorded at child age 21-27 months. All verb82 containing utterances without symbols indicating long pauses or unintelligible words were automatically parsed with the Charniak parser (Charniak, 1997) and annotated using an existing SRL system (Punyakanok et al., 2005). In this initial pass, sentences with parsing errors that misidentified argument boundaries were excluded. Final role labels were hand-corrected using the Propbank annotation scheme (Kingsbury and Palmer, 2002). The child-directed speech (CDS) training set consisted of about 2200 sentences, of which a majority had a single verb and two nouns to be labeled1. We used the annotated CDS training data to train our Baby SRL, converting labeled phrases to labeled nouns in the manner described above. 3 Experimental Results To evalua</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, E. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proc. National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ferreira</author>
</authors>
<title>The misinterpretation of noncanonical sentences.</title>
<date>2003</date>
<pages>47--164</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="29377" citStr="Ferreira, 2003" startWordPosition="4825" endWordPosition="4826">. This result is reminiscent of suggestions in the psycholinguis0 0.2 0.4 0.6 0.8 1 Noise Figure 2: Testing the ability of simple features to cope with varying amounts of noisy feedback. Even with noisy feedback, the noun pattern features support learning and generalization to new verbs of a simple agent-patient template for understanding transitive sentences. These results are lower than those found in table 1 due to slightly different training assumptions. tics literature that shallow representations of syntax persist in the adult parser, alongside more sophisticated representations (e.g., (Ferreira, 2003)). 3.7 Noisy Training So far, the Baby SRL has only been trained with perfect feedback. Theories of human language acquisition assume that learning to understand sentences is naturally a partially-supervised task: the child uses existing knowledge of words and syntax to assign a meaning to a sentence; the appropriateness of this meaning for the referential context provides the feedback (e.g., (Pinker, 1989)). But this feedback must be noisy. Referential scenes provide useful but often ambiguous information about the semantic roles of sentence participants. For example, a participant could be c</context>
</contexts>
<marker>Ferreira, 2003</marker>
<rawString>Ferreira, F. 2003. The misinterpretation of noncanonical sentences. Cognitive Psychology, 47:164–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fisher</author>
</authors>
<title>Structural limits on verb mapping: The role of analogy in children’s interpretation of sentences.</title>
<date>1996</date>
<pages>31--41</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="2055" citStr="Fisher, 1996" startWordPosition="291" endWordPosition="292">ld development. 1 Introduction How does the child get started in learning to interpret sentences? The structure-mapping view of early verb and syntax acquisition proposes that © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Dan Roth Department of Computer Science University of Illinois danr@uiuc.edu children start with a shallow structural analysis of sentences: children treat the number of nouns in the sentence as a cue to its semantic predicateargument structure (Fisher, 1996), and represent language experience in an abstract format that permits rapid generalization to new verbs (Gertner et al., 2006). The structure-mapping account makes strong predictions. First, as soon as children can identify some nouns, they should interpret transitive and intransitive sentences differently, simply by assigning a distinct semantic role to each noun in the sentence. Second, language-specific syntactic learning should transfer rapidly to new verbs. Third, some striking errors of interpretation can occur. In “Fred and Ginger danced”, an intransitive verb is presented with two nou</context>
</contexts>
<marker>Fisher, 1996</marker>
<rawString>Fisher, C. 1996. Structural limits on verb mapping: The role of analogy in children’s interpretation of sentences. Cognitive Psychology, 31:41–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gertner</author>
<author>C Fisher</author>
</authors>
<title>Predicted errors in early verb learning.</title>
<date>2006</date>
<booktitle>In 31st Annual Boston University Conference on Language Development.</booktitle>
<contexts>
<context position="3575" citStr="Gertner and Fisher, 2006" startWordPosition="523" endWordPosition="526">, 21-month-olds use the number of nouns to understand sentences containing new verbs (Yuan et al., 2007). Second, 21-month-olds generalize what they have learned about English transitive word-order to sentences containing new verbs: Children who heard ”The girl is gorping the boy” interpreted the girl as an agent and the boy as a patient (Gertner et al., 2006). Third, 21-montholds make the predicted error, treating intransitive sentences containing two nouns as if they were transitive: they interpret the first noun in “The girl and the boy are gorping” as an agent and the second as a patient (Gertner and Fisher, 2006). This error is short-lived. By 25 months, children add new features to their representations of sentences, and interpret conjoined-subject intransitives differ81 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 81–88 Manchester, August 2008 ently from transitives (Naigles, 1990). These experimental results shed light on what syntactic information children might have available for early sentence comprehension, but do not rule out the possibility that children’s early performance is based on a more complex underlying system. In this paper, we test</context>
</contexts>
<marker>Gertner, Fisher, 2006</marker>
<rawString>Gertner, Y. and C. Fisher. 2006. Predicted errors in early verb learning. In 31st Annual Boston University Conference on Language Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gertner</author>
<author>C Fisher</author>
<author>J Eisengart</author>
</authors>
<title>Learning words and rules: Abstract knowledge of word order in early sentence comprehension.</title>
<date>2006</date>
<journal>Psychological Science,</journal>
<pages>17--684</pages>
<contexts>
<context position="2182" citStr="Gertner et al., 2006" startWordPosition="309" endWordPosition="312"> view of early verb and syntax acquisition proposes that © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Dan Roth Department of Computer Science University of Illinois danr@uiuc.edu children start with a shallow structural analysis of sentences: children treat the number of nouns in the sentence as a cue to its semantic predicateargument structure (Fisher, 1996), and represent language experience in an abstract format that permits rapid generalization to new verbs (Gertner et al., 2006). The structure-mapping account makes strong predictions. First, as soon as children can identify some nouns, they should interpret transitive and intransitive sentences differently, simply by assigning a distinct semantic role to each noun in the sentence. Second, language-specific syntactic learning should transfer rapidly to new verbs. Third, some striking errors of interpretation can occur. In “Fred and Ginger danced”, an intransitive verb is presented with two nouns. If children interpret any two-noun sentence as if it were transitive, they should be fooled into interpreting the order of </context>
</contexts>
<marker>Gertner, Fisher, Eisengart, 2006</marker>
<rawString>Gertner, Y., C. Fisher, and J. Eisengart. 2006. Learning words and rules: Abstract knowledge of word order in early sentence comprehension. Psychological Science, 17:684–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Grove</author>
<author>D Roth</author>
</authors>
<title>Linear concepts and hidden variables.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--1</pages>
<contexts>
<context position="8002" citStr="Grove and Roth, 2001" startWordPosition="1238" endWordPosition="1241">d sentence comprehension children can tell which words in a sentence are nouns (Waxman and Booth, 2001), and treat each noun as a candidate argument. Feedback is provided based on annotation in Propbank style: in training, each noun receives the role label of the phrase that noun is part of. Feedback is given at the level of the macro-role (agent, patient, etc., labeled A0-A4 for core arguments, and AM-* adjuncts). We also introduced a NO label for nouns that are not part of any argument. For argument classification we use a linear classifier trained with a regularized perceptron update rule (Grove and Roth, 2001). This learning algorithm provides a simple and general linear classifier that has been demonstrated to work well in other text classification tasks, and allows us to inspect the weights of key features to determine their importance for classification. The Baby SRL does not use inference for the final classification. Instead it classifies every argument independently; thus multiple nouns can have the same role. 2.1 Training The training data were samples of parental speech to one child (’Eve’; (Brown, 1973), available via Childes (MacWhinney, 2000)). We trained on parental utterances in sample</context>
</contexts>
<marker>Grove, Roth, 2001</marker>
<rawString>Grove, A. and D. Roth. 2001. Linear concepts and hidden variables. Machine Learning, 42(1/2):123– 141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC-2002,</booktitle>
<contexts>
<context position="9097" citStr="Kingsbury and Palmer, 2002" startWordPosition="1408" endWordPosition="1411">al speech to one child (’Eve’; (Brown, 1973), available via Childes (MacWhinney, 2000)). We trained on parental utterances in samples 9 through 20, recorded at child age 21-27 months. All verb82 containing utterances without symbols indicating long pauses or unintelligible words were automatically parsed with the Charniak parser (Charniak, 1997) and annotated using an existing SRL system (Punyakanok et al., 2005). In this initial pass, sentences with parsing errors that misidentified argument boundaries were excluded. Final role labels were hand-corrected using the Propbank annotation scheme (Kingsbury and Palmer, 2002). The child-directed speech (CDS) training set consisted of about 2200 sentences, of which a majority had a single verb and two nouns to be labeled1. We used the annotated CDS training data to train our Baby SRL, converting labeled phrases to labeled nouns in the manner described above. 3 Experimental Results To evaluate the Baby SRL we tested it with sentences like those used for the experiments with children described above. All test sentences contained a novel verb (’gorp’). We constructed two test sentence templates: ’A gorps B’ and ’A and B gorp’, where A and B were replaced with nouns th</context>
<context position="16682" citStr="Kingsbury and Palmer, 2002" startWordPosition="2721" endWordPosition="2724">t common nouns in the Eve corpus are pronouns that are strongly biased in their positions and in their semantic roles (e.g., ’you’, ’it’). Despite this high baseline, however, we see the same pattern in the unbiased and biased experiments in table 1. The addition of the NPattern features (feature set 3) substantially improves performance on ’A gorps B’ test sentences, and promotes overgeneralization errors on ’A and B gorp’ sentences. 3.3 More Complex Training Data For comparison purposes we also trained the Baby SRL on a subset of the Propbank training data of Wall Street Journal (WSJ) text (Kingsbury and Palmer, 2002). To approximate the simpler sentences of child-directed speech we selected only those sentences with 8 or fewer words. This provided a training set of about 2500 sentences, most with a single verb and two nouns to be labeled. The CDS and WSJ data pose similar problems for learning abstract and verb-specific knowledge. However, newspaper text differs from casual speech to children in many ways, including vocabulary and sentence complexity. One could argue that the WSJ corpus presents a worst-case scenario for learning based on shallow representations of sentence structure: Full passive sentenc</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Kingsbury, P. and M. Palmer. 2002. From Treebank to PropBank. In Proceedings of LREC-2002, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Third Edition. Lawrence Elrbaum Associates,</title>
<date>2000</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="8556" citStr="MacWhinney, 2000" startWordPosition="1328" endWordPosition="1329">ith a regularized perceptron update rule (Grove and Roth, 2001). This learning algorithm provides a simple and general linear classifier that has been demonstrated to work well in other text classification tasks, and allows us to inspect the weights of key features to determine their importance for classification. The Baby SRL does not use inference for the final classification. Instead it classifies every argument independently; thus multiple nouns can have the same role. 2.1 Training The training data were samples of parental speech to one child (’Eve’; (Brown, 1973), available via Childes (MacWhinney, 2000)). We trained on parental utterances in samples 9 through 20, recorded at child age 21-27 months. All verb82 containing utterances without symbols indicating long pauses or unintelligible words were automatically parsed with the Charniak parser (Charniak, 1997) and annotated using an existing SRL system (Punyakanok et al., 2005). In this initial pass, sentences with parsing errors that misidentified argument boundaries were excluded. Final role labels were hand-corrected using the Propbank annotation scheme (Kingsbury and Palmer, 2002). The child-directed speech (CDS) training set consisted of</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, B. 2000. The CHILDES project: Tools for analyzing talk. Third Edition. Lawrence Elrbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Naigles</author>
</authors>
<title>Children use syntax to learn verb meanings.</title>
<date>1990</date>
<journal>Journal of Child Language,</journal>
<pages>17--357</pages>
<contexts>
<context position="3903" citStr="Naigles, 1990" startWordPosition="569" endWordPosition="570"> (Gertner et al., 2006). Third, 21-montholds make the predicted error, treating intransitive sentences containing two nouns as if they were transitive: they interpret the first noun in “The girl and the boy are gorping” as an agent and the second as a patient (Gertner and Fisher, 2006). This error is short-lived. By 25 months, children add new features to their representations of sentences, and interpret conjoined-subject intransitives differ81 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 81–88 Manchester, August 2008 ently from transitives (Naigles, 1990). These experimental results shed light on what syntactic information children might have available for early sentence comprehension, but do not rule out the possibility that children’s early performance is based on a more complex underlying system. In this paper, we tested the consequences of our representational assumptions by performing experiments with a system for automatic semantic role labeling (SRL), whose knowledge of sentence structure is under our control. Computational models of semantic role labeling learn to identify, for each verb in a sentence, all constituents that fill a sema</context>
</contexts>
<marker>Naigles, 1990</marker>
<rawString>Naigles, L. R. 1990. Children use syntax to learn verb meanings. Journal of Child Language, 17:357–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Learnability and Cognition.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="29787" citStr="Pinker, 1989" startWordPosition="4891" endWordPosition="4892"> 1 due to slightly different training assumptions. tics literature that shallow representations of syntax persist in the adult parser, alongside more sophisticated representations (e.g., (Ferreira, 2003)). 3.7 Noisy Training So far, the Baby SRL has only been trained with perfect feedback. Theories of human language acquisition assume that learning to understand sentences is naturally a partially-supervised task: the child uses existing knowledge of words and syntax to assign a meaning to a sentence; the appropriateness of this meaning for the referential context provides the feedback (e.g., (Pinker, 1989)). But this feedback must be noisy. Referential scenes provide useful but often ambiguous information about the semantic roles of sentence participants. For example, a participant could be construed as an agent of fleeing or as a patient being chased. In a final set of experiments, we examined the generalization abilities of the Baby SRL as a function of the integrity of semantic feedback. We provided noisy semantic-role feedback during training by giving a randomly-selected argument label on 0 to 100% of examples. Following this training, we tested with the ’A gorps B’ test sentences, using t</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>Pinker, S. 1989. Learnability and Cognition. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1117--1123</pages>
<contexts>
<context position="4627" citStr="Punyakanok et al., 2005" startWordPosition="681" endWordPosition="684">for early sentence comprehension, but do not rule out the possibility that children’s early performance is based on a more complex underlying system. In this paper, we tested the consequences of our representational assumptions by performing experiments with a system for automatic semantic role labeling (SRL), whose knowledge of sentence structure is under our control. Computational models of semantic role labeling learn to identify, for each verb in a sentence, all constituents that fill a semantic role, and to determine their roles. We adopt the architecture proposed by Roth and colleagues (Punyakanok et al., 2005), limiting the classifier’s features to a set of lexical features and shallow structural features suggested by the structure-mapping account. Learning ability is measured by the level of SRL accuracy and, more importantly, the types of errors made by the system on sentences containing novel verbs. Testing these predictions on the automatic SRL provides us with a demonstration that it is possible to learn how to correctly assign semantic roles based only on these very simple cues. From an NLP perspective this feature study provides evidence for the efficacy of alternative, simpler syntactic rep</context>
<context position="6152" citStr="Punyakanok et al., 2005" startWordPosition="922" endWordPosition="925">uage development in children, we can explore the nature of initial representations of syntactic structure and build more complex features from there, further mimicking child development. 2 Learning Model We trained a simplified SRL classifier (Baby SRL) with sets of features derived from the structuremapping account. Our test used novel verbs to mimic sentences presented in experiments with children. Our learning task is similar to the full SRL task (Carreras and M`arquez, 2004), except that we classify the roles of individual words rather than full phrases. A full automatic SRL system (e.g. (Punyakanok et al., 2005)) typically involves multiple stages to 1) parse the input, 2) identify arguments, 3) classify those arguments, and then 4) run inference to make sure the final labeling for the full sentence does not violate any linguistic constraints. Our simplified SRL architecture (Baby SRL) essentially replaces the first two steps with heuristics. Rather than identifying arguments via a learned classifier with access to a full syntactic parse, the Baby SRL treats each noun in the sentence as a candidate argument and assigns a semantic role to it. A simple heuristic collapsed compound or sequential nouns t</context>
<context position="8886" citStr="Punyakanok et al., 2005" startWordPosition="1377" endWordPosition="1380">y SRL does not use inference for the final classification. Instead it classifies every argument independently; thus multiple nouns can have the same role. 2.1 Training The training data were samples of parental speech to one child (’Eve’; (Brown, 1973), available via Childes (MacWhinney, 2000)). We trained on parental utterances in samples 9 through 20, recorded at child age 21-27 months. All verb82 containing utterances without symbols indicating long pauses or unintelligible words were automatically parsed with the Charniak parser (Charniak, 1997) and annotated using an existing SRL system (Punyakanok et al., 2005). In this initial pass, sentences with parsing errors that misidentified argument boundaries were excluded. Final role labels were hand-corrected using the Propbank annotation scheme (Kingsbury and Palmer, 2002). The child-directed speech (CDS) training set consisted of about 2200 sentences, of which a majority had a single verb and two nouns to be labeled1. We used the annotated CDS training data to train our Baby SRL, converting labeled phrases to labeled nouns in the manner described above. 3 Experimental Results To evaluate the Baby SRL we tested it with sentences like those used for the e</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>Punyakanok, V., D. Roth, and W. Yih. 2005. The necessity of syntactic parsing for semantic role labeling. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), pages 1117–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roland</author>
<author>F Dick</author>
<author>J L Elman</author>
</authors>
<title>Frequency of basic english grammatical structures: A corpus analysis.</title>
<date>2007</date>
<journal>Journal of Memory and Language,</journal>
<pages>57--348</pages>
<contexts>
<context position="17411" citStr="Roland et al., 2007" startWordPosition="2841" endWordPosition="2844">wer words. This provided a training set of about 2500 sentences, most with a single verb and two nouns to be labeled. The CDS and WSJ data pose similar problems for learning abstract and verb-specific knowledge. However, newspaper text differs from casual speech to children in many ways, including vocabulary and sentence complexity. One could argue that the WSJ corpus presents a worst-case scenario for learning based on shallow representations of sentence structure: Full passive sentences are more common in written corpora such as the WSJ than in samples of conversational speech, for example (Roland et al., 2007). As a result of such differences, two-noun sequences are less likely to display an A0-A1 sequence in the WSJ (0.42 A0- A1 in 2-noun sentences) than in the CDS training data (0.67 A0-A1). The WSJ data provides a more demanding test of the Baby SRL. We trained the Baby SRL on the WSJ data, and tested it using the biased lexical choices as described above, sampling A and B nouns for novelverb test sentences based on the distribution of nouns seen as the first of two nouns in training, and as the second of two nouns, respectively. The WSJ training produced performance strikingly similar to the pe</context>
</contexts>
<marker>Roland, Dick, Elman, 2007</marker>
<rawString>Roland, D., F. Dick, and J. L. Elman. 2007. Frequency of basic english grammatical structures: A corpus analysis. Journal of Memory and Language, 57:348–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Waxman</author>
<author>A Booth</author>
</authors>
<title>Seeing pink elephants: Fourteen-month-olds’s interpretations of novel nouns and adjectives. Cognitive Psychology,</title>
<date>2001</date>
<pages>43--217</pages>
<contexts>
<context position="7484" citStr="Waxman and Booth, 2001" startWordPosition="1148" endWordPosition="1151">d as the single noun ’Smith’. Other complex noun phrases were not simplified in this way. Thus, a phrase such as ’the toy on the floor’ would be treated as two separate nouns, ’toy’ and ’floor’. This represents the assumption that young children know ’Mr. Smith’ is a single name, but they do not know all the predicating terms that may link multiple nouns into a single noun phrase. The simplified learning task of the Baby SRL implements a key assumption of the structure-mapping account: that at the start of multiword sentence comprehension children can tell which words in a sentence are nouns (Waxman and Booth, 2001), and treat each noun as a candidate argument. Feedback is provided based on annotation in Propbank style: in training, each noun receives the role label of the phrase that noun is part of. Feedback is given at the level of the macro-role (agent, patient, etc., labeled A0-A4 for core arguments, and AM-* adjuncts). We also introduced a NO label for nouns that are not part of any argument. For argument classification we use a linear classifier trained with a regularized perceptron update rule (Grove and Roth, 2001). This learning algorithm provides a simple and general linear classifier that has</context>
<context position="26870" citStr="Waxman and Booth, 2001" startWordPosition="4412" endWordPosition="4416">120 5 60 # 0 5000 10000 15000 20000 -0.5 3.5 2.5 0.5 1.5 3 2 0 1 Known Verbs _N V 90 30 0 180 150 120 5 # 60 86 mational bottleneck on the relative weighting of NPattern and VPosition features during training, we modified the Baby SRL’s training procedure such that NPattern features were always active, but VPosition features were active during training only when the verb in the current example had been encountered a critical number of times. This represents the assumption that the child can recognize which words in the sentence are nouns, based on lexical familiarity or morphological context (Waxman and Booth, 2001), but is less likely to be able to represent position relative to the verb without knowing the verb well. Figure 1 shows the tendency of the NPattern feature ’ N’ (first of two nouns) and the VPosition feature ’ V’ (pre-verbal noun) to predict the role A0 as opposed to A1 as the difference between the weights of these connections in the learned network. Figure 1(a) shows the results when VPosition features were active whenever the target verb had occurred at least 5 times; in Figure 1(b) the threshold for verb familiarity was 20. In both figures we see that the VPosition features win out over </context>
</contexts>
<marker>Waxman, Booth, 2001</marker>
<rawString>Waxman, S. R. and A. Booth. 2001. Seeing pink elephants: Fourteen-month-olds’s interpretations of novel nouns and adjectives. Cognitive Psychology, 43:217–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yuan</author>
<author>C Fisher</author>
<author>Y Gertner</author>
<author>J Snedeker</author>
</authors>
<title>Participants are more than physical bodies: 21-month-olds assign relational meaning to novel transitive verbs.</title>
<date>2007</date>
<booktitle>In Biennial Meeting of the Society for Research in Child Development,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="3054" citStr="Yuan et al., 2007" startWordPosition="437" endWordPosition="440">ce. Second, language-specific syntactic learning should transfer rapidly to new verbs. Third, some striking errors of interpretation can occur. In “Fred and Ginger danced”, an intransitive verb is presented with two nouns. If children interpret any two-noun sentence as if it were transitive, they should be fooled into interpreting the order of two nouns in such conjoined-subject intransitive sentences as conveying agent-patient role information. Experiments with young children support these predictions. First, 21-month-olds use the number of nouns to understand sentences containing new verbs (Yuan et al., 2007). Second, 21-month-olds generalize what they have learned about English transitive word-order to sentences containing new verbs: Children who heard ”The girl is gorping the boy” interpreted the girl as an agent and the boy as a patient (Gertner et al., 2006). Third, 21-montholds make the predicted error, treating intransitive sentences containing two nouns as if they were transitive: they interpret the first noun in “The girl and the boy are gorping” as an agent and the second as a patient (Gertner and Fisher, 2006). This error is short-lived. By 25 months, children add new features to their r</context>
</contexts>
<marker>Yuan, Fisher, Gertner, Snedeker, 2007</marker>
<rawString>Yuan, S., C. Fisher, Y. Gertner, and J. Snedeker. 2007. Participants are more than physical bodies: 21-month-olds assign relational meaning to novel transitive verbs. In Biennial Meeting of the Society for Research in Child Development, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>