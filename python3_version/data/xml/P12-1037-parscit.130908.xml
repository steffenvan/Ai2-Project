<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076509">
<title confidence="0.933052">
Learning to “Read Between the Lines” using Bayesian Logic Programs
</title>
<author confidence="0.998559">
Sindhu Raghavan Raymond J. Mooney Hyeonseo Ku
</author>
<affiliation confidence="0.9994045">
Department of Computer Science
The University of Texas at Austin
</affiliation>
<address confidence="0.9833015">
1616 Guadalupe, Suite 2.408
Austin, TX 78701, USA
</address>
<email confidence="0.999848">
{sindhu,mooney,yorq}@cs.utexas.edu
</email>
<sectionHeader confidence="0.998607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999644909090909">
Most information extraction (IE) systems
identify facts that are explicitly stated in text.
However, in natural language, some facts are
implicit, and identifying them requires “read-
ing between the lines”. Human readers nat-
urally use common sense knowledge to in-
fer such implicit information from the explic-
itly stated facts. We propose an approach
that uses Bayesian Logic Programs (BLPs),
a statistical relational model combining first-
order logic and Bayesian networks, to infer
additional implicit information from extracted
facts. It involves learning uncertain common-
sense knowledge (in the form of probabilis-
tic first-order rules) from natural language text
by mining a large corpus of automatically ex-
tracted facts. These rules are then used to de-
rive additional facts from extracted informa-
tion using BLP inference. Experimental eval-
uation on a benchmark data set for machine
reading demonstrates the efficacy of our ap-
proach.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965636363637">
The task of information extraction (IE) involves au-
tomatic extraction of typed entities and relations
from unstructured text. IE systems (Cowie and
Lehnert, 1996; Sarawagi, 2008) are trained to extract
facts that are stated explicitly in text. However, some
facts are implicit, and human readers naturally “read
between the lines” and infer them from the stated
facts using commonsense knowledge. Answering
many queries can require inferring such implicitly
stated facts. Consider the text “Barack Obama is the
president of the United States of America.” Given
the query “Barack Obama is a citizen of what coun-
try?”, standard IE systems cannot identify the an-
swer since citizenship is not explicitly stated in the
text. However, a human reader possesses the com-
monsense knowledge that the president of a country
is almost always a citizen of that country, and easily
infers the correct answer.
The standard approach to inferring implicit infor-
mation involves using commonsense knowledge in
the form of logical rules to deduce additional in-
formation from the extracted facts. Since manually
developing such a knowledge base is difficult and
arduous, an effective alternative is to automatically
learn such rules by mining a substantial database of
facts that an IE system has already automatically
extracted from a large corpus of text (Nahm and
Mooney, 2000). Most existing rule learners assume
that the training data is largely accurate and com-
plete. However, the facts extracted by an IE sys-
tem are always quite noisy and incomplete. Conse-
quently, a purely logical approach to learning and in-
ference is unlikely to be effective. Consequently, we
propose using statistical relational learning (SRL)
(Getoor and Taskar, 2007), specifically, Bayesian
Logic Programs (BLPs) (Kersting and De Raedt,
2007), to learn probabilistic rules in first-order logic
from a large corpus of extracted facts and then use
the resulting BLP to make effective probabilistic in-
ferences when interpreting new documents.
We have implemented this approach by using an
off-the-shelf IE system and developing novel adap-
tations of existing learning methods to efficiently
construct fast and effective BLPs for “reading be-
</bodyText>
<page confidence="0.986354">
349
</page>
<note confidence="0.9857615">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349–358,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999871269230769">
tween the lines.” We present an experimental evalu-
ation of our resulting system on a realistic test cor-
pus from DARPA’s Machine Reading project, and
demonstrate improved performance compared to a
purely logical approach based on Inductive Logic
Programming (ILP) (Lavra˘c and D˘zeroski, 1994),
and an alternative SRL approach based on Markov
Logic Networks (MLNs) (Domingos and Lowd,
2009).
To the best of our knowledge, this is the first paper
that employs BLPs for inferring implicit information
from natural language text. We demonstrate that it
is possible to learn the structure and the parameters
of BLPs automatically using only noisy extractions
from natural language text, which we then use to in-
fer additional facts from text.
The rest of the paper is organized as follows. Sec-
tion 2 discusses related work and highlights key dif-
ferences between our approach and existing work.
Section 3 provides a brief background on BLPs.
Section 4 describes our BLP-based approach to
learning to infer implicit facts. Section 5 describes
our experimental methodology and discusses the re-
sults of our evaluation. Finally, Section 6 discusses
potential future work and Section 7 presents our fi-
nal conclusions.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99556752238806">
Several previous projects (Nahm and Mooney, 2000;
Carlson et al., 2010; Schoenmackers et al., 2010;
Doppa et al., 2010; Sorower et al., 2011) have mined
inference rules from data automatically extracted
from text by an IE system. Similar to our approach,
these systems use the learned rules to infer addi-
tional information from facts directly extracted from
a document. Nahm and Mooney (2000) learn propo-
sitional rules using C4.5 (Quinlan, 1993) from data
extracted from computer-related job-postings, and
therefore cannot learn multi-relational rules with
quantified variables. Other systems (Carlson et al.,
2010; Schoenmackers et al., 2010; Doppa et al.,
2010; Sorower et al., 2011) learn first-order rules
(i.e. Horn clauses in first-order logic).
Carlson et al. (2010) modify an ILP system simi-
lar to FOIL (Quinlan, 1990) to learn rules with prob-
abilistic conclusions. They use purely logical de-
duction (forward-chaining) to infer additional facts.
Unlike BLPs, this approach does not use a well-
founded probabilistic graphical model to compute
coherent probabilities for inferred facts. Further,
Carlson et al. (2010) used a human judge to man-
ually evaluate the quality of the learned rules before
using them to infer additional facts. Our approach,
on the other hand, is completely automated and
learns fully parameterized rules in a well-defined
probabilistic logic.
Schoenmackers et al. (2010) develop a system
called SHERLOCK that uses statistical relevance to
learn first-order rules. Unlike our system and others
(Carlson et al., 2010; Doppa et al., 2010; Sorower et
al., 2011) that use a pre-defined ontology, they auto-
matically identify a set of entity types and relations
using “open IE.” They use HOLMES (Schoenmack-
ers et al., 2008), an inference engine based on MLNs
(Domingos and Lowd, 2009) (an SRL approach that
combines first-order logic and Markov networks)
to infer additional facts. However, MLNs include
all possible type-consistent groundings of the rules
in the corresponding Markov net, which, for larger
datasets, can result in an intractably large graphical
model. To overcome this problem, HOLMES uses
a specialized model construction process to control
the grounding process. Unlike MLNs, BLPs natu-
rally employ a more “focused” approach to ground-
ing by including only those literals that are directly
relevant to the query.
Doppa et al. (2010) use FARMER (Nijssen and
Kok, 2003), an existing ILP system, to learn first-
order rules. They propose several approaches to
score the rules, which are used to infer additional
facts using purely logical deduction. Sorower et al.
(2011) propose a probabilistic approach to modeling
implicit information as missing facts and use MLNs
to infer these missing facts. They learn first-order
rules for the MLN by performing exhaustive search.
As mentioned earlier, inference using both these ap-
proaches, logical deduction and MLNs, have certain
limitations, which BLPs help overcome.
DIRT (Lin and Pantel, 2001) and RESOLVER
(Yates and Etzioni, 2007) learn inference rules, also
called entailment rules that capture synonymous re-
lations and entities from text. Berant et al. (Berant
et al., 2011) propose an approach that uses transitiv-
ity constraints for learning entailment rules for typed
predicates. Unlike the systems described above,
</bodyText>
<page confidence="0.993975">
350
</page>
<bodyText confidence="0.9989932">
these systems do not learn complex first-order rules
that capture common sense knowledge. Further,
most of these systems do not use extractions from
an IE system to learn entailment rules, thereby mak-
ing them less related to our approach.
</bodyText>
<sectionHeader confidence="0.938768" genericHeader="method">
3 Bayesian Logic Programs
</sectionHeader>
<bodyText confidence="0.965000793103448">
Bayesian logic programs (BLPs) (Kersting and De
Raedt, 2007; Kersting and Raedt, 2008) can be con-
sidered as templates for constructing directed graph-
ical models (Bayes nets). Formally, a BLP con-
sists of a set of Bayesian clauses, definite clauses
of the form a|a1, a2, a3, an, where n &gt; 0 and
a, a1, a2, a3, ,an are Bayesian predicates (de-
fined below), and where a is called the head of
the clause (head(c)) and (a1, a2, a3,....,an) is the
body (body(c)). When n = 0, a Bayesian clause
is a fact. Each Bayesian clause c is assumed to
be universally quantified and range restricted, i.e
variables{head} C_ variables{body}, and has an
associated conditional probability table CPT(c) =
P(head(c)|body(c)). A Bayesian predicate is a pred-
icate with a finite domain, and each ground atom for
a Bayesian predicate represents a random variable.
Associated with each Bayesian predicate is a com-
bining rule such as noisy-or or noisy-and that maps
a finite set of CPTs into a single CPT.
Given a knowledge base as a BLP, standard logi-
cal inference (SLD resolution) is used to automat-
ically construct a Bayes net for a given problem.
More specifically, given a set of facts and a query,
all possible Horn-clause proofs of the query are con-
structed and used to build a Bayes net for answering
the query. The probability of a joint assignment of
truth values to the final set of ground propositions is
defined as follows:
</bodyText>
<equation confidence="0.979725">
P(X) = Ili P(Xi|Pa(Xi)),
</equation>
<bodyText confidence="0.999267222222222">
where X = X1, X2, ..., Xn represents the set of
random variables in the network and Pa(Xi) rep-
resents the parents of Xi. Once a ground network is
constructed, standard probabilistic inference meth-
ods can be used to answer various types of queries
as reviewed by Koller and Friedman (2009). The
parameters in the BLP model can be learned using
the methods described by Kersting and De Raedt
(2008).
</bodyText>
<sectionHeader confidence="0.860252" genericHeader="method">
4 Learning BLPs to Infer Implicit Facts
</sectionHeader>
<subsectionHeader confidence="0.999311">
4.1 Learning Rules from Extracted Data
</subsectionHeader>
<bodyText confidence="0.999990046511628">
The first step involves learning commonsense
knowledge in the form of first-order Horn rules from
text. We first extract facts that are explicitly stated
in the text using SIRE (Florian et al., 2004), an IE
system developed by IBM. We then learn first-order
rules from these extracted facts using LIME (Mc-
creath and Sharma, 1998), an ILP system designed
for noisy training data.
We first identify a set of target relations we want
to infer. Typically, an ILP system takes a set of
positive and negative instances for a target relation,
along with a background knowledge base (in our
case, other facts extracted from the same document)
from which the positive instances are potentially in-
ferable. In our task, we only have direct access to
positive instances of target relations, i.e the relevant
facts extracted from the text. So we artificially gen-
erate negative instances using the closed world as-
sumption, which states that any instance of a rela-
tion that is not extracted can be considered a nega-
tive instance. While there are exceptions to this as-
sumption, it typically generates a useful (if noisy)
set of negative instances. For each relation, we gen-
erate all possible type-consistent instances using all
constants in the domain. All instances that are not
extracted facts (i.e. positive instances) are labeled
as negative. The total number of such closed-world
negatives can be intractably large, so we randomly
sample a fixed-size subset. The ratio of 1:20 for
positive to negative instances worked well in our ap-
proach.
Since LIME can learn rules using only positive in-
stances, or both positive and negative instances, we
learn rules using both settings. We include all unique
rules learned from both settings in the final set, since
the goal of this step is to learn a large set of po-
tentially useful rules whose relative strengths will
be determined in the next step of parameter learn-
ing. Other approaches could also be used to learn
candidate rules. We initially tried using the popular
ALEPH ILP system (Srinivasan, 2001), but it did not
produce useful rules, probably due to the high level
of noise in our training data.
</bodyText>
<page confidence="0.99237">
351
</page>
<subsectionHeader confidence="0.996748">
4.2 Learning BLP Parameters
</subsectionHeader>
<bodyText confidence="0.99999635">
The parameters of a BLP include the CPT entries as-
sociated with the Bayesian clauses and the parame-
ters of combining rules associated with the Bayesian
predicates. For simplicity, we use a deterministic
logical-and model to encode the CPT entries associ-
ated with Bayesian clauses, and use noisy-or to com-
bine evidence coming from multiple ground rules
that have the same head (Pearl, 1988). The noisy-
or model requires just a single parameter for each
rule, which can be learned from training data.
We learn the noisy-or parameters using the EM
algorithm adapted for BLPs by Kersting and De
Raedt (2008). In our task, the supervised training
data consists of facts that are extracted from the
natural language text. However, we usually do not
have evidence for inferred facts as well as noisy-or
nodes. As a result, there are a number of variables in
the ground networks which are always hidden, and
hence EM is appropriate for learning the requisite
parameters from the partially observed training data.
</bodyText>
<subsectionHeader confidence="0.998642">
4.3 Inference of Additional Facts using BLPs
</subsectionHeader>
<bodyText confidence="0.999997333333334">
Inference in the BLP framework involves backward
chaining (Russell and Norvig, 2003) from a spec-
ified query (SLD resolution) to obtain all possi-
ble deductive proofs for the query. In our context,
each target relation becomes a query on which we
backchain. We then construct a ground Bayesian
network using the resulting deductive proofs for
all target relations and learned parameters using
the standard approach described in Section 3. Fi-
nally, we perform standard probabilistic inference
to estimate the marginal probability of each inferred
fact. Our system uses Sample Search (Gogate and
Dechter, 2007), an approximate sampling algorithm
developed for Bayesian networks with determinis-
tic constraints (0 values in CPTs). We tried several
exact and approximate inference algorithms on our
data, and this was the method that was both tractable
and produced the best results.
</bodyText>
<sectionHeader confidence="0.995393" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.946588">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999860653846154">
For evaluation, we used DARPA’s machine-reading
intelligence-community (IC) data set, which con-
sists of news articles on terrorist events around the
world. There are 10, 000 documents each contain-
ing an average of 89.5 facts extracted by SIRE (Flo-
rian et al., 2004). SIRE assigns each extracted fact
a confidence score and we used only those with a
score of 0.5 or higher for learning and inference. An
average of 86.8 extractions per document meet this
threshold.
DARPA also provides an ontology describing the
entities and relations in the IC domain. It con-
sists of 57 entity types and 79 relations. The
entity types include Agent, PhysicalThing, Event,
TimeLocation, Gender, and Group, each with sev-
eral subtypes. The type hierarchy is a DAG rather
than a tree, and several types have multiple super-
classes. For instance, a GeopoliticalEntity can be
a HumanAgent as well as a Location. This can
cause some problems for systems that rely on a
strict typing system, such as MLNs which rely on
types to limit the space of ground literals that are
considered. Some sample relations are attended-
School, approximateNumberOfMembers, mediatin-
gAgent, employs, hasMember, hasMemberHuman-
Agent, and hasBirthPlace.
</bodyText>
<subsectionHeader confidence="0.981758">
5.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999763">
We evaluated our approach using 10-fold cross vali-
dation. We learned first-order rules for the 13 tar-
get relations shown in Table 3 from the facts ex-
tracted from the training documents (Section 4.1).
These relations were selected because the extrac-
tor’s recall for them was low. Since LIME does not
scale well to large data sets, we could train it on
at most about 2, 500 documents. Consequently, we
split the 9, 000 training documents into four disjoint
subsets and learned first-order rules from each sub-
set. The final knowledge base included all unique
rules learned from any subset. LIME learned sev-
eral rules that had only entity types in their bodies.
Such rules make many incorrect inferences; hence
we eliminated them. We also eliminated rules vio-
lating type constraints. We learned an average of 48
rules per fold. Table 1 shows some sample learned
rules.
We then learned parameters as described in Sec-
tion 4.2. We initially set all noisy-or parameters to
0.9 based on the intuition that if exactly one rule for
a consequent was satisfied, it could be inferred with
a probability of 0.9.
</bodyText>
<page confidence="0.980913">
352
</page>
<table confidence="0.999375181818182">
governmentOrganization(A) n employs(A,B) —* hasMember(A,B)
If a government organization A employs person B, then B is a member of A
eventLocation(A,B) n bombing(A) —* thingPhysicallyDamaged(A,B)
If a bombing event A took place in location B, then B is physically damaged
isLedBy(A,B) —* hasMemberPerson(A,B)
If a group A is led by person B, then B is a member of A
nationState(B) n eventLocationGPE(A,B) —* eventLocation(A,B)
If an event A occurs in a geopolitical entity B, then the event location for that event is B
mediatingAgent(A,B) n humanAgentKillingAPerson(A) —* killingHumanAgent(A,B)
If A is an event in which a human agent is killing a person and the mediating agent of A is an agent B, then B is
the human agent that is killing in event A
</table>
<tableCaption confidence="0.999925">
Table 1: A sample set of rules learned using LIME
</tableCaption>
<bodyText confidence="0.9992166">
For each test document, we performed BLP in-
ference as described in Section 4.3. We ranked all
inferences by their marginal probability, and evalu-
ated the results by either choosing the top n infer-
ences or accepting inferences whose marginal prob-
ability was equal to or exceeded a specified thresh-
old. We evaluated two BLPs with different param-
eter settings: BLP-Learned-Weights used noisy-or
parameters learned using EM, BLP-Manual-Weights
used fixed noisy-or weights of 0.9.
</bodyText>
<subsectionHeader confidence="0.982542">
5.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999607">
The lack of ground truth annotation for inferred facts
prevents an automated evaluation, so we resorted
to a manual evaluation. We randomly sampled 40
documents (4 from each test fold), judged the ac-
curacy of the inferences for those documents, and
computed precision, the fraction of inferences that
were deemed correct. For probabilistic methods like
BLPs and MLNs that provide certainties for their
inferences, we also computed precision at top n,
which measures the precision of the n inferences
with the highest marginal probability across the 40
test documents. Measuring recall for making infer-
ences is very difficult since it would require labeling
a reasonable-sized corpus of documents with all of
the correct inferences for a given set of target rela-
tions, which would be extremely time consuming.
Our evaluation is similar to that used in previous re-
lated work (Carlson et al., 2010; Schoenmackers et
al., 2010).
SIRE frequently makes incorrect extractions, and
therefore inferences made from these extractions are
also inaccurate. To account for the mistakes made
by the extractor, we report two different precision
scores. The “unadjusted” (UA) score, does not cor-
rect for errors made by the extractor. The “adjusted”
(AD) score does not count mistakes due to extraction
errors. That is, if an inference is incorrect because
it was based on incorrect extracted facts, we remove
it from the set of inferences and calculate precision
for the remaining inferences.
</bodyText>
<subsectionHeader confidence="0.983114">
5.4 Baselines
</subsectionHeader>
<bodyText confidence="0.9996375">
Since none of the existing approaches have been
evaluated on the IC data, we cannot directly compare
our performance to theirs. Therefore, we compared
to the following methods:
</bodyText>
<listItem confidence="0.7573925">
• Logical Deduction: This method forward
chains on the extracted facts using the first-
order rules learned by LIME to infer additional
facts. This approach is unable to provide any
confidence or probability for its conclusions.
• Markov Logic Networks (MLNs): We use the
</listItem>
<bodyText confidence="0.981500769230769">
rules learned by LIME to define the structure
of an MLN. In the first setting, which we call
MLN-Learned-Weights, we learn the MLN’s
parameters using the generative weight learn-
ing algorithm (Domingos and Lowd, 2009),
which we modified to process training exam-
ples in an online manner. In online generative
learning, gradients are calculated and weights
are estimated after processing each example
and the learned weights are used as the start-
ing weights for the next example. The pseudo-
likelihood of one round is obtained by multi-
plying the pseudo-likelihood of all examples.
</bodyText>
<page confidence="0.998015">
353
</page>
<table confidence="0.9842105">
UA AD
Precision 29.73 (443/1490) 35.24 (443/1257)
</table>
<tableCaption confidence="0.966943">
Table 2: Precision for logical deduction. “UA” and “AD”
refer to the unadjusted and adjusted scores respectively
</tableCaption>
<bodyText confidence="0.999940230769231">
In our approach, the initial weights of clauses
are set to 10. The average number of itera-
tions needed to acquire the optimal weights is
131. In the second setting, which we call MLN-
Manual-Weights, we assign a weight of 10 to
all rules and maximum likelihood prior to all
predicates. MLN-Manual-Weights is similar to
BLP-Manual-Weights in that all rules are given
the same weight. We then use the learned rules
and parameters to probabilistically infer addi-
tional facts using the MC-SAT algorithm im-
plemented in Alchemy,1 an open-source MLN
package.
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.998958">
6.1 Comparison to Baselines
</subsectionHeader>
<bodyText confidence="0.999946826086957">
Table 2 gives the unadjusted (UA) and adjusted
(AD) precision for logical deduction. Out of 1, 490
inferences for the 40 evaluation documents, 443
were judged correct, giving an unadjusted preci-
sion of 29.73%. Out of these 1, 490 inferences, 233
were determined to be incorrect due to extraction er-
rors, improving the adjusted precision to a modest
35.24%.
MLNs made about 127, 000 inferences for the 40
evaluation documents. Since it is not feasible to
manually evaluate all the inferences made by the
MLN, we calculated precision using only the top
1000 inferences. Figure 1 shows both unadjusted
and adjusted precision at top-n for various values
of n for different BLP and MLN models. For both
BLPs and MLNs, simple manual weights result in
superior performance than the learned weights. De-
spite the fairly large size of the overall training sets
(9,000 documents), the amount of data for each
target relation is apparently still not sufficient to
learn particularly accurate weights for both BLPs
and MLNs. However, for BLPs, learned weights
do show a substantial improvement initially (i.e.
</bodyText>
<footnote confidence="0.925433">
1http://alchemy.cs.washington.edu/
</footnote>
<bodyText confidence="0.999951666666666">
top 25–50 inferences), with an average of 1 infer-
ence per document at 91% adjusted precision as
opposed to an average of 5 inferences per docu-
ment at 85% adjusted precision for BLP-Manual-
Weights. For MLNs, learned weights show a small
improvement initially only with respect to adjusted
precision. Between BLPs and MLNs, BLPs per-
form substantially better than MLNs at most points
in the curve. However, MLN-Manual-Weights im-
prove marginally over BLP-Learned-Weights at later
points (top 600 and above) on the curve, where the
precision is generally very low. Here, the superior
performance of BLPs over MLNs could be possibly
due to the focused grounding used in the BLP frame-
work.
For BLPs, as n increases towards including all of
the logically sanctioned inferences, as expected, the
precision converges to the results for logical deduc-
tion. However, as n decreases, both adjusted and
unadjusted precision increase fairly steadily. This
demonstrates that probabilistic BLP inference pro-
vides a clear improvement over logical deduction,
allowing the system to accurately select the best in-
ferences that are most likely to be correct. Unlike the
two BLP models, MLN-Manual-Weights has more
or less the same performance at most points on the
curve, and it is slightly better than that of purely-
logical deduction. MLN-Learned-Weights is worse
than purely-logical deduction at most points on the
curve.
</bodyText>
<subsectionHeader confidence="0.526417">
6.2 Results for Individual Target Relations
</subsectionHeader>
<bodyText confidence="0.9999855625">
Table 3 shows the adjusted precision for each
relation for instances inferred using logical de-
duction, BLP-Manual-Weights and BLP-Learned-
Weights with a confidence threshold of 0.95. The
probabilities estimated for inferences by MLNs are
not directly comparable to those estimated by BLPs.
As a result, we do not include results for MLNs
here. For this evaluation, using a confidence thresh-
old based cutoff is more appropriate than using top-
n inferences made by the BLP models since the esti-
mated probabilities can be directly compared across
target relations.
For logical deduction, precision is high for a few
relations like employs, hasMember, and hasMem-
berHumanAgent, indicating that the rules learned
for these relations are more accurate than the ones
</bodyText>
<page confidence="0.995867">
354
</page>
<figure confidence="0.999330057142857">
Unadjusted Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0 100 200 300 400 500 600 700 800 900 1000
BLP−Learned−Weights
BLP−Manual−Weights
MLN−Learned−Weights
MLN−Manual−Weights
Adjusted Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 100 200 300 400 500 600 700 800 900 1000
1
BLP−Learned−Weights
BLP−Manual−Weights
MLN−Learned−Weights
MLN−Manual−Weights
Top−n inferences Top−n inferences
</figure>
<figureCaption confidence="0.999976">
Figure 1: Unadjusted and adjusted precision at top-n for different BLP and MLN models for various values of n
</figureCaption>
<bodyText confidence="0.999984419354839">
learned for the remaining relations. Unlike rela-
tions like hasMember that are easily inferred from
relations like employs and isLedBy, certain relations
like hasBirthPlace are not easily inferable using the
information in the ontology. As a result, it might
not be possible to learn accurate rules for such tar-
get relations. Other reasons include the lack of a
sufficiently large number of target-relation instances
during training and lack of strictly defined types in
the IC ontology.
Both BLP-Manual-Weights and BLP-Learned-
Weights also have high precision for several re-
lations (eventLocation, hasMemberHumanAgent,
thingPhysicallyDamaged). However, the actual
number of inferences can be fairly low. For in-
stance, 103 instances of hasMemberHumanAgent
are inferred by logical deduction (i.e. 0 confidence
threshold), but only 2 of them are inferred by BLP-
Learned-Weights at 0.95 confidence threshold, in-
dicating that the parameters learned for the corre-
sponding rules are not very high. For several rela-
tions like hasMember, hasMemberPerson, and em-
ploys, no instances were inferred by BLP-Learned-
Weights at 0.95 confidence threshold. Lack of suffi-
cient training instances (extracted facts) is possibly
the reason for learning low weights for such rules.
On the other hand, BLP-Manual-Weights has in-
ferred 26 instances of hasMemberHumanAgent, out
which all are correct. These results therefore demon-
strate the need for sufficient training examples to
learn accurate parameters.
</bodyText>
<subsectionHeader confidence="0.989768">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99990165625">
We now discuss the potential reasons for BLP’s su-
perior performance compared to other approaches.
Probabilistic reasoning used in BLPs allows for a
principled way of determining the most confident
inferences, thereby allowing for improved precision
over purely logical deduction. The primary dif-
ference between BLPs and MLNs lies in the ap-
proaches used to construct the ground network. In
BLPs, only propositions that can be logically de-
duced from the extracted evidence are included in
the ground network. On the other hand, MLNs in-
clude all possible type-consistent groundings of all
rules in the network, introducing many ground liter-
als which cannot be logically deduced from the ev-
idence. This generally results in several incorrect
inferences, thereby yielding poor performance.
Even though learned weights in BLPs do not re-
sult in a superior performance, learned weights in
MLNs are substantially worse. Lack of sufficient
training data is one of the reasons for learning less
accurate weights by the MLN weight learner. How-
ever, a more important issue is due to the use of the
closed world assumption during learning, which we
believe is adversely impacting the weights learned.
As mentioned earlier, for the task considered in the
paper, if a fact is not explicitly stated in text, and
hence not extracted by the extractor, it does not nec-
essarily imply that it is not true. Since existing
weight learning approaches for MLNs do not deal
with missing data and open world assumption, de-
veloping such approaches is a topic for future work.
Apart from developing novel approaches for
</bodyText>
<page confidence="0.994691">
355
</page>
<table confidence="0.999733785714286">
Relation Logical Deduction BLP-Manual-Weights-.95 BLP-Learned-Weights-.95 No. training instances
employs 69.44 (25/36) 92.85 (13/14) nil (0/0) 18440
eventLocation 18.75 (18/96) 100.00 (1/1) 100 (1/1) 6902
hasMember 95.95 (95/99) 97.26 (71/73) nil (0/0) 1462
hasMemberPerson 43.75 (42/96) 100.00 (14/14) nil (0/0) 705
isLedBy 12.30 (8/65) nil (0/0) nil (0/0) 8402
mediatingAgent 19.73 (15/76) nil (0/0) nil (0/0) 92998
thingPhysicallyDamaged 25.72 (62/241) 90.32 (28/31) 90.32 (28/31) 24662
hasMemberHumanAgent 95.14 (98/103) 100.00 (26/26) 100.00 (2/2) 3619
killingHumanAgent 15.35 (43/280) 33.33 (2/6) 66.67 (2/3) 3341
hasBirthPlace 0 (0/88) nil (0/0) nil (0/0) 89
thingPhysicallyDestroyed nil (0/0) nil (0/0) nil (0/0) 800
hasCitizenship 48.05 (37/77) 58.33 (35/60) nil (0/0) 222
attendedSchool nil (0/0) nil (0/0) nil (0/0) 2
</table>
<tableCaption confidence="0.999643">
Table 3: Adjusted precision for individual relations (highest values are in bold)
</tableCaption>
<bodyText confidence="0.999863583333333">
weight learning, additional engineering could poten-
tially improve the performance of MLNs on the IC
data set. Due to MLN’s grounding process, sev-
eral spurious facts like employs(a,a) were inferred.
These inferences can be prevented by including ad-
ditional clauses in the MLN that impose integrity
constraints that prevent such nonsensical proposi-
tions. Further, techniques proposed by Sorower et
al. (2011) can be incorporated to explicitly han-
dle missing information in text. Lack of strict typ-
ing on the arguments of relations in the IC ontol-
ogy has also resulted in inferior performance of the
MLNs. To overcome this, relations that do not have
strictly defined types could be specialized. Finally,
we could use the deductive proofs constructed by
BLPs to constrain the ground Markov network, sim-
ilar to the model-construction approach adopted by
Singla and Mooney (2011).
However, in contrast to MLNs, BLPs that use
first-order rules that are learned by an off-the-shelf
ILP system and given simple intuitive hand-coded
weights, are able to provide fairly high-precision in-
ferences that augment the output of an IE system and
allow it to effectively “read between the lines.”
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999969736842105">
A primary goal for future research is developing an
on-line structure learner for BLPs that can directly
learn probabilistic first-order rules from uncertain
training data. This will address important limita-
tions of LIME, which cannot accept uncertainty in
the extractions used for training, is not specifically
optimized for learning rules for BLPs, and does not
scale well to large datasets. Given the relatively poor
performance of BLP parameters learned using EM,
tests on larger training corpora of extracted facts and
the development of improved parameter-learning al-
gorithms are clearly indicated. We also plan to per-
form a larger-scale evaluation by employing crowd-
sourcing to evaluate inferred facts for a bigger cor-
pus of test documents. As described above, a num-
ber of methods could be used to improve the per-
formance of MLNs on this task. Finally, it would
be useful to evaluate our methods on several other
diverse domains.
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99998375">
We have introduced a novel approach using
Bayesian Logic Programs to learn to infer implicit
information from facts extracted from natural lan-
guage text. We have demonstrated that it can learn
effective rules from a large database of noisy extrac-
tions. Our experimental evaluation on the IC data
set demonstrates the advantage of BLPs over logical
deduction and an approach based on MLNs.
</bodyText>
<sectionHeader confidence="0.996273" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.850844833333333">
We thank the SIRE team from IBM for providing SIRE
extractions on the IC data set. This research was funded
by MURI ARO grant W911NF-08-1-0242 and Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program. Experiments were run on the
Mastodon Cluster, provided by NSF grant EIA-0303609.
</bodyText>
<page confidence="0.998053">
356
</page>
<sectionHeader confidence="0.996331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849235849057">
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACl-HLT 2011), pages 610–619.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Conference on Artificial Intelligence
(AAAI), pages 1306–1313. AAAI Press.
Jim Cowie and Wendy Lehnert. 1996. Information ex-
traction. CACM, 39(1):80–91.
P. Domingos and D. Lowd. 2009. Markov Logic: An
Interface Layer for Artificial Intelligence. Morgan &amp;
Claypool, San Rafael, CA.
Janardhan Rao Doppa, Mohammad NasrEsfahani, Mo-
hammad S. Sorower, Thomas G. Dietterich, Xiaoli
Fern, and Prasad Tadepalli. 2010. Towards learn-
ing rules from natural texts. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing (FAM-LbR 2010), pages 70–77, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A statisti-
cal model for multilingual entity detection and track-
ing. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL-HLT 2004), pages 1–8.
L. Getoor and B. Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Vibhav Gogate and Rina Dechter. 2007. Samplesearch:
A scheme that searches for consistent samples. In Pro-
ceedings of Eleventh International Conference on Ar-
tificial Intelligence and Statistics (AISTATS-07).
K. Kersting and L. De Raedt. 2007. Bayesian Logic
Programming: Theory and tool. In L. Getoor and
B. Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press, Cambridge, MA.
Kristian Kersting and Luc De Raedt. 2008. Basic princi-
ples of learning Bayesian Logic Programs. Springer-
Verlag, Berlin, Heidelberg.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Nada Lavra˘c and Saso D˘zeroski. 1994. Inductive Logic
Programming: Techniques and Applications. Ellis
Horwood.
Deaking Lin and Patrick Pantel. 2001. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343–360.
Eric Mccreath and Arun Sharma. 1998. Lime: A system
for learning relations. In Ninth International Work-
shop on Algorithmic Learning Theory, pages 336–374.
Springer-Verlag.
Un Yong Nahm and Raymond J. Mooney. 2000. A mu-
tually beneficial integration of data mining and infor-
mation extraction. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence (AAAI
2000), pages 627–632, Austin, TX, July.
Siegfried Nijssen and Joost N. Kok. 2003. Efficient fre-
quent query discovery in FARMER. In Proceedings
of the Seventh Conference in Principles and Practices
of Knowledge Discovery in Database (PKDD 2003),
pages 350–362. Springer.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo,CA.
J. Ross Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239–266.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo,CA.
Stuart Russell and Peter Norvig. 2003. Artificial Intel-
ligence: A Modern Approach. Prentice Hall, Upper
Saddle River, NJ, 2 edition.
S. Sarawagi. 2008. Information extraction. Foundations
and Trends in Databases, 1(3):261–377.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2008),
pages 79–88, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order Horn
clauses from web text. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2010), pages 1088–1098, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Parag Singla and Raymond Mooney. 2011. Abductive
Markov Logic for plan recognition. In Twenty-fifth
National Conference on Artificial Intelligence.
Mohammad S. Sorower, Thomas G. Dietterich, Janard-
han Rao Doppa, Orr Walker, Prasad Tadepalli, and Xi-
aoli Fern. 2011. Inverting Grice’s maxims to learn
rules from natural language extractions. In Proceed-
ings of Advances in Neural Information Processing
Systems 24.
A. Srinivasan, 2001. The Aleph manual.
http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/.
Alexander Yates and Oren Etzioni. 2007. Unsupervised
resolution of objects and relations on the web. In Pro-
</reference>
<page confidence="0.977342">
357
</page>
<reference confidence="0.98061225">
ceedings of Human Language Technologies: The An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT 2007).
</reference>
<page confidence="0.997884">
358
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640024">
<title confidence="0.999943">Learning to “Read Between the Lines” using Bayesian Logic Programs</title>
<author confidence="0.99914">Sindhu Raghavan Raymond J Mooney Hyeonseo</author>
<affiliation confidence="0.994102">Department of Computer The University of Texas at</affiliation>
<address confidence="0.976978">1616 Guadalupe, Suite Austin, TX 78701,</address>
<abstract confidence="0.985857956521739">Most information extraction (IE) systems identify facts that are explicitly stated in text. However, in natural language, some facts are implicit, and identifying them requires “reading between the lines”. Human readers natuse common sense knowledge to inimplicit information from the explicitly stated facts. We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining firstorder logic and Bayesian networks, to infer additional implicit information from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</booktitle>
<pages>610--619</pages>
<contexts>
<context position="7998" citStr="Berant et al., 2011" startWordPosition="1230" endWordPosition="1233">tional facts using purely logical deduction. Sorower et al. (2011) propose a probabilistic approach to modeling implicit information as missing facts and use MLNs to infer these missing facts. They learn first-order rules for the MLN by performing exhaustive search. As mentioned earlier, inference using both these approaches, logical deduction and MLNs, have certain limitations, which BLPs help overcome. DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007) learn inference rules, also called entailment rules that capture synonymous relations and entities from text. Berant et al. (Berant et al., 2011) propose an approach that uses transitivity constraints for learning entailment rules for typed predicates. Unlike the systems described above, 350 these systems do not learn complex first-order rules that capture common sense knowledge. Further, most of these systems do not use extractions from an IE system to learn entailment rules, thereby making them less related to our approach. 3 Bayesian Logic Programs Bayesian logic programs (BLPs) (Kersting and De Raedt, 2007; Kersting and Raedt, 2008) can be considered as templates for constructing directed graphical models (Bayes nets). Formally, a </context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACl-HLT 2011), pages 610–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1306--1313</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4906" citStr="Carlson et al., 2010" startWordPosition="752" endWordPosition="755">uage text, which we then use to infer additional facts from text. The rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning to infer implicit facts. Section 5 describes our experimental methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorowe</context>
<context position="6381" citStr="Carlson et al., 2010" startWordPosition="977" endWordPosition="980">) to infer additional facts. Unlike BLPs, this approach does not use a wellfounded probabilistic graphical model to compute coherent probabilities for inferred facts. Further, Carlson et al. (2010) used a human judge to manually evaluate the quality of the learned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called SHERLOCK that uses statistical relevance to learn first-order rules. Unlike our system and others (Carlson et al., 2010; Doppa et al., 2010; Sorower et al., 2011) that use a pre-defined ontology, they automatically identify a set of entity types and relations using “open IE.” They use HOLMES (Schoenmackers et al., 2008), an inference engine based on MLNs (Domingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding Markov net, which, for larger datasets, can result in an intractably large graphical model. To overcome this problem, HOLMES uses a specialized</context>
<context position="18968" citStr="Carlson et al., 2010" startWordPosition="3043" endWordPosition="3046">raction of inferences that were deemed correct. For probabilistic methods like BLPs and MLNs that provide certainties for their inferences, we also computed precision at top n, which measures the precision of the n inferences with the highest marginal probability across the 40 test documents. Measuring recall for making inferences is very difficult since it would require labeling a reasonable-sized corpus of documents with all of the correct inferences for a given set of target relations, which would be extremely time consuming. Our evaluation is similar to that used in previous related work (Carlson et al., 2010; Schoenmackers et al., 2010). SIRE frequently makes incorrect extractions, and therefore inferences made from these extractions are also inaccurate. To account for the mistakes made by the extractor, we report two different precision scores. The “unadjusted” (UA) score, does not correct for errors made by the extractor. The “adjusted” (AD) score does not count mistakes due to extraction errors. That is, if an inference is incorrect because it was based on incorrect extracted facts, we remove it from the set of inferences and calculate precision for the remaining inferences. 5.4 Baselines Sinc</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hruschka Jr., and T.M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Proceedings of the Conference on Artificial Intelligence (AAAI), pages 1306–1313. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Cowie</author>
<author>Wendy Lehnert</author>
</authors>
<date>1996</date>
<journal>Information extraction. CACM,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="1383" citStr="Cowie and Lehnert, 1996" startWordPosition="202" endWordPosition="205">nal implicit information from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach. 1 Introduction The task of information extraction (IE) involves automatic extraction of typed entities and relations from unstructured text. IE systems (Cowie and Lehnert, 1996; Sarawagi, 2008) are trained to extract facts that are stated explicitly in text. However, some facts are implicit, and human readers naturally “read between the lines” and infer them from the stated facts using commonsense knowledge. Answering many queries can require inferring such implicitly stated facts. Consider the text “Barack Obama is the president of the United States of America.” Given the query “Barack Obama is a citizen of what country?”, standard IE systems cannot identify the answer since citizenship is not explicitly stated in the text. However, a human reader possesses the com</context>
</contexts>
<marker>Cowie, Lehnert, 1996</marker>
<rawString>Jim Cowie and Wendy Lehnert. 1996. Information extraction. CACM, 39(1):80–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
<author>D Lowd</author>
</authors>
<title>Markov Logic: An Interface Layer for Artificial Intelligence. Morgan &amp; Claypool,</title>
<date>2009</date>
<location>San Rafael, CA.</location>
<contexts>
<context position="4004" citStr="Domingos and Lowd, 2009" startWordPosition="606" endWordPosition="609">st and effective BLPs for “reading be349 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349–358, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tween the lines.” We present an experimental evaluation of our resulting system on a realistic test corpus from DARPA’s Machine Reading project, and demonstrate improved performance compared to a purely logical approach based on Inductive Logic Programming (ILP) (Lavra˘c and D˘zeroski, 1994), and an alternative SRL approach based on Markov Logic Networks (MLNs) (Domingos and Lowd, 2009). To the best of our knowledge, this is the first paper that employs BLPs for inferring implicit information from natural language text. We demonstrate that it is possible to learn the structure and the parameters of BLPs automatically using only noisy extractions from natural language text, which we then use to infer additional facts from text. The rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning</context>
<context position="6644" citStr="Domingos and Lowd, 2009" startWordPosition="1022" endWordPosition="1025">arned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called SHERLOCK that uses statistical relevance to learn first-order rules. Unlike our system and others (Carlson et al., 2010; Doppa et al., 2010; Sorower et al., 2011) that use a pre-defined ontology, they automatically identify a set of entity types and relations using “open IE.” They use HOLMES (Schoenmackers et al., 2008), an inference engine based on MLNs (Domingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding Markov net, which, for larger datasets, can result in an intractably large graphical model. To overcome this problem, HOLMES uses a specialized model construction process to control the grounding process. Unlike MLNs, BLPs naturally employ a more “focused” approach to grounding by including only those literals that are directly relevant to the query. Doppa et al. (2010) use FARMER (Nijssen and Kok, 2003</context>
<context position="20227" citStr="Domingos and Lowd, 2009" startWordPosition="3242" endWordPosition="3245">ave been evaluated on the IC data, we cannot directly compare our performance to theirs. Therefore, we compared to the following methods: • Logical Deduction: This method forward chains on the extracted facts using the firstorder rules learned by LIME to infer additional facts. This approach is unable to provide any confidence or probability for its conclusions. • Markov Logic Networks (MLNs): We use the rules learned by LIME to define the structure of an MLN. In the first setting, which we call MLN-Learned-Weights, we learn the MLN’s parameters using the generative weight learning algorithm (Domingos and Lowd, 2009), which we modified to process training examples in an online manner. In online generative learning, gradients are calculated and weights are estimated after processing each example and the learned weights are used as the starting weights for the next example. The pseudolikelihood of one round is obtained by multiplying the pseudo-likelihood of all examples. 353 UA AD Precision 29.73 (443/1490) 35.24 (443/1257) Table 2: Precision for logical deduction. “UA” and “AD” refer to the unadjusted and adjusted scores respectively In our approach, the initial weights of clauses are set to 10. The avera</context>
</contexts>
<marker>Domingos, Lowd, 2009</marker>
<rawString>P. Domingos and D. Lowd. 2009. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan &amp; Claypool, San Rafael, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janardhan Rao Doppa</author>
<author>Mohammad NasrEsfahani</author>
<author>Mohammad S Sorower</author>
<author>Thomas G Dietterich</author>
<author>Xiaoli Fern</author>
<author>Prasad Tadepalli</author>
</authors>
<title>Towards learning rules from natural texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010),</booktitle>
<pages>70--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4954" citStr="Doppa et al., 2010" startWordPosition="760" endWordPosition="763">acts from text. The rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning to infer implicit facts. Section 5 describes our experimental methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) learn first-order rules (i.e. Ho</context>
<context position="6401" citStr="Doppa et al., 2010" startWordPosition="981" endWordPosition="984">facts. Unlike BLPs, this approach does not use a wellfounded probabilistic graphical model to compute coherent probabilities for inferred facts. Further, Carlson et al. (2010) used a human judge to manually evaluate the quality of the learned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called SHERLOCK that uses statistical relevance to learn first-order rules. Unlike our system and others (Carlson et al., 2010; Doppa et al., 2010; Sorower et al., 2011) that use a pre-defined ontology, they automatically identify a set of entity types and relations using “open IE.” They use HOLMES (Schoenmackers et al., 2008), an inference engine based on MLNs (Domingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding Markov net, which, for larger datasets, can result in an intractably large graphical model. To overcome this problem, HOLMES uses a specialized model construction </context>
</contexts>
<marker>Doppa, NasrEsfahani, Sorower, Dietterich, Fern, Tadepalli, 2010</marker>
<rawString>Janardhan Rao Doppa, Mohammad NasrEsfahani, Mohammad S. Sorower, Thomas G. Dietterich, Xiaoli Fern, and Prasad Tadepalli. 2010. Towards learning rules from natural texts. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), pages 70–77, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaoqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10520" citStr="Florian et al., 2004" startWordPosition="1659" endWordPosition="1662">variables in the network and Pa(Xi) represents the parents of Xi. Once a ground network is constructed, standard probabilistic inference methods can be used to answer various types of queries as reviewed by Koller and Friedman (2009). The parameters in the BLP model can be learned using the methods described by Kersting and De Raedt (2008). 4 Learning BLPs to Infer Implicit Facts 4.1 Learning Rules from Extracted Data The first step involves learning commonsense knowledge in the form of first-order Horn rules from text. We first extract facts that are explicitly stated in the text using SIRE (Florian et al., 2004), an IE system developed by IBM. We then learn first-order rules from these extracted facts using LIME (Mccreath and Sharma, 1998), an ILP system designed for noisy training data. We first identify a set of target relations we want to infer. Typically, an ILP system takes a set of positive and negative instances for a target relation, along with a background knowledge base (in our case, other facts extracted from the same document) from which the positive instances are potentially inferable. In our task, we only have direct access to positive instances of target relations, i.e the relevant fac</context>
<context position="14716" citStr="Florian et al., 2004" startWordPosition="2346" endWordPosition="2350">stem uses Sample Search (Gogate and Dechter, 2007), an approximate sampling algorithm developed for Bayesian networks with deterministic constraints (0 values in CPTs). We tried several exact and approximate inference algorithms on our data, and this was the method that was both tractable and produced the best results. 5 Experimental Evaluation 5.1 Data For evaluation, we used DARPA’s machine-reading intelligence-community (IC) data set, which consists of news articles on terrorist events around the world. There are 10, 000 documents each containing an average of 89.5 facts extracted by SIRE (Florian et al., 2004). SIRE assigns each extracted fact a confidence score and we used only those with a score of 0.5 or higher for learning and inference. An average of 86.8 extractions per document meet this threshold. DARPA also provides an ontology describing the entities and relations in the IC domain. It consists of 57 entity types and 79 relations. The entity types include Agent, PhysicalThing, Event, TimeLocation, Gender, and Group, each with several subtypes. The type hierarchy is a DAG rather than a tree, and several types have multiple superclasses. For instance, a GeopoliticalEntity can be a HumanAgent</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2004), pages 1–8.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>L. Getoor and B. Taskar, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>2007</marker>
<rawString>L. Getoor and B. Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vibhav Gogate</author>
<author>Rina Dechter</author>
</authors>
<title>Samplesearch: A scheme that searches for consistent samples.</title>
<date>2007</date>
<booktitle>In Proceedings of Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS-07).</booktitle>
<contexts>
<context position="14145" citStr="Gogate and Dechter, 2007" startWordPosition="2258" endWordPosition="2261">l Facts using BLPs Inference in the BLP framework involves backward chaining (Russell and Norvig, 2003) from a specified query (SLD resolution) to obtain all possible deductive proofs for the query. In our context, each target relation becomes a query on which we backchain. We then construct a ground Bayesian network using the resulting deductive proofs for all target relations and learned parameters using the standard approach described in Section 3. Finally, we perform standard probabilistic inference to estimate the marginal probability of each inferred fact. Our system uses Sample Search (Gogate and Dechter, 2007), an approximate sampling algorithm developed for Bayesian networks with deterministic constraints (0 values in CPTs). We tried several exact and approximate inference algorithms on our data, and this was the method that was both tractable and produced the best results. 5 Experimental Evaluation 5.1 Data For evaluation, we used DARPA’s machine-reading intelligence-community (IC) data set, which consists of news articles on terrorist events around the world. There are 10, 000 documents each containing an average of 89.5 facts extracted by SIRE (Florian et al., 2004). SIRE assigns each extracted</context>
</contexts>
<marker>Gogate, Dechter, 2007</marker>
<rawString>Vibhav Gogate and Rina Dechter. 2007. Samplesearch: A scheme that searches for consistent samples. In Proceedings of Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS-07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kersting</author>
<author>L De Raedt</author>
</authors>
<title>Bayesian Logic Programming: Theory and tool.</title>
<date>2007</date>
<booktitle>Introduction to Statistical Relational Learning.</booktitle>
<editor>In L. Getoor and B. Taskar, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Kersting, De Raedt, 2007</marker>
<rawString>K. Kersting and L. De Raedt. 2007. Bayesian Logic Programming: Theory and tool. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Kersting</author>
<author>Luc De Raedt</author>
</authors>
<title>Basic principles of learning Bayesian Logic Programs.</title>
<date>2008</date>
<publisher>SpringerVerlag,</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Kersting, De Raedt, 2008</marker>
<rawString>Kristian Kersting and Luc De Raedt. 2008. Basic principles of learning Bayesian Logic Programs. SpringerVerlag, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10132" citStr="Koller and Friedman (2009)" startWordPosition="1594" endWordPosition="1597">s net for a given problem. More specifically, given a set of facts and a query, all possible Horn-clause proofs of the query are constructed and used to build a Bayes net for answering the query. The probability of a joint assignment of truth values to the final set of ground propositions is defined as follows: P(X) = Ili P(Xi|Pa(Xi)), where X = X1, X2, ..., Xn represents the set of random variables in the network and Pa(Xi) represents the parents of Xi. Once a ground network is constructed, standard probabilistic inference methods can be used to answer various types of queries as reviewed by Koller and Friedman (2009). The parameters in the BLP model can be learned using the methods described by Kersting and De Raedt (2008). 4 Learning BLPs to Infer Implicit Facts 4.1 Learning Rules from Extracted Data The first step involves learning commonsense knowledge in the form of first-order Horn rules from text. We first extract facts that are explicitly stated in the text using SIRE (Florian et al., 2004), an IE system developed by IBM. We then learn first-order rules from these extracted facts using LIME (Mccreath and Sharma, 1998), an ILP system designed for noisy training data. We first identify a set of targe</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nada Lavra˘c</author>
<author>Saso D˘zeroski</author>
</authors>
<title>Inductive Logic Programming: Techniques and Applications. Ellis Horwood.</title>
<date>1994</date>
<marker>Lavra˘c, D˘zeroski, 1994</marker>
<rawString>Nada Lavra˘c and Saso D˘zeroski. 1994. Inductive Logic Programming: Techniques and Applications. Ellis Horwood.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deaking Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="7813" citStr="Lin and Pantel, 2001" startWordPosition="1201" endWordPosition="1204">oppa et al. (2010) use FARMER (Nijssen and Kok, 2003), an existing ILP system, to learn firstorder rules. They propose several approaches to score the rules, which are used to infer additional facts using purely logical deduction. Sorower et al. (2011) propose a probabilistic approach to modeling implicit information as missing facts and use MLNs to infer these missing facts. They learn first-order rules for the MLN by performing exhaustive search. As mentioned earlier, inference using both these approaches, logical deduction and MLNs, have certain limitations, which BLPs help overcome. DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007) learn inference rules, also called entailment rules that capture synonymous relations and entities from text. Berant et al. (Berant et al., 2011) propose an approach that uses transitivity constraints for learning entailment rules for typed predicates. Unlike the systems described above, 350 these systems do not learn complex first-order rules that capture common sense knowledge. Further, most of these systems do not use extractions from an IE system to learn entailment rules, thereby making them less related to our approach. 3 Bayesian Logic Programs Ba</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Deaking Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mccreath</author>
<author>Arun Sharma</author>
</authors>
<title>Lime: A system for learning relations.</title>
<date>1998</date>
<booktitle>In Ninth International Workshop on Algorithmic Learning Theory,</booktitle>
<pages>336--374</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10650" citStr="Mccreath and Sharma, 1998" startWordPosition="1680" endWordPosition="1684">c inference methods can be used to answer various types of queries as reviewed by Koller and Friedman (2009). The parameters in the BLP model can be learned using the methods described by Kersting and De Raedt (2008). 4 Learning BLPs to Infer Implicit Facts 4.1 Learning Rules from Extracted Data The first step involves learning commonsense knowledge in the form of first-order Horn rules from text. We first extract facts that are explicitly stated in the text using SIRE (Florian et al., 2004), an IE system developed by IBM. We then learn first-order rules from these extracted facts using LIME (Mccreath and Sharma, 1998), an ILP system designed for noisy training data. We first identify a set of target relations we want to infer. Typically, an ILP system takes a set of positive and negative instances for a target relation, along with a background knowledge base (in our case, other facts extracted from the same document) from which the positive instances are potentially inferable. In our task, we only have direct access to positive instances of target relations, i.e the relevant facts extracted from the text. So we artificially generate negative instances using the closed world assumption, which states that an</context>
</contexts>
<marker>Mccreath, Sharma, 1998</marker>
<rawString>Eric Mccreath and Arun Sharma. 1998. Lime: A system for learning relations. In Ninth International Workshop on Algorithmic Learning Theory, pages 336–374. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Un Yong Nahm</author>
<author>Raymond J Mooney</author>
</authors>
<title>A mutually beneficial integration of data mining and information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI</booktitle>
<pages>627--632</pages>
<location>Austin, TX,</location>
<contexts>
<context position="2580" citStr="Nahm and Mooney, 2000" startWordPosition="392" endWordPosition="395"> reader possesses the commonsense knowledge that the president of a country is almost always a citizen of that country, and easily infers the correct answer. The standard approach to inferring implicit information involves using commonsense knowledge in the form of logical rules to deduce additional information from the extracted facts. Since manually developing such a knowledge base is difficult and arduous, an effective alternative is to automatically learn such rules by mining a substantial database of facts that an IE system has already automatically extracted from a large corpus of text (Nahm and Mooney, 2000). Most existing rule learners assume that the training data is largely accurate and complete. However, the facts extracted by an IE system are always quite noisy and incomplete. Consequently, a purely logical approach to learning and inference is unlikely to be effective. Consequently, we propose using statistical relational learning (SRL) (Getoor and Taskar, 2007), specifically, Bayesian Logic Programs (BLPs) (Kersting and De Raedt, 2007), to learn probabilistic rules in first-order logic from a large corpus of extracted facts and then use the resulting BLP to make effective probabilistic inf</context>
<context position="4884" citStr="Nahm and Mooney, 2000" startWordPosition="748" endWordPosition="751">tions from natural language text, which we then use to infer additional facts from text. The rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning to infer implicit facts. Section 5 describes our experimental methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Dopp</context>
</contexts>
<marker>Nahm, Mooney, 2000</marker>
<rawString>Un Yong Nahm and Raymond J. Mooney. 2000. A mutually beneficial integration of data mining and information extraction. In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI 2000), pages 627–632, Austin, TX, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siegfried Nijssen</author>
<author>Joost N Kok</author>
</authors>
<title>Efficient frequent query discovery in FARMER.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference in Principles and Practices of Knowledge Discovery in Database (PKDD</booktitle>
<pages>350--362</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7245" citStr="Nijssen and Kok, 2003" startWordPosition="1114" endWordPosition="1117">ingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding Markov net, which, for larger datasets, can result in an intractably large graphical model. To overcome this problem, HOLMES uses a specialized model construction process to control the grounding process. Unlike MLNs, BLPs naturally employ a more “focused” approach to grounding by including only those literals that are directly relevant to the query. Doppa et al. (2010) use FARMER (Nijssen and Kok, 2003), an existing ILP system, to learn firstorder rules. They propose several approaches to score the rules, which are used to infer additional facts using purely logical deduction. Sorower et al. (2011) propose a probabilistic approach to modeling implicit information as missing facts and use MLNs to infer these missing facts. They learn first-order rules for the MLN by performing exhaustive search. As mentioned earlier, inference using both these approaches, logical deduction and MLNs, have certain limitations, which BLPs help overcome. DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni</context>
</contexts>
<marker>Nijssen, Kok, 2003</marker>
<rawString>Siegfried Nijssen and Joost N. Kok. 2003. Efficient frequent query discovery in FARMER. In Proceedings of the Seventh Conference in Principles and Practices of Knowledge Discovery in Database (PKDD 2003), pages 350–362. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo,CA.</location>
<contexts>
<context position="12879" citStr="Pearl, 1988" startWordPosition="2056" endWordPosition="2057"> candidate rules. We initially tried using the popular ALEPH ILP system (Srinivasan, 2001), but it did not produce useful rules, probably due to the high level of noise in our training data. 351 4.2 Learning BLP Parameters The parameters of a BLP include the CPT entries associated with the Bayesian clauses and the parameters of combining rules associated with the Bayesian predicates. For simplicity, we use a deterministic logical-and model to encode the CPT entries associated with Bayesian clauses, and use noisy-or to combine evidence coming from multiple ground rules that have the same head (Pearl, 1988). The noisyor model requires just a single parameter for each rule, which can be learned from training data. We learn the noisy-or parameters using the EM algorithm adapted for BLPs by Kersting and De Raedt (2008). In our task, the supervised training data consists of facts that are extracted from the natural language text. However, we usually do not have evidence for inferred facts as well as noisy-or nodes. As a result, there are a number of variables in the ground networks which are always hidden, and hence EM is appropriate for learning the requisite parameters from the partially observed </context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo,CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>Learning logical definitions from relations.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="5662" citStr="Quinlan, 1990" startWordPosition="870" endWordPosition="871"> by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) learn first-order rules (i.e. Horn clauses in first-order logic). Carlson et al. (2010) modify an ILP system similar to FOIL (Quinlan, 1990) to learn rules with probabilistic conclusions. They use purely logical deduction (forward-chaining) to infer additional facts. Unlike BLPs, this approach does not use a wellfounded probabilistic graphical model to compute coherent probabilities for inferred facts. Further, Carlson et al. (2010) used a human judge to manually evaluate the quality of the learned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called </context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J. Ross Quinlan. 1990. Learning logical definitions from relations. Machine Learning, 5(3):239–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo,CA.</location>
<contexts>
<context position="5281" citStr="Quinlan, 1993" startWordPosition="814" endWordPosition="815">methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) learn first-order rules (i.e. Horn clauses in first-order logic). Carlson et al. (2010) modify an ILP system similar to FOIL (Quinlan, 1990) to learn rules with probabilistic conclusions. They use purely logical deduction (forward-chaining) to infer additional facts. Unlike BLPs, this approach does not use a wellfounded probabilistic graphical model to comp</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo,CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach. Prentice Hall, Upper Saddle River,</title>
<date>2003</date>
<journal>NJ,</journal>
<volume>2</volume>
<pages>edition.</pages>
<contexts>
<context position="13623" citStr="Russell and Norvig, 2003" startWordPosition="2176" endWordPosition="2179">he noisy-or parameters using the EM algorithm adapted for BLPs by Kersting and De Raedt (2008). In our task, the supervised training data consists of facts that are extracted from the natural language text. However, we usually do not have evidence for inferred facts as well as noisy-or nodes. As a result, there are a number of variables in the ground networks which are always hidden, and hence EM is appropriate for learning the requisite parameters from the partially observed training data. 4.3 Inference of Additional Facts using BLPs Inference in the BLP framework involves backward chaining (Russell and Norvig, 2003) from a specified query (SLD resolution) to obtain all possible deductive proofs for the query. In our context, each target relation becomes a query on which we backchain. We then construct a ground Bayesian network using the resulting deductive proofs for all target relations and learned parameters using the standard approach described in Section 3. Finally, we perform standard probabilistic inference to estimate the marginal probability of each inferred fact. Our system uses Sample Search (Gogate and Dechter, 2007), an approximate sampling algorithm developed for Bayesian networks with deter</context>
</contexts>
<marker>Russell, Norvig, 2003</marker>
<rawString>Stuart Russell and Peter Norvig. 2003. Artificial Intelligence: A Modern Approach. Prentice Hall, Upper Saddle River, NJ, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
</authors>
<date>2008</date>
<booktitle>Information extraction. Foundations and Trends in Databases,</booktitle>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="1400" citStr="Sarawagi, 2008" startWordPosition="206" endWordPosition="207">from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach. 1 Introduction The task of information extraction (IE) involves automatic extraction of typed entities and relations from unstructured text. IE systems (Cowie and Lehnert, 1996; Sarawagi, 2008) are trained to extract facts that are stated explicitly in text. However, some facts are implicit, and human readers naturally “read between the lines” and infer them from the stated facts using commonsense knowledge. Answering many queries can require inferring such implicitly stated facts. Consider the text “Barack Obama is the president of the United States of America.” Given the query “Barack Obama is a citizen of what country?”, standard IE systems cannot identify the answer since citizenship is not explicitly stated in the text. However, a human reader possesses the commonsense knowledg</context>
</contexts>
<marker>Sarawagi, 2008</marker>
<rawString>S. Sarawagi. 2008. Information extraction. Foundations and Trends in Databases, 1(3):261–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling textual inference to the web.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008),</booktitle>
<pages>79--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6583" citStr="Schoenmackers et al., 2008" startWordPosition="1011" endWordPosition="1015">0) used a human judge to manually evaluate the quality of the learned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called SHERLOCK that uses statistical relevance to learn first-order rules. Unlike our system and others (Carlson et al., 2010; Doppa et al., 2010; Sorower et al., 2011) that use a pre-defined ontology, they automatically identify a set of entity types and relations using “open IE.” They use HOLMES (Schoenmackers et al., 2008), an inference engine based on MLNs (Domingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding Markov net, which, for larger datasets, can result in an intractably large graphical model. To overcome this problem, HOLMES uses a specialized model construction process to control the grounding process. Unlike MLNs, BLPs naturally employ a more “focused” approach to grounding by including only those literals that are directly relevant to the</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld. 2008. Scaling textual inference to the web. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 79–88, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Jesse Davis</author>
</authors>
<title>Learning first-order Horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>1088--1098</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4934" citStr="Schoenmackers et al., 2010" startWordPosition="756" endWordPosition="759">en use to infer additional facts from text. The rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning to infer implicit facts. Section 5 describes our experimental methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) learn first-</context>
<context position="6237" citStr="Schoenmackers et al. (2010)" startWordPosition="955" endWordPosition="958">modify an ILP system similar to FOIL (Quinlan, 1990) to learn rules with probabilistic conclusions. They use purely logical deduction (forward-chaining) to infer additional facts. Unlike BLPs, this approach does not use a wellfounded probabilistic graphical model to compute coherent probabilities for inferred facts. Further, Carlson et al. (2010) used a human judge to manually evaluate the quality of the learned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called SHERLOCK that uses statistical relevance to learn first-order rules. Unlike our system and others (Carlson et al., 2010; Doppa et al., 2010; Sorower et al., 2011) that use a pre-defined ontology, they automatically identify a set of entity types and relations using “open IE.” They use HOLMES (Schoenmackers et al., 2008), an inference engine based on MLNs (Domingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding</context>
<context position="18997" citStr="Schoenmackers et al., 2010" startWordPosition="3047" endWordPosition="3050">that were deemed correct. For probabilistic methods like BLPs and MLNs that provide certainties for their inferences, we also computed precision at top n, which measures the precision of the n inferences with the highest marginal probability across the 40 test documents. Measuring recall for making inferences is very difficult since it would require labeling a reasonable-sized corpus of documents with all of the correct inferences for a given set of target relations, which would be extremely time consuming. Our evaluation is similar to that used in previous related work (Carlson et al., 2010; Schoenmackers et al., 2010). SIRE frequently makes incorrect extractions, and therefore inferences made from these extractions are also inaccurate. To account for the mistakes made by the extractor, we report two different precision scores. The “unadjusted” (UA) score, does not correct for errors made by the extractor. The “adjusted” (AD) score does not count mistakes due to extraction errors. That is, if an inference is incorrect because it was based on incorrect extracted facts, we remove it from the set of inferences and calculate precision for the remaining inferences. 5.4 Baselines Since none of the existing approa</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, Davis, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and Jesse Davis. 2010. Learning first-order Horn clauses from web text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 1088–1098, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Raymond Mooney</author>
</authors>
<title>Abductive Markov Logic for plan recognition.</title>
<date>2011</date>
<booktitle>In Twenty-fifth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="30074" citStr="Singla and Mooney (2011)" startWordPosition="4773" endWordPosition="4776">ional clauses in the MLN that impose integrity constraints that prevent such nonsensical propositions. Further, techniques proposed by Sorower et al. (2011) can be incorporated to explicitly handle missing information in text. Lack of strict typing on the arguments of relations in the IC ontology has also resulted in inferior performance of the MLNs. To overcome this, relations that do not have strictly defined types could be specialized. Finally, we could use the deductive proofs constructed by BLPs to constrain the ground Markov network, similar to the model-construction approach adopted by Singla and Mooney (2011). However, in contrast to MLNs, BLPs that use first-order rules that are learned by an off-the-shelf ILP system and given simple intuitive hand-coded weights, are able to provide fairly high-precision inferences that augment the output of an IE system and allow it to effectively “read between the lines.” 7 Future Work A primary goal for future research is developing an on-line structure learner for BLPs that can directly learn probabilistic first-order rules from uncertain training data. This will address important limitations of LIME, which cannot accept uncertainty in the extractions used fo</context>
</contexts>
<marker>Singla, Mooney, 2011</marker>
<rawString>Parag Singla and Raymond Mooney. 2011. Abductive Markov Logic for plan recognition. In Twenty-fifth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad S Sorower</author>
<author>Thomas G Dietterich</author>
<author>Janardhan Rao Doppa</author>
<author>Orr Walker</author>
<author>Prasad Tadepalli</author>
<author>Xiaoli Fern</author>
</authors>
<title>Inverting Grice’s maxims to learn rules from natural language extractions.</title>
<date>2011</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 24.</booktitle>
<contexts>
<context position="4977" citStr="Sorower et al., 2011" startWordPosition="764" endWordPosition="767">rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning to infer implicit facts. Section 5 describes our experimental methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) learn first-order rules (i.e. Horn clauses in first-ord</context>
<context position="6424" citStr="Sorower et al., 2011" startWordPosition="985" endWordPosition="988">this approach does not use a wellfounded probabilistic graphical model to compute coherent probabilities for inferred facts. Further, Carlson et al. (2010) used a human judge to manually evaluate the quality of the learned rules before using them to infer additional facts. Our approach, on the other hand, is completely automated and learns fully parameterized rules in a well-defined probabilistic logic. Schoenmackers et al. (2010) develop a system called SHERLOCK that uses statistical relevance to learn first-order rules. Unlike our system and others (Carlson et al., 2010; Doppa et al., 2010; Sorower et al., 2011) that use a pre-defined ontology, they automatically identify a set of entity types and relations using “open IE.” They use HOLMES (Schoenmackers et al., 2008), an inference engine based on MLNs (Domingos and Lowd, 2009) (an SRL approach that combines first-order logic and Markov networks) to infer additional facts. However, MLNs include all possible type-consistent groundings of the rules in the corresponding Markov net, which, for larger datasets, can result in an intractably large graphical model. To overcome this problem, HOLMES uses a specialized model construction process to control the </context>
<context position="29606" citStr="Sorower et al. (2011)" startWordPosition="4696" endWordPosition="4699">0/0) nil (0/0) nil (0/0) 800 hasCitizenship 48.05 (37/77) 58.33 (35/60) nil (0/0) 222 attendedSchool nil (0/0) nil (0/0) nil (0/0) 2 Table 3: Adjusted precision for individual relations (highest values are in bold) weight learning, additional engineering could potentially improve the performance of MLNs on the IC data set. Due to MLN’s grounding process, several spurious facts like employs(a,a) were inferred. These inferences can be prevented by including additional clauses in the MLN that impose integrity constraints that prevent such nonsensical propositions. Further, techniques proposed by Sorower et al. (2011) can be incorporated to explicitly handle missing information in text. Lack of strict typing on the arguments of relations in the IC ontology has also resulted in inferior performance of the MLNs. To overcome this, relations that do not have strictly defined types could be specialized. Finally, we could use the deductive proofs constructed by BLPs to constrain the ground Markov network, similar to the model-construction approach adopted by Singla and Mooney (2011). However, in contrast to MLNs, BLPs that use first-order rules that are learned by an off-the-shelf ILP system and given simple int</context>
</contexts>
<marker>Sorower, Dietterich, Doppa, Walker, Tadepalli, Fern, 2011</marker>
<rawString>Mohammad S. Sorower, Thomas G. Dietterich, Janardhan Rao Doppa, Orr Walker, Prasad Tadepalli, and Xiaoli Fern. 2011. Inverting Grice’s maxims to learn rules from natural language extractions. In Proceedings of Advances in Neural Information Processing Systems 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Srinivasan</author>
</authors>
<title>The Aleph manual.</title>
<date>2001</date>
<note>http://web.comlab.ox.ac.uk/oucl/ research/areas/machlearn/Aleph/.</note>
<contexts>
<context position="12357" citStr="Srinivasan, 2001" startWordPosition="1968" endWordPosition="1969"> sample a fixed-size subset. The ratio of 1:20 for positive to negative instances worked well in our approach. Since LIME can learn rules using only positive instances, or both positive and negative instances, we learn rules using both settings. We include all unique rules learned from both settings in the final set, since the goal of this step is to learn a large set of potentially useful rules whose relative strengths will be determined in the next step of parameter learning. Other approaches could also be used to learn candidate rules. We initially tried using the popular ALEPH ILP system (Srinivasan, 2001), but it did not produce useful rules, probably due to the high level of noise in our training data. 351 4.2 Learning BLP Parameters The parameters of a BLP include the CPT entries associated with the Bayesian clauses and the parameters of combining rules associated with the Bayesian predicates. For simplicity, we use a deterministic logical-and model to encode the CPT entries associated with Bayesian clauses, and use noisy-or to combine evidence coming from multiple ground rules that have the same head (Pearl, 1988). The noisyor model requires just a single parameter for each rule, which can </context>
</contexts>
<marker>Srinivasan, 2001</marker>
<rawString>A. Srinivasan, 2001. The Aleph manual. http://web.comlab.ox.ac.uk/oucl/ research/areas/machlearn/Aleph/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised resolution of objects and relations on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACLHLT</booktitle>
<contexts>
<context position="7852" citStr="Yates and Etzioni, 2007" startWordPosition="1207" endWordPosition="1210">en and Kok, 2003), an existing ILP system, to learn firstorder rules. They propose several approaches to score the rules, which are used to infer additional facts using purely logical deduction. Sorower et al. (2011) propose a probabilistic approach to modeling implicit information as missing facts and use MLNs to infer these missing facts. They learn first-order rules for the MLN by performing exhaustive search. As mentioned earlier, inference using both these approaches, logical deduction and MLNs, have certain limitations, which BLPs help overcome. DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007) learn inference rules, also called entailment rules that capture synonymous relations and entities from text. Berant et al. (Berant et al., 2011) propose an approach that uses transitivity constraints for learning entailment rules for typed predicates. Unlike the systems described above, 350 these systems do not learn complex first-order rules that capture common sense knowledge. Further, most of these systems do not use extractions from an IE system to learn entailment rules, thereby making them less related to our approach. 3 Bayesian Logic Programs Bayesian logic programs (BLPs) (Kersting </context>
</contexts>
<marker>Yates, Etzioni, 2007</marker>
<rawString>Alexander Yates and Oren Etzioni. 2007. Unsupervised resolution of objects and relations on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACLHLT 2007).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>