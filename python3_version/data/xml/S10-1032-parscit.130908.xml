<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.156994">
<title confidence="0.9948775">
Task 5: Single document keyphrase extraction using sentence clustering
and Latent Dirichlet Allocation
</title>
<author confidence="0.992111">
Claude Pasquier
</author>
<affiliation confidence="0.9870865">
Institute of Developmental Biology &amp; Cancer
University of Nice Sophia-Antipolis
</affiliation>
<address confidence="0.637554666666667">
UNSA/CNRS UMR-6543
Parc Valrose
06108 NICE Cedex 2, France
</address>
<email confidence="0.996305">
claude.pasquier@unice.fr
</email>
<sectionHeader confidence="0.993791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985090909091">
This paper describes the design of a sys-
tem for extracting keyphrases from a sin-
gle document The principle of the algo-
rithm is to cluster sentences of the doc-
uments in order to highlight parts of text
that are semantically related. The clusters
of sentences, that reflect the themes of the
document, are then analyzed to find the
main topics of the text. Finally, the most
important words, or groups of words, from
these topics are proposed as keyphrases.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929611111111">
Keyphrases are words, or groups of words, that
capture the key ideas of a document. They repre-
sent important information concerning a document
and constitute an alternative, or a complement, to
full-text indexing. Pertinent keyphrases are also
useful to potential readers who can have a quick
overview of the content of a document and can se-
lect easily which document to read.
Currently, the most powerful keyphrases extrac-
tion algorithms are based on supervised learning.
These methods address the problem of associat-
ing keyphrases to documents as a classification
task. However, the fact that this approach requires
a corpus of similar documents, which is not al-
ways readily available, constitutes a major draw-
back. For example, if one encounters a new Web
page, one might like to know quickly the main top-
ics addressed. In this case, a domain-independent
keyword extraction system that applies to a single
document is needed.
Several methods have been proposed for ex-
tracting keywords from a single document (Mat-
suo and Ishizuka, 2004; Palshikar, 2007). The re-
ported performances were slightly higher than that
obtained using a corpus and selecting the words
with the highest TF-IDF 1 measure (Salton et al.,
1975).
The paper describes a new keyphrase extraction
algorithm from a single document. We show that
our system performs well without the need for a
corpus.
The paper is organized as follows. The next sec-
tion describes the principles of our keyphrase ex-
traction system. We present the main parts of the
algorithm in section 3, we detail the methods in
section 4 and we conclude the paper.
</bodyText>
<sectionHeader confidence="0.977183" genericHeader="introduction">
2 Principles
</sectionHeader>
<bodyText confidence="0.999970625">
When authors write documents, they have to think
first at the way they will present their ideas. Most
of the time, they establish content summaries that
highlight the main topics of their texts. Then, they
write the content of the documents by carefully
selecting the most appropriate words to describe
each topic. In this paper, we make the assumption
that the words, or the set of words, that are repre-
sentative of each topic constitute the keyphrases of
the document. In the following of this paper, we
call terms, the components of a document that con-
stitute the vocabulary (see the detail of the identi-
fication of terms in subsection 4.3).
In statistical natural language processing, one
common way of modeling the contributions of dif-
ferent topics to a document is to treat each topic as
a probability distribution over words. Therefore, a
document is considered as a probabilistic mixture
of these topics (Griffiths and Steyvers, 2004).
Generative models can be used to relate a set of
observations (in our case, the terms used in a doc-
ument) to a set of latent variables (the topics). A
particular generative model, which is well suited
for the modeling of text, is called Latent Dirichlet
</bodyText>
<footnote confidence="0.98494975">
1The TF-IDF weight gives the degree of importance of a
word in a collection of documents. The importance increases
if the word is frequently used in the set of documents but
decreases if it is used by too many documents.
</footnote>
<page confidence="0.99281">
154
</page>
<bodyText confidence="0.935214230769231">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 154–157,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Allocation (LDA) (Blei et al., 2003). Given a set
of documents, the algorithms describes each doc-
ument as a mixture over topics, where each topic
is characterized by a distribution over words.
The idea is to perform first a clustering of the
sentences of the document based on their semantic
similarity. Intuitively, one can see each cluster as
a part of the text dealing with semantically related
content. Therefore, the initial document is divided
into a set of clusters and LDA can then be applied
on this new representation.
</bodyText>
<sectionHeader confidence="0.995182" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.987352">
The algorithm is composed of 8 steps:
</bodyText>
<listItem confidence="0.997944230769231">
1. Identification and expansion of abbrevia-
tions.
2. Splitting the content of the document into m
sentences.
3. Identification of the n unique terms in the
document that are potential keyphrases.
4. Creation of a m x n sentence-term matrix
X to identify the occurrences of the n terms
within a collection of m sentences.
5. Dimensionality reduction to transform data in
the high-dimensional matrix X to a space of
fewer dimensions.
6. Data clustering performed in the reduced
</listItem>
<bodyText confidence="0.9418012">
space. The result of the clustering is used to
build a new representation of the source doc-
ument, which is now considered as a set of
clusters, with each cluster consisting of a bag
of terms.
</bodyText>
<listItem confidence="0.9554605">
7. Execution of LDA on the new document rep-
resentation.
8. Selection of best keyphrases by analyzing
LDA’s results.
</listItem>
<sectionHeader confidence="0.995011" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.9998885">
Our implementation is build on UIMA (Un-
structured Information Management Architecture)
(http://incubator.apache.org/uima/), a robust and
flexible framework that facilitates interoperability
between tools dedicated to unstructured informa-
tion processsing. The method processes one doc-
ument at a time by performing the steps described
below.
</bodyText>
<subsectionHeader confidence="0.99842">
4.1 Abbreviation Expansion
</subsectionHeader>
<bodyText confidence="0.999948333333333">
The program ExtractAbbrev (Schwartz and Hearst,
2003) is used to identify abbreviations (short
forms) and their corresponding definitions (long
forms). Once abbreviations have been identified,
each short form is replaced by its corresponding
long form in the processed document.
</bodyText>
<subsectionHeader confidence="0.997757">
4.2 Sentence Detection
</subsectionHeader>
<bodyText confidence="0.9999532">
Splitting the content of a document into sentences
is an important step of the method. To per-
form this task, we used the OpenNLP’s sentence
detector module (http://opennlp.sourceforge.net/)
trained on a corpus of general English texts.
</bodyText>
<subsectionHeader confidence="0.974586">
4.3 Term Identification
</subsectionHeader>
<bodyText confidence="0.9998635">
Word categories are identified by using the Ling-
Pipe’s general English part-of-speech (POS) tag-
ger trained on the Brown Corpus (http://alias-
i.com/lingpipe/). We leverage POS information to
collect, for each sentence, nominal groups that are
potential keyphrases.
</bodyText>
<subsectionHeader confidence="0.989129">
4.4 Matrix Creation
</subsectionHeader>
<bodyText confidence="0.999974809523809">
Let D = {d1, d2, ... , dnj be the complete vo-
cabulary set of the document identified in subsec-
tion 4.3 above. We build a mxn matrix X = [xij]
where m is the number of sentences in the doc-
ument, n is the number of terms and xij is the
weight of the jth term in the ith sentence. The
weight of a term in a sentence is the product of a
local and global weight given by xij = lij x gj,
where lij is the local weight of term j within sen-
tence i, and gj is the global weight of term j in
the document. The local weighting function mea-
sures the importance of a term within a sentence
and the global weighting function measures the
importance of a term across the entire document.
Three local weighting functions were investigated:
term frequency, log of term frequency and binary.
Five global weighting functions were also inves-
tigated: Normal, GFIDF (Global frequency x In-
verse document frequency, IDF (Inverse document
frequency), Entropy and none (details of calcula-
tion can be found in Dumais (1991) paper).
</bodyText>
<subsectionHeader confidence="0.975122">
4.5 Dimensionality Reduction
</subsectionHeader>
<bodyText confidence="0.9986315">
The matrix X is a representation of a document in
a high-dimensional space. Singular Value Decom-
position (SVD) (Forsythe et al., 1977) and Non-
Negative Matrix Factorization (NMF) (Lee and
</bodyText>
<page confidence="0.996076">
155
</page>
<bodyText confidence="0.994370571428571">
Seung, 1999) are two matrix decomposition tech-
niques that can be used to transform data in the
high-dimensional space to a space of fewer dimen-
sions.
With SVD, the original matrix X is decom-
posed as a factor of three other matrices U, E and
V such as:
</bodyText>
<equation confidence="0.966102">
X = UEVT
</equation>
<bodyText confidence="0.999787263157895">
where U is an mxm matrix, E is a mxn diagonal
matrix with nonnegative real numbers on the diag-
onal, and V T denotes the transpose of V , an n x n
matrix. It is often useful to approximate X using
only r singular values (with r &lt; min(m, n)), so
that we have X = UrEkVrT + E, where E is an
error or residual matrix, Ur is an m x r matrix,
Er is a k x r diagonal matrix, and Vr is an n x r
matrix.
NMF is a matrix factorization algorithm that
decomposes a matrix with only positive elements
into two positive elements matrices, with X =
WH+E. Usually, only r components are fit, so E
is an error or residual matrix, W is a non-negative
m x r matrix and H is a non-negative r x n ma-
trix. There are several ways in which W and H
may be found. In our system, we use Lee and Se-
ung’s multiplicative update method (Lee and Se-
ung, 1999).
</bodyText>
<subsectionHeader confidence="0.996122">
4.6 Sentence Clustering
</subsectionHeader>
<bodyText confidence="0.999935833333333">
The clustering of sentences is performed in the
reduced space by using the cosine similarity be-
tween sentence vectors. Several clustering tech-
niques have been investigated: k-means cluster-
ing, Markov Cluster Process (MCL) (Dongen,
2008) and ClassDens (Gu´enoche, 2004).
The latent semantic space derived by SVD does
not provide a direct indication of the data par-
titions. However, with NMF, the cluster mem-
bership of each document can be easily identi-
fied directly using the W matrix (Xu et al., 2003).
Each value wij of matrix W, indicates, indeed, to
which degree sentence i is associated with clus-
ter j. If NMF was calculated with the rank r,
then r clusters are represented on the matrix. We
use a simple rule to determine the content of each
cluster: sentence i belongs to cluster j if wij &gt;
wik. In our system, we fixed a = 0.1.
</bodyText>
<subsectionHeader confidence="0.993131">
4.7 Applying Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.999937923076923">
By using the result of the clustering, the source
document is now represented by c clusters of
terms. The terms associated with a clus-
ter ci is the sum of the terms belonging to
all the sentences in the cluster. JGibbLDA
(http://jgibblda.sourceforge.net/) is used to exe-
cute LDA on the new dataset. We tried to ex-
tract different numbers of topics t (with t E
12, 5, 10, 20, 50,100}) and we choose the Dirich-
let hyperparameters such as α = 0.1 and Q =
50/t. LDA inferences a topic model by estimating
the cluster-topic distribution O and the topic-word
distribution Φ (Blei et al., 2003).
</bodyText>
<subsectionHeader confidence="0.999659">
4.8 Term Ranking and Keyphrase Selection
</subsectionHeader>
<bodyText confidence="0.999960625">
We assume that topics covering a significant por-
tion of the document content are more important
than those covering little content. To reflect this
assumption, we calculate the importance of a term
in the document (its score) with a function that
takes into account the distribution of topics over
clusters given by θ, the distribution of terms over
topics given by Φ and the clusters’ size.
</bodyText>
<equation confidence="0.988229">
score(i) = max (Φji
jE{1...n}
</equation>
<bodyText confidence="0.999564">
where score(i) represents the score of term i and
s(k) is the size of the cluster k. We tested three
different functions for p: the constant function
p(i) = 1, the linear function p(i) = i and the
exponential function p(i) = i2.
When a score is attributed to each term of
the vocabulary, our system simply selects the top
terms with the highest score and proposes them as
keyphrases.
</bodyText>
<subsectionHeader confidence="0.999681">
4.9 Setting Tuning Parameters
</subsectionHeader>
<bodyText confidence="0.9999356875">
Numerous parameters have influence on the
method: the weighting of the terms in the doc-
ument matrix, the dimension reduction method
used, the number of dimension retained, the clus-
tering algorithm, the number of topics used to ex-
ecute LDA and the way best keyphrases are se-
lected.
The parameter that most affects the perfor-
mance is the method used to perform the dimen-
sion reduction. In all cases, whatever the other
parameters, NMF performs better than SVD. We
found that using only 10 components for the fac-
torization is sufficient. There was no significant
performance increase by using more factors.
The second most important parameter is the
clustering method used. When NMF is used, the
</bodyText>
<figure confidence="0.9698418">
a max
kE{1...m}
c
E (Okjp(s(k)))
k=1
</figure>
<page confidence="0.995829">
156
</page>
<bodyText confidence="0.999974259259259">
best results were achieved by retrieving clusters
from the W matrix. With SVD, ClassDens gets
the best results. We tested the performance of k-
means clustering by specifying a number of clus-
ters varying from 5 to 100. The best performances
were achieved with a number of clusters ≥ 20.
However, k-means scores a little bit below Class-
Dens and MCL is found to be the worst method.
The choice of the global weighting function is
also important. In our experiments, the use of IDF
and no global weighting gave the worst results.
Entropy and normal weighting gave the best re-
sults but, on average, entropy performs a little bet-
ter than normal weight. In the final version, the
global weighting function used is entropy.
The last parameter that has a visible influence
on the quality of extracted keyphrases is the selec-
tion of keyphrases from LDA’s results. In our ex-
periments, the exponential function performs best.
The remaining parameters do not have notable
influence on the results. As already stated by Lee
et al. (2005), the choice of local weighting func-
tion makes relatively little difference. Similarly,
the number of topics used for LDA has little in-
fluence. In our implementation we used term fre-
quency as local weighting and executed LDA with
a number of expected topics of 10.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="conclusions">
5 Results and Conclusion
</sectionHeader>
<bodyText confidence="0.999993344827586">
In Task 5, participants are invited to provide the
keyphrases for 100 scientific papers provided by
the organizers. Performances (precision, recall
and F-score) are calculated by comparing the pro-
posed keyphrases to keywords given by the au-
thors of documents, keywords selected by inde-
pendant readers and a combination of both. Com-
pared to other systems, our method gives the
best results on the keywords assigned by read-
ers. By performing the calculation on the first 5
keyphrases, our system ranks 9th out of 20 submit-
ted systems, with an F-score of 14.7%. This is be-
low the best method that obtains 18.2%, but above
the TD-IDF baseline of 10.44%. The same calcu-
lation performed on the first 15 kephrases gives a
F-score of 17.80% for our method (10th best F-
score). This is still below the best method, which
obtains an F-score of 23.50%, but a lot better than
the TD-IDF baseline (F-score=12.87%).
The evaluation shows that the performance of
our system is near the average of other submitted
systems. However, one has to note that our system
uses only the information available from a single
document. Compared to a selection of keywords
based on TF-IDF, which is often used as a refer-
ence, our system provides a notable improvement.
Therefore, the algorithm described here is an inter-
esting alternative to supervised learning methods
when no corpus of similar documents is available.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845186046512">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022.
Stijn Van Dongen. 2008. Graph clustering via a dis-
crete uncoupling process. SIAM J. Matrix Anal.
Appl., 30(1):121–141.
Susan T. Dumais. 1991. Improving the retrieval of in-
formation from external sources. Behavior Research
Methods, Instruments, &amp; Comp., 23(2):229–236.
George Forsythe, Michael Malcolm, and Cleve Moler.
1977. Computer Methods for Mathematical Com-
putations. Englewood Cliffs, NJ: Prentice Hall.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
Alain Gu´enoche. 2004. Clustering by vertex density
in a graph. In Classification, Clustering and Data
Mining, D. Banks et al. (Eds.), Springer, 15-23.
Daniel D. Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401:788.
Michael D. Lee, Brandon Pincombe, and Matthew
Welsh. 2005. A comparison of machine measures
of text document similarity with human judgments.
In proceedings of CogSci2005, pages 1254–1259.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Keyword
extraction from a single document using word co-
occurrence statistical information. Int. Journal on
Artificial Intelligence Tools, 13(1):157–169.
Girish Keshav Palshikar. 2007. Keyword extraction
from a single document using centrality measures.
LNCS, 4815/2007:503–510.
G Salton, C. S. Yang, and C. T. Yu. 1975. A theory
of term importance in automatic text analysis. Jour-
nal of the American Societyfor Information Science,
26(1):33–44.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In proceedings of PSB 2003, pages
451–462.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factoriza-
tion. In proceedings of SIGIR 03, pages 267–273.
</reference>
<page confidence="0.997752">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367100">
<title confidence="0.94227">Task 5: Single document keyphrase extraction using sentence clustering and Latent Dirichlet Allocation</title>
<author confidence="0.980741">Claude Pasquier</author>
<affiliation confidence="0.77784925">Institute of Developmental Biology &amp; Cancer University of Nice Sophia-Antipolis UNSA/CNRS UMR-6543 Parc Valrose</affiliation>
<address confidence="0.996399">06108 NICE Cedex 2, France</address>
<email confidence="0.997238">claude.pasquier@unice.fr</email>
<abstract confidence="0.999534416666667">This paper describes the design of a system for extracting keyphrases from a single document The principle of the algorithm is to cluster sentences of the documents in order to highlight parts of text that are semantically related. The clusters of sentences, that reflect the themes of the document, are then analyzed to find the main topics of the text. Finally, the most important words, or groups of words, from these topics are proposed as keyphrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="4014" citStr="Blei et al., 2003" startWordPosition="653" endWordPosition="656"> the terms used in a document) to a set of latent variables (the topics). A particular generative model, which is well suited for the modeling of text, is called Latent Dirichlet 1The TF-IDF weight gives the degree of importance of a word in a collection of documents. The importance increases if the word is frequently used in the set of documents but decreases if it is used by too many documents. 154 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 154–157, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Allocation (LDA) (Blei et al., 2003). Given a set of documents, the algorithms describes each document as a mixture over topics, where each topic is characterized by a distribution over words. The idea is to perform first a clustering of the sentences of the document based on their semantic similarity. Intuitively, one can see each cluster as a part of the text dealing with semantically related content. Therefore, the initial document is divided into a set of clusters and LDA can then be applied on this new representation. 3 Algorithm The algorithm is composed of 8 steps: 1. Identification and expansion of abbreviations. 2. Spli</context>
<context position="10363" citStr="Blei et al., 2003" startWordPosition="1756" endWordPosition="1759">ing Latent Dirichlet Allocation By using the result of the clustering, the source document is now represented by c clusters of terms. The terms associated with a cluster ci is the sum of the terms belonging to all the sentences in the cluster. JGibbLDA (http://jgibblda.sourceforge.net/) is used to execute LDA on the new dataset. We tried to extract different numbers of topics t (with t E 12, 5, 10, 20, 50,100}) and we choose the Dirichlet hyperparameters such as α = 0.1 and Q = 50/t. LDA inferences a topic model by estimating the cluster-topic distribution O and the topic-word distribution Φ (Blei et al., 2003). 4.8 Term Ranking and Keyphrase Selection We assume that topics covering a significant portion of the document content are more important than those covering little content. To reflect this assumption, we calculate the importance of a term in the document (its score) with a function that takes into account the distribution of topics over clusters given by θ, the distribution of terms over topics given by Φ and the clusters’ size. score(i) = max (Φji jE{1...n} where score(i) represents the score of term i and s(k) is the size of the cluster k. We tested three different functions for p: the con</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn Van Dongen</author>
</authors>
<title>Graph clustering via a discrete uncoupling process.</title>
<date>2008</date>
<journal>SIAM J. Matrix Anal. Appl.,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Van Dongen, 2008</marker>
<rawString>Stijn Van Dongen. 2008. Graph clustering via a discrete uncoupling process. SIAM J. Matrix Anal. Appl., 30(1):121–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
</authors>
<title>Improving the retrieval of information from external sources.</title>
<date>1991</date>
<journal>Behavior Research Methods, Instruments, &amp; Comp.,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="7559" citStr="Dumais (1991)" startWordPosition="1237" endWordPosition="1238">here lij is the local weight of term j within sentence i, and gj is the global weight of term j in the document. The local weighting function measures the importance of a term within a sentence and the global weighting function measures the importance of a term across the entire document. Three local weighting functions were investigated: term frequency, log of term frequency and binary. Five global weighting functions were also investigated: Normal, GFIDF (Global frequency x Inverse document frequency, IDF (Inverse document frequency), Entropy and none (details of calculation can be found in Dumais (1991) paper). 4.5 Dimensionality Reduction The matrix X is a representation of a document in a high-dimensional space. Singular Value Decomposition (SVD) (Forsythe et al., 1977) and NonNegative Matrix Factorization (NMF) (Lee and 155 Seung, 1999) are two matrix decomposition techniques that can be used to transform data in the high-dimensional space to a space of fewer dimensions. With SVD, the original matrix X is decomposed as a factor of three other matrices U, E and V such as: X = UEVT where U is an mxm matrix, E is a mxn diagonal matrix with nonnegative real numbers on the diagonal, and V T de</context>
</contexts>
<marker>Dumais, 1991</marker>
<rawString>Susan T. Dumais. 1991. Improving the retrieval of information from external sources. Behavior Research Methods, Instruments, &amp; Comp., 23(2):229–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forsythe</author>
<author>Michael Malcolm</author>
<author>Cleve Moler</author>
</authors>
<date>1977</date>
<publisher>Prentice Hall.</publisher>
<institution>Computer Methods for Mathematical Computations.</institution>
<location>Englewood Cliffs, NJ:</location>
<contexts>
<context position="7731" citStr="Forsythe et al., 1977" startWordPosition="1261" endWordPosition="1264">ce of a term within a sentence and the global weighting function measures the importance of a term across the entire document. Three local weighting functions were investigated: term frequency, log of term frequency and binary. Five global weighting functions were also investigated: Normal, GFIDF (Global frequency x Inverse document frequency, IDF (Inverse document frequency), Entropy and none (details of calculation can be found in Dumais (1991) paper). 4.5 Dimensionality Reduction The matrix X is a representation of a document in a high-dimensional space. Singular Value Decomposition (SVD) (Forsythe et al., 1977) and NonNegative Matrix Factorization (NMF) (Lee and 155 Seung, 1999) are two matrix decomposition techniques that can be used to transform data in the high-dimensional space to a space of fewer dimensions. With SVD, the original matrix X is decomposed as a factor of three other matrices U, E and V such as: X = UEVT where U is an mxm matrix, E is a mxn diagonal matrix with nonnegative real numbers on the diagonal, and V T denotes the transpose of V , an n x n matrix. It is often useful to approximate X using only r singular values (with r &lt; min(m, n)), so that we have X = UrEkVrT + E, where E </context>
</contexts>
<marker>Forsythe, Malcolm, Moler, 1977</marker>
<rawString>George Forsythe, Michael Malcolm, and Cleve Moler. 1977. Computer Methods for Mathematical Computations. Englewood Cliffs, NJ: Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="3319" citStr="Griffiths and Steyvers, 2004" startWordPosition="537" endWordPosition="540">. In this paper, we make the assumption that the words, or the set of words, that are representative of each topic constitute the keyphrases of the document. In the following of this paper, we call terms, the components of a document that constitute the vocabulary (see the detail of the identification of terms in subsection 4.3). In statistical natural language processing, one common way of modeling the contributions of different topics to a document is to treat each topic as a probability distribution over words. Therefore, a document is considered as a probabilistic mixture of these topics (Griffiths and Steyvers, 2004). Generative models can be used to relate a set of observations (in our case, the terms used in a document) to a set of latent variables (the topics). A particular generative model, which is well suited for the modeling of text, is called Latent Dirichlet 1The TF-IDF weight gives the degree of importance of a word in a collection of documents. The importance increases if the word is frequently used in the set of documents but decreases if it is used by too many documents. 154 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 154–157, Uppsala, Sweden, 15-16 J</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alain Gu´enoche</author>
</authors>
<title>Clustering by vertex density in a graph.</title>
<date>2004</date>
<booktitle>In Classification, Clustering and Data</booktitle>
<pages>15--23</pages>
<publisher>(Eds.), Springer,</publisher>
<marker>Gu´enoche, 2004</marker>
<rawString>Alain Gu´enoche. 2004. Clustering by vertex density in a graph. In Classification, Clustering and Data Mining, D. Banks et al. (Eds.), Springer, 15-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<pages>401--788</pages>
<contexts>
<context position="8875" citStr="Lee and Seung, 1999" startWordPosition="1495" endWordPosition="1499">gular values (with r &lt; min(m, n)), so that we have X = UrEkVrT + E, where E is an error or residual matrix, Ur is an m x r matrix, Er is a k x r diagonal matrix, and Vr is an n x r matrix. NMF is a matrix factorization algorithm that decomposes a matrix with only positive elements into two positive elements matrices, with X = WH+E. Usually, only r components are fit, so E is an error or residual matrix, W is a non-negative m x r matrix and H is a non-negative r x n matrix. There are several ways in which W and H may be found. In our system, we use Lee and Seung’s multiplicative update method (Lee and Seung, 1999). 4.6 Sentence Clustering The clustering of sentences is performed in the reduced space by using the cosine similarity between sentence vectors. Several clustering techniques have been investigated: k-means clustering, Markov Cluster Process (MCL) (Dongen, 2008) and ClassDens (Gu´enoche, 2004). The latent semantic space derived by SVD does not provide a direct indication of the data partitions. However, with NMF, the cluster membership of each document can be easily identified directly using the W matrix (Xu et al., 2003). Each value wij of matrix W, indicates, indeed, to which degree sentence</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401:788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Lee</author>
<author>Brandon Pincombe</author>
<author>Matthew Welsh</author>
</authors>
<title>A comparison of machine measures of text document similarity with human judgments.</title>
<date>2005</date>
<booktitle>In proceedings of CogSci2005,</booktitle>
<pages>1254--1259</pages>
<contexts>
<context position="12998" citStr="Lee et al. (2005)" startWordPosition="2207" endWordPosition="2210">eighting function is also important. In our experiments, the use of IDF and no global weighting gave the worst results. Entropy and normal weighting gave the best results but, on average, entropy performs a little better than normal weight. In the final version, the global weighting function used is entropy. The last parameter that has a visible influence on the quality of extracted keyphrases is the selection of keyphrases from LDA’s results. In our experiments, the exponential function performs best. The remaining parameters do not have notable influence on the results. As already stated by Lee et al. (2005), the choice of local weighting function makes relatively little difference. Similarly, the number of topics used for LDA has little influence. In our implementation we used term frequency as local weighting and executed LDA with a number of expected topics of 10. 5 Results and Conclusion In Task 5, participants are invited to provide the keyphrases for 100 scientific papers provided by the organizers. Performances (precision, recall and F-score) are calculated by comparing the proposed keyphrases to keywords given by the authors of documents, keywords selected by independant readers and a com</context>
</contexts>
<marker>Lee, Pincombe, Welsh, 2005</marker>
<rawString>Michael D. Lee, Brandon Pincombe, and Matthew Welsh. 2005. A comparison of machine measures of text document similarity with human judgments. In proceedings of CogSci2005, pages 1254–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Keyword extraction from a single document using word cooccurrence statistical information.</title>
<date>2004</date>
<journal>Int. Journal on Artificial Intelligence Tools,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="1800" citStr="Matsuo and Ishizuka, 2004" startWordPosition="281" endWordPosition="285">ases extraction algorithms are based on supervised learning. These methods address the problem of associating keyphrases to documents as a classification task. However, the fact that this approach requires a corpus of similar documents, which is not always readily available, constitutes a major drawback. For example, if one encounters a new Web page, one might like to know quickly the main topics addressed. In this case, a domain-independent keyword extraction system that applies to a single document is needed. Several methods have been proposed for extracting keywords from a single document (Matsuo and Ishizuka, 2004; Palshikar, 2007). The reported performances were slightly higher than that obtained using a corpus and selecting the words with the highest TF-IDF 1 measure (Salton et al., 1975). The paper describes a new keyphrase extraction algorithm from a single document. We show that our system performs well without the need for a corpus. The paper is organized as follows. The next section describes the principles of our keyphrase extraction system. We present the main parts of the algorithm in section 3, we detail the methods in section 4 and we conclude the paper. 2 Principles When authors write docu</context>
</contexts>
<marker>Matsuo, Ishizuka, 2004</marker>
<rawString>Yutaka Matsuo and Mitsuru Ishizuka. 2004. Keyword extraction from a single document using word cooccurrence statistical information. Int. Journal on Artificial Intelligence Tools, 13(1):157–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Keshav Palshikar</author>
</authors>
<title>Keyword extraction from a single document using centrality measures.</title>
<date>2007</date>
<booktitle>LNCS,</booktitle>
<pages>4815--2007</pages>
<contexts>
<context position="1818" citStr="Palshikar, 2007" startWordPosition="286" endWordPosition="287">are based on supervised learning. These methods address the problem of associating keyphrases to documents as a classification task. However, the fact that this approach requires a corpus of similar documents, which is not always readily available, constitutes a major drawback. For example, if one encounters a new Web page, one might like to know quickly the main topics addressed. In this case, a domain-independent keyword extraction system that applies to a single document is needed. Several methods have been proposed for extracting keywords from a single document (Matsuo and Ishizuka, 2004; Palshikar, 2007). The reported performances were slightly higher than that obtained using a corpus and selecting the words with the highest TF-IDF 1 measure (Salton et al., 1975). The paper describes a new keyphrase extraction algorithm from a single document. We show that our system performs well without the need for a corpus. The paper is organized as follows. The next section describes the principles of our keyphrase extraction system. We present the main parts of the algorithm in section 3, we detail the methods in section 4 and we conclude the paper. 2 Principles When authors write documents, they have t</context>
</contexts>
<marker>Palshikar, 2007</marker>
<rawString>Girish Keshav Palshikar. 2007. Keyword extraction from a single document using centrality measures. LNCS, 4815/2007:503–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
<author>C T Yu</author>
</authors>
<title>A theory of term importance in automatic text analysis.</title>
<date>1975</date>
<journal>Journal of the American Societyfor Information Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1980" citStr="Salton et al., 1975" startWordPosition="311" endWordPosition="314">is approach requires a corpus of similar documents, which is not always readily available, constitutes a major drawback. For example, if one encounters a new Web page, one might like to know quickly the main topics addressed. In this case, a domain-independent keyword extraction system that applies to a single document is needed. Several methods have been proposed for extracting keywords from a single document (Matsuo and Ishizuka, 2004; Palshikar, 2007). The reported performances were slightly higher than that obtained using a corpus and selecting the words with the highest TF-IDF 1 measure (Salton et al., 1975). The paper describes a new keyphrase extraction algorithm from a single document. We show that our system performs well without the need for a corpus. The paper is organized as follows. The next section describes the principles of our keyphrase extraction system. We present the main parts of the algorithm in section 3, we detail the methods in section 4 and we conclude the paper. 2 Principles When authors write documents, they have to think first at the way they will present their ideas. Most of the time, they establish content summaries that highlight the main topics of their texts. Then, th</context>
</contexts>
<marker>Salton, Yang, Yu, 1975</marker>
<rawString>G Salton, C. S. Yang, and C. T. Yu. 1975. A theory of term importance in automatic text analysis. Journal of the American Societyfor Information Science, 26(1):33–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In proceedings of PSB</booktitle>
<pages>451--462</pages>
<contexts>
<context position="5771" citStr="Schwartz and Hearst, 2003" startWordPosition="934" endWordPosition="937">onsidered as a set of clusters, with each cluster consisting of a bag of terms. 7. Execution of LDA on the new document representation. 8. Selection of best keyphrases by analyzing LDA’s results. 4 Methods Our implementation is build on UIMA (Unstructured Information Management Architecture) (http://incubator.apache.org/uima/), a robust and flexible framework that facilitates interoperability between tools dedicated to unstructured information processsing. The method processes one document at a time by performing the steps described below. 4.1 Abbreviation Expansion The program ExtractAbbrev (Schwartz and Hearst, 2003) is used to identify abbreviations (short forms) and their corresponding definitions (long forms). Once abbreviations have been identified, each short form is replaced by its corresponding long form in the processed document. 4.2 Sentence Detection Splitting the content of a document into sentences is an important step of the method. To perform this task, we used the OpenNLP’s sentence detector module (http://opennlp.sourceforge.net/) trained on a corpus of general English texts. 4.3 Term Identification Word categories are identified by using the LingPipe’s general English part-of-speech (POS)</context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In proceedings of PSB 2003, pages 451–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Xin Liu</author>
<author>Yihong Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In proceedings of SIGIR 03,</booktitle>
<pages>267--273</pages>
<contexts>
<context position="9402" citStr="Xu et al., 2003" startWordPosition="1580" endWordPosition="1583"> In our system, we use Lee and Seung’s multiplicative update method (Lee and Seung, 1999). 4.6 Sentence Clustering The clustering of sentences is performed in the reduced space by using the cosine similarity between sentence vectors. Several clustering techniques have been investigated: k-means clustering, Markov Cluster Process (MCL) (Dongen, 2008) and ClassDens (Gu´enoche, 2004). The latent semantic space derived by SVD does not provide a direct indication of the data partitions. However, with NMF, the cluster membership of each document can be easily identified directly using the W matrix (Xu et al., 2003). Each value wij of matrix W, indicates, indeed, to which degree sentence i is associated with cluster j. If NMF was calculated with the rank r, then r clusters are represented on the matrix. We use a simple rule to determine the content of each cluster: sentence i belongs to cluster j if wij &gt; wik. In our system, we fixed a = 0.1. 4.7 Applying Latent Dirichlet Allocation By using the result of the clustering, the source document is now represented by c clusters of terms. The terms associated with a cluster ci is the sum of the terms belonging to all the sentences in the cluster. JGibbLDA (htt</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-negative matrix factorization. In proceedings of SIGIR 03, pages 267–273.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>