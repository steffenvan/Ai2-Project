<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000438">
<title confidence="0.997895">
Paraphrase Lattice for Statistical Machine Translation
</title>
<author confidence="0.969834">
Takashi Onishi and Masao Utiyama and Eiichiro Sumita
</author>
<affiliation confidence="0.978109">
Language Translation Group, MASTAR Project
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.486669">
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN
</address>
<email confidence="0.999202">
{takashi.onishi,mutiyama,eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751705882353">
Lattice decoding in statistical machine
translation (SMT) is useful in speech
translation and in the translation of Ger-
man because it can handle input ambigu-
ities such as speech recognition ambigui-
ties and German word segmentation ambi-
guities. We show that lattice decoding is
also useful for handling input variations.
Given an input sentence, we build a lattice
which represents paraphrases of the input
sentence. We call this a paraphrase lattice.
Then, we give the paraphrase lattice as an
input to the lattice decoder. The decoder
selects the best path for decoding. Us-
ing these paraphrase lattices as inputs, we
obtained significant gains in BLEU scores
for IWSLT and Europarl datasets.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954608695652">
Lattice decoding in SMT is useful in speech trans-
lation and in the translation of German (Bertoldi
et al., 2007; Dyer, 2009). In speech translation,
by using lattices that represent not only 1-best re-
sult but also other possibilities of speech recogni-
tion, we can take into account the ambiguities of
speech recognition. Thus, the translation quality
for lattice inputs is better than the quality for 1-
best inputs.
In this paper, we show that lattice decoding is
also useful for handling input variations. “Input
variations” refers to the differences of input texts
with the same meaning. For example, “Is there
a beauty salon?” and “Is there a beauty par-
lor?” have the same meaning with variations in
“beauty salon” and “beauty parlor”. Since these
variations are frequently found in natural language
texts, a mismatch of the expressions in source sen-
tences and the expressions in training corpus leads
to a decrease in translation quality. Therefore,
we propose a novel method that can handle in-
put variations using paraphrases and lattice decod-
ing. In the proposed method, we regard a given
source sentence as one of many variations (1-best).
Given an input sentence, we build a paraphrase lat-
tice which represents paraphrases of the input sen-
tence. Then, we give the paraphrase lattice as an
input to the Moses decoder (Koehn et al., 2007).
Moses selects the best path for decoding. By using
paraphrases of source sentences, we can translate
expressions which are not found in a training cor-
pus on the condition that paraphrases of them are
found in the training corpus. Moreover, by using
lattice decoding, we can employ the source-side
language model as a decoding feature. Since this
feature is affected by the source-side context, the
decoder can choose a proper paraphrase and trans-
late correctly.
This paper is organized as follows: Related
works on lattice decoding and paraphrasing are
presented in Section 2. The proposed method is
described in Section 3. Experimental results for
IWSLT and Europarl dataset are presented in Sec-
tion 4. Finally, the paper is concluded with a sum-
mary and a few directions for future work in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998885307692308">
Lattice decoding has been used to handle ambigu-
ities of preprocessing. Bertoldi et al. (2007) em-
ployed a confusion network, which is a kind of lat-
tice and represents speech recognition hypotheses
in speech translation. Dyer (2009) also employed
a segmentation lattice, which represents ambigui-
ties of compound word segmentation in German,
Hungarian and Turkish translation. However, to
the best of our knowledge, there is no work which
employed a lattice representing paraphrases of an
input sentence.
On the other hand, paraphrasing has been used
to enrich the SMT model. Callison-Burch et
</bodyText>
<page confidence="0.790612">
1
</page>
<note confidence="0.573242">
Proceedings of the ACL 2010 Conference Short Papers, pages 1–5,
</note>
<page confidence="0.502541">
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</page>
<figure confidence="0.996220166666667">
Input sentence
Paraphrasing
Parallel Corpus
(for paraphrase)
Paraphrase
List
</figure>
<bodyText confidence="0.755763666666667">
phrase table and keep only appropriate phrase
pairs using the sigtest-filter (Johnson et al.,
2007).
</bodyText>
<figure confidence="0.9272745">
Paraphrase Lattice
Output sentence
</figure>
<figureCaption confidence="0.999869">
Figure 1: Overview of the proposed method.
</figureCaption>
<bodyText confidence="0.998431714285714">
al. (2006) and Marton et al. (2009) augmented
the translation phrase table with paraphrases to
translate unknown phrases. Bond et al. (2008)
and Nakov (2008) augmented the training data by
paraphrasing. However, there is no work which
augments input sentences by paraphrasing and
represents them in lattices.
</bodyText>
<sectionHeader confidence="0.950564" genericHeader="method">
3 Paraphrase Lattice for SMT
</sectionHeader>
<bodyText confidence="0.999983636363637">
Overview of the proposed method is shown in Fig-
ure 1. In advance, we automatically acquire a
paraphrase list from a parallel corpus. In order to
acquire paraphrases of unknown phrases, this par-
allel corpus is different from the parallel corpus
for training.
Given an input sentence, we build a lattice
which represents paraphrases of the input sentence
using the paraphrase list. We call this lattice a
paraphrase lattice. Then, we give the paraphrase
lattice to the lattice decoder.
</bodyText>
<subsectionHeader confidence="0.99985">
3.1 Acquiring the paraphrase list
</subsectionHeader>
<bodyText confidence="0.999854875">
We acquire a paraphrase list using Bannard and
Callison-Burch (2005)’s method. Their idea is, if
two different phrases e1, e2 in one language are
aligned to the same phrase c in another language,
they are hypothesized to be paraphrases of each
other. Our paraphrase list is acquired in the same
way.
The procedure is as follows:
</bodyText>
<listItem confidence="0.95645025">
1. Build a phrase table.
Build a phrase table from parallel corpus us-
ing standard SMT techniques.
2. Filter the phrase table by the sigtest-filter.
</listItem>
<bodyText confidence="0.8848365">
The phrase table built in 1 has many inappro-
priate phrase pairs. Therefore, we filter the
</bodyText>
<listItem confidence="0.956239">
3. Calculate the paraphrase probability.
</listItem>
<bodyText confidence="0.9215325">
Calculate the paraphrase probability p(e2|e1)
if e2 is hypothesized to be a paraphrase of e1.
</bodyText>
<equation confidence="0.9836735">
∑p(e2|e1) = P(c|e1)P(e2|c)
c
</equation>
<bodyText confidence="0.992138">
where P(·|·) is phrase translation probability.
</bodyText>
<listItem confidence="0.834838">
4. Acquire a paraphrase pair.
</listItem>
<bodyText confidence="0.9892105">
Acquire (e1, e2) as a paraphrase pair if
p(e2|e1) &gt; p(e1|e1). The purpose of this
threshold is to keep highly-accurate para-
phrase pairs. In experiments, more than 80%
of paraphrase pairs were eliminated by this
threshold.
</bodyText>
<subsectionHeader confidence="0.999907">
3.2 Building paraphrase lattice
</subsectionHeader>
<bodyText confidence="0.9999685">
An input sentence is paraphrased using the para-
phrase list and transformed into a paraphrase lat-
tice. The paraphrase lattice is a lattice which rep-
resents paraphrases of the input sentence. An ex-
ample of a paraphrase lattice is shown in Figure 2.
In this example, an input sentence is “is there a
beauty salon ?”. This paraphrase lattice contains
two paraphrase pairs “beauty salon” = “beauty
parlor” and “beauty salon” = “salon”, and rep-
resents following three sentences.
</bodyText>
<listItem confidence="0.999630333333334">
• is there a beauty salon ?
• is there a beauty parlor?
• is there a salon ?
</listItem>
<bodyText confidence="0.9867765">
In the paraphrase lattice, each node consists of
a token, the distance to the next node and features
for lattice decoding. We use following four fea-
tures for lattice decoding.
</bodyText>
<listItem confidence="0.998924">
• Paraphrase probability (p)
</listItem>
<bodyText confidence="0.9312755">
A paraphrase probability p(e2|e1) calculated
when acquiring the paraphrase.
</bodyText>
<equation confidence="0.471068">
hp = p(e2|e1)
</equation>
<listItem confidence="0.994761">
• Language model score (l)
</listItem>
<bodyText confidence="0.976744">
A ratio between the language model proba-
bility of the paraphrased sentence (para) and
that of the original sentence (orig).
</bodyText>
<equation confidence="0.697207">
hl = lm(para)
lm(o���)
</equation>
<figure confidence="0.954871071428571">
Parallel Corpus
(for training)
SMT model
Lattice Decoding
2
Token
0 -- (&amp;quot;is&amp;quot; , 1, 1, 1, 1)
1 -- (&amp;quot;there&amp;quot; , 1, 1, 1, 1)
2 -- (&amp;quot;a&amp;quot; , 1, 1, 1, 1)
Distance to the next node Features for lattice decoding
3 -- (&amp;quot;beauty&amp;quot; , 1, 1, 1, 2) (&amp;quot;beauty&amp;quot; , 0.250, 1.172, 1, 1) (&amp;quot;salon&amp;quot; , 0.133, 0.537, 0.367, 3)
4 -- (&amp;quot;parlor&amp;quot; , 1, 1, 1, 2)
5 -- (&amp;quot;salon&amp;quot; , 1, 1, 1, 1)
6 -- (&amp;quot;?&amp;quot; , 1, 1, 1, 1)
</figure>
<figureCaption confidence="0.815148">
Figure 2: An example of a paraphrase lattice, which contains three features of (p, l, d).
</figureCaption>
<figure confidence="0.973214">
Paraphrase probability (p)
Language model score (l)
Paraphrase length (d)
</figure>
<listItem confidence="0.588664">
• Normalized language model score (L)
</listItem>
<bodyText confidence="0.996926">
A language model score where the language
model probability is normalized by the sen-
tence length. The sentence length is calcu-
lated as the number of tokens.
</bodyText>
<equation confidence="0.996383">
hL = LM(para) LM(orig) ,
</equation>
<bodyText confidence="0.493808">
where LM(sent) = lm(sent)
</bodyText>
<listItem confidence="0.990989">
• Paraphrase length (d)
</listItem>
<bodyText confidence="0.999956">
The difference between the original sentence
length and the paraphrased sentence length.
</bodyText>
<equation confidence="0.921848">
hd = exp(length(para)−length(orig))
</equation>
<bodyText confidence="0.9998876">
The values of these features are calculated only
if the node is the first node of the paraphrase, for
example the second “beauty” and “salon” in line
3 of Figure 2. In other nodes, for example “par-
lor” in line 4 and original nodes, we use 1 as the
values of features.
The features related to the language model, such
as (l) and (L), are affected by the context of source
sentences even if the same paraphrase pair is ap-
plied. As these features can penalize paraphrases
which are not appropriate to the context, appropri-
ate paraphrases are chosen and appropriate trans-
lations are output in lattice decoding. The features
related to the sentence length, such as (L) and (d),
are added to penalize the language model score
in case the paraphrased sentence length is shorter
than the original sentence length and the language
model score is unreasonably low.
In experiments, we use four combinations of
these features, (p), (p, l), (p, L) and (p, l, d).
</bodyText>
<subsectionHeader confidence="0.9991">
3.3 Lattice decoding
</subsectionHeader>
<bodyText confidence="0.998451">
We use Moses (Koehn et al., 2007) as a decoder
for lattice decoding. Moses is an open source
SMT system which allows lattice decoding. In
lattice decoding, Moses selects the best path and
the best translation according to features added in
each node and other SMT features. These weights
are optimized using Minimum Error Rate Training
(MERT) (Och, 2003).
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999951823529412">
In order to evaluate the proposed method, we
conducted English-to-Japanese and English-to-
Chinese translation experiments using IWSLT
2007 (Fordyce, 2007) dataset. This dataset con-
tains EJ and EC parallel corpus for the travel
domain and consists of 40k sentences for train-
ing and about 500 sentences sets (dev1, dev2
and dev3) for development and testing. We used
the dev1 set for parameter tuning, the dev2 set
for choosing the setting of the proposed method,
which is described below, and the dev3 set for test-
ing.
The English-English paraphrase list was ac-
quired from the EC corpus for EJ translation and
53K pairs were acquired. Similarly, 47K pairs
were acquired from the EJ corpus for EC trans-
lation.
</bodyText>
<subsectionHeader confidence="0.979365">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.9998878">
As baselines, we used Moses and Callison-Burch
et al. (2006)’s method (hereafter CCB). In Moses,
we used default settings without paraphrases. In
CCB, we paraphrased the phrase table using the
automatically acquired paraphrase list. Then,
we augmented the phrase table with paraphrased
phrases which were not found in the original
phrase table. Moreover, we used an additional fea-
ture whose value was the paraphrase probability
(p) if the entry was generated by paraphrasing and
</bodyText>
<equation confidence="0.884182">
1
length(sent)
</equation>
<page confidence="0.992146">
3
</page>
<table confidence="0.994834666666667">
Moses (w/o Paraphrases) CCB Proposed Method
EJ 38.98 39.24 (+0.26) 40.34 (+1.36)
EC 25.11 26.14 (+1.03) 27.06 (+1.95)
</table>
<tableCaption confidence="0.769855666666667">
Table 1: Experimental results for IWSLT (%BLEU).
1 if otherwise. Weights of the feature and other
features in SMT were optimized using MERT.
</tableCaption>
<subsectionHeader confidence="0.986399">
4.2 Proposed method
</subsectionHeader>
<bodyText confidence="0.999866">
In the proposed method, we conducted experi-
ments with various settings for paraphrasing and
lattice decoding. Then, we chose the best setting
according to the result of the dev2 set.
</bodyText>
<subsectionHeader confidence="0.853957">
4.2.1 Limitation of paraphrasing
</subsectionHeader>
<bodyText confidence="0.9999881875">
As the paraphrase list was automatically ac-
quired, there were many erroneous paraphrase
pairs. Building paraphrase lattices with all erro-
neous paraphrase pairs and decoding these para-
phrase lattices caused high computational com-
plexity. Therefore, we limited the number of para-
phrasing per phrase and per sentence. The number
of paraphrasing per phrase was limited to three and
the number of paraphrasing per sentence was lim-
ited to twice the size of the sentence length.
As a criterion for limiting the number of para-
phrasing, we use three features (p), (l) and (L),
which are same as the features described in Sub-
section 3.2. When building paraphrase lattices, we
apply paraphrases in descending order of the value
of the criterion.
</bodyText>
<subsectionHeader confidence="0.945423">
4.2.2 Finding optimal settings
</subsectionHeader>
<bodyText confidence="0.999979875">
As previously mentioned, we have three choices
for the criterion for building paraphrase lattices
and four combinations of features for lattice de-
coding. Thus, there are 3 x 4 = 12 combinations
of these settings. We conducted parameter tuning
with the dev1 set for each setting and used as best
the setting which got the highest BLEU score for
the dev2 set.
</bodyText>
<subsectionHeader confidence="0.961233">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999985461538461">
The experimental results are shown in Table 1. We
used the case-insensitive BLEU metric for eval-
uation. In EJ translation, the proposed method
obtained the highest score of 40.34%, which
achieved an absolute improvement of 1.36 BLEU
points over Moses and 1.10 BLEU points over
CCB. In EC translation, the proposed method also
obtained the highest score of 27.06% and achieved
an absolute improvement of 1.95 BLEU points
over Moses and 0.92 BLEU points over CCB. As
the relation of three systems is Moses &lt; CCB &lt;
Proposed Method, paraphrasing is useful for SMT
and using paraphrase lattices and lattice decod-
ing is especially more useful than augmenting the
phrase table. In Proposed Method, the criterion for
building paraphrase lattices and the combination
of features for lattice decoding were (p) and (p, L)
in EJ translation and (L) and (p, l) in EC transla-
tion. Since features related to the source-side lan-
guage model were chosen in each direction, using
the source-side language model is useful for de-
coding paraphrase lattices.
We also tried a combination of Proposed
Method and CCB, which is a method of decoding
paraphrase lattices with an augmented phrase ta-
ble. However, the result showed no significant im-
provements. This is because the proposed method
includes the effect of augmenting the phrase table.
Moreover, we conducted German-English
translation using the Europarl corpus (Koehn,
2005). We used the WMT08 dataset&apos;, which
consists of 1M sentences for training and 2K sen-
tences for development and testing. We acquired
5.3M pairs of German-German paraphrases from
a 1M German-Spanish parallel corpus. We con-
ducted experiments with various sizes of training
corpus, using 10K, 20K, 40K, 80K, 160K and 1M.
Figure 3 shows the proposed method consistently
get higher score than Moses and CCB.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999303">
This paper has proposed a novel method for trans-
forming a source sentence into a paraphrase lattice
and applying lattice decoding. Since our method
can employ source-side language models as a de-
coding feature, the decoder can choose proper
paraphrases and translate properly. The exper-
imental results showed significant gains for the
IWSLT and Europarl dataset. In IWSLT dataset,
we obtained 1.36 BLEU points over Moses in EJ
translation and 1.95 BLEU points over Moses in
</bodyText>
<footnote confidence="0.987044">
1http://www.statmt.org/wmt08/
</footnote>
<page confidence="0.983477">
4
</page>
<figure confidence="0.9979779375">
BLEU score (%)
29
28
27
26
25
24
23
22
21
20
Moses
CCB
Proposed
10 100 1000
Corpus size (K)
</figure>
<figureCaption confidence="0.999983">
Figure 3: Effect of training corpus size.
</figureCaption>
<bodyText confidence="0.995987571428571">
EC translation. In Europarl dataset, the proposed
method consistently get higher score than base-
lines.
In future work, we plan to apply this method
with paraphrases derived from a massive corpus
such as the Web corpus and apply this method to a
hierarchical phrase based SMT.
</bodyText>
<sectionHeader confidence="0.998912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709969230769">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
597–604.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 1297–1300.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 150–157.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of the
Human Language Technology conference - North
American chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 17–24.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of the Human Language Technology confer-
ence - North American chapter of the Association
for Computational Linguistics (HLT-NAACL), pages
406–414.
Cameron S. Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 1–12.
J Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving Translation Qual-
ity by Discarding Most of the Phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967–975.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 177–180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit (MT Summit),
pages 79–86.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine
Translation Using Monolingually-Derived Para-
phrases. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 381–390.
Preslav Nakov. 2008. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In
Proceedings of the European Conference on Artifi-
cial Intelligence (ECAI), pages 338–342.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160–167.
</reference>
<page confidence="0.994326">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671882">
<title confidence="0.999922">Paraphrase Lattice for Statistical Machine Translation</title>
<author confidence="0.817221">Onishi Utiyama Sumita</author>
<affiliation confidence="0.9101215">Language Translation Group, MASTAR Project National Institute of Information and Communications Technology</affiliation>
<address confidence="0.997689">3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN</address>
<abstract confidence="0.991585">Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>597--604</pages>
<contexts>
<context position="5130" citStr="Bannard and Callison-Burch (2005)" startWordPosition="800" endWordPosition="803">them in lattices. 3 Paraphrase Lattice for SMT Overview of the proposed method is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. We call this lattice a paraphrase lattice. Then, we give the paraphrase lattice to the lattice decoder. 3.1 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases e1, e2 in one language are aligned to the same phrase c in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: 1. Build a phrase table. Build a phrase table from parallel corpus using standard SMT techniques. 2. Filter the phrase table by the sigtest-filter. The phrase table built in 1 has many inappropriate phrase pairs. Therefore, we filter the 3. Calculate the paraphrase probability. Calculate the paraphrase probability p(e2|e1) if e2 is hypot</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Richard Zens</author>
<author>Marcello Federico</author>
</authors>
<title>Speech translation by confusion network decoding.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>1297--1300</pages>
<contexts>
<context position="1157" citStr="Bertoldi et al., 2007" startWordPosition="165" endWordPosition="168">ies and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets. 1 Introduction Lattice decoding in SMT is useful in speech translation and in the translation of German (Bertoldi et al., 2007; Dyer, 2009). In speech translation, by using lattices that represent not only 1-best result but also other possibilities of speech recognition, we can take into account the ambiguities of speech recognition. Thus, the translation quality for lattice inputs is better than the quality for 1- best inputs. In this paper, we show that lattice decoding is also useful for handling input variations. “Input variations” refers to the differences of input texts with the same meaning. For example, “Is there a beauty salon?” and “Is there a beauty parlor?” have the same meaning with variations in “beauty</context>
<context position="3303" citStr="Bertoldi et al. (2007)" startWordPosition="521" endWordPosition="524">rce-side language model as a decoding feature. Since this feature is affected by the source-side context, the decoder can choose a proper paraphrase and translate correctly. This paper is organized as follows: Related works on lattice decoding and paraphrasing are presented in Section 2. The proposed method is described in Section 3. Experimental results for IWSLT and Europarl dataset are presented in Section 4. Finally, the paper is concluded with a summary and a few directions for future work in Section 5. 2 Related Work Lattice decoding has been used to handle ambiguities of preprocessing. Bertoldi et al. (2007) employed a confusion network, which is a kind of lattice and represents speech recognition hypotheses in speech translation. Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. However, to the best of our knowledge, there is no work which employed a lattice representing paraphrases of an input sentence. On the other hand, paraphrasing has been used to enrich the SMT model. Callison-Burch et 1 Proceedings of the ACL 2010 Conference Short Papers, pages 1–5, Uppsala, Sweden, 11-16 July 2010. c�</context>
</contexts>
<marker>Bertoldi, Zens, Federico, 2007</marker>
<rawString>Nicola Bertoldi, Richard Zens, and Marcello Federico. 2007. Speech translation by confusion network decoding. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 1297–1300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Eric Nichols</author>
<author>Darren Scott Appling</author>
<author>Michael Paul</author>
</authors>
<title>Improving Statistical Machine Translation by Paraphrasing the Training Data.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>150--157</pages>
<contexts>
<context position="4346" citStr="Bond et al. (2008)" startWordPosition="676" endWordPosition="679">d, paraphrasing has been used to enrich the SMT model. Callison-Burch et 1 Proceedings of the ACL 2010 Conference Short Papers, pages 1–5, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Input sentence Paraphrasing Parallel Corpus (for paraphrase) Paraphrase List phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). Paraphrase Lattice Output sentence Figure 1: Overview of the proposed method. al. (2006) and Marton et al. (2009) augmented the translation phrase table with paraphrases to translate unknown phrases. Bond et al. (2008) and Nakov (2008) augmented the training data by paraphrasing. However, there is no work which augments input sentences by paraphrasing and represents them in lattices. 3 Paraphrase Lattice for SMT Overview of the proposed method is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. We call this lattice a</context>
</contexts>
<marker>Bond, Nichols, Appling, Paul, 2008</marker>
<rawString>Francis Bond, Eric Nichols, Darren Scott Appling, and Michael Paul. 2008. Improving Statistical Machine Translation by Paraphrasing the Training Data. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved Statistical Machine Translation Using Paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>17--24</pages>
<contexts>
<context position="10200" citStr="Callison-Burch et al. (2006)" startWordPosition="1669" endWordPosition="1672">) dataset. This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for development and testing. We used the dev1 set for parameter tuning, the dev2 set for choosing the setting of the proposed method, which is described below, and the dev3 set for testing. The English-English paraphrase list was acquired from the EC corpus for EJ translation and 53K pairs were acquired. Similarly, 47K pairs were acquired from the EJ corpus for EC translation. 4.1 Baseline As baselines, we used Moses and Callison-Burch et al. (2006)’s method (hereafter CCB). In Moses, we used default settings without paraphrases. In CCB, we paraphrased the phrase table using the automatically acquired paraphrase list. Then, we augmented the phrase table with paraphrased phrases which were not found in the original phrase table. Moreover, we used an additional feature whose value was the paraphrase probability (p) if the entry was generated by paraphrasing and 1 length(sent) 3 Moses (w/o Paraphrases) CCB Proposed Method EJ 38.98 39.24 (+0.26) 40.34 (+1.36) EC 25.11 26.14 (+1.03) 27.06 (+1.95) Table 1: Experimental results for IWSLT (%BLEU</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLT-NAACL), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>406--414</pages>
<contexts>
<context position="1170" citStr="Dyer, 2009" startWordPosition="169" endWordPosition="170">mentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets. 1 Introduction Lattice decoding in SMT is useful in speech translation and in the translation of German (Bertoldi et al., 2007; Dyer, 2009). In speech translation, by using lattices that represent not only 1-best result but also other possibilities of speech recognition, we can take into account the ambiguities of speech recognition. Thus, the translation quality for lattice inputs is better than the quality for 1- best inputs. In this paper, we show that lattice decoding is also useful for handling input variations. “Input variations” refers to the differences of input texts with the same meaning. For example, “Is there a beauty salon?” and “Is there a beauty parlor?” have the same meaning with variations in “beauty salon” and “</context>
<context position="3440" citStr="Dyer (2009)" startWordPosition="545" endWordPosition="546"> and translate correctly. This paper is organized as follows: Related works on lattice decoding and paraphrasing are presented in Section 2. The proposed method is described in Section 3. Experimental results for IWSLT and Europarl dataset are presented in Section 4. Finally, the paper is concluded with a summary and a few directions for future work in Section 5. 2 Related Work Lattice decoding has been used to handle ambiguities of preprocessing. Bertoldi et al. (2007) employed a confusion network, which is a kind of lattice and represents speech recognition hypotheses in speech translation. Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. However, to the best of our knowledge, there is no work which employed a lattice representing paraphrases of an input sentence. On the other hand, paraphrasing has been used to enrich the SMT model. Callison-Burch et 1 Proceedings of the ACL 2010 Conference Short Papers, pages 1–5, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Input sentence Paraphrasing Parallel Corpus (for paraphrase) Paraphrase List phrase table </context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Chris Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLT-NAACL), pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron S Fordyce</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>1--12</pages>
<contexts>
<context position="9573" citStr="Fordyce, 2007" startWordPosition="1562" endWordPosition="1563">ur combinations of these features, (p), (p, l), (p, L) and (p, l, d). 3.3 Lattice decoding We use Moses (Koehn et al., 2007) as a decoder for lattice decoding. Moses is an open source SMT system which allows lattice decoding. In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. These weights are optimized using Minimum Error Rate Training (MERT) (Och, 2003). 4 Experiments In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for development and testing. We used the dev1 set for parameter tuning, the dev2 set for choosing the setting of the proposed method, which is described below, and the dev3 set for testing. The English-English paraphrase list was acquired from the EC corpus for EJ translation and 53K pairs were acquired. Similarly, 47K pairs were acquired from the EJ corpus for EC translation. 4.1 Baseline As baselines, we used Moses and C</context>
</contexts>
<marker>Fordyce, 2007</marker>
<rawString>Cameron S. Fordyce. 2007. Overview of the IWSLT 2007 Evaluation Campaign. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving Translation Quality by Discarding Most of the Phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>967--975</pages>
<contexts>
<context position="4126" citStr="Johnson et al., 2007" startWordPosition="643" endWordPosition="646">es of compound word segmentation in German, Hungarian and Turkish translation. However, to the best of our knowledge, there is no work which employed a lattice representing paraphrases of an input sentence. On the other hand, paraphrasing has been used to enrich the SMT model. Callison-Burch et 1 Proceedings of the ACL 2010 Conference Short Papers, pages 1–5, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Input sentence Paraphrasing Parallel Corpus (for paraphrase) Paraphrase List phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). Paraphrase Lattice Output sentence Figure 1: Overview of the proposed method. al. (2006) and Marton et al. (2009) augmented the translation phrase table with paraphrases to translate unknown phrases. Bond et al. (2008) and Nakov (2008) augmented the training data by paraphrasing. However, there is no work which augments input sentences by paraphrasing and represents them in lattices. 3 Paraphrase Lattice for SMT Overview of the proposed method is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrase</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>J Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving Translation Quality by Discarding Most of the Phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 967–975.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="2391" citStr="Koehn et al., 2007" startWordPosition="369" endWordPosition="372">auty parlor”. Since these variations are frequently found in natural language texts, a mismatch of the expressions in source sentences and the expressions in training corpus leads to a decrease in translation quality. Therefore, we propose a novel method that can handle input variations using paraphrases and lattice decoding. In the proposed method, we regard a given source sentence as one of many variations (1-best). Given an input sentence, we build a paraphrase lattice which represents paraphrases of the input sentence. Then, we give the paraphrase lattice as an input to the Moses decoder (Koehn et al., 2007). Moses selects the best path for decoding. By using paraphrases of source sentences, we can translate expressions which are not found in a training corpus on the condition that paraphrases of them are found in the training corpus. Moreover, by using lattice decoding, we can employ the source-side language model as a decoding feature. Since this feature is affected by the source-side context, the decoder can choose a proper paraphrase and translate correctly. This paper is organized as follows: Related works on lattice decoding and paraphrasing are presented in Section 2. The proposed method i</context>
<context position="9083" citStr="Koehn et al., 2007" startWordPosition="1486" endWordPosition="1489"> if the same paraphrase pair is applied. As these features can penalize paraphrases which are not appropriate to the context, appropriate paraphrases are chosen and appropriate translations are output in lattice decoding. The features related to the sentence length, such as (L) and (d), are added to penalize the language model score in case the paraphrased sentence length is shorter than the original sentence length and the language model score is unreasonably low. In experiments, we use four combinations of these features, (p), (p, l), (p, L) and (p, l, d). 3.3 Lattice decoding We use Moses (Koehn et al., 2007) as a decoder for lattice decoding. Moses is an open source SMT system which allows lattice decoding. In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. These weights are optimized using Minimum Error Rate Training (MERT) (Och, 2003). 4 Experiments In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit (MT Summit),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="13674" citStr="Koehn, 2005" startWordPosition="2232" endWordPosition="2233"> (p) and (p, L) in EJ translation and (L) and (p, l) in EC translation. Since features related to the source-side language model were chosen in each direction, using the source-side language model is useful for decoding paraphrase lattices. We also tried a combination of Proposed Method and CCB, which is a method of decoding paraphrase lattices with an augmented phrase table. However, the result showed no significant improvements. This is because the proposed method includes the effect of augmenting the phrase table. Moreover, we conducted German-English translation using the Europarl corpus (Koehn, 2005). We used the WMT08 dataset&apos;, which consists of 1M sentences for training and 2K sentences for development and testing. We acquired 5.3M pairs of German-German paraphrases from a 1M German-Spanish parallel corpus. We conducted experiments with various sizes of training corpus, using 10K, 20K, 40K, 80K, 160K and 1M. Figure 3 shows the proposed method consistently get higher score than Moses and CCB. 5 Conclusion This paper has proposed a novel method for transforming a source sentence into a paraphrase lattice and applying lattice decoding. Since our method can employ source-side language model</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the 10th Machine Translation Summit (MT Summit), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>381--390</pages>
<contexts>
<context position="4241" citStr="Marton et al. (2009)" startWordPosition="661" endWordPosition="664">, there is no work which employed a lattice representing paraphrases of an input sentence. On the other hand, paraphrasing has been used to enrich the SMT model. Callison-Burch et 1 Proceedings of the ACL 2010 Conference Short Papers, pages 1–5, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Input sentence Paraphrasing Parallel Corpus (for paraphrase) Paraphrase List phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). Paraphrase Lattice Output sentence Figure 1: Overview of the proposed method. al. (2006) and Marton et al. (2009) augmented the translation phrase table with paraphrases to translate unknown phrases. Bond et al. (2008) and Nakov (2008) augmented the training data by paraphrasing. However, there is no work which augments input sentences by paraphrasing and represents them in lattices. 3 Paraphrase Lattice for SMT Overview of the proposed method is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. Given an input sentence, we build a lat</context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 381–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improved Statistical Machine Translation Using Monolingual Paraphrases.</title>
<date>2008</date>
<booktitle>In Proceedings of the European Conference on Artificial Intelligence (ECAI),</booktitle>
<pages>338--342</pages>
<contexts>
<context position="4363" citStr="Nakov (2008)" startWordPosition="681" endWordPosition="682">n used to enrich the SMT model. Callison-Burch et 1 Proceedings of the ACL 2010 Conference Short Papers, pages 1–5, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Input sentence Paraphrasing Parallel Corpus (for paraphrase) Paraphrase List phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). Paraphrase Lattice Output sentence Figure 1: Overview of the proposed method. al. (2006) and Marton et al. (2009) augmented the translation phrase table with paraphrases to translate unknown phrases. Bond et al. (2008) and Nakov (2008) augmented the training data by paraphrasing. However, there is no work which augments input sentences by paraphrasing and represents them in lattices. 3 Paraphrase Lattice for SMT Overview of the proposed method is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. We call this lattice a paraphrase latti</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Improved Statistical Machine Translation Using Monolingual Paraphrases. In Proceedings of the European Conference on Artificial Intelligence (ECAI), pages 338–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<contexts>
<context position="9404" citStr="Och, 2003" startWordPosition="1540" endWordPosition="1541">core in case the paraphrased sentence length is shorter than the original sentence length and the language model score is unreasonably low. In experiments, we use four combinations of these features, (p), (p, l), (p, L) and (p, l, d). 3.3 Lattice decoding We use Moses (Koehn et al., 2007) as a decoder for lattice decoding. Moses is an open source SMT system which allows lattice decoding. In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. These weights are optimized using Minimum Error Rate Training (MERT) (Och, 2003). 4 Experiments In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for development and testing. We used the dev1 set for parameter tuning, the dev2 set for choosing the setting of the proposed method, which is described below, and the dev3 set for testing. The English-English paraphrase list was acquired from the EC corpus</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>