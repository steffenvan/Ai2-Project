<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.997048">
An Ontology-based Semantic Tagger for IE system
</title>
<author confidence="0.983479">
Narj`es Boufaden
</author>
<affiliation confidence="0.908356">
Department of Computer Science
Universit´e de Montr´eal
</affiliation>
<address confidence="0.814999">
Quebec, H3C 3J7 Canada
</address>
<email confidence="0.995821">
boufaden@iro.umontreal.ca
</email>
<sectionHeader confidence="0.996619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896625">
In this paper, we present a method for
the semantic tagging of word chunks ex-
tracted from a written transcription of con-
versations. This work is part of an ongo-
ing project for an information extraction
system in the field of maritime Search And
Rescue (SAR). Our purpose is to auto-
matically annotate parts of texts with con-
cepts from a SAR ontology. Our approach
combines two knowledge sources a SAR
ontology and the Wordsmyth dictionary-
thesaurus, and it uses a similarity measure
for the classification. Evaluation is carried
out by comparing the output of the system
with key answers of predefined extraction
templates.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978806451613">
This work is a part of a project aiming to imple-
ment an information extraction (IE) system in the
field of maritime Search And Rescue (SAR). It was
originally conducted by the Defense Research Es-
tablishment Valcartier (DREV) to develop a deci-
sion support tool to help in producing SAR plans
given the information extracted by the SAR IE sys-
tem from a collection of transcribed dialogs. The
goal of our project is to develop a robust approach
to extract relevant words for small-scale corpora and
transcribed speech dialogs. To achieve this task, we
developed a semantic tagger which annotates words
with domain-specific informations and a selection
process to extract or reject a word according to the
semantic tag and the context. The rationale behind
our approach, is that the relevance of a word depends
strongly on how close it is to the SAR domain and
its context of use. We believe that reasoning on se-
mantic tags instead of the word is a way of getting
around some of the problems of small-scale corpora.
In this paper, we focus on semantic tagging
based on a domain-specific ontology, a dictionary-
thesaurus and the overlapping coefficient similarity
measure (Manning and Schutze, 2001) to semanti-
cally annotate words.
We first describe the corpus (section 2), then the
overall IE system (section 3). Next we explain the
different components of the semantic tagger (section
4) and we present the preliminary results of our ex-
periments (section 5). Finally we give some direc-
tions for future work (section 6).
</bodyText>
<sectionHeader confidence="0.992415" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999879571428571">
The corpus is a collection of 95 manually tran-
scribed telephone conversations (about 39,000
words). They are mostly informative dialogs, where
two speakers (a caller C and an operator O) dis-
cuss the conditions and circumstances related to
a SAR mission. The conversations are either (1)
incident reports, such as reporting missing per-
sons or overdue boats, (2) SAR mission plans,
such as requesting an SAR airplane or coast guard
ships for a mission, or (3) debriefings, in which
case the results of the SAR mission are com-
municated. They can also be a combination of
the three kinds. Figure 1 is an excerpt of such
conversations. We can notice many disfluencies
</bodyText>
<figure confidence="0.953771514285714">
1-O:Hi, it’s Mr. Joe Blue.
 |{z }
PERSON
... an overdue boat on the South Coast of Newfoundland
3-O:We get  |{z }, missing boat ...
|{z}  |{z }  |{z }
STATUS MISSING-VESSEL MISSING-VESSEL LOCATION-TYPE
4-O:They did a radar search  |{z } for us in the area
 |{z }.
DETECTION-MEANS LOCATION
5-C:Hum, hum.
8-O:And I am wondering
 |{z }
about the possibility of outputting an Aurora in there for radar search.
 |{z }  |{z }  |{z }  |{z }
STATUS-REQUEST STATUS-REQUEST TASK SAR-AIRCRAFT-TYPE DETECTION-MEANS
...
11-O:They got
|{z}
a South East
 |{z }
to be flowing
 |{z }
there and it’s just gonna
 |{z }
be black thicker fog
 |{z }
the whole, whole South Coast .
 |{z }
STATUS DIRECTION-TYPE STATUS STATUS WEATHER-TYPE LOCATION-TYPE
12-C:OK.
...
56-:Ha, they should go to get going at first light.
 |{z }  |{z }  |{z }
STATUS STATUS TIME
</figure>
<figureCaption confidence="0.6219875">
Figure 1: An Excerpt of a conversation reporting an overdue vessel:the incident, a request for an SAR
airplane (Aurora) and the use of another SAR airplane (king Air). The words in bold are candidates for the
extraction. The tag below each bold chunk is a domain-specific information automatically generated by the
semantic tagger. Chunks like possibility, go, flowing and first light are annotated by using sense tagging
outputs. Whereas chunk such as Mr. Joe Blue, the South coast ofNewfoundland and Aurora are annotated
by the named concept extraction process.
</figureCaption>
<bodyText confidence="0.998082545454545">
(Shriberg, 1994) such as repetitions (13-O: Ha,
do, is there, is there ...) , omissions
and interruptions (3-O: we’ve been, actu-
ally had a ...). And, there is about 3% of
transcription errors such as flowing instead of
blowing (11-O Figure 1).
The underlined words are the relevant informa-
tions that will be extracted to fill in the IE tem-
plates. They are, for example, the incident, its lo-
cation, SAR resources needed for the mission, the
result of the SAR mission and weather conditions.
</bodyText>
<sectionHeader confidence="0.970261" genericHeader="method">
3 Overall system
</sectionHeader>
<bodyText confidence="0.999954222222222">
The information extraction system is a four stage
process (Figure 2). It begins with the extraction
of words that could be candidates to the extraction
(stage I). Then, the semantic tagger annotates the
extracted words (stage II). Next, given the context
and the semantic tag a word is extracted or rejected
(stage III). Finally, the extracted words are used
for the coreference resolution and to fill in IE tem-
plates (stage IV). The knowledge sources used for
the IE task are the SAR ontology and the Wordsmyth
dictionary-thesaurus).
In this section we describe the extraction of can-
didates, the SAR ontology design and the topic seg-
mentation which have already been implemented.
We leave the description of the topic labeling, the
selection of relevant words and the template genera-
tion to future work. The semantic tagger, is detailed
in section 4.
</bodyText>
<subsectionHeader confidence="0.999988">
3.1 Extraction of candidates
</subsectionHeader>
<bodyText confidence="0.87566184">
Candidates considered in the semantic tagging pro-
cess are noun phrases NP, proposition phrases PP,
verb phrases VP, adjectives ADJ and adverbs ADV.
To gather these candidates we used the Brill trans-
formational tagger (Brill, 1992) for the part-of-
speech step and the CASS partial parser for the pars-
ing step (Abney, 1994). However, because of the
disfluencies (repairs, substitutions and omissions)
encountered in the conversations, many errors oc-
curred when parsing large constructions. So, we re-
duced the set of grammatical rules used by CASS to
cover only minimal chunks and discard large con-
structions such as VP → VX NP? ADV* or noun
&apos;URL http://www.wordsmyth.net/.
Stage I Transcribed The topic segmentation system we developed is
Conversation based on a multi-knowledge source modeled by a
hidden Markov model. (N. Boufaden and al., 2001)
showed that by using linguistic features modeled by
a Hidden Markov Model, it is possible to detect
about 67% of topics boundaries.
��
Extraction
of candidates
Stage II:Semantic Tagging
y
</bodyText>
<figure confidence="0.998060117647059">
Named
Concepts oo
Extraction
Wordsmyth
Dictionary
Thesaurus
Stage III:Selecting relevant
candidates
Selection
I of relevant
words
Stage IV
_ __
��
IE Templates
generation
_ _ _
</figure>
<figureCaption confidence="0.990124666666667">
Figure 2: Main stages of the full SAR information
extraction system. Dashed squares represent pro-
cesses which are not developed in this paper.
</figureCaption>
<bodyText confidence="0.91643725">
phrases NP → NP CONT NP. The evaluation of the
semantic tagging process shows that about 14.4% of
the semantic annotation errors are partially due to
part-of-speech and parsing errors.
</bodyText>
<subsectionHeader confidence="0.999403">
3.2 Topic segmentation
</subsectionHeader>
<bodyText confidence="0.9999966">
Topic segmentation takes part to several stages in
our IE system (Figure 2). Dialogue-based IE sys-
tems have to deal with scattered information and
disfluencies. Question-answer pairs, widely used in
dialogues, are examples where information is con-
veyed through consecutive utterances. By divid-
ing the dialog into topical segment, we want to en-
sure the extraction of coherent and complete key an-
swers. Besides, topic segmentation is a valuable pre-
processing for coreference resolution, which is a dif-
ficult task in IE. Hence, for the extraction of relevant
candidates and the coreference resolution which is
part of the template generation stage (Figure 2), we
use topic segment as context instead of the utterance
or a word window of arbitrary size.
</bodyText>
<subsectionHeader confidence="0.983761">
3.3 The SAR ontology
</subsectionHeader>
<bodyText confidence="0.997638866666667">
The SAR ontology is an important component of our
IE system. We build it using domain related infor-
mations such as airplane names, locations, organi-
zations, detection means (radar search, div-
ing), status of a SAR mission (completed, con-
tinuing, planned), instance of maritime inci-
dents (drifting, overdue) and weather condi-
tions (wind, rain, fog). All these informations
were gathered from SAR manuals provided by the
National Search and Rescue Secretariat (SARMan-
ual, 2000) and from a sample of conversations (10
conversations about 10% of the corpus) to enumer-
ate the different status informations.
Our ontology was designed for two tasks of the
semantic tagging:
</bodyText>
<listItem confidence="0.905721333333333">
1. Annotate with the corresponding concept all
the extracted words that are instances of the on-
tology. This task is achieved by the named con-
cept extraction process (section 4.1).
2. For each word not in the ontology, generate
a concept-based representation composed of
</listItem>
<bodyText confidence="0.963252470588235">
similarity scores that provide information about
the closeness of the word to the SAR domain.
This is achieved by the sense tagging process
(section 4.2).
In addition to SAR manuals and corpus, we used
the IE templates given by the DREV for the de-
sign of the ontology. We used a combination of the
top-down and bottom-up design approaches (Frid-
man and Hafner, 1997). For the former, we used
the templates to enumerate the questions to be cov-
ered by the ontology and distinguish the major top
level classes (Figure 4). For the latter, we collected
the named entities along with airplane names, ves-
sel types, detection means, alert types and incidents.
The taxonomy is based on two hierarchical relations:
the is-a relation and the part-of relation. The is-a re-
lation is used for the semantic tagging. Whereas, the
</bodyText>
<figure confidence="0.9950705">
SAR
Ontology
i
Sense Tagging oo
�
�
I
I
Topic
Segmentation
Topic
Labeling �
OO_ _
I
</figure>
<table confidence="0.889071714285714">
ENT: wonder
SYL: won-der
PRO: wuhn dEr
POS: intransitive verb
INF: wondered, wondering, wonders
DEF: 1. to experience a sensation of admiration or amazement (often fol. by at):
EXA: She wondered at his bravery in combat.
SYN: marvel
SIM: gape, stare, gawk
DEF: 2. to be curious or skeptical about something:
EXA: I wonder about his truthfulness.
SYN: speculate (1)
SIM: deliberate, ponder, think, reflect, puzzle, conjecture
...
</table>
<figureCaption confidence="0.780959">
Figure 3: A fragment of the Wordsmyth dictionary-thesaurus entry of the verb wonder which is a verb
describing a STATUS-REQUEST concept (8-O Figure 1). The ENT, SYL, PRO, POS, INF, DEF, EXA, SYN,
</figureCaption>
<bodyText confidence="0.988085380952381">
SIM acronyms are respectively the entry, the syllable, the pronunciation, the part-of-speech, inflexion form,
textual definition, example, synonim words and similar words fields. To build the SAR ontology we used
the information given in the fields DEF, SYN and SIM. Whereas, to compute the similarity scores we used
only the information of the DEF field.
part-of relation will be used in the template genera-
tion process.
The overall ontology is composed of 31 concepts.
In the is-a hierarchy, each concept is represented by
a set of instances and their textual definitions. For
each instance we added a set of synonyms and simi-
lar words and their textual definitions to increase the
size of the SAR vocabulary which was found to be
insufficient to make the sense tagging approach ef-
fective.
All the synonyms and similar words along with
their definitions are provided by the Wordsmyth
dictionary-thesaurus. Figure 3 is an example of
Wordsmyth entries. Only textual definitions that
fit the SAR context were kept. This procedure in-
creases the ontology size from 480 for a total of 783
instances.
</bodyText>
<equation confidence="0.8781305">
✦ T
✦ ❵❵❵❵❵❵❵❵
✦ ✦
Physical
✘✘✘✘✘✘✘ ✑Entity
✑ ❆❆
✑ ❳❳❳❳❳❳❳
❍❍❍❍
Location Aircraft Vessel ... Detection
means
</equation>
<figureCaption confidence="0.982176">
Figure 4: Fragment of the is-a hierarchy. Location,
Aircraft ... are concepts of the ontology
</figureCaption>
<sectionHeader confidence="0.947406" genericHeader="method">
4 Semantic tagging
</sectionHeader>
<bodyText confidence="0.999970375">
The purpose of the semantic tagging process is to an-
notate words with domain-specific informations. In
our case, domain-specific informations are the con-
cepts of the SAR ontology. We want to determine
the concept Ck which is semantically the most ap-
propriate to annotate a word w. Hence, we look
for C* which has the highest similarity score for the
word w as shown in equation 1.
</bodyText>
<equation confidence="0.950232">
C* = argmax sim(w, Ck) (1)
Ck
</equation>
<bodyText confidence="0.939491529411765">
Basically, our approach is a two part process (fig-
ure 2). The named concept extraction is similar to
named entity extraction based on gazetteer (MUC,
1991). However it is a more general task since it
also recognizes entities such as, aircraft names, boat
names and detection means. It uses a finite state
automaton and the SAR ontology to recognize the
named concepts.
The sense tagging process generates a based-
concept representation for each word which couldn’t
be tagged by the named concept extraction process.
The concept-based representation is a vector of sim-
ilarity scores that measures how close is a word to
the SAR domain. As we mentioned before (section
1), the concept-based representation using similarity
Conceptual
/Entity,
</bodyText>
<equation confidence="0.4914905">
Event ... Search
Mission
</equation>
<bodyText confidence="0.999617166666667">
scores is a way to get around the problem of small-
scale corpora. Because we assume that the closer a
word is to an SAR concept, the more relevant it is,
this process is a key element for the selection of rel-
evant words (figure 2). In the next two sections, we
detail each component of the semantic tagger.
</bodyText>
<subsectionHeader confidence="0.988326">
4.1 Named concept extraction
</subsectionHeader>
<bodyText confidence="0.998764428571429">
This task, like the named entity extraction task, an-
notates words that are not instances of the ontol-
ogy. Basically, for every chunk, we look for the first
match with an instance concept. The match is based
on the word and its part-of-speech. When a match
succeeds, the semantic tag assigned is the concept
of the instance matched. The propagation of the se-
mantic tag is done by a two level automaton. The
first level propagates the semantic tag of the head
to the whole chunk. The second level deals with
cases where the first level automaton fails to recog-
nize collocations which are instances of the ontol-
ogy.
These cases occur when:
</bodyText>
<listItem confidence="0.987292428571429">
• the syntactic parser fails to produce a correct
parse. This mainly happens when the part of
speech tag isn’t correct because of disfluencies
encountered in the utterance or because of tran-
scription errors.
• the grammatical coverage is insufficient to
parse large constructions.
</listItem>
<bodyText confidence="0.999683117647059">
Whenever one of these reasons occur, the second
level automaton tries to match chunk collocations in-
stead of individual chunks. For example, the chunk
Rescue Coordination Centre which is an
organization, is an example where the parser pro-
duces two NP chunks (NP1:Rescue Coordina-
tion and NP2:Centre) instead of only one chunk.
In this case, the first level automaton fails to recog-
nize the organization. However, in the second level
automaton, the collocation NP1 NP2 is considered
for matching with an instance of the concept organi-
zation. Figure 5 shows two output examples of the
named concept extraction.
Finally, if the automaton fails to tag a chunk,
it assigns the tag OTHER if it’s an NP, OTHER-
PROPERTIES if it’s a ADJ or ADV and OTHER-
STATUS if it’s a VP.
</bodyText>
<subsectionHeader confidence="0.999081">
4.2 Sense tagging
</subsectionHeader>
<bodyText confidence="0.998001882352941">
Sense tagging takes place when a chunk is not an
instance of the ontology. In this case, the semantic
tagger looks for the most appropriate concept to an-
notate the chunk (equation 1). However, a first step
before annotation is to determine what word sense
is intended in conversations. Many studies (Resnik,
1999; Lesk, 1986; Stevenson, 2002) tackle the sense
tagging problem with approaches based on similar-
ity measures. Sense tagging is concerned with the
selection of the right word sense over all the pos-
sible word senses given some context or a particu-
lar domain. Our assumption is that when conversa-
tions are domain-specific, relevant words are too. It
means that sense tagging comes back to the prob-
lem of selecting the closer word sense with regard to
the SAR ontology. This assumption is translated in
equation 2.
</bodyText>
<equation confidence="0.464946666666667">
1Σall concepts ksim(w(l), k)
Nl
(2)
</equation>
<bodyText confidence="0.994970285714286">
Where Nl is the number of positive similarity
scores of the w(l) similarity vector. w(l) is the word
w given the word sense l. The closer word sense w*
is the highest mean computed from element of the
w(l) similarity vector.
In what follows, we explain how are generated the
similarity vectors and the result of our experiments.
</bodyText>
<subsectionHeader confidence="0.999637">
4.3 Similarity vector representation
</subsectionHeader>
<bodyText confidence="0.999713333333333">
A similarity vector is a vector where each element
is a similarity score between a word(l) (the word w
given the sense word l) and a concept Ck from the
SAR ontology. The similarity score is based on the
overlap coefficient similarity measure (Manning and
Schutze, 2001). This measure counts the number of
lemmatized content words in common between the
textual definition of the word and the concept. It is
defined as :
</bodyText>
<equation confidence="0.890766">
 |Dw(l)  |∩  |DCk |
sim(w(l), Ck) = min( |Dw(l) |,  |DCk |) (3)
</equation>
<bodyText confidence="0.9995595">
where Dw(l) and DCk are the sets of lemmatized
content words extracted from the textual definitions
</bodyText>
<figure confidence="0.9011255">
w* = argmax
w(l)
3-O:an overdue boat
VESSEL:[dt,an],[OTHER-PROPERTIES,overdue],[VESSEL,boat]
11-O:black thicker fog
WEATHER-TYPE:[COLOR-TYPE,black],[OTHER-PROPERTIES,thicker],[WEATHER-TYPE,fog]
</figure>
<figureCaption confidence="0.977558">
Figure 5: Output of the named concept extraction process. For both chunks the head semantic tag is propa-
gated to the whole chunk
</figureCaption>
<bodyText confidence="0.765895">
for each concept Ck of the SAR ontology; Ck E {incident,detection-means,status... }
for each instance Ij of Ck; Ij E {broken,missing,overdue... } for the concept incident
for each synonym Si of Ij; Si E {smach,crack... } for the instance broken
</bodyText>
<equation confidence="0.9797861">
sim(w(l), Si)= |D.(j)|∩|DS,|
min(|D.(j)|,|DS,|)
end
~vj def = (sim(w(l), S1), ... , sim(w(l), SNM
sim(w(l), Ij)=mediane(~vj)
end
~vk def = (sim(w(l), I1), ... , sim(w(l), IMJ)
sim(w(l), Ck)=max(~vk)
end
~vw(l) def = (sim(w(l), C1), ... , sim(w(l), CM))
</equation>
<figureCaption confidence="0.950792">
Figure 6: Similarity measure algorithm. Nj is the number of synonyms for the instance Ij, Mk the number
of the instance for the concept Ck and M the number of concepts in the ontology.
</figureCaption>
<bodyText confidence="0.9994965">
of w(l) and Ck. The textual definitions are provided
by the Wordsmyth thesaurus-dictionary.
However, since we have represented each concept
by a set of instances and their synonyms in the SAR
ontology (section 3.3), we modified the similarity
measure to take into account the textual definition
of concept instances and their synonyms. Basically,
we compute the similarity score between w(l) and
each synonym Si of a concept instance Ij. Then,
the similarity score between w(l) and the instance
concept Ij is the median of the resulting similarity
vector representing the similarity scores over all the
synonyms. Finally, the similarity score between a
concept Ck and w(l) is the highest similarity score
over all the concept instances. The algorithm de-
scribing these steps is given in Figure 6.
</bodyText>
<sectionHeader confidence="0.951391" genericHeader="method">
5 Preliminary results and discussion
</sectionHeader>
<bodyText confidence="0.856154875">
The evaluation of the semantic tagging process was
done on 521 extracted chunks (about 10 conversa-
tions). Only relevant chunks where considered for
Chunk Mean sim Nearest concepts
get 0.5 0.5 - status
suitable 0.53 0.53 - status
possibility 0.14 0.29-status;0.25-person
first light 0.25 0.25 - time
</bodyText>
<tableCaption confidence="0.723317">
Table 1: Output samples from the semantic tagger.
</tableCaption>
<bodyText confidence="0.998591846153846">
Mean sim is the mean of the similarity scores. It is
the selection criteria used to choose the closest word
sense.
the evaluation. The evaluation criteria is an assess-
ment about the appropriateness of the selected con-
cept to annotate the word. For example, the concept
time is appropriate for the word first light, whereas
the concept incident is not for the word detachment
which is closer to the search unit concept.
Table 2 shows the recall and precision scores for
each component and for the overall semantic tagger.
The third column shows the input error rates for each
component. The error rate in the first row comprises
</bodyText>
<table confidence="0.999672571428571">
Process Recall Precis. Inp.Err
Named concept 85.3% 94.8% 7.3%
extraction
Semantic tagger using 93.5% 72.6% 11.3%
sense tagging output
Average performance 89.4% 83.7% 8.3%
of the semantic tagger
</table>
<tableCaption confidence="0.99668">
Table 2: Precision and Recall scores for each com-
</tableCaption>
<bodyText confidence="0.982244518518519">
ponents of the semantic tagger
error rates of the part-of-speech tagger, the parsing
and the manual transcription. The error rate in the
second row are mostly part-of-speech errors. In spite
of the significant error rate, the approach based on
partial parsing is effective. The use of a minimal
grammar coverage to produce chunks reduced con-
siderably the parsing error rate.
As far as we know, no previous published work
on domain-specific WSD for speech transcriptions
has been presented, although, word sense disam-
biguation is an active research field as demonstrated
by SENSEVAL competitions2. Hence it is diffi-
cult to compare our results to similar experiments.
However, some comparative studies (Maynard and
Ananiadou, 1998; Li Shiuan and Hwee Tou, 1997)
on domain-specific well-written texts show results
ranging from 51,25% to 73,90%. Given the fact
that our corpus is composed of speech transcriptions
with the effect of increasing parsing errors, we con-
sider our results to be very encouraging.
Finally, results reported in Table 2 should be re-
garded as a basis for further improvement. In partic-
ular, the selection criteria in the sense tagging pro-
cess could be improved by considering other mea-
sures than the mean of all similarity scores as shown
in equation 2.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
6 Future work
</sectionHeader>
<bodyText confidence="0.992594631578947">
Extraction of relevant words is a hub for several ap-
plications such as question-answering and summa-
rization. It is based on semantically tagging words
and selecting the most relevant ones given the con-
text. In this paper, we developed a semantic tag-
ging approach that uses a domain-specific ontology,
a dictionary-thesaurus and the overlapping coeffi-
2URL:http://www.senseval.org/.
cient similarity measure to annotate words. We have
shown how the use of concepts to represent words
can alleviate the problem of small-scale corpora for
the selection of relevant words.
The next step in our project is the selection of rel-
evant words given the concepts annotating them and
the topic segments where they appear. Selection will
be based on a combination of a probabilistic model
taking into account the probability of observing a
concept given a word and the probability of observ-
ing that concept given a relevant topic.
</bodyText>
<sectionHeader confidence="0.998596" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999955">
We are grateful to Robert Parks at Wordsmyth orga-
nization for giving us the electronic Wordsmyth ver-
sion. Thanks to the Defense Research Establishment
Valcartier for providing us with the dialog transcrip-
tions and to National Search and rescue Secretariat
for the valuable SAR manuals.
</bodyText>
<sectionHeader confidence="0.99871" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99955056">
S. Abney. 1994. Partial parsing. Tutorial given at ANLP.
N. Boufaden G. Lapalme and Y. Bengio. 2001. Topic
segmentation : A first stage to dialog-based informa-
tion extraction. In Natural Language Processing Rim
Symposium, NLPRS’01, pages 273–280.
E. Brill. 1992. A simple rule-based part-of-speech tag-
ger. In Proceedings of the Third Conference on Ap-
plied Natural Language Processing, Trento, Italy.
Manual. Fisheries and Oceans Canada, Canadian Coast
Guard, Search and Rescue, 2000. SAR Seamanship
Reference Manual, Canadian Government Publishing,
Public Works and Government Services Canada edi-
tion, November. ISBN 0-660-18352-8.
N. Fridman and C.D. Hafner. 1997. State of the art in
ontology design. AI Magazine, 18(3):53–74.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings ofACM SIG-
DOC Conference, pages 24–26, Toronto, Canada.
C. D. Manning and H. Schutze, 2001. Foundations
of Statistical Natural Language Processing, chapter
Word Sense Disambiguation, pages 294–303. The
MIT Press Cambridge, Massachusetts London Eng-
land.
MUC,1991. Proceedings of the Third Message Under-
standing Conference. Morgan Kaufman.
D. Maynard and S. Ananiadou, 1998. 1998. Term
Sense Disambiguation using a Domain-Specific The-
saurus. In Proceedings of 1st International Confer-
ence on Language Resource and Evaluation (LREC),
Granada, Spain.
P. Resnik, 1999. Natural Language Processing using
Very Large Corpora, chapter Disambiguating Noun
Groupings with Respect to WordNet senses, pages 77–
98. S. Amstrong, K. Church, P. Isabelle, S. Manzi,
E, Tzoukermann and D. Yarowsky, kluwer Academic
Press edition.
E. Shriberg. Preliminaries to a Theory of Speech Disflu-
encies. Th`ese de doctorat, University of California at
Berkeley.
P. Li Shiuan and N. Hwee Tou 1997. Domain-Specific
Semantic Class Disambiguation Using Wordnet. In
Proceedings of the fifth Workshop on Very Large Cor-
pora, pages 56–64, Beijing and Hong Kong.
M. Stevenson. 2002. Combining Disambiguation Tech-
niques to Enrich an Ontology. In Proceedings of
the Fifteen European Conference on Artificial Intel-
ligence, workshop on Machine Learning and Natu-
ral Language Processing for Ontology Engineering,
Lyon,France.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919064">
<title confidence="0.999818">An Ontology-based Semantic Tagger for IE system</title>
<author confidence="0.995272">Narj`es Boufaden</author>
<affiliation confidence="0.999543">Department of Computer Science Universit´e de Montr´eal</affiliation>
<address confidence="0.990581">Quebec, H3C 3J7 Canada</address>
<email confidence="0.998438">boufaden@iro.umontreal.ca</email>
<abstract confidence="0.995931529411765">In this paper, we present a method for the semantic tagging of word chunks extracted from a written transcription of conversations. This work is part of an ongoing project for an information extraction system in the field of maritime Search And Rescue (SAR). Our purpose is to automatically annotate parts of texts with concepts from a SAR ontology. Our approach combines two knowledge sources a SAR ontology and the Wordsmyth dictionarythesaurus, and it uses a similarity measure for the classification. Evaluation is carried out by comparing the output of the system with key answers of predefined extraction templates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing. Tutorial given at ANLP.</title>
<date>1994</date>
<contexts>
<context position="6091" citStr="Abney, 1994" startWordPosition="1015" endWordPosition="1016">es, the SAR ontology design and the topic segmentation which have already been implemented. We leave the description of the topic labeling, the selection of relevant words and the template generation to future work. The semantic tagger, is detailed in section 4. 3.1 Extraction of candidates Candidates considered in the semantic tagging process are noun phrases NP, proposition phrases PP, verb phrases VP, adjectives ADJ and adverbs ADV. To gather these candidates we used the Brill transformational tagger (Brill, 1992) for the part-ofspeech step and the CASS partial parser for the parsing step (Abney, 1994). However, because of the disfluencies (repairs, substitutions and omissions) encountered in the conversations, many errors occurred when parsing large constructions. So, we reduced the set of grammatical rules used by CASS to cover only minimal chunks and discard large constructions such as VP → VX NP? ADV* or noun &apos;URL http://www.wordsmyth.net/. Stage I Transcribed The topic segmentation system we developed is Conversation based on a multi-knowledge source modeled by a hidden Markov model. (N. Boufaden and al., 2001) showed that by using linguistic features modeled by a Hidden Markov Model, </context>
</contexts>
<marker>Abney, 1994</marker>
<rawString>S. Abney. 1994. Partial parsing. Tutorial given at ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Boufaden G Lapalme</author>
<author>Y Bengio</author>
</authors>
<title>Topic segmentation : A first stage to dialog-based information extraction.</title>
<date>2001</date>
<booktitle>In Natural Language Processing Rim Symposium, NLPRS’01,</booktitle>
<pages>273--280</pages>
<marker>Lapalme, Bengio, 2001</marker>
<rawString>N. Boufaden G. Lapalme and Y. Bengio. 2001. Topic segmentation : A first stage to dialog-based information extraction. In Natural Language Processing Rim Symposium, NLPRS’01, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="6001" citStr="Brill, 1992" startWordPosition="998" endWordPosition="999">he Wordsmyth dictionary-thesaurus). In this section we describe the extraction of candidates, the SAR ontology design and the topic segmentation which have already been implemented. We leave the description of the topic labeling, the selection of relevant words and the template generation to future work. The semantic tagger, is detailed in section 4. 3.1 Extraction of candidates Candidates considered in the semantic tagging process are noun phrases NP, proposition phrases PP, verb phrases VP, adjectives ADJ and adverbs ADV. To gather these candidates we used the Brill transformational tagger (Brill, 1992) for the part-ofspeech step and the CASS partial parser for the parsing step (Abney, 1994). However, because of the disfluencies (repairs, substitutions and omissions) encountered in the conversations, many errors occurred when parsing large constructions. So, we reduced the set of grammatical rules used by CASS to cover only minimal chunks and discard large constructions such as VP → VX NP? ADV* or noun &apos;URL http://www.wordsmyth.net/. Stage I Transcribed The topic segmentation system we developed is Conversation based on a multi-knowledge source modeled by a hidden Markov model. (N. Boufaden </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fisheries</author>
<author>Oceans Canada</author>
</authors>
<title>Canadian Coast Guard, Search and Rescue,</title>
<date>2000</date>
<booktitle>SAR Seamanship Reference Manual, Canadian Government Publishing, Public Works and Government Services Canada edition, November. ISBN</booktitle>
<pages>0--660</pages>
<marker>Fisheries, Canada, 2000</marker>
<rawString>Manual. Fisheries and Oceans Canada, Canadian Coast Guard, Search and Rescue, 2000. SAR Seamanship Reference Manual, Canadian Government Publishing, Public Works and Government Services Canada edition, November. ISBN 0-660-18352-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fridman</author>
<author>C D Hafner</author>
</authors>
<title>State of the art in ontology design.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="9404" citStr="Fridman and Hafner, 1997" startWordPosition="1543" endWordPosition="1547">e with the corresponding concept all the extracted words that are instances of the ontology. This task is achieved by the named concept extraction process (section 4.1). 2. For each word not in the ontology, generate a concept-based representation composed of similarity scores that provide information about the closeness of the word to the SAR domain. This is achieved by the sense tagging process (section 4.2). In addition to SAR manuals and corpus, we used the IE templates given by the DREV for the design of the ontology. We used a combination of the top-down and bottom-up design approaches (Fridman and Hafner, 1997). For the former, we used the templates to enumerate the questions to be covered by the ontology and distinguish the major top level classes (Figure 4). For the latter, we collected the named entities along with airplane names, vessel types, detection means, alert types and incidents. The taxonomy is based on two hierarchical relations: the is-a relation and the part-of relation. The is-a relation is used for the semantic tagging. Whereas, the SAR Ontology i Sense Tagging oo � � I I Topic Segmentation Topic Labeling � OO_ _ I ENT: wonder SYL: won-der PRO: wuhn dEr POS: intransitive verb INF: w</context>
</contexts>
<marker>Fridman, Hafner, 1997</marker>
<rawString>N. Fridman and C.D. Hafner. 1997. State of the art in ontology design. AI Magazine, 18(3):53–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings ofACM SIGDOC Conference,</booktitle>
<pages>24--26</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="15413" citStr="Lesk, 1986" startWordPosition="2562" endWordPosition="2563">ith an instance of the concept organization. Figure 5 shows two output examples of the named concept extraction. Finally, if the automaton fails to tag a chunk, it assigns the tag OTHER if it’s an NP, OTHERPROPERTIES if it’s a ADJ or ADV and OTHERSTATUS if it’s a VP. 4.2 Sense tagging Sense tagging takes place when a chunk is not an instance of the ontology. In this case, the semantic tagger looks for the most appropriate concept to annotate the chunk (equation 1). However, a first step before annotation is to determine what word sense is intended in conversations. Many studies (Resnik, 1999; Lesk, 1986; Stevenson, 2002) tackle the sense tagging problem with approaches based on similarity measures. Sense tagging is concerned with the selection of the right word sense over all the possible word senses given some context or a particular domain. Our assumption is that when conversations are domain-specific, relevant words are too. It means that sense tagging comes back to the problem of selecting the closer word sense with regard to the SAR ontology. This assumption is translated in equation 2. 1Σall concepts ksim(w(l), k) Nl (2) Where Nl is the number of positive similarity scores of the w(l) </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings ofACM SIGDOC Conference, pages 24–26, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<date>2001</date>
<booktitle>Foundations of Statistical Natural Language Processing, chapter Word Sense Disambiguation,</booktitle>
<pages>294--303</pages>
<publisher>The MIT Press</publisher>
<location>Cambridge, Massachusetts London England.</location>
<contexts>
<context position="2009" citStr="Manning and Schutze, 2001" startWordPosition="325" endWordPosition="328">ic tagger which annotates words with domain-specific informations and a selection process to extract or reject a word according to the semantic tag and the context. The rationale behind our approach, is that the relevance of a word depends strongly on how close it is to the SAR domain and its context of use. We believe that reasoning on semantic tags instead of the word is a way of getting around some of the problems of small-scale corpora. In this paper, we focus on semantic tagging based on a domain-specific ontology, a dictionarythesaurus and the overlapping coefficient similarity measure (Manning and Schutze, 2001) to semantically annotate words. We first describe the corpus (section 2), then the overall IE system (section 3). Next we explain the different components of the semantic tagger (section 4) and we present the preliminary results of our experiments (section 5). Finally we give some directions for future work (section 6). 2 Corpus The corpus is a collection of 95 manually transcribed telephone conversations (about 39,000 words). They are mostly informative dialogs, where two speakers (a caller C and an operator O) discuss the conditions and circumstances related to a SAR mission. The conversati</context>
<context position="16584" citStr="Manning and Schutze, 2001" startWordPosition="2759" endWordPosition="2762">s the number of positive similarity scores of the w(l) similarity vector. w(l) is the word w given the word sense l. The closer word sense w* is the highest mean computed from element of the w(l) similarity vector. In what follows, we explain how are generated the similarity vectors and the result of our experiments. 4.3 Similarity vector representation A similarity vector is a vector where each element is a similarity score between a word(l) (the word w given the sense word l) and a concept Ck from the SAR ontology. The similarity score is based on the overlap coefficient similarity measure (Manning and Schutze, 2001). This measure counts the number of lemmatized content words in common between the textual definition of the word and the concept. It is defined as : |Dw(l) |∩ |DCk | sim(w(l), Ck) = min( |Dw(l) |, |DCk |) (3) where Dw(l) and DCk are the sets of lemmatized content words extracted from the textual definitions w* = argmax w(l) 3-O:an overdue boat VESSEL:[dt,an],[OTHER-PROPERTIES,overdue],[VESSEL,boat] 11-O:black thicker fog WEATHER-TYPE:[COLOR-TYPE,black],[OTHER-PROPERTIES,thicker],[WEATHER-TYPE,fog] Figure 5: Output of the named concept extraction process. For both chunks the head semantic tag </context>
</contexts>
<marker>Manning, Schutze, 2001</marker>
<rawString>C. D. Manning and H. Schutze, 2001. Foundations of Statistical Natural Language Processing, chapter Word Sense Disambiguation, pages 294–303. The MIT Press Cambridge, Massachusetts London England.</rawString>
</citation>
<citation valid="false">
<booktitle>MUC,1991. Proceedings of the Third Message Understanding Conference.</booktitle>
<publisher>Morgan Kaufman.</publisher>
<marker></marker>
<rawString>MUC,1991. Proceedings of the Third Message Understanding Conference. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Maynard</author>
<author>S Ananiadou</author>
</authors>
<title>Term Sense Disambiguation using a Domain-Specific Thesaurus.</title>
<date>1998</date>
<booktitle>In Proceedings of 1st International Conference on Language Resource and Evaluation (LREC),</booktitle>
<location>Granada,</location>
<contexts>
<context position="20681" citStr="Maynard and Ananiadou, 1998" startWordPosition="3406" endWordPosition="3409">ranscription. The error rate in the second row are mostly part-of-speech errors. In spite of the significant error rate, the approach based on partial parsing is effective. The use of a minimal grammar coverage to produce chunks reduced considerably the parsing error rate. As far as we know, no previous published work on domain-specific WSD for speech transcriptions has been presented, although, word sense disambiguation is an active research field as demonstrated by SENSEVAL competitions2. Hence it is difficult to compare our results to similar experiments. However, some comparative studies (Maynard and Ananiadou, 1998; Li Shiuan and Hwee Tou, 1997) on domain-specific well-written texts show results ranging from 51,25% to 73,90%. Given the fact that our corpus is composed of speech transcriptions with the effect of increasing parsing errors, we consider our results to be very encouraging. Finally, results reported in Table 2 should be regarded as a basis for further improvement. In particular, the selection criteria in the sense tagging process could be improved by considering other measures than the mean of all similarity scores as shown in equation 2. 6 Future work Extraction of relevant words is a hub fo</context>
</contexts>
<marker>Maynard, Ananiadou, 1998</marker>
<rawString>D. Maynard and S. Ananiadou, 1998. 1998. Term Sense Disambiguation using a Domain-Specific Thesaurus. In Proceedings of 1st International Conference on Language Resource and Evaluation (LREC), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Natural Language Processing using Very Large Corpora, chapter Disambiguating Noun Groupings with Respect to WordNet senses,</title>
<date>1999</date>
<pages>77--98</pages>
<publisher>kluwer Academic Press</publisher>
<note>edition.</note>
<contexts>
<context position="15401" citStr="Resnik, 1999" startWordPosition="2560" endWordPosition="2561">for matching with an instance of the concept organization. Figure 5 shows two output examples of the named concept extraction. Finally, if the automaton fails to tag a chunk, it assigns the tag OTHER if it’s an NP, OTHERPROPERTIES if it’s a ADJ or ADV and OTHERSTATUS if it’s a VP. 4.2 Sense tagging Sense tagging takes place when a chunk is not an instance of the ontology. In this case, the semantic tagger looks for the most appropriate concept to annotate the chunk (equation 1). However, a first step before annotation is to determine what word sense is intended in conversations. Many studies (Resnik, 1999; Lesk, 1986; Stevenson, 2002) tackle the sense tagging problem with approaches based on similarity measures. Sense tagging is concerned with the selection of the right word sense over all the possible word senses given some context or a particular domain. Our assumption is that when conversations are domain-specific, relevant words are too. It means that sense tagging comes back to the problem of selecting the closer word sense with regard to the SAR ontology. This assumption is translated in equation 2. 1Σall concepts ksim(w(l), k) Nl (2) Where Nl is the number of positive similarity scores </context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>P. Resnik, 1999. Natural Language Processing using Very Large Corpora, chapter Disambiguating Noun Groupings with Respect to WordNet senses, pages 77– 98. S. Amstrong, K. Church, P. Isabelle, S. Manzi, E, Tzoukermann and D. Yarowsky, kluwer Academic Press edition.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies. Th`ese de doctorat,</title>
<institution>University of California at Berkeley.</institution>
<marker>Shriberg, </marker>
<rawString>E. Shriberg. Preliminaries to a Theory of Speech Disfluencies. Th`ese de doctorat, University of California at Berkeley.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Li Shiuan</author>
<author>N Hwee</author>
</authors>
<title>Tou 1997. Domain-Specific Semantic Class Disambiguation Using Wordnet.</title>
<booktitle>In Proceedings of the fifth Workshop on Very Large Corpora,</booktitle>
<pages>56--64</pages>
<location>Beijing</location>
<marker>Shiuan, Hwee, </marker>
<rawString>P. Li Shiuan and N. Hwee Tou 1997. Domain-Specific Semantic Class Disambiguation Using Wordnet. In Proceedings of the fifth Workshop on Very Large Corpora, pages 56–64, Beijing and Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
</authors>
<title>Combining Disambiguation Techniques to Enrich an Ontology.</title>
<date>2002</date>
<booktitle>In Proceedings of the Fifteen European Conference on Artificial Intelligence, workshop on Machine Learning and Natural Language Processing for Ontology Engineering,</booktitle>
<location>Lyon,France.</location>
<contexts>
<context position="15431" citStr="Stevenson, 2002" startWordPosition="2564" endWordPosition="2565">nce of the concept organization. Figure 5 shows two output examples of the named concept extraction. Finally, if the automaton fails to tag a chunk, it assigns the tag OTHER if it’s an NP, OTHERPROPERTIES if it’s a ADJ or ADV and OTHERSTATUS if it’s a VP. 4.2 Sense tagging Sense tagging takes place when a chunk is not an instance of the ontology. In this case, the semantic tagger looks for the most appropriate concept to annotate the chunk (equation 1). However, a first step before annotation is to determine what word sense is intended in conversations. Many studies (Resnik, 1999; Lesk, 1986; Stevenson, 2002) tackle the sense tagging problem with approaches based on similarity measures. Sense tagging is concerned with the selection of the right word sense over all the possible word senses given some context or a particular domain. Our assumption is that when conversations are domain-specific, relevant words are too. It means that sense tagging comes back to the problem of selecting the closer word sense with regard to the SAR ontology. This assumption is translated in equation 2. 1Σall concepts ksim(w(l), k) Nl (2) Where Nl is the number of positive similarity scores of the w(l) similarity vector.</context>
</contexts>
<marker>Stevenson, 2002</marker>
<rawString>M. Stevenson. 2002. Combining Disambiguation Techniques to Enrich an Ontology. In Proceedings of the Fifteen European Conference on Artificial Intelligence, workshop on Machine Learning and Natural Language Processing for Ontology Engineering, Lyon,France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>