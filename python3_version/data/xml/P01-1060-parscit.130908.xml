<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.994009">
Parse Forest Computation of Expected Governors
</title>
<author confidence="0.99479">
Helmut Schmid
</author>
<affiliation confidence="0.9956195">
Institute for Computational Linguistics
University of Stuttgart
</affiliation>
<address confidence="0.8841415">
Azenbergstr. 12
70174 Stuttgart, Germany
</address>
<email confidence="0.997334">
schmid@ims.uni-stuttgart.de
</email>
<author confidence="0.979224">
Mats Rooth
</author>
<affiliation confidence="0.995805">
Department of Linguistics
Cornell University
</affiliation>
<address confidence="0.521212">
Morrill Hall
Ithaca, NY 14853, USA
</address>
<email confidence="0.998968">
mats@cs.cornell.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954294117647">
In a headed tree, each terminal word
can be uniquely labeled with a gov-
erning word and grammatical relation.
This labeling is a summary of a syn-
tactic analysis which eliminates detail,
reflects aspects of semantics, and for
some grammatical relations (such as
subject of finite verb) is nearly un-
controversial. We define a notion
of expected governor markup, which
sums vectors indexed by governors and
scaled by probabilistic tree weights.
The quantity is computed in a parse for-
est representation of the set of tree anal-
yses for a given sentence, using vector
sums and scaling by inside probability
and flow.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.866522564102564">
A labeled headed tree is one in which each non-
terminal vertex has a distinguished head child,
and in the usual way non-terminal nodes are la-
beled with non-terminal symbols (syntactic cat-
egories such as NP) and terminal vertices are
labeled with terminal symbols (words such as
The governor algorithm was designed and implemented
in the Reading Comprehension research group in the 2000
Workshop on Language Engineering at Johns Hopkins Uni-
versity. Thanks to Marc Light, Ellen Riloff, Pranav Anand,
Brianne Brown, Eric Breck, Gideon Mann, and Mike Thelen
for discussion and assistance. Oral presentations were made
at that workshop in August 2000, and at the University of
Sussex in January 2001. Thanks to Fred Jelinek, John Car-
roll, and other members of the audiences for their comments.
reads).&apos; We work with syntactic trees in which
terminals are in addition labeled with uninflected
word forms (lemmas) derived from the lexicon.
By percolating lemmas up the chains of heads,
each node in a headed tree may be labeled with
a lexical head. Figure 1 is an example, where lex-
ical heads are written as subscripts. We use the
notation for the lexical head of a vertex ,
and for the ordinary category or word label
of .
The governor label for a terminal vertex in
such a labeled tree is a triple which represents
the syntactic and lexical environment at the top
of the chain of vertices headed by . Where is
the maximal vertex of which is a head vertex,
and is the parent of , the governor label for
&apos;Headed trees maybe constructed as tree domains, which
are sets of addresses of vertices. 0 is used as the relative ad-
dress of the head vertex, negative integers are used as relative
addresses of child vertices before the head, and positive in-
tegers are used as relative addresses of child vertices after
the head. A headed tree domain is a set of finite sequences
of integers such that (i) if , then ; (ii) if and
or ,then
</bodyText>
<figure confidence="0.998916235294118">
S
NP
Peter
VP
V
reads
D
every
NP
N
paper
NP
PP:on
P:on
NP
on N
markup
</figure>
<figureCaption confidence="0.885557666666667">
Figure 1: A tree with percolated lexical heads.
.
Figure 2: Governor labels for the terminals in the
tree of Figure 1. For the head of the sentence,
special symbols startc and startw are used as the
parent category and parent lexical governor.
</figureCaption>
<bodyText confidence="0.999167285714286">
is the tuple .2 Governor labels
for the example tree are given in Figure 2.
As observed in Chomsky (1965), grammatical
relations such as subject and object may be re-
constructed as ordered pairs of category labels,
such as NP,S for subject. So, a governor label
encodes a grammatical relation and a governing
lexical head.
Given a unique tree structure for a sentence,
governor markup may be read off the tree. How-
ever, in view of the fact that robust broad coverage
parsers frequently deliver thousands, millions, or
thousands of millions of analyses for sentences of
free text, basing annotation on a unique tree (such
as the most probable tree analysis generated by a
probabilistic grammar) appears arbitrary.
Note that different trees may produce the same
governor labels for a given terminal position.
Suppose for instance that the yield of the tree in
Figure 1 has a different tree analysis in which the
PP is a child of the VP, rather than NP. In this
case, just as in the original tree, the label for the
fourth terminal position (with word label paper)
is NP,VP,read. Supposing that there are only
two tree analyses, this label can be assigned to the
fourth word with certainty, in the face of syntac-
tic ambiguity. The algorithm we will define pools
governor labels in this way.
</bodyText>
<sectionHeader confidence="0.983596" genericHeader="method">
2 Expected Governors
</sectionHeader>
<bodyText confidence="0.922584666666667">
Suppose that a probabilistic grammar licenses
headed tree analyses for a sentence ,
and assigns them probabilistic weights .
</bodyText>
<footnote confidence="0.92247">
2In a headed tree domain, is a head of if is of the
</footnote>
<table confidence="0.977355055555555">
form for .
that NP SC deprive .95 .99
would MD2 VFP deprive .98 1
deprive S STARTC startw 1 1
all DETPL2 NC student .83 .98
beginning NSG1 NPL1 student .75 .98
students NP VFP deprive .82 .98
NP VGP begin .16
of PP NP student .53
PP VFP deprive .38 .99
their DETPL2 NC lunch .92 .98
high ADJMOD NPL2 lunch .78 .23
ADJMOD NSG2 school .15 .76
school NCHAIN NPL1 lunch .16
NSG1 NPL1 lunch .76 .98
lunches NP PP of .91 .98
. PERC S deprive .88 .86
PERC X deprive .14
</table>
<figureCaption confidence="0.9772895">
Figure 3: Expected governors in the sentence That would
deprive all beginning students of their high school lunches.
</figureCaption>
<bodyText confidence="0.997195727272727">
For a label in column 2, column 3 gives as com-
puted with a PCFG weighting of trees, and column 4 gives
as computed with a head-lexicalized weighting of
trees. Values below 0.1 are omitted. According to the lexi-
calized model, the PP headed by of probably attaches to VFP
(finite verb phrase) rather than NP.
Let be the governor labels for word
position determined by respectively.
We define a scheme which divides a count of 1
among the different governor labels.
For a given governor tuple , let
</bodyText>
<equation confidence="0.523335">
def (1)
</equation>
<bodyText confidence="0.981663125">
The definition sums the probabilistic weights of
trees with markup , and normalizes by the sum
of the probabilities of all tree analyses of .
The definition may be justified as follows. We
work with a markup space
be the function which maps to 1, and to 0
for . We define a random variate
Trees
</bodyText>
<footnote confidence="0.690271">
which maps a tree to , where is the gov-
ernor markup for word position which is de-
</footnote>
<figure confidence="0.971172142857143">
position word governor label
1 Peter NP,S,read
2 reads S,STARTC,startw
3 every D,NP,paper
4 paper NP,VP,read
5 on P:ON,PP:ON,markup
6 markup NP,PP:ON,paper
</figure>
<bodyText confidence="0.994353185185185">
,
where is the set of category labels and is the
set of lemma labels. For a given markup triple ,
let
termined by tree . The random variate is de-
fined on labeled trees licensed by the probabilistic
grammar. Note that is a vector space
(with pointwise sums and scalar products), so that
expectations and conditional expectations may be
defined. In these terms, is the conditional ex-
pectation of , conditioned on the yield being .
This definition, instead of a single governor la-
bel for a given word position, gives us a set of
pairs of a markup and a real number in
[0,1], such that the real numbers in the pairs sum
to 1. In our implementation (which is based on
Schmid (2000a)), we use a cutoff of 0.1, and print
only indices where is above the cutoff.
Figure 3 is an example.
A direct implementation of the above definition
using an iteration over trees to compute would
be unusable because in the robust grammar of En-
glish we work with, the number of tree analyses
for a sentence is frequently large, greater than
for about 1/10 of the sentences in the British Na-
tional Corpus. We instead calculate in a parse
forest representation of a set of tree analyses.
</bodyText>
<sectionHeader confidence="0.992744" genericHeader="method">
3 Parse Forests
</sectionHeader>
<bodyText confidence="0.994621375">
A parse forest (see also Billot and Lang (1989))
in labeled grammar notation is a tu-
ple where
is a context free gram-
mar (consisting of non-terminals , terminals
, rules , and a start symbol ) and
is a function which maps elements of
to non-terminals in an underlying grammar
and elements of to termi-
nals in . By using on symbols on the left
hand and right hand sides of a parse forest rule,
can be extended to map the set of parse forest
rules to the set of underlying grammar rules
. is also extended to map trees licensed
by the parse forest grammar to trees licensed by
the underlying grammar. An example is given in
figure 4.
Where , let be the set of
trees licensed by which have
root symbol in the case of a symbol, and the set
of trees which have as the rule expanding the
root in the case or a rule. is defined to be
the multiset image of under . is
the multiset of inside trees represented by parse
</bodyText>
<equation confidence="0.974051142857143">
S NP VP
VP V NP
VP VP PP
NP NP PP
NP D N
PP P NP
VP V NP
</equation>
<figureCaption confidence="0.994901">
Figure 4: Rule set of a labeled grammar rep-
</figureCaption>
<bodyText confidence="0.994365">
resenting two tree analyses of John reads every
paper on markup. The labeling function drops
subscripts, so that VP VP.
forest symbol or rule .3 Let be the set of
trees in which contain as a symbol or
use as a rule. is defined to be the multiset
image of under . is the multiset
of complete trees represented by the parse forest
symbol or rule .
Where is a probability function on trees li-
censed by the underlying grammar and is a sym-
bol or rule in ,
</bodyText>
<equation confidence="0.914792">
def (2)
def (3)
</equation>
<bodyText confidence="0.999760769230769">
is called the inside probability for and
is called the flow for .4
Parse forests are often constructed so that all
inside trees represented by a parse forest nonter-
minal have the same span, as well as the
same parent category. To deal with headedness
and lexicalization of a probabilistic grammar, we
construct parse forests so that, in addition, all in-
side trees represented by a parse forest nontermi-
nal have the same lexical head. We add to the la-
beled grammar a function which labels parse
forest symbols with lexical heads. In our imple-
mentation, an ordinary context free parse forest is
</bodyText>
<footnote confidence="0.874683666666667">
3We use multisets rather than set images to achieve cor-
rectness of the inside algorithm in cases where represents
some tree more than once, something which is possible given
the definition of labeled grammars. A correct parser pro-
duces a parse forest which represents every parse for the in-
put sentence exactly once.
4These quantities can be given probabilistic interpreta-
tions and/or definitions, for instance with reference to con-
ditionally expected rule frequencies for flow.
</footnote>
<figure confidence="0.9826102">
NP N
NP Peter
V reads
D every
N paper
P on
N markup
PF-INSIDE( )
1 Initial. float array 0
2 for
3 do
4 for in in bottom-up order
5 do
6 lhs lhs
7 return
</figure>
<figureCaption confidence="0.998801">
Figure 5: Inside algorithm.
</figureCaption>
<figure confidence="0.7662165">
PF-FLOW( )
1 Initial. float array 0
2
3 for in in top-down order
4 do lhs
5 for in rhs
6 do
7 return
</figure>
<figureCaption confidence="0.999813">
Figure 6: Flow algorithm.
</figureCaption>
<bodyText confidence="0.99974553125">
first constructed by tabular parsing, and then in a
second pass parse forest symbols are split accord-
ing to headedness. Such an algorithm is shown
in appendix B. This procedure gives worst case
time and space complexity which is proportional
to the fifth power of the length of the sentence.
See Eisner and Satta (1999) for discussion and an
algorithm with time and space requirements pro-
portional to the fourth power of the length of the
input sentence in the worst case. In practical ex-
perience with broad-coverage context free gram-
mars of several languages, we have not observed
super-cubic average time or space requirements
for our implementation. We believe this is be-
cause, for our grammars and corpora, there is lim-
ited ambiguity in the position of the head within
a given category-span combination.
The governor algorithm stated in the next sec-
tion refers to headedness in parse forest rules.
This can be represented by constructing parse for-
est rules (as well as ordinary grammar rules) with
headed tree domains of depth one.5 Where is
a parse forest symbol on the right hand side of a
parse forest rule , we will simply state the con-
dition “ is the head of ”.
The flow and governor algorithms stated be-
low call an algorithm PF-INSIDE which
computes inside probabilities in , where is a
function giving probability parameters for the un-
derlying grammar. Any probability weighting of
trees may be used which allows inside probabil-
ities to be computed in parse forests. The inside
</bodyText>
<footnote confidence="0.956172666666667">
5See footnote 1. Constructed in this way, the first rule in
parse forest in Figure 4 has domain , and labeling
function S NP VP . When parse forest
rules are mapped to underlying grammar rules, the domain is
preserved, so that applied to the parse forest rule just de-
scribed is the tree with domain and label function
</footnote>
<equation confidence="0.286789">
S NP VP . is the empty string.
</equation>
<bodyText confidence="0.985927909090909">
algorithm for ordinary PCFGs is given in figure
5. The parameter maps the set of underlying
grammar rules which is the image of on
to reals, with the interpretation of rule proba-
bilities. In step 5, maps the parse forest rule
to a grammar rule which is the argument
of . The functions lhs and rhs map rules to their
left hand and right hand sides, respectively.
Given an inside algorithm, the flow may be
computed by the flow algorithm in Figure 6, or
by the inside-outside algorithm.
</bodyText>
<sectionHeader confidence="0.996633" genericHeader="method">
4 Governors Algorithm
</sectionHeader>
<bodyText confidence="0.96895235">
The governor algorithm annotates parse forest
symbols and rules with functions from governor
labels to real numbers. Let be a tree in the parse
forest grammar, let be a symbol in, let be the
maximal symbol in of which is a head, or
itself if is a non-head child of its parent in, and
let be the parent of in. Recall that
(4)
is a vector mapping the markup triple
to 1 and other markups
to 0. We have constructed parse forests such
that agrees with the
governor label for the lexical head of the node
corresponding to in
.
A parse forest tree and symbol in thus de-
termine the vector (4), where and are defined
as above. Call the vector determined in this way
. Where is parse forest symbol in and
is a parse forest rule in , let
</bodyText>
<figure confidence="0.955690384615384">
def (5)
PF-GOVERNORS( )
1 PF-INSIDE
2 PF-FLOW
3 Initialize array to empty maps from governor labels to float
4 startcstartw
5 for in in top-down order
6 do lhs
7 for in rhs
8 do if is the head of
9 then
10 else
11 return
</figure>
<figureCaption confidence="0.99889">
Figure 7: Parse forest computation of governor vector.
</figureCaption>
<equation confidence="0.6575755">
def lhs
(6)
</equation>
<bodyText confidence="0.994714118421053">
Assuming that is
a parse forest representing each tree analysis for
a sentence exactly once, the quantity for termi-
nal position (as defined in section 1) is found
by summing for terminal symbols in
which have string position .6
The algorithm PF-GOVERNORS is stated in Fig-
ure 3. Working top down, if fills in an array
which is supposed to agree with the quan-
tity defined above. Scaled governor vectors
are created for non-head children in step 10, and
summed down the chain of heads in step 9. In
step 6, vectors are divided in proportion to inside
probabilities (just as in the flow algorithm), be-
cause the set of complete trees for the left hand
side of are partitioned among the parse forest
rules which expand the left hand side of .
Consider a parse forest rule , and a parse for-
est symbol on its right hand side which is not
the head of . In each tree in ,is the top
of a chain of heads, because is a non-head child
in rule . Instep 10, the governor tuple describing
the syntactic environment of in trees in
(or rather, their images under ) is constructed
TThis procedure requires that symbols in correspond
to a unique string position, something which is not enforced
by our definition of parse forests. Indeed, such cases may
arise ifparse forest symbols are constructed as pairs of gram-
mar symbols and strings (Tendeau, 1998) rather than pairs
of grammar symbols and spans. Our parser constructs parse
forests organized according to span.
as . The scalar multi-
plier is
the relative weight of trees in . This is ap-
propriate because as defined in equation (5)
is to be scaled by the relative weight of trees in
.
In line 9 of the algorithm, is summed into the
head child . There is no scaling, because every
tree in is a tree in .
A probability parameter vector is used in the
inside algorithm. In our implementation, we can
use either a probabilistic context free grammar, or
a lexicalized context free grammar which condi-
tions rules on parent category and parent lexical
head, and conditions the heads of non-head chil-
dren on child category, parent category, and par-
ent head (Eisner, 1997; Charniak, 1995; Carroll
and Rooth, 1998). The requisite information is di-
rectly represented in our parse forests by and
. Thus the call to PF-INSIDE in line 1 of PF-
GOVERNORS may involve either a computation
of PCFG inside probabilities, or head-lexicalized
inside probabilities. However, in both cases the
algorithm requires that the parse forest symbols
be split according to heads, because of the ref-
erence to in line 10. Construction of head-
marked parse forests is presented in the appendix.
The LoPar parser (Schmid, 2000a) on which
our implementation of the governor algorithm is
based represents the parse forest as a graph with
at most binary branching structure. Nodes with
more than two daughter nodes in a conventional
parse forest are replaced with a right-branching
tree structure and common sub-trees are shared
between different analyses. The worst-case space
complexity of this representation is cubic (cmp.
Billot and Lang (1989)).
LoPar already provided functions for the com-
putation of the head-marked parse forest, for the
flow computation and for traversing the parse for-
est in depth-first and topologically-sorted order
(see Cormen et al. (1994)). So it was only neces-
sary to add functions for data initialization, for the
computation of the governor vector at each node
and for printing the result.
</bodyText>
<sectionHeader confidence="0.650933" genericHeader="method">
5 Pooling of grammatical relations
</sectionHeader>
<bodyText confidence="0.999946644444444">
The governor labels defined above are derived
from the specific symbols of a context free gram-
mar. In contrast, according to the general markup
methodology of current computational linguis-
tics, labels should not be tied to a specific gram-
mar and formalism. The same markup labels
should be produced by different systems, making
it possible to substitute one system for another,
and to compare systems using objective tests.
Carroll et al. (1998) and Carroll et al. (1999)
propose a system of grammatical relation markup
to which we would like to assimilate our proposal.
As grammatical relation symbols, they use atomic
labels such as dobj (direct object) an ncsubj (non-
clausal subject). The labels are arranged in a hier-
archy, with for instance subj having subtypes nc-
subj, xsubj, and csubj.
There is another problem with the labels we
have used so far. Our grammar codes a variety
of features, such as the feature VFORM on verb
projections. As a result, instead of a single object
grammatical relationNP,VP, we have grammati-
cal relations NP,VP.N,NP,VP.FIN,NP,VP.TO,
NP,VP.BASE, and so forth. This may result in
frequency mass being split among different but
similar labels. For instance, a verb phrase will
have read every paper might have some analy-
ses in which read is the head of a base form
VP and paper is the head of the object of read,
and others where read is a head of a finite form
VP, and paper is the head of the object of read.
In this case, frequencies would be split between
NP,VP.BASE,read andNP,VP.FIN,read as gov-
ernor labels for paper.
To address these problems, we employ a pool-
ing function which maps pairs of categories
to symbols such as ncsubj or obj. The gover-
nor tuple is then replaced by
in the definition of the
governor label for a terminal vertex . Line 10
of PF-GOVERNORS is changed to
More flexibility could be gained by using a rule
and the address of a constituent on the right hand
side as arguments of . This would allow the
following assignments.
</bodyText>
<equation confidence="0.83363275">
VP.FIN VC.FIN’ NP NP dobj
VP.FIN VC.FIN’ NP NP obj2
VP.FIN VC.FIN’ VP.TO xcomp
VP.FIN VP.FIN’ VP.TO xmod
</equation>
<bodyText confidence="0.999061083333333">
The head of a rule is marked with a prime. In the
first pair, the objects in double object construction
are distinguished using the address. In each case,
the child-parent category pair is NP,VP.FIN, so
that the original proposal could not distinguish the
grammatical relations. In the second pair, a VP.TO
argument is distinguished from a VP.TO modifier
using the category of the head. In each case, the
child-parent category pair isVP.TO,VP.FIN. No-
tice that in Line 10 of PF-GOVERNORS, the rule
is available, so that the arguments of could
be changed in this way.
</bodyText>
<sectionHeader confidence="0.99847" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99995492">
The governor algorithm was designed as a com-
ponent of Spot, a free-text question answering
system. Current systems usually extract a set
of candidate answers (e.g. sentences), score
them and return the n highest-scoring candidates
as possible answers. The system described in
Harabagiu et al. (2000) scores possible answers
based on the overlap in the semantic represen-
tations of the question and the answer candi-
dates. Their semantic representation is basically
identical to the head-head relations computed by
the governor algorithm. However, Harabagiu
et al. extract this information only from maxi-
mal probability parses whereas the governor al-
gorithm considers all analyses of a sentence and
returns all possible relations weighted with esti-
mated frequencies. Our application in Spot works
as follows: the question is parsed with a spe-
cialized question grammar, and features including
the governor of the trace are extracted from the
question. Governors are among the features used
for ranking sentences, and answer terms within
sentences. In collaboration with Pranav Anand
and Eric Breck, we have incorporated governor
markup in the question answering prototype, but
not debugged or evaluated it.
Expected governor markup summarizes syn-
tactic structure in a weighted parse forest which
is the product of exhaustive parsing and inside-
outside computation. This is a strategy of
dumbing down the product of computation-
ally intensive statistical parsing into unstructured
markup. Estimated frequency computations in
parse forests have previously been applied to
tagging and chunking (Schulte im Walde and
Schmid, 2000). Governor markup differs in that
it is reflective of higher-level syntax. The strat-
egy has the advantage, in our view, that it allows
one to base markup algorithms on relatively so-
phisticated grammars, and to take advantage of
the lexically sensitive probabilistic weighting of
trees which is provided by a lexicalized probabil-
ity model.
Localizing markup on the governed word in-
creases pooling of frequencies, because the span
of the phrase headed by the governed item is
ignored. This idea could be exploited in other
markup tasks. In a chunking task, categories and
heads of chunks could be identified, rather than
categories and boundaries.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.997015491803279">
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings ofthe 27th Annual Meeting of the ACL, Univer-
sity of British Columbia, Vancouver, B.C., Canada.
Glenn Carroll and Mats Rooth. 1998. Valence induc-
tion with a head-lexicalized PCFG. In Proceedings
of Third Conference on Empirical Methods in Nat-
ural Language Processing, Granada, Spain.
John Carroll, Antonio Sanfilippo, and Ted Briscoe.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the International Confer-
ence ofLanguage Resources and Evaluation, pages
447–454, Granada, Spain.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Pro-
ceedings of the EACL99 workshop on Linguisti-
cally Interpreted Corpora (LINC), Bergen, Norway,
June.
Eugene Charniak. 1993. Statistical Language Learn-
ing. The MIT Press, Cambridge, Massachusetts.
Eugene Charniak. 1995. Parsing with context-
free grammars and word statistics. Technical Re-
port CS-95-28, Department of Computer Science,
Brown University.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. M.I.T. Press, Cambridge, MA.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1994. Introduction to Algo-
rithms. The MIT Press, Cambridge, Massachusetts.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’99), College Park, MD.
Jason Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the 4th
international Workshop on Parsing Technologies,
Cambridge, MA.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Gîrju, V. Rus, and
P. Morarescu. 2000. Falcon: Boosting knowledge
for answer engines. In Proceedings of the Ninth
Text REtrieval Conference (TREC 9), Gaithersburg,
MD, USA, November.
Helmut Schmid. 2000a. LoPar: Design and Imple-
mentation. Number 149 in Arbeitspapiere des Son-
derforschungsbereiches 340. Institute for Computa-
tional Linguistics, University of Stuttgart.
Helmut Schmid. 2000b. Lopar man pages. Insti-
tute for Computational Linguistics, University of
Stuttgart.
Sabine Schulte im Walde and Helmut Schmid. 2000.
Robust german noun chunking with a probabilistic
context-free grammar. In Proceedings of the 18th
International Conference on Computational Lin-
guistics, pages 726–732, Saarbrücken, Germany,
August.
Frederic Tendeau. 1998. Computing abstract decora-
tions of parse forests using dynamic programming
and algebraic power series. Theoretical Computer
Science, (199):145–166.
</reference>
<sectionHeader confidence="0.539963" genericHeader="method">
A Relation Between Flow and
Inside-Outside Algorithm
</sectionHeader>
<bodyText confidence="0.9985289375">
.
We will show that these quantities are related
to the flow by the equation
. is the inside probability of
the root symbol, which is also the sum of the
probabilities of all parse trees.
According to Charniak (1993), the outside
probabilities in a parse forest are computed by:
The outside probability of the start symbol is 1.
We prove by induction over the depth of the parse
forest that the following relationship holds:
It is easy to see that the assumption holds for
the root symbol :
After a few transformations, we get the equation
according to the definition of . So, the in-
duction hypothesis is generally true.
</bodyText>
<subsectionHeader confidence="0.6053">
B Parse Forest Lexicalization
</subsectionHeader>
<bodyText confidence="0.9996434">
The function LEXICALIZE below takes an unlex-
icalized parse forest as argument and returns a
lexicalized parse forests, where each symbol is
uniquely labeled with a lexical head. Symbols are
split if they have more than one lexical head.
</bodyText>
<figure confidence="0.924011888888889">
LEXICALIZE( )
1 initialize as an empty parse forest
2 initialize array
3 for in
4 do NEWT
5
6 for in in bottom-up order
7 do assume rhs
8 assume is the head of
9 for
10 do if
11 then LEM
12 else
13
14 lhs
15 r’ ADD( )
16 lhs
17 return
</figure>
<bodyText confidence="0.982034260869565">
LEXICALIZE creates new terminal symbols by
calling the function NEWT. The new symbols are
linked to the original ones by means of . For
each rule in the old parse forest, the set of all
possible combinations of the lexicalized daugh-
ter symbols is generated. The function LEM
returns the lemma associated with lexical rule .
For each combination of lexicalized daughter
symbols, a new rule is inserted by calling ADD.
ADD calls NEWNT to create new non-terminals
and NEWRULE to generate new rules. A non-
terminal is only created if no symbol with the
same lexical head was linked to the original node.
lhs
lhs lhs
lhs
which is equivalent to
lhs
The inside-outside algorithm computes inside
probabilities and outside probabilities
The flow in a parse forest is computed by:
lhs lhs
Now, we insert the induction hypothesis:
</bodyText>
<figure confidence="0.963330555555556">
ADD( )
1 if lhs s.t.
2 then
3 else NEWNT
4 lhs
5
6 NEWRULE
7
8 return
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922173">
<title confidence="0.999701">Parse Forest Computation of Expected Governors</title>
<author confidence="0.976337">Helmut Schmid</author>
<affiliation confidence="0.9997575">Institute for Computational Linguistics University of Stuttgart</affiliation>
<address confidence="0.997415">Azenbergstr. 12 70174 Stuttgart, Germany</address>
<email confidence="0.997808">schmid@ims.uni-stuttgart.de</email>
<author confidence="0.998252">Mats Rooth</author>
<affiliation confidence="0.9999685">Department of Linguistics Cornell University</affiliation>
<address confidence="0.9994075">Morrill Hall Ithaca, NY 14853, USA</address>
<email confidence="0.999731">mats@cs.cornell.edu</email>
<abstract confidence="0.997443833333334">In a headed tree, each terminal word can be uniquely labeled with a governing word and grammatical relation. This labeling is a summary of a syntactic analysis which eliminates detail, reflects aspects of semantics, and for some grammatical relations (such as of finite verb) is nearly uncontroversial. We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights. The quantity is computed in a parse forest representation of the set of tree analyses for a given sentence, using vector sums and scaling by inside probability and flow.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings ofthe 27th Annual Meeting of the ACL,</booktitle>
<institution>University of British Columbia,</institution>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="7500" citStr="Billot and Lang (1989)" startWordPosition="1318" endWordPosition="1321">he pairs sum to 1. In our implementation (which is based on Schmid (2000a)), we use a cutoff of 0.1, and print only indices where is above the cutoff. Figure 3 is an example. A direct implementation of the above definition using an iteration over trees to compute would be unusable because in the robust grammar of English we work with, the number of tree analyses for a sentence is frequently large, greater than for about 1/10 of the sentences in the British National Corpus. We instead calculate in a parse forest representation of a set of tree analyses. 3 Parse Forests A parse forest (see also Billot and Lang (1989)) in labeled grammar notation is a tuple where is a context free grammar (consisting of non-terminals , terminals , rules , and a start symbol ) and is a function which maps elements of to non-terminals in an underlying grammar and elements of to terminals in . By using on symbols on the left hand and right hand sides of a parse forest rule, can be extended to map the set of parse forest rules to the set of underlying grammar rules . is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar. An example is given in figure 4. Where , let be th</context>
<context position="16712" citStr="Billot and Lang (1989)" startWordPosition="3001" endWordPosition="3004">s that the parse forest symbols be split according to heads, because of the reference to in line 10. Construction of headmarked parse forests is presented in the appendix. The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure. Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses. The worst-case space complexity of this representation is cubic (cmp. Billot and Lang (1989)). LoPar already provided functions for the computation of the head-marked parse forest, for the flow computation and for traversing the parse forest in depth-first and topologically-sorted order (see Cormen et al. (1994)). So it was only necessary to add functions for data initialization, for the computation of the governor vector at each node and for printing the result. 5 Pooling of grammatical relations The governor labels defined above are derived from the specific symbols of a context free grammar. In contrast, according to the general markup methodology of current computational linguist</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings ofthe 27th Annual Meeting of the ACL, University of British Columbia, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of Third Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Granada,</location>
<contexts>
<context position="15807" citStr="Carroll and Rooth, 1998" startWordPosition="2857" endWordPosition="2860">s is appropriate because as defined in equation (5) is to be scaled by the relative weight of trees in . In line 9 of the algorithm, is summed into the head child . There is no scaling, because every tree in is a tree in . A probability parameter vector is used in the inside algorithm. In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which conditions rules on parent category and parent lexical head, and conditions the heads of non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998). The requisite information is directly represented in our parse forests by and . Thus the call to PF-INSIDE in line 1 of PFGOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities. However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to in line 10. Construction of headmarked parse forests is presented in the appendix. The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse forest as a graph with at mo</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of Third Conference on Empirical Methods in Natural Language Processing, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Antonio Sanfilippo</author>
<author>Ted Briscoe</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference ofLanguage Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<contexts>
<context position="17568" citStr="Carroll et al. (1998)" startWordPosition="3139" endWordPosition="3142">nly necessary to add functions for data initialization, for the computation of the governor vector at each node and for printing the result. 5 Pooling of grammatical relations The governor labels defined above are derived from the specific symbols of a context free grammar. In contrast, according to the general markup methodology of current computational linguistics, labels should not be tied to a specific grammar and formalism. The same markup labels should be produced by different systems, making it possible to substitute one system for another, and to compare systems using objective tests. Carroll et al. (1998) and Carroll et al. (1999) propose a system of grammatical relation markup to which we would like to assimilate our proposal. As grammatical relation symbols, they use atomic labels such as dobj (direct object) an ncsubj (nonclausal subject). The labels are arranged in a hierarchy, with for instance subj having subtypes ncsubj, xsubj, and csubj. There is another problem with the labels we have used so far. Our grammar codes a variety of features, such as the feature VFORM on verb projections. As a result, instead of a single object grammatical relationNP,VP, we have grammatical relations NP,VP</context>
</contexts>
<marker>Carroll, Sanfilippo, Briscoe, 1998</marker>
<rawString>John Carroll, Antonio Sanfilippo, and Ted Briscoe. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the International Conference ofLanguage Resources and Evaluation, pages 447–454, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL99 workshop on Linguistically Interpreted Corpora (LINC),</booktitle>
<location>Bergen, Norway,</location>
<contexts>
<context position="17594" citStr="Carroll et al. (1999)" startWordPosition="3144" endWordPosition="3147">ions for data initialization, for the computation of the governor vector at each node and for printing the result. 5 Pooling of grammatical relations The governor labels defined above are derived from the specific symbols of a context free grammar. In contrast, according to the general markup methodology of current computational linguistics, labels should not be tied to a specific grammar and formalism. The same markup labels should be produced by different systems, making it possible to substitute one system for another, and to compare systems using objective tests. Carroll et al. (1998) and Carroll et al. (1999) propose a system of grammatical relation markup to which we would like to assimilate our proposal. As grammatical relation symbols, they use atomic labels such as dobj (direct object) an ncsubj (nonclausal subject). The labels are arranged in a hierarchy, with for instance subj having subtypes ncsubj, xsubj, and csubj. There is another problem with the labels we have used so far. Our grammar codes a variety of features, such as the feature VFORM on verb projections. As a result, instead of a single object grammatical relationNP,VP, we have grammatical relations NP,VP.N,NP,VP.FIN,NP,VP.TO, NP,</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACL99 workshop on Linguistically Interpreted Corpora (LINC), Bergen, Norway, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Parsing with contextfree grammars and word statistics.</title>
<date>1995</date>
<tech>Technical Report CS-95-28,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="15781" citStr="Charniak, 1995" startWordPosition="2855" endWordPosition="2856">f trees in . This is appropriate because as defined in equation (5) is to be scaled by the relative weight of trees in . In line 9 of the algorithm, is summed into the head child . There is no scaling, because every tree in is a tree in . A probability parameter vector is used in the inside algorithm. In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which conditions rules on parent category and parent lexical head, and conditions the heads of non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998). The requisite information is directly represented in our parse forests by and . Thus the call to PF-INSIDE in line 1 of PFGOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities. However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to in line 10. Construction of headmarked parse forests is presented in the appendix. The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse fo</context>
</contexts>
<marker>Charniak, 1995</marker>
<rawString>Eugene Charniak. 1995. Parsing with contextfree grammars and word statistics. Technical Report CS-95-28, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>M.I.T. Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3279" citStr="Chomsky (1965)" startWordPosition="557" endWordPosition="558">nd positive integers are used as relative addresses of child vertices after the head. A headed tree domain is a set of finite sequences of integers such that (i) if , then ; (ii) if and or ,then S NP Peter VP V reads D every NP N paper NP PP:on P:on NP on N markup Figure 1: A tree with percolated lexical heads. . Figure 2: Governor labels for the terminals in the tree of Figure 1. For the head of the sentence, special symbols startc and startw are used as the parent category and parent lexical governor. is the tuple .2 Governor labels for the example tree are given in Figure 2. As observed in Chomsky (1965), grammatical relations such as subject and object may be reconstructed as ordered pairs of category labels, such as NP,S for subject. So, a governor label encodes a grammatical relation and a governing lexical head. Given a unique tree structure for a sentence, governor markup may be read off the tree. However, in view of the fact that robust broad coverage parsers frequently deliver thousands, millions, or thousands of millions of analyses for sentences of free text, basing annotation on a unique tree (such as the most probable tree analysis generated by a probabilistic grammar) appears arbi</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax. M.I.T. Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1994</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="16933" citStr="Cormen et al. (1994)" startWordPosition="3035" endWordPosition="3038">mentation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure. Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses. The worst-case space complexity of this representation is cubic (cmp. Billot and Lang (1989)). LoPar already provided functions for the computation of the head-marked parse forest, for the flow computation and for traversing the parse forest in depth-first and topologically-sorted order (see Cormen et al. (1994)). So it was only necessary to add functions for data initialization, for the computation of the governor vector at each node and for printing the result. 5 Pooling of grammatical relations The governor labels defined above are derived from the specific symbols of a context free grammar. In contrast, according to the general markup methodology of current computational linguistics, labels should not be tied to a specific grammar and formalism. The same markup labels should be produced by different systems, making it possible to substitute one system for another, and to compare systems using obj</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1994</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. 1994. Introduction to Algorithms. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99),</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="10627" citStr="Eisner and Satta (1999)" startWordPosition="1915" endWordPosition="1918"> NP Peter V reads D every N paper P on N markup PF-INSIDE( ) 1 Initial. float array 0 2 for 3 do 4 for in in bottom-up order 5 do 6 lhs lhs 7 return Figure 5: Inside algorithm. PF-FLOW( ) 1 Initial. float array 0 2 3 for in in top-down order 4 do lhs 5 for in rhs 6 do 7 return Figure 6: Flow algorithm. first constructed by tabular parsing, and then in a second pass parse forest symbols are split according to headedness. Such an algorithm is shown in appendix B. This procedure gives worst case time and space complexity which is proportional to the fifth power of the length of the sentence. See Eisner and Satta (1999) for discussion and an algorithm with time and space requirements proportional to the fourth power of the length of the input sentence in the worst case. In practical experience with broad-coverage context free grammars of several languages, we have not observed super-cubic average time or space requirements for our implementation. We believe this is because, for our grammars and corpora, there is limited ambiguity in the position of the head within a given category-span combination. The governor algorithm stated in the next section refers to headedness in parse forest rules. This can be repre</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and a cubictime probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 4th international Workshop on Parsing Technologies,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="15765" citStr="Eisner, 1997" startWordPosition="2853" endWordPosition="2854">ative weight of trees in . This is appropriate because as defined in equation (5) is to be scaled by the relative weight of trees in . In line 9 of the algorithm, is summed into the head child . There is no scaling, because every tree in is a tree in . A probability parameter vector is used in the inside algorithm. In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which conditions rules on parent category and parent lexical head, and conditions the heads of non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998). The requisite information is directly represented in our parse forests by and . Thus the call to PF-INSIDE in line 1 of PFGOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities. However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to in line 10. Construction of headmarked parse forests is presented in the appendix. The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represe</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Jason Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In Proceedings of the 4th international Workshop on Parsing Technologies, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>M Surdeanu</author>
<author>R Bunescu</author>
<author>R Gîrju</author>
<author>V Rus</author>
<author>P Morarescu</author>
</authors>
<title>Falcon: Boosting knowledge for answer engines.</title>
<date>2000</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference (TREC 9),</booktitle>
<location>Gaithersburg, MD, USA,</location>
<contexts>
<context position="20091" citStr="Harabagiu et al. (2000)" startWordPosition="3571" endWordPosition="3574"> grammatical relations. In the second pair, a VP.TO argument is distinguished from a VP.TO modifier using the category of the head. In each case, the child-parent category pair isVP.TO,VP.FIN. Notice that in Line 10 of PF-GOVERNORS, the rule is available, so that the arguments of could be changed in this way. 6 Discussion The governor algorithm was designed as a component of Spot, a free-text question answering system. Current systems usually extract a set of candidate answers (e.g. sentences), score them and return the n highest-scoring candidates as possible answers. The system described in Harabagiu et al. (2000) scores possible answers based on the overlap in the semantic representations of the question and the answer candidates. Their semantic representation is basically identical to the head-head relations computed by the governor algorithm. However, Harabagiu et al. extract this information only from maximal probability parses whereas the governor algorithm considers all analyses of a sentence and returns all possible relations weighted with estimated frequencies. Our application in Spot works as follows: the question is parsed with a specialized question grammar, and features including the govern</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Gîrju, Rus, Morarescu, 2000</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Gîrju, V. Rus, and P. Morarescu. 2000. Falcon: Boosting knowledge for answer engines. In Proceedings of the Ninth Text REtrieval Conference (TREC 9), Gaithersburg, MD, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>LoPar: Design and Implementation.</title>
<date>2000</date>
<journal>Number</journal>
<booktitle>in Arbeitspapiere des Sonderforschungsbereiches 340. Institute</booktitle>
<volume>149</volume>
<institution>for Computational Linguistics, University of Stuttgart.</institution>
<contexts>
<context position="6950" citStr="Schmid (2000" startWordPosition="1221" endWordPosition="1222">abels. For a given markup triple , let termined by tree . The random variate is defined on labeled trees licensed by the probabilistic grammar. Note that is a vector space (with pointwise sums and scalar products), so that expectations and conditional expectations may be defined. In these terms, is the conditional expectation of , conditioned on the yield being . This definition, instead of a single governor label for a given word position, gives us a set of pairs of a markup and a real number in [0,1], such that the real numbers in the pairs sum to 1. In our implementation (which is based on Schmid (2000a)), we use a cutoff of 0.1, and print only indices where is above the cutoff. Figure 3 is an example. A direct implementation of the above definition using an iteration over trees to compute would be unusable because in the robust grammar of English we work with, the number of tree analyses for a sentence is frequently large, greater than for about 1/10 of the sentences in the British National Corpus. We instead calculate in a parse forest representation of a set of tree analyses. 3 Parse Forests A parse forest (see also Billot and Lang (1989)) in labeled grammar notation is a tuple where is </context>
<context position="16292" citStr="Schmid, 2000" startWordPosition="2939" endWordPosition="2940">f non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998). The requisite information is directly represented in our parse forests by and . Thus the call to PF-INSIDE in line 1 of PFGOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities. However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to in line 10. Construction of headmarked parse forests is presented in the appendix. The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure. Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses. The worst-case space complexity of this representation is cubic (cmp. Billot and Lang (1989)). LoPar already provided functions for the computation of the head-marked parse forest, for the flow computation and for traversing the parse forest in depth-first and topological</context>
<context position="21412" citStr="Schmid, 2000" startWordPosition="3771" endWordPosition="3772">, and answer terms within sentences. In collaboration with Pranav Anand and Eric Breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it. Expected governor markup summarizes syntactic structure in a weighted parse forest which is the product of exhaustive parsing and insideoutside computation. This is a strategy of dumbing down the product of computationally intensive statistical parsing into unstructured markup. Estimated frequency computations in parse forests have previously been applied to tagging and chunking (Schulte im Walde and Schmid, 2000). Governor markup differs in that it is reflective of higher-level syntax. The strategy has the advantage, in our view, that it allows one to base markup algorithms on relatively sophisticated grammars, and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model. Localizing markup on the governed word increases pooling of frequencies, because the span of the phrase headed by the governed item is ignored. This idea could be exploited in other markup tasks. In a chunking task, categories and heads of chunks could be ident</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid. 2000a. LoPar: Design and Implementation. Number 149 in Arbeitspapiere des Sonderforschungsbereiches 340. Institute for Computational Linguistics, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Lopar man pages.</title>
<date>2000</date>
<institution>Institute for Computational Linguistics, University of Stuttgart.</institution>
<contexts>
<context position="6950" citStr="Schmid (2000" startWordPosition="1221" endWordPosition="1222">abels. For a given markup triple , let termined by tree . The random variate is defined on labeled trees licensed by the probabilistic grammar. Note that is a vector space (with pointwise sums and scalar products), so that expectations and conditional expectations may be defined. In these terms, is the conditional expectation of , conditioned on the yield being . This definition, instead of a single governor label for a given word position, gives us a set of pairs of a markup and a real number in [0,1], such that the real numbers in the pairs sum to 1. In our implementation (which is based on Schmid (2000a)), we use a cutoff of 0.1, and print only indices where is above the cutoff. Figure 3 is an example. A direct implementation of the above definition using an iteration over trees to compute would be unusable because in the robust grammar of English we work with, the number of tree analyses for a sentence is frequently large, greater than for about 1/10 of the sentences in the British National Corpus. We instead calculate in a parse forest representation of a set of tree analyses. 3 Parse Forests A parse forest (see also Billot and Lang (1989)) in labeled grammar notation is a tuple where is </context>
<context position="16292" citStr="Schmid, 2000" startWordPosition="2939" endWordPosition="2940">f non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998). The requisite information is directly represented in our parse forests by and . Thus the call to PF-INSIDE in line 1 of PFGOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities. However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to in line 10. Construction of headmarked parse forests is presented in the appendix. The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure. Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses. The worst-case space complexity of this representation is cubic (cmp. Billot and Lang (1989)). LoPar already provided functions for the computation of the head-marked parse forest, for the flow computation and for traversing the parse forest in depth-first and topological</context>
<context position="21412" citStr="Schmid, 2000" startWordPosition="3771" endWordPosition="3772">, and answer terms within sentences. In collaboration with Pranav Anand and Eric Breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it. Expected governor markup summarizes syntactic structure in a weighted parse forest which is the product of exhaustive parsing and insideoutside computation. This is a strategy of dumbing down the product of computationally intensive statistical parsing into unstructured markup. Estimated frequency computations in parse forests have previously been applied to tagging and chunking (Schulte im Walde and Schmid, 2000). Governor markup differs in that it is reflective of higher-level syntax. The strategy has the advantage, in our view, that it allows one to base markup algorithms on relatively sophisticated grammars, and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model. Localizing markup on the governed word increases pooling of frequencies, because the span of the phrase headed by the governed item is ignored. This idea could be exploited in other markup tasks. In a chunking task, categories and heads of chunks could be ident</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid. 2000b. Lopar man pages. Institute for Computational Linguistics, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Helmut Schmid</author>
</authors>
<title>Robust german noun chunking with a probabilistic context-free grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>726--732</pages>
<location>Saarbrücken, Germany,</location>
<contexts>
<context position="21412" citStr="Walde and Schmid, 2000" startWordPosition="3769" endWordPosition="3772"> sentences, and answer terms within sentences. In collaboration with Pranav Anand and Eric Breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it. Expected governor markup summarizes syntactic structure in a weighted parse forest which is the product of exhaustive parsing and insideoutside computation. This is a strategy of dumbing down the product of computationally intensive statistical parsing into unstructured markup. Estimated frequency computations in parse forests have previously been applied to tagging and chunking (Schulte im Walde and Schmid, 2000). Governor markup differs in that it is reflective of higher-level syntax. The strategy has the advantage, in our view, that it allows one to base markup algorithms on relatively sophisticated grammars, and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model. Localizing markup on the governed word increases pooling of frequencies, because the span of the phrase headed by the governed item is ignored. This idea could be exploited in other markup tasks. In a chunking task, categories and heads of chunks could be ident</context>
</contexts>
<marker>Walde, Schmid, 2000</marker>
<rawString>Sabine Schulte im Walde and Helmut Schmid. 2000. Robust german noun chunking with a probabilistic context-free grammar. In Proceedings of the 18th International Conference on Computational Linguistics, pages 726–732, Saarbrücken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Tendeau</author>
</authors>
<title>Computing abstract decorations of parse forests using dynamic programming and algebraic power series. Theoretical Computer Science,</title>
<date>1998</date>
<contexts>
<context position="15002" citStr="Tendeau, 1998" startWordPosition="2716" endWordPosition="2717">ft hand side of . Consider a parse forest rule , and a parse forest symbol on its right hand side which is not the head of . In each tree in ,is the top of a chain of heads, because is a non-head child in rule . Instep 10, the governor tuple describing the syntactic environment of in trees in (or rather, their images under ) is constructed TThis procedure requires that symbols in correspond to a unique string position, something which is not enforced by our definition of parse forests. Indeed, such cases may arise ifparse forest symbols are constructed as pairs of grammar symbols and strings (Tendeau, 1998) rather than pairs of grammar symbols and spans. Our parser constructs parse forests organized according to span. as . The scalar multiplier is the relative weight of trees in . This is appropriate because as defined in equation (5) is to be scaled by the relative weight of trees in . In line 9 of the algorithm, is summed into the head child . There is no scaling, because every tree in is a tree in . A probability parameter vector is used in the inside algorithm. In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which condition</context>
</contexts>
<marker>Tendeau, 1998</marker>
<rawString>Frederic Tendeau. 1998. Computing abstract decorations of parse forests using dynamic programming and algebraic power series. Theoretical Computer Science, (199):145–166.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>