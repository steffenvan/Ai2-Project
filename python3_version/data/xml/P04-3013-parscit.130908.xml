<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008464">
<title confidence="0.948773">
Exploiting Unannotated Corpora for Tagging and Chunking
</title>
<author confidence="0.667689">
Rie Kubota Ando
</author>
<note confidence="0.598713666666667">
IBM T.J. Watson Research Center
19 Skyline Dr., Hawthorne, NY 10532
riel@us . ibm. corn
</note>
<sectionHeader confidence="0.902415" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999323875">
We present a method that exploits unannotated
corpora for compensating the paucity of anno-
tated training data on the chunking and tagging
tasks. It collects and compresses feature fre-
quencies from a large unannotated corpus for
use by linear classifiers. Experiments on two
tasks show that it consistently produces signifi-
cant performance improvements.
</bodyText>
<sectionHeader confidence="0.995503" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980844378378378">
This paper presents a method for exploiting
large unannotated corpora for the tagging and
chunking tasks. We report experiments on
entity mention detection&apos; and part-of-speech
(POS) tagging. To apply classification tech-
niques to chunking tasks, a common approach
is to cast the task to that of token tagging,
where token tags encode chunk information,
e.g., 13-PERSON&apos; (beginning of person chunk),
&apos;I-PERSON&apos; (inside of person chunk), and &apos;0&apos;
(outside of any entity chunk). The challenge
for a classifier is to learn unknown relationships
between token tags and features (such as token
strings and context information) from tagged
examples. To achieve reasonable performance,
a sufficiently large number of representative ex-
amples are required. Our goal is to compensate
for the paucity of tagged examples or their dif-
ferences from test data, by using untagged ex-
amples.
One type of approaches to this problem in-
volves iterative and automatic tagging of the
untagged data such as bootstrapping or co-
training. Expectation Maximization (EM) also
uses untagged data for iteratively improving
model parameters. Another type uses untagged
&apos;The task objective of entity mention detection is to
detect and classify text spans that mention (or refer to)
certain types of entities in the real world such as per-
sons and organizations. We experiment with the data
from the ACE (Automatic Content Extraction) program
(http://www.nist.gov/speech/index.htm).
corpora for improving feature representation,
e.g. (Schiietze, 1992). We take the latter ap-
proach.
To see how unannotated corpora may help
tagging, consider the following examples:
</bodyText>
<listItem confidence="0.9998665">
• • • the president/B-PERSON and • • •
• • • our chairman/B-PERSON is • • •
</listItem>
<bodyText confidence="0.999807086956522">
Suppose that &amp;quot;president&amp;quot; appeared in the train-
ing data, but &amp;quot;chairman&amp;quot; didn&apos;t, and that in
a large corpus, both words (&amp;quot;chairman&amp;quot; and
&amp;quot;president&amp;quot;) often appear as the subject of
&amp;quot;said&amp;quot;, &amp;quot;visited&amp;quot;, etc., and that both are of-
ten modified by &amp;quot;vice&amp;quot;, &amp;quot;powerful&amp;quot;, etc. It is
intuitive that such corpus statistics would help
a classifier to tag &amp;quot;chairman&amp;quot; correctly even if
&amp;quot;chairman&amp;quot; did not appear in the training data.
Given some set of features designed for the
task (see Figure 1 for example), we count fea-
ture occurrences in all the word instances in
the unannotated corpus to generate feature-by-
word co-occurrence frequency matrices. When
we encounter a training or test instance of word
w, we generate two kinds of features. One
is the features observed in that instance (as
usual). The other is the features derived from
the columns (corresponding to w) of the feature-
by-word co-occurrence matrices — collections of
w&apos;s context in the untagged corpus — which we
call corpus-context features.
Our experiments show that the corpus-
context features consistently improve perfor-
mance on the two tasks. There are two im-
portant elements for achieving such effective-
ness in this simple framework. One is a high-
performance linear classifier, Robust Risk Mini-
mization (RRM) (Zhang et al., 2002), which has
an ability to ignore irrelevant features while cop-
ing with mutually-dependent features. (RRM
learns feature weights by minimizing classifica-
tion errors with regularization on the tagged
training data.) Therefore, we take a &apos;feature-
rich&apos; strategy to use a variety of types of cor-
pus context information. To enable classifier
training with many types of corpus statistics,
such vast amounts of information from a large
corpus must be compressed. Hence, the sec-
ond key element is a dimension reduction tech-
nique. We adapt a variation of LSI, specifi-
cally designed for feature occurrence frequen-
cies (Ando, 2004). As such, the objective of
this paper is to show that a right combination
of techniques produces a useful tool for coping
with the paucity of tagged training data.
</bodyText>
<sectionHeader confidence="0.922406" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.889711">
2.1 Collecting corpus statistics
</subsectionHeader>
<bodyText confidence="0.999984625">
From a given set of features designed for the
task (see Figure 1 and Figure 6 for example),
we use context features only (i.e., excluding fea-
tures that strongly depend on words2) to gener-
ate feature-by-word co-occurrence matrices. We
generate one matrix for each type, e.g., a &apos;left
adjacent word&apos;-by-word matrix, a &apos;right adja-
cent word&apos;-by-word matrix, and so forth.
</bodyText>
<subsectionHeader confidence="0.946243">
2.2 Vector compression
</subsectionHeader>
<bodyText confidence="0.838885322580645">
To compress feature-by-word matrices, we
adapt a procedure proposed for semantic lexi-
con construction (Ando, 2004). That is to ap-
ply singular value decomposition (SVD) only to
a smaller matrix consisting of several selected
columns of the co-occurrence matrix and to &apos;fold
in&apos; the rest of the columns to the reduced di-
mensions. The choice of columns is important.
The columns corresponding to the most frequent
words should be selected. The intuition be-
hind its theoretical justification (Ando, 2004) is
that more reliable statistics from high-frequency
words should produce a better representation
space, which should result in improving statis-
tically &apos;poor&apos; vectors for low-frequency words.
Thus, we choose k most frequent words and re-
duce the dimensions to h. The dimensionality h
should be no smaller than the number of target
classes3.
We compress each of feature-by-word co-
occurrence matrix independently of one another.
This is important, as it gives more freedom to
2For instance, it is useless to count &apos;co-occurrences&apos;
of words and their endings. Moreover, features that are
nearly conditionally independent of words given classes
are more useful for the purpose, since ultimately we want
to capture correlations of words to classes (through their
co-occurrences with features) rather than their correla-
tions to specific features.
3Intuitively, there need at least h dimensions to ex-
press correlations to h classes.
</bodyText>
<listItem confidence="0.9996153">
• token, capitalization, POS in 3-token window
• bi-grams of adjacent words in 5-token window
• words in the same syntactic chunk.
• head words in 3-chunk window
• word uni- and bi-grams based on subject-verb-
object and preposition-noun constructions.
• syntactic chunk types
• tags in 2-token window to the left
• tri-grams of POS, capitalization, and word ending
• tri-grams of POS, capitalization, and left tag
</listItem>
<figureCaption confidence="0.998913">
Figure 1: Features for entity detection
</figureCaption>
<bodyText confidence="0.99988995">
sophisticated classifiers to weight relevant types
of features more heavily than irrelevant ones.
If all are compressed together, the classifiers
can not tear them apart. For efficient training,
though optionally, we further reduce non-zero
entries by zeroing out all but n entries that have
the largest absolute values in each compressed
vector. We call the entries of the resultant vec-
tors corpus-context features. For a training or
test instance of word w, we have two kinds of
features: features derived from the instance (as
usual), and the corpus-context features gener-
ated from w&apos;s context in the corpus.
For our experiments, we set (k,h,n) =
(1000, 50, 6) using held-out data (the develop-
ment set described below). Performance is rel-
atively insensitive to the changes of these pa-
rameters4. We use the same parameter setting
for both entity mention detection and part-of-
speech tagging experiments.
</bodyText>
<sectionHeader confidence="0.981809" genericHeader="method">
3 Entity mention detection
experiments
</sectionHeader>
<subsectionHeader confidence="0.858528">
3.1 Experimental framework
</subsectionHeader>
<bodyText confidence="0.981374352941176">
Entity classes and evaluation metric We
experiment with 10 classes from the ACE entity
classes — obtained by combining five entity types
(Person, Organization, Facility, GPE, Location)
and two mention types (Name, Nominal), which
make 21-way classification when chunk bound-
ary information is encoded into token tags. Pro-
posed mention chunks are counted as correct
only if both mention boundaries and classes are
correct. We combine precision and recall into
F-measure with equal weight.
Features Figure 1 describes features used for
entity mention detection experiments. We gen-
erate corpus-context features from the features
40n the held-out data, k E [1000,5000] produced es-
sentially similar performance, and so did h E [30,60] and
Ti E [6,10].
</bodyText>
<figure confidence="0.992892074074074">
50
Ratio (%)
40
30
20
10
0
Per
Name
Loc
nom
Fac
Name
Loc
Name
Fac
nom
GPE
nom
Org
nom
Org
Name
Per
nom
GPE
Name
</figure>
<listItem confidence="0.978169">
• token, capitalization in 5-token windows
• ending (length 1 to 4)
• uni- and bi-grams of tags at the left
• tag-word bi-grams in 3-token windows
bi-grams of adjacent words in 5-token windows
</listItem>
<table confidence="0.958445222222222">
Figure 6: Features for POS tagging
RRM H1\41\4
Corpus-ctx - with BW w/o
5K 90.2 (+7.4) 82.8 82.1 (+5.0) 77.1
9K 92.7 (+5.0) 87.7 84.9 (+2.7) 82.2
19K 93.7 (+2.8) 90.9 87.1 (+0.3) 86.8
38K 94.7 (+1.8) 92.9 89.8 (-0.2) 89.6
75K 95.2 (+1.6) 93.6 91.2 (-0.6) 91.8
149K 95.6 (+0.9) 94.7 92.3 (-1.0) 93.3
</table>
<figureCaption confidence="0.961305666666667">
Figure 7: POS tagging accuracy results. Num-
bers in parentheses are differences from their coun-
terparts that do not use the untagged corpus.
</figureCaption>
<bodyText confidence="0.977919">
pus), indeed, compensates for the differences
between tagged training data (ACE) and test
data (CNS). The other classifiers are apparently
suffering from the dissimilarity.
</bodyText>
<sectionHeader confidence="0.997117" genericHeader="method">
4 POS Tagging Experiments
</sectionHeader>
<bodyText confidence="0.999933948717949">
Features Figure 6 shows the features we use
for POS tagging. Among them, we use word
uni- and bi-grams that do not overlap with the
current word, to generate corpus-context fea-
tures.
Baseline As our baseline, we implement an
HMM tagger with and without Baum-Welch
reestimation (EM for HMM). We smooth transi-
tion probabilities by deleted interpolation. For
unseen and low-frequency words, word emission
probabilities are estimated as Weischedel et al.
(1993) do while interpolating emission proba-
bilities of words and endings (length 1 to 4).
We estimate these probabilities by relative fre-
quencies in tagged training corpora, and per-
form 10 EM iterations using unannotated data.
To avoid underestimating the baseline, we re-
port its best performance among the iterations.
POS tagging results We report results on
the standard Brown corpus. The test data
was fixed to arbitrarily-drawn one fifth of the
corpus (230K words). We use the rest (930K
words) as tagged and untagged training data:
all 930K words as untagged data for collect-
ing corpus context and for the BW reestima-
tion; and arbitrarily-drawn various portions as
tagged training data. Figure 7 shows accu-
racy (# of correctly tagged words divided by
# of words) in relation to the number of tagged
training examples. The performance differ-
ences between HMM and RRM mainly derive
from the differences in the &apos;richness&apos; of infor-
mation they make use of. The additional fea-
tures5 used by RRM are apparently effective
for compensating for the paucity of the tagged
data. Corpus-context features further improve
the performance up to 7.4%. This is in contrast
to the Baum-Welch reestimation, which some-
times rather degrades performance.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999972875">
The method we present is intended for the
chunking/tagging tasks in which words serve
as strongly effective features. Performance im-
provements obtained by corpus-context features
are especially large when tagged training is
small or different from test data, which is useful
for expediting the adaptation of the system to
new domains.
</bodyText>
<sectionHeader confidence="0.997492" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99945125">
This work was supported by the Advanced Re-
search and Development Activity under the
Novel Intelligence and Massive Data (NIMD)
program PNWD-SW-6059.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.894347958333333">
Rie Kubota Ando. 2004. Semantic lexicon con-
struction: Learning from unlabeled data via
spectral analysis. In Proceedings of C oNLL-
2004.
Hinrich Schiietze. 1992. Dimensions of mean-
ing. In Proceedings of Supercomputing &apos;92,
pages 787-796.
Ralph Weischedel, Marie Meteer, Richard
Schwartz, Lance Ramshaw, and Jeff Pal-
mucci. 1993. Coping with ambiguity and un-
known words through probabilistic models.
Computational Linguistics, 19(2):359-382.
Tong Zhang, Fred Damerau, and David John-
son. 2002. Text chunking based on a gen-
eralization of Winnow. Journal of Machine
Learning Research, 2:615-637.
5As many of the features used with RRM are mutu-
ally dependent, there is no easy way to exploit them with
HMM. However, we note that when trained with over
one million tagged examples, RRM (with and without
corpus context) and HMM taggers produce essentially
similar high accuracy. That is, the mutually-dependent
features become redundant once sufficiently large tagged
data becomes available.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545018">
<title confidence="0.999971">Exploiting Unannotated Corpora for Tagging and Chunking</title>
<author confidence="0.99608">Rie Kubota Ando</author>
<affiliation confidence="0.999817">T.J. Research Center</affiliation>
<address confidence="0.991838">19 Skyline Dr., Hawthorne, NY 10532</address>
<author confidence="0.557509">corn</author>
<abstract confidence="0.998458555555556">We present a method that exploits unannotated corpora for compensating the paucity of annotated training data on the chunking and tagging tasks. It collects and compresses feature frequencies from a large unannotated corpus for use by linear classifiers. Experiments on two tasks show that it consistently produces significant performance improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
</authors>
<title>Semantic lexicon construction: Learning from unlabeled data via spectral analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of C oNLL2004.</booktitle>
<contexts>
<context position="4145" citStr="Ando, 2004" startWordPosition="650" endWordPosition="651">as an ability to ignore irrelevant features while coping with mutually-dependent features. (RRM learns feature weights by minimizing classification errors with regularization on the tagged training data.) Therefore, we take a &apos;featurerich&apos; strategy to use a variety of types of corpus context information. To enable classifier training with many types of corpus statistics, such vast amounts of information from a large corpus must be compressed. Hence, the second key element is a dimension reduction technique. We adapt a variation of LSI, specifically designed for feature occurrence frequencies (Ando, 2004). As such, the objective of this paper is to show that a right combination of techniques produces a useful tool for coping with the paucity of tagged training data. 2 Method 2.1 Collecting corpus statistics From a given set of features designed for the task (see Figure 1 and Figure 6 for example), we use context features only (i.e., excluding features that strongly depend on words2) to generate feature-by-word co-occurrence matrices. We generate one matrix for each type, e.g., a &apos;left adjacent word&apos;-by-word matrix, a &apos;right adjacent word&apos;-by-word matrix, and so forth. 2.2 Vector compression To</context>
</contexts>
<marker>Ando, 2004</marker>
<rawString>Rie Kubota Ando. 2004. Semantic lexicon construction: Learning from unlabeled data via spectral analysis. In Proceedings of C oNLL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiietze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing &apos;92,</booktitle>
<pages>787--796</pages>
<contexts>
<context position="2050" citStr="Schiietze, 1992" startWordPosition="306" endWordPosition="307">this problem involves iterative and automatic tagging of the untagged data such as bootstrapping or cotraining. Expectation Maximization (EM) also uses untagged data for iteratively improving model parameters. Another type uses untagged &apos;The task objective of entity mention detection is to detect and classify text spans that mention (or refer to) certain types of entities in the real world such as persons and organizations. We experiment with the data from the ACE (Automatic Content Extraction) program (http://www.nist.gov/speech/index.htm). corpora for improving feature representation, e.g. (Schiietze, 1992). We take the latter approach. To see how unannotated corpora may help tagging, consider the following examples: • • • the president/B-PERSON and • • • • • • our chairman/B-PERSON is • • • Suppose that &amp;quot;president&amp;quot; appeared in the training data, but &amp;quot;chairman&amp;quot; didn&apos;t, and that in a large corpus, both words (&amp;quot;chairman&amp;quot; and &amp;quot;president&amp;quot;) often appear as the subject of &amp;quot;said&amp;quot;, &amp;quot;visited&amp;quot;, etc., and that both are often modified by &amp;quot;vice&amp;quot;, &amp;quot;powerful&amp;quot;, etc. It is intuitive that such corpus statistics would help a classifier to tag &amp;quot;chairman&amp;quot; correctly even if &amp;quot;chairman&amp;quot; did not appear in the training d</context>
</contexts>
<marker>Schiietze, 1992</marker>
<rawString>Hinrich Schiietze. 1992. Dimensions of meaning. In Proceedings of Supercomputing &apos;92, pages 787-796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmucci</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="9739" citStr="Weischedel et al. (1993)" startWordPosition="1548" endWordPosition="1551">e differences between tagged training data (ACE) and test data (CNS). The other classifiers are apparently suffering from the dissimilarity. 4 POS Tagging Experiments Features Figure 6 shows the features we use for POS tagging. Among them, we use word uni- and bi-grams that do not overlap with the current word, to generate corpus-context features. Baseline As our baseline, we implement an HMM tagger with and without Baum-Welch reestimation (EM for HMM). We smooth transition probabilities by deleted interpolation. For unseen and low-frequency words, word emission probabilities are estimated as Weischedel et al. (1993) do while interpolating emission probabilities of words and endings (length 1 to 4). We estimate these probabilities by relative frequencies in tagged training corpora, and perform 10 EM iterations using unannotated data. To avoid underestimating the baseline, we report its best performance among the iterations. POS tagging results We report results on the standard Brown corpus. The test data was fixed to arbitrarily-drawn one fifth of the corpus (230K words). We use the rest (930K words) as tagged and untagged training data: all 930K words as untagged data for collecting corpus context and fo</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw, and Jeff Palmucci. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="3525" citStr="Zhang et al., 2002" startWordPosition="550" endWordPosition="553">test instance of word w, we generate two kinds of features. One is the features observed in that instance (as usual). The other is the features derived from the columns (corresponding to w) of the featureby-word co-occurrence matrices — collections of w&apos;s context in the untagged corpus — which we call corpus-context features. Our experiments show that the corpuscontext features consistently improve performance on the two tasks. There are two important elements for achieving such effectiveness in this simple framework. One is a highperformance linear classifier, Robust Risk Minimization (RRM) (Zhang et al., 2002), which has an ability to ignore irrelevant features while coping with mutually-dependent features. (RRM learns feature weights by minimizing classification errors with regularization on the tagged training data.) Therefore, we take a &apos;featurerich&apos; strategy to use a variety of types of corpus context information. To enable classifier training with many types of corpus statistics, such vast amounts of information from a large corpus must be compressed. Hence, the second key element is a dimension reduction technique. We adapt a variation of LSI, specifically designed for feature occurrence freq</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David Johnson. 2002. Text chunking based on a generalization of Winnow. Journal of Machine Learning Research, 2:615-637.</rawString>
</citation>
<citation valid="false">
<title>5As many of the features used with RRM are mutually dependent, there is no easy way to exploit them with HMM. However, we note that when trained with over one million tagged examples, RRM (with and without corpus context) and HMM taggers produce essentially similar high accuracy. That is, the mutually-dependent features become redundant once sufficiently large tagged data becomes available.</title>
<marker></marker>
<rawString>5As many of the features used with RRM are mutually dependent, there is no easy way to exploit them with HMM. However, we note that when trained with over one million tagged examples, RRM (with and without corpus context) and HMM taggers produce essentially similar high accuracy. That is, the mutually-dependent features become redundant once sufficiently large tagged data becomes available.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>