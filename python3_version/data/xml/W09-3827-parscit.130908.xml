<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028381">
<title confidence="0.98013">
The effect of correcting grammatical errors on parse probabilities
</title>
<author confidence="0.994561">
Joachim Wagner
</author>
<affiliation confidence="0.983455333333333">
CNGL
School of Computing
Dublin City University, Ireland
</affiliation>
<email confidence="0.994237">
jwagner@computing.dcu.ie
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994095">
We parse the sentences in three parallel er-
ror corpora using a generative, probabilis-
tic parser and compare the parse probabil-
ities of the most likely analyses for each
grammatical sentence and its closely re-
lated ungrammatical counterpart.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98948853125">
The syntactic analysis of a sentence provided by
a parser is used to guide the interpretation process
required, to varying extents, by applications such
as question-answering, sentiment analysis and ma-
chine translation. In theory, however, parsing also
provides a grammaticality judgement as shown in
Figure 1. Whether or not a sentence is grammati-
cal is determined by its parsability with a grammar
of the language in question.
The use of parsing to determine whether a sen-
tence is grammatical has faded into the back-
ground as hand-written grammars aiming to de-
scribe only the grammatical sequences in a lan-
guage have been largely supplanted by treebank-
derived grammars. Grammars read from treebanks
tend to overgenerate. This overgeneration is un-
problematic if a probabilistic model is used to rank
analyses and if the parser is not being used to pro-
vide a grammaticality judgement. The combina-
tion of grammar size, probabilistic parse selection
and smoothing techniques results in high robust-
ness to errors and broad language coverage, de-
sirable properties in applications requiring a syn-
tactic analysis of any input, regardless of noise.
However, for applications which rely on a parser’s
ability to distinguish grammatical sequences from
ungrammatical ones, e.g. grammar checkers, over-
generating grammars are perhaps less useful as
they fail to reject ungrammatical strings.
A naive solution might be to assume that the
probability assigned to a parse tree by its proba-
bilistic model could be leveraged in some way to
</bodyText>
<figure confidence="0.206865">
Jennifer Foster
NCLT
School of Computing
Dublin City University, Ireland.
jfoster@computing.dcu.ie
</figure>
<figureCaption confidence="0.999695">
Figure 1: Grammaticality and formal languages
</figureCaption>
<bodyText confidence="0.999077">
determine the sentence’s grammaticality. In this
paper, we explore one aspect of this question by
using three parallel error corpora to determine the
effect of common English grammatical errors on
the parse probability of the most likely parse tree
returned by a generative probabilistic parser.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984130434783">
The probability of a parse tree has been used be-
fore in error detection systems. Sun et al. (2007)
report only a very modest improvement when they
include a parse probability feature in their system
whose features mostly consist of linear sequential
patterns. Lee and Seneff (2006) detect ungram-
matical sentences by comparing the parse proba-
bility of a possibly ill-formed input sentence to the
parse probabilities of candidate corrections which
are generated by arbitrarily deleting, inserting and
substituting articles, prepositions and auxiliaries
and changing the inflection of verbs and nouns.
Foster et al. (2008) compare the parse probabil-
ity returned by a parser trained on a regular tree-
bank to the probability returned by the same parser
trained on a “noisy” treebank and use the differ-
ence to decide whether the sentence is ill-formed.
Research in the field of psycholinguistics has
explored the link between frequency and gram-
maticality, often focusing on borderline acceptable
sentences (see Crocker and Keller (2006) for a dis-
cussion of the literature). Koonst-Garboden and
Jaeger (2003) find a weak correlation between the
</bodyText>
<page confidence="0.943547">
176
</page>
<bodyText confidence="0.966432666666667">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 176–179,
Paris, October 2009. c�2009 Association for Computational Linguistics
frequency ratios of competing surface realisations
and human acceptability judgements. Hale (2003)
calculates the information-theoretic load of words
in sentences assuming that they were generated ac-
cording to a probabilistic grammar and finds that
these values are good predictors for observed read-
ing time and other measures of cognitive load.
</bodyText>
<sectionHeader confidence="0.996625" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999969065217391">
The aim of this experiment is to find out to
what extent ungrammatical sentences behave dif-
ferently from correct sentences as regards their
parse probabilities. There are two types of corpora
we study: two parallel error corpora that consist
of authentic ungrammatical sentences and manual
corrections, and a parallel error corpus that con-
sists of authentic grammatical sentences and auto-
matically induced errors. Using parallel corpora
allows us to compare pairs of sentences that have
the same or very similar lexical content and dif-
fer only with respect to their grammaticality. A
corpus with automatically induced errors is in-
cluded because such a corpus is much larger and
controlled error insertion allows us to examine di-
rectly the effect of a particular error type.
The first parallel error corpus contains 1,132
sentence pairs each comprising an ungrammatical
sentence and a correction (Foster, 2005). The sen-
tences are taken from written texts and contain ei-
ther one or two grammatical errors. The errors in-
clude those made by native English speakers. We
call this the Foster corpus. The second corpus
is a learner corpus. It contains transcribed spo-
ken utterances produced by learners of English of
varying L1s and levels of experience in a class-
room setting. Wagner et al. (2009) manually cor-
rected 500 sentences of the transcribed utterances,
producing a parallel error corpus which we call
Gonzaga 500. The third parallel corpus contains
199,600 sentences taken from the British National
Corpus and ungrammatical sentences produced by
introducing errors of the following five types into
the original BNC sentences: errors involving an
extra word, errors involving a missing word, real-
word spelling errors, agreement errors and errors
involving an incorrect verbal inflection.
All sentence pairs in the three parallel cor-
pora are parsed using the June 2006 version
of the first-stage parser of Charniak and John-
son (2005), a lexicalised, generative, probabilistic
parser achieving competitive performance on Wall
Street Journal text. We compare the probability of
the highest ranked tree for the grammatical sen-
tence in the pair to the probability of the highest
ranked tree for the ungrammatical sentence.
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999968577777778">
Figure 2 shows the results for the Foster corpus.
For ranges of 4 points on the logarithmic scale,
the bars depict how many sentence pairs have a
probability ratio within the respective range. For
example, there are 48 pairs (5th bar from left) for
which the correction has a parse probability which
is between 8 and 12 points lower than the parse
probability of its erroneous original, or, in other
words, for which the probability ratio is between
e−12 and e−8. 853 pairs show a higher probabil-
ity for the correction vs. 279 pairs which do not.
Since the probability of a tree is the product of
its rule probabilities, sentence length is a factor.
If we focus on corrections that do not change the
sentence length, the ratio sharpens to 414 vs. 90
pairs. Ungrammatical sentences do often receive
lower parse probabilities than their corrections.
Figure 3 shows the results for the Gonzaga 500.
Here we see a picture similar to the Foster cor-
pus although the peak for the range from e0 = 1
to e4 pz� 54.6 is more pronounced. This time
there are more cases where the parse probability
drops despite a sentence being shortened and vice
versa. Overall, 348 sentence pairs show an in-
creased parse probability, 152 do not. For sen-
tences that stay the same length the ratio is 154
to 34, or 4.53:1, for this corpus which is almost
identical to the Foster corpus (4.60:1).
How do these observations translate to the artifi-
cial parallel error corpus created from BNC data?
Figure 4 shows the results for the BNC data. In
order to keep the orientation of the graph as be-
fore, we change the sign by looking at decrements
instead of increments. Also, we swap the keys
for shortened and lengthened sentences. Clearly,
the distribution is wider and moved to the right.
The peak is at the bar labelled 10. Accordingly,
the ratio of the number of sentence pairs above
and below the zero line is much higher than be-
fore (overall 32,111 to 167, 489 = 5.22, for same
length only 8,537 to 111,171 = 13.02), suggest-
ing that our artificial errors might have a stronger
effect on parse probability than authentic errors.
Another possible explanation is that the BNC data
only contains five error types, whereas the range of
</bodyText>
<page confidence="0.99588">
177
</page>
<figureCaption confidence="0.995331333333333">
Figure 2: Effect of correcting erroneous sentences (Foster corpus) on the probability of the best parse.
Each bar is broken down by whether and how the correction changed the sentence length in tokens. A
bar labelled x covers ratios from ex−2 to ex+2 (exclusive).
Figure 3: Effect of correcting erroneous sentences (Gonzaga 500 corpus) on the probability of the best
parse.
Figure 4: Effect of inserting errors into BNC sentences on the probability of the best parse.
</figureCaption>
<page confidence="0.994796">
178
</page>
<bodyText confidence="0.99992325">
errors in the Foster and Gonzaga corpus is wider.
Analysing the BNC data by error type and look-
ing firstly at those error types that do not involve a
change in sentence length, we see that:
</bodyText>
<listItem confidence="0.923926571428571">
• 96% of real-word spelling errors cause a re-
duction in parse probability.
• 91% of agreement errors cause a reduction in
parse probability. Agreement errors involving
articles most reliably decrease the probability.
• 92% of verb form errors cause a reduction.
Changing the form from present participle to
</listItem>
<bodyText confidence="0.988720952380952">
past participle1 is least likely to cause a reduc-
tion, whereas changing it from past participle
to third singular is most likely.
The effect of error types which change sentence
length is more difficult to interpret. Almost all of
the extra word errors cause a reduction in parse
probability and it is difficult to know whether this
is happening because the sentence length has in-
creased or because an error has been introduced.
The errors involving missing words do not system-
atically result in an increase in parse probability
– 41% of them cause a reduction in parse proba-
bility, and this is much more likely to occur if the
missing word is a function word (article, auxiliary,
preposition).
Since the Foster corpus is also error-annotated,
we can also examine its results by error type. This
analysis broadly agrees with that of the BNC data,
although the percentage of ill-formed sentences
for which there is a reduction in parse probability
is generally lower (see Fig. 2 vs. Fig. 4).
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.923940913043478">
We have parsed the sentences in three parallel er-
ror corpora using a generative, probabilistic parser
and examined the parse probability of the most
likely analysis of each sentence. We find that
grammatical errors have some negative effect on
the probability assigned to the best parse, a find-
ing which corroborates previous evidence linking
sentence grammaticality to frequency. In our ex-
periment, we approximate sentence probability by
looking only at the most likely analysis – it might
be useful to see if the same effect holds if we sum
1This raises the issue of covert errors, resulting in gram-
matical sentence structures. Lee and Seneff (2008) give the
example I am preparedfor the exam which was produced by
a learner of English instead of I am preparing for the exam.
These occur in authentic error corpora and cannot be com-
pletely avoided when automatically introducing errors.
over parse trees. To fully exploit parse or sentence
probability in an error detection system, it is nec-
essary to fully account for the effect on probability
of 1) non-structural factors such as sentence length
and 2) particular error types. This study repre-
sents a contribution towards the latter.
</bodyText>
<sectionHeader confidence="0.990278" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998115857142857">
We are grateful to James Hunter from Gonzaga
University for providing us with a learner corpus.
We thank Josef van Genabith and the reviewers for
their comments and acknowledge the Irish Cen-
tre for High-End Computing for the provision of
computational facilities. The BNC is distributed
by Oxford University Computing Services.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842914285714">
Eugene Charniak and Mark Johnson. 2005. Course-
to-fine n-best-parsing and maxent discriminative
reranking. In Proceedings ofACL.
Matthew W. Crocker and Frank Keller. 2006. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gisbert Fanselow, C. F´ery,
R. Vogel, and M. Schlesewsky, editors, Gradience
in Grammar: Generative Perspectives, pages 227–
245. Oxford University Press.
Jennifer Foster, Joachim Wagner, and Josef van Gen-
abith. 2008. Adapting a WSJ-trained parser to
grammatically noisy text. In Proceedings ofACL.
Jennifer Foster. 2005. Good Reasons for Noting Bad
Grammar: Empirical Investigations into the Pars-
ing of Ungrammatical Written English. Ph.D. the-
sis, University of Dublin, Trinity College.
John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101–123.
Andrew Koontz-Garboden and T. Florian Jaeger.
2003. An empirical investigation of the frequency-
grammaticality correlation hypothesis. Student es-
say received or downloaded on 2006-03-13.
John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Interspeech 2006 - 9th ICSLP, pages 1978–1981.
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. In Proceedings ofACL.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proc. ofACL.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experiments
in sentence classification. CALICO Journal, 26(3).
</reference>
<page confidence="0.998807">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884097">
<title confidence="0.999522">The effect of correcting grammatical errors on parse probabilities</title>
<author confidence="0.999207">Joachim</author>
<affiliation confidence="0.9985235">School of Dublin City University,</affiliation>
<email confidence="0.985123">jwagner@computing.dcu.ie</email>
<abstract confidence="0.985582285714286">We parse the sentences in three parallel error corpora using a generative, probabilistic parser and compare the parse probabilities of the most likely analyses for each grammatical sentence and its closely related ungrammatical counterpart.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Courseto-fine n-best-parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6017" citStr="Charniak and Johnson (2005)" startWordPosition="923" endWordPosition="927">ted 500 sentences of the transcribed utterances, producing a parallel error corpus which we call Gonzaga 500. The third parallel corpus contains 199,600 sentences taken from the British National Corpus and ungrammatical sentences produced by introducing errors of the following five types into the original BNC sentences: errors involving an extra word, errors involving a missing word, realword spelling errors, agreement errors and errors involving an incorrect verbal inflection. All sentence pairs in the three parallel corpora are parsed using the June 2006 version of the first-stage parser of Charniak and Johnson (2005), a lexicalised, generative, probabilistic parser achieving competitive performance on Wall Street Journal text. We compare the probability of the highest ranked tree for the grammatical sentence in the pair to the probability of the highest ranked tree for the ungrammatical sentence. 4 Results Figure 2 shows the results for the Foster corpus. For ranges of 4 points on the logarithmic scale, the bars depict how many sentence pairs have a probability ratio within the respective range. For example, there are 48 pairs (5th bar from left) for which the correction has a parse probability which is b</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Courseto-fine n-best-parsing and maxent discriminative reranking. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
<author>Frank Keller</author>
</authors>
<title>Probabilistic grammars as models of gradience in language processing.</title>
<date>2006</date>
<booktitle>Gradience in Grammar: Generative Perspectives,</booktitle>
<pages>227--245</pages>
<editor>In Gisbert Fanselow, C. F´ery, R. Vogel, and M. Schlesewsky, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3438" citStr="Crocker and Keller (2006)" startWordPosition="525" endWordPosition="528">abilities of candidate corrections which are generated by arbitrarily deleting, inserting and substituting articles, prepositions and auxiliaries and changing the inflection of verbs and nouns. Foster et al. (2008) compare the parse probability returned by a parser trained on a regular treebank to the probability returned by the same parser trained on a “noisy” treebank and use the difference to decide whether the sentence is ill-formed. Research in the field of psycholinguistics has explored the link between frequency and grammaticality, often focusing on borderline acceptable sentences (see Crocker and Keller (2006) for a discussion of the literature). Koonst-Garboden and Jaeger (2003) find a weak correlation between the 176 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 176–179, Paris, October 2009. c�2009 Association for Computational Linguistics frequency ratios of competing surface realisations and human acceptability judgements. Hale (2003) calculates the information-theoretic load of words in sentences assuming that they were generated according to a probabilistic grammar and finds that these values are good predictors for observed reading time and other meas</context>
</contexts>
<marker>Crocker, Keller, 2006</marker>
<rawString>Matthew W. Crocker and Frank Keller. 2006. Probabilistic grammars as models of gradience in language processing. In Gisbert Fanselow, C. F´ery, R. Vogel, and M. Schlesewsky, editors, Gradience in Grammar: Generative Perspectives, pages 227– 245. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Josef van Genabith</author>
</authors>
<title>Adapting a WSJ-trained parser to grammatically noisy text.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Foster, Wagner, van Genabith, 2008</marker>
<rawString>Jennifer Foster, Joachim Wagner, and Josef van Genabith. 2008. Adapting a WSJ-trained parser to grammatically noisy text. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>Good Reasons for Noting Bad Grammar: Empirical Investigations into the Parsing of Ungrammatical Written English.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Dublin, Trinity College.</institution>
<contexts>
<context position="4992" citStr="Foster, 2005" startWordPosition="761" endWordPosition="762">tions, and a parallel error corpus that consists of authentic grammatical sentences and automatically induced errors. Using parallel corpora allows us to compare pairs of sentences that have the same or very similar lexical content and differ only with respect to their grammaticality. A corpus with automatically induced errors is included because such a corpus is much larger and controlled error insertion allows us to examine directly the effect of a particular error type. The first parallel error corpus contains 1,132 sentence pairs each comprising an ungrammatical sentence and a correction (Foster, 2005). The sentences are taken from written texts and contain either one or two grammatical errors. The errors include those made by native English speakers. We call this the Foster corpus. The second corpus is a learner corpus. It contains transcribed spoken utterances produced by learners of English of varying L1s and levels of experience in a classroom setting. Wagner et al. (2009) manually corrected 500 sentences of the transcribed utterances, producing a parallel error corpus which we call Gonzaga 500. The third parallel corpus contains 199,600 sentences taken from the British National Corpus </context>
</contexts>
<marker>Foster, 2005</marker>
<rawString>Jennifer Foster. 2005. Good Reasons for Noting Bad Grammar: Empirical Investigations into the Parsing of Ungrammatical Written English. Ph.D. thesis, University of Dublin, Trinity College.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>The information conveyed by words in sentences.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="3814" citStr="Hale (2003)" startWordPosition="577" endWordPosition="578">rence to decide whether the sentence is ill-formed. Research in the field of psycholinguistics has explored the link between frequency and grammaticality, often focusing on borderline acceptable sentences (see Crocker and Keller (2006) for a discussion of the literature). Koonst-Garboden and Jaeger (2003) find a weak correlation between the 176 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 176–179, Paris, October 2009. c�2009 Association for Computational Linguistics frequency ratios of competing surface realisations and human acceptability judgements. Hale (2003) calculates the information-theoretic load of words in sentences assuming that they were generated according to a probabilistic grammar and finds that these values are good predictors for observed reading time and other measures of cognitive load. 3 Experimental Setup The aim of this experiment is to find out to what extent ungrammatical sentences behave differently from correct sentences as regards their parse probabilities. There are two types of corpora we study: two parallel error corpora that consist of authentic ungrammatical sentences and manual corrections, and a parallel error corpus </context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>John Hale. 2003. The information conveyed by words in sentences. Journal of Psycholinguistic Research, 32(2):101–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Koontz-Garboden</author>
<author>T Florian Jaeger</author>
</authors>
<title>An empirical investigation of the frequencygrammaticality correlation hypothesis. Student essay received or downloaded on</title>
<date>2003</date>
<pages>2006--03</pages>
<marker>Koontz-Garboden, Jaeger, 2003</marker>
<rawString>Andrew Koontz-Garboden and T. Florian Jaeger. 2003. An empirical investigation of the frequencygrammaticality correlation hypothesis. Student essay received or downloaded on 2006-03-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Automatic grammar correction for second-language learners.</title>
<date>2006</date>
<booktitle>In Interspeech 2006 - 9th ICSLP,</booktitle>
<pages>1978--1981</pages>
<contexts>
<context position="2689" citStr="Lee and Seneff (2006)" startWordPosition="411" endWordPosition="414">icality and formal languages determine the sentence’s grammaticality. In this paper, we explore one aspect of this question by using three parallel error corpora to determine the effect of common English grammatical errors on the parse probability of the most likely parse tree returned by a generative probabilistic parser. 2 Related Work The probability of a parse tree has been used before in error detection systems. Sun et al. (2007) report only a very modest improvement when they include a parse probability feature in their system whose features mostly consist of linear sequential patterns. Lee and Seneff (2006) detect ungrammatical sentences by comparing the parse probability of a possibly ill-formed input sentence to the parse probabilities of candidate corrections which are generated by arbitrarily deleting, inserting and substituting articles, prepositions and auxiliaries and changing the inflection of verbs and nouns. Foster et al. (2008) compare the parse probability returned by a parser trained on a regular treebank to the probability returned by the same parser trained on a “noisy” treebank and use the difference to decide whether the sentence is ill-formed. Research in the field of psycholin</context>
</contexts>
<marker>Lee, Seneff, 2006</marker>
<rawString>John Lee and Stephanie Seneff. 2006. Automatic grammar correction for second-language learners. In Interspeech 2006 - 9th ICSLP, pages 1978–1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Correcting misuse of verb forms.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="11144" citStr="Lee and Seneff (2008)" startWordPosition="1799" endWordPosition="1802">n three parallel error corpora using a generative, probabilistic parser and examined the parse probability of the most likely analysis of each sentence. We find that grammatical errors have some negative effect on the probability assigned to the best parse, a finding which corroborates previous evidence linking sentence grammaticality to frequency. In our experiment, we approximate sentence probability by looking only at the most likely analysis – it might be useful to see if the same effect holds if we sum 1This raises the issue of covert errors, resulting in grammatical sentence structures. Lee and Seneff (2008) give the example I am preparedfor the exam which was produced by a learner of English instead of I am preparing for the exam. These occur in authentic error corpora and cannot be completely avoided when automatically introducing errors. over parse trees. To fully exploit parse or sentence probability in an error detection system, it is necessary to fully account for the effect on probability of 1) non-structural factors such as sentence length and 2) particular error types. This study represents a contribution towards the latter. Acknowledgements We are grateful to James Hunter from Gonzaga U</context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008. Correcting misuse of verb forms. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
<author>Xiaohua Liu</author>
<author>Gao Cong</author>
<author>Ming Zhou</author>
<author>Zhongyang Xiong</author>
<author>John Lee</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Detecting erroneous sentences using automatically mined sequential patterns.</title>
<date>2007</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="2506" citStr="Sun et al. (2007)" startWordPosition="383" endWordPosition="386">e by its probabilistic model could be leveraged in some way to Jennifer Foster NCLT School of Computing Dublin City University, Ireland. jfoster@computing.dcu.ie Figure 1: Grammaticality and formal languages determine the sentence’s grammaticality. In this paper, we explore one aspect of this question by using three parallel error corpora to determine the effect of common English grammatical errors on the parse probability of the most likely parse tree returned by a generative probabilistic parser. 2 Related Work The probability of a parse tree has been used before in error detection systems. Sun et al. (2007) report only a very modest improvement when they include a parse probability feature in their system whose features mostly consist of linear sequential patterns. Lee and Seneff (2006) detect ungrammatical sentences by comparing the parse probability of a possibly ill-formed input sentence to the parse probabilities of candidate corrections which are generated by arbitrarily deleting, inserting and substituting articles, prepositions and auxiliaries and changing the inflection of verbs and nouns. Foster et al. (2008) compare the parse probability returned by a parser trained on a regular treeba</context>
</contexts>
<marker>Sun, Liu, Cong, Zhou, Xiong, Lee, Lin, 2007</marker>
<rawString>Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee, and Chin-Yew Lin. 2007. Detecting erroneous sentences using automatically mined sequential patterns. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal, 26(3).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>