<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978509">
Critical Tokenization and its Properties
</title>
<author confidence="0.998779">
Jin Guo*
</author>
<affiliation confidence="0.996791">
National University of Singapore
</affiliation>
<bodyText confidence="0.997472764705883">
Tokenization is the process of mapping sentences from character strings into strings of words.
This paper sets out to study critical tokenization, a distinctive type of tokenization following the
principle of maximum tokenization. The objective in this paper is to develop its mathematical
description and understanding.
The main results are as follows: (1) Critical points are all and only unambiguous token
boundaries for any character string on a complete dictionary; (2) Any critically tokenized word
string is a minimal element in the partially ordered set of all tokenized word strings with respect
to the word string cover relation; (3) Any tokenized string can be reproduced from a critically
tokenized word string but not vice versa; (4) Critical tokenization forms the sound mathemati-
cal foundation for categorizing tokenization ambiguity into critical and hidden types, a precise
mathematical understanding of conventional concepts like combinational and overlapping ambi-
guities; (5) Many important maximum tokenization variations, such as forward and backward
maximum matching and shortest tokenization, are all true subclasses of critical tokenization.
It is believed that critical tokenization provides a precise mathematical description of the
principle of maximum tokenization. Important implications and practical applications of critical
tokenization in effective ambiguity resolution and in efficient tokenization implementation are
also carefully examined.
</bodyText>
<sectionHeader confidence="0.992281" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.895487823529412">
Words, and tokens in general, are the primary building blocks in almost all linguistic
theories (e.g., Gazdar, Klein, Pullum, and Sag 1985; Hudson 1984) and language pro-
cessing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string,
tokenization, the process of mapping sentences from character strings to strings of
words, is the initial step in natural language processing (Webster and Kit 1992).
Since in written Chinese there is no explicit word delimiter (equivalent to the blank
space in written English), the problem of Chinese sentence tokenization has been the
focus of considerable research efforts, and significant advancements have been made
(e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai
1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang
1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen
1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan,
and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat
and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and
Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et
al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990;
Yeh and Lee 1991; Zhang, Chen, and Chen 1991).
</bodyText>
<note confidence="0.855073">
* Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail:
</note>
<email confidence="0.630199">
guojin@iss.nrts.sg
</email>
<note confidence="0.898742">
C.) 1997 Association for Computational Linguistics
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999568833333334">
The tokenization problem exists in almost all natural languages, including Japanese
(Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German
(Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various
media, such as continuous speech and cursive handwriting, and in numerous appli-
cations, such as translation, recognition, indexing, and proofreading.
For Chinese, sentence tokenization is still an unsolved problem, which is in part
due to its overall complexity but also due to the lack of a good mathematical de-
scription and understanding of the problem. The theme in this paper is therefore to
develop such a mathematical description.
In particular, this paper focuses on critical tokenization&apos;, a distinctive type of
tokenization following the maximum principle. What is to be established in this paper
is the notion of critical tokenization itself, together with its precise descriptions and
well-proved properties.
We will prove that critical points are all and only unambiguous token boundaries
for any character string on a complete dictionary. We will show that any critically to-
kenized word string is a minimal element in the partially ordered set of all tokenized
word strings on the word string cover relation. We will also show that any tokenized
string can be reproduced from a critically tokenized word string but not vice versa. In
other words, critical tokenization is the most compact representation of tokenization.
In addition, we will show that critical tokenization forms a sound mathematical foun-
dation for categorizing critical ambiguity and hidden ambiguity in tokenizations,
which provides a precise mathematical understanding of conventional concepts like
combinational and overlapping ambiguities. Moreover, we will confirm that some im-
portant maximum tokenization variations, such as forward and backward maximum
matching and shortest tokenization, are all subclasses of critical tokenization.
Based on a mathematical understanding of tokenization, we reported, in Guo
(1997), a series of interesting findings. For instance, there exists an optimal algorithm
that can identify all and only critical points, and thus all unambiguous token bound-
aries, in time proportional to the input character string length but independent of
the size of the tokenization dictionary. Tested on a representative corpus, about 98%
of the critical fragments generated are by themselves desired tokens. In other words,
about 98% close-dictionary tokenization accuracy can be achieved efficiently without
disambiguation.
Another interesting finding is that, for those critical fragments with critical ambi-
guities, by replacing the conventionally adopted meaning preservation criterion with
the critical tokenization criterion, disagreements among (human) judges on the ac-
ceptability of a tokenization basically become non-existent. Consequently, an objective
(human) analysis and annotation of all (critical) tokenizations in a corpus becomes
achievable, which in turn leads to some important observations. For instance, we ob-
served from a Chinese corpus of four million morphemes a very strong tendency to
have one tokenization per source. Naturally, this observation suggests tokenization disam-
biguation strategies notably different from the mainstream best-path-finding strategy.
For instance, the simple strategy of tokenization by memorization alone could easily ex-
hibit critical ambiguity resolution accuracy of no less than 90%, which is notably higher
than what has been achieved in the literature. Moreover, it has been observed that crit-
ical tokenization can also provide helpful guidance in identifying hidden ambiguities
and in determining unregistered (unknown) tokens (Guo 1997). While these are just
some of the very primitive findings, they are nevertheless promising and motivate
</bodyText>
<footnote confidence="0.809853">
1 All terms mentioned here will be precisely defined later in this paper.
</footnote>
<page confidence="0.986715">
570
</page>
<note confidence="0.899574">
Guo Critical Tokenization
</note>
<bodyText confidence="0.998871266666667">
us to rigorously formalize the tokenization problem and to carefully explore logical
consequences.
The rest of the paper is organized as follows: In Section 2, we formally define
the string generation and tokenization operations that form the basis of our frame-
work. In Section 3, we will study tokenization ambiguities and explore the concepts
of critical points and critical fragments. In Section 4, we define the word string cover
relation and prove it to be a partial order, define critical tokenization as the set of min-
imal elements of the tokenization partially ordered set, and illustrate the relationship
between critical tokenization and string tokenization. Section 5 discusses the relation-
ship between critical tokenization and various types of tokenization ambiguities, while
Section 6 addresses the relationship between critical tokenization and various types
of maximum tokenizations. Finally, in Sections 7 and 8, after discussing some helpful
implications of critical tokenization in effective tokenization disambiguation and in
efficient tokenization implementation, we suggest areas for future research and draw
some conclusions.
</bodyText>
<sectionHeader confidence="0.939311" genericHeader="categories and subject descriptors">
2. Generation and Tokenization
</sectionHeader>
<bodyText confidence="0.998339625">
In order to address the topic clearly and accurately, a precise and well-defined formal
notation is required. What is used in this paper is primarily from elementary Boolean
algebra and Formal Language Theory, which can be found in most graduate-level
textbooks on discrete mathematics. This section aims at refreshing several simple terms
and conventions that will be applied throughout this paper and at introducing the
two new concepts of character string generation and tokenization. For the remaining
basic concepts and conventions, we mainly follow Aho and Ullman (1972, Chapter 0,
Mathematical Preliminaries), and Kolman and Busby (1987).
</bodyText>
<subsectionHeader confidence="0.9051875">
2.1 Character, Alphabet, and Character String
Definition 1
</subsectionHeader>
<bodyText confidence="0.946585">
An alphabet E = {a, b, c,. .} is a finite set of symbols. Each symbol in the alphabet is a
character. The alphabet size is the number of characters in the alphabet and is denoted
1E1. Character strings over an alphabet E are defined2 in the following manner:
</bodyText>
<listItem confidence="0.9956054">
1. e is a character string over E. e is called the empty character string.
2. If S is a character string over E and a is a character in E, then Sa is a
character string over E.
3. S&apos; is a character string over E if and only if its being so follows from (1)
and (2).
</listItem>
<bodyText confidence="0.9999456">
The length of a character string S is the number of characters in the string and
is denoted IS. A position in a character string is the position after a character in the
string. If characters in a character string are indexed from 1 to n, then positions in the
string are indexed from 0 to n, with 0 for the position before the first character and n
for that after the last character.
</bodyText>
<footnote confidence="0.405168">
2 This definition is adapted from Aho and Ullman (1972, 15).
</footnote>
<page confidence="0.989187">
571
</page>
<note confidence="0.594625">
Computational Linguistics Volume 23, Number 4
Example 1
</note>
<bodyText confidence="0.99961525">
The set of 26 upper case and 26 lower case English characters forms the English
alphabet E = {a, b,. . . ,z, A, B,. , Z} . S = thisishisbook is a character string over the
alphabet. Its string length is 13.
In this paper, characters are represented with small characters a, b, c, or their
subscript form ak, bk, and ck. The capital letter S or its expanded form S = c1 c, is
used to represent a character string. We let E* denote the set containing all character
strings over E including e, and E+ denote the set of all character strings over E but
excluding e.
</bodyText>
<subsectionHeader confidence="0.912638">
2.2 Word, Dictionary, and Word String
Definition 2
</subsectionHeader>
<bodyText confidence="0.998987">
Let alphabet E = {a, b, c, . . .} be a finite set of characters. A dictionary D is a set of
character strings over the alphabet E. That is, D = {x, y, z, . . .} C E*. Any element in
the dictionary is a word. The dictionary size is the number of words in the dictionary
and is denoted IDI. Word strings over a dictionary D are defined in the following
manner:
</bodyText>
<listItem confidence="0.9964994">
1. v is a word string over D. v is called the empty word string.
2. If W is a word string over D and w is a word in D, then Ww is a word
string over D.
3. W&apos; is a word string over D if and only if its being so follows from (1)
and (2).
</listItem>
<bodyText confidence="0.99357195">
The length of a word string W is the number of words in the string and is denoted
I WI. We let D* denote the set containing all word strings over D, including v and let
D+ denote the set of all word strings over D but excluding v.
Example 1 (cont.)
The set D = {this, is, his, book} is a tiny English dictionary from the English alphabet.
Both his and book are words over the English alphabet. The dictionary size is 4, i.e.,
IDI = 4. &amp;quot;this is his book&amp;quot; is a word string. Its string length is 4.
To differentiate between character string and word string, blank spaces are added
between words in word strings. For example, &amp;quot;this is his book&amp;quot; represents a word string
of length 4 (four words concatenated) while thisishisbook consists of a character string
of length 13 (13 characters in sequence). Slash / is sometimes used as a (hidden) word
delimiter. For instance, this/is/his/book is an equivalent representation to &amp;quot;this is his
book&amp;quot;.
Generally, capital letters X, Y, Z, and W, or their expanded forms such as W =
wrn, represent word strings. Small letters x, y, z, and w, or their expanded forms
such as w = c1 cn, represent both words as elements in a dictionary and character
strings over an alphabet. In other words, they are both w E D and w E E*. The word
string made up of the single word w alone is represented by w1. In cases where context
makes it clear, the superscript can be omitted and w is also used for representing the
single word string w1.
</bodyText>
<page confidence="0.986838">
572
</page>
<note confidence="0.888833">
Guo Critical Tokenization
</note>
<subsectionHeader confidence="0.8466605">
2.3 Character String Generation
Definition 3
</subsectionHeader>
<bodyText confidence="0.993261">
Let E = {a, b, c, . . .} be an alphabet and D = {x, y, z, ...} be a dictionary over the
alphabet. The character string generation operation G is a mapping G: D*
defined as:
</bodyText>
<listItem confidence="0.997062142857143">
1. Empty word string v is mapped to empty character string e. That is,
G(v) = e.
2. Single word string w1 is mapped to the character string of the single
word. That is, G(w1) = w.
3. If W is a word string over dictionary D and w is a word in D, then, the
word string Ww is mapped to the concatenation of character string G(W)
and G(w). That is, G(Ww) = G(W)G(w).
</listItem>
<bodyText confidence="0.848139428571429">
G(W) is said to be the generated character string of the word string W from dictio-
nary D.
Note that the character string generation operation G is a homomorphism (Aho and
Ullman 1972, 17) with property G(w1) = w.
Example 1 (cont.)
The character string thisishisbook is the generated character string of the word string
&amp;quot;this is his book&amp;quot;. That is, Gr this is his book&amp;quot;) = thisishisbook.
</bodyText>
<subsectionHeader confidence="0.9987165">
2.4 Character String Tokenization
Definition 4
</subsectionHeader>
<bodyText confidence="0.999545818181818">
The character string tokenization operation T is a mapping TD: E* 2D* defined
as: if S is a character string in E*, then TD(S) is the set of dictionary word strings
mapped by the character string generation operation G to the character string S. That
is, TD(S) = {WIG(W) = S, W E D*}. Any word string W in TD(S) is a tokenized word
string, or simply a tokenization, of the character string S.
Sometimes the character string tokenization operation is emphasized as the ex-
haustive tokenization operation or ET operation for short. In addition, the tokenized
word string or tokenization is emphasized as the exhaustively tokenized word string
or exhaustive tokenization or ET tokenization for short.
Note that the character string tokenization operation TD is the inverse homomorphism
(Aho and Ullman 1972, 18) of the character string generation operation G.
</bodyText>
<subsectionHeader confidence="0.419601">
Example 1 (cont.)
</subsectionHeader>
<bodyText confidence="0.97817">
Given character string thisishisbook, for the tiny English dictionary D = {this, is, his,
book}, there is TD(thisishisbook) = {&amp;quot;this is his book&amp;quot;}. In other words, the word string
&amp;quot;this is his book&amp;quot; is the only tokenization over the dictionary D.
Given dictionary D&apos; = {th, this, is, his, book}, in which th is also a word, there is
(thisishisbook) {&amp;quot;th is is his book&amp;quot;, &amp;quot;this is his book&amp;quot;}. In other words, the character
string has two tokenizations over the dictionary D&apos;.
</bodyText>
<subsectionHeader confidence="0.818553">
Example 2
</subsectionHeader>
<bodyText confidence="0.994963333333333">
For character string fundsand and the tiny English dictionary D = {fund, funds, and,
sand}, there is TD(fundsand) {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}. In other words, both &amp;quot;funds
and&amp;quot; and &amp;quot;fund sand&amp;quot; are tokenizations of character string fundsand.
</bodyText>
<page confidence="0.995901">
573
</page>
<note confidence="0.461563">
Computational Linguistics Volume 23, Number 4
</note>
<subsectionHeader confidence="0.986151">
2.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999978194444444">
Our intention, in formally defining characters and words, is to establish our mathe-
matical system clearly and accurately. To keep discussion concise, the definitions of
elementary concepts such as strings and substrings, although widely used in this pa-
per, will be taken for granted. We limit our basic notion to what has already been
defined in Aho and Ullman (1972) and Kolman and Busby (1987).
Mathematically, word strings are nothing but symbol strings, with each symbol
representing a word in the dictionary. In that sense, the word string definition is
redundant as it is already covered by the definition of character string. However,
since the relationships between character strings and word strings are very important
in this paper, we believe it to be appropriate to list both definitions explicitly.
What is new in this section is mathematical definitions for character string genera-
tion and tokenization. We consider them fundamental to our mathematical description
of the string tokenization problem.
There are two points worth highlighting here. The first relates to the introduction
of the character string generation operation. In the literature, the tokenization problem
is normally modeled independently with no connection whatsoever with the charac-
ter string generation problem. By contrast, we model tokenization and generation as
inverse problems to each other. In this way, we establish a well-defined mathematical
system consisting of an alphabet, a dictionary, and the (generation) homomorphism
(operation) and its inverse defined on the alphabet and dictionary. As will be seen
throughout this paper, the establishment of the generation operation renders various
types of tokenization problems easy to describe. The generation problem is relatively
simple and easy to manage, so any modeling of the tokenization problem as its inverse
(that is, as the generation problem) should make it more tractable.
The second point is in regard to the tokenization definition. In the literature, the
string tokenization operation is normally required to generate a unique tokenized word
string. Following such a definition of tokenization, introducing tokenization disam-
biguation at the very beginning is inevitable. We believe this to be a pitfall that has
trapped many researchers. In contrast, we define the character string tokenization op-
eration as the inverse operation (inverse homomorphism) of the character string gen-
eration operation (homomorphism). Naturally, the result of the tokenization operation
is a set of tokenizations rather than a single word string. Such treatment suggests
that we could use the divide-and-conquer problem-solving strategy—to decompose
the complex string tokenization problem into several smaller and, hopefully, simpler
subproblems. That is the basis of our two-stage, five-step iterative problem-solving
strategy for sentence tokenization (Guo 1997).
</bodyText>
<subsectionHeader confidence="0.751865">
3. Critical Point and Fragment
</subsectionHeader>
<bodyText confidence="0.999711166666667">
After clarifying both sentence generation and tokenization operations, we undertake
next to further clarify sentence tokenization ambiguities. Among all the concepts to
be introduced, critical points and critical fragments are probably two of the most
important. We will prove that, for any character string on a complete tokenization
dictionary, its critical points are all and only unambiguous token boundaries, and its
critical fragments are the longest substrings with all inner positions ambiguous.
</bodyText>
<subsectionHeader confidence="0.99945">
3.1 Ambiguity
</subsectionHeader>
<bodyText confidence="0.994241">
Let E be an alphabet, D a dictionary, and S a character string over the alphabet.
</bodyText>
<page confidence="0.997191">
574
</page>
<note confidence="0.521137">
Guo Critical Tokenization
</note>
<subsectionHeader confidence="0.751004">
Definition 5
</subsectionHeader>
<bodyText confidence="0.981950052631579">
The character string S from the alphabet E has tokenization ambiguity on dictionary
D, if ITD(S)1 &gt; 1. S has no tokenization ambiguity, if ITD(S)1 = 1. S is ill-formed on
dictionary D, if I TD(S)I = 0. A tokenization W E TD(S) has tokenization ambiguity, if
there exists another tokenization W&apos; E TD(S), W&apos; W.
Example 2 (cont.)
Since TD(fundsand) = {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}, i.e., ITD(fundsand)I = 2 &gt; 1, the
character string fundsand has tokenization ambiguity. In other words, it is ambiguous
in tokenization. Moreover, the tokenization &amp;quot;funds and&amp;quot; has tokenization ambiguity
since there exists another possible tokenization &amp;quot;fund sand&amp;quot; for the same character
string.
This definition is quite intuitive. If a character string could be tokenized in multiple
ways, it would be ambiguous in tokenization. If a character string could only be
tokenized in a unique way, it would have no tokenization ambiguity. If a character
string could not be tokenized at all, it would be ill-formed. In this latter case, the
dictionary is incomplete.
Intuitively, a position in a character string is ambiguous in tokenization or is an
ambiguous token boundary if it is a token boundary in one tokenization but not in
another. Formally, let S = cn be a character string over an alphabet E and let D
be a dictionary over the alphabet.
</bodyText>
<subsectionHeader confidence="0.894291">
Definition 6
</subsectionHeader>
<bodyText confidence="0.9395584">
Position p has tokenization ambiguity or is an ambiguous token boundary, if there
are two tokenizations X = x1 ... xs and Y = ytin TD(S), such that G(xi . • • xu)
cp and G(xu±i xs) = cp+i c, for some index u, and for any index v, there
is neither G(yi yv) = cp nor G(yn+i yt) -= cp+i cn. Otherwise, the position
has no tokenization ambiguity, or is an unambiguous token boundary.
Example 1 (cont.)
Given a typical English dictionary and the character string S = thisishisbook, all three
positions after character s are unambiguous in tokenization or are unambiguous to-
ken boundaries, since all possible tokenizations must take these positions as token
boundaries.
</bodyText>
<subsectionHeader confidence="0.481641">
Example 2 (cont.)
</subsectionHeader>
<bodyText confidence="0.99015425">
Given a typical English dictionary and the character string S =fundsand, the position
after the middle character s is ambiguous in tokenization or is an ambiguous token
boundary since it is a token boundary in tokenization &amp;quot;funds and&amp;quot; but not in another
tokenization &amp;quot;fund sand&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.999961">
3.2 Complete Dictionary
</subsectionHeader>
<bodyText confidence="0.9996195">
To avoid ill-formedness in sentence tokenization, we now introduce the concept of a
complete tokenization dictionary.
</bodyText>
<subsectionHeader confidence="0.834204">
Definition 7
</subsectionHeader>
<bodyText confidence="0.9292425">
A dictionary D over an alphabet E is complete if for any character string S from the
alphabet, S E E*, there is I TD(S)I &gt; 1.
</bodyText>
<page confidence="0.996537">
575
</page>
<subsectionHeader confidence="0.655896">
Computational Linguistics Volume 23, Number 4
</subsectionHeader>
<bodyText confidence="0.970762">
That is, for any character string S = c, from the alphabet, there exists at least
one word string W = wn, with S as its generated character string, G(W) = S.
</bodyText>
<subsectionHeader confidence="0.705427">
Theorem 1
</subsectionHeader>
<bodyText confidence="0.993675">
A dictionary D over an alphabet E is complete if and only if all the characters in the
alphabet are single-character words in the dictionary
</bodyText>
<subsectionHeader confidence="0.968053">
Proof
</subsectionHeader>
<bodyText confidence="0.9999922">
On the one hand, every single character is also a character string (of length 1). To
ensure that such a single-character string is being tokenized, the single character must
be a word in the dictionary. On the other hand, if all the characters are words in the
dictionary, any character string can at least be tokenized as a string of single-character
words. 0
Theorem 1 spells out a simple way of making any dictionary complete, which calls
for adding all the characters of an alphabet into a dictionary as single-character words.
This is referred to as the dictionary completion process. If not specified otherwise,
in this paper, when referring to a complete dictionary or tokenization dictionary, we
mean the dictionary after the completion process.
</bodyText>
<subsectionHeader confidence="0.999984">
3.3 Critical Point and Fragment
</subsectionHeader>
<bodyText confidence="0.9978155">
Let S = c1 cp be a character string over the alphabet E and let D be a dictionary
over the alphabet. In addition, let TD (S) be the tokenization set of S on D.
</bodyText>
<subsectionHeader confidence="0.378731">
Definition 8
</subsectionHeader>
<bodyText confidence="0.425973833333333">
Position p in character string S = c1 c, is a critical point, if for any word string
W = wi w„, in TD(S), there exists an index k, 0 &lt; k &lt; m, such that G(wi • • • &apos;wk) =
cp and G(wk±i wm) = cp+i ... cp. In particular, the starting position 0 and the
ending position n are the two ordinary critical points. Substring cp+1 cq is a critical
fragment of S on D, if both p and q are critical points and any other position r in
between them, p &lt; r &lt; q, is not a critical point.
</bodyText>
<subsectionHeader confidence="0.448225">
Example 1 (cont.)
</subsectionHeader>
<bodyText confidence="0.996475666666667">
Given a typical English dictionary, there are five critical points in the character string
S = thisishisbook. They are 0, 4, 6, 9, and 13. The corresponding four critical fragments
are this, is, his, and book.
</bodyText>
<subsectionHeader confidence="0.47289">
Example 2 (cont.)
</subsectionHeader>
<bodyText confidence="0.991877571428571">
Given a typical English dictionary, there is no extraordinary critical point in the char-
acter string S fundsand. It is by itself the only critical fragment of this character
string.
Given a complete tokenization dictionary, it is obvious that all single-character crit-
ical fragments or, more generally, single-character strings, possess unique tokenization.
That is, they possess neither ambiguity nor ill-formedness in tokenization. However,
the truth of the statement below (Lemma 1) is less obvious.
</bodyText>
<subsectionHeader confidence="0.683956">
Lemma 1
</subsectionHeader>
<bodyText confidence="0.8606825">
For a complete tokenization dictionary, all multicharacter critical fragments and all of
their inner positions are ambiguous in tokenization.
</bodyText>
<page confidence="0.997205">
576
</page>
<note confidence="0.537776">
Guo Critical Tokenization
</note>
<subsectionHeader confidence="0.875458">
Proof
</subsectionHeader>
<bodyText confidence="0.9984045">
Let S = c1 cn, n&gt; 1, be a multicharacter critical fragment. Because the tokenization
dictionary is complete, the critical fragment can at least be tokenized as a string of
single-character words. On the other hand, because it is a critical fragment, for any
position p, 1 &lt; p &lt; n — 1, there must exist a tokenization W = zy„, in TD(S)
such that for any index k, 0 &lt; k &lt; m, there is neither G(wi .. • Wk) c1 cp nor
G(wk+i . • . Wm) -= Cp+1 • • • cn• As this tokenization differs from the above-mentioned
tokenization of the string of single-character words, the critical fragment has at least
two different tokenizations and thus has tokenization ambiguity. 0
</bodyText>
<subsectionHeader confidence="0.752187">
Theorem 2
</subsectionHeader>
<bodyText confidence="0.9995375">
For any character string on a complete tokenization dictionary, its critical points are
all and only unambiguous token boundaries.
</bodyText>
<subsectionHeader confidence="0.834335">
Proof
</subsectionHeader>
<bodyText confidence="0.994259">
By Lemma 1, all positions within critical fragments are ambiguous in tokenization. By
Definition 8, critical points are unambiguous in tokenization. 0
</bodyText>
<subsectionHeader confidence="0.746726">
Corollary
</subsectionHeader>
<bodyText confidence="0.999745">
For any character string on a complete tokenization dictionary, its critical fragments
are the longest substrings with all inner positions ambiguous.
</bodyText>
<subsectionHeader confidence="0.897705">
Proof
</subsectionHeader>
<bodyText confidence="0.996969">
By Theorem 2. 0
</bodyText>
<subsectionHeader confidence="0.983062">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.985131173913043">
In this section, we have described sentence tokenization ambiguity from three differ-
ent angles: character strings, tokenizations, and individual string positions. The basic
idea is conceptually simple: ambiguity exists when there are different means to the
same end. For instance, as long as a character string has multiple tokenizations, it is
ambiguous.
This description of ambiguity is complete. Given a character string and a dic-
tionary, it is always possible to answer deterministically whether or not a string is
ambiguous in tokenization. Conceptually, for any character string, by checking every
one of its possible substrings in a dictionary, and then by enumerating all valid word
concatenations, all word strings with the character string as their generated character
string can be produced. Just counting the number of such word strings will provide
the answer to whether or not the character string is ambiguous.
Some researchers question the validity of the complete dictionary assumption.
Here we argue that, even in the strictest linguistic sense, there exists no single character
that cannot be used as a single-character word in sentences. In any case, any natural
language must allow us to directly refer to single characters. For instance, you could
say &amp;quot;character x has many written forms&amp;quot; or &amp;quot;the character x in this word can be
omitted&amp;quot; for any character x.&apos;
3 Even so, some researchers might still insist that the character x here is just for temporary use and
cannot be regarded as a regular word with the many linguistic properties generally associated with
words. Understanding the importance of such a distinction, we will use the more generic term token,
rather than the loaded term word, when we need to highlight the distinction. It must be added,
however, that the two are largely used interchangeably in this paper.
</bodyText>
<page confidence="0.959936">
577
</page>
<note confidence="0.423238">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999972151515151">
The validity of the complete dictionary assumption can also be justified from an
engineering perspective. To ensure a so-called soft landing, any practical application
system must be designed so that every input character string can always be tokenized.
In other words, a complete dictionary is an operational must. Moreover, without such
a complete dictionary, it would not be possible to avoid ill-formedness in sentence
tokenization nor to make the generation-tokenization system for character and words
closed and complete. Without such definitions of well-formedness, any rigorous formal
study would be impossible.
The concepts of critical point and critical fragment are fundamental to our sen-
tence tokenization theory. By adopting the complete dictionary assumption, it has
been proven that critical points are all and only unambiguous token boundaries while
critical fragments are the longest substrings with all inner positions ambiguous.
This is a very strong and significant statement. It provides us with a precise un-
derstanding of what and where tokenization ambiguities are. Although the proof itself
is easy to follow, the result has nonetheless been a surprise. As demonstrated in Guo
(1997), many researchers have tried but failed to answer the question in such a precise
and complete way. Consequently, while they proposed many sophisticated algorithms
for the discovery of ambiguity (and certainty), they never were able to arrive at such
a concise and complete solution.
As critical points are all and only unambiguous token boundaries, an identifica-
tion of all of them would allow for a long character string to be broken down into
several short but fully ambiguous critical fragments. As shown in Guo (1997), critical
points can be completely identified in linear time. Moreover, in practice, most criti-
cal fragments are dictionary tokens by themselves, and the remaining nondictionary
fragments are generally very short. In short, the understanding of critical points and
fragments will significantly assist us in both efficient tokenization implementation and
tokenization ambiguity resolution.
The concepts of critical point and critical fragment are similar to those of segment
point and character segment in Wang (1989, 37), which were defined on a sentence
word graph for the purpose of analyzing the computational complexity of his new
tokenization algorithm. However, Wang (1989) neither noticed their connection with
tokenization ambiguities nor realized the importance of the complete dictionary as-
sumption, and hence failed to demonstrate their crucial role in sentence tokenization.
</bodyText>
<sectionHeader confidence="0.702344" genericHeader="method">
4. Critical Tokenization
</sectionHeader>
<bodyText confidence="0.9987886">
This section seeks to disclose an important structure of the set of different tokeniza-
tions. We will see that different tokenizations can be linked by the cover relationship
to form a partially ordered set. Based on that, we will establish the notion of criti-
cal tokenization and prove that every tokenization is a subtokenization of a critical
tokenization, but no critical tokenization has true supertokenization.
</bodyText>
<subsectionHeader confidence="0.9731375">
4.1 Cover Relationship
Definition 9
</subsectionHeader>
<bodyText confidence="0.9695165">
Let X and Y be word strings. X covers Y, or X has a cover relation to Y, denoted
X &lt; Y, if for any substring Xs of X, there exists substring Y, of Y, such that I Xs 1 &lt; Y
and G(X) = G(Ys). If X &lt; Y, then X is called a covering word string of Y, and Y a
covered word string of X.
Intuitively, X &lt; Y implies I XI &lt; YI. In other words, shorter word strings cover
longer word strings. However, an absence of X &lt; Y does not imply the existence of
</bodyText>
<page confidence="0.97997">
578
</page>
<note confidence="0.360809">
Guo Critical Tokenization
</note>
<bodyText confidence="0.706164428571429">
Y &lt; X. Some word strings do not cover each other. In other words, shorter word
strings do not always cover longer word strings.
Example 1 (cont.)
The word string &amp;quot;this is his book&amp;quot; covers the word string &amp;quot;th is is his book&amp;quot;, but not vice
versa.
Example 2 (cont.)
The word strings &amp;quot;funds and&amp;quot; and &amp;quot;fund sand&amp;quot; do not cover each other.
</bodyText>
<subsectionHeader confidence="0.879318">
Definition 9&apos;
</subsectionHeader>
<bodyText confidence="0.999772666666667">
Let A and B be sets of word strings. A covers B, or A has a cover relation to B, denoted
A &lt;B, if for any Y e B, there is X E A, such that X &lt; Y. If A &lt;B, A is called a covering
word string set of B, and B a covered word string set of A.
</bodyText>
<subsectionHeader confidence="0.631965">
Example 3
</subsectionHeader>
<bodyText confidence="0.999078">
Given the alphabet E ---- fa,b,c, dl, dictionary D = {a,b,c,d,ab,bc,cd,abc,bcd}, and char-
acter string S = abcd from the alphabet, there is TD(S) = falblcld,alblcd,albc1d,albcd,
ablcld, ab/cd, abc1c11. Among them, there are fabc1d1 &lt; {ablcld, albcId}, {ab/cd} &lt;
{ablcld, alb/cd}, {albcd} &lt; {albc1d, alb/cd} and {ablcld, albc1d, alb/cd} &lt; falb1c1c11.
Moreover, there is {abc1d, ab/cd, albcd} &lt; TD(S).
</bodyText>
<subsectionHeader confidence="0.9307285">
4.2 Partially Ordered Set
Lemma 2
</subsectionHeader>
<bodyText confidence="0.961838521739131">
The cover relation is transitive, reflexive, and antisymmetric. That is, the cover relation
is a (reflexive) partial order.
Lemma 2, proved in Guo (1997), reveals that the cover relation is a partial order—
a well-defined mathematical structure with good mathematical properties. Conse-
quently, from any textbook on discrete mathematics (Kolman and Busby [1987], for
example), it is known that the tokenization set TD(S), together with the word string
cover relation &lt;, forms a partially ordered set, or simply a poset. We shall denote this
poset by (TD(S), &lt;). In case there is no confusion, we may refer to the poset simply
as TD(S).
In the literature, usually a poset is graphically presented in a Hasse diagram, which
is a digraph with vertices representing poset elements and arcs representing direct
partial order relations between poset elements. In a Hasse diagram, all connections
implied by the partial order&apos;s transitive property are eliminated. That is, if X &lt; Y and
Y &lt; Z, there should be no arc from X to Z.
Example 3 (cont.)
The poset TD(abcd) = {alblcld, alb/cd, albc1d, albcd, ablcld, ab/cd, abcId} can be graph-
ically presented in the Hasse diagram in Figure 1.
Certain elements in a poset are of special importance for many of the properties
and applications of posets. In this paper, we are particularly interested in the minimal
elements and least elements. In standard textbooks, they are defined in the following
manner: Let (A, &lt;) be a poset. An element a E A is called a minimal element of A if
there is no element c E A, c a, such that c &lt; a. An element a E A is called a least
element of A if a &lt; x for all x E A. (Kolman and Busby 1987, 195-196).
</bodyText>
<page confidence="0.99686">
579
</page>
<figureCaption confidence="0.970704">
Figure 1
</figureCaption>
<bodyText confidence="0.974333142857143">
The Hasse diagram for the poset TD(abcd) = falblcld, alb/cd, albcld, a/bcd, ablcld, ab/cd,
abc/d}.
Example 1 (cont.)
The word string &amp;quot;this is his book&amp;quot; is both the minimal element and the least element of
both TD(thisishisbook) = {&amp;quot;this is his book&amp;quot;} and TD, (thisishisbook) = {&amp;quot;th is is his book&amp;quot;,
&amp;quot;this is his book&amp;quot;}.
Example 2 (cont.)
The poset TD(fundsand) = {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;} has both &amp;quot;funds and&amp;quot; and &amp;quot;fund
sand&amp;quot; as its minimal elements, but has no least element.
Example 3 (cont.)
The poset TD(abcd) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d} has three
minimal elements: abc/d, ab/cd, a/bcd. It has no least element.
Note that any finite nonempty poset has at least one minimal element. Any poset
has at most one least element (Kolman and Busby 1987, 195-198).
</bodyText>
<subsectionHeader confidence="0.999015">
4.3 Critical Tokenization
</subsectionHeader>
<bodyText confidence="0.999881666666667">
This section deals with the most important concept—critical tokenization. Let E be an
alphabet, D a dictionary over the alphabet, and S a character string over the alphabet.
In this case, (TD(S), &lt;) is the poset.
</bodyText>
<subsectionHeader confidence="0.404542">
Definition 10
</subsectionHeader>
<bodyText confidence="0.995175">
The character string critical tokenization operation CD is a mapping CD: E*
2D* defined as: for any S in E*, CD(S) = {W I W is a minimal element of the poset
(TD(S),&lt;)}. Any word string W in CD(S) is a critically tokenized word string, or
simply a critical tokenization, or CT tokenization for short, of the character string S.
And CD(S) is the set of critical tokenizations.
</bodyText>
<figure confidence="0.9914226">
Computational Linguistics
Volume 23, Number 4
.ahfc /(11
albc/d a/b/cd
ab/cd
</figure>
<page confidence="0.678065">
580
</page>
<note confidence="0.362013">
Guo Critical Tokenization
</note>
<bodyText confidence="0.934082916666667">
In other words, the critical tokenization operation maps any character string to its
set of critical tokenizations. A word string is critical if any other word string does not
cover it.
Example 1 (cont.)
Given the English alphabet, the tiny Dictionary D -= {th, this, is, his, book}, and the
character string S = thisishisbook, there is CD(S) = {&amp;quot;this is his book&amp;quot;}. This critical
tokenization set contains the unique critical tokenization &amp;quot;this is his book&amp;quot;. Note that
the only difference between &amp;quot;this is his book&amp;quot; and &amp;quot;th is is his book&amp;quot; is that the word this
in the former is split into two words th and is in the latter.
Example 2 (cont.)
Given the English alphabet, the tiny Dictionary D = {fund, funds, and, sand}, and the
character string S = fundsand, there is CD(S) = {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}.
</bodyText>
<equation confidence="0.697729">
Example 3 (cont.)
Let E = {a, b, c, d} and D = {a, b, c, d, ab, bc, cd, abc, bcd} . There is CD(abcd) = {abc / d,
ab / cd, a / bcd} . If D&apos; = {a, b, c, d, ab, bc, cd}, then CD, (abcd) = {a / bc / d, ab / cd}
Example 4
</equation>
<bodyText confidence="0.99988275">
Given the English alphabet, the tiny Dictionary D = {the, blue, print, blueprint}, and
the character string S = theblueprint, there are TD(S) = {&amp;quot;the blueprint&amp;quot;, &amp;quot;the blue print&amp;quot;}
and CD(S) = {&amp;quot;the blueprint&amp;quot;}. Note that the tokenization &amp;quot;the blue print&amp;quot; is not critical
(not a critical tokenization).
</bodyText>
<subsectionHeader confidence="0.998634">
4.4 Super- and SubTokenization
</subsectionHeader>
<bodyText confidence="0.999980333333333">
Intuitively, a tokenization is a subtokenization of another tokenization if further to-
kenizing words in the latter can produce the former. Formally, let S be a character
string over an alphabet E and let D be a dictionary over the alphabet. In addition, let
</bodyText>
<equation confidence="0.675876571428571">
X = xi ... xn and Y = yn, be tokenizations of S on D, X, Y E TD(S). That gives us.
the following definition:
Definition 11
Y is a subtokenization of X and X is a supertokenization of Y if, for any word x in
X, there exists a substring Ys of Y such that x = G(Ys). Y is a true subtokenization of
X and X is a true supertokenization of Y, if Y is a subtokenization of X and X Y.
Example 1 (cont.)
</equation>
<bodyText confidence="0.8792458">
The tokenization &amp;quot;th is is his book&amp;quot; is a subtokenization of the critical tokenization &amp;quot;this
is his book&amp;quot;
Example 4 (cont.)
The tokenization &amp;quot;the blue print&amp;quot; is a subtokenization of the critical tokenization &amp;quot;the
blueprint&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.8430145">
4.5 Theorem
Lemma 3
</subsectionHeader>
<bodyText confidence="0.570449">
Y is a subtokenization of X if and only if X &lt; Y.
</bodyText>
<subsectionHeader confidence="0.232331">
Proof
</subsectionHeader>
<bodyText confidence="0.1877845">
If X &lt; Y, by definition, for any substring X, of X, there exists substring Ys of Y, such
that &apos;Xs&apos; &lt; lYs I and G(Xs) =- G(Ys). Also by definition, there is x = G(x) for every
</bodyText>
<page confidence="0.966841">
581
</page>
<note confidence="0.626289">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.946366352941177">
single word x. As any single word in a word string is also its single-word substring,
it can be concluded that for any word x in X, there exists a substring Y, of Y, such
that x = G(Ys).
On the other hand, if Y is a subtokenization of X, by definition, for any word x
in X, there exists a substring Y, of Y such that x = G(1(). Thus, given any substring
X, of X, X, =- ... xn, for any k, 1 &lt;k &lt; n, there exists a substring Yk of Y such that
xk = G(Yk). Denote Y, there is I Xs1 &lt; I Ysi and G(X5) -= G(Ys). By definition,
there is X &lt; Y. 0
Lemma 3 reveals that a word string is covered by another word string if and only
if every word in the latter is realized in the former as a word string. In other words,
a covering word string is in a more compact form than its covered word string.
Theorem 3
Every tokenization has a critical tokenization as its supertokenization, but critical to-
kenization has no true supertokenization.
That is, for any tokenization Y, Y E TD(S), there exists critical tokenization X, X E
CD(S), such that X is a supertokenization of Y. Moreover, if Y is a critical tokenization
and X is its supertokenization, there is X = Y.
</bodyText>
<subsectionHeader confidence="0.836781">
Proof
</subsectionHeader>
<bodyText confidence="0.981604333333333">
By definition, for any tokenization Y, Y E TD(S), there is a critical tokenization X,
X E CD(S), such that X &lt; Y. By Lemma 3, it would be the same as saying that X
is a supertokenization of Y. The second part of the theorem is from the definition of
critical tokenization. 0
Theorem 3 states that no critical tokenization can be produced by further tokeniz-
ing words in other tokenizations. However, all other tokenizations can be produced
from at least one critical tokenization by further tokenizing words in it.
Example 3 (cont.)
Given TD(S) = {alblcId, alb/cd, albc1d, albcd, abIcld, ab/cd, abc1d}, there is CD(S) =
{abc1d, ab I cd, a lbcd} &lt; TD(S). By splitting the word abc in abc d E CD(S) into alb I c,
ablc or albc, we can make another three tokenizations in TD(S): alblcId, ablcld and
albcId. Similarly, from ab/cd, we can bring back alblcId, abIcld and alb/cd; and from
abcId, we can recover alblcId,ablcId and albcId. By merging all word strings produced
together with word strings in CD(S) = {abc1d, ab/cd, albcd}, the entire tokenization set
TD(S) is reclaimed.
</bodyText>
<subsectionHeader confidence="0.973675">
4.6 Discussion
</subsectionHeader>
<bodyText confidence="0.999946444444444">
Since the theory of partially ordered sets is well established, we can use it to enhance
our understanding of the mathematical structure of string tokenization. One of the
obvious and immediate results is the concept of critical tokenization, which is simply
another name for the minimal element set of a poset. The least element is another
important concept. Although it may seem trivial to the string tokenization problem,
the critical tokenization is, in fact, absolutely crucial. For instance, Theorem 3 states
that, from critical tokenization, any tokenization can be produced (enumerated). As the
number of critical tokenizations is normally considerably less than the total amount of
all possible tokenizations, this theorem leads us to focus on the study of a few critical
</bodyText>
<page confidence="0.987169">
582
</page>
<note confidence="0.719549">
Guo Critical Tokenization
</note>
<bodyText confidence="0.8433075">
ones. In the next few sections, we shall further investigate certain important aspects
of critical tokenizations.
</bodyText>
<sectionHeader confidence="0.623926" genericHeader="method">
5. Critical and Hidden Ambiguities
</sectionHeader>
<bodyText confidence="0.9914695">
This section clarifies the relationship between critical tokenization and various types
of tokenization ambiguities.
</bodyText>
<subsectionHeader confidence="0.985664">
5.1 Critical Ambiguity in Tokenization
Definition 12
</subsectionHeader>
<bodyText confidence="0.9735667">
Let E be an alphabet, D a dictionary, and S a character string over the alphabet.
The character string S from the alphabet E has critical ambiguity in tokenization on
dictionary D if I CD (S) I &gt; 1. S has no critical ambiguity in tokenization if I CD (S) I = 1.
A tokenization W e TD(S) has critical ambiguity in tokenization if there exists another
tokenization W&apos; E TD(S), Wf W, such that neither W &lt; W&apos; nor W&apos; &lt; W holds.
Example 2 (cont.)
Since CD(fundsand) =-- {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}, i.e., ICD(fundsand)I = 2 &gt; 1, the
character string fundsand has critical ambiguity in tokenization. Moreover, the tok-
enization &amp;quot;funds and&amp;quot; has critical ambiguity in tokenization since there exists another
possible tokenization &amp;quot;fund sand&amp;quot; such that both &amp;quot;funds and&amp;quot; &lt; &amp;quot;fund sand&amp;quot; and &amp;quot;fund
sand&amp;quot; &lt; &amp;quot;funds and&amp;quot; do not hold.
Example 4 (cont.)
Since CD(theblueprint) = {&amp;quot;the blueprint&amp;quot;}, the character string theblueprint does not
have critical ambiguity in tokenization.
It helps to clarify that the only difference between the definition of tokenization
ambiguity and that of critical ambiguity in tokenization lies in the tokenization set:
While tokenization ambiguity is defined on the entire tokenization set TD(S), critical
ambiguity in tokenization is defined only on the critical tokenization set CD(S), which
is a subset of TD(S).
As all critical tokenizations are minimal elements on the word string cover re-
lationship, the existence of critical ambiguity in tokenization implies that the &amp;quot;most
powerful and commonly used&amp;quot; (Chen and Liu 1992, 104) principle of maximum to-
kenization would not be effective in resolving critical ambiguity in tokenization and
implies that other means such as statistical inferencing or grammatical reasoning have
to be introduced. In other words, critical ambiguity in tokenization is unquestionably
critical.
Critical ambiguity in tokenization is the precise mathematical description of con-
ventional concepts such as disjunctive ambiguity (Webster and Kit [1992, 11081, for
example) and overlapping ambiguity (Sun and T&apos;sou [1995, 121], for example). We
will return to this topic in Section 5.4.
</bodyText>
<subsectionHeader confidence="0.98607">
5.2 Hidden Ambiguity in Tokenization
Definition 13
</subsectionHeader>
<bodyText confidence="0.9780986">
Let E be an alphabet, D a dictionary, and S a character string over the alphabet.
The character string S from the alphabet E has hidden ambiguity in tokenization
on dictionary D if TD(S) CD(S). A tokenization W e TD(S) has hidden ambiguity
in tokenization if there exists another tokenization E TD(S), WI W, such that
W &lt; W&apos;.
</bodyText>
<page confidence="0.984658">
583
</page>
<figure confidence="0.489972">
Computational Linguistics Volume 23, Number 4
Example 4 (cont.)
</figure>
<bodyText confidence="0.9851206">
Let S = theblueprint, TD(S) = {&amp;quot;the blueprint&amp;quot;, &amp;quot;the blue print&amp;quot;}, and CD(S) = {&amp;quot;the
blueprint&amp;quot;}. Since TD (5) CD(S), the character sting theblueprint has hidden ambigu-
ity in tokenization. Since &amp;quot;the blueprint&amp;quot; &lt; &amp;quot;the blue print&amp;quot;, the character string &amp;quot;the
blueprint&amp;quot; has hidden ambiguity in tokenization.
Intuitively, a tokenization has hidden ambiguity in tokenization, if some words
in it can be further decomposed into word strings, such as &amp;quot;blueprint&amp;quot; to &amp;quot;blue print&amp;quot;.
They are called hidden or invisible because others cover them. The resolution of hidden
ambiguity in tokenization is the aim of the principle of maximum tokenization (Tie
1989; Jie and Liang 1991). Under this principle, only covering tokenizations win and
all covered tokenizations are discarded.
Hidden ambiguity in tokenization is the precise mathematical description of con-
ventional concepts such as conjunctive ambiguity (Webster and Kit [1992, 11081, for
example), combinational ambiguity (Liang [1987], for example) and categorical ambi-
guity (Sun and T&apos;sou [1995, 121], for example). We will return to this topic in Sec-
tion 5.4.
</bodyText>
<subsectionHeader confidence="0.988951">
5.3 Ambiguity = Critical + Hidden
</subsectionHeader>
<bodyText confidence="0.964257277777778">
Let E be an alphabet, D a dictionary, and S a character string over the alphabet.
Theorem 4
A character string S over an alphabet E has tokenization ambiguity on a tokenization
dictionary D if and only if S has either critical ambiguity in tokenization or hidden
ambiguity in tokenization.
Proof
If S has critical ambiguity in tokenization, by definition, there is ICD(S)I &gt; 1. If S has
hidden ambiguity in tokenization, by definition, there is TD(S) CD(S). In both cases,
since CD(S) C TD(S), there must be ITD(S)I &gt; 1. By definition, S has tokenization
ambiguity.
If S has tokenization ambiguity, by definition, there is I TD(S)I &gt; 1. Since any
finite nonempty poset has at least one minimal element, there is ICD(S)I &gt; 0. Since
CD(S) C TD(S), there is TD(S) CD(S) if ICD(S)I = 1. In this case, by definition, S has
hidden ambiguity in tokenization. If I CD(S)! &gt; 1, by definition, S has critical ambiguity
in tokenization. Ci
Theorem 4 explicitly and precisely states that tokenization ambiguity is the union
of critical ambiguity in tokenization and hidden ambiguity in tokenization. This result
helps us in the understanding of character string tokenization ambiguity.
</bodyText>
<subsectionHeader confidence="0.719376">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99989925">
By freezing the problem of token identity determination, tokenization ambiguity iden-
tification and resolution are all that is required in sentence tokenization. Consequently,
it must be crucial and beneficial to pursue an explicit and accurate understanding of
various types of character string tokenization ambiguities and their relationships.
In the literature, however, the general practice is not to formally define and classify
ambiguities but to apply various terms to them, such as overlapping ambiguity and
combinational ambiguity in their intuitive and normally fuzzy senses. Nevertheless,
efforts do exist to rigorously assign them precise, formal meanings. As a representa-
</bodyText>
<page confidence="0.990051">
584
</page>
<table confidence="0.2893254">
Guo Critical Tokenization
tive example, in Webster and Kit (1992, 1108), both conjunctive (combinational) and
disjunctive (overlapping) ambiguities are defined in the manner given below.
1. TYPE I: In a sequence of Chinese&apos; characters S = al atbi bj, if
al at, b1 bj, and S are each a word, then there is conjunctive
</table>
<bodyText confidence="0.8215345">
ambiguity in S. The segment S. which is itself a word, contains other
words. This is also known as multi-combinational ambiguity.
</bodyText>
<listItem confidence="0.597006">
2. TYPE II: In a sequence of Chinese characters S = al . aibi bici ck, if
</listItem>
<bodyText confidence="0.98928240625">
al ... b) and b1 ck are each a word, then S is an
overlapping ambiguous segment, or in other words, the segment S
displays disjunctive ambiguity. The segment b1 bj is known as an
overlap, which is usually one character long.
The definitions above contain nothing improper. In fact, conjunctive (combina-
tional) ambiguity as defined above is a special case of hidden ambiguity in tokeniza-
tion, since &amp;quot;al atbi .1)1&amp;quot; &lt; &amp;quot;a1 a,/ bi&amp;quot;. Moreover, disjunctive (overlapping)
ambiguity is a special case of critical ambiguity in tokenization, since for the character
string S = atbi .bici ck, both &amp;quot;al atbi bi / ci ck&amp;quot; and &amp;quot;a1 . a, bi bici
. . . ck&amp;quot; are critical tokenizations.
The definitions above, however, are neither complete nor critical. In our opinion, a
definition is complete only if any phenomenon in the problem domain can be properly
described (defined). With regard to the character string tokenization problem proper,
this completeness requirement can be translated as: given an alphabet, a dictionary,
and a character string, the definition should be sufficient to answer the following two
questions: (1) does this character string have tokenization ambiguity? (2) if yes, what
type of ambiguity does it have?
The definitions above cannot fulfill this completeness requirement. For instance,
if al ... at, b1 bi, ck, and al atbi bici ck are all words in a dictionary, the
character string S = al atbi bici ck, while intuitively in Type I (conjunctive am-
biguity), is, in fact, captured neither by Type I nor by Type II.
We agree that, although to do so would not be trivial, it is nevertheless possible
to make the definitions above complete by carefully listing and including all possible
cases. However, criticality, which is what is being explored in this paper, would most
probably still not be captured in such a carefully generalized ambiguity definition.
What we believe to be crucial is the association between tokenization ambigu-
ity and the maximization or minimization property of the partially ordered set on
the cover relation. As will be illustrated later in this paper, such an association is
exceptionally important in attempting to understand ambiguities and in developing
disambiguation strategies.
In short, both the cover relation and critical tokenization have given us a clear
picture of character string tokenization ambiguity as expressed in Theorem 4.
</bodyText>
<sectionHeader confidence="0.905711" genericHeader="method">
6. Maximum Tokenization
</sectionHeader>
<bodyText confidence="0.99900475">
This section clarifies the relationship between critical tokenization (CT) and three other
representative implementations of the principle of maximum tokenization, i.e., forward
maximum tokenization (FT), backward maximum tokenization (BT) and shortest to-
kenization (ST). It will be proven that ST, FT and BT are all true subclasses of CT.
</bodyText>
<footnote confidence="0.651025">
4 Although Webster and Kit include the modifier Chinese, the definition has nothing to do with specific
characteristics of Chinese but is general (multilingual).
</footnote>
<page confidence="0.993904">
585
</page>
<note confidence="0.618869">
Computational Linguistics Volume 23, Number 4
</note>
<subsectionHeader confidence="0.992297">
6.1 Forward Maximum Tokenization
</subsectionHeader>
<bodyText confidence="0.9945755">
Let E be an alphabet, D a dictionary on the alphabet, and S a character string over
the alphabet.
</bodyText>
<figure confidence="0.4145105">
Definition 14
A tokenization W zvn, E TD (S) is a forward maximum tokenization of S over
E and D, or FT tokenization for short, if, for any k, 1 &lt; k &lt; m, there exist i and j,
1 &lt;i &lt;j &lt; n, such that5
</figure>
<listItem confidence="0.908063">
1. G(wi wk_i) = cl • • • ci-1,
2. Wk = Ci Ci, and
3. For any j&apos;, j &lt; j&apos; &lt;n, there is ci V D.
</listItem>
<bodyText confidence="0.959422333333333">
The forward maximum tokenization operation, or FT operation for short, is a map-
ping FD: E* 2D* defined as: for any S E E*, FD(S) = {W I W is a FT tokenization of
S over E and D}.
This definition is in fact a descriptive interpretation of the widely recommended
conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b;
Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992).
</bodyText>
<equation confidence="0.840026857142857">
Example 3 (cont.)
The character string S = abcd has the word string abc/d as its sole FT tokenization in
TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}, i.e., FD(S) = {abc/d}.
Example 2 (cont.)
FD(fundsand) = {&amp;quot;funds and&amp;quot;}, i.e., the character string fundsand has its sole FT tok-
enization &amp;quot;funds and&amp;quot;.
Example 4 (cont.)
</equation>
<bodyText confidence="0.98002">
FD(S) = {&amp;quot;the blueprint&amp;quot;}, i.e., the word string &amp;quot;the blueprint&amp;quot; is the only FT tokenization
for the character string S = the blueprint.
</bodyText>
<subsectionHeader confidence="0.715309">
Lemma 4
</subsectionHeader>
<bodyText confidence="0.975263">
For all S E E*, there are IFD(S)I &lt;1 and FD(S) c CD (S).
That is to say, any character string has, at most, a single FT tokenization. Moreover,
if the FT tokenization exists, it is a CT tokenization.
</bodyText>
<subsectionHeader confidence="0.8864">
Proof
</subsectionHeader>
<bodyText confidence="0.9997912">
Certain character strings do not have FT tokenization on some dictionaries, even if they
have many possible tokenizations. For example, given the alphabet E = {a, b, c, d} and
the dictionary D = {a, abc, bcd}, there is TD(abcd) = {a/bcd}. But the single tokenization
does not fulfill condition (3) in the definition above for k = 1, because the longer word
abc exists in the dictionary.
</bodyText>
<footnote confidence="0.944329">
5 Note, as a widely adopted convention, in case k &lt; 1, w1 wk_i represents the empty word string v
and c1 ck_i represents the empty character string e.
</footnote>
<page confidence="0.994878">
586
</page>
<subsectionHeader confidence="0.18844">
Guo Critical Tokenization
</subsectionHeader>
<bodyText confidence="0.9540485">
Assume both X = Xi ... xn, and Y = yni, are FT tokenizations, X 0 Y. Then,
there must exist k, 1 &lt; k &lt; min(m, m&apos;), such that xe = ye, for all k&apos;, 1 &lt; k&apos; &lt; k, but
xk yk. Since G(X) = G(Y), there must be xkl 0 lYk I. Consequently, either X or Y is
unable to fulfill condition (3) of definition 14. By contradiction, there must be X = Y.
In other words, any character string at most has single FT tokenization.
Assume the FT tokenization X = x1 ....xm is not a CT tokenization. By Theorem 3,
there must exist a CT tokenization Y = ym, such that X 0 Y and Y &lt; X. Thus,
by the cover relation definition, for any substring Y, of Y, there exists substring X,
of X, such that I YS I &lt; XI and G(X) = G(Ys). Since X 0 Y, there must exist k,
1 &lt;k &lt; min(m,m&apos;), such that xe = ye, for all k&apos;, 1 &lt;k&apos; &lt;k, but JxkI yk I. This leads
to a conflict with condition (3) in the definition. In other words, X cannot be an FT
tokenization if it is not a CT tokenization.
</bodyText>
<subsectionHeader confidence="0.999894">
6.2 Backward Maximum Tokenization
</subsectionHeader>
<bodyText confidence="0.9998815">
Let E be an alphabet, D a dictionary on the alphabet, and S a character strings over
the alphabet.
</bodyText>
<subsectionHeader confidence="0.786327">
Definition 15
</subsectionHeader>
<bodyText confidence="0.999164333333333">
A tokenization W = Wm E TD (5) is a backward maximum tokenization of S
over E and D, or BT tokenization for short, if for any k, 1 &lt;k &lt; m, there exist i and
j, 1 &lt;1 &lt;j &lt; n, such that
</bodyText>
<listItem confidence="0.998076333333333">
1. G(Wk+i • • • Wm) = Cj+1 • • • Cm,
2. wk = ci and
3. For any i&apos;, 1 &lt; &lt;i, there is ...Ci 0 D.
</listItem>
<bodyText confidence="0.861655352941177">
The backward maximum tokenization operation is a mapping BD: E* 2D* defined
as: for any S E E*, BD(S) = {W I W is a BT tokenization of S over E and D}.
This definition is in fact a descriptive interpretation of the widely recommended con-
ventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b;
Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992).
Example 3 (cont.)
For the character string S = abcd, the word string a/bcd is the only BT tokeniza-
tion in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, BD(S) =
{a/bcd}.
Example 2 (cont.)
For the character string S = fundsand, there is BD(fundsand) = {&amp;quot;fund sand&amp;quot;}. That is,
the word string &amp;quot;fund sand&amp;quot; is the only BT tokenization.
Example 4 (cont.)
For the character string S = theblueprint, there is BD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the
word string &amp;quot;the blueprint&amp;quot; is the only BT tokenization.
Lemma 5
For all S E E*, there are IBD(S)I &lt;1 and BD(S) c cp(s).
</bodyText>
<page confidence="0.984712">
587
</page>
<note confidence="0.620173">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.9014655">
That is, any character string has at most one BT tokenization. Moreover, if the BT
tokenization exists, it is a CT tokenization.
</bodyText>
<subsectionHeader confidence="0.740435">
Proof
</subsectionHeader>
<bodyText confidence="0.989975">
Parallel to the proof for Lemma 4. 0
</bodyText>
<subsectionHeader confidence="0.9986125">
6.3 Shortest Tokenization
Definition 16
</subsectionHeader>
<bodyText confidence="0.956602142857143">
The shortest tokenization operation SD is a mapping SD: E* —› 2D* defined as: for
any S in E*, SD(S) -= {W I IWI= Every tokenization Win SD(S) is a
shortest tokenization, or ST tokenization for short, of the character string S.
In other words, a tokenization W of a character string S is a shortest tokenization
if and only if the word string has the minimum word string length among all possible
tokenizations.
This definition is in fact a descriptive interpretation of the constructive shortest
path finding tokenization procedure proposed by Wang (1989) and Wang, Wang, and
Bai (1991).
Example 3 (cont.)
Given the character string S = abcd. For the dictionary D = {a, b, c, d, ab, bc, cd, abc, bcd} ,
both abc/d and a/bcd are ST tokenizations in TD(S) = {a/b/c/d, a/b/cd, a/bc/d,
a/bcd, ab/c/d, ab/cd, abc/d}. That is, SD(S) = {abc/d, a/bcd}. For D&apos; = {a,b,c,d,ab,bc,cd},
however, there is Sly (S) = {ab/cd}. Note, in this case, the CT tokenization a/bc/d is
not in SD, (S).
Example 2 (cont.)
For the character string S = fundsand, there is SD(fundsand) = {&amp;quot;funds and&amp;quot;, &amp;quot;fund
sand&amp;quot;}. That is, both &amp;quot;funds and&amp;quot; and &amp;quot;fund sand&amp;quot; are ST tokenizations.
Example 4 (cont.)
For the character string S = theblueprint, there is SD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the
word string &amp;quot;the blueprint&amp;quot; is the only ST tokenization.
</bodyText>
<subsectionHeader confidence="0.755423">
Lemma 6
</subsectionHeader>
<bodyText confidence="0.899566">
SD(S) C CD(S) for all S E E*. That is, every ST tokenization is a CT tokenization.
</bodyText>
<subsectionHeader confidence="0.882539">
Proof
</subsectionHeader>
<bodyText confidence="0.9981164">
Let X be an ST tokenization, X E SD(S). Assume Xis not a CT tokenization, X CD(S).
Then, by Theorem 3, there exists a CT tokenization Y CD(S), Y 0 X, such that Y &lt; X.
By the definition of the cover relation, there is Yl &lt; IXI. In fact, as X 0 Y, there must
be I Yl &lt; IXI. This is in conflict with the fact that X is an ST tokenization. Hence, the
lemma is proven by contradiction. 0
</bodyText>
<subsectionHeader confidence="0.9889655">
6.4 Theorem
Theorem 5
</subsectionHeader>
<bodyText confidence="0.99535525">
FD(S)UBD(S) C CD(S) and SD(S) C CD(S) for all S E E. Moreover, there exists S E E*,
such that FD(S) U BD(S) CD(S) or SD(S) CD(S). That is, the forward maximum
tokenization, the backward maximum tokenization, and the shortest tokenization are
all true subclasses of critical tokenization.
</bodyText>
<page confidence="0.995215">
588
</page>
<note confidence="0.585682">
Guo Critical Tokenization
</note>
<subsectionHeader confidence="0.430648">
Proof
</subsectionHeader>
<bodyText confidence="0.7198855">
The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified
by Example 3 above. El
</bodyText>
<subsectionHeader confidence="0.999737">
6.5 Principle of Maximum Tokenization
</subsectionHeader>
<bodyText confidence="0.999863048780488">
The three tokenization definitions in this section are essentially descriptive restatements
of the corresponding constructive tokenization procedures, which in turn are realiza-
tions of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang
1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a,
b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su
1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun
and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996).
The first work closest to this principle, according to Liu (1986, 1988), was the
5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This
algorithm is a special version of the greedy-type implementation of the forward max-
imum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995)
recently applied it to Korean compound tokenization.
It is understood that forward maximum tokenization, backward maximum to-
kenization and shortest tokenization are the three most representative and widely
quoted works following the general principle of maximum tokenization. However,
the principle itself is not crystal-clear in the literature. Rather, it only serves as a gen-
eral guideline, as different researchers make different interpretations. As Chen and
Liu (1992, 104) noted, &amp;quot;there are a few variations of the sense of maximal matching.&amp;quot;
Hence, many variations have been derived after decades of fine-tuning and modifi-
cation. As Webster and Kit (1992, 1108) acknowledged, different realizations of the
principle &amp;quot;were invented one after another and seemed inexhaustible.&amp;quot;
While researchers generally agree that a dictionary word should be tokenized as
itself, they usually have different opinions on how a non-dictionary word (critical) frag-
ment should be tokenized. While they all agree that a certain form of extremes must be
attained, they nevertheless have their own understanding of what the form should be.
Consequently, it should come as no surprise to see various kinds of theoretical
generalization or summarization work in the literature. A good representative work
is by Kit and his colleagues (Jie 1989; Jie, Liu, and Liang 1991a, b; Webster and Kit
1992), who proposed a three-dimensional structural tokenization model. This model,
called ASM for Automatic Segmentation Model, is capable of characterizing up to eight
classes of different maximum or minimum tokenization procedures. Among the eight
procedures, based on both analytical inferences and experimental studies, both forward
maximum tokenization and backward maximum tokenization are recommended as
good solutions. Unfortunately, in Webster and Kit (1992, 1108), they unnecessarily
made the following overly strong claim:
It is believed that all elemental methods are included in this model.
Furthermore, it can be viewed as the ultimate model for methods
of string matching of any elements, including methods for finding
English idioms.
The shortest tokenization proposed by Wang (1989) provides an obvious coun-
terexample. As Wang (1989) exemplified&apos;, for the alphabet E = {a, b, c, d, e} and the
</bodyText>
<page confidence="0.941804">
6 The original example is &amp;quot;M 1=1 a -T &amp;quot;, a widely quoted Chinese phrase difficult to tokenize. Its
589
</page>
<note confidence="0.73819">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999842878048781">
dictionary D = fa,b,c,d,e,ab,bc,cd,del, the character string S = abcde has FT set
FD(S) = {ab/cd/e}, BT set BD(S) = {albcIde} and ST set SD(S) = {ab/cd/e, albcIde,
ablcIde}. Clearly, the ST tokenization ablcIde, which fulfills the principle of maximum
tokenization and is the desired tokenization in some cases, is neither FT nor BT tok-
enization. Moreover, careful checking showed that the missed ST tokenization is not in
any of the eight tokenization solutions covered by the ASM model. In short, the ASM
model is not a complete interpretation of the principle of maximum tokenization.
Furthermore, the shortest tokenization still does not capture all the essences of
the principle. For instance, given the alphabet E = {a, b,c,d} and the dictionary D =
{a,b,c,d,ab,bc,cd}, the character string S = abcd has the same tokenization set FD (5) =
BD(S) = SD(S) = {ab/cd} for FT. BT and ST, but a different CT tokenization set
CD(S) = {ab/cd, albc1d}. In other words, the CT tokenization albcld is left out in all
the other three sets. As the tokenization albcld is not a subtokenization of any other
possible tokenizations, it fulfills the principle of maximum tokenization.
It is now clear that, while the principle of maximum tokenization is very useful in
sentence tokenization, it lacks precise understanding in the literature. Consequently, no
solution proposed in the literature is complete with regards to realizing the principle.
Recall that, in the previous sections, the character string tokenization operation
was modeled as the inverse of the generation operation. Under the tokenization oper-
ation, every character string can be tokenized into a set of different tokenizations. The
cover relationship between tokenizations was recognized and the set of tokenizations
was proven to be a poset (partially ordered set) on the cover relationship. The set
of critical tokenizations was defined as the set of minimum elements in the poset. In
addition, it was proven that every tokenization has at least one critical tokenization
as its supertokenization and only critical tokenization has no true supertokenization.
Consequently, a noncritical tokenization would conflict with the principle of max-
imum tokenization, since it is a true subtokenization of others. As compared with its
true supertokenization, it requires the extra effort of subtokenization. On the other
hand, a critical tokenization would fully realize the principle of maximum tokeniza-
tion, since it has already attained an extreme form and cannot be simplified or com-
pressed further. As compared with all other tokenizations, no effort can be
saved.
Based on this understanding, it is now apparent why forward maximum tok-
enization, backward maximum tokenization, and shortest tokenization are all special
cases of critical tokenization, but not vice versa. In addition, it has been proven, in
Guo (1997), that critical tokenization also covers other types of maximum tokenization
implementations such as profile tokenization and shortest tokenization.
We believe that critical tokenization is the only type of tokenization completely
fulfilling the principle of maximum tokenization. In other words, critical tokenization is
the precise mathematical description of the commonly adopted principle of maximum
tokenization.
</bodyText>
<sectionHeader confidence="0.565896" genericHeader="method">
7. Further Discussion
</sectionHeader>
<bodyText confidence="0.9577145">
This section explores some helpful implications of critical tokenization in effective
tokenization disambiguation and in efficient tokenization implementation.
</bodyText>
<note confidence="0.380007">
desired tokenization, in many contexts, is&amp;quot; I J /ft -T &amp;quot;.
</note>
<page confidence="0.993015">
590
</page>
<note confidence="0.8334">
Guo Critical Tokenization
</note>
<subsectionHeader confidence="0.98576">
7.1 String Generation and Tokenization versus Language Derivation and Parsing
</subsectionHeader>
<bodyText confidence="0.999980772727273">
The relationship between the operations of sentence derivation and sentence parsing in
the theory of parsing, translation, and compiling (Aho and Ullman 1972) is an obvious
analogue with the relationship between the operations of character string generation
and character string tokenization that are defined in this paper. As the former pair
of operations is well established, and has great influence in the literature of sentence
tokenization, many researchers have, either consciously or unconsciously, been trying
to transplant it to the latter. We believe this worthy of reexamination.
Normally, sentence derivation and parsing are governed by complex grammars.
Consequently, the bulk of the work has been in developing, representing, and process-
ing grammar. Although it is a well known fact that some sentences may have several
derivations or parses, the focus has always been either on (1) grammar enhancement,
such as introducing semantic categories and consistency checking rules (selectional
restrictions), not to mention those great works on grammar formalisms, or on (2) am-
biguity resolution, such as introducing various heuristics and tricks including leftmost
parsing and operator preferences (Aho and Ullman 1972; Aho, Sethi, and Ullman 1986;
Allen 1995; Grosz, Jones, and Webber 1986).
Following this line, we observed two tendencies in tokenization research. One is
the tendency to bring every possible knowledge source into the character string gener-
ation operation. For example, Gan (1995) titled his Ph.D. dissertation Integrating Word
Boundary Disambiguation with Sentence Understanding. Here, in addition to traditional
devices such as syntax and semantics, he even employed principles of psychology
and chemistry, such as crystallization. Another is the tendency of enumerating al-
most blindly every heuristic and trick possible in ambiguity resolution. As Webster
and Kit (1992, 1108) noted, &amp;quot;segmentation methods were invented one after another
and seemed inexhaustible.&amp;quot; For example, Chen and Liu (1992) acknowledged that the
heuristic of maximum matching alone has &amp;quot;many variations&amp;quot; and tested six different
implementations.
We are not convinced of the effectiveness and necessity of both of the schools of
tokenization research. The principle argument is, while research is by nature trial-and-
error and different knowledge sources contribute to different facets of the solution, it is
nonetheless more crucial and productive to understand where the core of the problem
really lies.
As depicted in this paper, unlike general sentence derivation for complex natural
languages, the character string generation process can be very simple and straight-
forward. Many seemingly important factors such as natural language syntax and se-
mantics do not assume fundamental roles in the process. They are definitely helpful,
but only at a later stage. Moreover, as emphasized in this paper, the tokenization
set has some very good mathematical properties. By taking advantage of these prop-
erties, the tokenization problem can be greatly simplified. For example, among the
huge number of possible tokenizations, we can first concentrate on the much smaller.
critical tokenization set, since the former can be completely reproduced from the lat-
ter. Furthermore, by contrasting critical tokenizations, we can easily identify a few
critically ambiguous positions, which allows us to avoid wasting energy at useless
positions.
</bodyText>
<subsectionHeader confidence="0.998717">
7.2 Critical Tokenization and the Syntactic Graph
</subsectionHeader>
<footnote confidence="0.9426985">
It is worth noting that similar ideas do exist in natural language derivation and parsing.
For example, Seo and Simmons (1989) introduced the concept of the syntactic graph,
which is, in essence, a union of all possible parse trees. With this graph representation,
&amp;quot;it is fairly easy to focus on the syntactically ambiguous points&amp;quot; (p. 19, italics added).
</footnote>
<page confidence="0.985369">
591
</page>
<note confidence="0.734941">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999961310344828">
These syntactically ambiguous points are critical in at least two senses. First, they
are the only problems requiring knowledge and heuristics beyond the existing syntax.
In other words, any syntactic or semantics development should be guided by ambi-
guity resolution at these points. If a semantic enhancement does not interact with any
of these points, the enhancement is considered ineffective. If a grammar revision in
turn leads to additional syntactically ambiguous points, such a revision would be in
the wrong direction.
Second, these syntactically ambiguous points are critical in efficiently resolving
ambiguity. After all, these points are the only places where disambiguation decisions
must be made. Ideally, we should invest no energy in investigating anything that is
irrelevant to these points. However, unless all parse trees are merged together to form
the syntactic graph, the only thing feasible is to check every possible position in every
parse tree by applying all available knowledge and every possible heuristic, since we
are unaware of the effectiveness of any checking that occurs beforehand.
The critical tokenization introduced in this paper has a similar role in string tok-
enization to that of the syntactic graph in sentence parsing. By Theorem 3, critical tok-
enization is, in essence, the union of the whole tokenization set and thus the compact
representation of it. As long as the principle of maximum tokenization is accepted, the
resolution of critical ambiguity in tokenization is the only problem requiring knowl-
edge and heuristics beyond the existing dictionary. In other words, any introduction of
&amp;quot;high-level&amp;quot; knowledge must at least be effective in resolving some critical ambiguities
in tokenization. This should be a fundamental guideline in tokenization research.
Even if the principle of maximum tokenization is not accepted, critical ambiguity in
tokenization must nevertheless be resolved. Therefore, any investment, as mentioned
above, will not be a waste in any sense. What needs to be undertaken now is to
substitute something more precise for the principle of maximum tokenization. It is
only at this stage that we touch on the problem of identifying and resolving hidden
ambiguity in tokenization. That is one of the reasons why this type of ambiguity is
called hidden.
</bodyText>
<subsectionHeader confidence="0.999922">
7.3 Critical Tokenization and Best-Path Finding
</subsectionHeader>
<bodyText confidence="0.9999353">
The theme in this paper is to study the problem of sentence tokenization in the frame-
work of formal languages, a direction that has recently attracted some attention. For
instance, in Ma (1996), words in a tokenization dictionary are represented as pro-
duction rules and character strings are modeled as derivatives of these rules under
a string concatenation operation. Although not stated explicitly in his thesis, this is
obviously a finite-state model, as evidenced from his employment of (finite-) state
diagrams for representing both the tokenization dictionary and character strings. The
weighted finite-state transducer model developed by Sproat et al. (1996) is another
excellent representative example.
&apos; They both stop at merely representing possible tokenizations as a single large
finite-state diagram (word graph). The focus is then shifted to the problem of defining
scores for evaluating each possible tokenization and to the associated problem of
searching for the best-path in the word graph. To emphasize this point, Ma (1996)
explicitly called his approach &amp;quot;evaluation-based.&amp;quot;
In comparison, we have continued within the framework and established the criti-
cal tokenization together with its interesting properties. We believe the additional step
is worthwhile. While tokenization evaluation is important, it would be more effective
if employed at a later stage.
On the one hand, critical tokenization can help greatly in developing tokenization
knowledge and heuristics, especially those tokenization specific understandings, such
</bodyText>
<page confidence="0.993154">
592
</page>
<note confidence="0.775294">
Guo Critical Tokenization
</note>
<bodyText confidence="0.99958608">
as the observation of &amp;quot;one tokenization per source&amp;quot; and the trick of highlighting hidden
ambiguities by contrasting competing critical tokenizations (Guo 1997).
While it may not be totally impossible to fully incorporate such knowledge and
heuristics into the general framework of path evaluation and searching, they are ap-
parently employed neither in Sproat et al. (1996) nor in Ma (1996). Further, what has
been implemented in the two systems is basically a token unigram function, which
has been shown to be practically irrelevant to hidden ambiguity resolution and not to
be much better than some simple maximum tokenization approaches such as shortest
tokenization (Guo 1997).
On the other hand, critical tokenization can help significantly in boosting tokeniza-
tion efficiency. As has been observed, the tokenization of about 98% of the text can
be completed in the first parse of critical point identification, which can be done in
linear time. Moreover, as practically all acceptable tokenizations are critical tokeniza-
tions and ambiguous critical fragments are generally very short, the remaining 2% of
the text with tokenization ambiguities can also be settled efficiently through critical
tokenization generation and disambiguation (Guo 1997).
In comparison, if the best path is to be searched on the token graph of a complete
sentence, while a simple evaluation function such as token unigram cannot be very
effective in ambiguity resolution, a sophisticated evaluation function incorporating
multiple knowledge sources, such as language experiences, statistics, syntax, seman-
tics, and discourse as suggested in Ma (1996), can only be computationally prohibitive,
as Ma himself acknowledged.
In summary, the critical tokenization is crucial both in knowledge development for
effective tokenization disambiguation and in system implementation for complete and
efficient tokenization. Further discussions and examples can be found in Guo (1997).
</bodyText>
<sectionHeader confidence="0.782803" genericHeader="conclusions">
8. Summary
</sectionHeader>
<bodyText confidence="0.999977913043478">
The objective in this paper has been to lay down a mathematical foundation for sen-
tence tokenization. As the basis of the overall mathematical model, we have introduced
both sentence generation and sentence tokenization operations. What is unique here
is our attempt to model sentence tokenization as the inverse problem of sentence
generation.
Upon that basis, both critical point and critical fragment constitute our first group
of findings. We have proven that, under a complete dictionary assumption, critical
points in sentences are all and only unambiguous token boundaries.
Critical tokenization is the most important concept among the second group of
findings. We have proven that every tokenization has a critical tokenization as its
supertokenization. That is, any tokenization can be reproduced from a critical tok-
enization.
Critical ambiguity and hidden ambiguity in tokenization constitute our third group
of findings. We have proven that tokenization ambiguity can be categorized as either
critical type or hidden type. Moreover, it has been shown that critical tokenization
provides a sound basis for precisely describing various types of tokenization ambigu-
ities.
In short, we have presented a complete and precise understanding of ambiguity
in sentence tokenizations. While the existence of tokenization ambiguities is jointly
described by critical points and critical fragments, the characteristics of tokenization
ambiguities will be jointly specified by critical ambiguities and hidden ambiguities.
Moreover, we have proven that the three widely employed tokenization algorithms,
namely forward maximum matching, backward maximum matching, and shortest
</bodyText>
<page confidence="0.995342">
593
</page>
<note confidence="0.713668">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999802416666667">
length matching, are all subclasses of critical tokenization and that critical tokenization
is the precise mathematical description of the principle of maximum tokenization.
In this paper, we have also discussed some important implications of the notion
of critical tokenization in the area of character string tokenization research and de-
velopment. In this area, our primary claim is that critical tokenization is an excellent
intermediate representation that offers much assistance both in the development of
effective tokenization knowledge and heuristics and in the improvement and imple-
mentation of efficient tokenization algorithms.
Besides providing a framework to better understand previous work, as has been
attempted here, a good formalization should also lead to new questions and insights.
While some of the findings and observations achieved so far (Guo 1997) have been
mentioned here, much more work remains to be done.
</bodyText>
<sectionHeader confidence="0.939855" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9877659">
The author would like to thank Ho-Chung
Lui for his supervision, and Kok-Wee Gan,
Zhibiao Wu, Zhendong Dong, Paul Horng
Jyh Wu, Kim-Teng Lua, Chunyu Kit, and
Teow-Hin Ngair for many helpful
discussions. The author is also very grateful
to four anonymous reviewers for their
insightful comments on earlier versions of
the paper. Alexandra Vaz Hugh and Ng
Chay Hwee helped in correcting grammar.
</bodyText>
<sectionHeader confidence="0.921521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996482934210526">
Aho, Alfred V., R. Sethi, and Jeffrey
D. Ullman. 1986. Compilers, Principles,
Techniques, and Tools. Addison-Wesley
Publishing Co.
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
The Theory of Parsing, Translation, and
Compiling, Volume 1: Parsing. Prentice-Hall,
Inc.
Allen, James. 1995. Natural Language
Understanding, 2nd edition. The
Benjamin/Cummings Publishing Co.
Bai, Shuanhu. 1995. An integrated model of
Chinese word segmentation and part of
speech tagging. In Liwei Chen and Qi
Yuan, editors, Advances and Applications on
Computational Linguistics. Tsinghua
University Press, pages 56-61.
Chen, Keh-Jiann and Shing-Huan Liu. 1992.
Word identification for Mandarin Chinese
sentences. In Proceedings of the 14th
International Conference on Computational
Linguistics (COLING&apos;92), pages 101-107.
Nantes, France.
Chiang, Tung-Hui, Jing-Shin Chang,
Ming-Yu Lin, and Keh-Yih Su. 1992.
Statistical models for word segmentation
and unknown word resolution. In
Proceedings of the 5th R.O.C. Computational
Linguistics Conference (ROCLING V), pages
121-146, Taiwan.
Fan, C-K. and W-H. Tsai. 1988. Automatic
word identification in Chinese sentences
by the relaxation technique. Computer
Processing of Chinese and Oriental Languages
4(1):33-56.
Gan, Kok-Wee. 1995. Integrating Word
Boundary Disambiguation with Sentence
Understanding. Ph.D. dissertation,
Department of Computer Science and
Information Systems, National University
of Singapore.
Gan, Kok-Wee, Martha Palmer, and
Kim-Teng Lua. 1996. A statistically
emergent approach for language
processing: Application to modeling
context effects in ambiguous Chinese
word boundary perception. Computational
Linguistics 22(4):531-553.
Garside, Roger, Geoffrey Leech, and
Geoffrey Sampson, editors. 1987. The
Computational Analysis of English: A
Corpus-based Approach. Longman, London.
Gazdar, G., E. Klein, G. Pullum, and I. Sag.
1985. Generalized Phrase Structure Grammar.
Harvard University Press.
Grosz, B. J., K. S. Jones, and B. L. Webber,
editors. 1986. Readings in Natural Language
Processing. M. Kaufmann Publishers.
Guo, Jin. 1993. Statistical language modeling
and some experimental results on Chinese
syllables to words transcription. Journal of
Chinese Information Processing 7(1):18-27.
Guo, Jin. 1997. Chinese Language Modeling for
Speech Recognition. Ph.D. dissertation,
Institute of Systems Science, National
University of Singapore.
He, Kekang, Hui Xu, and Bo Sun. 1991.
Design principles of an expert system for
automatic word segmentation of written
Chinese texts. Journal of Chinese Information
Processing 5(2):1-14.
Huang, Xiangxi. 1989. A produce-test
approach to automatic segmentation of
written Chinese. Journal of Chinese
Information Processing 3(4):42-49.
Huang, Changning and Ying Xia, editors.
</reference>
<page confidence="0.99702">
594
</page>
<note confidence="0.813509">
Guo Critical Tokenization
</note>
<reference confidence="0.999195131147541">
1996. Essays on Language Information
Processing. Tsinghua University Press,
Beijing.
Hudson, R. A. 1984. Word Grammar. Basil
Blackwell.
Jie, Chunyu. 1989. A systematic approach
model for methods of Chinese automatic
word segmentation and their evaluation.
In Proceedings of the Chinese Computing
Conference, pages 71-78, Beijing.
Jie, Chunyu, Yuan Liu, and Nanyuan Liang.
1991a. On the methods of Chinese
automatic segmentation. Journal of Chinese
Information Processing 3(1):1-9.
Jie, Chunyu, Yuan Liu, and Nanyuan Liang.
1991b. The design and implementation of
the CASS practical automatic Chinese
word segmentation system. Journal of
Chinese Information Processing 5(4):27-34.
Jin, Wanying and Lu Chen. 1995.
Identifying unknown words in Chinese
corpora. In Proceedings of the 1995 Natural
Language Processing Pacific Rim Symposium
(NLPRS&apos;95), pages 234-239, Seoul.
Kolman, Bernard and Robert C. Busby. 1987.
Discrete Mathematical Structures for
Computer Science, 2nd edition.
Prentice-Hall, Inc.
Lai, T. B. Y., S. C. Lun, C. F. Sun, and
M. S. Sun. 1992. A tagging-based
first-order Markov model approach to
automatic word identification for Chinese
sentences. In Proceedings of the 1992
International Conference on Computer
Processing of Chinese and Oriental Languages,
pages 17-23.
Li, Wen-Jie, H-H. Pan, M. Zhou, K-F. Wong,
and V. Lum. 1995. Corpus-based
maximum-length Chinese noun phrase
extraction. In Proceedings of the 1995
Natural Language Processing Pacific Rim
Symposium (NLPRS&apos;95), pages 246-251,
Seoul.
Liang, Nanyuan. 1986. On computer
automatic word segmentation of written
Chinese. Journal of Chinese Information
Processing 1(1).
Liang, Nanyuan. 1987. CDWS-A written
Chinese automatic word segmentation
system. Journal of Chinese Information
Processing 1(2):44-52.
Liang, Nanyuan. 1990. The knowledge of
Chinese words segmentation. Journal of
Chinese Information Processing 4(2):29-33.
Liu, Yongquan. 1986a. Language
Modernization and Computer. Wuhan
University Press, Wuhan, China.
Liu, Yongquan. 1986b. On dictionary. Journal
of Chinese Information Processing 1(1).
Liu, Yongquan. 1988. Word re-examination.
Journal of Chinese Information Processing
2(2):47-50.
Liu, Yuan, Qiang Tan, and Xukun Shen.
1994. Contemporary Chinese Language Word
Segmentation Specification for Information
Processing and Automatic Word Segmentation
Methods. Tsinghua University Press,
Beijing.
Lua, Kim Teng. 1990. From character to
word-An application of information
theory. Computer Processing of Chinese and
Oriental Languages 4(4):304-313.
Lua, Kim Teng. 1994. Application of
information theory binding in word
segmentation. Computer Processing of
Chinese and Oriental Languages 8(1):115-124.
Lua, Kim Teng. 1995. Experiments on the
use of bigram mutual information in
Chinese natural language processing. In
Proceedings of the 1995 International
Conference on Computer Processing of
Oriental Languages (ICCPOL-95),
pages 306-313, Hawaii.
Ma, Yan. 1996. The study and realization of
an evaluation-based automatic
segmentation system. In Changning
Huang and Ying Xia, editors, Essays in
Language Information Processing. Tsinghua
University Press, Beijing, pages 2-36.
Nie, Jieyun, Jin Wanying, and M-L. Hannan.
1994. A hybrid approach to unknown
word detection and segmentation of
Chinese. In Proceedings of the International
Conference on Chinese Computing 1994
(ICCC-94), pages 326-335, Singapore.
Pachunke, T., 0. Mertineit, K. Wothke, and
R. Schmidt. 1992. Broad coverage
automatic morphological segmentation of
German words. In Proceedings of the 14th
International Conference on Computational
Linguistics (COLING&apos;92), pages 1218-1222,
Nantes, France.
Seo, J. and R. F. Simmons. 1989. Syntactic
graphs: A representation for the union of
all ambiguous parse trees. Computational
Linguistics 15(1):19-32.
Sproat, Richard and Chilin Shih. 1990. A
statistical method for finding word
boundaries in Chinese text. Computer
Processing of Chinese and Oriental Languages
4(4):336-349.
Sproat, Richard, Chain Shih, William Gale,
and Nancy Chang. 1996. A stochastic
finite-state word-segmentation algorithm
for Chinese. Computational Linguistics
22(3):377-404.
Sun, Maosong and Changning Huang. 1996.
Word segmentation and part-of-speech
tagging for unrestricted Chinese texts.
Tutorial given at the 1996 International
Conference on Chinese Computing
(ICCC-96), Singapore.
</reference>
<page confidence="0.967208">
595
</page>
<note confidence="0.368888">
Computational Linguistics Volume 23, Number 4
</note>
<reference confidence="0.9997125625">
Sun, Maosong and Benjemin T&apos;sou. 1995.
Ambiguity resolution in Chinese word
segmentation. In Proceedings of the 10th
Pacific Asia Conference on Language,
Information and Computation (PACLIC-95),
pages 121-126, Hong Kong.
Tung, C. H. and H. J. Lee. 1994.
Identification of unknown words from
corpus. Computer Processing of Chinese and
Oriental Languages 8(Supplement):131-146.
Wang, Yongcheng, Haiju Su, and Yan Mo.
1990. Automatic processing Chinese
word. Journal of Chinese Information
Processing 4(4):1-11.
Wang, Xiaolong. 1989. Word Separating and
Mutual Translation of Syllable and Character
Strings. Ph.D. dissertation, Department of
Computer Science and Engineering,
Harbin Institute of Technology, Harbin,
China.
Wang, Xiaolong, Kaizhu Wang, and Xiaohua
Bai. 1991. Separating syllables and
characters into words in natural language
understanding. Journal of Chinese
Information Processing 5(3):48-58.
Webster, Jonathan J. and Chunyu Kit. 1992.
Tokenization as the initial phase in NLP.
In Proceedings of the 14th International
Conference on Computational Linguistics
(COLING&apos;92), pages 1,106-1,110, Nantes,
France.
Wong, K-F., H-H. Pan, B-T. Low,
C-H. Cheng, V. Lum, and S-S. Lam. 1995.
A tool for computer-assisted open
response analysis. In Proceedings of the
1995 International Conference on Computer
Processing of Oriental Languages,
pages 191-198, Hawaii.
Wong, K. F., V. Y. Lum, C-Y. Leung,
C-H. Leung, W-K. Kan, and L-C. Chan.
1994. A parallel approach for identifying
word boundaries in Chinese text. IPOC
Technical Report, /CHIRP/WP/SE/022,
Department of Systems Engineering,
Chinese University of Hong Kong.
Wu, Horng Jyh, Jin Guo, Ho Chung Lui,
and Hwee Boon Low. 1994. Corpus-based
speech and language research in the
Institute of Systems Science. In
Proceedings of the International Symposium
on Speech, Image Processing and Neural
Networks (ISPIPNN&apos;94), pages 142-145,
Hong Kong.
Wu, Ming-Wen and Keh-Yih Su. 1993.
Corpus-based automatic compound
extraction with mutual information and
relative frequency count. In Proceedings of
R.O.C. Computational Linguistics Conference
(ROCLING) VI, pages 207-216, Taiwan.
Yao, Tian-Shun, Gui-Ping Zhang, and
Ying-Ming Wu. 1990. A rule-based
Chinese automatic segmentation system.
Journal of Chinese Information Processing
4(1):37-43.
Yeh, C-L. and H-J. Lee. 1991. Rule-based
word identification for Mandarin Chinese
sentences—A unification approach.
Computer Processing of Chinese and Oriental
Languages 5(2):97-118.
Yosiyuki, K., T. Takenobu, and T. Hozumi.
1992. Analysis of Japanese compound
nouns using collocation information. In
Proceedings of the 14th International
Conference on Computational Linguistics
(COLING&apos;92), pages 865-869, Nantes,
France.
Yun, B-H., H. Lee, and H-C. Rim. 1995.
Analysis of Korean compound nouns
using statistical information. In
Proceedings of the 1995 International
Conference on Computer Processing of
Oriental Languages (ICCPOL-95),
pages 76-79, Honolulu, Hawaii.
Zhang, Jun-Sheng, Zhi-Da Chen, and
Shun-De Chen. 1991. A method of word
identification for Chinese by constraint
satisfaction and statistical optimization
techniques. In Proceedings of R.O.C.
Computational Linguistics Conference
(ROCLING) IV, pages 147-165, Taiwan.
Zhang, Jun-Sheng, Shun-De Chen, S. J. Ker,
Y. Chan, and J. S. Liu. 1994. A
multi-corpus approach to recognition of
proper names in Chinese texts. Computer
Processing of Chinese and Oriental Languages
8(1):73-86.
</reference>
<page confidence="0.998646">
596
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980200">
<title confidence="0.999499">Critical Tokenization and its Properties</title>
<author confidence="0.999232">Jin Guo</author>
<affiliation confidence="0.996441">National University of Singapore</affiliation>
<abstract confidence="0.999051941176471">Tokenization is the process of mapping sentences from character strings into strings of words. This paper sets out to study critical tokenization, a distinctive type of tokenization following the principle of maximum tokenization. The objective in this paper is to develop its mathematical description and understanding. The main results are as follows: (1) Critical points are all and only unambiguous token boundaries for any character string on a complete dictionary; (2) Any critically tokenized word string is a minimal element in the partially ordered set of all tokenized word strings with respect to the word string cover relation; (3) Any tokenized string can be reproduced from a critically tokenized word string but not vice versa; (4) Critical tokenization forms the sound mathematical foundation for categorizing tokenization ambiguity into critical and hidden types, a precise mathematical understanding of conventional concepts like combinational and overlapping ambiguities; (5) Many important maximum tokenization variations, such as forward and backward maximum matching and shortest tokenization, are all true subclasses of critical tokenization. It is believed that critical tokenization provides a precise mathematical description of the principle of maximum tokenization. Important implications and practical applications of critical tokenization in effective ambiguity resolution and in efficient tokenization implementation are also carefully examined.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>R Sethi</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1986</date>
<booktitle>Compilers, Principles, Techniques, and Tools.</booktitle>
<publisher>Addison-Wesley Publishing Co.</publisher>
<marker>Aho, Sethi, Ullman, 1986</marker>
<rawString>Aho, Alfred V., R. Sethi, and Jeffrey D. Ullman. 1986. Compilers, Principles, Techniques, and Tools. Addison-Wesley Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing.</booktitle>
<publisher>Prentice-Hall, Inc.</publisher>
<contexts>
<context position="8904" citStr="Aho and Ullman (1972" startWordPosition="1326" endWordPosition="1329"> conclusions. 2. Generation and Tokenization In order to address the topic clearly and accurately, a precise and well-defined formal notation is required. What is used in this paper is primarily from elementary Boolean algebra and Formal Language Theory, which can be found in most graduate-level textbooks on discrete mathematics. This section aims at refreshing several simple terms and conventions that will be applied throughout this paper and at introducing the two new concepts of character string generation and tokenization. For the remaining basic concepts and conventions, we mainly follow Aho and Ullman (1972, Chapter 0, Mathematical Preliminaries), and Kolman and Busby (1987). 2.1 Character, Alphabet, and Character String Definition 1 An alphabet E = {a, b, c,. .} is a finite set of symbols. Each symbol in the alphabet is a character. The alphabet size is the number of characters in the alphabet and is denoted 1E1. Character strings over an alphabet E are defined2 in the following manner: 1. e is a character string over E. e is called the empty character string. 2. If S is a character string over E and a is a character in E, then Sa is a character string over E. 3. S&apos; is a character string over E</context>
<context position="13529" citStr="Aho and Ullman 1972" startWordPosition="2227" endWordPosition="2230">he character string generation operation G is a mapping G: D* defined as: 1. Empty word string v is mapped to empty character string e. That is, G(v) = e. 2. Single word string w1 is mapped to the character string of the single word. That is, G(w1) = w. 3. If W is a word string over dictionary D and w is a word in D, then, the word string Ww is mapped to the concatenation of character string G(W) and G(w). That is, G(Ww) = G(W)G(w). G(W) is said to be the generated character string of the word string W from dictionary D. Note that the character string generation operation G is a homomorphism (Aho and Ullman 1972, 17) with property G(w1) = w. Example 1 (cont.) The character string thisishisbook is the generated character string of the word string &amp;quot;this is his book&amp;quot;. That is, Gr this is his book&amp;quot;) = thisishisbook. 2.4 Character String Tokenization Definition 4 The character string tokenization operation T is a mapping TD: E* 2D* defined as: if S is a character string in E*, then TD(S) is the set of dictionary word strings mapped by the character string generation operation G to the character string S. That is, TD(S) = {WIG(W) = S, W E D*}. Any word string W in TD(S) is a tokenized word string, or simpl</context>
<context position="15811" citStr="Aho and Ullman (1972)" startWordPosition="2602" endWordPosition="2605"> dictionary D = {fund, funds, and, sand}, there is TD(fundsand) {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}. In other words, both &amp;quot;funds and&amp;quot; and &amp;quot;fund sand&amp;quot; are tokenizations of character string fundsand. 573 Computational Linguistics Volume 23, Number 4 2.5 Discussion Our intention, in formally defining characters and words, is to establish our mathematical system clearly and accurately. To keep discussion concise, the definitions of elementary concepts such as strings and substrings, although widely used in this paper, will be taken for granted. We limit our basic notion to what has already been defined in Aho and Ullman (1972) and Kolman and Busby (1987). Mathematically, word strings are nothing but symbol strings, with each symbol representing a word in the dictionary. In that sense, the word string definition is redundant as it is already covered by the definition of character string. However, since the relationships between character strings and word strings are very important in this paper, we believe it to be appropriate to list both definitions explicitly. What is new in this section is mathematical definitions for character string generation and tokenization. We consider them fundamental to our mathematical </context>
<context position="64296" citStr="Aho and Ullman 1972" startWordPosition="10751" endWordPosition="10754">l tokenization is the precise mathematical description of the commonly adopted principle of maximum tokenization. 7. Further Discussion This section explores some helpful implications of critical tokenization in effective tokenization disambiguation and in efficient tokenization implementation. desired tokenization, in many contexts, is&amp;quot; I J /ft -T &amp;quot;. 590 Guo Critical Tokenization 7.1 String Generation and Tokenization versus Language Derivation and Parsing The relationship between the operations of sentence derivation and sentence parsing in the theory of parsing, translation, and compiling (Aho and Ullman 1972) is an obvious analogue with the relationship between the operations of character string generation and character string tokenization that are defined in this paper. As the former pair of operations is well established, and has great influence in the literature of sentence tokenization, many researchers have, either consciously or unconsciously, been trying to transplant it to the latter. We believe this worthy of reexamination. Normally, sentence derivation and parsing are governed by complex grammars. Consequently, the bulk of the work has been in developing, representing, and processing gra</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Alfred V. and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing. Prentice-Hall, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding, 2nd edition.</title>
<date>1995</date>
<publisher>The Benjamin/Cummings Publishing Co.</publisher>
<contexts>
<context position="1778" citStr="Allen 1995" startWordPosition="250" endWordPosition="251">ing and shortest tokenization, are all true subclasses of critical tokenization. It is believed that critical tokenization provides a precise mathematical description of the principle of maximum tokenization. Important implications and practical applications of critical tokenization in effective ambiguity resolution and in efficient tokenization implementation are also carefully examined. 1. Introduction Words, and tokens in general, are the primary building blocks in almost all linguistic theories (e.g., Gazdar, Klein, Pullum, and Sag 1985; Hudson 1984) and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua</context>
<context position="65406" citStr="Allen 1995" startWordPosition="10918" endWordPosition="10919">rammars. Consequently, the bulk of the work has been in developing, representing, and processing grammar. Although it is a well known fact that some sentences may have several derivations or parses, the focus has always been either on (1) grammar enhancement, such as introducing semantic categories and consistency checking rules (selectional restrictions), not to mention those great works on grammar formalisms, or on (2) ambiguity resolution, such as introducing various heuristics and tricks including leftmost parsing and operator preferences (Aho and Ullman 1972; Aho, Sethi, and Ullman 1986; Allen 1995; Grosz, Jones, and Webber 1986). Following this line, we observed two tendencies in tokenization research. One is the tendency to bring every possible knowledge source into the character string generation operation. For example, Gan (1995) titled his Ph.D. dissertation Integrating Word Boundary Disambiguation with Sentence Understanding. Here, in addition to traditional devices such as syntax and semantics, he even employed principles of psychology and chemistry, such as crystallization. Another is the tendency of enumerating almost blindly every heuristic and trick possible in ambiguity reso</context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>Allen, James. 1995. Natural Language Understanding, 2nd edition. The Benjamin/Cummings Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuanhu Bai</author>
</authors>
<title>An integrated model of Chinese word segmentation and part of speech tagging.</title>
<date>1995</date>
<booktitle>In Liwei Chen and Qi Yuan, editors, Advances and Applications on Computational Linguistics. Tsinghua</booktitle>
<pages>56--61</pages>
<publisher>University Press,</publisher>
<contexts>
<context position="2269" citStr="Bai 1995" startWordPosition="324" endWordPosition="325">c theories (e.g., Gazdar, Klein, Pullum, and Sag 1985; Hudson 1984) and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994</context>
<context position="57708" citStr="Bai 1995" startWordPosition="9752" endWordPosition="9753">above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most representative and widely quoted works following the general pr</context>
</contexts>
<marker>Bai, 1995</marker>
<rawString>Bai, Shuanhu. 1995. An integrated model of Chinese word segmentation and part of speech tagging. In Liwei Chen and Qi Yuan, editors, Advances and Applications on Computational Linguistics. Tsinghua University Press, pages 56-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Shing-Huan Liu</author>
</authors>
<title>Word identification for Mandarin Chinese sentences.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92),</booktitle>
<pages>101--107</pages>
<location>Nantes, France.</location>
<contexts>
<context position="2307" citStr="Chen and Liu 1992" startWordPosition="330" endWordPosition="333">in, Pullum, and Sag 1985; Hudson 1984) and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao,</context>
<context position="42644" citStr="Chen and Liu 1992" startWordPosition="7132" endWordPosition="7135">e critical ambiguity in tokenization. It helps to clarify that the only difference between the definition of tokenization ambiguity and that of critical ambiguity in tokenization lies in the tokenization set: While tokenization ambiguity is defined on the entire tokenization set TD(S), critical ambiguity in tokenization is defined only on the critical tokenization set CD(S), which is a subset of TD(S). As all critical tokenizations are minimal elements on the word string cover relationship, the existence of critical ambiguity in tokenization implies that the &amp;quot;most powerful and commonly used&amp;quot; (Chen and Liu 1992, 104) principle of maximum tokenization would not be effective in resolving critical ambiguity in tokenization and implies that other means such as statistical inferencing or grammatical reasoning have to be introduced. In other words, critical ambiguity in tokenization is unquestionably critical. Critical ambiguity in tokenization is the precise mathematical description of conventional concepts such as disjunctive ambiguity (Webster and Kit [1992, 11081, for example) and overlapping ambiguity (Sun and T&apos;sou [1995, 121], for example). We will return to this topic in Section 5.4. 5.2 Hidden Am</context>
<context position="50974" citStr="Chen and Liu 1992" startWordPosition="8493" endWordPosition="8496">rd maximum tokenization of S over E and D, or FT tokenization for short, if, for any k, 1 &lt; k &lt; m, there exist i and j, 1 &lt;i &lt;j &lt; n, such that5 1. G(wi wk_i) = cl • • • ci-1, 2. Wk = Ci Ci, and 3. For any j&apos;, j &lt; j&apos; &lt;n, there is ci V D. The forward maximum tokenization operation, or FT operation for short, is a mapping FD: E* 2D* defined as: for any S E E*, FD(S) = {W I W is a FT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) The character string S = abcd has the word string abc/d as its sole FT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}, i.e., FD(S) = {abc/d}. Example 2 (cont.) FD(fundsand) = {&amp;quot;funds and&amp;quot;}, i.e., the character string fundsand has its sole FT tokenization &amp;quot;funds and&amp;quot;. Example 4 (cont.) FD(S) = {&amp;quot;the blueprint&amp;quot;}, i.e., the word string &amp;quot;the blueprint&amp;quot; is the only FT tokenization for the character string S = the blueprint. Lemma 4 For all S E E*, there are IFD(S)I &lt;1 and FD(S) c CD (S). That is to say, any character string </context>
<context position="53980" citStr="Chen and Liu 1992" startWordPosition="9085" endWordPosition="9088">W = Wm E TD (5) is a backward maximum tokenization of S over E and D, or BT tokenization for short, if for any k, 1 &lt;k &lt; m, there exist i and j, 1 &lt;1 &lt;j &lt; n, such that 1. G(Wk+i • • • Wm) = Cj+1 • • • Cm, 2. wk = ci and 3. For any i&apos;, 1 &lt; &lt;i, there is ...Ci 0 D. The backward maximum tokenization operation is a mapping BD: E* 2D* defined as: for any S E E*, BD(S) = {W I W is a BT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) For the character string S = abcd, the word string a/bcd is the only BT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, BD(S) = {a/bcd}. Example 2 (cont.) For the character string S = fundsand, there is BD(fundsand) = {&amp;quot;fund sand&amp;quot;}. That is, the word string &amp;quot;fund sand&amp;quot; is the only BT tokenization. Example 4 (cont.) For the character string S = theblueprint, there is BD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the word string &amp;quot;the blueprint&amp;quot; is the only BT tokenization. Lemma 5 For all S E E*, there are IBD(S)I &lt;1 and B</context>
<context position="57555" citStr="Chen and Liu 1992" startWordPosition="9717" endWordPosition="9720">of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum</context>
<context position="66166" citStr="Chen and Liu (1992)" startWordPosition="11025" endWordPosition="11028">y possible knowledge source into the character string generation operation. For example, Gan (1995) titled his Ph.D. dissertation Integrating Word Boundary Disambiguation with Sentence Understanding. Here, in addition to traditional devices such as syntax and semantics, he even employed principles of psychology and chemistry, such as crystallization. Another is the tendency of enumerating almost blindly every heuristic and trick possible in ambiguity resolution. As Webster and Kit (1992, 1108) noted, &amp;quot;segmentation methods were invented one after another and seemed inexhaustible.&amp;quot; For example, Chen and Liu (1992) acknowledged that the heuristic of maximum matching alone has &amp;quot;many variations&amp;quot; and tested six different implementations. We are not convinced of the effectiveness and necessity of both of the schools of tokenization research. The principle argument is, while research is by nature trial-anderror and different knowledge sources contribute to different facets of the solution, it is nonetheless more crucial and productive to understand where the core of the problem really lies. As depicted in this paper, unlike general sentence derivation for complex natural languages, the character string gener</context>
</contexts>
<marker>Chen, Liu, 1992</marker>
<rawString>Chen, Keh-Jiann and Shing-Huan Liu. 1992. Word identification for Mandarin Chinese sentences. In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92), pages 101-107. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tung-Hui Chiang</author>
<author>Jing-Shin Chang</author>
<author>Ming-Yu Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Statistical models for word segmentation and unknown word resolution.</title>
<date>1992</date>
<booktitle>In Proceedings of the 5th R.O.C. Computational Linguistics Conference (ROCLING V),</booktitle>
<pages>121--146</pages>
<contexts>
<context position="2327" citStr="Chiang et al. 1992" startWordPosition="334" endWordPosition="337"> 1985; Hudson 1984) and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990;</context>
</contexts>
<marker>Chiang, Chang, Lin, Su, 1992</marker>
<rawString>Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin, and Keh-Yih Su. 1992. Statistical models for word segmentation and unknown word resolution. In Proceedings of the 5th R.O.C. Computational Linguistics Conference (ROCLING V), pages 121-146, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-K Fan</author>
<author>W-H Tsai</author>
</authors>
<title>Automatic word identification in Chinese sentences by the relaxation technique.</title>
<date>1988</date>
<booktitle>Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>4--1</pages>
<contexts>
<context position="2346" citStr="Fan and Tsai 1988" startWordPosition="338" endWordPosition="341">and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; </context>
</contexts>
<marker>Fan, Tsai, 1988</marker>
<rawString>Fan, C-K. and W-H. Tsai. 1988. Automatic word identification in Chinese sentences by the relaxation technique. Computer Processing of Chinese and Oriental Languages 4(1):33-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
</authors>
<title>Integrating Word Boundary Disambiguation with Sentence Understanding.</title>
<date>1995</date>
<tech>Ph.D. dissertation,</tech>
<institution>Department of Computer Science and Information Systems, National University of Singapore.</institution>
<contexts>
<context position="65646" citStr="Gan (1995)" startWordPosition="10954" endWordPosition="10955">rammar enhancement, such as introducing semantic categories and consistency checking rules (selectional restrictions), not to mention those great works on grammar formalisms, or on (2) ambiguity resolution, such as introducing various heuristics and tricks including leftmost parsing and operator preferences (Aho and Ullman 1972; Aho, Sethi, and Ullman 1986; Allen 1995; Grosz, Jones, and Webber 1986). Following this line, we observed two tendencies in tokenization research. One is the tendency to bring every possible knowledge source into the character string generation operation. For example, Gan (1995) titled his Ph.D. dissertation Integrating Word Boundary Disambiguation with Sentence Understanding. Here, in addition to traditional devices such as syntax and semantics, he even employed principles of psychology and chemistry, such as crystallization. Another is the tendency of enumerating almost blindly every heuristic and trick possible in ambiguity resolution. As Webster and Kit (1992, 1108) noted, &amp;quot;segmentation methods were invented one after another and seemed inexhaustible.&amp;quot; For example, Chen and Liu (1992) acknowledged that the heuristic of maximum matching alone has &amp;quot;many variations&amp;quot;</context>
</contexts>
<marker>Gan, 1995</marker>
<rawString>Gan, Kok-Wee. 1995. Integrating Word Boundary Disambiguation with Sentence Understanding. Ph.D. dissertation, Department of Computer Science and Information Systems, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
<author>Martha Palmer</author>
<author>Kim-Teng Lua</author>
</authors>
<title>A statistically emergent approach for language processing: Application to modeling context effects in ambiguous Chinese word boundary perception.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<pages>22--4</pages>
<marker>Gan, Palmer, Lua, 1996</marker>
<rawString>Gan, Kok-Wee, Martha Palmer, and Kim-Teng Lua. 1996. A statistically emergent approach for language processing: Application to modeling context effects in ambiguous Chinese word boundary perception. Computational Linguistics 22(4):531-553.</rawString>
</citation>
<citation valid="true">
<title>The Computational Analysis of English: A Corpus-based Approach.</title>
<date>1987</date>
<editor>Garside, Roger, Geoffrey Leech, and Geoffrey Sampson, editors.</editor>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="32338" citStr="[1987]" startWordPosition="5305" endWordPosition="5305">/cd, abc1c11. Among them, there are fabc1d1 &lt; {ablcld, albcId}, {ab/cd} &lt; {ablcld, alb/cd}, {albcd} &lt; {albc1d, alb/cd} and {ablcld, albc1d, alb/cd} &lt; falb1c1c11. Moreover, there is {abc1d, ab/cd, albcd} &lt; TD(S). 4.2 Partially Ordered Set Lemma 2 The cover relation is transitive, reflexive, and antisymmetric. That is, the cover relation is a (reflexive) partial order. Lemma 2, proved in Guo (1997), reveals that the cover relation is a partial order— a well-defined mathematical structure with good mathematical properties. Consequently, from any textbook on discrete mathematics (Kolman and Busby [1987], for example), it is known that the tokenization set TD(S), together with the word string cover relation &lt;, forms a partially ordered set, or simply a poset. We shall denote this poset by (TD(S), &lt;). In case there is no confusion, we may refer to the poset simply as TD(S). In the literature, usually a poset is graphically presented in a Hasse diagram, which is a digraph with vertices representing poset elements and arcs representing direct partial order relations between poset elements. In a Hasse diagram, all connections implied by the partial order&apos;s transitive property are eliminated. That</context>
<context position="44657" citStr="[1987]" startWordPosition="7442" endWordPosition="7442">some words in it can be further decomposed into word strings, such as &amp;quot;blueprint&amp;quot; to &amp;quot;blue print&amp;quot;. They are called hidden or invisible because others cover them. The resolution of hidden ambiguity in tokenization is the aim of the principle of maximum tokenization (Tie 1989; Jie and Liang 1991). Under this principle, only covering tokenizations win and all covered tokenizations are discarded. Hidden ambiguity in tokenization is the precise mathematical description of conventional concepts such as conjunctive ambiguity (Webster and Kit [1992, 11081, for example), combinational ambiguity (Liang [1987], for example) and categorical ambiguity (Sun and T&apos;sou [1995, 121], for example). We will return to this topic in Section 5.4. 5.3 Ambiguity = Critical + Hidden Let E be an alphabet, D a dictionary, and S a character string over the alphabet. Theorem 4 A character string S over an alphabet E has tokenization ambiguity on a tokenization dictionary D if and only if S has either critical ambiguity in tokenization or hidden ambiguity in tokenization. Proof If S has critical ambiguity in tokenization, by definition, there is ICD(S)I &gt; 1. If S has hidden ambiguity in tokenization, by definition, th</context>
</contexts>
<marker>1987</marker>
<rawString>Garside, Roger, Geoffrey Leech, and Geoffrey Sampson, editors. 1987. The Computational Analysis of English: A Corpus-based Approach. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., E. Klein, G. Pullum, and I. Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<date>1986</date>
<booktitle>Readings in Natural Language Processing.</booktitle>
<editor>Grosz, B. J., K. S. Jones, and B. L. Webber, editors.</editor>
<publisher>M. Kaufmann Publishers.</publisher>
<contexts>
<context position="57802" citStr="(1986, 1988)" startWordPosition="9768" endWordPosition="9769"> section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most representative and widely quoted works following the general principle of maximum tokenization. However, the principle itself is not crystal-clear in the lit</context>
</contexts>
<marker>1986</marker>
<rawString>Grosz, B. J., K. S. Jones, and B. L. Webber, editors. 1986. Readings in Natural Language Processing. M. Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Guo</author>
</authors>
<title>Statistical language modeling and some experimental results on Chinese syllables to words transcription.</title>
<date>1993</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>7--1</pages>
<contexts>
<context position="2393" citStr="Guo 1993" startWordPosition="349" endWordPosition="350">, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Sys</context>
<context position="57565" citStr="Guo 1993" startWordPosition="9721" endWordPosition="9722">ation. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenizat</context>
</contexts>
<marker>Guo, 1993</marker>
<rawString>Guo, Jin. 1993. Statistical language modeling and some experimental results on Chinese syllables to words transcription. Journal of Chinese Information Processing 7(1):18-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Guo</author>
</authors>
<title>Chinese Language Modeling for Speech Recognition.</title>
<date>1997</date>
<tech>Ph.D. dissertation,</tech>
<institution>Institute of Systems Science, National University of Singapore.</institution>
<contexts>
<context position="5244" citStr="Guo (1997)" startWordPosition="797" endWordPosition="798"> compact representation of tokenization. In addition, we will show that critical tokenization forms a sound mathematical foundation for categorizing critical ambiguity and hidden ambiguity in tokenizations, which provides a precise mathematical understanding of conventional concepts like combinational and overlapping ambiguities. Moreover, we will confirm that some important maximum tokenization variations, such as forward and backward maximum matching and shortest tokenization, are all subclasses of critical tokenization. Based on a mathematical understanding of tokenization, we reported, in Guo (1997), a series of interesting findings. For instance, there exists an optimal algorithm that can identify all and only critical points, and thus all unambiguous token boundaries, in time proportional to the input character string length but independent of the size of the tokenization dictionary. Tested on a representative corpus, about 98% of the critical fragments generated are by themselves desired tokens. In other words, about 98% close-dictionary tokenization accuracy can be achieved efficiently without disambiguation. Another interesting finding is that, for those critical fragments with crit</context>
<context position="6940" citStr="Guo 1997" startWordPosition="1036" endWordPosition="1037">very strong tendency to have one tokenization per source. Naturally, this observation suggests tokenization disambiguation strategies notably different from the mainstream best-path-finding strategy. For instance, the simple strategy of tokenization by memorization alone could easily exhibit critical ambiguity resolution accuracy of no less than 90%, which is notably higher than what has been achieved in the literature. Moreover, it has been observed that critical tokenization can also provide helpful guidance in identifying hidden ambiguities and in determining unregistered (unknown) tokens (Guo 1997). While these are just some of the very primitive findings, they are nevertheless promising and motivate 1 All terms mentioned here will be precisely defined later in this paper. 570 Guo Critical Tokenization us to rigorously formalize the tokenization problem and to carefully explore logical consequences. The rest of the paper is organized as follows: In Section 2, we formally define the string generation and tokenization operations that form the basis of our framework. In Section 3, we will study tokenization ambiguities and explore the concepts of critical points and critical fragments. In </context>
<context position="18348" citStr="Guo 1997" startWordPosition="2974" endWordPosition="2975">s. In contrast, we define the character string tokenization operation as the inverse operation (inverse homomorphism) of the character string generation operation (homomorphism). Naturally, the result of the tokenization operation is a set of tokenizations rather than a single word string. Such treatment suggests that we could use the divide-and-conquer problem-solving strategy—to decompose the complex string tokenization problem into several smaller and, hopefully, simpler subproblems. That is the basis of our two-stage, five-step iterative problem-solving strategy for sentence tokenization (Guo 1997). 3. Critical Point and Fragment After clarifying both sentence generation and tokenization operations, we undertake next to further clarify sentence tokenization ambiguities. Among all the concepts to be introduced, critical points and critical fragments are probably two of the most important. We will prove that, for any character string on a complete tokenization dictionary, its critical points are all and only unambiguous token boundaries, and its critical fragments are the longest substrings with all inner positions ambiguous. 3.1 Ambiguity Let E be an alphabet, D a dictionary, and S a cha</context>
<context position="28591" citStr="Guo (1997)" startWordPosition="4670" endWordPosition="4671">y would be impossible. The concepts of critical point and critical fragment are fundamental to our sentence tokenization theory. By adopting the complete dictionary assumption, it has been proven that critical points are all and only unambiguous token boundaries while critical fragments are the longest substrings with all inner positions ambiguous. This is a very strong and significant statement. It provides us with a precise understanding of what and where tokenization ambiguities are. Although the proof itself is easy to follow, the result has nonetheless been a surprise. As demonstrated in Guo (1997), many researchers have tried but failed to answer the question in such a precise and complete way. Consequently, while they proposed many sophisticated algorithms for the discovery of ambiguity (and certainty), they never were able to arrive at such a concise and complete solution. As critical points are all and only unambiguous token boundaries, an identification of all of them would allow for a long character string to be broken down into several short but fully ambiguous critical fragments. As shown in Guo (1997), critical points can be completely identified in linear time. Moreover, in pr</context>
<context position="32131" citStr="Guo (1997)" startWordPosition="5275" endWordPosition="5276">ing set of A. Example 3 Given the alphabet E ---- fa,b,c, dl, dictionary D = {a,b,c,d,ab,bc,cd,abc,bcd}, and character string S = abcd from the alphabet, there is TD(S) = falblcld,alblcd,albc1d,albcd, ablcld, ab/cd, abc1c11. Among them, there are fabc1d1 &lt; {ablcld, albcId}, {ab/cd} &lt; {ablcld, alb/cd}, {albcd} &lt; {albc1d, alb/cd} and {ablcld, albc1d, alb/cd} &lt; falb1c1c11. Moreover, there is {abc1d, ab/cd, albcd} &lt; TD(S). 4.2 Partially Ordered Set Lemma 2 The cover relation is transitive, reflexive, and antisymmetric. That is, the cover relation is a (reflexive) partial order. Lemma 2, proved in Guo (1997), reveals that the cover relation is a partial order— a well-defined mathematical structure with good mathematical properties. Consequently, from any textbook on discrete mathematics (Kolman and Busby [1987], for example), it is known that the tokenization set TD(S), together with the word string cover relation &lt;, forms a partially ordered set, or simply a poset. We shall denote this poset by (TD(S), &lt;). In case there is no confusion, we may refer to the poset simply as TD(S). In the literature, usually a poset is graphically presented in a Hasse diagram, which is a digraph with vertices repre</context>
<context position="63372" citStr="Guo (1997)" startWordPosition="10630" endWordPosition="10631"> As compared with its true supertokenization, it requires the extra effort of subtokenization. On the other hand, a critical tokenization would fully realize the principle of maximum tokenization, since it has already attained an extreme form and cannot be simplified or compressed further. As compared with all other tokenizations, no effort can be saved. Based on this understanding, it is now apparent why forward maximum tokenization, backward maximum tokenization, and shortest tokenization are all special cases of critical tokenization, but not vice versa. In addition, it has been proven, in Guo (1997), that critical tokenization also covers other types of maximum tokenization implementations such as profile tokenization and shortest tokenization. We believe that critical tokenization is the only type of tokenization completely fulfilling the principle of maximum tokenization. In other words, critical tokenization is the precise mathematical description of the commonly adopted principle of maximum tokenization. 7. Further Discussion This section explores some helpful implications of critical tokenization in effective tokenization disambiguation and in efficient tokenization implementation. </context>
<context position="72137" citStr="Guo 1997" startWordPosition="11929" endWordPosition="11930">ithin the framework and established the critical tokenization together with its interesting properties. We believe the additional step is worthwhile. While tokenization evaluation is important, it would be more effective if employed at a later stage. On the one hand, critical tokenization can help greatly in developing tokenization knowledge and heuristics, especially those tokenization specific understandings, such 592 Guo Critical Tokenization as the observation of &amp;quot;one tokenization per source&amp;quot; and the trick of highlighting hidden ambiguities by contrasting competing critical tokenizations (Guo 1997). While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apparently employed neither in Sproat et al. (1996) nor in Ma (1996). Further, what has been implemented in the two systems is basically a token unigram function, which has been shown to be practically irrelevant to hidden ambiguity resolution and not to be much better than some simple maximum tokenization approaches such as shortest tokenization (Guo 1997). On the other hand, critical tokenization can help significantly in boosting t</context>
<context position="73931" citStr="Guo (1997)" startWordPosition="12196" endWordPosition="12197">ce, while a simple evaluation function such as token unigram cannot be very effective in ambiguity resolution, a sophisticated evaluation function incorporating multiple knowledge sources, such as language experiences, statistics, syntax, semantics, and discourse as suggested in Ma (1996), can only be computationally prohibitive, as Ma himself acknowledged. In summary, the critical tokenization is crucial both in knowledge development for effective tokenization disambiguation and in system implementation for complete and efficient tokenization. Further discussions and examples can be found in Guo (1997). 8. Summary The objective in this paper has been to lay down a mathematical foundation for sentence tokenization. As the basis of the overall mathematical model, we have introduced both sentence generation and sentence tokenization operations. What is unique here is our attempt to model sentence tokenization as the inverse problem of sentence generation. Upon that basis, both critical point and critical fragment constitute our first group of findings. We have proven that, under a complete dictionary assumption, critical points in sentences are all and only unambiguous token boundaries. Critic</context>
</contexts>
<marker>Guo, 1997</marker>
<rawString>Guo, Jin. 1997. Chinese Language Modeling for Speech Recognition. Ph.D. dissertation, Institute of Systems Science, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kekang He</author>
<author>Hui Xu</author>
<author>Bo Sun</author>
</authors>
<title>Design principles of an expert system for automatic word segmentation of written Chinese texts.</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>5--2</pages>
<marker>He, Xu, Sun, 1991</marker>
<rawString>He, Kekang, Hui Xu, and Bo Sun. 1991. Design principles of an expert system for automatic word segmentation of written Chinese texts. Journal of Chinese Information Processing 5(2):1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangxi Huang</author>
</authors>
<title>A produce-test approach to automatic segmentation of written Chinese.</title>
<date>1989</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>3--4</pages>
<contexts>
<context position="2427" citStr="Huang 1989" startWordPosition="356" endWordPosition="357">nce, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University </context>
</contexts>
<marker>Huang, 1989</marker>
<rawString>Huang, Xiangxi. 1989. A produce-test approach to automatic segmentation of written Chinese. Journal of Chinese Information Processing 3(4):42-49.</rawString>
</citation>
<citation valid="true">
<date>1996</date>
<booktitle>Essays on Language Information Processing.</booktitle>
<editor>Huang, Changning and Ying Xia, editors.</editor>
<publisher>Tsinghua University Press,</publisher>
<location>Beijing.</location>
<contexts>
<context position="70591" citStr="(1996)" startWordPosition="11710" endWordPosition="11710">tment, as mentioned above, will not be a waste in any sense. What needs to be undertaken now is to substitute something more precise for the principle of maximum tokenization. It is only at this stage that we touch on the problem of identifying and resolving hidden ambiguity in tokenization. That is one of the reasons why this type of ambiguity is called hidden. 7.3 Critical Tokenization and Best-Path Finding The theme in this paper is to study the problem of sentence tokenization in the framework of formal languages, a direction that has recently attracted some attention. For instance, in Ma (1996), words in a tokenization dictionary are represented as production rules and character strings are modeled as derivatives of these rules under a string concatenation operation. Although not stated explicitly in his thesis, this is obviously a finite-state model, as evidenced from his employment of (finite-) state diagrams for representing both the tokenization dictionary and character strings. The weighted finite-state transducer model developed by Sproat et al. (1996) is another excellent representative example. &apos; They both stop at merely representing possible tokenizations as a single large </context>
<context position="72350" citStr="(1996)" startWordPosition="11964" endWordPosition="11964">tive if employed at a later stage. On the one hand, critical tokenization can help greatly in developing tokenization knowledge and heuristics, especially those tokenization specific understandings, such 592 Guo Critical Tokenization as the observation of &amp;quot;one tokenization per source&amp;quot; and the trick of highlighting hidden ambiguities by contrasting competing critical tokenizations (Guo 1997). While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apparently employed neither in Sproat et al. (1996) nor in Ma (1996). Further, what has been implemented in the two systems is basically a token unigram function, which has been shown to be practically irrelevant to hidden ambiguity resolution and not to be much better than some simple maximum tokenization approaches such as shortest tokenization (Guo 1997). On the other hand, critical tokenization can help significantly in boosting tokenization efficiency. As has been observed, the tokenization of about 98% of the text can be completed in the first parse of critical point identification, which can be done in linear time. Moreover, as practica</context>
<context position="73610" citStr="(1996)" startWordPosition="12154" endWordPosition="12154">tions and ambiguous critical fragments are generally very short, the remaining 2% of the text with tokenization ambiguities can also be settled efficiently through critical tokenization generation and disambiguation (Guo 1997). In comparison, if the best path is to be searched on the token graph of a complete sentence, while a simple evaluation function such as token unigram cannot be very effective in ambiguity resolution, a sophisticated evaluation function incorporating multiple knowledge sources, such as language experiences, statistics, syntax, semantics, and discourse as suggested in Ma (1996), can only be computationally prohibitive, as Ma himself acknowledged. In summary, the critical tokenization is crucial both in knowledge development for effective tokenization disambiguation and in system implementation for complete and efficient tokenization. Further discussions and examples can be found in Guo (1997). 8. Summary The objective in this paper has been to lay down a mathematical foundation for sentence tokenization. As the basis of the overall mathematical model, we have introduced both sentence generation and sentence tokenization operations. What is unique here is our attempt</context>
</contexts>
<marker>1996</marker>
<rawString>Huang, Changning and Ying Xia, editors. 1996. Essays on Language Information Processing. Tsinghua University Press, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Hudson</author>
</authors>
<title>Word Grammar.</title>
<date>1984</date>
<publisher>Basil Blackwell.</publisher>
<contexts>
<context position="1728" citStr="Hudson 1984" startWordPosition="242" endWordPosition="243">riations, such as forward and backward maximum matching and shortest tokenization, are all true subclasses of critical tokenization. It is believed that critical tokenization provides a precise mathematical description of the principle of maximum tokenization. Important implications and practical applications of critical tokenization in effective ambiguity resolution and in efficient tokenization implementation are also carefully examined. 1. Introduction Words, and tokens in general, are the primary building blocks in almost all linguistic theories (e.g., Gazdar, Klein, Pullum, and Sag 1985; Hudson 1984) and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992;</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Hudson, R. A. 1984. Word Grammar. Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Jie</author>
</authors>
<title>A systematic approach model for methods of Chinese automatic word segmentation and their evaluation.</title>
<date>1989</date>
<booktitle>In Proceedings of the Chinese Computing Conference,</booktitle>
<pages>71--78</pages>
<location>Beijing.</location>
<contexts>
<context position="2457" citStr="Jie 1989" startWordPosition="362" endWordPosition="363">e process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Sing</context>
<context position="57443" citStr="Jie 1989" startWordPosition="9696" endWordPosition="9697">tokenization, the backward maximum tokenization, and the shortest tokenization are all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun</context>
<context position="59389" citStr="Jie 1989" startWordPosition="10009" endWordPosition="10010">ciple &amp;quot;were invented one after another and seemed inexhaustible.&amp;quot; While researchers generally agree that a dictionary word should be tokenized as itself, they usually have different opinions on how a non-dictionary word (critical) fragment should be tokenized. While they all agree that a certain form of extremes must be attained, they nevertheless have their own understanding of what the form should be. Consequently, it should come as no surprise to see various kinds of theoretical generalization or summarization work in the literature. A good representative work is by Kit and his colleagues (Jie 1989; Jie, Liu, and Liang 1991a, b; Webster and Kit 1992), who proposed a three-dimensional structural tokenization model. This model, called ASM for Automatic Segmentation Model, is capable of characterizing up to eight classes of different maximum or minimum tokenization procedures. Among the eight procedures, based on both analytical inferences and experimental studies, both forward maximum tokenization and backward maximum tokenization are recommended as good solutions. Unfortunately, in Webster and Kit (1992, 1108), they unnecessarily made the following overly strong claim: It is believed tha</context>
</contexts>
<marker>Jie, 1989</marker>
<rawString>Jie, Chunyu. 1989. A systematic approach model for methods of Chinese automatic word segmentation and their evaluation. In Proceedings of the Chinese Computing Conference, pages 71-78, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Jie</author>
<author>Yuan Liu</author>
<author>Nanyuan Liang</author>
</authors>
<title>On the methods of Chinese automatic segmentation.</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>3--1</pages>
<marker>Jie, Liu, Liang, 1991</marker>
<rawString>Jie, Chunyu, Yuan Liu, and Nanyuan Liang. 1991a. On the methods of Chinese automatic segmentation. Journal of Chinese Information Processing 3(1):1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Jie</author>
<author>Yuan Liu</author>
<author>Nanyuan Liang</author>
</authors>
<title>The design and implementation of the CASS practical automatic Chinese word segmentation system.</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>5--4</pages>
<marker>Jie, Liu, Liang, 1991</marker>
<rawString>Jie, Chunyu, Yuan Liu, and Nanyuan Liang. 1991b. The design and implementation of the CASS practical automatic Chinese word segmentation system. Journal of Chinese Information Processing 5(4):27-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanying Jin</author>
<author>Lu Chen</author>
</authors>
<title>Identifying unknown words in Chinese corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 Natural Language Processing Pacific Rim Symposium (NLPRS&apos;95),</booktitle>
<pages>234--239</pages>
<location>Seoul.</location>
<contexts>
<context position="2510" citStr="Jin and Chen 1995" startWordPosition="370" endWordPosition="373">r strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Ass</context>
</contexts>
<marker>Jin, Chen, 1995</marker>
<rawString>Jin, Wanying and Lu Chen. 1995. Identifying unknown words in Chinese corpora. In Proceedings of the 1995 Natural Language Processing Pacific Rim Symposium (NLPRS&apos;95), pages 234-239, Seoul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Kolman</author>
<author>Robert C Busby</author>
</authors>
<title>edition.</title>
<date>1987</date>
<publisher>Prentice-Hall, Inc.</publisher>
<institution>Discrete Mathematical Structures for Computer Science,</institution>
<contexts>
<context position="8973" citStr="Kolman and Busby (1987)" startWordPosition="1335" endWordPosition="1338">he topic clearly and accurately, a precise and well-defined formal notation is required. What is used in this paper is primarily from elementary Boolean algebra and Formal Language Theory, which can be found in most graduate-level textbooks on discrete mathematics. This section aims at refreshing several simple terms and conventions that will be applied throughout this paper and at introducing the two new concepts of character string generation and tokenization. For the remaining basic concepts and conventions, we mainly follow Aho and Ullman (1972, Chapter 0, Mathematical Preliminaries), and Kolman and Busby (1987). 2.1 Character, Alphabet, and Character String Definition 1 An alphabet E = {a, b, c,. .} is a finite set of symbols. Each symbol in the alphabet is a character. The alphabet size is the number of characters in the alphabet and is denoted 1E1. Character strings over an alphabet E are defined2 in the following manner: 1. e is a character string over E. e is called the empty character string. 2. If S is a character string over E and a is a character in E, then Sa is a character string over E. 3. S&apos; is a character string over E if and only if its being so follows from (1) and (2). The length of </context>
<context position="15839" citStr="Kolman and Busby (1987)" startWordPosition="2607" endWordPosition="2610">ds, and, sand}, there is TD(fundsand) {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}. In other words, both &amp;quot;funds and&amp;quot; and &amp;quot;fund sand&amp;quot; are tokenizations of character string fundsand. 573 Computational Linguistics Volume 23, Number 4 2.5 Discussion Our intention, in formally defining characters and words, is to establish our mathematical system clearly and accurately. To keep discussion concise, the definitions of elementary concepts such as strings and substrings, although widely used in this paper, will be taken for granted. We limit our basic notion to what has already been defined in Aho and Ullman (1972) and Kolman and Busby (1987). Mathematically, word strings are nothing but symbol strings, with each symbol representing a word in the dictionary. In that sense, the word string definition is redundant as it is already covered by the definition of character string. However, since the relationships between character strings and word strings are very important in this paper, we believe it to be appropriate to list both definitions explicitly. What is new in this section is mathematical definitions for character string generation and tokenization. We consider them fundamental to our mathematical description of the string to</context>
<context position="33639" citStr="Kolman and Busby 1987" startWordPosition="5544" endWordPosition="5547">he poset TD(abcd) = {alblcld, alb/cd, albc1d, albcd, ablcld, ab/cd, abcId} can be graphically presented in the Hasse diagram in Figure 1. Certain elements in a poset are of special importance for many of the properties and applications of posets. In this paper, we are particularly interested in the minimal elements and least elements. In standard textbooks, they are defined in the following manner: Let (A, &lt;) be a poset. An element a E A is called a minimal element of A if there is no element c E A, c a, such that c &lt; a. An element a E A is called a least element of A if a &lt; x for all x E A. (Kolman and Busby 1987, 195-196). 579 Figure 1 The Hasse diagram for the poset TD(abcd) = falblcld, alb/cd, albcld, a/bcd, ablcld, ab/cd, abc/d}. Example 1 (cont.) The word string &amp;quot;this is his book&amp;quot; is both the minimal element and the least element of both TD(thisishisbook) = {&amp;quot;this is his book&amp;quot;} and TD, (thisishisbook) = {&amp;quot;th is is his book&amp;quot;, &amp;quot;this is his book&amp;quot;}. Example 2 (cont.) The poset TD(fundsand) = {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;} has both &amp;quot;funds and&amp;quot; and &amp;quot;fund sand&amp;quot; as its minimal elements, but has no least element. Example 3 (cont.) The poset TD(abcd) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d} has</context>
</contexts>
<marker>Kolman, Busby, 1987</marker>
<rawString>Kolman, Bernard and Robert C. Busby. 1987. Discrete Mathematical Structures for Computer Science, 2nd edition. Prentice-Hall, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T B Y Lai</author>
<author>S C Lun</author>
<author>C F Sun</author>
<author>M S Sun</author>
</authors>
<title>A tagging-based first-order Markov model approach to automatic word identification for Chinese sentences.</title>
<date>1992</date>
<booktitle>In Proceedings of the 1992 International Conference on Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>17--23</pages>
<contexts>
<context position="2527" citStr="Lai et al. 1992" startWordPosition="374" endWordPosition="377">s of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Comp</context>
</contexts>
<marker>Lai, Lun, Sun, Sun, 1992</marker>
<rawString>Lai, T. B. Y., S. C. Lun, C. F. Sun, and M. S. Sun. 1992. A tagging-based first-order Markov model approach to automatic word identification for Chinese sentences. In Proceedings of the 1992 International Conference on Computer Processing of Chinese and Oriental Languages, pages 17-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Jie Li</author>
<author>H-H Pan</author>
<author>M Zhou</author>
<author>K-F Wong</author>
<author>V Lum</author>
</authors>
<title>Corpus-based maximum-length Chinese noun phrase extraction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 Natural Language Processing Pacific Rim Symposium (NLPRS&apos;95),</booktitle>
<pages>246--251</pages>
<location>Seoul.</location>
<contexts>
<context position="2543" citStr="Li et al. 1995" startWordPosition="378" endWordPosition="381">e initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Lingui</context>
<context position="57660" citStr="Li et al. 1995" startWordPosition="9740" endWordPosition="9743">5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most representative </context>
</contexts>
<marker>Li, Pan, Zhou, Wong, Lum, 1995</marker>
<rawString>Li, Wen-Jie, H-H. Pan, M. Zhou, K-F. Wong, and V. Lum. 1995. Corpus-based maximum-length Chinese noun phrase extraction. In Proceedings of the 1995 Natural Language Processing Pacific Rim Symposium (NLPRS&apos;95), pages 246-251, Seoul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
</authors>
<title>On computer automatic word segmentation of written Chinese.</title>
<date>1986</date>
<journal>Journal of Chinese Information Processing</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2555" citStr="Liang 1986" startWordPosition="382" endWordPosition="383">n natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Comput</context>
<context position="50949" citStr="Liang 1986" startWordPosition="8490" endWordPosition="8491"> TD (S) is a forward maximum tokenization of S over E and D, or FT tokenization for short, if, for any k, 1 &lt; k &lt; m, there exist i and j, 1 &lt;i &lt;j &lt; n, such that5 1. G(wi wk_i) = cl • • • ci-1, 2. Wk = Ci Ci, and 3. For any j&apos;, j &lt; j&apos; &lt;n, there is ci V D. The forward maximum tokenization operation, or FT operation for short, is a mapping FD: E* 2D* defined as: for any S E E*, FD(S) = {W I W is a FT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) The character string S = abcd has the word string abc/d as its sole FT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}, i.e., FD(S) = {abc/d}. Example 2 (cont.) FD(fundsand) = {&amp;quot;funds and&amp;quot;}, i.e., the character string fundsand has its sole FT tokenization &amp;quot;funds and&amp;quot;. Example 4 (cont.) FD(S) = {&amp;quot;the blueprint&amp;quot;}, i.e., the word string &amp;quot;the blueprint&amp;quot; is the only FT tokenization for the character string S = the blueprint. Lemma 4 For all S E E*, there are IFD(S)I &lt;1 and FD(S) c CD (S). That is to s</context>
<context position="53955" citStr="Liang 1986" startWordPosition="9082" endWordPosition="9083">15 A tokenization W = Wm E TD (5) is a backward maximum tokenization of S over E and D, or BT tokenization for short, if for any k, 1 &lt;k &lt; m, there exist i and j, 1 &lt;1 &lt;j &lt; n, such that 1. G(Wk+i • • • Wm) = Cj+1 • • • Cm, 2. wk = ci and 3. For any i&apos;, 1 &lt; &lt;i, there is ...Ci 0 D. The backward maximum tokenization operation is a mapping BD: E* 2D* defined as: for any S E E*, BD(S) = {W I W is a BT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) For the character string S = abcd, the word string a/bcd is the only BT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, BD(S) = {a/bcd}. Example 2 (cont.) For the character string S = fundsand, there is BD(fundsand) = {&amp;quot;fund sand&amp;quot;}. That is, the word string &amp;quot;fund sand&amp;quot; is the only BT tokenization. Example 4 (cont.) For the character string S = theblueprint, there is BD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the word string &amp;quot;the blueprint&amp;quot; is the only BT tokenization. Lemma 5 For all S E E*, t</context>
<context position="57414" citStr="Liang 1986" startWordPosition="9691" endWordPosition="9692">. That is, the forward maximum tokenization, the backward maximum tokenization, and the shortest tokenization are all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in </context>
</contexts>
<marker>Liang, 1986</marker>
<rawString>Liang, Nanyuan. 1986. On computer automatic word segmentation of written Chinese. Journal of Chinese Information Processing 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
</authors>
<title>CDWS-A written Chinese automatic word segmentation system.</title>
<date>1987</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>1--2</pages>
<marker>Liang, 1987</marker>
<rawString>Liang, Nanyuan. 1987. CDWS-A written Chinese automatic word segmentation system. Journal of Chinese Information Processing 1(2):44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
</authors>
<title>The knowledge of Chinese words segmentation.</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>4--2</pages>
<marker>Liang, 1990</marker>
<rawString>Liang, Nanyuan. 1990. The knowledge of Chinese words segmentation. Journal of Chinese Information Processing 4(2):29-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongquan Liu</author>
</authors>
<title>Language Modernization and Computer.</title>
<date>1986</date>
<publisher>Wuhan University Press,</publisher>
<location>Wuhan, China.</location>
<contexts>
<context position="2577" citStr="Liu 1986" startWordPosition="386" endWordPosition="387">ssing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Vo</context>
<context position="50929" citStr="Liu 1986" startWordPosition="8487" endWordPosition="8488">enization W zvn, E TD (S) is a forward maximum tokenization of S over E and D, or FT tokenization for short, if, for any k, 1 &lt; k &lt; m, there exist i and j, 1 &lt;i &lt;j &lt; n, such that5 1. G(wi wk_i) = cl • • • ci-1, 2. Wk = Ci Ci, and 3. For any j&apos;, j &lt; j&apos; &lt;n, there is ci V D. The forward maximum tokenization operation, or FT operation for short, is a mapping FD: E* 2D* defined as: for any S E E*, FD(S) = {W I W is a FT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) The character string S = abcd has the word string abc/d as its sole FT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}, i.e., FD(S) = {abc/d}. Example 2 (cont.) FD(fundsand) = {&amp;quot;funds and&amp;quot;}, i.e., the character string fundsand has its sole FT tokenization &amp;quot;funds and&amp;quot;. Example 4 (cont.) FD(S) = {&amp;quot;the blueprint&amp;quot;}, i.e., the word string &amp;quot;the blueprint&amp;quot; is the only FT tokenization for the character string S = the blueprint. Lemma 4 For all S E E*, there are IFD(S)I &lt;1 and FD(S) c </context>
<context position="53935" citStr="Liu 1986" startWordPosition="9079" endWordPosition="9080">habet. Definition 15 A tokenization W = Wm E TD (5) is a backward maximum tokenization of S over E and D, or BT tokenization for short, if for any k, 1 &lt;k &lt; m, there exist i and j, 1 &lt;1 &lt;j &lt; n, such that 1. G(Wk+i • • • Wm) = Cj+1 • • • Cm, 2. wk = ci and 3. For any i&apos;, 1 &lt; &lt;i, there is ...Ci 0 D. The backward maximum tokenization operation is a mapping BD: E* 2D* defined as: for any S E E*, BD(S) = {W I W is a BT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) For the character string S = abcd, the word string a/bcd is the only BT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, BD(S) = {a/bcd}. Example 2 (cont.) For the character string S = fundsand, there is BD(fundsand) = {&amp;quot;fund sand&amp;quot;}. That is, the word string &amp;quot;fund sand&amp;quot; is the only BT tokenization. Example 4 (cont.) For the character string S = theblueprint, there is BD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the word string &amp;quot;the blueprint&amp;quot; is the only BT tokenization. Lemma</context>
<context position="57402" citStr="Liu 1986" startWordPosition="9689" endWordPosition="9690">D(S) CD(S). That is, the forward maximum tokenization, the backward maximum tokenization, and the shortest tokenization are all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and </context>
</contexts>
<marker>Liu, 1986</marker>
<rawString>Liu, Yongquan. 1986a. Language Modernization and Computer. Wuhan University Press, Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongquan Liu</author>
</authors>
<title>On dictionary.</title>
<date>1986</date>
<journal>Journal of Chinese Information Processing</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2577" citStr="Liu 1986" startWordPosition="386" endWordPosition="387">ssing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Vo</context>
<context position="50929" citStr="Liu 1986" startWordPosition="8487" endWordPosition="8488">enization W zvn, E TD (S) is a forward maximum tokenization of S over E and D, or FT tokenization for short, if, for any k, 1 &lt; k &lt; m, there exist i and j, 1 &lt;i &lt;j &lt; n, such that5 1. G(wi wk_i) = cl • • • ci-1, 2. Wk = Ci Ci, and 3. For any j&apos;, j &lt; j&apos; &lt;n, there is ci V D. The forward maximum tokenization operation, or FT operation for short, is a mapping FD: E* 2D* defined as: for any S E E*, FD(S) = {W I W is a FT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) The character string S = abcd has the word string abc/d as its sole FT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}, i.e., FD(S) = {abc/d}. Example 2 (cont.) FD(fundsand) = {&amp;quot;funds and&amp;quot;}, i.e., the character string fundsand has its sole FT tokenization &amp;quot;funds and&amp;quot;. Example 4 (cont.) FD(S) = {&amp;quot;the blueprint&amp;quot;}, i.e., the word string &amp;quot;the blueprint&amp;quot; is the only FT tokenization for the character string S = the blueprint. Lemma 4 For all S E E*, there are IFD(S)I &lt;1 and FD(S) c </context>
<context position="53935" citStr="Liu 1986" startWordPosition="9079" endWordPosition="9080">habet. Definition 15 A tokenization W = Wm E TD (5) is a backward maximum tokenization of S over E and D, or BT tokenization for short, if for any k, 1 &lt;k &lt; m, there exist i and j, 1 &lt;1 &lt;j &lt; n, such that 1. G(Wk+i • • • Wm) = Cj+1 • • • Cm, 2. wk = ci and 3. For any i&apos;, 1 &lt; &lt;i, there is ...Ci 0 D. The backward maximum tokenization operation is a mapping BD: E* 2D* defined as: for any S E E*, BD(S) = {W I W is a BT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) For the character string S = abcd, the word string a/bcd is the only BT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, BD(S) = {a/bcd}. Example 2 (cont.) For the character string S = fundsand, there is BD(fundsand) = {&amp;quot;fund sand&amp;quot;}. That is, the word string &amp;quot;fund sand&amp;quot; is the only BT tokenization. Example 4 (cont.) For the character string S = theblueprint, there is BD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the word string &amp;quot;the blueprint&amp;quot; is the only BT tokenization. Lemma</context>
<context position="57402" citStr="Liu 1986" startWordPosition="9689" endWordPosition="9690">D(S) CD(S). That is, the forward maximum tokenization, the backward maximum tokenization, and the shortest tokenization are all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and </context>
</contexts>
<marker>Liu, 1986</marker>
<rawString>Liu, Yongquan. 1986b. On dictionary. Journal of Chinese Information Processing 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongquan Liu</author>
</authors>
<title>Word re-examination.</title>
<date>1988</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>2--2</pages>
<marker>Liu, 1988</marker>
<rawString>Liu, Yongquan. 1988. Word re-examination. Journal of Chinese Information Processing 2(2):47-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Liu</author>
<author>Qiang Tan</author>
<author>Xukun Shen</author>
</authors>
<title>Contemporary Chinese Language Word Segmentation Specification for Information Processing and Automatic Word Segmentation Methods.</title>
<date>1994</date>
<publisher>Tsinghua University Press,</publisher>
<location>Beijing.</location>
<marker>Liu, Tan, Shen, 1994</marker>
<rawString>Liu, Yuan, Qiang Tan, and Xukun Shen. 1994. Contemporary Chinese Language Word Segmentation Specification for Information Processing and Automatic Word Segmentation Methods. Tsinghua University Press, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Teng Lua</author>
</authors>
<title>From character to word-An application of information theory.</title>
<date>1990</date>
<booktitle>Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>4--4</pages>
<contexts>
<context position="2620" citStr="Lua 1990" startWordPosition="394" endWordPosition="395">ten Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem </context>
</contexts>
<marker>Lua, 1990</marker>
<rawString>Lua, Kim Teng. 1990. From character to word-An application of information theory. Computer Processing of Chinese and Oriental Languages 4(4):304-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Teng Lua</author>
</authors>
<title>Application of information theory binding in word segmentation.</title>
<date>1994</date>
<booktitle>Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>8--1</pages>
<marker>Lua, 1994</marker>
<rawString>Lua, Kim Teng. 1994. Application of information theory binding in word segmentation. Computer Processing of Chinese and Oriental Languages 8(1):115-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Teng Lua</author>
</authors>
<title>Experiments on the use of bigram mutual information in Chinese natural language processing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages (ICCPOL-95),</booktitle>
<pages>306--313</pages>
<location>Hawaii.</location>
<marker>Lua, 1995</marker>
<rawString>Lua, Kim Teng. 1995. Experiments on the use of bigram mutual information in Chinese natural language processing. In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages (ICCPOL-95), pages 306-313, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Ma</author>
</authors>
<title>The study and realization of an evaluation-based automatic segmentation system.</title>
<date>1996</date>
<booktitle>In Changning Huang and Ying Xia, editors, Essays in Language Information Processing.</booktitle>
<pages>2--36</pages>
<publisher>Tsinghua University Press,</publisher>
<location>Beijing,</location>
<contexts>
<context position="2645" citStr="Ma 1996" startWordPosition="399" endWordPosition="400">plicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natu</context>
<context position="70591" citStr="Ma (1996)" startWordPosition="11709" endWordPosition="11710">vestment, as mentioned above, will not be a waste in any sense. What needs to be undertaken now is to substitute something more precise for the principle of maximum tokenization. It is only at this stage that we touch on the problem of identifying and resolving hidden ambiguity in tokenization. That is one of the reasons why this type of ambiguity is called hidden. 7.3 Critical Tokenization and Best-Path Finding The theme in this paper is to study the problem of sentence tokenization in the framework of formal languages, a direction that has recently attracted some attention. For instance, in Ma (1996), words in a tokenization dictionary are represented as production rules and character strings are modeled as derivatives of these rules under a string concatenation operation. Although not stated explicitly in his thesis, this is obviously a finite-state model, as evidenced from his employment of (finite-) state diagrams for representing both the tokenization dictionary and character strings. The weighted finite-state transducer model developed by Sproat et al. (1996) is another excellent representative example. &apos; They both stop at merely representing possible tokenizations as a single large </context>
<context position="72367" citStr="Ma (1996)" startWordPosition="11967" endWordPosition="11968">ed at a later stage. On the one hand, critical tokenization can help greatly in developing tokenization knowledge and heuristics, especially those tokenization specific understandings, such 592 Guo Critical Tokenization as the observation of &amp;quot;one tokenization per source&amp;quot; and the trick of highlighting hidden ambiguities by contrasting competing critical tokenizations (Guo 1997). While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apparently employed neither in Sproat et al. (1996) nor in Ma (1996). Further, what has been implemented in the two systems is basically a token unigram function, which has been shown to be practically irrelevant to hidden ambiguity resolution and not to be much better than some simple maximum tokenization approaches such as shortest tokenization (Guo 1997). On the other hand, critical tokenization can help significantly in boosting tokenization efficiency. As has been observed, the tokenization of about 98% of the text can be completed in the first parse of critical point identification, which can be done in linear time. Moreover, as practically all acceptabl</context>
<context position="73610" citStr="Ma (1996)" startWordPosition="12153" endWordPosition="12154">izations and ambiguous critical fragments are generally very short, the remaining 2% of the text with tokenization ambiguities can also be settled efficiently through critical tokenization generation and disambiguation (Guo 1997). In comparison, if the best path is to be searched on the token graph of a complete sentence, while a simple evaluation function such as token unigram cannot be very effective in ambiguity resolution, a sophisticated evaluation function incorporating multiple knowledge sources, such as language experiences, statistics, syntax, semantics, and discourse as suggested in Ma (1996), can only be computationally prohibitive, as Ma himself acknowledged. In summary, the critical tokenization is crucial both in knowledge development for effective tokenization disambiguation and in system implementation for complete and efficient tokenization. Further discussions and examples can be found in Guo (1997). 8. Summary The objective in this paper has been to lay down a mathematical foundation for sentence tokenization. As the basis of the overall mathematical model, we have introduced both sentence generation and sentence tokenization operations. What is unique here is our attempt</context>
</contexts>
<marker>Ma, 1996</marker>
<rawString>Ma, Yan. 1996. The study and realization of an evaluation-based automatic segmentation system. In Changning Huang and Ying Xia, editors, Essays in Language Information Processing. Tsinghua University Press, Beijing, pages 2-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jieyun Nie</author>
<author>Jin Wanying</author>
<author>M-L Hannan</author>
</authors>
<title>A hybrid approach to unknown word detection and segmentation of Chinese.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Chinese Computing 1994 (ICCC-94),</booktitle>
<pages>326--335</pages>
<marker>Nie, Wanying, Hannan, 1994</marker>
<rawString>Nie, Jieyun, Jin Wanying, and M-L. Hannan. 1994. A hybrid approach to unknown word detection and segmentation of Chinese. In Proceedings of the International Conference on Chinese Computing 1994 (ICCC-94), pages 326-335, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wothke Mertineit</author>
<author>R Schmidt</author>
</authors>
<title>Broad coverage automatic morphological segmentation of German words.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92),</booktitle>
<pages>1218--1222</pages>
<location>Nantes, France.</location>
<marker>Mertineit, Schmidt, 1992</marker>
<rawString>Pachunke, T., 0. Mertineit, K. Wothke, and R. Schmidt. 1992. Broad coverage automatic morphological segmentation of German words. In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92), pages 1218-1222, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Seo</author>
<author>R F Simmons</author>
</authors>
<title>Syntactic graphs: A representation for the union of all ambiguous parse trees.</title>
<date>1989</date>
<journal>Computational Linguistics</journal>
<pages>15--1</pages>
<contexts>
<context position="67749" citStr="Seo and Simmons (1989)" startWordPosition="11264" endWordPosition="11267">of these properties, the tokenization problem can be greatly simplified. For example, among the huge number of possible tokenizations, we can first concentrate on the much smaller. critical tokenization set, since the former can be completely reproduced from the latter. Furthermore, by contrasting critical tokenizations, we can easily identify a few critically ambiguous positions, which allows us to avoid wasting energy at useless positions. 7.2 Critical Tokenization and the Syntactic Graph It is worth noting that similar ideas do exist in natural language derivation and parsing. For example, Seo and Simmons (1989) introduced the concept of the syntactic graph, which is, in essence, a union of all possible parse trees. With this graph representation, &amp;quot;it is fairly easy to focus on the syntactically ambiguous points&amp;quot; (p. 19, italics added). 591 Computational Linguistics Volume 23, Number 4 These syntactically ambiguous points are critical in at least two senses. First, they are the only problems requiring knowledge and heuristics beyond the existing syntax. In other words, any syntactic or semantics development should be guided by ambiguity resolution at these points. If a semantic enhancement does not i</context>
</contexts>
<marker>Seo, Simmons, 1989</marker>
<rawString>Seo, J. and R. F. Simmons. 1989. Syntactic graphs: A representation for the union of all ambiguous parse trees. Computational Linguistics 15(1):19-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>A statistical method for finding word boundaries</title>
<date>1990</date>
<booktitle>in Chinese text. Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>4--4</pages>
<contexts>
<context position="2694" citStr="Sproat and Shih 1990" startWordPosition="406" endWordPosition="409"> the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Take</context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Sproat, Richard and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese and Oriental Languages 4(4):336-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chain Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<pages>22--3</pages>
<contexts>
<context position="2714" citStr="Sproat et al. 1996" startWordPosition="410" endWordPosition="413">itten English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 199</context>
<context position="57628" citStr="Sproat et al. 1996" startWordPosition="9732" endWordPosition="9735">part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization ar</context>
<context position="71064" citStr="Sproat et al. (1996)" startWordPosition="11775" endWordPosition="11778">m of sentence tokenization in the framework of formal languages, a direction that has recently attracted some attention. For instance, in Ma (1996), words in a tokenization dictionary are represented as production rules and character strings are modeled as derivatives of these rules under a string concatenation operation. Although not stated explicitly in his thesis, this is obviously a finite-state model, as evidenced from his employment of (finite-) state diagrams for representing both the tokenization dictionary and character strings. The weighted finite-state transducer model developed by Sproat et al. (1996) is another excellent representative example. &apos; They both stop at merely representing possible tokenizations as a single large finite-state diagram (word graph). The focus is then shifted to the problem of defining scores for evaluating each possible tokenization and to the associated problem of searching for the best-path in the word graph. To emphasize this point, Ma (1996) explicitly called his approach &amp;quot;evaluation-based.&amp;quot; In comparison, we have continued within the framework and established the critical tokenization together with its interesting properties. We believe the additional step i</context>
<context position="72350" citStr="Sproat et al. (1996)" startWordPosition="11961" endWordPosition="11964"> be more effective if employed at a later stage. On the one hand, critical tokenization can help greatly in developing tokenization knowledge and heuristics, especially those tokenization specific understandings, such 592 Guo Critical Tokenization as the observation of &amp;quot;one tokenization per source&amp;quot; and the trick of highlighting hidden ambiguities by contrasting competing critical tokenizations (Guo 1997). While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apparently employed neither in Sproat et al. (1996) nor in Ma (1996). Further, what has been implemented in the two systems is basically a token unigram function, which has been shown to be practically irrelevant to hidden ambiguity resolution and not to be much better than some simple maximum tokenization approaches such as shortest tokenization (Guo 1997). On the other hand, critical tokenization can help significantly in boosting tokenization efficiency. As has been observed, the tokenization of about 98% of the text can be completed in the first parse of critical point identification, which can be done in linear time. Moreover, as practica</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Sproat, Richard, Chain Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics 22(3):377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Changning Huang</author>
</authors>
<title>Word segmentation and part-of-speech tagging for unrestricted Chinese texts.</title>
<date>1996</date>
<booktitle>Tutorial given at the 1996 International Conference on Chinese Computing (ICCC-96),</booktitle>
<contexts>
<context position="2754" citStr="Sun and Huang 1996" startWordPosition="418" endWordPosition="421">entence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), Ger</context>
<context position="57729" citStr="Sun and Huang 1996" startWordPosition="9754" endWordPosition="9757">6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most representative and widely quoted works following the general principle of maximum to</context>
</contexts>
<marker>Sun, Huang, 1996</marker>
<rawString>Sun, Maosong and Changning Huang. 1996. Word segmentation and part-of-speech tagging for unrestricted Chinese texts. Tutorial given at the 1996 International Conference on Chinese Computing (ICCC-96), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Benjemin T&apos;sou</author>
</authors>
<title>Ambiguity resolution in Chinese word segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation (PACLIC-95),</booktitle>
<pages>121--126</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2734" citStr="Sun and T&apos;sou 1995" startWordPosition="414" endWordPosition="417">problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee</context>
<context position="57680" citStr="Sun and T&apos;sou 1995" startWordPosition="9744" endWordPosition="9747">cond part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most representative and widely quoted wo</context>
</contexts>
<marker>Sun, T&apos;sou, 1995</marker>
<rawString>Sun, Maosong and Benjemin T&apos;sou. 1995. Ambiguity resolution in Chinese word segmentation. In Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation (PACLIC-95), pages 121-126, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Tung</author>
<author>H J Lee</author>
</authors>
<title>Identification of unknown words from corpus.</title>
<date>1994</date>
<booktitle>Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>8--131</pages>
<contexts>
<context position="2773" citStr="Tung and Lee 1994" startWordPosition="422" endWordPosition="425"> has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al</context>
</contexts>
<marker>Tung, Lee, 1994</marker>
<rawString>Tung, C. H. and H. J. Lee. 1994. Identification of unknown words from corpus. Computer Processing of Chinese and Oriental Languages 8(Supplement):131-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongcheng Wang</author>
<author>Haiju Su</author>
<author>Yan Mo</author>
</authors>
<title>Automatic processing Chinese word.</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>4--4</pages>
<marker>Wang, Su, Mo, 1990</marker>
<rawString>Wang, Yongcheng, Haiju Su, and Yan Mo. 1990. Automatic processing Chinese word. Journal of Chinese Information Processing 4(4):1-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolong Wang</author>
</authors>
<title>Word Separating and Mutual Translation of Syllable and Character Strings.</title>
<date>1989</date>
<tech>Ph.D. dissertation,</tech>
<institution>Department of Computer Science and Engineering, Harbin Institute of Technology,</institution>
<location>Harbin, China.</location>
<contexts>
<context position="2807" citStr="Wang 1989" startWordPosition="431" endWordPosition="432">ch efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Lee</context>
<context position="29629" citStr="Wang (1989" startWordPosition="4828" endWordPosition="4829"> broken down into several short but fully ambiguous critical fragments. As shown in Guo (1997), critical points can be completely identified in linear time. Moreover, in practice, most critical fragments are dictionary tokens by themselves, and the remaining nondictionary fragments are generally very short. In short, the understanding of critical points and fragments will significantly assist us in both efficient tokenization implementation and tokenization ambiguity resolution. The concepts of critical point and critical fragment are similar to those of segment point and character segment in Wang (1989, 37), which were defined on a sentence word graph for the purpose of analyzing the computational complexity of his new tokenization algorithm. However, Wang (1989) neither noticed their connection with tokenization ambiguities nor realized the importance of the complete dictionary assumption, and hence failed to demonstrate their crucial role in sentence tokenization. 4. Critical Tokenization This section seeks to disclose an important structure of the set of different tokenizations. We will see that different tokenizations can be linked by the cover relationship to form a partially ordered s</context>
<context position="55412" citStr="Wang (1989)" startWordPosition="9334" endWordPosition="9335">for Lemma 4. 0 6.3 Shortest Tokenization Definition 16 The shortest tokenization operation SD is a mapping SD: E* —› 2D* defined as: for any S in E*, SD(S) -= {W I IWI= Every tokenization Win SD(S) is a shortest tokenization, or ST tokenization for short, of the character string S. In other words, a tokenization W of a character string S is a shortest tokenization if and only if the word string has the minimum word string length among all possible tokenizations. This definition is in fact a descriptive interpretation of the constructive shortest path finding tokenization procedure proposed by Wang (1989) and Wang, Wang, and Bai (1991). Example 3 (cont.) Given the character string S = abcd. For the dictionary D = {a, b, c, d, ab, bc, cd, abc, bcd} , both abc/d and a/bcd are ST tokenizations in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, SD(S) = {abc/d, a/bcd}. For D&apos; = {a,b,c,d,ab,bc,cd}, however, there is Sly (S) = {ab/cd}. Note, in this case, the CT tokenization a/bc/d is not in SD, (S). Example 2 (cont.) For the character string S = fundsand, there is SD(fundsand) = {&amp;quot;funds and&amp;quot;, &amp;quot;fund sand&amp;quot;}. That is, both &amp;quot;funds and&amp;quot; and &amp;quot;fund sand&amp;quot; are ST tokenizations. Examp</context>
<context position="57433" citStr="Wang 1989" startWordPosition="9694" endWordPosition="9695">rd maximum tokenization, the backward maximum tokenization, and the shortest tokenization are all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For ins</context>
<context position="60236" citStr="Wang (1989)" startWordPosition="10131" endWordPosition="10132">t maximum or minimum tokenization procedures. Among the eight procedures, based on both analytical inferences and experimental studies, both forward maximum tokenization and backward maximum tokenization are recommended as good solutions. Unfortunately, in Webster and Kit (1992, 1108), they unnecessarily made the following overly strong claim: It is believed that all elemental methods are included in this model. Furthermore, it can be viewed as the ultimate model for methods of string matching of any elements, including methods for finding English idioms. The shortest tokenization proposed by Wang (1989) provides an obvious counterexample. As Wang (1989) exemplified&apos;, for the alphabet E = {a, b, c, d, e} and the 6 The original example is &amp;quot;M 1=1 a -T &amp;quot;, a widely quoted Chinese phrase difficult to tokenize. Its 589 Computational Linguistics Volume 23, Number 4 dictionary D = fa,b,c,d,e,ab,bc,cd,del, the character string S = abcde has FT set FD(S) = {ab/cd/e}, BT set BD(S) = {albcIde} and ST set SD(S) = {ab/cd/e, albcIde, ablcIde}. Clearly, the ST tokenization ablcIde, which fulfills the principle of maximum tokenization and is the desired tokenization in some cases, is neither FT nor BT tokeniz</context>
</contexts>
<marker>Wang, 1989</marker>
<rawString>Wang, Xiaolong. 1989. Word Separating and Mutual Translation of Syllable and Character Strings. Ph.D. dissertation, Department of Computer Science and Engineering, Harbin Institute of Technology, Harbin, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolong Wang</author>
<author>Kaizhu Wang</author>
<author>Xiaohua Bai</author>
</authors>
<title>Separating syllables and characters into words in natural language understanding.</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>5--3</pages>
<marker>Wang, Wang, Bai, 1991</marker>
<rawString>Wang, Xiaolong, Kaizhu Wang, and Xiaohua Bai. 1991. Separating syllables and characters into words in natural language understanding. Journal of Chinese Information Processing 5(3):48-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan J Webster</author>
<author>Chunyu Kit</author>
</authors>
<title>Tokenization as the initial phase in NLP.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92),</booktitle>
<pages>1--106</pages>
<location>Nantes, France.</location>
<contexts>
<context position="1997" citStr="Webster and Kit 1992" startWordPosition="281" endWordPosition="284"> Important implications and practical applications of critical tokenization in effective ambiguity resolution and in efficient tokenization implementation are also carefully examined. 1. Introduction Words, and tokens in general, are the primary building blocks in almost all linguistic theories (e.g., Gazdar, Klein, Pullum, and Sag 1985; Hudson 1984) and language processing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986). Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992). Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, </context>
<context position="46759" citStr="Webster and Kit (1992" startWordPosition="7775" endWordPosition="7778">ired in sentence tokenization. Consequently, it must be crucial and beneficial to pursue an explicit and accurate understanding of various types of character string tokenization ambiguities and their relationships. In the literature, however, the general practice is not to formally define and classify ambiguities but to apply various terms to them, such as overlapping ambiguity and combinational ambiguity in their intuitive and normally fuzzy senses. Nevertheless, efforts do exist to rigorously assign them precise, formal meanings. As a representa584 Guo Critical Tokenization tive example, in Webster and Kit (1992, 1108), both conjunctive (combinational) and disjunctive (overlapping) ambiguities are defined in the manner given below. 1. TYPE I: In a sequence of Chinese&apos; characters S = al atbi bj, if al at, b1 bj, and S are each a word, then there is conjunctive ambiguity in S. The segment S. which is itself a word, contains other words. This is also known as multi-combinational ambiguity. 2. TYPE II: In a sequence of Chinese characters S = al . aibi bici ck, if al ... b) and b1 ck are each a word, then S is an overlapping ambiguous segment, or in other words, the segment S displays disjunctive ambiguit</context>
<context position="50997" citStr="Webster and Kit 1992" startWordPosition="8497" endWordPosition="8500">tion of S over E and D, or FT tokenization for short, if, for any k, 1 &lt; k &lt; m, there exist i and j, 1 &lt;i &lt;j &lt; n, such that5 1. G(wi wk_i) = cl • • • ci-1, 2. Wk = Ci Ci, and 3. For any j&apos;, j &lt; j&apos; &lt;n, there is ci V D. The forward maximum tokenization operation, or FT operation for short, is a mapping FD: E* 2D* defined as: for any S E E*, FD(S) = {W I W is a FT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) The character string S = abcd has the word string abc/d as its sole FT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}, i.e., FD(S) = {abc/d}. Example 2 (cont.) FD(fundsand) = {&amp;quot;funds and&amp;quot;}, i.e., the character string fundsand has its sole FT tokenization &amp;quot;funds and&amp;quot;. Example 4 (cont.) FD(S) = {&amp;quot;the blueprint&amp;quot;}, i.e., the word string &amp;quot;the blueprint&amp;quot; is the only FT tokenization for the character string S = the blueprint. Lemma 4 For all S E E*, there are IFD(S)I &lt;1 and FD(S) c CD (S). That is to say, any character string has, at most, a single </context>
<context position="54003" citStr="Webster and Kit 1992" startWordPosition="9089" endWordPosition="9092">a backward maximum tokenization of S over E and D, or BT tokenization for short, if for any k, 1 &lt;k &lt; m, there exist i and j, 1 &lt;1 &lt;j &lt; n, such that 1. G(Wk+i • • • Wm) = Cj+1 • • • Cm, 2. wk = ci and 3. For any i&apos;, 1 &lt; &lt;i, there is ...Ci 0 D. The backward maximum tokenization operation is a mapping BD: E* 2D* defined as: for any S E E*, BD(S) = {W I W is a BT tokenization of S over E and D}. This definition is in fact a descriptive interpretation of the widely recommended conventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992). Example 3 (cont.) For the character string S = abcd, the word string a/bcd is the only BT tokenization in TD(S) = {a/b/c/d, a/b/cd, a/bc/d, a/bcd, ab/c/d, ab/cd, abc/d}. That is, BD(S) = {a/bcd}. Example 2 (cont.) For the character string S = fundsand, there is BD(fundsand) = {&amp;quot;fund sand&amp;quot;}. That is, the word string &amp;quot;fund sand&amp;quot; is the only BT tokenization. Example 4 (cont.) For the character string S = theblueprint, there is BD(S) = {&amp;quot;the blueprint&amp;quot;}. That is, the word string &amp;quot;the blueprint&amp;quot; is the only BT tokenization. Lemma 5 For all S E E*, there are IBD(S)I &lt;1 and BD(S) c cp(s). 587 Compu</context>
<context position="57536" citStr="Webster and Kit 1992" startWordPosition="9713" endWordPosition="9716">e all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood t</context>
<context position="59442" citStr="Webster and Kit 1992" startWordPosition="10017" endWordPosition="10020">nd seemed inexhaustible.&amp;quot; While researchers generally agree that a dictionary word should be tokenized as itself, they usually have different opinions on how a non-dictionary word (critical) fragment should be tokenized. While they all agree that a certain form of extremes must be attained, they nevertheless have their own understanding of what the form should be. Consequently, it should come as no surprise to see various kinds of theoretical generalization or summarization work in the literature. A good representative work is by Kit and his colleagues (Jie 1989; Jie, Liu, and Liang 1991a, b; Webster and Kit 1992), who proposed a three-dimensional structural tokenization model. This model, called ASM for Automatic Segmentation Model, is capable of characterizing up to eight classes of different maximum or minimum tokenization procedures. Among the eight procedures, based on both analytical inferences and experimental studies, both forward maximum tokenization and backward maximum tokenization are recommended as good solutions. Unfortunately, in Webster and Kit (1992, 1108), they unnecessarily made the following overly strong claim: It is believed that all elemental methods are included in this model. F</context>
<context position="66038" citStr="Webster and Kit (1992" startWordPosition="11007" endWordPosition="11010">nes, and Webber 1986). Following this line, we observed two tendencies in tokenization research. One is the tendency to bring every possible knowledge source into the character string generation operation. For example, Gan (1995) titled his Ph.D. dissertation Integrating Word Boundary Disambiguation with Sentence Understanding. Here, in addition to traditional devices such as syntax and semantics, he even employed principles of psychology and chemistry, such as crystallization. Another is the tendency of enumerating almost blindly every heuristic and trick possible in ambiguity resolution. As Webster and Kit (1992, 1108) noted, &amp;quot;segmentation methods were invented one after another and seemed inexhaustible.&amp;quot; For example, Chen and Liu (1992) acknowledged that the heuristic of maximum matching alone has &amp;quot;many variations&amp;quot; and tested six different implementations. We are not convinced of the effectiveness and necessity of both of the schools of tokenization research. The principle argument is, while research is by nature trial-anderror and different knowledge sources contribute to different facets of the solution, it is nonetheless more crucial and productive to understand where the core of the problem real</context>
</contexts>
<marker>Webster, Kit, 1992</marker>
<rawString>Webster, Jonathan J. and Chunyu Kit. 1992. Tokenization as the initial phase in NLP. In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92), pages 1,106-1,110, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-F Wong</author>
<author>H-H Pan</author>
<author>B-T Low</author>
<author>C-H Cheng</author>
<author>V Lum</author>
<author>S-S Lam</author>
</authors>
<title>A tool for computer-assisted open response analysis.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages,</booktitle>
<pages>191--198</pages>
<location>Hawaii.</location>
<contexts>
<context position="2851" citStr="Wong et al. 1995" startWordPosition="438" endWordPosition="441">nts have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, suc</context>
<context position="57698" citStr="Wong et al. 1995" startWordPosition="9748" endWordPosition="9751">fied by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most representative and widely quoted works following the </context>
</contexts>
<marker>Wong, Pan, Low, Cheng, Lum, Lam, 1995</marker>
<rawString>Wong, K-F., H-H. Pan, B-T. Low, C-H. Cheng, V. Lum, and S-S. Lam. 1995. A tool for computer-assisted open response analysis. In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages, pages 191-198, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K F Wong</author>
<author>V Y Lum</author>
<author>C-Y Leung</author>
<author>C-H Leung</author>
<author>W-K Kan</author>
<author>L-C Chan</author>
</authors>
<title>A parallel approach for identifying word boundaries in Chinese text.</title>
<date>1994</date>
<tech>IPOC Technical Report, /CHIRP/WP/SE/022,</tech>
<institution>Department of Systems Engineering, Chinese University of Hong Kong.</institution>
<contexts>
<context position="2869" citStr="Wong et al. 1994" startWordPosition="442" endWordPosition="445"> (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, such as continuous sp</context>
</contexts>
<marker>Wong, Lum, Leung, Leung, Kan, Chan, 1994</marker>
<rawString>Wong, K. F., V. Y. Lum, C-Y. Leung, C-H. Leung, W-K. Kan, and L-C. Chan. 1994. A parallel approach for identifying word boundaries in Chinese text. IPOC Technical Report, /CHIRP/WP/SE/022, Department of Systems Engineering, Chinese University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horng Jyh Wu</author>
<author>Jin Guo</author>
<author>Ho Chung Lui</author>
<author>Hwee Boon Low</author>
</authors>
<title>Corpus-based speech and language research in the Institute of Systems Science.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Symposium on Speech, Image Processing and Neural Networks (ISPIPNN&apos;94),</booktitle>
<pages>142--145</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2885" citStr="Wu et al. 1994" startWordPosition="446" endWordPosition="449">Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, such as continuous speech and cursive</context>
<context position="57644" citStr="Wu et al. 1994" startWordPosition="9736" endWordPosition="9739">ion of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward maximum tokenization and shortest tokenization are the three most</context>
</contexts>
<marker>Wu, Guo, Lui, Low, 1994</marker>
<rawString>Wu, Horng Jyh, Jin Guo, Ho Chung Lui, and Hwee Boon Low. 1994. Corpus-based speech and language research in the Institute of Systems Science. In Proceedings of the International Symposium on Speech, Image Processing and Neural Networks (ISPIPNN&apos;94), pages 142-145, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wen Wu</author>
<author>Keh-Yih Su</author>
</authors>
<title>Corpus-based automatic compound extraction with mutual information and relative frequency count.</title>
<date>1993</date>
<booktitle>In Proceedings of R.O.C. Computational Linguistics Conference (ROCLING) VI,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="2901" citStr="Wu and Su 1993" startWordPosition="450" endWordPosition="453">4; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, such as continuous speech and cursive handwriting, an</context>
<context position="57581" citStr="Wu and Su 1993" startWordPosition="9723" endWordPosition="9726"> Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenization. It is understood that forward maximum tokenization, backward ma</context>
</contexts>
<marker>Wu, Su, 1993</marker>
<rawString>Wu, Ming-Wen and Keh-Yih Su. 1993. Corpus-based automatic compound extraction with mutual information and relative frequency count. In Proceedings of R.O.C. Computational Linguistics Conference (ROCLING) VI, pages 207-216, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian-Shun Yao</author>
<author>Gui-Ping Zhang</author>
<author>Ying-Ming Wu</author>
</authors>
<title>A rule-based Chinese automatic segmentation system.</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing</journal>
<pages>4--1</pages>
<marker>Yao, Zhang, Wu, 1990</marker>
<rawString>Yao, Tian-Shun, Gui-Ping Zhang, and Ying-Ming Wu. 1990. A rule-based Chinese automatic segmentation system. Journal of Chinese Information Processing 4(1):37-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-L Yeh</author>
<author>H-J Lee</author>
</authors>
<title>Rule-based word identification for Mandarin Chinese sentences—A unification approach.</title>
<date>1991</date>
<booktitle>Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>5--2</pages>
<contexts>
<context position="2944" citStr="Yeh and Lee 1991" startWordPosition="459" endWordPosition="462"> Fan and Tsai 1988; Can 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996; Sun and T&apos;sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al. 1995; Wong et al. 1994; Wu et al. 1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991). * Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-mail: guojin@iss.nrts.sg C.) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, such as continuous speech and cursive handwriting, and in numerous applications, such as transla</context>
<context position="57514" citStr="Yeh and Lee 1991" startWordPosition="9709" endWordPosition="9712">st tokenization are all true subclasses of critical tokenization. 588 Guo Critical Tokenization Proof The first part is the combination of Lemma 4, 5, and 6. The second part is exemplified by Example 3 above. El 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannan 1994; Sproat et al. 1996; Wu et al. 1994; Li et al. 1995; Sun and T&apos;sou 1995; Wong et al. 1995; Bai 1995; Sun and Huang 1996). The first work closest to this principle, according to Liu (1986, 1988), was the 5-4-3-2-1 tokenization algorithm proposed by a Russian MT practitioner in 1956. This algorithm is a special version of the greedy-type implementation of the forward maximum tokenization and is still in active use. For instance, Yun, Lee, and Rim (1995) recently applied it to Korean compound tokenizati</context>
</contexts>
<marker>Yeh, Lee, 1991</marker>
<rawString>Yeh, C-L. and H-J. Lee. 1991. Rule-based word identification for Mandarin Chinese sentences—A unification approach. Computer Processing of Chinese and Oriental Languages 5(2):97-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yosiyuki</author>
<author>T Takenobu</author>
<author>T Hozumi</author>
</authors>
<title>Analysis of Japanese compound nouns using collocation information.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92),</booktitle>
<pages>865--869</pages>
<location>Nantes, France.</location>
<marker>Yosiyuki, Takenobu, Hozumi, 1992</marker>
<rawString>Yosiyuki, K., T. Takenobu, and T. Hozumi. 1992. Analysis of Japanese compound nouns using collocation information. In Proceedings of the 14th International Conference on Computational Linguistics (COLING&apos;92), pages 865-869, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-H Yun</author>
<author>H Lee</author>
<author>H-C Rim</author>
</authors>
<title>Analysis of Korean compound nouns using statistical information.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages (ICCPOL-95),</booktitle>
<pages>76--79</pages>
<location>Honolulu, Hawaii.</location>
<marker>Yun, Lee, Rim, 1995</marker>
<rawString>Yun, B-H., H. Lee, and H-C. Rim. 1995. Analysis of Korean compound nouns using statistical information. In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages (ICCPOL-95), pages 76-79, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Sheng Zhang</author>
<author>Zhi-Da Chen</author>
<author>Shun-De Chen</author>
</authors>
<title>A method of word identification for Chinese by constraint satisfaction and statistical optimization techniques.</title>
<date>1991</date>
<booktitle>In Proceedings of R.O.C. Computational Linguistics Conference (ROCLING) IV,</booktitle>
<pages>147--165</pages>
<marker>Zhang, Chen, Shun-De Chen, 1991</marker>
<rawString>Zhang, Jun-Sheng, Zhi-Da Chen, and Shun-De Chen. 1991. A method of word identification for Chinese by constraint satisfaction and statistical optimization techniques. In Proceedings of R.O.C. Computational Linguistics Conference (ROCLING) IV, pages 147-165, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Sheng Zhang</author>
<author>S J Ker Shun-De Chen</author>
<author>Y Chan</author>
<author>J S Liu</author>
</authors>
<title>A multi-corpus approach to recognition of proper names in Chinese texts.</title>
<date>1994</date>
<booktitle>Computer Processing of Chinese and Oriental Languages</booktitle>
<pages>8--1</pages>
<marker>Zhang, Shun-De Chen, Chan, Liu, 1994</marker>
<rawString>Zhang, Jun-Sheng, Shun-De Chen, S. J. Ker, Y. Chan, and J. S. Liu. 1994. A multi-corpus approach to recognition of proper names in Chinese texts. Computer Processing of Chinese and Oriental Languages 8(1):73-86.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>