<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993549">
Dependency Constraints for Lexical Disambiguation
</title>
<author confidence="0.782899">
Guillaume Bonfante Bruno Guillaume Mathieu Morey
</author>
<note confidence="0.283436">
LORIA INPL LORIA INRIA LORIA Nancy-Universit´e
</note>
<email confidence="0.92453">
guillaume.bonfante@loria.fr bruno.guillaume@loria.fr mathieu.morey@loria.fr
</email>
<sectionHeader confidence="0.996732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960875">
We propose a generic method to per-
form lexical disambiguation in lexicalized
grammatical formalisms. It relies on de-
pendency constraints between words. The
soundness of the method is due to invariant
properties of the parsing in a given gram-
mar that can be computed statically from
the grammar.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978014084507">
In this work, we propose a method of lexical dis-
ambiguation based on the notion of dependencies.
In modern linguistics, Lucien Tesni`ere devel-
oped a formal and sophisticated theory with de-
pendencies (Tesni`ere, 1959). Nowadays, many
current grammatical formalisms rely more or less
explicitly on the notion of dependencies between
words. The most straightforward examples are
formalisms in the Dependency Grammars family
but it is also true of the phrase structure based for-
malisms which consider that words introduce in-
complete syntactic structures which must be com-
pleted by other words. This idea is at the core of
Categorial Grammars (CG) (Lambek, 1958) and
all its trends such as Abstract Categorial Gram-
mars (ACG) (de Groote, 2001) or Combinatory
Categorial Grammars (CCG) (Steedman, 2000),
being mostly encoded in their type system. De-
pendencies in CG were studied in (Moortgat and
Morrill, 1991) and for CCG in (Clark et al.,
2002; Koller and Kuhlmann, 2009). Other for-
malisms can be viewed as modeling and using
dependencies, such as Tree Adjoining Grammars
(TAG) (Joshi, 1987) with their substitution and ad-
junction operations. Dependencies for TAG were
studied in (Joshi and Rambow, 2003). More re-
cently, Marchand et al. (2009) showed that it is
also possible to extract a dependency structure
from a syntactic analysis in Interaction Grammars
(IG) (Guillaume and Perrier, 2008).
Another much more recent concept of polarity
can be used in grammatical formalisms to express
that words introduce incomplete syntactic struc-
tures. IG directly use polarities to describe these
structures but it is also possible to use polarities
in other formalisms in order to make explicit the
more or less implicit notion of incomplete struc-
tures: for instance, in CG (Lamarche, 2008) or in
TAG (Kahane, 2006; Bonfante et al., 2004; Gar-
dent and Kow, 2005). On this regard, Marchand
et al. (2009) exhibited a direct link between polar-
ities and dependencies. This encourages us to say
that in many respects dependencies and polarities
are two sides of the same coin.
The aim of this paper is to show that dependen-
cies can be used to express constraints on the tag-
gings of a sentence and hence these dependency
constraints can be used to partially disambiguate
the words of a sentence. We will see that, in prac-
tice, using the link between dependencies and po-
larities, these dependency constraints can be com-
puted directly from polarized structures.
Exploiting the dependencies encoded in lexical
entries to perform disambiguation is the intuition
behind supertagging (Bangalore and Joshi, 1999),
a method introduced for LTAG and successfully
applied since then to CCG (Clark and Curran,
2004) and HPSG (Ninomiya et al., 2006). These
approaches select the most likely lexical entry (en-
tries) for each word, based on Hidden Markov
Models or Maximum Entropy Models. Like the
work done by Boullier (2003), our method is not
based on statistics nor heuristics, but on a neces-
sary condition of the deep parsing. Consequently,
we accept to have more than one lexical tagging
for a sentence, as long as we can ensure to have
the good ones (when they exist!). This property is
particulary useful to ensure that the deep parsing
will not fail because of an error at the disambigua-
tion step.
In wide-coverage lexicalized grammars, a word
</bodyText>
<page confidence="0.958517">
242
</page>
<note confidence="0.8776785">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 242–253,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999914857142857">
typically has about 10 corresponding lexical de-
scriptions, which implies that for a short sentence
of 10 words, we get 1010 possible taggings. It is
not reasonable to treat them individually. To avoid
this, it is convenient to use an automaton to rep-
resent the set of all paths. This automaton has lin-
ear size with regard to the initial lexical ambiguity.
The idea of using automata is not new. In partic-
ular, methods based on Hidden Markov Models
(HMM) use such a technique for part-of-speech
tagging (Kupiec, 1992; Merialdo, 1994). Using
automata, we benefit from dynamic programming
procedures, and consequently from an exponential
temporal and space speed up.
</bodyText>
<sectionHeader confidence="0.98615" genericHeader="introduction">
2 Abstract Grammatical Framework
</sectionHeader>
<bodyText confidence="0.9668685">
Our filtering method is applicable to any lexical-
ized grammatical formalism which exhibits some
basic properties. In this section we establish these
properties and define from them the notion of Ab-
stract Grammatical Framework (AGF).
Formally, an Abstract Grammatical Frame-
work is an n-tuple (V, 8, 9, anc, T, p, dep)
where:
</bodyText>
<listItem confidence="0.950461333333333">
• V is the vocabulary: a finite set of words of
the modeled natural language;
• 8 is the set of syntactic structures used by
the formalism;
• 9 C 8 is the grammar: the finite set of initial
syntactic structures; a finite list [t1, ... , t&apos;] of
elements of 9 is called a lexical tagging;
• anc : 9 —* V maps initial syntactic struc-
tures to their anchors;
• T C 8 is the set of final syntactic structures
that the parsing process builds (for instance
trees);
• p is the parsing function from lexical tag-
gings to finite subsets of T;
• dep is the dependency function which maps
a couple composed of a lexical tagging and a
final syntactic structure to dependency struc-
tures.
</listItem>
<bodyText confidence="0.999823375">
Note that the anc function implies that the
grammar is lexicalized: each initial structure in 9
is associated to an element of V. Note also that no
particular property is required on the dependency
structures that are obtained with the dep function,
they can be non-projective for instance.
We call lexicon the function (written e) from V
to subsets of 9 defined by:
</bodyText>
<equation confidence="0.890287">
e(w) = {t E 9  |anc(t) = w}.
</equation>
<bodyText confidence="0.978801142857143">
We will say that a lexical tagging L =
[t1, ... , tr,,] is a lexical tagging of the sentence
[anc(t1), ... , anc(t,,,)].
The final structures in p (L) C T are called the
parsing solutions of L.
Henceforth, in our examples, we will consider
the ambiguous French sentence (1).
</bodyText>
<listItem confidence="0.94855025">
(1) “La belle ferme la porte”
Example 1 We consider the following toy AGF,
suited for parsing our sentence:
• V = { “la”, “belle”, “ferme”, “porte” };
• the grammar 9 is given in the table be-
low: each x corresponds to an element in
9, written with the category and the French
word as subscript. For instance, the French
word “porte” can be either a common noun
(“door”) or a transitive verb (“hangs”);
hence 9 contains the 2 elements CNporte and
TrVporte.
</listItem>
<table confidence="0.610183875">
la belle ferme porte
Det x
LAdj x x
RAdj x x
CN x x x x
Clit x
TrV x x
IntrV x
</table>
<bodyText confidence="0.998278833333333">
In our example, categories stand for, respec-
tively: determiner, left adjective, right adjec-
tive, common noun, clitic pronoun, transitive
verb and intransitive verb.
With respect to our lexicon, for sentence (1),
there are 3 x 3 x 5 x 3 x 2 = 270 lexical tag-
gings.
The parsing function p is such that 3 lexical
taggings have one solution and the 267 remaining
ones have no solution; we do not need to precise
the final structures, so we only give the English
translation as the result of the parsing function:
</bodyText>
<page confidence="0.995806">
243
</page>
<listItem confidence="0.992692833333333">
• P([Detla, CNbelle, TrVferme, Detla, CNporte]) =
{“The nice girl closes the door”}
• P([Detla, LAdjbelle, CNferme, Clitla, TrVporte]) =
{“The nice farm hangs it”}
• P([Detla, CNbelle, RAdjferme, Clitla, TrVporte]) =
{“The firm nice girl hangs it”}
</listItem>
<sectionHeader confidence="0.980257" genericHeader="method">
3 The Companionship Principle
</sectionHeader>
<bodyText confidence="0.999565333333333">
We have stated in the previous section the frame-
work and the definitions required to describe our
principle.
</bodyText>
<subsectionHeader confidence="0.997528">
3.1 Potential Companion
</subsectionHeader>
<bodyText confidence="0.999937055555555">
We say that u ∈ G is a companion of t ∈ G if
anc(t) and anc(u) are linked by a dependency
in dep(L, m) for some lexical tagging L which
contains t and u and some m ∈ p(L). The subset
of elements of G that are companions of t is called
the potential companion set of t.
The Companionship Principle says that if a lex-
ical tagging contains some t but no potential com-
panion of t, then it can be removed.
In what follows, we will generalize a bit this
idea in two ways. First, the same t can be implied
in more than one kind of dependency and hence it
can have several different companion sets with re-
spect to the different kinds of dependencies. Sec-
ondly, it can be the case that some companion t
has to be on the right (resp. on the left) to fulfill its
duty. These generalizations are done through the
notion of atomic constraints defined below.
</bodyText>
<subsectionHeader confidence="0.999664">
3.2 Atomic constraints
</subsectionHeader>
<bodyText confidence="0.997469">
We say that a pair (L, R) of subsets of G is an
atomic constraint for an initial structure t ∈ G
if for each lexical tagging L = [t1, ... , tn] such
that p(L) =6 ∅ and t = ti for some i then:
</bodyText>
<listItem confidence="0.9986825">
• either there is some j &lt; i such that tj ∈ L,
• or there is some j &gt; i such that tj ∈ R.
</listItem>
<bodyText confidence="0.9969621">
In other words, (L, R) lists the potential com-
panions of t, respectively on the left and on the
right.
A system of constraints for a grammar G is a
function C which associates a finite set of atomic
constraints to each element of G.
The Companionship Principle is an immedi-
ate consequence of the definition of atomic con-
straints. It can be stated as the necessary condi-
tion:
</bodyText>
<subsectionHeader confidence="0.830822">
The Companionship Principle
</subsectionHeader>
<bodyText confidence="0.929587333333333">
If a lexical tagging [t1, ... , tn] has a solution
then for all i and for all atomic constraints
(L, R) ∈ C(ti)
</bodyText>
<listItem confidence="0.9991495">
• {t1, . . . , ti−1} ∩ L =6 ∅
• or {ti+1,...,tn} ∩ R =6∅.
</listItem>
<bodyText confidence="0.988773285714286">
Example 2 Often, constraints can be expressed
independently of the anchors. In our example, we
use the category to refer to the subset of G of struc-
tures defined with this category: LAdj for instance
refers to the subset {LAdjbelle, LAdjferme}.
We complete the example of the previous section
with the following constraints&apos;:
</bodyText>
<figure confidence="0.983293222222222">
O t ∈ CN ⇒ (Det, ∅) ∈ C(t)
O t ∈ LAdj ⇒ (∅, CN) ∈ C(t)
© t ∈ RAdj ⇒ (CN, ∅) ∈ C(t)
O t ∈ Det ⇒ (∅, CN) ∈ C(t)
© t ∈ Det ⇒ (TrV, TrV ∪ IntrV) ∈ C(t)
© t ∈ TrV ⇒ (Clit, Det) ∈ C(t)
O t ∈ TrV ⇒ (Det, ∅) ∈ C(t)
9 t ∈ IntrV ⇒ (Det, ∅) ∈ C(t)
O t ∈ Clit ⇒ (∅, TrV) ∈ C(t)
</figure>
<bodyText confidence="0.8750448">
The two constraints O and ©for instance ex-
press that every determiner is implied in two de-
pendencies. First, it must find a common noun on
its right to build a noun phrase. Second, the noun
phrase has to be used in a verbal construction.
Now, let us consider the lexical tagging:
[Detla, LAdjbelle, TrVferme, Clitla, CNporte] and
the constraint O (a clitic is waiting for a transitive
verb on its right). This constraint is not fulfilled
by the tagging so this tagging has no solution.
</bodyText>
<subsectionHeader confidence="0.979997">
3.3 The “Companionship Principle”
language
</subsectionHeader>
<bodyText confidence="0.951633833333333">
Actually, a lexical tagging is an element of the
formal language G* and we can consider the fol-
lowing three languages. First, G* itself. Second,
the set C ⊆ G* corresponds to the lexical tag-
gings which can be parsed. The aim of lexical
disambiguation is then to exhibit for each sen-
tence [w1, ... , wn] all the lexical taggings that are
within C. Third, the Companionship Principle de-
fines the language P of lexical taggings which ver-
ify this Principle. P squeezes between the two lat-
&apos;These constraints are relative to our toy grammar and are
not linguistically valid in a larger context.
</bodyText>
<page confidence="0.9971">
244
</page>
<bodyText confidence="0.997613888888889">
ter sets C ⊆ P ⊆ G∗. Remarkably, the language
P can be described as a regular language. Since C
is presumably not a regular language (at least for
natural languages!), P is a better regular approxi-
mation than the trivial G∗.
Let us consider one lexical entry t and an atomic
constraint (L, R) ∈ C(t). Then, the set of lexical
taggings verifying this constraint can be described
as
</bodyText>
<equation confidence="0.992125">
Lt:(L,R) = C((CL)∗t(CR)∗)
</equation>
<bodyText confidence="0.933328">
where C denotes the complement of a set.
</bodyText>
<equation confidence="0.7061558">
Since P is defined as the lexical taggings verify-
ing all constraints, P is a regular language defined
by :
P = I I Lt:(L,R)
(L,R)∈C(t)
</equation>
<bodyText confidence="0.973183555555556">
From the Companionship Principle, we derive
a lexical disambiguation Principle which simply
tests tagging candidates with P. Notice that P can
be statically computed (at least, in theory) from
the grammar itself.
Example 3 For instance, for our example gram-
mar, this automaton is given in the figure 1 where
c=Clit, n=CN, d=Det, i=IntrV, l=LAdj, r=RAdj
and t=TrV.
A rough approximation of the size of the au-
tomaton corresponding to P can be easily com-
puted. Since each automaton Lt:(L,R) has 4 states,
P has at most 4m states where m is the num-
ber of atomic constraints. For instance, the gram-
mar used in the experiments contains more than
one atomic constraint for each lexical entry, and
m &gt; |G |&gt; 106. Computing P by brute-force is
then intractable.
</bodyText>
<sectionHeader confidence="0.663398" genericHeader="method">
4 Implementation of the Companionship
</sectionHeader>
<subsectionHeader confidence="0.861246">
Principle with automata
</subsectionHeader>
<bodyText confidence="0.999981833333333">
In this section we show how to use the Compan-
ionship Principle for disambiguation. Actually, we
propose two implementations based on the princi-
ple, an exact one and an approximate one. The
latter is really fast and can be used as a first step
before applying the first one.
</bodyText>
<subsectionHeader confidence="0.9609495">
4.1 Automaton to represent sets of lexical
taggings
</subsectionHeader>
<bodyText confidence="0.971064380952381">
The number of lexical taggings grows exponen-
tially with the length of sentences. To avoid that,
we represent sets of lexical taggings as the sets of
paths of some acyclic automata where transitions
are labeled by elements of G . We call such an
automaton a lexical taggings automaton (LTA).
Generally speaking, such automata save a lot of
space. For instance, given a sentence [w1, ... , wn]
the number of lexical taggings to consider at the
beginning of the parsing process is Π1≤i≤n|`(wi)|.
This set of taggings can be efficiently represented
as the set of paths of the automaton with n + 1
states s0, ... , sn and with a transition from si−1
to si with the label t for each t ∈ `(wi). This
automaton has E1≤i≤n |`(wi) |transitions.
Example 4 With the data of the previous exam-
ples, we have the initial automaton:
TrV
To improve readability, only the categories are
given on the edges, while the French words can be
inferred from the position in the automaton.
</bodyText>
<subsectionHeader confidence="0.992335">
4.2 Exact Companionship Principle (ECP)
</subsectionHeader>
<bodyText confidence="0.999899142857143">
Suppose we have a LTA A for a sentence
[w1, ... , wn]. For each transition t and for each
atomic constraint in (L, R) ∈ C(t), we construct
an automaton At,L,R in the following way.
Each state s of At,L,R is labeled with a triple
composed of a state of the automaton A and
two booleans. The intended meaning of the first
boolean is to say that each path reaching this
state passes through the transition t. The second
boolean means that the atomic constraint (L, R) is
necessarily fulfilled.
The initial state is labeled (s0, F, F) where s0 is
the initial state of A and other states are labeled as
follows: ifs u−→ s0 in A then, in At,L,R, we have:
</bodyText>
<listItem confidence="0.992812666666667">
1. (s, F, b) u−→ (s0, T, b) if u = t
2. (s, F, b) u−→ (s0, F, T) if u ∈ L
3. (s, F, b) u−→ (s0, F, b) if u ∈/ L
4. (s, T, b) u−→ (s0, T, T) if u ∈ R
5. (s, T, b) u−→ (s0, T, b) if u ∈/ R
where b ∈ {T, F}.
</listItem>
<figure confidence="0.99811303968254">
0 CN 1
Clit
Det
LAdj
RAdj
CN
IntrV
2 LAdj
RAdj
CN
3
CN
Clit
Det
4
CN
TrV
5
245
{c,i,l,n,r}
d
20
t
{c,i,l}n
{i,l,n,r,t}
c
{i,l,t}
c
19
t
23
n
d
21
d
c
{c,l}
d
{d,i,l,t}
c
d
{c,d,i,l}
c
4
c
c
18
c
t
c
14
t
l
{d,i,l}
n
c
{i,l,t}
n
l
0
6
d
{i,l}
{d,l}
d
22
{d,i,l,r,t}
n
{i,n,r,t}
t
15
t
n
i
d
13
5
{d,l}
{c,i,n,r} t
c
{i,n,r}
2
11
d
t
n {i,n,r}
n
t
c
c
{d,i,l,r}{d,l}
c
7
{c,d,i,l,r}
i
3
1
{d,l}
{i,l,n,r,t}
t
t
n
12
d
{d,l,r}
n
17
d
10
{d,l} {n,r} t
c
t
8
n
16
i
c
t
l
d
n
{i,l,r}
n
c
9
c
</figure>
<figureCaption confidence="0.999941">
Figure 1: The P language for G
</figureCaption>
<bodyText confidence="0.892357">
It is then routine to show that, for each state la-
beled (s, b1, b2):
</bodyText>
<listItem confidence="0.5686005">
• b1 is T iff all paths from the initial state to s
contain the transition t;
• b2 is T iff for all paths p reaching this state,
either there is some u E L or p goes through
t and there is some u E R. In other words,
the constraint is fulfilled.
</listItem>
<bodyText confidence="0.934966722222222">
In conclusion, a path ending with (sf, T, F) with
sf a final state of A is built with transitions 1, 3
and 5 only and hence contains t but no transition
able to fulfill the constraint. The final states are:
• (sf, F, b): each path ending here does not
contain the edge t and thus the constraint
does not apply here,
• (sf, T, T) each path ending here contains the
edge t but it contains also either a transition
2 or 4, so the constraint is fulfilled by these
paths.
The size of these automata is easily bounded by
4n where n is the size of A. Using a slightly more
intricated presentation, we built automata of size
2n.
Example 5 We give below the automaton A for
the atomic constraint ➑ (an intransitive verb is
waiting for a determiner on its left):
</bodyText>
<figure confidence="0.999523036363636">
LAdj
RAdj
CN
TrV
Det
CN
Clit
3,F,T
4,F,T
CN
TrV
5,F,T
LAdj
RAdj
CN
2,F,T
Det
CN 4,T,T
Clit
1,F,T
IntrV
CN
TrV
3,T,T
5,T,T
Det
0,F,F
CN
Clit
LAdj
RAdj
CN
LAdj
RAdj
CN
TrV
1,F,F
Det
CN
4,F,F
Clit
CN
TrV
5,F,F
3,F,F
2,F,F
IntrV
3,T,F
Det
CN
4,T,F
Clit
CN
TrV
5,T,F
</figure>
<bodyText confidence="0.636853666666667">
The dotted part of the graph corresponds to the
part of the automaton that can be safely removed.
After minimization, we finally obtain:
</bodyText>
<figure confidence="0.992671548387097">
LAdj
RAdj
CN
TrV
LAdj
RAdj
CN
2
1
Det
CN
Clit
Det
CN
TrV
IntrV
LAdj
4
3
5
0
CN
Clit
LAdj
RAdj
CN
1&apos;
RAdj
CN
TrV
2&apos;
</figure>
<bodyText confidence="0.997376875">
This automaton contains 234 paths (36 lexical
taggings are removed by this constraint).
For each transition t of the lexical taggings au-
tomaton and for each constraint (L, R) E C(t),
we construct the atomic constraint automaton
At,L,y,. The intersection of these automata rep-
resents all the possible lexical taggings of the sen-
tence which respect the Companionship Principle.
</bodyText>
<page confidence="0.95546">
246
</page>
<equation confidence="0.873732">
That is, we output:
�ACP = At,L,R
1&lt;i&lt;n, tEA;(L,R)EC(t)
</equation>
<bodyText confidence="0.999461333333333">
It can be shown that the automaton is the same
as the one obtained by intersection with the au-
tomaton of the language defined in 3.3:
</bodyText>
<figure confidence="0.503816">
ACP = A n P
</figure>
<figureCaption confidence="0.303986333333333">
Example 6 In our example, the intersection of
the 9 automata built for the atomic constraints is
given below:
</figureCaption>
<bodyText confidence="0.970755">
This automaton has 8 paths: there are 8 lexical
taggings which fulfill every constraint.
</bodyText>
<subsectionHeader confidence="0.935592">
4.3 Approximation: the Quick
Companionship Principle (QCP)
</subsectionHeader>
<bodyText confidence="0.922324404761905">
The issue with the previous algorithm is that it in-
volves a large number of automata (actually O(n))
where n is the size of the input sentence. Each
of these automata has size O(n). The theoreti-
cal complexity of the intersection is then O(nn).
Sometimes, we face the exponential. So, let
us provide an algorithm which approximates the
Principle. The idea is to consider at the same time
all the paths that contain some transition.
We consider a LTA A. We write --&lt;A the prece-
dence relation on transitions in an automaton A.
We define lA(t) = {u E G, u --&lt;A t} and rA(t) =
{u E G, t --&lt;A u}.
For each transition s t�) s&apos; and each constraint
(L, R) E C(t), if lA(t) n L = 0 and rA(t) n R =
0, then none of the lexical taggings which use the
transition t has a solution and the transition t can
be safely removed from the automaton.
This can be computed by a double-for loop: for
each atomic constraint of each transition, verify
that either the left context or the right context of
the transition contains some structure to solve the
constraint. Observe that the cost of this algorithm
is O(n2), where n is the size of the input automa-
ton.
Note that one must iterate this algorithm until a
fixpoint is reached. Indeed, removing a transition
which serves as a potential companion breaks the
verification. Nevertheless, since for each step be-
fore the fixpoint is reached, we remove at least one
transition, we iterate the double-for at most O(n)
times. The complexity of the whole algorithm is
then O(n3). In practice, we have observed that the
complexity is closer to O(n2): only 2 or 3 loops
are enough to reach the fixpoint.
Example 7 If we apply the QCP to the automaton
of Example 4, in the first step, only the transition
0 CN ) 1 is removed by applying the atomic con-
straint ➊. In the next step, the transition 1 RAdj ) 2
is removed by applying the atomic constraint ➌.
The fixpoint is reached and the output automaton
(with 120 paths) is:
</bodyText>
<equation confidence="0.50424775">
LAdj
Det
0 1
Clit
</equation>
<sectionHeader confidence="0.8529055" genericHeader="method">
5 The Generalized Companionship
Principle
</sectionHeader>
<bodyText confidence="0.999550363636364">
In practice, of course, we have to face the problem
of the computation of the constraints. In a large
coverage grammar, the size of G is too big to com-
pute all the constraints in advance. However, as
we have seen in example 2 we can identify sub-
sets of G that have the same constraints; the same
way, we can use these subsets to give a more con-
cise presentation of the L and R sets of the atomic
constraints. This motivates us to define a General-
ized Principle which is stated on a quotient set of
G.
</bodyText>
<subsectionHeader confidence="0.994435">
5.1 Generalized atomic constraints
</subsectionHeader>
<bodyText confidence="0.999691571428571">
Let U be a set of subsets of G that are a partition
of G. For t E G, we write t the subset of U which
contains t.
We say that a pair (L, R) of subsets of U is a
generalized atomic constraint for u E U if for
each lexical tagging L = [t1,... , tn] such that
p(L) =� 0 and u = ti for some i then:
</bodyText>
<listItem confidence="0.9978435">
• either there is some j &lt; i such that tj E L,
• or there is some j &gt; i such that tj E R.
</listItem>
<bodyText confidence="0.991545">
A system of generalized constraints for a par-
tition U of a grammar G is a function C which asso-
</bodyText>
<figure confidence="0.999012842105263">
IntrV
3c
CN
TrV
LAdj
2
CN 3b
Det
CN
CN
Det
0 1
TrV
5
CN
4&apos;
TrV
2
Clit
IntrV 3d 4
RAdj
Clit
3a
CN
LAdj
CN
RAdj
2 CN
TrV
IntrV
3
Det
CN
Clit
4
CN
TrV
5
</figure>
<page confidence="0.989438">
247
</page>
<bodyText confidence="0.9914465">
ciates a finite set of generalized atomic constraints
to each element of U.
</bodyText>
<subsectionHeader confidence="0.998885">
5.2 The Generalized Principle
</subsectionHeader>
<bodyText confidence="0.965302857142857">
The Generalized Companionship Principle is then
an immediate consequence of the previous defini-
tion and can be stated as the necessary condition:
The Generalized Companionship Principle
If a lexical tagging [t1, ... , tn] has a solution
then for all i and for all generalized atomic con-
straints (L, R) ∈ C(ti)
</bodyText>
<listItem confidence="0.879205">
•
{t1, ... , ti−1} ∩ L =6 ∅
• or {ti+1, ... , tn} ∩ R =6 ∅.
</listItem>
<bodyText confidence="0.3693655">
Example 8 The constraints given in example 2
are in fact generalized atomic constraints on the
set (recall that we write LAdj for the 2 elements
set {LAdjbelle, LAdjfernee}):
</bodyText>
<equation confidence="0.971608">
U = {Det, LAdj, RAdj, CN, Clit, TrV, IntrV}.
</equation>
<bodyText confidence="0.9930342">
Then the constraints are expressed on |U |= 7 el-
ements and not on |G |= 13.
A generalized atomic constraint on U can, of
course, be expressed as a set of atomic constraints
on G: let u ∈ U and t ∈ G such that t = u
</bodyText>
<equation confidence="0.672801">
�R ∈ C(t)
</equation>
<subsectionHeader confidence="0.997979">
5.3 Implementation of lexicalized grammars
</subsectionHeader>
<bodyText confidence="0.9999604">
In implementations of large coverage linguistic re-
sources, it is very common to have, first, the de-
scription of the set of “different” structures needed
to describe the modeled natural language and then
an anchoring mechanism that explains how words
of the lexicon are linked to these structures. We
call unanchored grammar the set U of differ-
ent structures (not yet related to words) that are
needed to describe the grammar. In this context,
the lexicon is split in two parts:
</bodyText>
<listItem confidence="0.7783492">
• a function ` from V to subsets of U,
• an anchoring function α which builds the
grammar elements from a word w ∈ V and
an unanchored structure u ∈ `(w); we sup-
pose that α verifies that anc(α(w, u)) = w.
</listItem>
<bodyText confidence="0.999513">
In applications, we suppose that U, ` and α are
given. In this context, we define the grammar as
the codomain of the anchoring function:
</bodyText>
<equation confidence="0.9990875">
G = U α(w,u)
wEV,uEt(w)
</equation>
<bodyText confidence="0.999835">
Now, we can define generalized constraints on
the unanchored grammar, which are independent
of the lexicon and can be computed statically for a
given unanchored grammar.
</bodyText>
<sectionHeader confidence="0.953394" genericHeader="method">
6 Application to Interaction Grammars
</sectionHeader>
<bodyText confidence="0.999953833333333">
In this section, we apply the Companionship Prin-
ciple to the Interaction Grammars formalism. We
first give a short and simplified description of IG
and an example to illustrate them at work; we re-
fer the reader to (Guillaume and Perrier, 2008) for
a complete and detailed presentation.
</bodyText>
<subsectionHeader confidence="0.98864">
6.1 Interaction Grammars
</subsectionHeader>
<bodyText confidence="0.987316">
We illustrate some of the important features on
the French sentence (2). In this sentence, “la”
is an object clitic pronoun which is placed before
the verb whereas the canonical place for the (non-
clitic) object is on the right of the verb.
(2) “Jean la demande.” [John asks for it]
The set F of final structures, used as output of
the parsing process, contains ordered trees called
parse trees (PT). An example of a PT for the sen-
tence (2) is given in Figure 2. A PT for a sentence
contains the words of the sentence or the empty
word E in its leaves (the left-right order of the tree
leaves follows the left-right order of words in the
input sentence). The internal nodes of a PT repre-
sent the constituents of the sentence. The morpho-
syntactic properties of these constituents are de-
scribed with feature structures (only the category
is shown in the figure).
As IG use the Model-Theoretic Syntax (MTS)
framework, a PT is defined as the model of a set
of constraints. Constraints are defined at the word
level: words are associated to a set of constraints
formally described as a polarized tree descrip-
tion (PTD). A PTD is a set of nodes provided with
relations between these nodes. The three PTDs
used to build the model above are given in Fig-
ure 3. The relations used in the PTDs are: imme-
diate dominance (lines) and immediate sisterhood
(arrows). Nodes represent syntactic constituents
</bodyText>
<figure confidence="0.4191015">
(L,R)∈C(u)=⇒ U UL,
LEL RER
</figure>
<page confidence="0.906032">
248
</page>
<figureCaption confidence="0.999905">
Figure 2: The PT of sentence (2)
</figureCaption>
<bodyText confidence="0.996192555555556">
and relations express structural dependencies be-
tween these constituents.
Moreover, nodes carry a polarity: the set of po-
larities is {+, −, =, ∼}. A + (resp.−) polarity
represents an available (resp. needed) resource, a
∼ polarity describes a node which is unsaturated.
Each + must be associated to exactly one − (and
vice versa) and each ∼ must be associated to at
least another polarity.
</bodyText>
<figureCaption confidence="0.989456">
Figure 3: PTDs for the sentence (2)
</figureCaption>
<bodyText confidence="0.999926">
Now, we define a PT to be a model of a set of
PTDs if there is a surjective function J from nodes
of the PTDs to nodes of the PT such that:
</bodyText>
<listItem confidence="0.998065363636363">
• relations in the PTDs are realized in the PT:
if M is a daughter (resp. immediate sister)
of N in some PTD then J(M) is a daughter
(resp. immediate sister) of J(N);
• each node N in the PT is saturated: the
composition of the polarities of the nodes in
J−1(N) with the associative and commuta-
tive rule given in Table 4 is =;
• the feature structure of a node N in the PT is
the unification of the feature structures of the
nodes in J−1(N).
</listItem>
<bodyText confidence="0.999714555555556">
One of the strong points of IG is the flexibility
given by the MTS approach: PTDs can be partially
superposed to produce the final tree (whereas su-
perposition is limited in usual CG or in TAG for
instance). In our example, the four grey nodes
in the PTD which contains “la” are superposed
to the four grey nodes in the PTD which contains
“demande” to produce the four grey nodes in the
model.
</bodyText>
<equation confidence="0.99873">
∼ − + =
∼ ∼ − + =
− − =
+ + =
= =
</equation>
<figureCaption confidence="0.996494">
Figure 4: Polarity composition
</figureCaption>
<bodyText confidence="0.998122333333333">
In order to give an idea of the full IG system,
we briefly give here the main differences between
our presentation and the full system.
</bodyText>
<listItem confidence="0.992207125">
• Dominance relations can be underspecified:
for instance a PTD can impose a node to be an
ancestor of another one without constraining
the length of the path in the model. This is
mainly used to model unbounded extraction.
• Sisterhood relations can also be underspeci-
fied: when the order on subconstituents is not
total, it can be modeled without using several
PTDs.
• Polarities are attached to features rather than
nodes: it sometimes gives more freedom
to the grammar writer when the same con-
stituent plays a role in different constructions.
• Feature values can be shared between several
nodes: once again, this is a way to factorize
the unanchored grammar.
</listItem>
<bodyText confidence="0.837078833333333">
The application of the Companionship Princi-
ple is described on the reduced IG but it can
be straightforwardly extended to full IG with
unessential technical details.
Following the notation given in 5.3, an IG is
made of:
</bodyText>
<listItem confidence="0.9998935">
• A finite set V of words;
• A finite set U of unanchored PTDs (without
any word attached to them);
• A lexicon function E from V to subsets of U.
</listItem>
<figure confidence="0.999195926829268">
C2-C3
=V
B1-B3
=NP
D2-D3
=NP
A2-A3
=S
Jean
E2
=Cl
la demande
F2-F3
=V
ε
C3
=V
D3
-NP
A3
=S
B3
-NP
F3
=V
demande
B1
+NP
A2
~S
Jean
C2
~V
D2
+NP
E2
=Cl
F2
~V
ε
la
</figure>
<page confidence="0.995573">
249
</page>
<bodyText confidence="0.999553285714286">
When t E e(w), we can construct the anchored
PTD α(w, u). Technically, in each unanchored
PTD u, a place is marked to be the anchor, i.e.
to be replaced by the word during the anchoring
process. Moreover, the anchoring process can also
be used to refine some features. The fact that
the feature can be refined gives more flexibility
and more compactness to the unanchored gram-
mar construction. In the French IG grammar, the
same unanchored PTD can be used for masculine
or feminine common nouns and the gender is spec-
ified during the anchoring to produce distinct an-
chored PTDs for masculine and feminine nouns. �
is defined by:
</bodyText>
<equation confidence="0.9989475">
� = � α(w,u)
wEV,uEt(w)
</equation>
<bodyText confidence="0.999415333333333">
The parsing solutions of a lexical tagging is the
set of PTs that are models of the list of PTDs de-
scribed by the lexical tagging:
</bodyText>
<equation confidence="0.889291">
p(L) = {m E T  |m is a model of L}
</equation>
<bodyText confidence="0.999074">
With the definitions of this section, an IG is a
special case of AGF as defined in section 2.
</bodyText>
<subsectionHeader confidence="0.99949">
6.2 Companionship Principle for IG
</subsectionHeader>
<bodyText confidence="0.999929896551724">
In order to apply the Companionship Principle, we
have to explain how the generalized atomic con-
straints are built for a given grammar. One way
is to look at dependency structures but in IG po-
larities are built in and then we can read the de-
pendency information we need directly on polari-
ties. A requirement to build a model is the satura-
tion of all the polarities. This requirement can be
expressed using atomic constraints. Each time a
PTD contains an unsaturated polarity +, — or —,
we have to find some other compatible dual po-
larity somewhere else in the grammar to saturate
it.
From the general MTS definition of IG above,
we can define a step by step process to build mod-
els of a lexical tagging. The idea is to build in-
crementally the interpretation function 9 with the
atomic operation of node merging. In this atomic
operation, we choose two nodes and make the hy-
pothesis that they have the same image through 9
and hence that they can be identified.
Now, suppose that the unanchored PTD u con-
tains some unsaturated polarity p. We can use the
atomic operation of node merging to test if the
unanchored PTD u&apos; can be used to saturate the po-
larity p. Let G (resp 92,) be the set of PTDs that
can be used on the left (resp. on the right) of u
to saturate p, then (G, 92,) is a generalized atomic
constraint in C(u).
</bodyText>
<sectionHeader confidence="0.940788" genericHeader="method">
7 Companionship Principle for other
formalisms
</sectionHeader>
<bodyText confidence="0.999983666666667">
As we said in the introduction, many current gram-
matical formalisms can more or less directly be
used to generate dependency structures and hence
are candidate for disambiguation with the Com-
panionship Principle. With IG, we have seen that
dependencies are strongly related to polarities: de-
pendency constraints in IG are built with the polar-
ity system.
We give below two short examples of polarity
use to define atomic constraints on TAG and on
CG. We use, as for IG, the polarity view of depen-
dencies to describe how the constraints are built.
</bodyText>
<subsectionHeader confidence="0.98758">
7.1 Tree Adjoining Grammars
</subsectionHeader>
<bodyText confidence="0.995644620689655">
Feature-based Tree Adjoining Grammars (here-
after FTAG) (Joshi, 1987) are a unification based
version of Tree Adjoining Grammars. An FTAG
consists of a set of elementary trees and of two
tree composition operations: substitution and ad-
junction. There are two kinds of trees: auxiliary
and initial. Substitution inserts a tree t with root
r onto a leaf node l of another tree t&apos; under the
condition that l is marked as a place for substitu-
tion and l and r have compatible feature structures.
Adjunction inserts an auxiliary tree t into a tree t&apos;
by splitting a node n of t&apos; under the condition that
the feature structures of the root and foot nodes of
t are compatible with the top and bottom ones of
n.
Getting the generalized atomic constraints and
the model building procedure for lexical tagging
is extremely similar to what was previously de-
scribed for IG if we extend the polarization pro-
cedure which was described in (Gardent and Kow,
2005) to do polarity based filtering in FTAG. The
idea is that for each initial tree t, its root of cate-
gory C is marked as having the polarity +C, and
its substitution nodes of category 5 are marked as
having the polarity —5. A first constraint set con-
tains trees t&apos; whose root is polarized +5 and such
that feature structures are unifiable. A second con-
straint set contains trees t&apos;&apos; which have a leaf that
is polarized —C. We can extend this procedure to
</bodyText>
<page confidence="0.978294">
250
</page>
<bodyText confidence="0.9998982">
auxiliary trees: each auxiliary tree t of category A
needs to be inserted in a node of category A of an-
other tree t&apos;. This gives us a constraint in the spirit
of the ∼ polarity in IG: C(t) contains all the trees
t&apos; in which t could be inserted2.
</bodyText>
<subsectionHeader confidence="0.992019">
7.2 Categorial Grammars
</subsectionHeader>
<bodyText confidence="0.9999407">
In their type system, Categorial Grammars en-
code linearity constraints and dependencies be-
tween constituents. For example, a transitive verb
is typed NP\5/NP, meaning that it waits for a
subject NP on its left and an object NP on its
right. This type can be straightforwardly decom-
posed as two −NP and one +5 polarities. Then
again, getting the generalized atomic constraints
is immediate and in the same spirit as what was
described for IG.
</bodyText>
<subsectionHeader confidence="0.686818">
7.3 Non-lexicalized formalisms
</subsectionHeader>
<bodyText confidence="0.999935444444444">
The lexicalization condition stated in section 2
excludes non-lexicalized formalisms like LFG or
HPSG. Nothing actually prevents our method
from being applied to these, but adding non-
lexicalized combinators requires to complexify the
formal account of the method. Adapting our
method to HPSG would result in a generaliza-
tion and unification of some of the techniques de-
scribed in (Kiefer et al., 1999).
</bodyText>
<sectionHeader confidence="0.996716" genericHeader="method">
8 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.892584">
8.1 Setup
</subsectionHeader>
<bodyText confidence="0.990895">
The experiments are performed using a French IG
grammar on a set of 31000 sentences taken from
the newspaper Le Monde.
The French grammar we consider (Perrier,
2007) contains |U |= 2088 unanchored trees.
It covers 88% of the grammatical sentences and
rejects 85% of the ungrammatical ones on the
TSNLP (Lehmann et al., 1996) corpus.
The constraints have been computed on the
unanchored grammar as explained in section 5:
each tree contains several polarities and therefore
several atomic constraints. Overall, the grammar
contains 20 627 atomic constraints. It takes 2 days
to compute the set of constraints and the results
can be stored in a constraints file of 10MB. Of
course, an atomic constraint is more interesting
when the sizes of Z and 9, are small. In our gram-
mar, 50% of the constraints set (either 9, or Z)
2Note that in the adjunction case, the constraint is not ori-
ented and then L= R.
contain at most 40 elements and 80% of these sets
contain at most 200 elements over 2 088.
We give in figure 5 the number of sentences of
each length in the corpus we consider.
</bodyText>
<figureCaption confidence="0.952604">
Figure 5: number of sentences of each length
</figureCaption>
<sectionHeader confidence="0.835239" genericHeader="evaluation">
8.2 Results
</sectionHeader>
<bodyText confidence="0.99885955">
Two preliminary comments need to be made on
the treatment of the results.
First, as we observed above, the number n
of lexical taggings is a priori exponential in the
length of the sentence. We thus consider its log.
Moreover, because we use a raw corpus, some
sentences are considered as ungrammatical by the
grammar; in this case it may happen that the dis-
ambiguation method removes all taggings. In or-
der to avoid undefined values when n = 0, we in
fact consider logio(1 + n).
Second, as expected, the ECP method is more
time consuming and for some sentences the time
and/or memory required is problematic. To be able
to apply the ECP to a large number of sentences,
we have used it after another filtering method
based on polarities and described in (Bonfante et
al., 2004).
Thus, for each sentence we have computed 3
different filters, each one finer than the previous:
</bodyText>
<listItem confidence="0.9691114">
• QCP the Quick Companionship Principle;
• QCP+POL QCP followed by a filtering tech-
nique based on polarity counting;
• QCP+POL+ECP the Exact Companionship
Principle applied to the previous filter.
</listItem>
<bodyText confidence="0.99213175">
Figure 6 displays the mean computation time
for each length: it confirms that the ECP is more
time consuming and goes up to 5s for our long sen-
tences.
</bodyText>
<figure confidence="0.989841142857143">
number of sentences
5000
4500
4000
3500
3000
2500
2000
1500
1000
500
0
6 7 8 9 10 11 12 13 14 15 16 17 18 19
sentence length (number of words)
</figure>
<page confidence="0.992795">
251
</page>
<bodyText confidence="0.999904">
Finally, we report the number of lexical tag-
gings that each method returns. Figure 7 displays
the mean value of logio(1 + n) where n is either
the initial number of lexical taggings or the num-
ber of lexical taggings left by the filter.
</bodyText>
<figureCaption confidence="0.965886">
Figure 6: mean execution time (in s)
Figure 7: number of taggings (initial and after the
3 disambiguation methods)
</figureCaption>
<bodyText confidence="0.9992093">
We can observe that the slope of the lines cor-
responds to the mean word ambiguity: if the
mean ambiguity is a then the number of taggings
for a sentence of length n is about an and then
log(an) = n · log(a). As a consequence, the mean
ambiguity can be read as 10s where s is the slope
in the last figure.
An illustration is given in figure 8 which ex-
hibits the mean word ambiguity for sentences of
length 16.
</bodyText>
<table confidence="0.5964065">
init QCP QCP+POL QCP+POL+ECP
6.13 3.41 1.93 1.41
</table>
<figureCaption confidence="0.925446">
Figure 8: mean word ambiguity for sentences of
length 16
</figureCaption>
<sectionHeader confidence="0.997012" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999995222222222">
We have presented a disambiguation method
based on dependency constraints which allows to
filter out many wrong lexical taggings before en-
tering the deep parsing. As this method relies on
the computation of static constraints on the lin-
guistic data and not on a statistical model, we can
be sure that we will never remove any correct lex-
ical tagging. Moreover, we manage to apply our
methods to an interesting set of data and prove that
it is efficient for a large coverage grammar and not
only for a toy grammar.
These results are also an encouragement to de-
velop further this kind of disambiguation methods.
In the near future, we would like to explore some
improvements.
First, we have seen that our principle cannot be
computed on the whole grammar and that in its im-
plementation we consider unanchored structures.
We would like to explore the possibility of com-
puting finer constraints (relative to the full gram-
mar) on the fly for each sentence. We believe that
this can eliminate some more taggings before en-
tering the deep parsing.
Concerning the ECP, as we have seen, there is a
kind of interplay between the efficiency of the fil-
tering and the time of the computation. We would
like to explore the possibility to define some in-
termediate way between QCP and ECP either by
using approximate automata or using the ECP but
only on a subset of elements where it is known to
be efficient.
Another challenging method we would like to
investigate is to use the Companionship Principle
not only as a disambiguation method but as a guide
for the deep parsing. Actually, we have observed
for at least 20% of the words that dependencies are
completely determined by the filtering methods. If
deep parsing can be adapted to use this observation
(this is the case for IG), this can be of great help.
Finally, we can improve the filtering using both
worlds: the Companionship Principle and the po-
larity counting method. Two different constraints
cannot be fulfilled by the same potential compan-
ion: this may allow to discover some more lexical
taggings that can be safely removed.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995095">
We would like to thank the anonymous reviewers
for their helpful comments and suggestions.
</bodyText>
<figure confidence="0.999548407407407">
�mes (in s)
0.01
0.1
10
1
6 7 8 9 10 11 12 13 14 15 16 17 18 19
sentence length (number of words)
QCP
QCP+POL
QCP+POL+ECP
Log (1+n)
18
16
14
12
10
8
6
4
2
0
6 7 8 9 10 11 12 13 14 15 16 17 18 19
sentence length (number of words)
QCP
QCP+POL
QCP+POL+ECP
Ini�al
</figure>
<page confidence="0.988443">
252
</page>
<sectionHeader confidence="0.997698" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999677655172414">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Comput.
Linguist., 25(2):237–265.
G. Bonfante, B. Guillaume, and G. Perrier. 2004.
Polarization and abstraction of grammatical for-
malisms as methods for lexical disambiguation. In
CoLing 2004, pages 303–309, Gen`eve, Switzerland.
P. Boullier. 2003. Supertagging : A non-statistical
parsing-based approach. In Pro- ceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 55–65, Nancy, France.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In COLING ’04: Proceedings of the 20th in-
ternational conference on Computational Linguis-
tics, page 282, Morristown, NJ, USA. Association
for Computational Linguistics.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building Deep Dependency Structures with a Wide-
Coverage CCG Parser. In Proceedings of ACL’02,
pages 327–334, Philadephia, PA.
Ph. de Groote. 2001. Towards abstract categorial
grammars. In Association for Computational Lin-
guistics, 39th Annual Meeting and 10th Conference
of the European Chapter, Proceedings of the Confer-
ence, pages 148–155.
C. Gardent and E. Kow. 2005. Generating and se-
lecting grammatical paraphrases. Proceedings of the
ENLG, Aug.
B. Guillaume and G. Perrier. 2008. Interaction Gram-
mars. Research Report RR-6621, INRIA.
A. Joshi and O. Rambow. 2003. A Formalism for De-
pendency Grammar Based on Tree Adjoining Gram-
mar. In Proceedings of the Conference on Meaning-
Text Theory.
A. Joshi. 1987. An Introduction to Tree Adjoining
Grammars. Mathematics of Language.
S. Kahane. 2006. Polarized unification grammar. In
Proceedings of Coling-ACL’02, Sydney.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A bag of useful techniques for
efficient and robust parsing. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 473–480, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Koller and M. Kuhlmann. 2009. Dependency
trees and the strong generative capacity of CCG. In
EACL’ 2009, Athens, Greece.
J. Kupiec. 1992. Robust Part-of-Speech Tagging Us-
ing a Hidden Markov Model. Computer Speech and
Language, 6(3):225–242.
F. Lamarche. 2008. Proof Nets for Intuitionistic Linear
Logic: Essential Nets. Technical report, INRIA.
J. Lambek. 1958. The mathematics of sentence struc-
ture. American mathematical monthly, pages 154–
170.
S. Lehmann, S. Oepen, S. Regnier-Prost, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Esti-
val, E. Dauphin, H. Compagnion, J. Baur, L. Balkan,
and D. Arnold. 1996. TSNLP: Test Suites for Nat-
ural Language Processing. In Proceedings of the
16th conference on Computational linguistics, pages
711–716.
J. Marchand, B. Guillaume, and G. Perrier. 2009.
Analyse en d´ependances a` l’aide des grammaires
d’interaction. In Actes de TALN 09, Senlis, France.
B. Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational linguistics, 20:155–
157.
M. Moortgat and G. Morrill. 1991. Heads and phrases.
Type calculus for dependency and constituent struc-
ture. In Journal of Language, Logic and Informa-
tion.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 155–163, Sydney, Australia, July.
Association for Computational Linguistics.
G. Perrier. 2007. A French Interaction Grammar. In
RANLP 2007, pages 463–467, Borovets Bulgarie.
M. Steedman. 2000. The Syntactic Process. MIT
Press.
L. Tesni`ere. 1959. ´El´ements de syntaxe structurale.
Klinksieck.
</reference>
<page confidence="0.99894">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657573">
<title confidence="0.999767">Dependency Constraints for Lexical Disambiguation</title>
<author confidence="0.8865455">Guillaume Bonfante Bruno Guillaume Mathieu Morey LORIA INPL LORIA INRIA LORIA Nancy-Universit´e</author>
<email confidence="0.90466">guillaume.bonfante@loria.frbruno.guillaume@loria.frmathieu.morey@loria.fr</email>
<abstract confidence="0.990797">We propose a generic method to perform lexical disambiguation in lexicalized grammatical formalisms. It relies on dependency constraints between words. The soundness of the method is due to invariant properties of the parsing in a given grammar that can be computed statically from the grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Comput. Linguist.,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3127" citStr="Bangalore and Joshi, 1999" startWordPosition="483" endWordPosition="486">urages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensu</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Comput. Linguist., 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bonfante</author>
<author>B Guillaume</author>
<author>G Perrier</author>
</authors>
<title>Polarization and abstraction of grammatical formalisms as methods for lexical disambiguation. In CoLing</title>
<date>2004</date>
<pages>303--309</pages>
<location>Gen`eve, Switzerland.</location>
<contexts>
<context position="2366" citStr="Bonfante et al., 2004" startWordPosition="358" endWordPosition="361">e recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polari</context>
<context position="34946" citStr="Bonfante et al., 2004" startWordPosition="6442" endWordPosition="6445">tial in the length of the sentence. We thus consider its log. Moreover, because we use a raw corpus, some sentences are considered as ungrammatical by the grammar; in this case it may happen that the disambiguation method removes all taggings. In order to avoid undefined values when n = 0, we in fact consider logio(1 + n). Second, as expected, the ECP method is more time consuming and for some sentences the time and/or memory required is problematic. To be able to apply the ECP to a large number of sentences, we have used it after another filtering method based on polarities and described in (Bonfante et al., 2004). Thus, for each sentence we have computed 3 different filters, each one finer than the previous: • QCP the Quick Companionship Principle; • QCP+POL QCP followed by a filtering technique based on polarity counting; • QCP+POL+ECP the Exact Companionship Principle applied to the previous filter. Figure 6 displays the mean computation time for each length: it confirms that the ECP is more time consuming and goes up to 5s for our long sentences. number of sentences 5000 4500 4000 3500 3000 2500 2000 1500 1000 500 0 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sentence length (number of words) 251 Finally</context>
</contexts>
<marker>Bonfante, Guillaume, Perrier, 2004</marker>
<rawString>G. Bonfante, B. Guillaume, and G. Perrier. 2004. Polarization and abstraction of grammatical formalisms as methods for lexical disambiguation. In CoLing 2004, pages 303–309, Gen`eve, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boullier</author>
</authors>
<title>Supertagging : A non-statistical parsing-based approach.</title>
<date>2003</date>
<booktitle>In Pro- ceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>55--65</pages>
<location>Nancy, France.</location>
<contexts>
<context position="3433" citStr="Boullier (2003)" startWordPosition="536" endWordPosition="537">We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensure that the deep parsing will not fail because of an error at the disambiguation step. In wide-coverage lexicalized grammars, a word 242 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 242–253, Paris, October 2009. c�2009 Association for Computational Linguistics typ</context>
</contexts>
<marker>Boullier, 2003</marker>
<rawString>P. Boullier. 2003. Supertagging : A non-statistical parsing-based approach. In Pro- ceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 55–65, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>282</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3225" citStr="Clark and Curran, 2004" startWordPosition="499" endWordPosition="502">e aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensure that the deep parsing will not fail because of an error at the disambiguation step. In wide-cov</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 282, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>Building Deep Dependency Structures with a WideCoverage CCG Parser.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>327--334</pages>
<location>Philadephia, PA.</location>
<contexts>
<context position="1479" citStr="Clark et al., 2002" startWordPosition="217" endWordPosition="220">es between words. The most straightforward examples are formalisms in the Dependency Grammars family but it is also true of the phrase structure based formalisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures.</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>S. Clark, J. Hockenmaier, and M. Steedman. 2002. Building Deep Dependency Structures with a WideCoverage CCG Parser. In Proceedings of ACL’02, pages 327–334, Philadephia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de Groote</author>
</authors>
<title>Towards abstract categorial grammars.</title>
<date>2001</date>
<booktitle>In Association for Computational Linguistics, 39th Annual Meeting and 10th Conference of the European Chapter, Proceedings of the Conference,</booktitle>
<pages>148--155</pages>
<marker>de Groote, 2001</marker>
<rawString>Ph. de Groote. 2001. Towards abstract categorial grammars. In Association for Computational Linguistics, 39th Annual Meeting and 10th Conference of the European Chapter, Proceedings of the Conference, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
<author>E Kow</author>
</authors>
<title>Generating and selecting grammatical paraphrases.</title>
<date>2005</date>
<booktitle>Proceedings of the ENLG,</booktitle>
<contexts>
<context position="2390" citStr="Gardent and Kow, 2005" startWordPosition="362" endWordPosition="366"> al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiti</context>
<context position="31399" citStr="Gardent and Kow, 2005" startWordPosition="5817" endWordPosition="5820">e t with root r onto a leaf node l of another tree t&apos; under the condition that l is marked as a place for substitution and l and r have compatible feature structures. Adjunction inserts an auxiliary tree t into a tree t&apos; by splitting a node n of t&apos; under the condition that the feature structures of the root and foot nodes of t are compatible with the top and bottom ones of n. Getting the generalized atomic constraints and the model building procedure for lexical tagging is extremely similar to what was previously described for IG if we extend the polarization procedure which was described in (Gardent and Kow, 2005) to do polarity based filtering in FTAG. The idea is that for each initial tree t, its root of category C is marked as having the polarity +C, and its substitution nodes of category 5 are marked as having the polarity —5. A first constraint set contains trees t&apos; whose root is polarized +5 and such that feature structures are unifiable. A second constraint set contains trees t&apos;&apos; which have a leaf that is polarized —C. We can extend this procedure to 250 auxiliary trees: each auxiliary tree t of category A needs to be inserted in a node of category A of another tree t&apos;. This gives us a constrain</context>
</contexts>
<marker>Gardent, Kow, 2005</marker>
<rawString>C. Gardent and E. Kow. 2005. Generating and selecting grammatical paraphrases. Proceedings of the ENLG, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Guillaume</author>
<author>G Perrier</author>
</authors>
<title>Interaction Grammars.</title>
<date>2008</date>
<tech>Research Report RR-6621, INRIA.</tech>
<contexts>
<context position="1930" citStr="Guillaume and Perrier, 2008" startWordPosition="287" endWordPosition="290">al Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many</context>
<context position="23243" citStr="Guillaume and Perrier, 2008" startWordPosition="4317" endWordPosition="4320">c(α(w, u)) = w. In applications, we suppose that U, ` and α are given. In this context, we define the grammar as the codomain of the anchoring function: G = U α(w,u) wEV,uEt(w) Now, we can define generalized constraints on the unanchored grammar, which are independent of the lexicon and can be computed statically for a given unanchored grammar. 6 Application to Interaction Grammars In this section, we apply the Companionship Principle to the Interaction Grammars formalism. We first give a short and simplified description of IG and an example to illustrate them at work; we refer the reader to (Guillaume and Perrier, 2008) for a complete and detailed presentation. 6.1 Interaction Grammars We illustrate some of the important features on the French sentence (2). In this sentence, “la” is an object clitic pronoun which is placed before the verb whereas the canonical place for the (nonclitic) object is on the right of the verb. (2) “Jean la demande.” [John asks for it] The set F of final structures, used as output of the parsing process, contains ordered trees called parse trees (PT). An example of a PT for the sentence (2) is given in Figure 2. A PT for a sentence contains the words of the sentence or the empty wo</context>
</contexts>
<marker>Guillaume, Perrier, 2008</marker>
<rawString>B. Guillaume and G. Perrier. 2008. Interaction Grammars. Research Report RR-6621, INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>O Rambow</author>
</authors>
<title>A Formalism for Dependency Grammar Based on Tree Adjoining Grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on MeaningText Theory.</booktitle>
<contexts>
<context position="1740" citStr="Joshi and Rambow, 2003" startWordPosition="257" endWordPosition="260"> other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2</context>
</contexts>
<marker>Joshi, Rambow, 2003</marker>
<rawString>A. Joshi and O. Rambow. 2003. A Formalism for Dependency Grammar Based on Tree Adjoining Grammar. In Proceedings of the Conference on MeaningText Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>An Introduction to Tree Adjoining Grammars. Mathematics of Language.</title>
<date>1987</date>
<contexts>
<context position="1627" citStr="Joshi, 1987" startWordPosition="242" endWordPosition="243">malisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the</context>
<context position="30522" citStr="Joshi, 1987" startWordPosition="5663" endWordPosition="5664">, many current grammatical formalisms can more or less directly be used to generate dependency structures and hence are candidate for disambiguation with the Companionship Principle. With IG, we have seen that dependencies are strongly related to polarities: dependency constraints in IG are built with the polarity system. We give below two short examples of polarity use to define atomic constraints on TAG and on CG. We use, as for IG, the polarity view of dependencies to describe how the constraints are built. 7.1 Tree Adjoining Grammars Feature-based Tree Adjoining Grammars (hereafter FTAG) (Joshi, 1987) are a unification based version of Tree Adjoining Grammars. An FTAG consists of a set of elementary trees and of two tree composition operations: substitution and adjunction. There are two kinds of trees: auxiliary and initial. Substitution inserts a tree t with root r onto a leaf node l of another tree t&apos; under the condition that l is marked as a place for substitution and l and r have compatible feature structures. Adjunction inserts an auxiliary tree t into a tree t&apos; by splitting a node n of t&apos; under the condition that the feature structures of the root and foot nodes of t are compatible w</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>A. Joshi. 1987. An Introduction to Tree Adjoining Grammars. Mathematics of Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
</authors>
<title>Polarized unification grammar.</title>
<date>2006</date>
<booktitle>In Proceedings of Coling-ACL’02,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="2343" citStr="Kahane, 2006" startWordPosition="356" endWordPosition="357">ow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be comput</context>
</contexts>
<marker>Kahane, 2006</marker>
<rawString>S. Kahane. 2006. Polarized unification grammar. In Proceedings of Coling-ACL’02, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Kiefer</author>
<author>Hans-Ulrich Krieger</author>
<author>John Carroll</author>
<author>Rob Malouf</author>
</authors>
<title>A bag of useful techniques for efficient and robust parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>473--480</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="33000" citStr="Kiefer et al., 1999" startWordPosition="6097" endWordPosition="6100"> be straightforwardly decomposed as two −NP and one +5 polarities. Then again, getting the generalized atomic constraints is immediate and in the same spirit as what was described for IG. 7.3 Non-lexicalized formalisms The lexicalization condition stated in section 2 excludes non-lexicalized formalisms like LFG or HPSG. Nothing actually prevents our method from being applied to these, but adding nonlexicalized combinators requires to complexify the formal account of the method. Adapting our method to HPSG would result in a generalization and unification of some of the techniques described in (Kiefer et al., 1999). 8 Experimental results 8.1 Setup The experiments are performed using a French IG grammar on a set of 31000 sentences taken from the newspaper Le Monde. The French grammar we consider (Perrier, 2007) contains |U |= 2088 unanchored trees. It covers 88% of the grammatical sentences and rejects 85% of the ungrammatical ones on the TSNLP (Lehmann et al., 1996) corpus. The constraints have been computed on the unanchored grammar as explained in section 5: each tree contains several polarities and therefore several atomic constraints. Overall, the grammar contains 20 627 atomic constraints. It take</context>
</contexts>
<marker>Kiefer, Krieger, Carroll, Malouf, 1999</marker>
<rawString>Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and Rob Malouf. 1999. A bag of useful techniques for efficient and robust parsing. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 473–480, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>M Kuhlmann</author>
</authors>
<title>Dependency trees and the strong generative capacity of CCG.</title>
<date>2009</date>
<booktitle>In EACL’ 2009,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1507" citStr="Koller and Kuhlmann, 2009" startWordPosition="221" endWordPosition="224">e most straightforward examples are formalisms in the Dependency Grammars family but it is also true of the phrase structure based formalisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities </context>
</contexts>
<marker>Koller, Kuhlmann, 2009</marker>
<rawString>A. Koller and M. Kuhlmann. 2009. Dependency trees and the strong generative capacity of CCG. In EACL’ 2009, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust Part-of-Speech Tagging Using a Hidden Markov Model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="4545" citStr="Kupiec, 1992" startWordPosition="721" endWordPosition="722">ies (IWPT), pages 242–253, Paris, October 2009. c�2009 Association for Computational Linguistics typically has about 10 corresponding lexical descriptions, which implies that for a short sentence of 10 words, we get 1010 possible taggings. It is not reasonable to treat them individually. To avoid this, it is convenient to use an automaton to represent the set of all paths. This automaton has linear size with regard to the initial lexical ambiguity. The idea of using automata is not new. In particular, methods based on Hidden Markov Models (HMM) use such a technique for part-of-speech tagging (Kupiec, 1992; Merialdo, 1994). Using automata, we benefit from dynamic programming procedures, and consequently from an exponential temporal and space speed up. 2 Abstract Grammatical Framework Our filtering method is applicable to any lexicalized grammatical formalism which exhibits some basic properties. In this section we establish these properties and define from them the notion of Abstract Grammatical Framework (AGF). Formally, an Abstract Grammatical Framework is an n-tuple (V, 8, 9, anc, T, p, dep) where: • V is the vocabulary: a finite set of words of the modeled natural language; • 8 is the set o</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec. 1992. Robust Part-of-Speech Tagging Using a Hidden Markov Model. Computer Speech and Language, 6(3):225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lamarche</author>
</authors>
<title>Proof Nets for Intuitionistic Linear Logic: Essential Nets.</title>
<date>2008</date>
<tech>Technical report, INRIA.</tech>
<contexts>
<context position="2319" citStr="Lamarche, 2008" startWordPosition="351" endWordPosition="352"> studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and Kow, 2005). On this regard, Marchand et al. (2009) exhibited a direct link between polarities and dependencies. This encourages us to say that in many respects dependencies and polarities are two sides of the same coin. The aim of this paper is to show that dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency c</context>
</contexts>
<marker>Lamarche, 2008</marker>
<rawString>F. Lamarche. 2008. Proof Nets for Intuitionistic Linear Logic: Essential Nets. Technical report, INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>The mathematics of sentence structure. American mathematical monthly,</title>
<date>1958</date>
<pages>154--170</pages>
<contexts>
<context position="1198" citStr="Lambek, 1958" startWordPosition="172" endWordPosition="173"> disambiguation based on the notion of dependencies. In modern linguistics, Lucien Tesni`ere developed a formal and sophisticated theory with dependencies (Tesni`ere, 1959). Nowadays, many current grammatical formalisms rely more or less explicitly on the notion of dependencies between words. The most straightforward examples are formalisms in the Dependency Grammars family but it is also true of the phrase structure based formalisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is </context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>J. Lambek. 1958. The mathematics of sentence structure. American mathematical monthly, pages 154– 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lehmann</author>
<author>S Oepen</author>
<author>S Regnier-Prost</author>
<author>K Netter</author>
<author>V Lux</author>
<author>J Klein</author>
<author>K Falkedal</author>
<author>F Fouvry</author>
<author>D Estival</author>
<author>E Dauphin</author>
<author>H Compagnion</author>
<author>J Baur</author>
<author>L Balkan</author>
<author>D Arnold</author>
</authors>
<title>TSNLP: Test Suites for Natural Language Processing.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>711--716</pages>
<contexts>
<context position="33359" citStr="Lehmann et al., 1996" startWordPosition="6157" endWordPosition="6160"> from being applied to these, but adding nonlexicalized combinators requires to complexify the formal account of the method. Adapting our method to HPSG would result in a generalization and unification of some of the techniques described in (Kiefer et al., 1999). 8 Experimental results 8.1 Setup The experiments are performed using a French IG grammar on a set of 31000 sentences taken from the newspaper Le Monde. The French grammar we consider (Perrier, 2007) contains |U |= 2088 unanchored trees. It covers 88% of the grammatical sentences and rejects 85% of the ungrammatical ones on the TSNLP (Lehmann et al., 1996) corpus. The constraints have been computed on the unanchored grammar as explained in section 5: each tree contains several polarities and therefore several atomic constraints. Overall, the grammar contains 20 627 atomic constraints. It takes 2 days to compute the set of constraints and the results can be stored in a constraints file of 10MB. Of course, an atomic constraint is more interesting when the sizes of Z and 9, are small. In our grammar, 50% of the constraints set (either 9, or Z) 2Note that in the adjunction case, the constraint is not oriented and then L= R. contain at most 40 eleme</context>
</contexts>
<marker>Lehmann, Oepen, Regnier-Prost, Netter, Lux, Klein, Falkedal, Fouvry, Estival, Dauphin, Compagnion, Baur, Balkan, Arnold, 1996</marker>
<rawString>S. Lehmann, S. Oepen, S. Regnier-Prost, K. Netter, V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival, E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and D. Arnold. 1996. TSNLP: Test Suites for Natural Language Processing. In Proceedings of the 16th conference on Computational linguistics, pages 711–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Marchand</author>
<author>B Guillaume</author>
<author>G Perrier</author>
</authors>
<title>Analyse en d´ependances a` l’aide des grammaires d’interaction.</title>
<date>2009</date>
<booktitle>In Actes de TALN 09,</booktitle>
<location>Senlis, France.</location>
<contexts>
<context position="1779" citStr="Marchand et al. (2009)" startWordPosition="264" endWordPosition="267">f Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introduce incomplete syntactic structures. IG directly use polarities to describe these structures but it is also possible to use polarities in other formalisms in order to make explicit the more or less implicit notion of incomplete structures: for instance, in CG (Lamarche, 2008) or in TAG (Kahane, 2006; Bonfante et al., 2004; Gardent and</context>
</contexts>
<marker>Marchand, Guillaume, Perrier, 2009</marker>
<rawString>J. Marchand, B. Guillaume, and G. Perrier. 2009. Analyse en d´ependances a` l’aide des grammaires d’interaction. In Actes de TALN 09, Senlis, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model. Computational linguistics,</title>
<date>1994</date>
<pages>20--155</pages>
<contexts>
<context position="4562" citStr="Merialdo, 1994" startWordPosition="723" endWordPosition="724">ges 242–253, Paris, October 2009. c�2009 Association for Computational Linguistics typically has about 10 corresponding lexical descriptions, which implies that for a short sentence of 10 words, we get 1010 possible taggings. It is not reasonable to treat them individually. To avoid this, it is convenient to use an automaton to represent the set of all paths. This automaton has linear size with regard to the initial lexical ambiguity. The idea of using automata is not new. In particular, methods based on Hidden Markov Models (HMM) use such a technique for part-of-speech tagging (Kupiec, 1992; Merialdo, 1994). Using automata, we benefit from dynamic programming procedures, and consequently from an exponential temporal and space speed up. 2 Abstract Grammatical Framework Our filtering method is applicable to any lexicalized grammatical formalism which exhibits some basic properties. In this section we establish these properties and define from them the notion of Abstract Grammatical Framework (AGF). Formally, an Abstract Grammatical Framework is an n-tuple (V, 8, 9, anc, T, p, dep) where: • V is the vocabulary: a finite set of words of the modeled natural language; • 8 is the set of syntactic struc</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English Text with a Probabilistic Model. Computational linguistics, 20:155– 157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
<author>G Morrill</author>
</authors>
<title>Heads and phrases. Type calculus for dependency and constituent structure.</title>
<date>1991</date>
<journal>In Journal of Language, Logic and Information.</journal>
<contexts>
<context position="1444" citStr="Moortgat and Morrill, 1991" startWordPosition="209" endWordPosition="212"> less explicitly on the notion of dependencies between words. The most straightforward examples are formalisms in the Dependency Grammars family but it is also true of the phrase structure based formalisms which consider that words introduce incomplete syntactic structures which must be completed by other words. This idea is at the core of Categorial Grammars (CG) (Lambek, 1958) and all its trends such as Abstract Categorial Grammars (ACG) (de Groote, 2001) or Combinatory Categorial Grammars (CCG) (Steedman, 2000), being mostly encoded in their type system. Dependencies in CG were studied in (Moortgat and Morrill, 1991) and for CCG in (Clark et al., 2002; Koller and Kuhlmann, 2009). Other formalisms can be viewed as modeling and using dependencies, such as Tree Adjoining Grammars (TAG) (Joshi, 1987) with their substitution and adjunction operations. Dependencies for TAG were studied in (Joshi and Rambow, 2003). More recently, Marchand et al. (2009) showed that it is also possible to extract a dependency structure from a syntactic analysis in Interaction Grammars (IG) (Guillaume and Perrier, 2008). Another much more recent concept of polarity can be used in grammatical formalisms to express that words introdu</context>
</contexts>
<marker>Moortgat, Morrill, 1991</marker>
<rawString>M. Moortgat and G. Morrill. 1991. Heads and phrases. Type calculus for dependency and constituent structure. In Journal of Language, Logic and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>155--163</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3258" citStr="Ninomiya et al., 2006" startWordPosition="505" endWordPosition="508">t dependencies can be used to express constraints on the taggings of a sentence and hence these dependency constraints can be used to partially disambiguate the words of a sentence. We will see that, in practice, using the link between dependencies and polarities, these dependency constraints can be computed directly from polarized structures. Exploiting the dependencies encoded in lexical entries to perform disambiguation is the intuition behind supertagging (Bangalore and Joshi, 1999), a method introduced for LTAG and successfully applied since then to CCG (Clark and Curran, 2004) and HPSG (Ninomiya et al., 2006). These approaches select the most likely lexical entry (entries) for each word, based on Hidden Markov Models or Maximum Entropy Models. Like the work done by Boullier (2003), our method is not based on statistics nor heuristics, but on a necessary condition of the deep parsing. Consequently, we accept to have more than one lexical tagging for a sentence, as long as we can ensure to have the good ones (when they exist!). This property is particulary useful to ensure that the deep parsing will not fail because of an error at the disambiguation step. In wide-coverage lexicalized grammars, a wor</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 155–163, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Perrier</author>
</authors>
<title>A French Interaction Grammar. In RANLP</title>
<date>2007</date>
<pages>463--467</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="33200" citStr="Perrier, 2007" startWordPosition="6132" endWordPosition="6133">d formalisms The lexicalization condition stated in section 2 excludes non-lexicalized formalisms like LFG or HPSG. Nothing actually prevents our method from being applied to these, but adding nonlexicalized combinators requires to complexify the formal account of the method. Adapting our method to HPSG would result in a generalization and unification of some of the techniques described in (Kiefer et al., 1999). 8 Experimental results 8.1 Setup The experiments are performed using a French IG grammar on a set of 31000 sentences taken from the newspaper Le Monde. The French grammar we consider (Perrier, 2007) contains |U |= 2088 unanchored trees. It covers 88% of the grammatical sentences and rejects 85% of the ungrammatical ones on the TSNLP (Lehmann et al., 1996) corpus. The constraints have been computed on the unanchored grammar as explained in section 5: each tree contains several polarities and therefore several atomic constraints. Overall, the grammar contains 20 627 atomic constraints. It takes 2 days to compute the set of constraints and the results can be stored in a constraints file of 10MB. Of course, an atomic constraint is more interesting when the sizes of Z and 9, are small. In our</context>
</contexts>
<marker>Perrier, 2007</marker>
<rawString>G. Perrier. 2007. A French Interaction Grammar. In RANLP 2007, pages 463–467, Borovets Bulgarie. M. Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<publisher>Klinksieck.</publisher>
<marker>Tesni`ere, 1959</marker>
<rawString>L. Tesni`ere. 1959. ´El´ements de syntaxe structurale. Klinksieck.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>