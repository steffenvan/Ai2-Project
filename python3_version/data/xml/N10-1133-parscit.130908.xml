<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.983021">
Quantifying the Limits and Success of Extractive Summarization Systems
Across Domains
</title>
<figure confidence="0.4545364">
Umut ¨Ozertem
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
umut@yahoo-inc.com
</figure>
<author confidence="0.966275">
Hakan Ceylan and Rada Mihalcea
</author>
<affiliation confidence="0.9993315">
Department of Computer Science
University of North Texas
</affiliation>
<address confidence="0.939356">
Denton, TX 76203
</address>
<email confidence="0.9998">
{hakan,rada}@unt.edu
</email>
<sectionHeader confidence="0.996675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999655111111111">
This paper analyzes the topic identification
stage of single-document automatic text sum-
marization across four different domains, con-
sisting of newswire, literary, scientific and le-
gal documents. We present a study that ex-
plores the summary space of each domain
via an exhaustive search strategy, and finds
the probability density function (pdf) of the
ROUGE score distributions for each domain.
We then use this pdf to calculate the per-
centile rank of extractive summarization sys-
tems. Our results introduce a new way to
judge the success of automatic summarization
systems and bring quantified explanations to
questions such as why it was so hard for the
systems to date to have a statistically signifi-
cant improvement over the lead baseline in the
news domain.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999561642857143">
Topic identification is the first stage of the gener-
ally accepted three-phase model in automatic text
summarization, in which the goal is to identify the
most important units in a document, i.e., phrases,
sentences, or paragraphs (Hovy and Lin, 1999; Lin,
1999; Sparck-Jones, 1999). This stage is followed
by the topic interpretation and summary generation
steps where the identified units are further processed
to bring the summary into a coherent, human read-
able abstract form. The extractive summarization
systems, however, only employ the topic identifi-
cation stage, and simply output a ranked list of the
units according to a compression ratio criterion. In
general, for most systems sentences are the preferred
</bodyText>
<author confidence="0.722551">
Elena Lloret and Manuel Palomar
</author>
<affiliation confidence="0.968542666666667">
Department of
Software and Computing Systems
University of Alicante
</affiliation>
<address confidence="0.913335">
San Vicente del Raspeig
Alicante 03690, Spain
</address>
<email confidence="0.996411">
{elloret,mpalomar}@dlsi.ua.es
</email>
<bodyText confidence="0.999901735294118">
units in this stage, as they are the smallest grammat-
ical units that can express a statement.
Since the sentences in a document are reproduced
verbatim in extractive summaries, it is theoretically
possible to explore the search space of this problem
through an enumeration of all possible extracts for
a document. Such an exploration would not only
allow us to see how far we can go with extractive
summarization, but we would also be able to judge
the difficulty of the problem by looking at the dis-
tribution of the evaluation scores for the generated
extracts. Moreover, the high scoring extracts could
also be used to train a machine learning algorithm.
However, such an enumeration strategy has an
exponential complexity as it requires all possible
sentence combinations of a document to be gener-
ated, constrained by a given word or sentence length.
Thus the problem quickly becomes impractical as
the number of sentences in a document increases and
the compression ratio decreases. In this work, we try
to overcome this bottleneck by using a large cluster
of computers, and decomposing the task into smaller
problems by using the given section boundaries or a
linear text segmentation method. As a result of this
exploration, we generate a probability density func-
tion (pdf) of the ROUGE score (Lin, 2004) distri-
butions for four different domains, which shows the
distribution of the evaluation scores for the gener-
ated extracts, and allows us to assess the difficulty
of each domain for extractive summarization.
Furthermore, using these pdfs, we introduce a
new success measure for extractive summarization
systems. Namely, given a system’s average score
over a data set, we show how to calculate the per-
</bodyText>
<page confidence="0.981215">
903
</page>
<note confidence="0.752892">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903–911,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995484">
centile rank of this system from the corresponding
pdf of the data set. This allows us to see the true
improvement a system achieves over another, such
as a baseline, and provides a standardized scoring
scheme for systems performing on the same data set.
</bodyText>
<sectionHeader confidence="0.999325" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971138888889">
Despite the large amount of work in automatic
text summarization, there are only a few studies
in the literature that employ an exhaustive search
strategy to create extracts, which is mainly due to
the prohibitively large search space of the prob-
lem. Furthermore, the research regarding the align-
ment of abstracts to original documents has shown
great variations across domains (Kupiec et al., 1995;
Teufel and Moens, 1997; Marcu, 1999; Jing, 2002;
Ceylan and Mihalcea, 2009), which indicates that
the extractive summarization techniques are not ap-
plicable to all domains at the same level.
In order to automate the process of corpus
construction for automatic summarization systems,
(Marcu, 1999) used exhaustive search to generate
the best Extract from a given (Abstract, Text) tuple,
where the best Extract contains a set of clauses from
Text that have the highest similarity to the given Ab-
stract.
In addition, (Donaway et al., 2000) used exhaus-
tive search to create all the sentence extracts of
length three starting with 15 TREC Documents, in
order to judge the performance of several summary
evaluation measures suggested in their paper.
Finally, the study most similar to ours was done
by (Lin and Hovy, 2003), who used the articles with
less than 30 sentences from the DUC 2001 data set
to find oracle extracts of 100 and 150 (±5) words.
These extracts were compared against one summary
source, selected as the one that gave the highest
inter-human agreement. Although it was concluded
that a 10% improvement was possible for extrac-
tive summarization systems, which typically score
around the lead baseline, there was no report on how
difficult it would be to achieve this improvement,
which is the main objective of our paper.
</bodyText>
<sectionHeader confidence="0.660596" genericHeader="method">
3 Description of the Data Set
</sectionHeader>
<bodyText confidence="0.9901665">
Our data set is composed of four different domains:
newswire, literary, scientific and legal. For all the
</bodyText>
<table confidence="0.9968504">
Domain µDw µSw µR µC µCw
Newswire 641 101 84% 1 641
Literary 4973 1148 77% 6 196
Scientific 1989 160 92% 9 221
Legal 3469 865 75% 18 192
</table>
<tableCaption confidence="0.9958525">
Table 1: Statistical properties of the data set. µD71, and
µS71 represent the average number of words for each doc-
ument and summary respectively; µR indicates the av-
erage compression ratio; and µC and µC71 represent the
average number of sections for each document, and the
average number of words for each section respectively.
</tableCaption>
<bodyText confidence="0.985000875">
domains we used 50 documents and only one sum-
mary for each document, except for newswire where
we used two summaries per document. For the
newswire domain, we selected the articles and their
summaries from the DUC 2002 data set,1. For the
literary domain, we obtained 10 novels that are lit-
erature classics, and available online in text format.
Further, we collected the corresponding summaries
for these novels from various websites such as
CliffsNotes (www.cliffsnotes.com) and SparkNotes
(www.sparknotes.com), which make available hu-
man generated abstracts for literary works. These
sources give a summary for each chapter of the
novel, so each chapter can be treated as a sepa-
rate document. Thus we evaluate 50 chapters in to-
tal. For the scientific domain, we selected the ar-
ticles from the medical journal Autoimmunity Re-
views2 were selected, and their abstracts are used
as summaries. Finally, for the legal domain, we
gathered 50 law documents and their corresponding
summaries from the European Legislation Website,3
which comprises four types of laws - Council Di-
rectives, Acts, Communications, and Decisions over
several topics, such as society, environment, educa-
tion, economics and employment.
Although all the summaries are human generated
abstracts for all the domains, it is worth mention-
ing that the documents and their corresponding sum-
maries exhibit a specific writing style for each do-
main, in terms of the vocabulary used and the length
of the sentences. We list some of the statistical prop-
erties of each domain in Table 1.
</bodyText>
<footnote confidence="0.999981">
1http://www-nlpir.nist.gov/projects/duc/data.html
2http://www.elsevier.com/wps/product/cws home/622356
3http://eur-lex.europa.eu/en/legis/index.htm
</footnote>
<page confidence="0.998192">
904
</page>
<sectionHeader confidence="0.998412" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.998528333333333">
As mentioned in Section 1, an exhaustive search
algorithm requires generating all possible sentence
combinations from a document, and evaluating each
one individually. For example, using the values from
Table 1, and assuming 20 words per sentence, we
find that the search space for the news domain con-
</bodyText>
<equation confidence="0.52263025">
52) × 50 = 10, 068, 800 sum-
tains approximately (maries. The same calculation method for the sci-
entific domain gives us (99) × 50 = 8.56 × 1012
8
</equation>
<bodyText confidence="0.999919294117647">
summaries. Obviously the search space gets much
bigger for the legal and literary domains due to their
larger text size.
In order to be able to cope with such a huge
search space, the first thing we did was to modify
the ROUGE 1.5.54 Perl script by fixing the parame-
ters to those used in the DUC experiments,5 and also
by modifying the way it handles the input and output
to make it suitable for streaming on the cluster.
The resulting script evaluates around 25-30 sum-
maries per second on an Intel 2.33 GHz processor.
Next, we streamed the resulting ROUGE script for
each (document, summary) pair on a large cluster
of computers running on an Hadoop Map-Reduce
framework.6 Based on the size of the search space
for a (document, summary) pair, the number of com-
puters allocated in the cluster ranged from just a few
to more than one thousand.
Although the combination of a large cluster and a
faster ROUGE is enough to handle most of the doc-
uments in the news domain in just a few hours, a
simple calculation shows that the problem is still im-
practical for the other domains. Hence for the scien-
tific, legal, and literary domains, rather than consid-
ering each document as a whole, we divide them into
sections, and create extracts for each section such
that the length of the extract is proportional to the
length of the section in the original document. For
the legal and scientific domains, we use the given
section boundaries (without considering the subsec-
tions for scientific documents). For the novels, we
treat each chapter as a single document (since each
chapter has its own summary), which is further di-
vided into sections using a publicly available linear
</bodyText>
<footnote confidence="0.999855333333333">
4http://berouge.com
5-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0
6http://hadoop.apache.org/
</footnote>
<bodyText confidence="0.999835413793103">
text segmentation algorithm by (Utiyama and Isa-
hara, 2001).7 In all cases, we let the algorithm pick
the number of segments automatically.
To evaluate the sections, we modified ROUGE
further so that it applies the length constraint to the
extracts only, not to the model summaries. This is
due to the fact that we evaluate the extracts of each
section individually against the whole model sum-
mary, which is larger than the extract. This way,
we can get an overall ROUGE recall score for a
document extract, simply by summing up the re-
call scores of each section extracts. The precision
score for the entire document can also be found by
adding the weighted precision scores for each sec-
tion, where the weight is proportional to the length
of the section in the original document. In our study,
however, we only use recall scores.
Note that, since for the legal, scientific, and lit-
erary domains we consider each section of a doc-
ument independently, we are not performing a true
exhaustive search for these domains, but rather solv-
ing a suboptimal problem, as we divide the number
of words in the model summary to each section pro-
portional to the section’s length. However, we be-
lieve that this is a fair assumption, as it has been
shown repeatedly in the past that text segmentation
helps improving the performance of text summariza-
tion systems (yen Kan et al., 1998; Nakao, 2000;
Mihalcea and Ceylan, 2007).
</bodyText>
<sectionHeader confidence="0.995462" genericHeader="method">
5 Exhaustive Search Algorithm
</sectionHeader>
<bodyText confidence="0.998897166666667">
Let Eik = Si1, Si2, ..., Sik be the ith extract that
has k sentences, and generated from a document
D with n sentences D = S1, S2,.. . , Sn. Further,
let len(Sj) give the number of words in sentence
Sj. We enforce that Eik satisfies the following con-
straints:
</bodyText>
<equation confidence="0.9999935">
len(Eik) = len(Si1) + ... + len(Sik) ≥ L
len(Eik−1) = len(Si1) + ... + len(Sik−1) &lt; L
</equation>
<bodyText confidence="0.98333075">
where L is the length constraint on all the extracts
of document D. We note that for any Eik, the or-
der of the sentences in Eik−1 does not affect the
ROUGE scores, since only the last sentence may be
</bodyText>
<footnote confidence="0.9989245">
7http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-
1.211.tar.gz
</footnote>
<page confidence="0.997403">
905
</page>
<bodyText confidence="0.999977153846154">
chopped off due to the length constraint.8 Hence, we
start generating sentence combinations (r) in lexico-
graphic order, for r = 1...n, and for each combina-
tion Eik = Si1, Si2, ..., Sik where k &gt; 1, we gener-
ate additional extracts E′ik by successfully swapping
Si, with Sik for j = 1, ..., k − 1 and checking to see
if the above constraints are still satisfied. Therefore
from a combination with k sentences that satisfies
the constraints, we might generate up to k − 1 ad-
ditional extracts. Finally, we stop the process either
when r = n and the last combination is generated,
or we cannot find any extract that satisfies the con-
straints for r.
</bodyText>
<sectionHeader confidence="0.977848" genericHeader="method">
6 Generating pdfs
</sectionHeader>
<bodyText confidence="0.999925">
Once the extracts for a document are generated and
evaluated, we go through each result and assign its
recall score to a range, which we refer to as a bin.
We use 1, 000 equally spaced bins between 0 and
1. As an example, a recall score of 0.46873 would
be assigned to the bin [0.468, 0.469]. By keeping
a count for each bin, we are in fact building a his-
togram of scores for the document. Let this his-
togram be h, and h[j] be the value in the jth bin of
the histogram. We then define the normalized his-
togram h as:
</bodyText>
<equation confidence="0.998739666666667">
h[j] = N
N h[j] (1)
�i=1 h[j]
</equation>
<bodyText confidence="0.9477337">
where N = 1, 000 is the number of bins in the his-
togram. Note that since the width of each bin is 1N ,
the Riemann sum of the normalized histogram h is
�
equal to 1, so h can be used as an approximation
to the underlying pdf. As an example, we show the
�
histogram h for the newswire document AP890323-
0218 in Figure 1.
We combine the normalized histograms of all the
documents in a domain in order to find the pdf for
that domain. This requires multiplying the value
of each bin in a document’s histogram, with all
the other possible combinations of bin values taken
from each of the remaining histograms, and assign-
ing the result to the average bin for each combina-
8Note that we do not take the coherence of extracts into ac-
count, i.e. the sentences in an extract do not need to be sorted
in order of their appearance in the original document. We also
do not change the position of the words in a sentence.
</bodyText>
<figure confidence="0.817903">
0 100 200 300 400 500 600 700 800 900 1000
</figure>
<figureCaption confidence="0.986461">
Figure 1: The normalized histogram h of ROUGE-1 re-
call scores for the newswire document AP890323-0218.
</figureCaption>
<bodyText confidence="0.780993142857143">
tion. This can be done iteratively by keeping a mov-
ing average. We illustrate this procedure in Algo-
rithm 1, where K represents the number of docu-
ments in a domain.
�
Algorithm 1 Combine hi’s for i = 1, ... , K to cre-
ate hd, the histogram for domain d.
</bodyText>
<listItem confidence="0.9466824">
1: hd := {}
2: for i = 1 to N do
3: hd[i] := �h1[i]
4: end for
5: for i = 2 to K do
6: ht = {}
7: for j = 1 to N do
8: fork = 1 to N do
10: ht[a] = ht[a] + (hd[k] *
9: a = round(((k * (i − 1)) + j)/i)
�hi[j])
11: end for
12: end for
13: hd := ht
14: end for
</listItem>
<bodyText confidence="0.999773909090909">
The resulting histogram hd, when normalized us-
ing Equation 1, is an approximation to the pdf for
domain d. Furthermore, we used the round() func-
tion in line 9, which rounds a number to the nearest
integer, as the bins are indexed by integers. Note
that this rounding introduces an error, which is dis-
tributed uniformly due to the nature of the round()
function. It is also possible to lower the affect of this
error with higher resolutions (i.e. larger number of
bins). In Figure 2, we show a sample hd, obtained
by combining 10 documents from the newswire do-
</bodyText>
<figure confidence="0.979699714285714">
&amp;quot;AP890323-0218.dat&amp;quot;
50
45
40
35
30
25
20
15
10
5
0
906
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</figure>
<figureCaption confidence="0.999636333333333">
Figure 2: An example pdf obtained by combining 10 doc-
ument histograms of ROUGE-1 recall scores from the
newswire domain. The x-axis is normalized to [0,1].
</figureCaption>
<bodyText confidence="0.991637">
main.
Recall from Section 4 that the documents in
the literary, legal, and scientific domains are di-
vided into sections either by using the given section
boundaries or by applying a text segmentation al-
gorithm, and the extracts of each section are then
evaluated individually. Hence for these domains, we
first calculate the histogram of each section individ-
ually, and then combine them to find the histogram
of a document. The combination procedure for the
section histograms is similar to Algorithm 1, except
that in this case we do not keep a moving average,
but rather sum up the bins of the sections. Note
that when bin i and j are added, the resulting val-
ues should be expected to be half the times in bin
i + j, and half the times in i + j − 1.
</bodyText>
<sectionHeader confidence="0.945593" genericHeader="method">
7 Calculating Percentile Ranks
</sectionHeader>
<bodyText confidence="0.999208142857143">
Given a pdf for a domain, the success of a system
having a ROUGE recall score of S could be sim-
ply measured by finding the area bounded by S.
This gives us the percentile rank of the system in
the overall distribution. Assuming 0 &lt; S &lt; 1, let
S� = LN x S], then the formula to calculate the per-
centile rank can be simply given as:
</bodyText>
<equation confidence="0.723825">
�hd[i] (2)
</equation>
<table confidence="0.995891277777778">
ROUGE-1
Domain µ Q max min
Newswire 39.39 0.87 65.70 20.20
Literary 45.20 0.47 63.90 28.40
Scientific 45.99 0.68 71.90 24.20
Legal 72.82 0.28 82.40 62.80
ROUGE-2
Domain µ Q max min
Newswire 11.57 0.79 37.40 1.60
Literary 5.41 0.34 16.90 1.80
Scientific 10.98 0.60 33.30 1.30
Legal 28.74 0.29 40.90 19.60
ROUGE-SU4
Domain µ Q max min
Newswire 15.33 0.69 38.10 6.40
Literary 13.28 0.30 24.30 6.90
Scientific 16.13 0.50 35.80 6.20
Legal 35.63 0.25 45.70 28.70
</table>
<tableCaption confidence="0.994058">
Table 2: Statistical properties of the pdfs
</tableCaption>
<sectionHeader confidence="0.999404" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.99978775">
The ensemble distributions of ROUGE-1 recall
scores per document are shown in Figure 3. The
ensemble distributions tell us that the performance
of the extracts, especially for the news and the sci-
entific domains, are mostly uniform for each docu-
ment. This is due to the fact that documents in these
domains, and their corresponding summaries, are
written with a certain conventional style. There is
however a little scattering in the distributions of the
literary and the legal domains. This is an expected
result for the literary domain, as there is no specific
summarization style for these documents, but some-
how surprising for the legal domain, where the effect
is probably due to the different types of legal docu-
ments in the data set.
The pdf plots resulting from the ROUGE-1 recall
scores are shown in Figure 4.9 In order to analyze
the pdf plots, and better understand their differences,
Table 2 lists the mean (µ) and the standard deviation
(Q) measures of the pdfs, as well as the average min-
imum and maximum scores that an extractive sum-
marization system can get for each domain.
By looking at the pdf plots and the minimum and
maximum columns from Table 2, we notice that for
</bodyText>
<footnote confidence="0.6448005">
9Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4,
even if at a different scale.
</footnote>
<figure confidence="0.955599753846154">
&amp;quot;newswire_10-ROUGE-1.dat&amp;quot;
20
18
16
14
12
10
8
6
4
2
0
100
P R(S) =
N
S
Z=1
907
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 5 10 15 20 25 30 35 40 45 50
&amp;quot;Ensemble-Newswire-50-ROUGE-1.dat&amp;quot;
0 5 10 15 20 25 30 35 40 45 50
0 5 10 15 20 25 30 35 40 45 50
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 5 10 15 20 25 30 35 40 45 50
&amp;quot;Legal-50-Ensemble-ROUGE-1.dat&amp;quot;
&amp;quot;Medical-50-Ensemble-ROUGE-1.dat&amp;quot;
&amp;quot;Literary-50-Ensemble-ROUGE-1.dat&amp;quot;
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<figureCaption confidence="0.998785">
Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains,
respectively from left to right.
</figureCaption>
<figure confidence="0.999400717391304">
&amp;quot;Newswire-50-ROUGE-1.dat&amp;quot;
0.36 0.38 0.4 0.42 0.44
90
80
70
60
50
40
30
20
10
0
0.4 0.41 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5
70
60
50
40
30
20
10
0
0.4 0.41 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5
&amp;quot;Legal-50-ROUGE-1.dat&amp;quot;
0
0.7 0.72 0.74 0.76 0.78 0.8
0
&amp;quot;Literary-50-ROUGE-1.dat&amp;quot;
&amp;quot;Medical-50-ROUGE-1.dat&amp;quot;
160
140
120
100
80
60
40
20
50
45
40
35
30
25
20
15
10
5
</figure>
<figureCaption confidence="0.996316">
Figure 4: Probability Density Functions of ROUGE-1 recall scores for the Newswire, Literary, Scientific and Legal
Domains, respectively from left to right. The resolution of the x-axis is increased to 0.1.
</figureCaption>
<bodyText confidence="0.999970172413793">
all the domains, the pdfs are long-tailed distribu-
tions. This immediately implies that most of the
extracts in a summary space are clustered around
the mean, which means that for automatic summa-
rization systems, it is very easy to get scores around
this range. Furthermore, we can judge the hardness
of each domain by looking at the standard devia-
tion values. A lower standard deviation indicates a
steeper curve, which implies that improving a sys-
tem would be harder. From the table, we can in-
fer that the legal domain is the hardest while the
newswire is the easiest.
Comparing Table 2 with the values in Table 1,
we also notice that the compression ratio affects the
performance differently for each domain. For ex-
ample, although the scientific domain has the high-
est compression ratio, it has a higher mean than
the literary and the newswire domains for ROUGE-
1 and ROUGE-SU4 recall scores. This implies
that although the abstracts of the medical journals
are highly compressed, they have a high overlap
with the document, probably caused by their writ-
ing style. This was in fact confirmed earlier by the
experiments in (Kupiec et al., 1995), where it was
found out that for a data set of 188 scientific arti-
cles, 79% of the sentences in the abstracts could be
perfectly matched with the sentences in the corre-
sponding documents.
Next, we confirm our experiments by testing three
</bodyText>
<page confidence="0.989865">
908
</page>
<bodyText confidence="0.999746172413793">
different extractive summarization systems on our
data set. The first system that we implement is called
Random, and gives a random score between 1 and
100 to each sentence in a document, and then se-
lects the top scoring sentences. The second system,
Lead, implements the lead baseline method which
takes the first k sentences of a document until the
length limit is reached. Finally, the last system that
we implement is TextRank, which uses a variation of
the PageRank graph centrality algorithm in order to
identify the most important sentences in a document
(Page et al., 1999; Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). We selected TextRank as it has a
performance competitive with the top systems par-
ticipating in DUC ’02 (Mihalcea and Tarau, 2004).
We would also like to mention that for the literary,
scientific, and legal domains, the systems apply the
algorithms for each section and each section is eval-
uated independently, and their resulting recall scores
are summed up. This is needed in order to be con-
sistent with our exhaustive search experiments.
The ROUGE recall scores of the three systems are
shown in Table 3. As expected, for the literary and
legal domains, the Random, and the Lead systems
score around the mean. This is due to the fact that
the leading sentences for these two domains do not
indicate any significance, hence the Lead system just
behaves like Random. However for the scientific and
newswire domains, the leading sentences do have
</bodyText>
<table confidence="0.999495111111111">
ROUGE-1
Domain Random Lead TextRank
Newswire 39.13 45.63 44.43
Literary 45.39 45.36 46.12
Scientific 45.75 47.18 49.26
Legal 73.04 72.42 74.82
ROUGE-2
Domain Random Lead TextRank
Newswire 11.39 19.60 17.99
Literary 5.33 5.41 5.92
Scientific 10.73 12.07 12.76
Legal 28.56 28.92 31.06
ROUGE-SU4
Domain Random Lead TextRank
Newswire 15.07 21.58 20.46
Literary 13.21 13.28 13.81
Scientific 15.92 17.12 17.85
Legal 35.41 35.55 37.64
</table>
<tableCaption confidence="0.9984005">
Table 3: ROUGE recall scores of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
</tableCaption>
<bodyText confidence="0.999125962962963">
importance so the Lead system consistently outper-
forms Random. Furthermore, although TextRank is
the best system for the literary, scientific, and legal
domains, it gets outperformed by the Lead system
on the newswire domain. This is also an expected re-
sult as none of the single-document summarization
systems were able to achieve a statistically signifi-
cant improvement over the lead baseline in the previ-
ous Document Understanding Conferences (DUC).
The ROUGE scoring scheme does not tell us how
much improvement a system achieved over another,
or how far it is from the upper bound. Since we now
have access to the pdf of each domain in our data set,
we can find this information simply by calculating
the percentile rank of each system using the formula
given in Equation 2.
The percentile ranks of all three systems for each
domain are shown in Table 4. Notice how different
the gap is between the scores of each system this
time, compared to the scores in Table 3. For ex-
ample, we see in Table 3 that TextRank on scientific
domain has only a 3.51 ROUGE-1 score improve-
ment over a system that randomly selects sentences
to include in the extract. However, Table 4 tells us
that this improvement is in fact 57.57%.
From Table 4, we see that both TextRank and
the Lead system are in the 99.99% percentile of
</bodyText>
<table confidence="0.999500611111111">
ROUGE-1
Domain Random Lead TextRank
Newswire %39.18 %99.99 %99.99
Literary %62.89 %62.89 %97.90
Scientific %42.30 %95.56 %99.87
Legal %79.47 %16.19 %99.99
ROUGE-2
Domain Random Lead TextRank
Newswire %39.57 %99.99 %99.99
Literary %42.20 %54.32 %94.34
Scientific %35.6 %96.03 %99.79
Legal %36.68 %75.38 %99.99
ROUGE-SU4
Domain Random Lead TextRank
Newswire %40.68 %99.99 %99.99
Literary %46.39 %46.39 %96.84
Scientific %36.37 %97.69 %99.94
Legal %23.53 %42.00 %99.99
</table>
<tableCaption confidence="0.9978705">
Table 4: Percentile rankings of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
</tableCaption>
<bodyText confidence="0.99997575">
the newswire domain although the systems have
1.20, 1.61, and 1.12 difference in their ROUGE-1,
ROUGE-2, and ROUGE-SU4 scores respectively.
The high percentile for the Lead system explains
why it was so hard to improve over these baseline in
previous evaluations on newswire data (e.g., see the
evaluations from the Document Understanding Con-
ferences). Furthermore, we see from Table 2 that the
upper bounds corresponding to these scores are 65.7,
37.4, and 38.1 respectively, which are well above
both the TextRank and the Lead systems. There-
fore, the percentile rankings of the Lead and the Tex-
tRank systems for this domain do not seem to give
us clues about how the two systems compare to each
other, nor about their actual distance from the up-
per bounds. There are two reasons for this: First,
as we mentioned earlier, most of the summary space
consists of easy extracts, which make the distribu-
tion long-tailed.10 Therefore even though we have
quite a bit of systems achieving high scores, their
number is negligible compared to the millions of ex-
tracts that are clustered around the mean. Secondly,
we need a higher resolution (i.e. larger number of
bins) in constructing the pdfs in order to be able to
</bodyText>
<footnote confidence="0.519157">
10This also accounts for the fact that even though we might
have two very close ROUGE scores that are not statistically sig-
nificant, their percentile rankings might differ quite a bit.
</footnote>
<page confidence="0.99594">
909
</page>
<bodyText confidence="0.999979157894737">
see the difference more clearly between the two sys-
tems. Finally, when comparing two successful sys-
tems using percentile ranks, we believe the use of
error reduction would be more beneficial.
As a final note, we also randomly sampled ex-
tracts from documents in the scientific and legal do-
mains, but this time without considering the section
boundaries and without performing any segmenta-
tion. We kept the number of samples for each doc-
ument equal to the number of extracts we generated
from the same document using a divide-and-conquer
approach. We evaluated the samples using ROUGE-
1 recall scores, and obtained pdfs for each domain
using the same strategy discussed earlier in the pa-
per. The resulting pdfs, although they exhibit simi-
lar characteristics, they have mean values (p) around
10% lower than the ones we listed in Table 2, which
supports the findings from earlier research that seg-
mentation is useful for text summarization.
</bodyText>
<sectionHeader confidence="0.986694" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991683333334">
In this paper, we described a study that explores the
search space of extractive summaries across four dif-
ferent domains. For the news domain we generated
all possible extracts of the given documents, and
for the literary, scientific, and legal domains we fol-
lowed a divide-and-conquer approach by chunking
the documents into sections, handled each section
independently, and combined the resulting scores at
the end. We then used the distributions of the eval-
uations scores to generate the probability density
functions (pdfs) for each domain. Various statistical
properties of these pdfs helped us asses the difficulty
of each domain. Finally, we introduced a new scor-
ing scheme for automatic text summarization sys-
tems that can be derived from the pdfs. The new
scheme calculates a percentile rank of the ROUGE-
1 recall score of a system, which gives scores in the
range [0-100]. This lets us see how far each sys-
tem is from the upper bound, and thus make a better
comparison among the systems. The new scoring
system showed us that while there is a 20.1% gap
between the upper bound and the lead baseline for
the news domain, closing this gap is difficult, as the
percentile rank of the lead baseline system, 99.99%,
indicates that the system is already very close to the
upper bound.
Furthermore, except for the literary domain, the
percentile rank of the TertRank system is also very
close to the upperbound. This result does not sug-
gest that additional improvements cannot be made
in these domains, but that making further improve-
ments using only extractive summarization will be
considerably difficult. Moreover, in order to see
these future improvements, a higher resolution (i.e.
larger number of bins) will be needed when con-
structing the pdfs.
In all our experiments we used the ROUGE
(Lin, 2004) evaluation package and its ROUGE-
1, ROUGE-2, and ROUGE-SU4 recall scores. We
would like to note that since ROUGE performs its
evaluations based on the n-gram overlap between
the peer and the model summary, it does not take
other summary quality metrics such as coherence
and cohesion into account. However, our goal in this
paper was to analyze the topic-identification stage
only, which concentrates on selecting the right con-
tent from the document to include in the summary,
and the ROUGE scores were found to correlate well
with the human judgments on assessing the content
overlap of summaries.
In the future, we would like to apply a similar ex-
haustive search strategy, but this time with differ-
ent compression ratios, in order to see the impact
of compression ratios on the pdf of each domain.
Furthermore, we would also like to analyze the
high scoring extracts found by the exhaustive search,
in terms of coherence, position and other features.
Such an analysis would allow us to see whether these
extracts exhibit certain properties which could be
used in training machine learning systems.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999722">
The authors would like to thank the anonymous re-
viewers of NAACL-HLT 2010 for their feedback.
The work of the first author has been partly sup-
ported by an award from Google, Inc. The work of
the fourth and fifth authors has been supported by an
FPI grant (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation, under the project
TEXT-MESS (TIN2006-15265-C06-01) funded by
the Spanish Government, and the project PROME-
TEO Desarrollo de Tcnicas Inteligentes e Interacti-
vas de Minera de Textos (2009/119) from the Valen-
cian Government.
</bodyText>
<page confidence="0.994597">
910
</page>
<sectionHeader confidence="0.993824" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999651370786517">
Hakan Ceylan and Rada Mihalcea. 2009. The decompo-
sition of human-written book summaries. In CICLing
’09: Proceedings of the 10th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, pages 582–593, Berlin, Heidelberg. Springer-
Verlag.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In NAACL-
ANLP 2000 Workshop on Automatic summarization,
pages 69–78, Morristown, NJ, USA. Association for
Computational Linguistics.
G. Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal ofArtificial Intelligence Research, 22.
Eduard H. Hovy and Chin Yew Lin. 1999. Automated
text summarization in summarist. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81–97. MIT Press.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28(4):527–543.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ’95: Pro-
ceedings of the 18th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 68–73, New York, NY, USA.
ACM.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73–80, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Chin-Yew Lin. 1999. Training a selection function for
extraction. In CIKM ’99: Proceedings of the eighth
international conference on Information and knowl-
edge management, pages 55–62, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74–
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ’99: Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 137–144, New
York, NY, USA. ACM.
Rada Mihalcea and Hakan Ceylan. 2007. Explorations in
automatic book summarization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 380–
389, Prague, Czech Republic, June. Association for
Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Yoshio Nakao. 2000. An algorithm for one-page sum-
marization of a long text based on thematic hierarchy
detection. In ACL ’00: Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 302–309, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
InfoLab.
Karen Sparck-Jones. 1999. Automatic summarising:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1–13. MIT Press.
Simone Teufel and Marc Moens. 1997. Sentence ex-
traction as a classification task. In Proceedings of the
ACL’97/EACL’97 Workshop on Intelligent Scallable
Text Summarization, Madrid, Spain, July.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 491–498.
Min yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 1998. Linear segmentation and segment sig-
nificance. In In Proceedings of the 6th International
Workshop of Very Large Corpora, pages 197–205.
</reference>
<page confidence="0.998139">
911
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.109364">
<title confidence="0.9530715">Quantifying the Limits and Success of Extractive Summarization Systems Across Domains</title>
<affiliation confidence="0.262631">Yahoo!</affiliation>
<address confidence="0.8951435">701 First Sunnyvale, CA</address>
<email confidence="0.998466">umut@yahoo-inc.com</email>
<author confidence="0.826834">Ceylan</author>
<affiliation confidence="0.999809">Department of Computer University of North</affiliation>
<address confidence="0.712821">Denton, TX</address>
<abstract confidence="0.998990263157895">This paper analyzes the topic identification stage of single-document automatic text summarization across four different domains, consisting of newswire, literary, scientific and legal documents. We present a study that explores the summary space of each domain via an exhaustive search strategy, and finds the probability density function (pdf) of the ROUGE score distributions for each domain. We then use this pdf to calculate the percentile rank of extractive summarization systems. Our results introduce a new way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hakan Ceylan</author>
<author>Rada Mihalcea</author>
</authors>
<title>The decomposition of human-written book summaries.</title>
<date>2009</date>
<booktitle>In CICLing ’09: Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>582--593</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4633" citStr="Ceylan and Mihalcea, 2009" startWordPosition="725" endWordPosition="728">stem achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set. 2 Related Work Despite the large amount of work in automatic text summarization, there are only a few studies in the literature that employ an exhaustive search strategy to create extracts, which is mainly due to the prohibitively large search space of the problem. Furthermore, the research regarding the alignment of abstracts to original documents has shown great variations across domains (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009), which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts of length three starting with 15 TREC Documents, in order to judge the perform</context>
</contexts>
<marker>Ceylan, Mihalcea, 2009</marker>
<rawString>Hakan Ceylan and Rada Mihalcea. 2009. The decomposition of human-written book summaries. In CICLing ’09: Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, pages 582–593, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert L Donaway</author>
<author>Kevin W Drummey</author>
<author>Laura A Mather</author>
</authors>
<title>A comparison of rankings produced by summarization evaluation measures.</title>
<date>2000</date>
<booktitle>In NAACLANLP 2000 Workshop on Automatic summarization,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5095" citStr="Donaway et al., 2000" startWordPosition="799" endWordPosition="802"> original documents has shown great variations across domains (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009), which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts of length three starting with 15 TREC Documents, in order to judge the performance of several summary evaluation measures suggested in their paper. Finally, the study most similar to ours was done by (Lin and Hovy, 2003), who used the articles with less than 30 sentences from the DUC 2001 data set to find oracle extracts of 100 and 150 (±5) words. These extracts were compared against one summary source, selected as the one that gave the highest inter-human agreement. Although it was concluded that a 10% improvement was possible for ex</context>
</contexts>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>Robert L. Donaway, Kevin W. Drummey, and Laura A. Mather. 2000. A comparison of rankings produced by summarization evaluation measures. In NAACLANLP 2000 Workshop on Automatic summarization, pages 69–78, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>22</volume>
<contexts>
<context position="22180" citStr="Erkan and Radev, 2004" startWordPosition="3857" endWordPosition="3860">testing three 908 different extractive summarization systems on our data set. The first system that we implement is called Random, and gives a random score between 1 and 100 to each sentence in a document, and then selects the top scoring sentences. The second system, Lead, implements the lead baseline method which takes the first k sentences of a document until the length limit is reached. Finally, the last system that we implement is TextRank, which uses a variation of the PageRank graph centrality algorithm in order to identify the most important sentences in a document (Page et al., 1999; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). We selected TextRank as it has a performance competitive with the top systems participating in DUC ’02 (Mihalcea and Tarau, 2004). We would also like to mention that for the literary, scientific, and legal domains, the systems apply the algorithms for each section and each section is evaluated independently, and their resulting recall scores are summed up. This is needed in order to be consistent with our exhaustive search experiments. The ROUGE recall scores of the three systems are shown in Table 3. As expected, for the literary and legal domains, the Random, and</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal ofArtificial Intelligence Research, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Eduard</author>
</authors>
<title>Hovy and Chin Yew Lin.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>81--97</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<marker>Eduard, 1999</marker>
<rawString>Eduard H. Hovy and Chin Yew Lin. 1999. Automated text summarization in summarist. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, pages 81–97. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="4605" citStr="Jing, 2002" startWordPosition="723" endWordPosition="724">ovement a system achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set. 2 Related Work Despite the large amount of work in automatic text summarization, there are only a few studies in the literature that employ an exhaustive search strategy to create extracts, which is mainly due to the prohibitively large search space of the problem. Furthermore, the research regarding the alignment of abstracts to original documents has shown great variations across domains (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009), which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts of length three starting with 15 TREC Documents, i</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden markov modeling to decompose human-written summaries. Comput. Linguist., 28(4):527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In SIGIR ’95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>68--73</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4556" citStr="Kupiec et al., 1995" startWordPosition="713" endWordPosition="716">g pdf of the data set. This allows us to see the true improvement a system achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set. 2 Related Work Despite the large amount of work in automatic text summarization, there are only a few studies in the literature that employ an exhaustive search strategy to create extracts, which is mainly due to the prohibitively large search space of the problem. Furthermore, the research regarding the alignment of abstracts to original documents has shown great variations across domains (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009), which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts o</context>
<context position="21334" citStr="Kupiec et al., 1995" startWordPosition="3712" endWordPosition="3715">he hardest while the newswire is the easiest. Comparing Table 2 with the values in Table 1, we also notice that the compression ratio affects the performance differently for each domain. For example, although the scientific domain has the highest compression ratio, it has a higher mean than the literary and the newswire domains for ROUGE1 and ROUGE-SU4 recall scores. This implies that although the abstracts of the medical journals are highly compressed, they have a high overlap with the document, probably caused by their writing style. This was in fact confirmed earlier by the experiments in (Kupiec et al., 1995), where it was found out that for a data set of 188 scientific articles, 79% of the sentences in the abstracts could be perfectly matched with the sentences in the corresponding documents. Next, we confirm our experiments by testing three 908 different extractive summarization systems on our data set. The first system that we implement is called Random, and gives a random score between 1 and 100 to each sentence in a document, and then selects the top scoring sentences. The second system, Lead, implements the lead baseline method which takes the first k sentences of a document until the length</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In SIGIR ’95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 68–73, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The potential and limitations of automatic sentence extraction for summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 on Text summarization workshop,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5375" citStr="Lin and Hovy, 2003" startWordPosition="846" endWordPosition="849">r to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts of length three starting with 15 TREC Documents, in order to judge the performance of several summary evaluation measures suggested in their paper. Finally, the study most similar to ours was done by (Lin and Hovy, 2003), who used the articles with less than 30 sentences from the DUC 2001 data set to find oracle extracts of 100 and 150 (±5) words. These extracts were compared against one summary source, selected as the one that gave the highest inter-human agreement. Although it was concluded that a 10% improvement was possible for extractive summarization systems, which typically score around the lead baseline, there was no report on how difficult it would be to achieve this improvement, which is the main objective of our paper. 3 Description of the Data Set Our data set is composed of four different domains</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. The potential and limitations of automatic sentence extraction for summarization. In Proceedings of the HLT-NAACL 03 on Text summarization workshop, pages 73–80, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Training a selection function for extraction.</title>
<date>1999</date>
<booktitle>In CIKM ’99: Proceedings of the eighth international conference on Information and knowledge management,</booktitle>
<pages>55--62</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1331" citStr="Lin, 1999" startWordPosition="203" endWordPosition="204"> calculate the percentile rank of extractive summarization systems. Our results introduce a new way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain. 1 Introduction Topic identification is the first stage of the generally accepted three-phase model in automatic text summarization, in which the goal is to identify the most important units in a document, i.e., phrases, sentences, or paragraphs (Hovy and Lin, 1999; Lin, 1999; Sparck-Jones, 1999). This stage is followed by the topic interpretation and summary generation steps where the identified units are further processed to bring the summary into a coherent, human readable abstract form. The extractive summarization systems, however, only employ the topic identification stage, and simply output a ranked list of the units according to a compression ratio criterion. In general, for most systems sentences are the preferred Elena Lloret and Manuel Palomar Department of Software and Computing Systems University of Alicante San Vicente del Raspeig Alicante</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Chin-Yew Lin. 1999. Training a selection function for extraction. In CIKM ’99: Proceedings of the eighth international conference on Information and knowledge management, pages 55–62, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81,</booktitle>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3285" citStr="Lin, 2004" startWordPosition="513" endWordPosition="514">n exponential complexity as it requires all possible sentence combinations of a document to be generated, constrained by a given word or sentence length. Thus the problem quickly becomes impractical as the number of sentences in a document increases and the compression ratio decreases. In this work, we try to overcome this bottleneck by using a large cluster of computers, and decomposing the task into smaller problems by using the given section boundaries or a linear text segmentation method. As a result of this exploration, we generate a probability density function (pdf) of the ROUGE score (Lin, 2004) distributions for four different domains, which shows the distribution of the evaluation scores for the generated extracts, and allows us to assess the difficulty of each domain for extractive summarization. Furthermore, using these pdfs, we introduce a new success measure for extractive summarization systems. Namely, given a system’s average score over a data set, we show how to calculate the per903 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903–911, Los Angeles, California, June 2010. c�2010 Association for Computational Linguisti</context>
<context position="30032" citStr="Lin, 2004" startWordPosition="5091" endWordPosition="5092">seline system, 99.99%, indicates that the system is already very close to the upper bound. Furthermore, except for the literary domain, the percentile rank of the TertRank system is also very close to the upperbound. This result does not suggest that additional improvements cannot be made in these domains, but that making further improvements using only extractive summarization will be considerably difficult. Moreover, in order to see these future improvements, a higher resolution (i.e. larger number of bins) will be needed when constructing the pdfs. In all our experiments we used the ROUGE (Lin, 2004) evaluation package and its ROUGE1, ROUGE-2, and ROUGE-SU4 recall scores. We would like to note that since ROUGE performs its evaluations based on the n-gram overlap between the peer and the model summary, it does not take other summary quality metrics such as coherence and cohesion into account. However, our goal in this paper was to analyze the topic-identification stage only, which concentrates on selecting the right content from the document to include in the summary, and the ROUGE scores were found to correlate well with the human judgments on assessing the content overlap of summaries. I</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>137--144</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4593" citStr="Marcu, 1999" startWordPosition="721" endWordPosition="722">the true improvement a system achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set. 2 Related Work Despite the large amount of work in automatic text summarization, there are only a few studies in the literature that employ an exhaustive search strategy to create extracts, which is mainly due to the prohibitively large search space of the problem. Furthermore, the research regarding the alignment of abstracts to original documents has shown great variations across domains (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009), which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts of length three starting with 15 TREC </context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 137–144, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Hakan Ceylan</author>
</authors>
<title>Explorations in automatic book summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>380--389</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11833" citStr="Mihalcea and Ceylan, 2007" startWordPosition="1930" endWordPosition="1933">n our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section’s length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007). 5 Exhaustive Search Algorithm Let Eik = Si1, Si2, ..., Sik be the ith extract that has k sentences, and generated from a document D with n sentences D = S1, S2,.. . , Sn. Further, let len(Sj) give the number of words in sentence Sj. We enforce that Eik satisfies the following constraints: len(Eik) = len(Si1) + ... + len(Sik) ≥ L len(Eik−1) = len(Si1) + ... + len(Sik−1) &lt; L where L is the length constraint on all the extracts of document D. We note that for any Eik, the order of the sentences in Eik−1 does not affect the ROUGE scores, since only the last sentence may be 7http://mastarpj.nict.</context>
</contexts>
<marker>Mihalcea, Ceylan, 2007</marker>
<rawString>Rada Mihalcea and Hakan Ceylan. 2007. Explorations in automatic book summarization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 380– 389, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="22207" citStr="Mihalcea and Tarau, 2004" startWordPosition="3861" endWordPosition="3864">rent extractive summarization systems on our data set. The first system that we implement is called Random, and gives a random score between 1 and 100 to each sentence in a document, and then selects the top scoring sentences. The second system, Lead, implements the lead baseline method which takes the first k sentences of a document until the length limit is reached. Finally, the last system that we implement is TextRank, which uses a variation of the PageRank graph centrality algorithm in order to identify the most important sentences in a document (Page et al., 1999; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). We selected TextRank as it has a performance competitive with the top systems participating in DUC ’02 (Mihalcea and Tarau, 2004). We would also like to mention that for the literary, scientific, and legal domains, the systems apply the algorithms for each section and each section is evaluated independently, and their resulting recall scores are summed up. This is needed in order to be consistent with our exhaustive search experiments. The ROUGE recall scores of the three systems are shown in Table 3. As expected, for the literary and legal domains, the Random, and the Lead systems score aro</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshio Nakao</author>
</authors>
<title>An algorithm for one-page summarization of a long text based on thematic hierarchy detection.</title>
<date>2000</date>
<booktitle>In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>302--309</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11805" citStr="Nakao, 2000" startWordPosition="1928" endWordPosition="1929">l document. In our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section’s length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007). 5 Exhaustive Search Algorithm Let Eik = Si1, Si2, ..., Sik be the ith extract that has k sentences, and generated from a document D with n sentences D = S1, S2,.. . , Sn. Further, let len(Sj) give the number of words in sentence Sj. We enforce that Eik satisfies the following constraints: len(Eik) = len(Si1) + ... + len(Sik) ≥ L len(Eik−1) = len(Si1) + ... + len(Sik−1) &lt; L where L is the length constraint on all the extracts of document D. We note that for any Eik, the order of the sentences in Eik−1 does not affect the ROUGE scores, since only the last sentence m</context>
</contexts>
<marker>Nakao, 2000</marker>
<rawString>Yoshio Nakao. 2000. An algorithm for one-page summarization of a long text based on thematic hierarchy detection. In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 302–309, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical report, Stanford InfoLab.</tech>
<contexts>
<context position="22157" citStr="Page et al., 1999" startWordPosition="3853" endWordPosition="3856">our experiments by testing three 908 different extractive summarization systems on our data set. The first system that we implement is called Random, and gives a random score between 1 and 100 to each sentence in a document, and then selects the top scoring sentences. The second system, Lead, implements the lead baseline method which takes the first k sentences of a document until the length limit is reached. Finally, the last system that we implement is TextRank, which uses a variation of the PageRank graph centrality algorithm in order to identify the most important sentences in a document (Page et al., 1999; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). We selected TextRank as it has a performance competitive with the top systems participating in DUC ’02 (Mihalcea and Tarau, 2004). We would also like to mention that for the literary, scientific, and legal domains, the systems apply the algorithms for each section and each section is evaluated independently, and their resulting recall scores are summed up. This is needed in order to be consistent with our exhaustive search experiments. The ROUGE recall scores of the three systems are shown in Table 3. As expected, for the literary and legal d</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
</authors>
<title>Automatic summarising: Factors and directions.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>1--13</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1363" citStr="Sparck-Jones, 1999" startWordPosition="207" endWordPosition="208">ile rank of extractive summarization systems. Our results introduce a new way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain. 1 Introduction Topic identification is the first stage of the generally accepted three-phase model in automatic text summarization, in which the goal is to identify the most important units in a document, i.e., phrases, sentences, or paragraphs (Hovy and Lin, 1999; Lin, 1999; Sparck-Jones, 1999). This stage is followed by the topic interpretation and summary generation steps where the identified units are further processed to bring the summary into a coherent, human readable abstract form. The extractive summarization systems, however, only employ the topic identification stage, and simply output a ranked list of the units according to a compression ratio criterion. In general, for most systems sentences are the preferred Elena Lloret and Manuel Palomar Department of Software and Computing Systems University of Alicante San Vicente del Raspeig Alicante 03690, Spain {elloret,mpalomar}</context>
</contexts>
<marker>Sparck-Jones, 1999</marker>
<rawString>Karen Sparck-Jones. 1999. Automatic summarising: Factors and directions. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, pages 1–13. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL’97/EACL’97 Workshop on Intelligent Scallable Text Summarization,</booktitle>
<location>Madrid, Spain,</location>
<contexts>
<context position="4580" citStr="Teufel and Moens, 1997" startWordPosition="717" endWordPosition="720">. This allows us to see the true improvement a system achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set. 2 Related Work Despite the large amount of work in automatic text summarization, there are only a few studies in the literature that employ an exhaustive search strategy to create extracts, which is mainly due to the prohibitively large search space of the problem. Furthermore, the research regarding the alignment of abstracts to original documents has shown great variations across domains (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009), which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems, (Marcu, 1999) used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition, (Donaway et al., 2000) used exhaustive search to create all the sentence extracts of length three starting </context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>Simone Teufel and Marc Moens. 1997. Sentence extraction as a classification task. In Proceedings of the ACL’97/EACL’97 Workshop on Intelligent Scallable Text Summarization, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation. In</title>
<date>2001</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="10484" citStr="Utiyama and Isahara, 2001" startWordPosition="1694" endWordPosition="1698">nto sections, and create extracts for each section such that the length of the extract is proportional to the length of the section in the original document. For the legal and scientific domains, we use the given section boundaries (without considering the subsections for scientific documents). For the novels, we treat each chapter as a single document (since each chapter has its own summary), which is further divided into sections using a publicly available linear 4http://berouge.com 5-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 6http://hadoop.apache.org/ text segmentation algorithm by (Utiyama and Isahara, 2001).7 In all cases, we let the algorithm pick the number of segments automatically. To evaluate the sections, we modified ROUGE further so that it applies the length constraint to the extracts only, not to the model summaries. This is due to the fact that we evaluate the extracts of each section individually against the whole model summary, which is larger than the extract. This way, we can get an overall ROUGE recall score for a document extract, simply by summing up the recall scores of each section extracts. The precision score for the entire document can also be found by adding the weighted p</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min yen Kan</author>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Linear segmentation and segment significance. In</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th International Workshop of Very Large Corpora,</booktitle>
<pages>197--205</pages>
<contexts>
<context position="11792" citStr="Kan et al., 1998" startWordPosition="1924" endWordPosition="1927">ion in the original document. In our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section’s length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007). 5 Exhaustive Search Algorithm Let Eik = Si1, Si2, ..., Sik be the ith extract that has k sentences, and generated from a document D with n sentences D = S1, S2,.. . , Sn. Further, let len(Sj) give the number of words in sentence Sj. We enforce that Eik satisfies the following constraints: len(Eik) = len(Si1) + ... + len(Sik) ≥ L len(Eik−1) = len(Si1) + ... + len(Sik−1) &lt; L where L is the length constraint on all the extracts of document D. We note that for any Eik, the order of the sentences in Eik−1 does not affect the ROUGE scores, since only the la</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>Min yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear segmentation and segment significance. In In Proceedings of the 6th International Workshop of Very Large Corpora, pages 197–205.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>