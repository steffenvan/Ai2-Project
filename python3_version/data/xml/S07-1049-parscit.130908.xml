<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001057">
<title confidence="0.961737">
MELB-KB: Nominal Classification as Noun Compound Interpretation
</title>
<author confidence="0.988132">
Su Nam Kim and Timothy Baldwin
</author>
<affiliation confidence="0.9984955">
Computer Science and Software Engineering
University of Melbourne, Australia
</affiliation>
<email confidence="0.997673">
{snkim,tim}@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999875583333333">
In this paper, we outline our approach to
interpreting semantic relations in nominal
pairs in SemEval-2007 task #4: Classifica-
tion of Semantic Relations between Nomi-
nals. We build on two baseline approaches
to interpreting noun compounds: sense col-
location, and constituent similarity. These
are consolidated into an overall system in
combination with co-training, to expand the
training data. Our two systems attained an
average F-score over the test data of 58.7%
and 57.8%, respectively.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875454545455">
This paper describes two systems entered in
SemEval-2007 task #4: Classification of Semantic
Relations between Nominals. A key contribution of
this research is that we examine the compatibility of
noun compound (NC) interpretation methods over
the extended task of nominal classification, to gain
empirical insight into the relative complexity of the
two tasks.
The goal of the nominal classification task is to
identify the compatibility of a given semantic re-
lation with each of a set of test nominal pairs,
e.g. between climate and forest in the fragment the
climate in the forest with respect to the CONTENT-
CONTAINER relation. Semantic relations (or SRs)
in nominals represent the underlying interpretation
of the nominal, in the form of the directed relation
between the two nominals.
The proposed task is a generalisation of the more
conventional task of interpreting noun compounds
(NCs), in which we take a NC such as cookie jar and
interpret it according to a pre-defined inventory of
semantic relations (Levi, 1979; Vanderwende, 1994;
Barker and Szpakowicz, 1998). Examples of seman-
tic relations are MAKE,&apos;, as exemplified in apple pie
where the pie is made from apple(s), and POSSES-
SOR, as exemplified in family car where the car is
possessed by a family.
In the SemEval-2007 task, SR interpretation
takes the form of a binary decision for a
given nominal pair in context and a given SR,
in judging whether that nominal pair conforms
to the SR. Seven relations were used in the
task: CAUSE-EFFECT, INSTRUMENT-AGENCY,
PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME-
TOOL, PART-WHOLE and CONTENT-CONTAINER.
Our approach to the task was to: (1) naively treat
all nominal pairs as NCs (e.g. the climate in the for-
est is treated as an instance of climate forest); and
(2) translate the individual binary classification tasks
into a single multiclass classification task, in the in-
terests of benchmarking existing SR interpretation
methods over a common dataset. That is, we take
all positive training instances for each SR and pool
them together into a single training dataset. For each
test instance, we make a prediction according to one
of the seven relations in the task, which we then
map onto a binary classification for final evaluation
purposes. This mapping is achieved by determining
which binary SR classification the test instance was
sourced from, and returning a positive classification
if the predicted SR coincides with the target SR, and
a negative classification if not.
We make three (deliberately naive) assumptions
in our approach to the nominal interpretation task.
First, we assume that all the positive training in-
</bodyText>
<footnote confidence="0.97734975">
&apos;For direct comparability with our earlier research, seman-
tic relations used in our examples are taken from (Barker and
Szpakowicz, 1998), and differ slightly from those used in the
SemEval-2007 task.
</footnote>
<page confidence="0.976471">
231
</page>
<bodyText confidence="0.989588">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 231–236,
Prague, June 2007. c�2007 Association for Computational Linguistics
stances correspond uniquely to the SR in question,
despite the task organisers making it plain that there
is semantic overlap between the SRs. As a machine
learning task, this makes the task considerably more
difficult, as the performance for the standard base-
lines drops considerably from that for the binary
tasks. Second, we assume that each nominal pair
maps onto a NC. This is clearly a misconstrual of the
task, and intended to empirically validate whether
such an approach is viable. In line with this assump-
tion, we will refer to nominal pairs as NCs for the
remainder of the paper. Third and finally, we assume
that the SR annotation of each training and test in-
stance is insensitive to the original context, and use
only the constituent words in the NC to make our
prediction. This is for direct comparability with ear-
lier research, and we acknowledge that the context
(and word sense) is a strong determinant of the SR
in practice.
Our aim in this paper is to demonstrate the effec-
tiveness of general-purpose SR interpretation over
the nominal classification task, and establish a new
baseline for the task.
The remainder of this paper is structured as fol-
lows. We present our methods in Section 2 and de-
pict the system architectures in Section 4. We then
describe and discuss the performance of our meth-
ods in Section 5 and conclude the paper in Section 6.
</bodyText>
<sectionHeader confidence="0.990357" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.99977825">
We used two basic NC interpretation methods. The
first method uses sense collocations as proposed by
Moldovan et al. (2004), and the second method uses
the lexical similarity of the component words in the
NC as proposed by Kim and Baldwin (2005). Note
that neither method uses the context of usage of the
NC, i.e. the only features are the words contained in
the NC.
</bodyText>
<subsectionHeader confidence="0.987892">
2.1 Sense Collocation Method
</subsectionHeader>
<bodyText confidence="0.998884">
Moldovan et al. (2004) proposed a method called se-
mantic scattering for interpreting NCs. The intuition
behind this method is that when the sense colloca-
tion of NCs is the same, their SR is most likely the
same. For example, the sense collocation of auto-
mobile factory is the same as that of car factory, be-
cause the senses of automobile and car, and factory
in the two instances, are identical. As a result, the
two NCs have the semantic relation MAKE.
The semantic scattering model is outlined below.
The probability P(r|fifj) (simplified to
P (r|fij)) of a semantic relation r for word
senses fi and fj is calculated based on simple
maximum likelihood estimation:
</bodyText>
<equation confidence="0.993899666666667">
n(r, fij)
P(r|fij) = (1)
n(fij)
</equation>
<bodyText confidence="0.992416333333333">
and the preferred SR r∗ for the given word sense
combination is that which maximises the probabil-
ity:
</bodyText>
<equation confidence="0.993664333333333">
*
r = argmaxrCRP (r|fij)
= argmaxrCRP(fij|r)P(r) (2)
</equation>
<bodyText confidence="0.9994898">
Note that in limited cases, the same sense collo-
cation can lead to multiple SRs. However, since we
do not take context into account in our method, we
make the simplifying assumption that a given sense
collocation leads to a unique SR.
</bodyText>
<subsectionHeader confidence="0.99961">
2.2 Constituent Similarity Method
</subsectionHeader>
<bodyText confidence="0.999988764705882">
In earlier work (Kim and Baldwin, 2005), we pro-
posed a simplistic general-purpose method based on
the lexical similarity of unseen NCs with training
instances. That is, the semantic relation of a test
instance is derived from the train instance which
has the highest similarity with the test instance, in
the form of a 1-nearest neighbour classifier. For
example, assuming the test instance chocolate milk
and training instances apple juice and morning milk,
we would calculate the similarity between modifier
chocolate and each of apple and morning, and head
noun milk and each of juice and milk, and find, e.g.,
the similarities .71 and .27, and .83 and 1.00 respec-
tively. We would then add these up to derive the
overall similarity for a given NC and find that apple
juice is a better match. From this, we would assign
the SR of MAKE from apple juice to chocolate milk.
</bodyText>
<equation confidence="0.7130824">
Formally, SA is the similarity between NCs
(Ni,1, Ni,2) and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((αS1 + S1) x ((1 − α)S2 + S2)) (3)
2
</equation>
<bodyText confidence="0.9958635">
where S1 is the modifier similarity (i.e.
S(Ni,1, Bj1)) and S2 is head noun similarity
</bodyText>
<page confidence="0.96566">
232
</page>
<bodyText confidence="0.999820363636364">
(i.e. S(Ni,2, Bj2)); α ∈ [0, 1] is a weighting factor.
The similarity scores are calculated using the
method of Wu and Palmer (1994) as implemented
in WordNet::Similarity (Patwardhan et al.,
2003). This is done for each pairing of WordNet
senses of each of the two words in question, and the
overall lexical similarity is calculated as the average
across the pairwise sense similarities.
The final classification is derived from the training
instance which has the highest lexical similarity with
the test instance in question.
</bodyText>
<sectionHeader confidence="0.866946" genericHeader="method">
3 Co-Training
</sectionHeader>
<bodyText confidence="0.999942333333333">
As with many semantic annotation tasks, SR tag-
ging is a time-consuming and expensive process. At
the same time, due to the inherent complexity of the
SR interpretation task, we require large amounts of
training data in order for our methods to perform
well. In order to generate additional training data to
train our methods over, we experiment with different
co-training methodologies for each of our two basic
methods.
</bodyText>
<subsectionHeader confidence="0.981365">
3.1 Co-Training for the Sense Collocation
Method
</subsectionHeader>
<bodyText confidence="0.999974964285714">
For the sense collocation method, we experiment
with a substitution method whereby we replace one
constituent in a training NC instance by a similar
word, and annotate the new instance with the same
SR as the original NC. For example, car in car fac-
tory (SR = MAKE) has similar words automobile,
vehicle, truck from the synonym, hypernym and sis-
ter word taxonomic relations, respectively. When
car is replaced by a similar word, the new noun
compound(s) (i.e. automobile/vehicle/truck factory)
share the same SR as the original car factory. Note
that each constituent in our original example is
tagged for word sense, which we use both in ac-
cessing sense-specific substitution candidates (via
WordNet), and sense-annotating the newly gener-
ated NCs.
Substitution is restricted to one constituent at a
time in order to avoid extreme semantic variation.
This procedure can be repeated to generate more
training data. However, as the procedure goes fur-
ther, we introduce increasingly more noise.
In our experiments, we use this co-training
method with the sense collocation method to expand
the size and variation of training data, using syn-
onym, hypernym and sister word relations. For our
experiment, we ran the expansion procedure for only
one iteration in order to avoid generating excessive
amounts of incorrectly-tagged NCs.
</bodyText>
<subsectionHeader confidence="0.961972">
3.2 Co-Training for the Constituent Similarity
Method
</subsectionHeader>
<bodyText confidence="0.999963588235294">
Our experiments with the constituent similarity
method over the trial data showed, encouragingly,
that there is a strong correlation between the strength
of overall similarity with the best-matching training
NC, and the accuracy of the prediction. From this,
we experimented with implementing the constituent
similarity method in a cascading architecture. That
is, we batch evaluate all test instances on each it-
eration, and tag those test instances for which the
best match with a training instance is above a pre-
set threshold, which we decrease on each iteration.
In subsequent iterations, all tagged test instances are
included in the training data. Hence, on each itera-
tion, the number of training instances is increasing.
As our threshold, we used a starting value of 0.85,
which was decreased down to 0.65 in increments of
0.05.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="method">
4 Architectures
</sectionHeader>
<bodyText confidence="0.999884">
In Section 4.1 and Section 4.2, we describe the ar-
chitecture of our two systems.
</bodyText>
<subsectionHeader confidence="0.997542">
4.1 Architecture (I)
</subsectionHeader>
<bodyText confidence="0.998975625">
Figure 1 presents the architecture of our first system,
which interleaves sense collocation and constituent
similarity, and includes co-training for each. There
are five steps in this system.
First, we apply the basic sense collocation method
relative to the original training data. If the sense col-
location between the test and training instances is
the same, we judge the predicted SR to be correct.
Second, we apply the similarity method described
in Section 2.2 over the original training data. How-
ever, we only classify test instances where the final
similarity is above a threshold of 0.8.
Third, we apply the sense collocation co-training
method and re-run the sense collocation method
over the expanded training data from the first two
steps. Since the sense collocations in the expanded
</bodyText>
<page confidence="0.993003">
233
</page>
<figure confidence="0.7510185">
Step 4
tagged data
Similarity
untagged test data
tagged data Step 5
Similarity
</figure>
<figureCaption confidence="0.999761">
Figure 1: System Architecture (I)
</figureCaption>
<bodyText confidence="0.999577111111111">
training data have been varied through the advent of
hypernyms and sister words, the number of sense
collocations in the expanded training data is much
greater than that of the original training data (937
vs. 16,676).
Fourth, we apply the constituent similarity co-
training method over the consolidated training data
(from both sense collocation and constituent simi-
larity co-training) with the threshold unchanged at
0.8.
Finally, we apply the constituent similarity
method over the combined training data, without any
threshold (to guarantee a SR prediction for every
test instance). However, since the generated train-
ing instances are more likely to contain errors, we
decrement the similarity values for generated train-
ing instances by 0.2, to prefer predictions based on
the original training instances.
</bodyText>
<subsectionHeader confidence="0.989316">
4.2 Architecture (II)
</subsectionHeader>
<bodyText confidence="0.837803666666667">
Figure 2 depicts our second system, which is based
solely on the constituent similarity method, with co-
training.
We perform iterative co-training as described in
finalize current
tags and end
</bodyText>
<figureCaption confidence="0.999087">
Figure 2: System Architecture (II)
</figureCaption>
<bodyText confidence="0.999978">
Section 3.2, with the slight variation that we hold
off reducing the threshold if more than 10% of the
test instances are tagged on a given iteration, giving
other test instances a chance to be tagged at a higher
threshold level relative to newly generated training
instances. The residue of test instances on comple-
tion of the final iteration (threshold = 0.6) are tagged
according to the best-matching training instance, ir-
respective of the magnitude of the similarity.
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999926588235294">
We group our evaluation into two categories: (A)
doesn’t use WordNet 2.1 or the query context;
and (B) uses WordNet 2.1 only (again with-
out the query context). Of our two basic meth-
ods the sense collocation method and co-training
method are based on WordNet 2.1 only, while
the constituent similarity method is based indirectly
on WordNet 2.1, but doesn’t preserve WordNet
2.1 sense information. Hence, our first system is
category B while our second system is (arguably)
category A.
Table 1 presents the three baselines for the task,
and the results for our two systems (System I and
System II). The performance for both systems ex-
ceeded all three baselines in terms of accuracy, and
all but the All True baseline (i.e. every instance is
judged to be compatible with the given SR) in terms
</bodyText>
<figure confidence="0.99874558974359">
TEST
Step 1
Sense Collcation
untagged test data
untagged test data
tagged data
TRAIN
Extension of
Training data
by similar words
− Synonym
− Hypernym
− Sister word
Extended TRAIN
Step 3
Sense Collcation
untagged test data
tagged data
Step 2
Similarity
tagged data
get Similarity
Tagged
#of Tagged
&gt;= 10% of test
N
Sim &gt;= T
Y
Y
Threshold
N
N
reduce Threshold
Y
TEST
TRAIN
if T == 0.6 &amp;
(#of Tagged &lt;
10% of test)
</figure>
<page confidence="0.992455">
234
</page>
<table confidence="0.999796666666667">
Method P R F A
All True 48.5 100.0 64.8 48.5
Probability 48.5 48.5 48.5 51.7
Majority 81.3 42.9 30.8 57.0
System I 61.7 56.8 58.7 62.5
System II 61.5 55.7 57.8 62.7
</table>
<tableCaption confidence="0.90572">
Table 1: System results (P = precision, R = recall, F
= F-score, and A = accuracy)
</tableCaption>
<table confidence="0.999920714285714">
Team P R F A
759 66.1 66.7 64.8 66.0
281 60.5 69.5 63.8 63.5
633 62.7 63.0 62.7 65.4
220 61.5 55.7 57.8 62.7
161 56.1 57.1 55.9 58.8
538 48.2 40.3 43.1 49.9
</table>
<tableCaption confidence="0.999735">
Table 2: Results of category A systems
</tableCaption>
<bodyText confidence="0.9990536">
of F-score and recall.
Tables 2 and 3 show the performance of the teams
which performed in the task, in categories A and B.
Team 220 in Table 2 is our second system, and team
220 in Table 3 is our first system.
In Figures 3 and 4, we present a breakdown of
the performance our first and second system, re-
spectively, over the individual semantic relations.
Our approaches performed best for the PRODUCT-
PRODUCER SR, and worst for the PART-WHOLE
SR. In general, our systems achieved similar perfor-
mance on most SRs, with only PART-WHOLE be-
ing notably worse. The lower performance of PART-
WHOLE pulls down our overall performance consid-
erably.
Tables 4 and 5 show the number of tagged and un-
tagged instances for each step of System I and Sys-
tem II, respectively. The first system tagged more
than half of the data in the fifth (and final) step,
where it weighs up predictions from the original and
expanded training data. Hence, the performance of
this approach relies heavily on the similarity method
and expanded training data. Additionally, the differ-
ence in quality between the original and expanded
training data will influence the performance of the
approach appreciably. On the other hand, the num-
ber of instances tagged by the second system is well
distributed across each iteration. However, since
we accumulate generated training instances on each
step, the relative noise level in the training data will
</bodyText>
<table confidence="0.999797333333333">
Team P R F A
901 79.7 69.8 72.4 76.3
777 70.9 73.4 71.8 72.9
281 72.8 70.6 71.5 73.2
129 69.9 64.6 66.8 71.4
333 62.0 71.7 65.4 67.0
538 66.7 62.8 64.3 67.2
571 55.7 66.7 60.4 59.1
759 66.4 58.1 60.3 63.6
220 61.7 56.8 58.7 62.5
371 56.8 56.3 56.1 57.7
495 55.9 57.8 51.4 53.7
</table>
<tableCaption confidence="0.999878">
Table 3: Results of category B systems
</tableCaption>
<figure confidence="0.990060777777778">
(%) precision recall F−score accuracy
100
80
60
40
20
0
CE IA PP OE TT PW CC
relations
</figure>
<figureCaption confidence="0.9726715">
Figure 3: System I performance for each rela-
tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-
</figureCaption>
<bodyText confidence="0.98961655">
AGENCY, PP=PRODUCT-PRODUCER,
OE=ORIGIN-ENTITY, TT=THEME-TOOL,
PW=PART-WHOLE, CC=CONTENT-CONTAINER)
increase across iterations, impacting on the final per-
formance of the system.
Over the trial data, we noticed that the system pre-
dictions are appreciably worse when the similarity
value is low. In future work, we intend to analyse
what is happening in terms of the overall system
performance at each step. This analysis is key to
improving the performance of our systems.
Recall that we are generalising from the set of
binary classification tasks in the original task, to a
multiclass classification task. As such, a direct com-
parison with the binary classification baselines is
perhaps unfair (particularly All True, which has no
correlate in a multiclass setting), and it is if anything
remarkable that our system compares favourably
compared to the baselines. Similarly, while we
clearly lag behind other systems participating in the
</bodyText>
<page confidence="0.988812">
235
</page>
<figure confidence="0.995663625">
precision recall F score accuracy
(%)
100
80
60
40
20
0
</figure>
<table confidence="0.948134428571429">
I T tagged accumulated untagged
i1 .85 73 13.3% 476
i2 .80 56 23.5% 420
i3 .75 74 37.0% 346
i4 .70 101 55.4% 245
i5 .65 222 95.8% 23
– &lt;.65 21 99.6% 2
</table>
<tableCaption confidence="0.953764">
Table 5: System II: data tagged on each iteration (T
= the threshold; iX = the iteration number)
</tableCaption>
<figure confidence="0.6334685">
CE IA PP OE TT PW CC
relations
</figure>
<figureCaption confidence="0.911392">
Figure 4: System II performance for each rela-
</figureCaption>
<table confidence="0.7360432">
tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-
AGENCY, PP=PRODUCT-PRODUCER,
OE=ORIGIN-ENTITY, TT=THEME-TOOL,
PW=PART-WHOLE, CC=CONTENT-CONTAINER)
step method tagged accumulated untagged
s1 SC 21 3.8% 528
s2 Sim 106 23.1% 422
s3 extSC 0 23.1% 422
s4 extSim 61 34.2% 361
s5 SvsExtS 359 99.6% 2
</table>
<tableCaption confidence="0.978704">
Table 4: System I: Tagged data from each step
</tableCaption>
<bodyText confidence="0.995590777777778">
(SC= sense collocation; Sim = the similarity method;
extSC = SC over the expanded training data; extSim
= similarity over the expanded training data; SvsExtS
= the final step over both the original and expanded
training data)
task, we believe we have demonstrated that NC in-
terpretation methods can be successfully deployed
over the more general task of nominal pair classifi-
cation.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999032">
In this paper, we presented two systems entered in
the SemEval-2007 Classification of Semantic Re-
lations between Nominals task. Both systems are
based on baseline NC interpretation methods, and
the naive assumption that the nominal classification
task is analogous to a conventional multiclass NC
interpretation task. Our results compare favourably
with the established baselines, and demonstrate that
NC interpretation methods are compatible with the
more general task of nominal classification.
</bodyText>
<sectionHeader confidence="0.99896" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<copyright confidence="0.531412">
This research was carried out with support from Australian Re-
search Council grant no. DP0663879.
</copyright>
<sectionHeader confidence="0.996824" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998016875">
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proc. of the
17th International Conference on Computational Linguis-
tics, pages 96–102, Montreal, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Timothy W. Finin. 1980. The Semantic Interpretation of Com-
pound Nominals. Ph.D. thesis, University of Illinois.
Su Nam Kim and Timothy Baldwin. 2005. Automatic inter-
pretation of Noun Compounds using WordNet similarity. In
Proc. of the 2nd International Joint Conference On Natural
Language Processing, pages 945–956, JeJu, Korea.
Judith Levi. 1979. The syntax and semantics of complex nom-
inals. In The Syntax and Semantics of Complex Nominals.
New York:Academic Press.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe,
and Roxana Girju. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of the HLT-NAACL 2004
Workshop on Computational Lexical Semantics, pages 60–
67, Boston, USA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.
2003. Using measures of semantic relatedness for word
sense disambiguation. In Proc. of the Fourth International
Conference on Intelligent Text Processing and Computa-
tional Linguistics, pages 241–57, Mexico City, Mexico.
Lucy Vanderwende. 1994. Algorithm for automatic interpreta-
tion of noun sequences. In Proc. of the 15th conference on
Computational linguistics, pages 782–788, Kyoto, Japan.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics and
lexical selection. In Proc. of the 32nd Annual Meeting of the
Association for Computational Linguistics, pages 133–138,
Las Cruces, USA.
</reference>
<page confidence="0.998547">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496845">
<title confidence="0.999572">MELB-KB: Nominal Classification as Noun Compound Interpretation</title>
<author confidence="0.99963">Su Nam Kim</author>
<author confidence="0.99963">Timothy Baldwin</author>
<affiliation confidence="0.77479">Computer Science and Software Engineering University of Melbourne, Australia</affiliation>
<email confidence="0.997391">snkim@csse.unimelb.edu.au</email>
<email confidence="0.997391">tim@csse.unimelb.edu.au</email>
<abstract confidence="0.992714230769231">In this paper, we outline our approach to interpreting semantic relations in nominal pairs in SemEval-2007 task #4: Classification of Semantic Relations between Nominals. We build on two baseline approaches to interpreting noun compounds: sense collocation, and constituent similarity. These are consolidated into an overall system in combination with co-training, to expand the training data. Our two systems attained an average F-score over the test data of 58.7% and 57.8%, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>In Proc. of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>96--102</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1792" citStr="Barker and Szpakowicz, 1998" startWordPosition="265" endWordPosition="268">iven semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the CONTENTCONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-defined inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are MAKE,&apos;, as exemplified in apple pie where the pie is made from apple(s), and POSSESSOR, as exemplified in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: CAUSE-EFFECT, INSTRUMENT-AGENCY, PRODUCT-PRODUCER, ORIGIN-ENTITY, THEMETOOL, PART-WHOLE and CONTENT-CONTAINER. Our approach to the task was to: (1) naively treat all nominal pair</context>
<context position="3490" citStr="Barker and Szpakowicz, 1998" startWordPosition="538" endWordPosition="541">seven relations in the task, which we then map onto a binary classification for final evaluation purposes. This mapping is achieved by determining which binary SR classification the test instance was sourced from, and returning a positive classification if the predicted SR coincides with the target SR, and a negative classification if not. We make three (deliberately naive) assumptions in our approach to the nominal interpretation task. First, we assume that all the positive training in&apos;For direct comparability with our earlier research, semantic relations used in our examples are taken from (Barker and Szpakowicz, 1998), and differ slightly from those used in the SemEval-2007 task. 231 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 231–236, Prague, June 2007. c�2007 Association for Computational Linguistics stances correspond uniquely to the SR in question, despite the task organisers making it plain that there is semantic overlap between the SRs. As a machine learning task, this makes the task considerably more difficult, as the performance for the standard baselines drops considerably from that for the binary tasks. Second, we assume that each nominal pair maps </context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In Proc. of the 17th International Conference on Computational Linguistics, pages 96–102, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy W Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois.</institution>
<marker>Finin, 1980</marker>
<rawString>Timothy W. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.D. thesis, University of Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic interpretation of Noun Compounds using WordNet similarity.</title>
<date>2005</date>
<booktitle>In Proc. of the 2nd International Joint Conference On Natural Language Processing,</booktitle>
<pages>945--956</pages>
<location>JeJu,</location>
<contexts>
<context position="5352" citStr="Kim and Baldwin (2005)" startWordPosition="851" endWordPosition="854"> general-purpose SR interpretation over the nominal classification task, and establish a new baseline for the task. The remainder of this paper is structured as follows. We present our methods in Section 2 and depict the system architectures in Section 4. We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6. 2 Approach We used two basic NC interpretation methods. The first method uses sense collocations as proposed by Moldovan et al. (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Note that neither method uses the context of usage of the NC, i.e. the only features are the words contained in the NC. 2.1 Sense Collocation Method Moldovan et al. (2004) proposed a method called semantic scattering for interpreting NCs. The intuition behind this method is that when the sense collocation of NCs is the same, their SR is most likely the same. For example, the sense collocation of automobile factory is the same as that of car factory, because the senses of automobile and car, and factory in the two instances, are identical. As a result, the two NCs have the semantic relation M</context>
<context position="6665" citStr="Kim and Baldwin, 2005" startWordPosition="1078" endWordPosition="1081"> to P (r|fij)) of a semantic relation r for word senses fi and fj is calculated based on simple maximum likelihood estimation: n(r, fij) P(r|fij) = (1) n(fij) and the preferred SR r∗ for the given word sense combination is that which maximises the probability: * r = argmaxrCRP (r|fij) = argmaxrCRP(fij|r)P(r) (2) Note that in limited cases, the same sense collocation can lead to multiple SRs. However, since we do not take context into account in our method, we make the simplifying assumption that a given sense collocation leads to a unique SR. 2.2 Constituent Similarity Method In earlier work (Kim and Baldwin, 2005), we proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. That is, the semantic relation of a test instance is derived from the train instance which has the highest similarity with the test instance, in the form of a 1-nearest neighbour classifier. For example, assuming the test instance chocolate milk and training instances apple juice and morning milk, we would calculate the similarity between modifier chocolate and each of apple and morning, and head noun milk and each of juice and milk, and find, e.g., the similarities .71 and </context>
</contexts>
<marker>Kim, Baldwin, 2005</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2005. Automatic interpretation of Noun Compounds using WordNet similarity. In Proc. of the 2nd International Joint Conference On Natural Language Processing, pages 945–956, JeJu, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Levi</author>
</authors>
<title>The syntax and semantics of complex nominals.</title>
<date>1979</date>
<booktitle>In The Syntax and Semantics of Complex Nominals.</booktitle>
<publisher>New York:Academic Press.</publisher>
<contexts>
<context position="1743" citStr="Levi, 1979" startWordPosition="261" endWordPosition="262">entify the compatibility of a given semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the CONTENTCONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-defined inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are MAKE,&apos;, as exemplified in apple pie where the pie is made from apple(s), and POSSESSOR, as exemplified in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: CAUSE-EFFECT, INSTRUMENT-AGENCY, PRODUCT-PRODUCER, ORIGIN-ENTITY, THEMETOOL, PART-WHOLE and CONTENT-CONTAINER. Our approach to th</context>
</contexts>
<marker>Levi, 1979</marker>
<rawString>Judith Levi. 1979. The syntax and semantics of complex nominals. In The Syntax and Semantics of Complex Nominals. New York:Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>In Proc. of the HLT-NAACL 2004 Workshop on Computational Lexical Semantics,</booktitle>
<pages>60--67</pages>
<location>Boston, USA.</location>
<contexts>
<context position="5230" citStr="Moldovan et al. (2004)" startWordPosition="829" endWordPosition="832">nd word sense) is a strong determinant of the SR in practice. Our aim in this paper is to demonstrate the effectiveness of general-purpose SR interpretation over the nominal classification task, and establish a new baseline for the task. The remainder of this paper is structured as follows. We present our methods in Section 2 and depict the system architectures in Section 4. We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6. 2 Approach We used two basic NC interpretation methods. The first method uses sense collocations as proposed by Moldovan et al. (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Note that neither method uses the context of usage of the NC, i.e. the only features are the words contained in the NC. 2.1 Sense Collocation Method Moldovan et al. (2004) proposed a method called semantic scattering for interpreting NCs. The intuition behind this method is that when the sense collocation of NCs is the same, their SR is most likely the same. For example, the sense collocation of automobile factory is the same as that of car factory, because the senses of </context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe, and Roxana Girju. 2004. Models for the semantic classification of noun phrases. In Proc. of the HLT-NAACL 2004 Workshop on Computational Lexical Semantics, pages 60– 67, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proc. of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--57</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="7931" citStr="Patwardhan et al., 2003" startWordPosition="1297" endWordPosition="1300">ld then add these up to derive the overall similarity for a given NC and find that apple juice is a better match. From this, we would assign the SR of MAKE from apple juice to chocolate milk. Formally, SA is the similarity between NCs (Ni,1, Ni,2) and (Bj,1, Bj,2): SA((Ni,1, Ni,2), (Bj,1, Bj,2)) = ((αS1 + S1) x ((1 − α)S2 + S2)) (3) 2 where S1 is the modifier similarity (i.e. S(Ni,1, Bj1)) and S2 is head noun similarity 232 (i.e. S(Ni,2, Bj2)); α ∈ [0, 1] is a weighting factor. The similarity scores are calculated using the method of Wu and Palmer (1994) as implemented in WordNet::Similarity (Patwardhan et al., 2003). This is done for each pairing of WordNet senses of each of the two words in question, and the overall lexical similarity is calculated as the average across the pairwise sense similarities. The final classification is derived from the training instance which has the highest lexical similarity with the test instance in question. 3 Co-Training As with many semantic annotation tasks, SR tagging is a time-consuming and expensive process. At the same time, due to the inherent complexity of the SR interpretation task, we require large amounts of training data in order for our methods to perform we</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proc. of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 241–57, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proc. of the 15th conference on Computational linguistics,</booktitle>
<pages>782--788</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1762" citStr="Vanderwende, 1994" startWordPosition="263" endWordPosition="264">ompatibility of a given semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the CONTENTCONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-defined inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are MAKE,&apos;, as exemplified in apple pie where the pie is made from apple(s), and POSSESSOR, as exemplified in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: CAUSE-EFFECT, INSTRUMENT-AGENCY, PRODUCT-PRODUCER, ORIGIN-ENTITY, THEMETOOL, PART-WHOLE and CONTENT-CONTAINER. Our approach to the task was to: (1) </context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Lucy Vanderwende. 1994. Algorithm for automatic interpretation of noun sequences. In Proc. of the 15th conference on Computational linguistics, pages 782–788, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proc. of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, USA.</location>
<contexts>
<context position="7867" citStr="Wu and Palmer (1994)" startWordPosition="1289" endWordPosition="1292">ilarities .71 and .27, and .83 and 1.00 respectively. We would then add these up to derive the overall similarity for a given NC and find that apple juice is a better match. From this, we would assign the SR of MAKE from apple juice to chocolate milk. Formally, SA is the similarity between NCs (Ni,1, Ni,2) and (Bj,1, Bj,2): SA((Ni,1, Ni,2), (Bj,1, Bj,2)) = ((αS1 + S1) x ((1 − α)S2 + S2)) (3) 2 where S1 is the modifier similarity (i.e. S(Ni,1, Bj1)) and S2 is head noun similarity 232 (i.e. S(Ni,2, Bj2)); α ∈ [0, 1] is a weighting factor. The similarity scores are calculated using the method of Wu and Palmer (1994) as implemented in WordNet::Similarity (Patwardhan et al., 2003). This is done for each pairing of WordNet senses of each of the two words in question, and the overall lexical similarity is calculated as the average across the pairwise sense similarities. The final classification is derived from the training instance which has the highest lexical similarity with the test instance in question. 3 Co-Training As with many semantic annotation tasks, SR tagging is a time-consuming and expensive process. At the same time, due to the inherent complexity of the SR interpretation task, we require large</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proc. of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 133–138, Las Cruces, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>