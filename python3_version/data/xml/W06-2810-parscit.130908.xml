<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006352">
<title confidence="0.992963">
Finding Similar Sentences across Multiple Languages in Wikipedia
</title>
<author confidence="0.956169">
Sisay Fissaha Adafre Maarten de Rijke
</author>
<affiliation confidence="0.966398">
ISLA, University of Amsterdam
</affiliation>
<address confidence="0.632848">
Kruislaan 403, 1098 SJ Amsterdam
</address>
<email confidence="0.990631">
sfissaha,mdr@science.uva.nl
</email>
<sectionHeader confidence="0.99541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838">
We investigate whether the Wikipedia cor-
pus is amenable to multilingual analysis
that aims at generating parallel corpora.
We present the results of the application of
two simple heuristics for the identification
of similar text across multiple languages
in Wikipedia. Despite the simplicity of the
methods, evaluation carried out on a sam-
ple of Wikipedia pages shows encouraging
results.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934275862069">
Parallel corpora form the basis of much multilin-
gual research in natural language processing, rang-
ing from developing multilingual lexicons to sta-
tistical machine translation systems. As a conse-
quence, collecting and aligning text corpora writ-
ten in different languages constitutes an important
prerequisite for these research activities.
Wikipedia is a multilingual free online encyclo-
pedia. Currently, it has entries for more than 200
languages, the English Wikipedia being the largest
one with 895,674 articles, and no fewer than eight
language versions having upwards of 100,000 ar-
ticles as of January 2006. As can be seen in Fig-
ure 1, Wikipedia pages for major European lan-
guages have reached a level where they can sup-
port multilingual research. Despite these devel-
opments in its content, research on Wikipedia has
largely focused on monolingual aspects so far; see
e.g., (Voss, 2005) for an overview.
In this paper, we focus on multilingual aspects
of Wikipedia. Particularly, we investigate to what
extent we can use properties of Wikipedia itself
to generate similar sentences acrose different lan-
guages. As usual, we consider two sentences sim-
ilar if they contain (some or a large amount of)
overlapping information. This includes cases in
which sentences may be exact translations of each
other, one sentence may be contained within an-
other, or both share some bits of information.
</bodyText>
<figure confidence="0.990202583333333">
1000000
900000
800000
700000
600000
500000
400000
300000
200000
100000
0
en de fr ja pl it sv nl pt es zh ru no fi da
</figure>
<figureCaption confidence="0.985971">
Figure 1: Wikipedia pages for the top 15 lan-
guages
</figureCaption>
<bodyText confidence="0.999979117647059">
The conceptually simple but fundamental task
of identifying similar sentences across multiple
languages has a number of motivations. For a
start, and as mentioned earlier, sentence aligned
corpora play an important role in corpus based lan-
guage processing methods in general. Second, in
the context of Wikipedia, being able to align sim-
ilar sentences across multiple languages provides
insight into Wikipedia as a knowledge source: to
which extent does a given topic get different kinds
of attention in different languages? And thirdly,
the ability to find similar content in other lan-
guages while creating a page for a topic in one lan-
guage constitutes a useful type of editing support.
Furthermore, finding similar content acrose differ-
ent languages can form the basis for multilingual
summarization and question answering support for
</bodyText>
<page confidence="0.998463">
62
</page>
<bodyText confidence="0.999974712121213">
Wikipedia; at present the latter task is being devel-
oped into a pilot for CLEF 2006 (WiQA, 2006).
There are different approaches for finding sim-
ilar sentences across multiple languages in non-
parallel but comparable corpora. Most methods
for finding similar sentences assume the availabil-
ity of a clean parallel corpus. In Wikipedia, two
versions of a Wikipedia topic in two different lan-
guages are a good starting point for searching sim-
ilar sentences. However, these pages may not al-
ways conform to the typical definitions of a bitext
which current techniques assume. Bitext gener-
ally refers to two versions of a text in two differ-
ent languages (Melamed, 1996). Though it is not
known how information is shared among the dif-
ferent languages in Wikipedia, some pages tend to
be translations of each other whereas the majority
of the pages tend to be written independently of
each other. Therefore, two versions of the same
topic in two different languages can not simply be
taken as parallel corpora. This in turn limits the
application of some of the currently available tech-
niques.
In this paper, we present two approaches for
finding similar sentences across multiple lan-
guages in Wikipedia. The first approach uses
freely available online machine translation re-
sources for translating pages and then carries out
monolingual sentence similarity. The approach
needs a translation system, and these are not avail-
able for every pair of languages in Wikipedia.
This motivates a second approach to finding
similar sentences across multiple languages, one
which uses a bilingual title translation lexicon in-
duced automatically using the link structure of
Wikipedia. Briefly, two sentences are similar if
they link to the same entities (or rather: to pages
about the same entities), and we use Wikipedia it-
self to relate pages about a given entity across mul-
tiple languages. In Wikipedia, pages on the same
topic in different languages are topically closely
related. This means that even if one page is not
a translation of another, they tend to share some
common information. Our underlying assumption
here is that there is a general agreement on the
kind of information that needs to be included in the
pages of different types of topics such as a biogra-
phy of a person, and the definition and description
of a concept etc., and that this agreement is to a
consderable extent “materialized” in the hypertext
links (and their anchor texts) in Wikipedia.
Our main research question in this paper is this:
how do the two methods just outlined differ? A
priori it seems that the translation based approach
to finding similar sentences across multiple lan-
guages will have a higher recall than the link-
based method, while the latter outperforms the for-
mer in terms of precision. Is this correct?
The remainder of the paper is organized as fol-
lows. In Section 2, we briefly discuss related
work. Section 3 provides a detailed description
of Wikipedia as a corpus. The two approaches to
identifying similar sentences across multiple lan-
guages are presented in Section 4. An experimen-
tal evaluation is presented in Section 5. We con-
clude in Section 6.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999765625">
The main focus of this paper lies with multilin-
gual text similarity and its application to infor-
mation access in the context of Wikipedia. Cur-
rent research work related to Wikipedia mostly
describes its monolingual properties (Ciffolilli,
2003; Vi´egas et al., 2004; Lih, 2004; Miller,
2005; Bellomi and Bonato, 2005; Voss, 2005; Fis-
saha Adafre and de Rijke, 2005). This is proba-
bly due to the fact that different language versions
of Wikipedia have different growth rates. Others
describe its application in question answering and
other types of IR systems (Ahn et al., 2005). We
believe that currently, Wikipedia pages for major
European languages have reached a level where
they can support multilingual research.
On the other hand, there is a rich body of knowl-
edge relating to multilingual text similarity. These
include example-based machine translation, cross-
lingual information retrieval, statistical machine
translation, sentence alignment cost functions, and
bilingual phrase translation (Kirk Evans, 2005).
Each approach uses relatively different features
(content and structural features) in identifying
similar text from bilingual corpora. Furthermore,
most methods assume that the bilingual corpora
can be sentence aligned. This assumption does
not hold for our case since our corpus is not par-
allel. In this paper, we use content based fea-
tures for identifying similar text across multilin-
gual corpora. Particularly, we compare bilingual
lexicon and MT system based methods for identi-
fying similar text in Wikipedia.
</bodyText>
<page confidence="0.998295">
63
</page>
<sectionHeader confidence="0.694101" genericHeader="method">
3 Wikipedia as a Multilingual Corpus
</sectionHeader>
<bodyText confidence="0.99999108">
Wikipedia is a free online encyclopedia which is
administered by the non-profit Wikimedia Foun-
dation. The aim of the project is to develop free
encyclopedias for different languages. It is a col-
laborative effort of a community of volunteers, and
its content can be edited by anyone. It is attracting
increasing attention amongst web users and has
joined the top 50 most popular sites.
As of January 1, 2006, there are versions of
Wikipedia in more than 200 languages, with sizes
ranging from 1 to over 800,000 articles. We used
the ascii text version of the English and Dutch
Wikipedia, which are available as database dumps.
Each entry of the encyclopedia (a page in the on-
line version) corresponds to a single line in the text
file. Each line consists of an ID (usually the name
of the entity) followed by its description. The de-
scription part contains the body of the text that de-
scribes the entity. It contains a mixture of plain
text and text with html tags. References to other
Wikipedia pages in the text are marked using “[[”
“]]” which corresponds to a hyperlink on the on-
line version of Wikipedia. Most of the formatting
information which is not relevant for the current
task has been removed.
</bodyText>
<subsectionHeader confidence="0.998188">
3.1 Links within a single language
</subsectionHeader>
<bodyText confidence="0.999954942857143">
Wikipedia is a hypertext document with a rich link
structure. A description of an entity usually con-
tains hypertext links to other pages within or out-
side Wikipedia. The majority of these links cor-
respond to entities, which are related to the en-
tity being described, and have a separate entry
in Wikipedia. These links are used to guide the
reader to a more detailed description of the con-
cept denoted by the anchor text. In other words,
the links in Wikipedia typically indicate a topical
association between the pages, or rather the enti-
ties being described by the pages. E.g., in describ-
ing a particular person, reference will be made to
such entities as country, organization and other im-
portant entities which are related to it and which
themselves have entries in Wikipedia. In general,
due to the peculiar characteristics of an encyclope-
dia corpus, the hyperlinks found in encyclopedia
text are used to exemplify those instances of hy-
perlinks that exist among topically related entities
(Ghani et al., 2001; Rao and Turoff, 1990).
Each Wikipedia page is identified with a unique
ID. These IDs are formed by concatenating the
words of the titles of the Wikipedia pages which
are unique for each page, e.g., the page on Vin-
cent van Gogh has “Vincent van Gogh” as its ti-
tle and “Vincent van Gogh” as its ID. Each page
may, however, be represented by different anchor
texts in a hyperlink. The anchor texts may be sim-
ple morphological variants of the title such as plu-
ral form or may represent closely related seman-
tic concept. For example, the anchor text “Dutch”
may point to the page for the Netherlands. In a
sense, the IDs function as the canonical form for
several related concepts.
</bodyText>
<subsectionHeader confidence="0.999615">
3.2 Links across different languages
</subsectionHeader>
<bodyText confidence="0.999945176470588">
Different versions of a page in different languages
are also hyperlinked. For a given page, transla-
tions of its title in other languages for which pages
exist are given as hyperlinks. This property is par-
ticularly useful for the current task as it helps us to
align the corpus at the page level. Furthermore, it
also allows us to induce bilingual lexicon consist-
ing of the Wikipedia titles. Conceptual mismatch
between the pages (e.g. Roof vs Dakconstructie)
is rare, and the lexicon is generally of high qual-
ity. Unlike the general lexicon, this lexicon con-
tains a relatively large number of names of indi-
viduals and other entities which are highly infor-
mative and hence are useful in identifying similar
text. This lexicon will form the backbone of one
of the methods for identifying similar text across
different languages, as will be shown in Section 4.
</bodyText>
<sectionHeader confidence="0.997453" genericHeader="method">
4 Approaches
</sectionHeader>
<bodyText confidence="0.999982722222222">
We describe two approaches for identifying simi-
lar sentences across different languages. The first
uses an MT system to obtain a rough translation of
a given page in one language into another and then
uses word overlap between sentences as a similar-
ity measure. One advantage of this method is that
it relies on a large lexical resource which is bigger
than what can be extracted from Wikipedia. How-
ever, the translation can be less accurate especially
for the Wikipedia titles which form part of the con-
tent of a page and are very informative.
The second approach relies on a bilingual lexi-
con which is generated from Wikipedia using the
link structure: pages on the same topic in differ-
ent languages are hyperlinked; see Figure 2. We
use the titles of the pages that are linked in this
manner to create a bilingual lexicon. Thus, our
bilingual lexicon consists of terms that represent
</bodyText>
<page confidence="0.997186">
64
</page>
<bodyText confidence="0.999255125">
concepts or entities that have entries in Wikipedia,
and we will represent sentences by entries from
this lexicon: an entry is used to represent the con-
tent of a sentence if the sentence contains a hy-
pertext link to the Wikipedia page for that entry.
Sentence similarity is then captured in terms of the
shared lexicon entries they share. In other words,
the similarity measure that we use in this approach
is based on “concept” or “page title” overlap. In-
tuitively, this approach has the advantage of pro-
ducing a brief but highly accurate representation
of sentences, more accurate, we assume than the
MT approach as the titles carry important seman-
tic information; it will also be more accurate than
the MT approach because the translations of the
titles are done manually.
</bodyText>
<figureCaption confidence="0.6134045">
Figure 2: Links to pages devoted to the same topic
in other languages.
</figureCaption>
<bodyText confidence="0.999883388888889">
Both approaches assume that the Wikipedia cor-
pus is aligned at the page level. This is eas-
ily achieved using the link structure since, again,
pages on the same topic in different languages are
hyperlinked. This, in turns, narrows down the
search for similar text to a page level. Hence, for
a given text of a page (sentence or chunk) in one
language, we search for its equivalent text (sen-
tence or chunk) only in the corresponding page in
the other language, not in the entire corpus.
We now describe the two approaches in more
detail. To remain focused and avoid getting lost
in technical details, we consider only two lan-
guages in our technical descriptions and evalua-
tions below: Dutch and English; it will be clear
from our presentation, however, that our second
approach can be used for any pair of languages in
Wikipedia.
</bodyText>
<subsectionHeader confidence="0.985447">
4.1 An MT based approach
</subsectionHeader>
<bodyText confidence="0.999993975609756">
In this approach, we translate the Dutch Wikipedia
page into English using an online MT system. We
refer to the English page as source and the trans-
lated (Dutch page) version as target. We used the
Babelfish MT system of Altavista. It supports a
number of language pairs among which are Dutch-
English pairs. After both pages have been made
available in English, we split the pages into sen-
tences or text chucks. We then link each text chunk
or sentence in the source to each chuck or sentence
in the target. Following this we compute a simple
word overlap score for each pair. We used the Jac-
card similarity measure for this purpose. Content
words are our main features for the computation
of similarity, hence, we remove stopwords. Gram-
matically correct translations may not be neces-
sary since we are using simple word overlap as our
similarity measure.
The above procedure will generate a large set
of pairs, not all of which will actually be similar.
Therefore, we filter the list assuming a one-to-one
correspondence, where for each source sentence
we identify at most one target sentence. This is
a rather strict criterion (another possibility being
one-to-many), given the fact that the corpus is gen-
erally assumed to be not parallel. But it gives some
idea on how much of the text corpus can be aligned
at smaller units (i.e., sentence or text chunks).
Filtering works as follows. First we sort the
pairs in decreasing order of their similarity scores.
This results in a ranked list of text pairs in which
the most similar pairs are ranked top whereas the
least similar pairs are ranked bottom. Next we take
the top most ranking pair. Since we are assuming
a one-to-one correspondence, we remove all other
pairs ranked lower in the list containing either of
the the sentences or text chunks in the top ranking
pair. We then repeat this process taking the second
top ranking pair. Each step results in a smaller list.
The process continues until there is no more pair
to remove.
</bodyText>
<subsectionHeader confidence="0.999439">
4.2 Using a link-based bilingual lexicon
</subsectionHeader>
<bodyText confidence="0.999966285714286">
As mentioned previously, this approach makes
use of a bilingual lexicon that is generated from
Wikipedia using the link structure. A high level
description of the algorithm is given in Figure 3.
Below, we first describe how the bilingual lexicon
is acquired and how it is used for enriching the link
structure of Wikipedia. Finally, we detail how the
</bodyText>
<page confidence="0.997977">
65
</page>
<listItem confidence="0.9957348">
• Generating bilingual lexicon
• Given a topic, get the corresponding pages
from English and Dutch Wikipedia
• Split pages into sentences and enrich the
hyperlinks in the sentence or identify
named-entities in the pages.
• Represent the sentences in these pages us-
ing the bilingual lexicon.
• Compute term overlap between the sen-
tences thus represented.
</listItem>
<figureCaption confidence="0.994848">
Figure 3: The Pseudo-algorithm for identifying
</figureCaption>
<bodyText confidence="0.63651">
similar sentences using a link-based bilingual lex-
icon.
bilingual lexicon is used for the identification of
similar sentences.
</bodyText>
<subsectionHeader confidence="0.561376">
Generating the bilingual lexicon
</subsectionHeader>
<bodyText confidence="0.999928739130435">
Unlike the MT based approach, which uses con-
tent words from the general vocabulary as fea-
tures, in this approach, we use page titles and their
translations (as obtained through hyperlinks as ex-
plained above) as our primitives for the compu-
tation of multilingual similarity. The first step of
this approach, then, is acquiring the bilingual lexi-
con, but this is relatively straightforward. For each
Wikipedia page in one language, translations of
the title in other languages, for which there are
separate entries, are given as hyperlinks. This in-
formation is used to generate a bilingual transla-
tion lexicon. Most of these titles are content bear-
ing noun phrases and are very useful in multilin-
gual similarity computation (Kirk Evans, 2005).
Most of these noun phrases are already disam-
buiguated, and may consist of either a single word
or multiword units.
Wikipedia uses a redirection facility to map
several titles into a canonical form. These titles
are mostly synonymous expressions. We used
Wikipedia’s redirect feature to identify synony-
mous expression.
</bodyText>
<subsectionHeader confidence="0.698608">
Canonical representation of a sentence
</subsectionHeader>
<bodyText confidence="0.981852517857143">
Once we have the bilingual lexicon, the next step
is to represent the sentences in both language pairs
using this lexicon. Each sentence is represented by
the set of hyperlinks it contains. We search each
hyperlink in the bilingual lexicon. If it is found,
we replace the hyperlink with the corresponding
unique identification of the bilingual lexicon entry.
If it is not found, the hyperlink will be included as
is as part of the representation. This is done since
Dutch and English are closely related languages
and may share many cognate pairs.
Enriching the Wikipedia link structure
As described in the previous section, the method
uses hyperlinks in a sentence as a highly focused
entity-based representation of the aboutness of the
sentence. In Wikipedia, not all occurrences of
named-entities or concepts that have entries in
Wikipedia are actually used as anchor text of a
hypertext link; because of this, a number of sen-
tences may needlessly be left out from the simi-
larity computation process. In order to avoid this
problem, we automatically identify other relevant
hyperlinks using the bilingual lexicon generated in
the previous section.
Identification of additional hyperlinks in
Wikipedia sentences works as follows. First
we split the sentences into constituent words.
We then generate N gram words keeping the
relative order of words in the sentences. Since the
anchor texts of hypertext links may be multiword
expressions, we start with higher order N gram
words (N=4). We search these N grams in the
bilingual lexicon. If the N gram is found in the
lexicon, it is taken as a new hyperlink and will
form part of the representation of a sentence. The
process is repeated for lower order N grams.
Identifying similar sentences
Once we are done representing the sentences as
described previously, the final step involves com-
putation of the term overlap between the sentence
pairs and filtering the resulting list. The remain-
ing steps are similar to those described in the MT
based approach. For completeness, we briefly re-
peat the steps here. First, all sentences from a
Dutch Wikipedia page are linked to all sentences
of the corresponding English Wikipedia page. We
then compute the similarity between the sentence
representations, using the Jaccard similarity coef-
ficient.
A sentence in Dutch page may be similar to
several sentences in English page which may re-
sult in a large number of spurious pairs. There-
fore, we filter the list using the following recursive
procedure. First, the sentence pairs are sorted by
their similarity scores. We take the pairs with the
highest similarity scores. We then eliminate all
</bodyText>
<page confidence="0.979227">
66
</page>
<bodyText confidence="0.9999365">
other sentence pairs from the list that contain ei-
ther of sentences in this pair. We continue this pro-
cess taking the second highest ranking pair. Note
that this procedure assumes a one-to-one matching
rule; a sentences in Dutch can be linked to at most
one sentence in English.
</bodyText>
<sectionHeader confidence="0.991742" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999605">
Now that we have described the two algorithms
for identifying similar sentences, we return to our
research questions. In order to answer them we
run the experiment described below.
</bodyText>
<subsectionHeader confidence="0.99759">
5.1 Set-up
</subsectionHeader>
<bodyText confidence="0.999993529411765">
We took a random sample of 30 English-Dutch
Wikipedia page pairs. Each page is split into sen-
tences. We generated candidate Dutch-English
sentence pairs and passed them on to the two
methods. Both methods return a ranked list of sen-
tence pairs that are similar. As explained above,
we assumed a one-to-one correspondence, i.e., one
English sentence can be linked to at most to one
Dutch sentence.
The outputs of the systems are manually evalu-
ated. We apply a relatively lenient criteria in as-
sessing the results. If two sentences overlap in-
terms of their information content then we con-
sider them to be similar. This includes cases in
which sentences may be exact translation of each
other, one sentence may be contained within an-
other, or both share some bits of information.
</bodyText>
<subsectionHeader confidence="0.89349">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999980898550725">
Table 1 shows the results of the two methods de-
scribed in Section 4. In the table, we give two
types of numbers for each of the two methods
MT and Bilingual lexicon: Total (the total number
of sentence pairs) and Match (the number of cor-
rectly identified sentence pairs) generated by the
two approaches.
Overall, the two approaches tend to produce
similar numbers of correctly identified similar sen-
tence pairs. The systems seem to perform well
on pages which tend to be alignable at sentence
level, i.e., parallel. This is clearly seen on the
following pages: Pierluigi Collina, Marcus Cor-
nelius Fronto, George F. Kennan, which show a
high similarity at sentence level. Some pages con-
tain very small description and hence the figures
for correct similar sentences are also small. Other
topics such as Classicism (Dutch: Classicisme),
Tennis, and Tank, though they are described in suf-
ficient details in both languages, there tends to be
less overlap among the text. The methods tend to
retrieve more accurate similar pairs from person
pages than other pages especially those pages de-
scribing a more abstract concepts. However, this
needs to be tested more thoroughly.
When we look at the total number of sentence
pairs returned, we notice that the bilingual lexi-
con based method consistently returns a smaller
amount of similar sentence pairs which makes
the method more accurate than the MT based ap-
proach. On average, the MT based approach re-
turns 4.5 (26%) correct sentences and the bilingual
lexicon based approach returns 2.9 correct sen-
tences (45%). But, on average, the MT approach
returns three times as many sentence pairs as bilin-
gual lexicon approach. This maybe due to the fact
that the former makes use of restricted set of im-
portant terms or concepts whereas the later uses a
large general lexicon. Though we remove some
of the most frequently occuring stopwords in the
MT based approach, it still generates a large num-
ber of incorrect similar sentence pairs due to some
common words.
In general, the number of correctly identified
similar pages extracted seems small. However,
most of the Dutch pages are relatively small,
which sets the upper bound on the number of
correctly identified sentence pairs that can be ex-
tracted. On average, each Dutch Wikipedia page
in the sample contains 18 sentences whereas En-
glish Wikipedia pages contain 65 sentences. Ex-
cluding the pages for Tennis, Tank (Dutch: vo-
ertuig), and Tricolor, which are relatively large,
each Dutch page contains on average 8 sentences,
which is even smaller. Given the fact that the
pages are in general not parallel, the methods,
using simple heuristics, identified high quality
translation equivalent sentence pairs from most
Wikipedia pages. Furthermore, a close examina-
tion of the output of the two approaches show that
both tend to identify the same set of similar sen-
tence pairs.
We ran our bilingual lexicon based approach on
the whole Dutch-English Wikipedia corpus. The
method returned about 80M of candidate similar
sentences. Though we do not have the resources
to evaluate this output, the results we got from
sample data (cf. Table 1) suggest that it contains
a significant amount of correctly identified similar
</bodyText>
<page confidence="0.997909">
67
</page>
<table confidence="0.999985454545454">
Title MT Bilingual Lexicon
English Dutch Total Match Total Match
Hersfeld Rotenburg Hersfeld Rotenburg 2 3 2
Manganese nodule Mangaanknol 5 2 1 1
Kettle Ketel 1 1
Treason Landverraad 2 1
Pierluigi Collina Pierluigi Collina 14 13 13 11
Province of Ferrara Ferrara (provincie) 7 1 1 1
Classicism Classicisme 8 1
Tennis Tennis 93 4 15 3
Hysteria Hysterie 14 6 9 5
George F. Kennan George Kennan 27 12 29 11
Marcus Cornelius Fronto Marcus Cornelius Fronto 11 9 5 5
Delphi Delphi (Griekenland) 34 2 8 1
De Beers De Beers 11 5 10 5
Pavel Popovich Pavel Popovytsj 7 4 4 4
Rice pudding Rijstebrij 11 1 4
Manta ray Reuzenmanta 15 3 7 2
Michelstadt Michelstadt 1 1 1 1
Tank Tank (voertuig) 84 3 27 2
Cheyenne(Wyoming) Cheyenne(Wyoming) 5 2 2 2
Goa Goa(deelstaat) 13 4 6 1
Tricolour Driekleur 57 36 13 12
Oral cancer Mondkanker 25 2 7 2
Pallium Pallium 12 2 5 4
Ajanta Ajanta 3 3 2 2
Captain Jack (band) Captain Jack 16 3 2 2
Proboscis Monkey Neusaap 15 6 4 1
Patti Smith Patti Smith 6 2 4 2
Flores Island, Portugal Flores (Azoren) 3 2 1 1
Mercury 8 Mercury MA 8 11 3 4 1
Mutation Mutatie 16 4 6 3
Average 17.6 4.5 6.5 2.9
</table>
<tableCaption confidence="0.74074825">
Table 1: Test topics (column 1 and 2). The total number of sentence pairs (column 3) and the number
of correctly identified similar sentence pairs (column 4) returned by the MT based approach. The to-
tal number of sentence pairs (column 5) and the number of correctly identified similar sentence pairs
(column 6) returned by the method using a bilingual lexicon.
</tableCaption>
<bodyText confidence="0.822241">
sentences.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999336380952381">
In this paper we focused on multilingual aspects of
Wikipedia. Particularly, we investigated the poten-
tial of Wikipedia for generating parallel corpora by
applying different methods for identifying similar
text across multiple languages. We presented two
methods and carried out an evaluation on a sam-
ple of Dutch-English Wikipedia pages. The results
show that both methods, using simple heuristics,
were able to identify similar text between the pair
of Wikipedia pages though they differ in accuracy.
The bilingual lexicon approach returns fewer in-
correct pairs than the MT based approach. We
interpret this as saying that our bilingual lexicon
based method provides a more accurate represen-
tation of the aboutness of sentences in Wikipedia
than the MT based approach. Furthermore, the re-
sult we obtained on a sample of Wikipedia pages
and the output of running the bilingual based ap-
proach on the whole Dutch-English gives some in-
dication of the potential of Wikipedia for generat-
ing parallel corpora.
</bodyText>
<page confidence="0.997659">
68
</page>
<bodyText confidence="0.999970625">
As to future work, the sentence similarity de-
tection methods that we considered are not perfect.
E.g., the MT based approach relies on rough trans-
lations; it is important to investigate the contri-
bution of high quality translations. The bilingual
lexicon approach uses only lexical features; other
language specific sentence features might help im-
prove results.
</bodyText>
<sectionHeader confidence="0.998115" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994295">
This research was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project numbers 017.001.190, 220-80-
001, 264-70-050, 612-13-001, 612.000.106,
612.000.207, 612.066.302, 612.069.006, 640.-
001.501, and 640.002.501.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998432392857142">
D. Ahn, V. Jijkoun, G. Mishne, K. M¨uller, M. de Rijke,
and S. Schlobach. 2005. Using Wikipedia at the
TREC QA Track. In E.M. Voorhees and L.P. Buck-
land, editors, The Thirteenth Text Retrieval Confer-
ence (TREC 2004).
F. Bellomi and R. Bonato. 2005. Lex-
ical authorities in an encyclopedic cor-
pus: a case study with wikipedia. URL:
http://www.fran.it/blog/2005/01/
lexical-authorities-in-encyclopedic.
htm%l. Site accessed on June 9, 2005.
A. Ciffolilli. 2003. Phantom authority, selfselective re-
cruitment and retention of members in virtual com-
munities: The case of Wikipedia. First Monday,
8(12).
S. Fissaha Adafre and M. de Rijke. 2005. Discovering
missing links in Wikipedia. In Proceedings of the
Workshop on Link Discovery: Issues, Approaches
and Applications (LinkKDD-2005).
R. Ghani, S. Slattery, and Y. Yang. 2001. Hypertext
categorization using hyperlink patterns and meta
data. In Carla Brodley and Andrea Danyluk, ed-
itors, Proceedings of ICML-01, 18th International
Conference on Machine Learning, pages 178–185.
D. Kirk Evans. 2005. Identifying similarity
in text: Multi-lingual analysis for summariza-
tion. URL: http://www1.cs.columbia.
edu/nlp/theses/dave_evans.pdf. Site
accessed on January 5, 2006.
A. Lih. 2004. Wikipedia as participatory journalism:
Reliable sources? Metrics for evaluating collabora-
tive media as a news resource. In Proceedings of the
5th International Symposium on Online Journalism.
D. Melamed. 1996. A geometric approach to mapping
bitext correspondence. In Eric Brill and Kenneth
Church, editors, Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1–12, Somerset, New Jersey. Association
for Computational Linguistics.
N. Miller. 2005. Wikipedia and the disappearing
“Author”. ETC: A Review of General Semantics,
62(1):37–40.
U. Rao and M. Turoff. 1990. Hypertext functionality:
A theoretical framework. International Journal of
Human-Computer Interaction.
F. Vi´egas, M. Wattenberg, and D. Kushal. 2004.
Studying cooperation and conflict between authors
with history flow visualization. In Proceedings of
the 2004 conference on Human factors in comput-
ing systems.
J. Voss. 2005. Measuring Wikipedia. In Proceedings
10th International Conference of the International
Society for Scientometrics and Informetrics.
WiQA. 2006. Question answering using Wikipedia.
URL: http://ilps.science.uva.nl/
WiQA/. Site accessed on January 5, 2006.
</reference>
<page confidence="0.999289">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961549">
<title confidence="0.999755">Finding Similar Sentences across Multiple Languages in Wikipedia</title>
<author confidence="0.99043">Sisay Fissaha Adafre Maarten de_Rijke</author>
<affiliation confidence="0.999853">ISLA, University of Amsterdam</affiliation>
<address confidence="0.996806">Kruislaan 403, 1098 SJ Amsterdam</address>
<email confidence="0.998108">sfissaha,mdr@science.uva.nl</email>
<abstract confidence="0.997788818181818">We investigate whether the Wikipedia corpus is amenable to multilingual analysis that aims at generating parallel corpora. We present the results of the application of two simple heuristics for the identification of similar text across multiple languages in Wikipedia. Despite the simplicity of the methods, evaluation carried out on a sample of Wikipedia pages shows encouraging results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Ahn</author>
<author>V Jijkoun</author>
<author>G Mishne</author>
<author>K M¨uller</author>
<author>M de Rijke</author>
<author>S Schlobach</author>
</authors>
<title>Using Wikipedia at the TREC QA Track.</title>
<date>2005</date>
<booktitle>The Thirteenth Text Retrieval Conference (TREC</booktitle>
<editor>In E.M. Voorhees and L.P. Buckland, editors,</editor>
<marker>Ahn, Jijkoun, Mishne, M¨uller, de Rijke, Schlobach, 2005</marker>
<rawString>D. Ahn, V. Jijkoun, G. Mishne, K. M¨uller, M. de Rijke, and S. Schlobach. 2005. Using Wikipedia at the TREC QA Track. In E.M. Voorhees and L.P. Buckland, editors, The Thirteenth Text Retrieval Conference (TREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bellomi</author>
<author>R Bonato</author>
</authors>
<title>Lexical authorities in an encyclopedic corpus: a case study with wikipedia.</title>
<date>2005</date>
<note>URL: http://www.fran.it/blog/2005/01/ lexical-authorities-in-encyclopedic. htm%l. Site accessed on</note>
<contexts>
<context position="6481" citStr="Bellomi and Bonato, 2005" startWordPosition="1046" endWordPosition="1049">on 2, we briefly discuss related work. Section 3 provides a detailed description of Wikipedia as a corpus. The two approaches to identifying similar sentences across multiple languages are presented in Section 4. An experimental evaluation is presented in Section 5. We conclude in Section 6. 2 Related Work The main focus of this paper lies with multilingual text similarity and its application to information access in the context of Wikipedia. Current research work related to Wikipedia mostly describes its monolingual properties (Ciffolilli, 2003; Vi´egas et al., 2004; Lih, 2004; Miller, 2005; Bellomi and Bonato, 2005; Voss, 2005; Fissaha Adafre and de Rijke, 2005). This is probably due to the fact that different language versions of Wikipedia have different growth rates. Others describe its application in question answering and other types of IR systems (Ahn et al., 2005). We believe that currently, Wikipedia pages for major European languages have reached a level where they can support multilingual research. On the other hand, there is a rich body of knowledge relating to multilingual text similarity. These include example-based machine translation, crosslingual information retrieval, statistical machine</context>
</contexts>
<marker>Bellomi, Bonato, 2005</marker>
<rawString>F. Bellomi and R. Bonato. 2005. Lexical authorities in an encyclopedic corpus: a case study with wikipedia. URL: http://www.fran.it/blog/2005/01/ lexical-authorities-in-encyclopedic. htm%l. Site accessed on June 9, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ciffolilli</author>
</authors>
<title>Phantom authority, selfselective recruitment and retention of members in virtual communities: The case of Wikipedia.</title>
<date>2003</date>
<journal>First Monday,</journal>
<volume>8</volume>
<issue>12</issue>
<contexts>
<context position="6408" citStr="Ciffolilli, 2003" startWordPosition="1036" endWordPosition="1037">ect? The remainder of the paper is organized as follows. In Section 2, we briefly discuss related work. Section 3 provides a detailed description of Wikipedia as a corpus. The two approaches to identifying similar sentences across multiple languages are presented in Section 4. An experimental evaluation is presented in Section 5. We conclude in Section 6. 2 Related Work The main focus of this paper lies with multilingual text similarity and its application to information access in the context of Wikipedia. Current research work related to Wikipedia mostly describes its monolingual properties (Ciffolilli, 2003; Vi´egas et al., 2004; Lih, 2004; Miller, 2005; Bellomi and Bonato, 2005; Voss, 2005; Fissaha Adafre and de Rijke, 2005). This is probably due to the fact that different language versions of Wikipedia have different growth rates. Others describe its application in question answering and other types of IR systems (Ahn et al., 2005). We believe that currently, Wikipedia pages for major European languages have reached a level where they can support multilingual research. On the other hand, there is a rich body of knowledge relating to multilingual text similarity. These include example-based mac</context>
</contexts>
<marker>Ciffolilli, 2003</marker>
<rawString>A. Ciffolilli. 2003. Phantom authority, selfselective recruitment and retention of members in virtual communities: The case of Wikipedia. First Monday, 8(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fissaha Adafre</author>
<author>M de Rijke</author>
</authors>
<title>Discovering missing links in Wikipedia.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Link Discovery: Issues, Approaches and Applications (LinkKDD-2005).</booktitle>
<marker>Adafre, de Rijke, 2005</marker>
<rawString>S. Fissaha Adafre and M. de Rijke. 2005. Discovering missing links in Wikipedia. In Proceedings of the Workshop on Link Discovery: Issues, Approaches and Applications (LinkKDD-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ghani</author>
<author>S Slattery</author>
<author>Y Yang</author>
</authors>
<title>Hypertext categorization using hyperlink patterns and meta data.</title>
<date>2001</date>
<booktitle>In Carla Brodley and Andrea Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="9989" citStr="Ghani et al., 2001" startWordPosition="1623" endWordPosition="1626">enoted by the anchor text. In other words, the links in Wikipedia typically indicate a topical association between the pages, or rather the entities being described by the pages. E.g., in describing a particular person, reference will be made to such entities as country, organization and other important entities which are related to it and which themselves have entries in Wikipedia. In general, due to the peculiar characteristics of an encyclopedia corpus, the hyperlinks found in encyclopedia text are used to exemplify those instances of hyperlinks that exist among topically related entities (Ghani et al., 2001; Rao and Turoff, 1990). Each Wikipedia page is identified with a unique ID. These IDs are formed by concatenating the words of the titles of the Wikipedia pages which are unique for each page, e.g., the page on Vincent van Gogh has “Vincent van Gogh” as its title and “Vincent van Gogh” as its ID. Each page may, however, be represented by different anchor texts in a hyperlink. The anchor texts may be simple morphological variants of the title such as plural form or may represent closely related semantic concept. For example, the anchor text “Dutch” may point to the page for the Netherlands. In</context>
</contexts>
<marker>Ghani, Slattery, Yang, 2001</marker>
<rawString>R. Ghani, S. Slattery, and Y. Yang. 2001. Hypertext categorization using hyperlink patterns and meta data. In Carla Brodley and Andrea Danyluk, editors, Proceedings of ICML-01, 18th International Conference on Machine Learning, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kirk Evans</author>
</authors>
<title>Identifying similarity in text: Multi-lingual analysis for summarization.</title>
<date>2005</date>
<booktitle>URL: http://www1.cs.columbia. edu/nlp/theses/dave_evans.pdf. Site accessed on</booktitle>
<contexts>
<context position="7181" citStr="Evans, 2005" startWordPosition="1152" endWordPosition="1153">different language versions of Wikipedia have different growth rates. Others describe its application in question answering and other types of IR systems (Ahn et al., 2005). We believe that currently, Wikipedia pages for major European languages have reached a level where they can support multilingual research. On the other hand, there is a rich body of knowledge relating to multilingual text similarity. These include example-based machine translation, crosslingual information retrieval, statistical machine translation, sentence alignment cost functions, and bilingual phrase translation (Kirk Evans, 2005). Each approach uses relatively different features (content and structural features) in identifying similar text from bilingual corpora. Furthermore, most methods assume that the bilingual corpora can be sentence aligned. This assumption does not hold for our case since our corpus is not parallel. In this paper, we use content based features for identifying similar text across multilingual corpora. Particularly, we compare bilingual lexicon and MT system based methods for identifying similar text in Wikipedia. 63 3 Wikipedia as a Multilingual Corpus Wikipedia is a free online encyclopedia whic</context>
<context position="17846" citStr="Evans, 2005" startWordPosition="2968" endWordPosition="2969">tles and their translations (as obtained through hyperlinks as explained above) as our primitives for the computation of multilingual similarity. The first step of this approach, then, is acquiring the bilingual lexicon, but this is relatively straightforward. For each Wikipedia page in one language, translations of the title in other languages, for which there are separate entries, are given as hyperlinks. This information is used to generate a bilingual translation lexicon. Most of these titles are content bearing noun phrases and are very useful in multilingual similarity computation (Kirk Evans, 2005). Most of these noun phrases are already disambuiguated, and may consist of either a single word or multiword units. Wikipedia uses a redirection facility to map several titles into a canonical form. These titles are mostly synonymous expressions. We used Wikipedia’s redirect feature to identify synonymous expression. Canonical representation of a sentence Once we have the bilingual lexicon, the next step is to represent the sentences in both language pairs using this lexicon. Each sentence is represented by the set of hyperlinks it contains. We search each hyperlink in the bilingual lexicon. </context>
</contexts>
<marker>Evans, 2005</marker>
<rawString>D. Kirk Evans. 2005. Identifying similarity in text: Multi-lingual analysis for summarization. URL: http://www1.cs.columbia. edu/nlp/theses/dave_evans.pdf. Site accessed on January 5, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lih</author>
</authors>
<title>Wikipedia as participatory journalism: Reliable sources? Metrics for evaluating collaborative media as a news resource.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th International Symposium on Online Journalism.</booktitle>
<contexts>
<context position="6441" citStr="Lih, 2004" startWordPosition="1042" endWordPosition="1043">ized as follows. In Section 2, we briefly discuss related work. Section 3 provides a detailed description of Wikipedia as a corpus. The two approaches to identifying similar sentences across multiple languages are presented in Section 4. An experimental evaluation is presented in Section 5. We conclude in Section 6. 2 Related Work The main focus of this paper lies with multilingual text similarity and its application to information access in the context of Wikipedia. Current research work related to Wikipedia mostly describes its monolingual properties (Ciffolilli, 2003; Vi´egas et al., 2004; Lih, 2004; Miller, 2005; Bellomi and Bonato, 2005; Voss, 2005; Fissaha Adafre and de Rijke, 2005). This is probably due to the fact that different language versions of Wikipedia have different growth rates. Others describe its application in question answering and other types of IR systems (Ahn et al., 2005). We believe that currently, Wikipedia pages for major European languages have reached a level where they can support multilingual research. On the other hand, there is a rich body of knowledge relating to multilingual text similarity. These include example-based machine translation, crosslingual in</context>
</contexts>
<marker>Lih, 2004</marker>
<rawString>A. Lih. 2004. Wikipedia as participatory journalism: Reliable sources? Metrics for evaluating collaborative media as a news resource. In Proceedings of the 5th International Symposium on Online Journalism.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Melamed</author>
</authors>
<title>A geometric approach to mapping bitext correspondence.</title>
<date>1996</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--12</pages>
<editor>In Eric Brill and Kenneth Church, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="3670" citStr="Melamed, 1996" startWordPosition="582" endWordPosition="583">developed into a pilot for CLEF 2006 (WiQA, 2006). There are different approaches for finding similar sentences across multiple languages in nonparallel but comparable corpora. Most methods for finding similar sentences assume the availability of a clean parallel corpus. In Wikipedia, two versions of a Wikipedia topic in two different languages are a good starting point for searching similar sentences. However, these pages may not always conform to the typical definitions of a bitext which current techniques assume. Bitext generally refers to two versions of a text in two different languages (Melamed, 1996). Though it is not known how information is shared among the different languages in Wikipedia, some pages tend to be translations of each other whereas the majority of the pages tend to be written independently of each other. Therefore, two versions of the same topic in two different languages can not simply be taken as parallel corpora. This in turn limits the application of some of the currently available techniques. In this paper, we present two approaches for finding similar sentences across multiple languages in Wikipedia. The first approach uses freely available online machine translatio</context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>D. Melamed. 1996. A geometric approach to mapping bitext correspondence. In Eric Brill and Kenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1–12, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Miller</author>
</authors>
<title>Wikipedia and the disappearing “Author”.</title>
<date>2005</date>
<journal>ETC: A Review of General Semantics,</journal>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="6455" citStr="Miller, 2005" startWordPosition="1044" endWordPosition="1045">lows. In Section 2, we briefly discuss related work. Section 3 provides a detailed description of Wikipedia as a corpus. The two approaches to identifying similar sentences across multiple languages are presented in Section 4. An experimental evaluation is presented in Section 5. We conclude in Section 6. 2 Related Work The main focus of this paper lies with multilingual text similarity and its application to information access in the context of Wikipedia. Current research work related to Wikipedia mostly describes its monolingual properties (Ciffolilli, 2003; Vi´egas et al., 2004; Lih, 2004; Miller, 2005; Bellomi and Bonato, 2005; Voss, 2005; Fissaha Adafre and de Rijke, 2005). This is probably due to the fact that different language versions of Wikipedia have different growth rates. Others describe its application in question answering and other types of IR systems (Ahn et al., 2005). We believe that currently, Wikipedia pages for major European languages have reached a level where they can support multilingual research. On the other hand, there is a rich body of knowledge relating to multilingual text similarity. These include example-based machine translation, crosslingual information retr</context>
</contexts>
<marker>Miller, 2005</marker>
<rawString>N. Miller. 2005. Wikipedia and the disappearing “Author”. ETC: A Review of General Semantics, 62(1):37–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Rao</author>
<author>M Turoff</author>
</authors>
<title>Hypertext functionality: A theoretical framework.</title>
<date>1990</date>
<journal>International Journal of Human-Computer Interaction.</journal>
<contexts>
<context position="10012" citStr="Rao and Turoff, 1990" startWordPosition="1627" endWordPosition="1630"> text. In other words, the links in Wikipedia typically indicate a topical association between the pages, or rather the entities being described by the pages. E.g., in describing a particular person, reference will be made to such entities as country, organization and other important entities which are related to it and which themselves have entries in Wikipedia. In general, due to the peculiar characteristics of an encyclopedia corpus, the hyperlinks found in encyclopedia text are used to exemplify those instances of hyperlinks that exist among topically related entities (Ghani et al., 2001; Rao and Turoff, 1990). Each Wikipedia page is identified with a unique ID. These IDs are formed by concatenating the words of the titles of the Wikipedia pages which are unique for each page, e.g., the page on Vincent van Gogh has “Vincent van Gogh” as its title and “Vincent van Gogh” as its ID. Each page may, however, be represented by different anchor texts in a hyperlink. The anchor texts may be simple morphological variants of the title such as plural form or may represent closely related semantic concept. For example, the anchor text “Dutch” may point to the page for the Netherlands. In a sense, the IDs funct</context>
</contexts>
<marker>Rao, Turoff, 1990</marker>
<rawString>U. Rao and M. Turoff. 1990. Hypertext functionality: A theoretical framework. International Journal of Human-Computer Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Vi´egas</author>
<author>M Wattenberg</author>
<author>D Kushal</author>
</authors>
<title>Studying cooperation and conflict between authors with history flow visualization.</title>
<date>2004</date>
<booktitle>In Proceedings of the</booktitle>
<marker>Vi´egas, Wattenberg, Kushal, 2004</marker>
<rawString>F. Vi´egas, M. Wattenberg, and D. Kushal. 2004. Studying cooperation and conflict between authors with history flow visualization. In Proceedings of the 2004 conference on Human factors in computing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Voss</author>
</authors>
<title>Measuring Wikipedia.</title>
<date>2005</date>
<booktitle>In Proceedings 10th International Conference of the International Society for Scientometrics and Informetrics.</booktitle>
<contexts>
<context position="1497" citStr="Voss, 2005" startWordPosition="224" endWordPosition="225">es an important prerequisite for these research activities. Wikipedia is a multilingual free online encyclopedia. Currently, it has entries for more than 200 languages, the English Wikipedia being the largest one with 895,674 articles, and no fewer than eight language versions having upwards of 100,000 articles as of January 2006. As can be seen in Figure 1, Wikipedia pages for major European languages have reached a level where they can support multilingual research. Despite these developments in its content, research on Wikipedia has largely focused on monolingual aspects so far; see e.g., (Voss, 2005) for an overview. In this paper, we focus on multilingual aspects of Wikipedia. Particularly, we investigate to what extent we can use properties of Wikipedia itself to generate similar sentences acrose different languages. As usual, we consider two sentences similar if they contain (some or a large amount of) overlapping information. This includes cases in which sentences may be exact translations of each other, one sentence may be contained within another, or both share some bits of information. 1000000 900000 800000 700000 600000 500000 400000 300000 200000 100000 0 en de fr ja pl it sv nl </context>
<context position="6493" citStr="Voss, 2005" startWordPosition="1050" endWordPosition="1051">elated work. Section 3 provides a detailed description of Wikipedia as a corpus. The two approaches to identifying similar sentences across multiple languages are presented in Section 4. An experimental evaluation is presented in Section 5. We conclude in Section 6. 2 Related Work The main focus of this paper lies with multilingual text similarity and its application to information access in the context of Wikipedia. Current research work related to Wikipedia mostly describes its monolingual properties (Ciffolilli, 2003; Vi´egas et al., 2004; Lih, 2004; Miller, 2005; Bellomi and Bonato, 2005; Voss, 2005; Fissaha Adafre and de Rijke, 2005). This is probably due to the fact that different language versions of Wikipedia have different growth rates. Others describe its application in question answering and other types of IR systems (Ahn et al., 2005). We believe that currently, Wikipedia pages for major European languages have reached a level where they can support multilingual research. On the other hand, there is a rich body of knowledge relating to multilingual text similarity. These include example-based machine translation, crosslingual information retrieval, statistical machine translation</context>
</contexts>
<marker>Voss, 2005</marker>
<rawString>J. Voss. 2005. Measuring Wikipedia. In Proceedings 10th International Conference of the International Society for Scientometrics and Informetrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WiQA</author>
</authors>
<title>Question answering using Wikipedia. URL: http://ilps.science.uva.nl/ WiQA/. Site accessed on</title>
<date>2006</date>
<contexts>
<context position="3105" citStr="WiQA, 2006" startWordPosition="490" endWordPosition="491">o align similar sentences across multiple languages provides insight into Wikipedia as a knowledge source: to which extent does a given topic get different kinds of attention in different languages? And thirdly, the ability to find similar content in other languages while creating a page for a topic in one language constitutes a useful type of editing support. Furthermore, finding similar content acrose different languages can form the basis for multilingual summarization and question answering support for 62 Wikipedia; at present the latter task is being developed into a pilot for CLEF 2006 (WiQA, 2006). There are different approaches for finding similar sentences across multiple languages in nonparallel but comparable corpora. Most methods for finding similar sentences assume the availability of a clean parallel corpus. In Wikipedia, two versions of a Wikipedia topic in two different languages are a good starting point for searching similar sentences. However, these pages may not always conform to the typical definitions of a bitext which current techniques assume. Bitext generally refers to two versions of a text in two different languages (Melamed, 1996). Though it is not known how inform</context>
</contexts>
<marker>WiQA, 2006</marker>
<rawString>WiQA. 2006. Question answering using Wikipedia. URL: http://ilps.science.uva.nl/ WiQA/. Site accessed on January 5, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>