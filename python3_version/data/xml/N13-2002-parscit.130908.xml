<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000086">
<title confidence="0.959141">
Reducing Annotation Effort on Unbalanced Corpus based on Cost Matrix
</title>
<author confidence="0.998789">
Wencan Luo, Diane Litman
</author>
<affiliation confidence="0.901524333333333">
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.99943">
{wencan,litman}@cs.pitt.edu
</email>
<author confidence="0.993802">
Joel Chan
</author>
<affiliation confidence="0.947258333333333">
Department of Psychology
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.989406">
chozen86@gmail.com
</email>
<sectionHeader confidence="0.99855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999778954545454">
Annotated corpora play a significant role in
many NLP applications. However, annota-
tion by humans is time-consuming and costly.
In this paper, a high recall predictor based
on a cost-sensitive learner is proposed as a
method to semi-automate the annotation of
unbalanced classes. We demonstrate the ef-
fectiveness of our approach in the context of
one form of unbalanced task: annotation of
transcribed human-human dialogues for pres-
ence/absence of uncertainty. In two data
sets, our cost-matrix based method of uncer-
tainty annotation achieved high levels of re-
call while maintaining acceptable levels of ac-
curacy. The method is able to reduce human
annotation effort by about 80% without a sig-
nificant loss in data quality, as demonstrated
by an extrinsic evaluation showing that results
originally achieved using manually-obtained
uncertainty annotations can be replicated us-
ing semi-automatically obtained uncertainty
annotations.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999322">
Annotated corpora are crucial for the development
of statistical-based NLP tools. However, the annota-
tion of corpora is most commonly done by humans,
which is time-consuming and costly. To obtain a
higher quality annotated corpus, it is necessary to
spend more time and money on data annotation. For
this reason, one often has to accept some tradeoff
between data quality and human effort.
A significant proportion of corpora are unbal-
anced, where the distribution of class categories are
</bodyText>
<page confidence="0.957896">
8
</page>
<bodyText confidence="0.9995955">
heavily skewed towards one or a few categories. Un-
balanced corpora are common in a number of dif-
ferent tasks, such as emotion detection (Ang et
al., 2002; Alm et al., 2005), sentiment classifica-
tion (Li et al., 2012), polarity of opinion (Carvalho
et al., 2011), uncertainty and correctness of student
answers in tutoring dialogue systems (Forbes-Riley
and Litman, 2011; Dzikovska et al., 2012), text
classification (Forman, 2003), information extrac-
tion (Hoffmann et al., 2011), and so on1.
In this paper, we present a semi-automated anno-
tation method that can reduce annotation effort for
the class of binary unbalanced corpora. Here is our
proposed annotation scheme: the first step is to build
a high-recall classifier with some initial annotated
data with an acceptable accuracy via a cost-sensitive
approach. The second step is to apply this classifier
to the rest of the unlabeled data, where the data are
then classified with positive or negative labels. The
last step is to manually check every positive label
and correct it if it is wrong.
To apply this method to work in practice, two re-
search questions have to be addressed. The first one
is how to get a high-recall classifier. High recall
means only a low proportion of true positives are
misclassified (false negatives). This property allows
for only positive labels to be corrected by human an-
notators in the third step, so that annotation effort
may be reduced. A related and separate research
question concerns the overall quality of data when
false negatives are not corrected: will a dataset anno-
tated with this method produce the same results as a
</bodyText>
<footnote confidence="0.9614825">
1The unbalanced degrees - proportion of minority class cat-
egory, of these corpora range from 3% to 24%.
</footnote>
<note confidence="0.9352705">
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 8–15,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999744333333333">
fully manually annotated version of the same dataset
when analyzed for substantive research questions?
In this paper, we will answer the two research
questions in the context of one form of binary un-
balanced task2: annotation of transcribed human-
human dialogue for presence/absence of uncertainty.
The contribution of this paper is twofold. First,
an extrinsic evaluation demonstrates the utility of
our approach, by showing that results originally
achieved using manually-obtained uncertainty anno-
tations can be replicated using semi-automatically
obtained uncertainty annotations. Second, a high
recall predictor based on a cost-sensitive learner is
proposed as a method to semi-automate the annota-
tion of unbalanced classes such as uncertainty.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.997807">
2.1 Reducing Annotation Effort
</subsectionHeader>
<bodyText confidence="0.998686444444444">
A number of semi-supervised learning methods have
been proposed in the literature for reducing annota-
tion effort, such as active learning (Cohn et al., 1994;
Zhu and Hovy, 2007; Zhu et al., 2010), co-training
(Blum and Mitchell, 1998) and self-training (Mihal-
cea, 2004). Active learning reduces annotation by
carefully selecting more useful samples. Co-training
relies on several conditional independent classifiers
to tag new unlabeled data and self-training takes
the advantage of full unlabeled data. These semi-
supervised learning methods demonstrate that with
a small proportion of annotated data, a classifier can
achieve comparable performance with all annotated
data. However, these approaches still need consid-
erable annotation effort when a large corpus has to
be annotated. In that case, all predicted labels have
to be rechecked by humans manually. In addition,
none of them take advantage of unbalanced data.
Another class of effort reduction techniques is
pre-annotation, which uses supervised machine-
learning systems to automatically assign labels to
the whole data and subsequently lets human anno-
tators correct them (Brants and Plaehn, 2000; Chiou
et al., 2001; Xue et al., 2002; Ganchev et al., 2007;
Chou et al., 2006; Rehbein et al., 2012).
Generally speaking, our annotation method be-
longs to the class of pre-annotation methods. How-
</bodyText>
<footnote confidence="0.664208">
2This annotation scheme can also benefit other kinds of
tasks.
</footnote>
<bodyText confidence="0.999970428571429">
ever, our method improves pre-annotation for unbal-
anced data in two ways. Firstly, we lower the thresh-
old for achieving a high recall classifier. Secondly,
with pre-annotation, although people only perform
a binary decision of whether the automatic classifier
is either right or wrong, they have to go through all
the unlabeled data one by one. In contrast, in our
scheme, people go through only the positive predic-
tions, which are much less than the whole unlabeled
data, due to the unbalanced structure of the data.
What’s more, reducing the annotation effort is the
goal of this paper but not building a high recall clas-
sifier such as Prabhakaran et al. (2012) and Ambati
et al. (2010).
The approach proposed by Tetreault and
Chodorow (2008) is similar to us. However, they
assumed they had a high recall classifier but did not
explicitly show how to build it. In addition, they
did not provide extrinsic evaluation to see whether a
corpus generated by pre-annotation is good enough
to be used in real applications.
</bodyText>
<subsectionHeader confidence="0.998695">
2.2 Uncertainty Prediction
</subsectionHeader>
<bodyText confidence="0.99941388">
Uncertainty is a lack of knowledge about internal
state (Pon-Barry and Shieber, 2011). In this paper,
we only focus on detection of uncertainty on text.
Commonly used features are lexical features such as
unigram (Forbes-Riley and Litman, 2011). More-
over, energy, dialogue features such as turn number,
tutor goal, and metadata like gender are also con-
sidered by Forbes-Riley and Litman (2011). Un-
certainty prediction is both substantively interesting
(Chan et al., 2012; Forbes-Riley and Litman, 2009)
and pragmatically expeditious for our purposes, due
to its binary classification and typical unbalanced
class structure.
CoNLL 2010 has launched a shared task to de-
tect hedges and their scope in natural language text
on two data sets: BioScope and Wikipedia (CoNLL,
2010). This first task to detect whether there is a
hedge present or not present in a sentence is very
similar to our uncertainty prediction task. 23 teams
participated in the shared task with the best re-
call of 0.8772 on the BioScope, and 0.5528 on the
Wikipedia. As we can see, uncertainty detection is
not trivial and it can be hard to get a high recall clas-
sifier.
In this paper, we focus on lexical features for our
</bodyText>
<page confidence="0.995163">
9
</page>
<bodyText confidence="0.999993166666667">
purpose because lexical features are simple to ex-
tract and sufficient for our scheme. Even though
other features may improve uncertainty prediction
performance, with the goal of reducing annotation
effort, such lexical features are shown to be good
enough for our task.
</bodyText>
<sectionHeader confidence="0.987591" genericHeader="method">
3 The Corpora
</sectionHeader>
<bodyText confidence="0.999987947368421">
We examine the following two data sets: the Mars
Exploration Rover (MER) mission (Tollinger et al.,
2006; Paletz and Schunn, 2011) and the student
engineering team (Eng) dataset (Jang and Schunn,
2012). The MER scientists are evaluating data
downloaded from the Rover, discussing their work
process, and/or making plans for the Rovers. They
come from a large team of about 100+ scien-
tists/faculty, graduate students, and technicians. At
any one time, conversations are between 2-10 peo-
ple. The Eng teams are natural teams of college un-
dergraduates working on their semester-long prod-
uct design projects. The conversations involve 2-6
individuals. Audio and video are available for both
data sets and transcripts are obtained with human an-
notators.
Our task is to annotate the transcribed human-
human dialogues for presence/absence of uncer-
tainty in each utterance. There are 12,331 tran-
scribed utterances in the MER data set, and 44,199
transcribed utterances in the Eng data set. Both data
sets are unbalanced: in the MER data, 1641 of all
the 12,331 (13.3%) utterances are annotated as un-
certain by trained human annotators; in the Eng data,
only 1558 utterances are annotated, 221 of which are
annotated as uncertain (14.2%). 96.5% of the utter-
ances in the Eng data set have not been annotated
yet, raising the need for an efficient annotated tech-
nique. Both data sets are annotated by two trained
coders with high inter-rater agreement, at Cohen’s
kappa of 0.75 (Cohen, 1960). A sample dialogue
snippet from the MER corpus is shown in Table 1.
The last column indicates whether the utterance is
labeled as uncertainty or not: ‘1’ means uncertainty
and ‘0’ means certainty.
The MER data serves as the initial annotated set
and a high recall classifier will be trained on it; the
Eng data3 serves as a simulated unlabeled data set to
</bodyText>
<footnote confidence="0.833977">
3The Eng data in this paper denotes the annotated subset of
</footnote>
<bodyText confidence="0.743833875">
utterance uncertainty?
You can’t see the forest through the trees.
Yea, we never could see the [missing words]
No we had to get above it
We just went right through it
Yea
I still don’t,
I’m not quite sure
</bodyText>
<tableCaption confidence="0.978095">
Table 1: Sample dialogue from the MER corpus
</tableCaption>
<bodyText confidence="0.487759">
test the performance of our annotation scheme.
</bodyText>
<sectionHeader confidence="0.95004" genericHeader="method">
4 High Recall Classifier
</sectionHeader>
<subsectionHeader confidence="0.998857">
4.1 Basic Classifier
</subsectionHeader>
<bodyText confidence="0.986181514285714">
The uncertainty prediction problem can be viewed
as a binary classification problem. It involves two
steps to build a high recall classifier for unbalanced
data. The first step is to build up a simple classifier;
the second step is to augment this classifier to favor
high recall.
Aiming for a simple classifier with high recall,
only some lexical words/phrases are used as fea-
tures here. There are several resources for the
words/phrases of uncertainty prediction. The main
resource is a guideline book used by our annotators
showing how to distinguish uncertainty utterance. It
gives three different kinds of words/phrases, shown
in Table 2 indicated by three superscripts ‘+’, ‘-’
and ‘*’. The words/phrases with ‘+’ show some
evidence of uncertainty; ones with ‘-’ mean that
they show no evidence of uncertainty; others with
‘*’ may or may not show uncertainty. The second
source is from existing literature. The words/phrases
with ‘1’ are from (Hiraishi et al., 2000) and ones
with ‘2’ are from (Holms, 1999).
For each word/phrase w, a binary feature is used
to indicate whether the word/phrase w is in the ut-
terance or not.
A Naive Bayes classifier is trained on the MER
data using these features and tested on the Eng data.
The performances of the model on the train set and
test set are shown in Table 3. Both weighted and un-
weighted false positive (FP) Rate, Precision, Recall
and F-Measure are reported. However, in later ex-
periments, we will focus on only the positive class
(the uncertainty class). A 0.689 recall means that
510 out of 1641 positive utterances are missed using
this model.
the original Eng corpus.
</bodyText>
<figure confidence="0.994775785714286">
speaker
S6
S1
S6
S4
S6
S1
0
1
0
0
0
0
1
</figure>
<page confidence="0.894095">
10
</page>
<table confidence="0.94967855">
as far as+ i hope+ somehow+ it will− don’t remember∗ maybe∗ tends to∗ doubtful1
as far as i know+ i think+ something+ it wont− essentially∗ most∗ that can vary∗ good chance1
as far as we know+ i thought+ something like this+ it would− fairly∗ mostly∗ typically∗ improbable1
believe+ i wont+ worried that+ would it be− for the most part∗ normally∗ uh∗ possible1
could+ im not sure+ you cannot tell+ about∗ frequently∗ pretty much∗ um∗ probable1
guess+ may+ can− almost∗ generally∗ quite∗ usually∗ relatively1
guessed+ might+ i am− any nonprecise amount∗ hes∗ should∗ very∗ roughly1
guessing+ not really+ i can− basically∗ hopefully∗ sometimes∗ virtually∗ tossup1
i believe+ not sure+ i will− believed∗ i assumed that∗ somewhat∗ whatever∗ unlikely1
i cant really+ possibly+ i would− cannot remember∗ it sounds as∗ somewhere∗ you know∗ of course2
i feel+ probably+ it can− can’t remember∗ kind of∗ stuff∗ almost certain1 sort of2
i guess+ really+ it is− do not remember∗ likely∗ tend to∗ almost impossible1
Table 2: Words/phrases for uncertainty prediction.
Data Set FP Rate Precision Recall F-Measure Class
.311 .954 .989 .971 0
MER .011 .908 .689 .784 1
.271 .948 .949 .946 (Weighted)
.475 .926 .981 .952 0
Eng .019 .817 .525 .639 1
.41 .91 .916 .803 (Weighted)
</table>
<tableCaption confidence="0.9812635">
Table 3: Naive Bayes classifier performance on the MER
(train set) and Eng (test set) with only the words/phrases
</tableCaption>
<table confidence="0.9887045">
assume I didn’t know more or less some kind
couldn’t i don’t even know no idea suppose
don’t know if not clear suspect
don’t think if it or think
don’t understand if we perhaps thought
doubt if you possibility unclear
either imagine potential what i understood
figured kinda presumably wondering
i bet kinds of seem
i can try like some
</table>
<tableCaption confidence="0.999448">
Table 4: New words/phrases for uncertainty prediction
</tableCaption>
<bodyText confidence="0.99973075">
After error analysis, a few new words/phrases are
added to the feature set, shown in Table 4. By sup-
plementing the original feature set in this way, we
reran the training yielding our final baseline, the
performance on the training data (MER) and test-
ing data (Eng) is shown in Table 5. This time, we
compare different classifiers including Naive Bayes
(NB), Decision Tree (DT) and Support Vector Ma-
chine (SVM). All of them are implemented using the
open source platform Weka (Hall et al., 2009) with
default parameters.
As we can see, test recall is worse than train recall.
</bodyText>
<table confidence="0.999550285714286">
Data Set Method TP FP Precision Recall F-Measure
NB .732 .016 .875 .732 .797
MER DT .831 .013 .908 .831 .868
SVM .811 .013 .905 .811 .855
NB .679 .014 .888 .679 .769
Eng DT .665 .021 .84 .665 .742
SVM .674 .022 .832 .674 .745
</table>
<tableCaption confidence="0.998754">
Table 5: Performance with original and new
</tableCaption>
<bodyText confidence="0.792781666666667">
words/phrases as a feature set: train on the MER
and test on the Eng data for class ‘1’. TP is true positive;
FP is false positive
In addition, although DT and SVM perform better
than NB on train data set, they have similar perfor-
mance on the test set. Thus, the performance of the
baseline is not unacceptable, but neither is it stellar.
In advance, it is not hard to build such a model, since
only simple features and classifiers are used here.
</bodyText>
<subsectionHeader confidence="0.681369">
4.2 Augmenting the Classifier using a Cost
Matrix
</subsectionHeader>
<bodyText confidence="0.997219714285714">
In our annotation framework, if the classifier
achieves 100% recall, the annotated data will be per-
fect because all the wrong predictions can be cor-
rected. That’s the reason why we are seeking for a
high recall classifier. A confusion matrix, is a com-
mon way to represent classifier performance. High
recall is indexed by a low false negative (FN) rate;
therefore, we aim to minimize FNs to achieve high
recall.
Following this idea, we employ a cost-sensitive
model, where the cost of FN is more than false pos-
itive (FP).
Following the same notation, we represent our
cost-sensitive classifier as a cost matrix. In our cost
matrix, classifying an actual class ‘1’ as ‘1’ costs
Ctp, an actual class ‘0’ as ‘1’ costs Cfp, an actual
class ‘1’ to ‘0’ costs Cfn, and ‘0’ to ‘0’ costs Ctn.
To achieve a high recall, Cfn should be more than
Cfp.
We can easily achieve 100% recall by classifying
all samples to ‘1’, but this would defeat our goal of
reducing human annotation effort, since all utterance
uncertainty predictions would need to be manually
corrected. Thus, at the same time of a high recall,
we should also balance the total ratio of TP and FP.
In our experiment, Ctp and Ctn are set to 0 since
they are perfectly correct. Additionally, Cfp = 1 all
the time and Cfn changes with different scales. FPs
</bodyText>
<page confidence="0.99763">
11
</page>
<table confidence="0.999894125">
Cfn FP Rate Precision Recall F-Measure (TP + FP)/N
1 .022 .831 .67 .742 .114
2 .024 .825 .683 .748 .117
3 .037 .771 .747 .759 .138
5 .052 .726 .828 .774 .162
10 .071 .674 .887 .766 .187
15 .091 .622 .91 .739 .207
20 .091 .622 .91 .739 .207
</table>
<tableCaption confidence="0.999703">
Table 6: Test performance with cost matrix
</tableCaption>
<bodyText confidence="0.99996459375">
mean wrong predictions, but we can correct them
during the second pass to check them. However, we
cannot correct FNs without going through the whole
data set, so they are a more egregious detriment to
the quality of the annotated data. During the exper-
iment, Cfn varies from 1 to 20. With increases in
Cfn, the cost of FN increases compared to FP.
The cost-sensitive classifier is relying on Weka
with reweighting training instances. In this task,
SVM performed better than NB and DT. Only SVM
results are included here due to space constraint.
The test results are shown in Table 64. The last col-
umn in the two tables is the total proportion of pos-
itive predictions (FP + TP). This value indicates
the total amount of data that humans have to check
in the second pass to verify whether positive predic-
tions are correct. To reduce human annotation effort,
we would like this value to be as low as possible.
As shown in Table 6, with the increase of Cfn, the
recall increases; however, the proportion of positive
predictions also increases. Therefore, it is a tradeoff
to achieve a high recall and a low ratio of TP and FP.
For the test set, the recall increases with larger
Cfn, even with a small increase of Cfn from 1 to
3. Remarkably, the classifier gives us a high recall
while keeping the proportion of positive predictions
at an acceptably low level. When Cfn = 20 for the
test set, only 20.7% of the data need to be manually
checked by humans, and less than 10% uncertain ut-
terances (19 out of 221 for the Eng data) are missed.
Now, we have achieved a high recall classifier
with an acceptable ratio of positive predictions.
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="method">
5 Extrinsic Evaluation of Semi-Automated
Annotation
</sectionHeader>
<bodyText confidence="0.985401">
Even with a high recall classifier, some of the true
positive data are labeled incorrectly in the final an-
</bodyText>
<footnote confidence="0.724322">
4Only Cfn = 1, 2, 3, 5, 10, 15, 20 are reported here due to
page limits
</footnote>
<bodyText confidence="0.9983105">
notated corpus. In addition, it also changes the dis-
tribution of class labels.
To test whether it hurts the overall data quality,
we performed an analysis, which demonstrates that
this annotation scheme is sufficient to produce qual-
ity data. We attempted to replicate an analysis on the
Eng data set, which examines the use of analogy, a
cognitive strategy where a source and target knowl-
edge structure are compared in terms of structural
correspondences as a strategy for solving problems
under uncertainty. The analysis we attempt to repli-
cate here focuses on examining how uncertainty lev-
els change relative to baseline before, during, and
after the use of analogies.
The overall Eng transcripts were segmented into
one of 5 block types: 1) pre-analogy (Lag -1) blocks,
10 utterances just prior to an analogy episode, 2)
during-analogy (Lag 0) blocks, utterances from the
beginning to end of an analogy episode, 3) post-
analogy (Lag 1) blocks, 10 utterances immediately
following an analogy episode, 4) post-post-analogy
(Lag 2) blocks, 10 utterances immediately follow-
ing post-analogy utterances, and 5) baseline blocks,
each block of 10 utterances at least 25 utterances
away from the other block types. The measure of un-
certainty in each block was the proportion of uncer-
tain utterances. The sampling strategy for the base-
line blocks was designed to provide an estimate of
uncertainty levels when the speakers were engaged
in pre-analogy, during-analogy, or post-analogy con-
versation, with the logic being that a certain amount
of lag or spillover of uncertainty was assumed to
take place surrounding analogy episodes.
Figure 1 shows the relationship of block type to
mean levels of uncertainty, comparing the pattern
with human vs. classifier-supported uncertainty la-
bels. The classifier-generated labels were first pre-
processed such that all FPs were removed, but FNs
remain. This re-analysis comparison thus provides
a test of whether the recall rate is high enough that
known statistical effects are not substantially altered
or removed. To examine how different settings of
Cfn might impact overall performance, we used la-
bels (corrected for false positives) for 4 different lev-
els of Cfn (1, 5, 10, 20) from the Table 6.
In the Eng data analyses, the main findings were
that analogy was triggered by local spikes in un-
certainty levels (Lag -1 &gt; baseline), replicating re-
</bodyText>
<page confidence="0.984476">
12
</page>
<figure confidence="0.786495">
Block type
</figure>
<figureCaption confidence="0.821434">
Figure 1: Mean % uncertainty by block type and label
source (Eng data set)
</figureCaption>
<tableCaption confidence="0.97355">
Table 7: Standardized mean difference (Cohen’s d) from
</tableCaption>
<table confidence="0.912329666666667">
baseline by block type and label source (the Eng data set)
(Note: ‘*’ denotes p &lt; .05, ‘**’ denotes p &lt; .01)
Block type
Lag -1 Lag 0 Lag 1 Lag 2
Human 0.54* 0.4 0.79** 0.46*
Cfn = 20 0.57* 0.3 0.78** 0.44
Cfn = 10 0.58** 0.32 0.73** 0.47*
Cfn = 5 0.57* 0.34 0.66** 0.48*
Cfn = 1 0.42 0.25 0.54* 0.40
</table>
<bodyText confidence="0.998212703703704">
sults from prior work with the MER dataset (Chan
et al., 2012); in contrast to the findings in MER,
uncertainty did not reduce to baseline levels follow-
ing analogy (Lags 1 and 2 &gt; baseline). Figure 1
plots the relationship of block type to mean levels
of uncertainty in this data set, comparing the pat-
tern with human vs. classifier-generated uncertainty
labels. Table 7 shows the standardized mean differ-
ence (Cohen’s d) (Cohen, 1988) from baseline by
block type and label source. The pattern of effects
(Lag -1 &gt; baseline, Lags 1 and 2 &gt; baseline) re-
mains substantially unchanged with the exception of
the Lag 2 vs. baseline comparison falling short of
statistical significance (although note that the stan-
dardized mean difference remains very similar) for
Cfn ranging from 20 to 5, although we can observe
a noticeable attenuation of effect sizes from Cfn of
5 and below, and a loss of statistical significance
for the main effect of uncertainty being significantly
higher than baseline for Lag -1 blocks when Cfn =
1.
The re-analysis clearly demonstrates that the re-
call rate of the classifier is sufficient to not substan-
tially alter or miss known statistical effects. We can
reasonably extrapolate that using this classifier for
uncertainty annotation in other datasets should be
satisfactory.
</bodyText>
<sectionHeader confidence="0.970633" genericHeader="conclusions">
6 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999988939393939">
In this paper, a simple high recall classifier is pro-
posed based on a cost matrix to semi-automate the
annotation of corpora with unbalanced classes. This
classifier maintains a good balance between high re-
call and high FP and NP ratio. In this way, humans
can employ this classifier to annotate new data with
significantly reduced effort (approximately 80% less
effort, depending on the degree of imbalance in the
data). Although the classifier does introduce some
misclassified samples to the final annotation, an ex-
trinsic evaluation demonstrates that the recall rate is
high enough and the performance does not sacrifice
data quality.
Like other semi-supervised or supervised meth-
ods for supporting annotation, our annotation
scheme has some limitations that should be noted.
Firstly, an initial annotated data set is needed to de-
rive a good performance classifier and the amount
of annotated data is dependent on the specific task5.
Secondly, the features and machine learning algo-
rithms used in semi-supervised annotation are also
domain specific. At the same time, there are some
unique challenges and opportunities that can be fur-
ther investigated for our annotation scheme on un-
balanced data. For example, even though the cost
matrix method can achieve a high recall for binary
classification problem, whether it can be generalized
to other tasks (e.g., multi-class classification tasks)
is an unanswered question. Another open question
is how the degree of unbalance between classes in
the corpora affects overall annotation quality. We
suggest that if the data is not unbalanced, the total
amount of effort that can be reduced will be lower.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9950185">
The collection of the engineering data was supported
by NSF grants SBE-0738071, SBE-0823628, and
SBE-0830210. Analogy analysis was supported by
NSF grant SBE-1064083.
</bodyText>
<footnote confidence="0.733499">
5For a new task, a new feature set is usually derived.
</footnote>
<figure confidence="0.986581692307692">
Human
Cfn=20
Cfn=10
Cfn=5
Cfn=1
0.15
0.1
0.05
0
Baseline Lag −1 Lag 0 Lag 1 Lag 2
0.25
0.2
Mean % uncertainty in block
</figure>
<page confidence="0.996089">
13
</page>
<sectionHeader confidence="0.995998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999473115384616">
Cecilia Ovesdotter Alm, Dan Roth and Richard Sproat.
2005. Emotions from text: Machine learning for
text-based emotion prediction. In Proceedings of
HLT/EMNLP 2005.
Bharat Ram Ambati, Mridul Gupta, Samar Husain and
Dipti Misra Sharma. 2010. A high recall error identi-
fication toolfor Hindi treebank validation. In Proceed-
ings of The 7th International Conference on Language
Resources and Evaluation (LREC), Valleta, Malta.
Jeremy Ang, Rajdip Dhillon, Ashley Krupski, Elizabeth
Shriberg and Andreas Stolcke. 2002. Prosody-based
automatic detection of annoyance and frustration in
human-computer Dialog. In INTERSPEECH-02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Compu-
tational learning theory, p.92-100, July 24-26, Madi-
son, Wisconsin, United States
Thorsten Brants and Oliver Plaehn. 2000. Interactive
corpus annotation. In Proceedings of LREC-2000.
Paula Carvalho, Lu´ıs Sarmento, Jorge Teixeira and M´ario
J. Silva. 2011. Liars and saviors in a sentiment an-
notated corpus of comments to political debates. In
Proceedings of the Association for Computational Lin-
guistics (ACL 2011), Portland, OR.
Joel Chan, Susannah B. F. Paletz and Christian D.
Schunn. 2012. Analogy as a strategy for supporting
complex problem solving under uncertainty. Memory
&amp; Cognition, 40, 1352-1365.
Fu-Dong Chiou, David Chiang and Martha Palmer. 2001.
Facilitating treebank annotation using a statistical
parser. In HLT’01. ACL.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku1, Ting-Yi Sung and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedical
proposition bank. In Proceedings of FLAC-2006.
David Cohn, Richard Ladner and Alex Waibel. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15 (2), 201-221.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20, 37-46.
Jacob Cohen. 1988. Statistical power analysis for the
behavioral sciences (2nd ed.). Lawrence Erlbaum.
CoNLL-2010 Shared Task. 2010. In Fourteenth Con-
ference on Computational Natural Language Learning,
Proceedings of the Shared Task.
Myroslava Dzikovska, Peter Bell, Amy Isard and Jo-
hanna D. Moore. 2012. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. EACL 2012: 471-481.
Kate Forbes-Riley and Diane Litman. 2009. Adapting
to student uncertainty improves tutoring dialogues. In
Proceedings 14th International Conference on Artifi-
cial Intelligence in Education (AIED2009), pp. 33-40.
Kate Forbes-Riley and Diane Litman. 2011. Bene-
fits and challenges of real-time uncertainty detection
and adaptation in a spoken dialogue computer tutor.
Speech Communication, v53, pp. 1115-1136.
George Forman 2003. An Extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3, 1289-1305.
Kuzman Ganchev, Fernando Pereira, Mark Mandel,
Steven Carroll and Peter White. 2007. Semi-
automated named entity annotation. In Proceedings
of the linguistic annotation workshop, pp. 53-56
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H.Witten. 2009.
The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Taka Hiraishi, Buruhani Nyenzi, Jim Penman and Semere
Habetsion. 2000. Quantifying uncertainties in prac-
tice. In Revised 1996 IPCC guidelines for national
greenhouse gas inventories.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL.
Janet Holmes. 1999. Women, men, and politeness. Lon-
don, SAGE publications, pp:86-96
Jooyoung Jang and Christian Schunn. 2012. Physical
design tools support and hinder innovative engineer-
ing design. Journal of Mechanical Design, vol. 134,
no. 4, pp. 041001-1-041001-9.
Shoushan Li, Shengfeng Ju, Guodong Zhou and Xiaojun
Li. 2012. Active learning for imbalanced sentiment
classification. EMNLP-CoNLL 2012: 139-148
Rada Mihalcea. 2004. Co-training and self-training for
word sense disambiguation. In Proceedings of the
8th Conference on Computational Natural Language
Learning (CoNLL, Boston, MA). 33-40.
Susannah B. F. Paletz and Christian D. Schunn. 2011.
Assessing group-level participation in fluid teams:
Testing a new metric. Behav Res 43:522-536.
Heather Pon-Barry and Stuart M. Shieber 2011. Rec-
ognizing uncertainty in speech. EURASIP Journal on
Advances in Signal Processing.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow and Benjamin Van Durme 2012 Sta-
tistical modality tagging from rule-based annotations
and crowdsourcing. In Proceedings of ACL Workshop
on Extra-propositional aspects of meaning in compu-
tational linguistics (ExProM).
</reference>
<page confidence="0.984361">
14
</page>
<reference confidence="0.999386290322581">
Ines Rehbein, Josef Ruppenhofer and Caroline Sporleder.
2012. Is it worth the effort? Assessing the benefits of
partial automatic pre-labeling for frame-semantic an-
notation. Language Resources and Evaluation, Vol.46,
No.1. pp. 1-23
Joel R. Tetreault and Martin Chodorow. Native judg-
ments of non-native usage: experiments in preposi-
tion error detection. In Proceedings of the Workshop
on Human Judgements in Computational Linguistics,
p.24-32, Manchester, United Kingdom.
Irene V. Tollinger, Christian D. Schunn and Alonso H.
Vera. 2006. What changes when a large team becomes
more expert? Analyses of speedup in the Mars Explo-
ration Rovers science planning process. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society (pp. 840-845). Mahwah, NJ: Erlbaum.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer. 2002.
Building a large-scale annotated chinese corpus. In
Proceedings of the 19th international conference on
Computational linguistics. ACL.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, 783-790.
Jingbo Zhu, Huizhen Wang, Eduard H. Hovy and
Matthew Y. Ma. 2010. Confidence-based stopping
criteria for active learning for data annotation. ACM
Transactions on Speech and Language Processing, 6,
124.
</reference>
<page confidence="0.997843">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847728">
<title confidence="0.999739">Reducing Annotation Effort on Unbalanced Corpus based on Cost Matrix</title>
<author confidence="0.998386">Wencan Luo</author>
<author confidence="0.998386">Diane</author>
<affiliation confidence="0.999931">Department of Computer University of</affiliation>
<address confidence="0.980574">Pittsburgh, PA 15260,</address>
<author confidence="0.999671">Joel Chan</author>
<affiliation confidence="0.9998545">Department of University of</affiliation>
<address confidence="0.923032">Pittsburgh, PA 15260,</address>
<email confidence="0.974702">chozen86@gmail.com</email>
<abstract confidence="0.998326913043478">Annotated corpora play a significant role in many NLP applications. However, annotation by humans is time-consuming and costly. In this paper, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes. We demonstrate the effectiveness of our approach in the context of one form of unbalanced task: annotation of transcribed human-human dialogues for presence/absence of uncertainty. In two data sets, our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy. The method is able to reduce human annotation effort by about 80% without a significant loss in data quality, as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
<author>Dan Roth</author>
<author>Richard Sproat</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="1929" citStr="Alm et al., 2005" startWordPosition="288" endWordPosition="291">ent of statistical-based NLP tools. However, the annotation of corpora is most commonly done by humans, which is time-consuming and costly. To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptab</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Cecilia Ovesdotter Alm, Dan Roth and Richard Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bharat Ram Ambati</author>
<author>Mridul Gupta</author>
<author>Samar Husain</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>A high recall error identification toolfor Hindi treebank validation.</title>
<date>2010</date>
<booktitle>In Proceedings of The 7th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valleta,</location>
<contexts>
<context position="6532" citStr="Ambati et al. (2010)" startWordPosition="1010" endWordPosition="1013">ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, people go through only the positive predictions, which are much less than the whole unlabeled data, due to the unbalanced structure of the data. What’s more, reducing the annotation effort is the goal of this paper but not building a high recall classifier such as Prabhakaran et al. (2012) and Ambati et al. (2010). The approach proposed by Tetreault and Chodorow (2008) is similar to us. However, they assumed they had a high recall classifier but did not explicitly show how to build it. In addition, they did not provide extrinsic evaluation to see whether a corpus generated by pre-annotation is good enough to be used in real applications. 2.2 Uncertainty Prediction Uncertainty is a lack of knowledge about internal state (Pon-Barry and Shieber, 2011). In this paper, we only focus on detection of uncertainty on text. Commonly used features are lexical features such as unigram (Forbes-Riley and Litman, 201</context>
</contexts>
<marker>Ambati, Gupta, Husain, Sharma, 2010</marker>
<rawString>Bharat Ram Ambati, Mridul Gupta, Samar Husain and Dipti Misra Sharma. 2010. A high recall error identification toolfor Hindi treebank validation. In Proceedings of The 7th International Conference on Language Resources and Evaluation (LREC), Valleta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Rajdip Dhillon</author>
<author>Ashley Krupski</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer Dialog.</title>
<date>2002</date>
<booktitle>In INTERSPEECH-02.</booktitle>
<contexts>
<context position="1910" citStr="Ang et al., 2002" startWordPosition="284" endWordPosition="287">l for the development of statistical-based NLP tools. However, the annotation of corpora is most commonly done by humans, which is time-consuming and costly. To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated da</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>Jeremy Ang, Rajdip Dhillon, Ashley Krupski, Elizabeth Shriberg and Andreas Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer Dialog. In INTERSPEECH-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<location>Madison, Wisconsin, United States</location>
<contexts>
<context position="4662" citStr="Blum and Mitchell, 1998" startWordPosition="715" endWordPosition="718">approach, by showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty. 2 Related Work 2.1 Reducing Annotation Effort A number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active learning (Cohn et al., 1994; Zhu and Hovy, 2007; Zhu et al., 2010), co-training (Blum and Mitchell, 1998) and self-training (Mihalcea, 2004). Active learning reduces annotation by carefully selecting more useful samples. Co-training relies on several conditional independent classifiers to tag new unlabeled data and self-training takes the advantage of full unlabeled data. These semisupervised learning methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rech</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, p.92-100, July 24-26, Madison, Wisconsin, United States</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Oliver Plaehn</author>
</authors>
<title>Interactive corpus annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-2000.</booktitle>
<contexts>
<context position="5584" citStr="Brants and Plaehn, 2000" startWordPosition="849" endWordPosition="852"> methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2This annotation scheme can also benefit other kinds of tasks. ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by </context>
</contexts>
<marker>Brants, Plaehn, 2000</marker>
<rawString>Thorsten Brants and Oliver Plaehn. 2000. Interactive corpus annotation. In Proceedings of LREC-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Carvalho</author>
<author>Lu´ıs Sarmento</author>
<author>Jorge Teixeira</author>
<author>M´ario J Silva</author>
</authors>
<title>Liars and saviors in a sentiment annotated corpus of comments to political debates.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL 2011),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="2018" citStr="Carvalho et al., 2011" startWordPosition="302" endWordPosition="305">nly done by humans, which is time-consuming and costly. To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to</context>
</contexts>
<marker>Carvalho, Sarmento, Teixeira, Silva, 2011</marker>
<rawString>Paula Carvalho, Lu´ıs Sarmento, Jorge Teixeira and M´ario J. Silva. 2011. Liars and saviors in a sentiment annotated corpus of comments to political debates. In Proceedings of the Association for Computational Linguistics (ACL 2011), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Chan</author>
<author>Susannah B F Paletz</author>
<author>Christian D Schunn</author>
</authors>
<title>Analogy as a strategy for supporting complex problem solving under uncertainty.</title>
<date>2012</date>
<journal>Memory &amp; Cognition,</journal>
<volume>40</volume>
<pages>1352--1365</pages>
<contexts>
<context position="7360" citStr="Chan et al., 2012" startWordPosition="1141" endWordPosition="1144">xtrinsic evaluation to see whether a corpus generated by pre-annotation is good enough to be used in real applications. 2.2 Uncertainty Prediction Uncertainty is a lack of knowledge about internal state (Pon-Barry and Shieber, 2011). In this paper, we only focus on detection of uncertainty on text. Commonly used features are lexical features such as unigram (Forbes-Riley and Litman, 2011). Moreover, energy, dialogue features such as turn number, tutor goal, and metadata like gender are also considered by Forbes-Riley and Litman (2011). Uncertainty prediction is both substantively interesting (Chan et al., 2012; Forbes-Riley and Litman, 2009) and pragmatically expeditious for our purposes, due to its binary classification and typical unbalanced class structure. CoNLL 2010 has launched a shared task to detect hedges and their scope in natural language text on two data sets: BioScope and Wikipedia (CoNLL, 2010). This first task to detect whether there is a hedge present or not present in a sentence is very similar to our uncertainty prediction task. 23 teams participated in the shared task with the best recall of 0.8772 on the BioScope, and 0.5528 on the Wikipedia. As we can see, uncertainty detection</context>
<context position="21723" citStr="Chan et al., 2012" startWordPosition="3626" endWordPosition="3629">ndings were that analogy was triggered by local spikes in uncertainty levels (Lag -1 &gt; baseline), replicating re12 Block type Figure 1: Mean % uncertainty by block type and label source (Eng data set) Table 7: Standardized mean difference (Cohen’s d) from baseline by block type and label source (the Eng data set) (Note: ‘*’ denotes p &lt; .05, ‘**’ denotes p &lt; .01) Block type Lag -1 Lag 0 Lag 1 Lag 2 Human 0.54* 0.4 0.79** 0.46* Cfn = 20 0.57* 0.3 0.78** 0.44 Cfn = 10 0.58** 0.32 0.73** 0.47* Cfn = 5 0.57* 0.34 0.66** 0.48* Cfn = 1 0.42 0.25 0.54* 0.40 sults from prior work with the MER dataset (Chan et al., 2012); in contrast to the findings in MER, uncertainty did not reduce to baseline levels following analogy (Lags 1 and 2 &gt; baseline). Figure 1 plots the relationship of block type to mean levels of uncertainty in this data set, comparing the pattern with human vs. classifier-generated uncertainty labels. Table 7 shows the standardized mean difference (Cohen’s d) (Cohen, 1988) from baseline by block type and label source. The pattern of effects (Lag -1 &gt; baseline, Lags 1 and 2 &gt; baseline) remains substantially unchanged with the exception of the Lag 2 vs. baseline comparison falling short of statist</context>
</contexts>
<marker>Chan, Paletz, Schunn, 2012</marker>
<rawString>Joel Chan, Susannah B. F. Paletz and Christian D. Schunn. 2012. Analogy as a strategy for supporting complex problem solving under uncertainty. Memory &amp; Cognition, 40, 1352-1365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fu-Dong Chiou</author>
<author>David Chiang</author>
<author>Martha Palmer</author>
</authors>
<title>Facilitating treebank annotation using a statistical parser. In</title>
<date>2001</date>
<booktitle>HLT’01. ACL.</booktitle>
<contexts>
<context position="5604" citStr="Chiou et al., 2001" startWordPosition="853" endWordPosition="856"> with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2This annotation scheme can also benefit other kinds of tasks. ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in</context>
</contexts>
<marker>Chiou, Chiang, Palmer, 2001</marker>
<rawString>Fu-Dong Chiou, David Chiang and Martha Palmer. 2001. Facilitating treebank annotation using a statistical parser. In HLT’01. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Chi Chou</author>
</authors>
<title>Richard Tzong-Han Tsai, Ying-Shan Su, Wei Ku1, Ting-Yi Sung and Wen-Lian Hsu.</title>
<date>2006</date>
<booktitle>In Proceedings of FLAC-2006.</booktitle>
<marker>Chou, 2006</marker>
<rawString>Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su, Wei Ku1, Ting-Yi Sung and Wen-Lian Hsu. 2006. A semi-automatic method for annotating a biomedical proposition bank. In Proceedings of FLAC-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Richard Ladner</author>
<author>Alex Waibel</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<pages>201--221</pages>
<contexts>
<context position="4584" citStr="Cohn et al., 1994" startWordPosition="702" endWordPosition="705">wofold. First, an extrinsic evaluation demonstrates the utility of our approach, by showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty. 2 Related Work 2.1 Reducing Annotation Effort A number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active learning (Cohn et al., 1994; Zhu and Hovy, 2007; Zhu et al., 2010), co-training (Blum and Mitchell, 1998) and self-training (Mihalcea, 2004). Active learning reduces annotation by carefully selecting more useful samples. Co-training relies on several conditional independent classifiers to tag new unlabeled data and self-training takes the advantage of full unlabeled data. These semisupervised learning methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large </context>
</contexts>
<marker>Cohn, Ladner, Waibel, 1994</marker>
<rawString>David Cohn, Richard Ladner and Alex Waibel. 1994. Improving generalization with active learning. Machine Learning, 15 (2), 201-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context position="9844" citStr="Cohen, 1960" startWordPosition="1555" endWordPosition="1556">ere are 12,331 transcribed utterances in the MER data set, and 44,199 transcribed utterances in the Eng data set. Both data sets are unbalanced: in the MER data, 1641 of all the 12,331 (13.3%) utterances are annotated as uncertain by trained human annotators; in the Eng data, only 1558 utterances are annotated, 221 of which are annotated as uncertain (14.2%). 96.5% of the utterances in the Eng data set have not been annotated yet, raising the need for an efficient annotated technique. Both data sets are annotated by two trained coders with high inter-rater agreement, at Cohen’s kappa of 0.75 (Cohen, 1960). A sample dialogue snippet from the MER corpus is shown in Table 1. The last column indicates whether the utterance is labeled as uncertainty or not: ‘1’ means uncertainty and ‘0’ means certainty. The MER data serves as the initial annotated set and a high recall classifier will be trained on it; the Eng data3 serves as a simulated unlabeled data set to 3The Eng data in this paper denotes the annotated subset of utterance uncertainty? You can’t see the forest through the trees. Yea, we never could see the [missing words] No we had to get above it We just went right through it Yea I still don’</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.</title>
<date>1988</date>
<contexts>
<context position="22096" citStr="Cohen, 1988" startWordPosition="3690" endWordPosition="3691"> -1 Lag 0 Lag 1 Lag 2 Human 0.54* 0.4 0.79** 0.46* Cfn = 20 0.57* 0.3 0.78** 0.44 Cfn = 10 0.58** 0.32 0.73** 0.47* Cfn = 5 0.57* 0.34 0.66** 0.48* Cfn = 1 0.42 0.25 0.54* 0.40 sults from prior work with the MER dataset (Chan et al., 2012); in contrast to the findings in MER, uncertainty did not reduce to baseline levels following analogy (Lags 1 and 2 &gt; baseline). Figure 1 plots the relationship of block type to mean levels of uncertainty in this data set, comparing the pattern with human vs. classifier-generated uncertainty labels. Table 7 shows the standardized mean difference (Cohen’s d) (Cohen, 1988) from baseline by block type and label source. The pattern of effects (Lag -1 &gt; baseline, Lags 1 and 2 &gt; baseline) remains substantially unchanged with the exception of the Lag 2 vs. baseline comparison falling short of statistical significance (although note that the standardized mean difference remains very similar) for Cfn ranging from 20 to 5, although we can observe a noticeable attenuation of effect sizes from Cfn of 5 and below, and a loss of statistical significance for the main effect of uncertainty being significantly higher than baseline for Lag -1 blocks when Cfn = 1. The re-analys</context>
</contexts>
<marker>Cohen, 1988</marker>
<rawString>Jacob Cohen. 1988. Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNLL-2010 Shared Task</author>
</authors>
<date>2010</date>
<booktitle>In Fourteenth Conference on Computational Natural Language Learning, Proceedings of the Shared Task.</booktitle>
<marker>Task, 2010</marker>
<rawString>CoNLL-2010 Shared Task. 2010. In Fourteenth Conference on Computational Natural Language Learning, Proceedings of the Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava Dzikovska</author>
<author>Peter Bell</author>
<author>Amy Isard</author>
<author>Johanna D Moore</author>
</authors>
<title>Evaluating language understanding accuracy with respect to objective outcomes in a dialogue system. EACL</title>
<date>2012</date>
<pages>471--481</pages>
<contexts>
<context position="2151" citStr="Dzikovska et al., 2012" startWordPosition="320" endWordPosition="323">ime and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels. The last step is to manually ch</context>
</contexts>
<marker>Dzikovska, Bell, Isard, Moore, 2012</marker>
<rawString>Myroslava Dzikovska, Peter Bell, Amy Isard and Johanna D. Moore. 2012. Evaluating language understanding accuracy with respect to objective outcomes in a dialogue system. EACL 2012: 471-481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
</authors>
<title>Adapting to student uncertainty improves tutoring dialogues.</title>
<date>2009</date>
<booktitle>In Proceedings 14th International Conference on Artificial Intelligence in Education (AIED2009),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="7392" citStr="Forbes-Riley and Litman, 2009" startWordPosition="1145" endWordPosition="1148"> to see whether a corpus generated by pre-annotation is good enough to be used in real applications. 2.2 Uncertainty Prediction Uncertainty is a lack of knowledge about internal state (Pon-Barry and Shieber, 2011). In this paper, we only focus on detection of uncertainty on text. Commonly used features are lexical features such as unigram (Forbes-Riley and Litman, 2011). Moreover, energy, dialogue features such as turn number, tutor goal, and metadata like gender are also considered by Forbes-Riley and Litman (2011). Uncertainty prediction is both substantively interesting (Chan et al., 2012; Forbes-Riley and Litman, 2009) and pragmatically expeditious for our purposes, due to its binary classification and typical unbalanced class structure. CoNLL 2010 has launched a shared task to detect hedges and their scope in natural language text on two data sets: BioScope and Wikipedia (CoNLL, 2010). This first task to detect whether there is a hedge present or not present in a sentence is very similar to our uncertainty prediction task. 23 teams participated in the shared task with the best recall of 0.8772 on the BioScope, and 0.5528 on the Wikipedia. As we can see, uncertainty detection is not trivial and it can be ha</context>
</contexts>
<marker>Forbes-Riley, Litman, 2009</marker>
<rawString>Kate Forbes-Riley and Diane Litman. 2009. Adapting to student uncertainty improves tutoring dialogues. In Proceedings 14th International Conference on Artificial Intelligence in Education (AIED2009), pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
</authors>
<title>Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor.</title>
<date>2011</date>
<journal>Speech Communication,</journal>
<volume>53</volume>
<pages>1115--1136</pages>
<contexts>
<context position="2126" citStr="Forbes-Riley and Litman, 2011" startWordPosition="316" endWordPosition="319">it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels. The la</context>
<context position="7134" citStr="Forbes-Riley and Litman, 2011" startWordPosition="1106" endWordPosition="1109">12) and Ambati et al. (2010). The approach proposed by Tetreault and Chodorow (2008) is similar to us. However, they assumed they had a high recall classifier but did not explicitly show how to build it. In addition, they did not provide extrinsic evaluation to see whether a corpus generated by pre-annotation is good enough to be used in real applications. 2.2 Uncertainty Prediction Uncertainty is a lack of knowledge about internal state (Pon-Barry and Shieber, 2011). In this paper, we only focus on detection of uncertainty on text. Commonly used features are lexical features such as unigram (Forbes-Riley and Litman, 2011). Moreover, energy, dialogue features such as turn number, tutor goal, and metadata like gender are also considered by Forbes-Riley and Litman (2011). Uncertainty prediction is both substantively interesting (Chan et al., 2012; Forbes-Riley and Litman, 2009) and pragmatically expeditious for our purposes, due to its binary classification and typical unbalanced class structure. CoNLL 2010 has launched a shared task to detect hedges and their scope in natural language text on two data sets: BioScope and Wikipedia (CoNLL, 2010). This first task to detect whether there is a hedge present or not pr</context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>Kate Forbes-Riley and Diane Litman. 2011. Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor. Speech Communication, v53, pp. 1115-1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An Extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1289--1305</pages>
<contexts>
<context position="2187" citStr="Forman, 2003" startWordPosition="326" endWordPosition="327">son, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels. The last step is to manually check every positive label and correct</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman 2003. An Extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3, 1289-1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Fernando Pereira</author>
<author>Mark Mandel</author>
<author>Steven Carroll</author>
<author>Peter White</author>
</authors>
<title>Semiautomated named entity annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the linguistic annotation workshop,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="5644" citStr="Ganchev et al., 2007" startWordPosition="861" endWordPosition="864">data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2This annotation scheme can also benefit other kinds of tasks. ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, people go through only the </context>
</contexts>
<marker>Ganchev, Pereira, Mandel, Carroll, White, 2007</marker>
<rawString>Kuzman Ganchev, Fernando Pereira, Mark Mandel, Steven Carroll and Peter White. 2007. Semiautomated named entity annotation. In Proceedings of the linguistic annotation workshop, pp. 53-56</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="14534" citStr="Hall et al., 2009" startWordPosition="2355" endWordPosition="2358">d kinda presumably wondering i bet kinds of seem i can try like some Table 4: New words/phrases for uncertainty prediction After error analysis, a few new words/phrases are added to the feature set, shown in Table 4. By supplementing the original feature set in this way, we reran the training yielding our final baseline, the performance on the training data (MER) and testing data (Eng) is shown in Table 5. This time, we compare different classifiers including Naive Bayes (NB), Decision Tree (DT) and Support Vector Machine (SVM). All of them are implemented using the open source platform Weka (Hall et al., 2009) with default parameters. As we can see, test recall is worse than train recall. Data Set Method TP FP Precision Recall F-Measure NB .732 .016 .875 .732 .797 MER DT .831 .013 .908 .831 .868 SVM .811 .013 .905 .811 .855 NB .679 .014 .888 .679 .769 Eng DT .665 .021 .84 .665 .742 SVM .674 .022 .832 .674 .745 Table 5: Performance with original and new words/phrases as a feature set: train on the MER and test on the Eng data for class ‘1’. TP is true positive; FP is false positive In addition, although DT and SVM perform better than NB on train data set, they have similar performance on the test se</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann and Ian H.Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taka Hiraishi</author>
<author>Buruhani Nyenzi</author>
<author>Jim Penman</author>
<author>Semere Habetsion</author>
</authors>
<title>Quantifying uncertainties in practice.</title>
<date>2000</date>
<booktitle>In Revised</booktitle>
<contexts>
<context position="11576" citStr="Hiraishi et al., 2000" startWordPosition="1845" endWordPosition="1848">rds/phrases are used as features here. There are several resources for the words/phrases of uncertainty prediction. The main resource is a guideline book used by our annotators showing how to distinguish uncertainty utterance. It gives three different kinds of words/phrases, shown in Table 2 indicated by three superscripts ‘+’, ‘-’ and ‘*’. The words/phrases with ‘+’ show some evidence of uncertainty; ones with ‘-’ mean that they show no evidence of uncertainty; others with ‘*’ may or may not show uncertainty. The second source is from existing literature. The words/phrases with ‘1’ are from (Hiraishi et al., 2000) and ones with ‘2’ are from (Holms, 1999). For each word/phrase w, a binary feature is used to indicate whether the word/phrase w is in the utterance or not. A Naive Bayes classifier is trained on the MER data using these features and tested on the Eng data. The performances of the model on the train set and test set are shown in Table 3. Both weighted and unweighted false positive (FP) Rate, Precision, Recall and F-Measure are reported. However, in later experiments, we will focus on only the positive class (the uncertainty class). A 0.689 recall means that 510 out of 1641 positive utterances</context>
</contexts>
<marker>Hiraishi, Nyenzi, Penman, Habetsion, 2000</marker>
<rawString>Taka Hiraishi, Buruhani Nyenzi, Jim Penman and Semere Habetsion. 2000. Quantifying uncertainties in practice. In Revised 1996 IPCC guidelines for national greenhouse gas inventories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2235" citStr="Hoffmann et al., 2011" startWordPosition="331" endWordPosition="334">off between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels. The last step is to manually check every positive label and correct it if it is wrong. To apply this method to work</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Holmes</author>
</authors>
<title>Women, men, and politeness.</title>
<date>1999</date>
<pages>86--96</pages>
<location>London, SAGE</location>
<note>publications,</note>
<marker>Holmes, 1999</marker>
<rawString>Janet Holmes. 1999. Women, men, and politeness. London, SAGE publications, pp:86-96</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jooyoung Jang</author>
<author>Christian Schunn</author>
</authors>
<title>Physical design tools support and hinder innovative engineering design.</title>
<date>2012</date>
<journal>Journal of Mechanical Design,</journal>
<volume>134</volume>
<pages>041001--1</pages>
<contexts>
<context position="8567" citStr="Jang and Schunn, 2012" startWordPosition="1344" endWordPosition="1347">ainty detection is not trivial and it can be hard to get a high recall classifier. In this paper, we focus on lexical features for our 9 purpose because lexical features are simple to extract and sufficient for our scheme. Even though other features may improve uncertainty prediction performance, with the goal of reducing annotation effort, such lexical features are shown to be good enough for our task. 3 The Corpora We examine the following two data sets: the Mars Exploration Rover (MER) mission (Tollinger et al., 2006; Paletz and Schunn, 2011) and the student engineering team (Eng) dataset (Jang and Schunn, 2012). The MER scientists are evaluating data downloaded from the Rover, discussing their work process, and/or making plans for the Rovers. They come from a large team of about 100+ scientists/faculty, graduate students, and technicians. At any one time, conversations are between 2-10 people. The Eng teams are natural teams of college undergraduates working on their semester-long product design projects. The conversations involve 2-6 individuals. Audio and video are available for both data sets and transcripts are obtained with human annotators. Our task is to annotate the transcribed humanhuman di</context>
</contexts>
<marker>Jang, Schunn, 2012</marker>
<rawString>Jooyoung Jang and Christian Schunn. 2012. Physical design tools support and hinder innovative engineering design. Journal of Mechanical Design, vol. 134, no. 4, pp. 041001-1-041001-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Shengfeng Ju</author>
<author>Guodong Zhou</author>
<author>Xiaojun Li</author>
</authors>
<title>Active learning for imbalanced sentiment classification. EMNLP-CoNLL</title>
<date>2012</date>
<pages>139--148</pages>
<contexts>
<context position="1973" citStr="Li et al., 2012" startWordPosition="295" endWordPosition="298">the annotation of corpora is most commonly done by humans, which is time-consuming and costly. To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are 8 heavily skewed towards one or a few categories. Unbalanced corpora are common in a number of different tasks, such as emotion detection (Ang et al., 2002; Alm et al., 2005), sentiment classification (Li et al., 2012), polarity of opinion (Carvalho et al., 2011), uncertainty and correctness of student answers in tutoring dialogue systems (Forbes-Riley and Litman, 2011; Dzikovska et al., 2012), text classification (Forman, 2003), information extraction (Hoffmann et al., 2011), and so on1. In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. T</context>
</contexts>
<marker>Li, Ju, Zhou, Li, 2012</marker>
<rawString>Shoushan Li, Shengfeng Ju, Guodong Zhou and Xiaojun Li. 2012. Active learning for imbalanced sentiment classification. EMNLP-CoNLL 2012: 139-148</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Co-training and self-training for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL,</booktitle>
<pages>33--40</pages>
<location>Boston, MA).</location>
<contexts>
<context position="4697" citStr="Mihalcea, 2004" startWordPosition="721" endWordPosition="723"> achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty. 2 Related Work 2.1 Reducing Annotation Effort A number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active learning (Cohn et al., 1994; Zhu and Hovy, 2007; Zhu et al., 2010), co-training (Blum and Mitchell, 1998) and self-training (Mihalcea, 2004). Active learning reduces annotation by carefully selecting more useful samples. Co-training relies on several conditional independent classifiers to tag new unlabeled data and self-training takes the advantage of full unlabeled data. These semisupervised learning methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In additi</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Co-training and self-training for word sense disambiguation. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL, Boston, MA). 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susannah B F Paletz</author>
<author>Christian D Schunn</author>
</authors>
<title>Assessing group-level participation in fluid teams: Testing a new metric.</title>
<date>2011</date>
<journal>Behav Res</journal>
<pages>43--522</pages>
<contexts>
<context position="8496" citStr="Paletz and Schunn, 2011" startWordPosition="1333" endWordPosition="1336">.8772 on the BioScope, and 0.5528 on the Wikipedia. As we can see, uncertainty detection is not trivial and it can be hard to get a high recall classifier. In this paper, we focus on lexical features for our 9 purpose because lexical features are simple to extract and sufficient for our scheme. Even though other features may improve uncertainty prediction performance, with the goal of reducing annotation effort, such lexical features are shown to be good enough for our task. 3 The Corpora We examine the following two data sets: the Mars Exploration Rover (MER) mission (Tollinger et al., 2006; Paletz and Schunn, 2011) and the student engineering team (Eng) dataset (Jang and Schunn, 2012). The MER scientists are evaluating data downloaded from the Rover, discussing their work process, and/or making plans for the Rovers. They come from a large team of about 100+ scientists/faculty, graduate students, and technicians. At any one time, conversations are between 2-10 people. The Eng teams are natural teams of college undergraduates working on their semester-long product design projects. The conversations involve 2-6 individuals. Audio and video are available for both data sets and transcripts are obtained with </context>
</contexts>
<marker>Paletz, Schunn, 2011</marker>
<rawString>Susannah B. F. Paletz and Christian D. Schunn. 2011. Assessing group-level participation in fluid teams: Testing a new metric. Behav Res 43:522-536.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Heather Pon-Barry</author>
<author>M Stuart</author>
</authors>
<title>Shieber 2011. Recognizing uncertainty in speech.</title>
<booktitle>EURASIP Journal on Advances in Signal Processing.</booktitle>
<marker>Pon-Barry, Stuart, </marker>
<rawString>Heather Pon-Barry and Stuart M. Shieber 2011. Recognizing uncertainty in speech. EURASIP Journal on Advances in Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Michael Bloodgood</author>
<author>Mona Diab</author>
<author>Bonnie Dorr</author>
<author>Lori Levin</author>
<author>Christine D Piatko</author>
</authors>
<title>Owen Rambow and Benjamin Van Durme</title>
<date>2012</date>
<booktitle>In Proceedings of ACL Workshop on Extra-propositional aspects of meaning in computational linguistics (ExProM).</booktitle>
<contexts>
<context position="6507" citStr="Prabhakaran et al. (2012)" startWordPosition="1005" endWordPosition="1008">on for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, people go through only the positive predictions, which are much less than the whole unlabeled data, due to the unbalanced structure of the data. What’s more, reducing the annotation effort is the goal of this paper but not building a high recall classifier such as Prabhakaran et al. (2012) and Ambati et al. (2010). The approach proposed by Tetreault and Chodorow (2008) is similar to us. However, they assumed they had a high recall classifier but did not explicitly show how to build it. In addition, they did not provide extrinsic evaluation to see whether a corpus generated by pre-annotation is good enough to be used in real applications. 2.2 Uncertainty Prediction Uncertainty is a lack of knowledge about internal state (Pon-Barry and Shieber, 2011). In this paper, we only focus on detection of uncertainty on text. Commonly used features are lexical features such as unigram (For</context>
</contexts>
<marker>Prabhakaran, Bloodgood, Diab, Dorr, Levin, Piatko, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow and Benjamin Van Durme 2012 Statistical modality tagging from rule-based annotations and crowdsourcing. In Proceedings of ACL Workshop on Extra-propositional aspects of meaning in computational linguistics (ExProM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
</authors>
<title>Is it worth the effort? Assessing the benefits of partial automatic pre-labeling for frame-semantic annotation. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>1--23</pages>
<location>Vol.46, No.1.</location>
<contexts>
<context position="5686" citStr="Rehbein et al., 2012" startWordPosition="869" endWordPosition="872"> performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2This annotation scheme can also benefit other kinds of tasks. ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, people go through only the positive predictions, which are much less </context>
</contexts>
<marker>Rehbein, Ruppenhofer, Sporleder, 2012</marker>
<rawString>Ines Rehbein, Josef Ruppenhofer and Caroline Sporleder. 2012. Is it worth the effort? Assessing the benefits of partial automatic pre-labeling for frame-semantic annotation. Language Resources and Evaluation, Vol.46, No.1. pp. 1-23</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Native judgments of non-native usage: experiments in preposition error detection.</title>
<booktitle>In Proceedings of the Workshop on Human Judgements in Computational Linguistics,</booktitle>
<pages>24--32</pages>
<location>Manchester, United Kingdom.</location>
<marker>Tetreault, Chodorow, </marker>
<rawString>Joel R. Tetreault and Martin Chodorow. Native judgments of non-native usage: experiments in preposition error detection. In Proceedings of the Workshop on Human Judgements in Computational Linguistics, p.24-32, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene V Tollinger</author>
<author>Christian D Schunn</author>
<author>Alonso H Vera</author>
</authors>
<title>What changes when a large team becomes more expert? Analyses of speedup in the Mars Exploration Rovers science planning process.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Conference of the Cognitive Science Society</booktitle>
<pages>840--845</pages>
<publisher>Erlbaum.</publisher>
<location>Mahwah, NJ:</location>
<contexts>
<context position="8470" citStr="Tollinger et al., 2006" startWordPosition="1329" endWordPosition="1332">ith the best recall of 0.8772 on the BioScope, and 0.5528 on the Wikipedia. As we can see, uncertainty detection is not trivial and it can be hard to get a high recall classifier. In this paper, we focus on lexical features for our 9 purpose because lexical features are simple to extract and sufficient for our scheme. Even though other features may improve uncertainty prediction performance, with the goal of reducing annotation effort, such lexical features are shown to be good enough for our task. 3 The Corpora We examine the following two data sets: the Mars Exploration Rover (MER) mission (Tollinger et al., 2006; Paletz and Schunn, 2011) and the student engineering team (Eng) dataset (Jang and Schunn, 2012). The MER scientists are evaluating data downloaded from the Rover, discussing their work process, and/or making plans for the Rovers. They come from a large team of about 100+ scientists/faculty, graduate students, and technicians. At any one time, conversations are between 2-10 people. The Eng teams are natural teams of college undergraduates working on their semester-long product design projects. The conversations involve 2-6 individuals. Audio and video are available for both data sets and tran</context>
</contexts>
<marker>Tollinger, Schunn, Vera, 2006</marker>
<rawString>Irene V. Tollinger, Christian D. Schunn and Alonso H. Vera. 2006. What changes when a large team becomes more expert? Analyses of speedup in the Mars Exploration Rovers science planning process. In Proceedings of the 28th Annual Conference of the Cognitive Science Society (pp. 840-845). Mahwah, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics. ACL.</booktitle>
<contexts>
<context position="5622" citStr="Xue et al., 2002" startWordPosition="857" endWordPosition="860">tion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that case, all predicted labels have to be rechecked by humans manually. In addition, none of them take advantage of unbalanced data. Another class of effort reduction techniques is pre-annotation, which uses supervised machinelearning systems to automatically assign labels to the whole data and subsequently lets human annotators correct them (Brants and Plaehn, 2000; Chiou et al., 2001; Xue et al., 2002; Ganchev et al., 2007; Chou et al., 2006; Rehbein et al., 2012). Generally speaking, our annotation method belongs to the class of pre-annotation methods. How2This annotation scheme can also benefit other kinds of tasks. ever, our method improves pre-annotation for unbalanced data in two ways. Firstly, we lower the threshold for achieving a high recall classifier. Secondly, with pre-annotation, although people only perform a binary decision of whether the automatic classifier is either right or wrong, they have to go through all the unlabeled data one by one. In contrast, in our scheme, peopl</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>Nianwen Xue, Fu-Dong Chiou and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In Proceedings of the 19th international conference on Computational linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Eduard Hovy</author>
</authors>
<title>Active learning for word sense disambiguation with methods for addressing the class imbalance problem.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>783--790</pages>
<contexts>
<context position="4604" citStr="Zhu and Hovy, 2007" startWordPosition="706" endWordPosition="709">xtrinsic evaluation demonstrates the utility of our approach, by showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty. 2 Related Work 2.1 Reducing Annotation Effort A number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active learning (Cohn et al., 1994; Zhu and Hovy, 2007; Zhu et al., 2010), co-training (Blum and Mitchell, 1998) and self-training (Mihalcea, 2004). Active learning reduces annotation by carefully selecting more useful samples. Co-training relies on several conditional independent classifiers to tag new unlabeled data and self-training takes the advantage of full unlabeled data. These semisupervised learning methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be ann</context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Jingbo Zhu and Eduard Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 783-790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
<author>Eduard H Hovy</author>
<author>Matthew Y Ma</author>
</authors>
<title>Confidence-based stopping criteria for active learning for data annotation.</title>
<date>2010</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>6</volume>
<pages>124</pages>
<contexts>
<context position="4623" citStr="Zhu et al., 2010" startWordPosition="710" endWordPosition="713">demonstrates the utility of our approach, by showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. Second, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes such as uncertainty. 2 Related Work 2.1 Reducing Annotation Effort A number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active learning (Cohn et al., 1994; Zhu and Hovy, 2007; Zhu et al., 2010), co-training (Blum and Mitchell, 1998) and self-training (Mihalcea, 2004). Active learning reduces annotation by carefully selecting more useful samples. Co-training relies on several conditional independent classifiers to tag new unlabeled data and self-training takes the advantage of full unlabeled data. These semisupervised learning methods demonstrate that with a small proportion of annotated data, a classifier can achieve comparable performance with all annotated data. However, these approaches still need considerable annotation effort when a large corpus has to be annotated. In that cas</context>
</contexts>
<marker>Zhu, Wang, Hovy, Ma, 2010</marker>
<rawString>Jingbo Zhu, Huizhen Wang, Eduard H. Hovy and Matthew Y. Ma. 2010. Confidence-based stopping criteria for active learning for data annotation. ACM Transactions on Speech and Language Processing, 6, 124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>