<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000185">
<title confidence="0.999739">
A Linguistically Annotated Reordering Model
for BTG-based Statistical Machine Translation
</title>
<author confidence="0.995861">
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li
</author>
<affiliation confidence="0.9841325">
Human Language Technology
Institute for Infocomm Research
</affiliation>
<address confidence="0.917563">
21 Heng Mui Keng Terrace, Singapore 119613
</address>
<email confidence="0.974101">
{dyxiong, mzhang, aaiti, hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999556769230769">
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based statis-
tical machine translation. The model incorpo-
rates linguistic knowledge to predict orders for
both syntactic and non-syntactic phrases. The
linguistic knowledge is automatically learned
from source-side parse trees through an an-
notation algorithm. We empirically demon-
strate that the proposed model leads to a sig-
nificant improvement of 1.55% in the BLEU
score over the baseline reordering model on
the NIST MT-05 Chinese-to-English transla-
tion task.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959619047619">
In recent years, Bracketing Transduction Grammar
(BTG) proposed by (Wu, 1997) has been widely
used in statistical machine translation (SMT). How-
ever, the original BTG does not provide an effec-
tive mechanism to predict the most appropriate or-
ders between two neighboring phrases. To address
this problem, Xiong et al. (2006) enhance the BTG
with a maximum entropy (MaxEnt) based reorder-
ing model which uses boundary words of bilingual
phrases as features. Although this model outper-
forms previous unlexicalized models, it does not uti-
lize any linguistically syntactic features, which have
proven useful for phrase reordering (Wang et al.,
2007). Zhang et al. (2007) integrates source-side
syntactic knowledge into a phrase reordering model
based on BTG-style rules. However, one limita-
tion of this method is that it only reorders syntac-
tic phrases because linguistic knowledge from parse
trees is only carried by syntactic phrases as far as re-
ordering is concerned, while non-syntactic phrases
are combined monotonously with a flat reordering
score.
In this paper, we propose a linguistically anno-
tated reordering model for BTG-based SMT, which
is a significant extension to the work mentioned
above. The new model annotates each BTG node
with linguistic knowledge by projecting source-side
parse trees onto the corresponding binary trees gen-
erated by BTG so that syntactic features can be used
for phrase reordering. Different from (Zhang et al.,
2007), our annotation algorithm is able to label both
syntactic and non-syntactic phrases. This enables
our model to reorder any phrases, not limited to syn-
tactic phrases. In addition, other linguistic informa-
tion such as head words, is also used to improve re-
ordering.
The rest of the paper is organized as follows. Sec-
tion 2 briefly describes our baseline system while
Section 3 introduces the linguistically annotated re-
ordering model. Section 4 reports the experiments
on a Chinese-to-English translation task. We con-
clude in Section 5.
</bodyText>
<sectionHeader confidence="0.971912" genericHeader="method">
2 Baseline SMT System
</sectionHeader>
<bodyText confidence="0.995047666666667">
The baseline system is a phrase-based system which
uses the BTG lexical rules (A —* x/y) to translate
source phrase x into target phrase y and the BTG
merging rules (A —* [A, A]|(A, A)) to combine two
neighboring phrases with a straight or inverted or-
der. The BTG lexical rules are weighted with several
features, such as phrase translation, word penalty
and language models, in a log-linear form. For the
merging rules, a MaxEnt-based reordering model
using boundary words of neighboring phrases as fea-
tures is used to predict the merging order, similar to
(Xiong et al., 2006). We call this reordering model
</bodyText>
<page confidence="0.989845">
149
</page>
<reference confidence="0.224355">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149–152,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.998402111111111">
boundary words based reordering model (BWR). In
this paper, we propose to incorporate a linguistically
annotated reordering model into the log-linear trans-
lation model, so as to strengthen the BWR’s phrase
reordering ability. We train all the model scaling fac-
tors on the development set to maximize the BLEU
score. A CKY-style decoder is developed to gener-
ate the best BTG binary tree for each input sentence,
which yields the best translation.
</bodyText>
<sectionHeader confidence="0.949016" genericHeader="method">
3 Linguistically Annotated Reordering
Model
</sectionHeader>
<bodyText confidence="0.976058285714286">
The linguistically annotated reordering
model (LAR) is a MaxEnt-based classifica-
tion model which predicts the phrase order
o ∈ {inverted, straight} during the application
of merging rules to combine their left and right
neighboring phrases Al and Ar into a larger phrase
A. 1 The model can be formulated as
</bodyText>
<equation confidence="0.929295333333333">
exp(Ei Bihi(o, Al, Ar, A))
LAR = (1)
Eo, exp(Ei Bihi(ol, Al, Ar, A))
</equation>
<bodyText confidence="0.999998333333333">
where the functions hi ∈ {0, 1} are reordering fea-
tures and Bi are weights of these features. We define
the features as linguistic elements which are anno-
tated for each BTG node through an annotation al-
gorithm, which comprise (1) head word hw, (2) the
part-of-speech (POS) tag ht of head word and (3)
syntactic label sl.
Each merging rule involves 3 nodes (A, Al, Ar)
and each node has 3 linguistic elements (hw, ht, sl).
Therefore, the model has 9 features in total. Taking
the left node Al as an example, the model could use
its head word w as feature as follows
</bodyText>
<equation confidence="0.826801">
� 1, Al.hw = w, o = straight
hi(o, A, Al, Ar) =
0, otherwise
</equation>
<subsectionHeader confidence="0.999369">
3.1 Annotation Algorithm
</subsectionHeader>
<bodyText confidence="0.995142571428572">
There are two steps to annotate a phrase or a BTG
node using source-side parse tree information: (1)
determining the span on the source side which is
exactly covered by the node or the phrase, then
(2) annotating the span according to the source-side
parse tree. If the span is exactly covered by a sin-
gle subtree in the source-side parse tree, it is called
</bodyText>
<footnote confidence="0.8435145">
1Each phrase is also a node in the BTG tree generated by the
decoder.
</footnote>
<listItem confidence="0.946422933333333">
1: Annotator (span s = hi, ji, source-side parse tree t)
2: if s is a syntactic span then
3: Find the subtree c in t which exactly covers s
4: s.{ } := {c.hw, c.ht, c.sl}
5: else
6: Find the smallest subtree c* subsuming s in t
7: if c*.hw ∈ s then
8: s.hw := c*.hw and s.ht := c*.ht
9: else
10: Find the word w ∈ s which is nearest to c*.hw
11: s.hw := w and s.ht := w.t /*w.t is the POS
tag of w*/
12: end if
13: Find the left boundary node ln of s in c*
14: Find the right boundary node rn of s in c*
</listItem>
<figure confidence="0.758911">
15: s.sl := ln.sl-c*.sl-rn.sl
16: end if
</figure>
<figureCaption confidence="0.999952">
Figure 1: The Annotation Algorithm.
</figureCaption>
<bodyText confidence="0.999950827586207">
syntactic span, otherwise it is non-syntactic span.
One of the challenges in this annotation algorithm
is that phrases (BTG nodes) are not always cover-
ing syntactic span, in other words, they are not al-
ways aligned to all constituent nodes in the source-
side tree. To solve this problem, we use heuristic
rules to generate pseudo head word and composite
label which consists of syntactic labels of three rel-
evant constituents for the non-syntactic span. In this
way, our annotation algorithm is capable of labelling
both syntactic and non-syntactic phrases and there-
fore providing linguistic information for any phrase
reordering.
The annotation algorithm is shown in Fig. 1. For
a syntactic span, the annotation is trivial. Annotation
elements directly come from the subtree that covers
the span exactly. For a non-syntactic span, the pro-
cess is much complicated. Firstly, we need to locate
the smallest subtree c* subsuming the span (line 6).
Secondly, we try to identify the head word/tag of the
span (line 7-12) by using its head word directly if it
is within the span. Otherwise, the word within the
span which is nearest to hw will be assigned as the
head word of the span. Finally, we determine the
composite label of the span (line 13-15), which is
formulated as L-C-R. L/R means the syntactic label
of the left/right boundary node of s which is the
highest leftmost/rightmost sub-node of c* not over-
lapping the span. If there is no such boundary node
</bodyText>
<page confidence="0.957757">
150
</page>
<figure confidence="0.933245">
IP(取得)
</figure>
<figureCaption confidence="0.976805">
Figure 2: A syntactic parse tree with head word annotated
for each internal node. The superscripts of leaf nodes
denote their surface positions from left to right.
</figureCaption>
<table confidence="0.9968392">
span hw ht sl
(1, 2) 金融 NN NULL-NP-NN
(2, 3) 工作 NN NP
(2, 4) 取得 VV NP-IP-NP
(3, 4) 取得 VV NP-IP-NP
</table>
<tableCaption confidence="0.998547">
Table 1: Annotation samples according to the tree shown
</tableCaption>
<bodyText confidence="0.892582666666667">
in Fig. 2. hw/ht represents the head word/tag, respec-
tively. sl means the syntactic label.
(the span s is exactly aligned to the left/right bound-
ary of c*), L/R will be set to NULL. C is the label of
c*. L, R and C together define the external syntactic
context of s.
Fig. 2 shows a syntactic parse tree for a Chinese
sentence, with head word annotated for each internal
node. Some sample annotations are given in Table 1.
</bodyText>
<subsectionHeader confidence="0.999267">
3.2 Training and Decoding
</subsectionHeader>
<bodyText confidence="0.999968863636364">
Training an LAR model takes three steps. Firstly, we
extract annotated reordering examples from source-
side parsed, word-aligned bilingual data using the
annotation algorithm and the reordering example
extraction algorithm of (Xiong et al., 2006). We
then generate features using linguistic elements of
these examples and finally estimate feature weights.
This training process flexibly learns rich syntactic
reordering information without explicitly construct-
ing BTG tree or forest for each sentence pair.
During decoding, each input source sentence is
firstly parsed to obtain its syntactic tree. Then the
CKY-style decoder tries to generate the best BTG
tree using the lexical and merging rules. When two
neighboring nodes are merged in a specific order, the
two embedded reordering models, BWR and LAR,
evaluate this merging independently with individual
scores. The former uses boundary words as features
while the latter uses the linguistic elements as fea-
tures, annotated on the BTG nodes through the anno-
tation algorithm according to the source-side parse
tree.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999896642857143">
All experiments in this section were carried out on
the Chinese-to-English translation task of the NIST
MT-05. The baseline system and the new system
with the LAR model were trained on the FBIS cor-
pus. We removed 15,250 sentences, for which the
Chinese parser (Xiong et al., 2005) failed to pro-
duce syntactic parse trees. The parser was trained
on the Penn Chinese Treebank with a F1 score of
79.4%. The remaining FBIS corpus (224,165 sen-
tence pairs) was used to obtain standard bilingual
phrases for the systems.
We extracted 2.8M reordering examples from
these sentences. From these examples, we gener-
ated 114.8K reordering features for the BWR model
using the right boundary words of phrases and 85K
features for the LAR model using linguistic annota-
tions. We ran the MaxEnt toolkit (Zhang, 2004) to
tune reordering feature weights with iteration num-
ber being set to 100 and Gaussian prior to 1 to avoid
overfitting.
We built our four-gram language model using
Xinhua section of the English Gigaword corpus
(181.1M words) with the SRILM toolkit (Stol-
cke, 2002). For the efficiency of minimum-error-
rate training (Och, 2003), we built our development
set (580 sentences) using sentences not exceeding
50 characters from the NIST MT-02 evaluation test
data.
</bodyText>
<subsectionHeader confidence="0.695798">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999850142857143">
We compared various reordering configurations in
the baseline system and new system. The base-
line system only has BWR as the reordering model,
while the new system employs two reordering mod-
els: BWR and LAR. For the linguistically anno-
tated reordering model LAR, we augment its feature
pool incrementally: firstly using only single labels
</bodyText>
<figure confidence="0.761357103448276">
得)
����� � � � � �
NP(成绩)
�� � �
ADJP(显著) NP(成绩)
NN
成绩7
achievement
NP(工作) VP(
�� � �
NP(西藏) NP(工作)
� �
NN
金融2
financial
NN
工作3
work
VV
取得4
gain
AS
了5
JJ
显著s
remarkable
NR
西藏1
Tibet
</figure>
<page confidence="0.988891">
151
</page>
<bodyText confidence="0.997338685714286">
2(SL) as features (132 features in total), then con-
structing composite labels for non-syntactic phrases
(+BNL) (6.7K features), and finally introducing
head words and their POS tags into the feature pool
(+BNL+HWT) (85K features). This series of exper-
iments demonstrate the impact and degree of con-
tribution made by each feature for reordering. We
also conducted experiments to investigate the ef-
fect of restricting reordering to syntactic phrases in
the new system using the best reordering feature
set (SL+BNL+HWT) for LAR. The experimental
results (case-sensitive BLEU scores together with
confidence intervals) are presented in Table 2, from
which we have the following observations:
(1) The LAR model improves the performance
statistically significantly. Even we only use the base-
line feature set SL with only 132 features for the
LAR, the BLEU score improves from 0.2497 to
0.2588. This is because most of the frequent reorder-
ing patterns between Chinese and English have been
captured using syntactic labels. For example, the
pre-verbal modifier PP in Chinese is translated into
post-verbal counterpart in English. This reordering
can be described by a rule with an inverted order:
V P —* (PP, V P), and captured by our syntactic
reordering features.
(2) Context information, provided by labels of
boundary nodes (BNL) and head word/tag pairs
(HWT), also improves phrase reordering. Produc-
ing composite labels for non-syntactic BTG nodes
(+BNL) and integrating head word/tag pairs into
the LAR as reordering features (+BNL+HWT) are
both effective, indicating that context information
complements syntactic label for capturing reorder-
ing patterns.
</bodyText>
<listItem confidence="0.52804">
(3) Restricting phrase reordering to syntactic
phrases is harmful. The BLEU score plummets from
0.2652 to 0.2512.
</listItem>
<sectionHeader confidence="0.99927" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998862">
In this paper, we have presented a linguistically an-
notated reordering model to effectively integrate lin-
guistic knowledge into phrase reordering by merg-
ing source-side parse trees with BTG binary trees.
Our experimental results show that, on the NIST
</bodyText>
<footnote confidence="0.9714605">
2For non-syntactic node, we only use the single label C,
without constructing composite label L-C-R.
</footnote>
<table confidence="0.9995745">
Reordering Configuration BLEU (%)
BWR 24.97 f 0.90
BWR + LAR (SL) 25.88 f 0.95
BWR + LAR (+BNL) 26.27 f 0.98
BWR + LAR (+BNL+HWT) 26.52 f 0.96
Only allowed SPs reordering 25.12 f 0.87
</table>
<tableCaption confidence="0.657747333333333">
Table 2: The effect of the linguistically annotated reorder-
ing model. BWR denotes the boundary word based re-
ordering model while LAR denotes the linguistically an-
notated reordering model. (SL) is the baseline feature set,
(+BNL) and (+BNL+HWT) are extended feature sets for
the LAR. SP means syntactic phrase.
</tableCaption>
<bodyText confidence="0.998066285714286">
MT-05 task of Chinese-to-English translation, the
proposed reordering model leads to BLEU improve-
ment of 1.55%. We believe that our linguistically
annotated reordering model can be further improved
by using better annotation which transfers more
knowledge (morphological, syntactic or semantic)
to the model.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.95695756">
Franz Josef Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings ofACL 2003.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings ofInternational Conference on
Spoken Language Processing, volume 2, pages 901-904.
Chao Wang, Michael Collins and Philipp Koehn. 2007. Chi-
nese Syntactic Reordering for Statistical Machine Transla-
tion. In Proceedings ofEMNLP-CoNLL 2007.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings ofIJCNLP, Jeju Island,
Korea.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings ofACL-COLING 2006.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007.
Phrase Reordering Model Integrating Syntactic Knowledge
for SMT. In Proceedings of EMNLP-CoNLL 2007.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
</reference>
<page confidence="0.995881">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856637">
<title confidence="0.9995835">A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation</title>
<author confidence="0.976131">Deyi Xiong</author>
<author confidence="0.976131">Min Zhang</author>
<author confidence="0.976131">Aiti Aw</author>
<author confidence="0.976131">Haizhou Li</author>
<affiliation confidence="0.986241">Human Language Technology Institute for Infocomm Research</affiliation>
<address confidence="0.959121">21 Heng Mui Keng Terrace, Singapore 119613</address>
<email confidence="0.953672">mzhang,aaiti,</email>
<abstract confidence="0.997867142857143">In this paper, we propose a linguistically annotated reordering model for BTG-based statistical machine translation. The model incorporates linguistic knowledge to predict orders for both syntactic and non-syntactic phrases. The linguistic knowledge is automatically learned from source-side parse trees through an annotation algorithm. We empirically demonstrate that the proposed model leads to a significant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>149--152</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149–152,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofInternational Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings ofInternational Conference on Spoken Language Processing, volume 2, pages 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese Syntactic Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL</booktitle>
<contexts>
<context position="1492" citStr="Wang et al., 2007" startWordPosition="216" endWordPosition="219">ng Transduction Grammar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score. In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above. The new model annotates each BTG node with </context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins and Philipp Koehn. 2007. Chinese Syntactic Reordering for Statistical Machine Translation. In Proceedings ofEMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="926" citStr="Wu, 1997" startWordPosition="129" endWordPosition="130">se a linguistically annotated reordering model for BTG-based statistical machine translation. The model incorporates linguistic knowledge to predict orders for both syntactic and non-syntactic phrases. The linguistic knowledge is automatically learned from source-side parse trees through an annotation algorithm. We empirically demonstrate that the proposed model leads to a significant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task. 1 Introduction In recent years, Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates s</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese Treebank with Semantic Knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP, Jeju Island,</booktitle>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang Qian. 2005. Parsing the Penn Chinese Treebank with Semantic Knowledge. In Proceedings ofIJCNLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL-COLING</booktitle>
<contexts>
<context position="1172" citStr="Xiong et al. (2006)" startWordPosition="167" endWordPosition="170">tically learned from source-side parse trees through an annotation algorithm. We empirically demonstrate that the proposed model leads to a significant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task. 1 Introduction In recent years, Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic p</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings ofACL-COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Phrase Reordering Model Integrating Syntactic Knowledge for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="1513" citStr="Zhang et al. (2007)" startWordPosition="220" endWordPosition="223">mar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score. In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above. The new model annotates each BTG node with linguistic knowledge </context>
</contexts>
<marker>Zhang, Li, Li, Zhou, 2007</marker>
<rawString>Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 2007. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. In Proceedings of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum Entropy Modeling Tooklkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/s0450736 /maxent toolkit.html.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum Entropy Modeling Tooklkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/s0450736 /maxent toolkit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>