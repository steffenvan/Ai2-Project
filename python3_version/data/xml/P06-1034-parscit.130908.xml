<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.994208">
Learning to Generate Naturalistic Utterances Using Reviews in Spoken
Dialogue Systems
</title>
<author confidence="0.993495">
Ryuichiro Higashinaka Rashmi Prasad Marilyn A. Walker
</author>
<affiliation confidence="0.997088">
NTT Corporation University of Pennsylvania University of Sheffield
</affiliation>
<email confidence="0.995134">
rh@cslab.kecl.ntt.co.jp rjprasad@linc.cis.upenn.edu walker@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959083333333">
Spoken language generation for dialogue
systems requires a dictionary of mappings
between semantic representations of con-
cepts the system wants to express and re-
alizations of those concepts. Dictionary
creation is a costly process; it is currently
done by hand for each dialogue domain.
We propose a novel unsupervised method
for learning such mappings from user re-
views in the target domain, and test it on
restaurant reviews. We test the hypothesis
that user reviews that provide individual
ratings for distinguished attributes of the
domain entity make it possible to map re-
view sentences to their semantic represen-
tation with high precision. Experimental
analyses show that the mappings learned
cover most of the domain ontology, and
provide good linguistic variation. A sub-
jective user evaluation shows that the con-
sistency between the semantic representa-
tions and the learned realizations is high
and that the naturalness of the realizations
is higher than a hand-crafted baseline.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882894736842">
One obstacle to the widespread deployment of
spoken dialogue systems is the cost involved
with hand-crafting the spoken language generation
module. Spoken language generation requires a
dictionary of mappings between semantic repre-
sentations of concepts the system wants to express
and realizations of those concepts. Dictionary cre-
ation is a costly process: an automatic method
for creating them would make dialogue technol-
ogy more scalable. A secondary benefit is that a
learned dictionary may produce more natural and
colloquial utterances.
We propose a novel method for mining user re-
views to automatically acquire a domain specific
generation dictionary for information presentation
in a dialogue system. Our hypothesis is that re-
views that provide individual ratings for various
distinguished attributes of review entities can be
used to map review sentences to a semantic rep-
</bodyText>
<table confidence="0.684053666666667">
An example user review (we8there.com)
Ratings Food=5, Service=5, Atmosphere=5,
Value=5, Overall=5
Review The best Spanish food in New York. I am
comment from Spain and I had my 28th birthday
there and we all had a great time. Salud!
</table>
<figure confidence="0.743252">
↓
Review comment after named entity recognition
The best {NE=foodtype, string=Spanish} {NE=food,
string=food, rating=5} in {NE=location, string=New
York}....
↓
Mapping between a semantic representation (a set of
relations) and a syntactic structure (DSyntS)
</figure>
<figureCaption confidence="0.9318215">
Figure 1: Example of procedure for acquiring a
generation dictionary mapping.
</figureCaption>
<bodyText confidence="0.999737">
resentation. Figure 1 shows a user review in the
restaurant domain, where we hypothesize that the
user rating food=5 indicates that the semantic rep-
resentation for the sentence “The best Spanish
food in New York” includes the relation ‘RESTAU-
RANT has foodquality=5.’
We apply the method to extract 451 mappings
from restaurant reviews. Experimental analyses
show that the mappings learned cover most of the
domain ontology, and provide good linguistic vari-
ation. A subjective user evaluation indicates that
the consistency between the semantic representa-
tions and the learned realizations is high and that
the naturalness of the realizations is significantly
higher than a hand-crafted baseline.
</bodyText>
<figure confidence="0.982608857142857">
• Relations:
RESTAURANT has FOODTYPE
RESTAURANT has foodquality=5
RESTAURANT has LOCATION
([foodtype, food=5, location] for shorthand.)
• DSyntS:
lexeme : food
class: common noun
number : sg
article : def
ATTR[lexeme : best 1
class : adjective J
lexeme : FOODTYPE
class: common noun
number: sg
article : no-art
lexeme : in
class : preposition
lexeme : LOCATION
class : proper noun
number: sg
article : no-art
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
II ⎣
⎤
⎦
⎤
⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
1� ⎦
⎡
ATTR ⎣
ATTR ⎡
⎢ ⎢ ⎢ ⎣
</figure>
<page confidence="0.99132">
265
</page>
<bodyText confidence="0.992024666666667">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 265–272,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Section 2 provides a step-by-step description of
the method. Sections 3 and 4 present the evalua-
tion results. Section 5 covers related work. Sec-
tion 6 summarizes and discusses future work.
</bodyText>
<sectionHeader confidence="0.435544" genericHeader="introduction">
2 Learning a Generation Dictionary
</sectionHeader>
<bodyText confidence="0.998673">
Our automatically created generation dictionary
consists of triples (U, R, S) representing a map-
ping between the original utterance U in the user
review, its semantic representation R(U), and its
syntactic structure S(U). Although templates are
widely used in many practical systems (Seneff and
Polifroni, 2000; Theune, 2003), we derive syn-
tactic structures to represent the potential realiza-
tions, in order to allow aggregation, and other
syntactic transformations of utterances, as well as
context specific prosody assignment (Walker et al.,
2003; Moore et al., 2004).
The method is outlined briefly in Fig. 1 and de-
scribed below. It comprises the following steps:
</bodyText>
<listItem confidence="0.99993325">
1. Collect user reviews on the web to create a
population of utterances U.
2. To derive semantic representations R(U):
• Identify distinguished attributes and
construct a domain ontology;
• Specify lexicalizations of attributes;
• Scrape webpages’ structured data for
named-entities;
• Tag named-entities.
3. Derive syntactic representations S(U).
4. Filter inappropriate mappings.
5. Add mappings (U, R, S) to dictionary.
</listItem>
<subsectionHeader confidence="0.999554">
2.1 Creating the corpus
</subsectionHeader>
<bodyText confidence="0.997528">
We created a corpus of restaurant reviews by
scraping 3,004 user reviews of 1,810 restau-
rants posted at we8there.com (http://www.we8-
there.com/), where each individual review in-
cludes a 1-to-5 Likert-scale rating of different
restaurant attributes. The corpus consists of
18,466 sentences.
</bodyText>
<subsectionHeader confidence="0.999833">
2.2 Deriving semantic representations
</subsectionHeader>
<bodyText confidence="0.863286105263158">
The distinguished attributes are extracted from the
webpages for each restaurant entity. They in-
clude attributes that the users are asked to rate,
i.e. food, service, atmosphere, value, and over-
all, which have scalar values. In addition, other
attributes are extracted from the webpage, such
as the name, foodtype and location of the restau-
rant, which have categorical values. The name
attribute is assumed to correspond to the restau-
rant entity. Given the distinguished attributes, a
Dist. Attr. Lexicalization
food food, meal
service service, staff, waitstaff, wait staff, server,
waiter, waitress
atmosphere atmosphere, decor, ambience, decoration
value value, price, overprice, pricey, expensive,
inexpensive, cheap, affordable, afford
overall recommend, place, experience, establish-
ment
</bodyText>
<tableCaption confidence="0.7303775">
Table 1: Lexicalizations for distinguished at-
tributes.
</tableCaption>
<bodyText confidence="0.9680635">
simple domain ontology can be automatically de-
rived by assuming that a meronymy relation, rep-
resented by the predicate ‘has’, holds between the
entity type (RESTAURANT) and the distinguished
attributes. Thus, the domain ontology consists of
the relations:
</bodyText>
<sectionHeader confidence="0.988746857142857" genericHeader="method">
RESTAURANT has foodquality
RESTAURANT has servicequality
RESTAURANT has valuequality
RESTAURANT has atmospherequality
RESTAURANT has overallquality
RESTAURANT has foodtype
RESTAURANT has location
</sectionHeader>
<bodyText confidence="0.997809535714286">
We assume that, although users may discuss
other attributes of the entity, at least some of the
utterances in the reviews realize the relations spec-
ified in the ontology. Our problem then is to iden-
tify these utterances. We test the hypothesis that,
if an utterance U contains named-entities corre-
sponding to the distinguished attributes, that R for
that utterance includes the relation concerning that
attribute in the domain ontology.
We define named-entities for lexicalizations of
the distinguished attributes, starting with the seed
word for that attribute on the webpage (Table 1).1
For named-entity recognition, we use GATE (Cun-
ningham et al., 2002), augmented with named-
entity lists for locations, food types, restaurant
names, and food subtypes (e.g. pizza), scraped
from the we8there webpages.
We also hypothesize that the rating given for the
distinguished attribute specifies the scalar value
of the relation. For example, a sentence contain-
ing food or meal is assumed to realize the re-
lation ‘RESTAURANT has foodquality.’, and the
value of the foodquality attribute is assumed to be
the value specified in the user rating for that at-
tribute, e.g. ‘RESTAURANT has foodquality = 5’ in
Fig. 1. Similarly, the other relations in Fig. 1 are
assumed to be realized by the utterance “The best
Spanish food in New York” because it contains
</bodyText>
<footnote confidence="0.967438333333333">
1In future, we will investigate other techniques for boot-
strapping these lexicalizations from the seed word on the
webpage.
</footnote>
<figure confidence="0.543967">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎩
</figure>
<page confidence="0.992164">
266
</page>
<table confidence="0.999187833333333">
filter filtered retained
No Relations Filter 7,947 10,519
Other Relations Filter 5,351 5,168
Contextual Filter 2,973 2,195
Unknown Words Filter 1,467 728
Parsing Filter 216 512
</table>
<tableCaption confidence="0.8413215">
Table 2: Filtering statistics: the number of sen-
tences filtered and retained by each filter.
</tableCaption>
<bodyText confidence="0.999091">
one FOODTYPE named-entity and one LOCATION
named-entity. Values of categorical attributes are
replaced by variables representing their type be-
fore the learned mappings are added to the dictio-
nary, as shown in Fig. 1.
</bodyText>
<subsectionHeader confidence="0.999337">
2.3 Parsing and DSyntS conversion
</subsectionHeader>
<bodyText confidence="0.999994583333333">
We adopt Deep Syntactic Structures (DSyntSs) as
a format for syntactic structures because they can
be realized by the fast portable realizer RealPro
(Lavoie and Rambow, 1997). Since DSyntSs are a
type of dependency structure, we first process the
sentences with Minipar (Lin, 1998), and then con-
vert Minipar’s representation into DSyntS. Since
user reviews are different from the newspaper ar-
ticles on which Minipar was trained, the output
of Minipar can be inaccurate, leading to failure in
conversion. We check whether conversion is suc-
cessful in the filtering stage.
</bodyText>
<subsectionHeader confidence="0.994191">
2.4 Filtering
</subsectionHeader>
<bodyText confidence="0.997934807692307">
The goal of filtering is to identify U that realize
the distinguished attributes and to guarantee high
precision for the learned mappings. Recall is less
important since systems need to convey requested
information as accurately as possible. Our proce-
dure for deriving semantic representations is based
on the hypothesis that if U contains named-entities
that realize the distinguished attributes, that R will
include the relevant relation in the domain ontol-
ogy. We also assume that if U contains named-
entities that are not covered by the domain ontol-
ogy, or words indicating that the meaning of U de-
pends on the surrounding context, that R will not
completely characterizes the meaning of U, and so
U should be eliminated. We also require an accu-
rate S for U. Therefore, the filters described be-
low eliminate U that (1) realize semantic relations
not in the ontology; (2) contain words indicating
that its meaning depends on the context; (3) con-
tain unknown words; or (4) cannot be parsed ac-
curately.
No Relations Filter: The sentence does not con-
tain any named-entities for the distinguished
attributes.
Other Relations Filter: The sentence contains
named-entities for food subtypes, person
</bodyText>
<table confidence="0.47587825">
Rating 1 2 3 4 5 Total
Dist.Attr.
food 5 8 6 18 57 94
service 15 3 6 17 56 97
atmosphere 0 3 3 8 31 45
value 0 0 1 8 12 21
overall 3 2 5 15 45 70
Total 23 15 21 64 201 327
</table>
<tableCaption confidence="0.882245">
Table 3: Domain coverage of single scalar-valued
relation mappings.
</tableCaption>
<bodyText confidence="0.996318236842105">
names, country names, dates (e.g., today, to-
morrow, Aug. 26th) or prices (e.g., 12 dol-
lars), or POS tag CD for numerals. These in-
dicate relations not in the ontology.
Contextual Filter: The sentence contains index-
icals such as I, you, that or cohesive markers
of rhetorical relations that connect it to some
part of the preceding text, which means that
the sentence cannot be interpreted out of con-
text. These include discourse markers, such
as list item markers with LS as the POS tag,
that signal the organization structure of the
text (Hirschberg and Litman, 1987), as well
as discourse connectives that signal semantic
and pragmatic relations of the sentence with
other parts of the text (Knott, 1996), such as
coordinating conjunctions at the beginning of
the utterance like and and but etc., and con-
junct adverbs such as however, also, then.
Unknown Words Filter: The sentence contains
words not in WordNet (Fellbaum, 1998)
(which includes typographical errors), or
POS tags contain NN (Noun), which may in-
dicate an unknown named-entity, or the sen-
tence has more than a fixed length of words,2
indicating that its meaning may not be esti-
mated solely by named entities.
Parsing Filter: The sentence fails the parsing to
DSyntS conversion. Failures are automati-
cally detected by comparing the original sen-
tence with the one realized by RealPro taking
the converted DSyntS as an input.
We apply the filters, in a cascading manner, to the
18,466 sentences with semantic representations.
As a result, we obtain 512 (2.8%) mappings of
(U, R, S). After removing 61 duplicates, 451 dis-
tinct (2.4%) mappings remain. Table 2 shows the
number of sentences eliminated by each filter.
</bodyText>
<sectionHeader confidence="0.995919" genericHeader="method">
3 Objective Evaluation
</sectionHeader>
<bodyText confidence="0.999447">
We evaluate the learned expressions with respect
to domain coverage, linguistic variation and gen-
erativity.
</bodyText>
<footnote confidence="0.935591">
2We used 20 as a threshold.
</footnote>
<page confidence="0.979429">
267
</page>
<table confidence="0.910408041666667">
# Combination of Dist. Attrs Count
1 food-service 39
2 food-value 21
3 atmosphere-food 14
4 atmosphere-service 10
5 atmosphere-food-service 7
6 food-foodtype 4
7 atmosphere-food-value 4
8 location-overall 3
9 food-foodtype-value 3
10 food-service-value 2
11 food-foodtype-location 2
12 food-overall 2
13 atmosphere-foodtype 2
14 atmosphere-overall 2
15 service-value 1
16 overall-service 1
17 overall-value 1
18 foodtype-overall 1
19 food-foodtype-location-overall 1
20 atmosphere-food-service-value 1
21 atmosphere-food-overall- 1
service-value
Total 122
</table>
<tableCaption confidence="0.973216">
Table 4: Counts for multi-relation mappings.
</tableCaption>
<subsectionHeader confidence="0.994122">
3.1 Domain Coverage
</subsectionHeader>
<bodyText confidence="0.999987208333333">
To be usable for a dialogue system, the mappings
must have good domain coverage. Table 3 shows
the distribution of the 327 mappings realizing a
single scalar-valued relation, categorized by the
associated rating score.3 For example, there are 57
mappings with R of ‘RESTAURANT has foodqual-
ity=5,’ and a large number of mappings for both
the foodquality and servicequality relations. Al-
though we could not obtain mappings for some re-
lations such as price={1,2}, coverage for express-
ing a single relation is fairly complete.
There are also mappings that express several re-
lations. Table 4 shows the counts of mappings
for multi-relation mappings, with those contain-
ing a food or service relation occurring more fre-
quently as in the single scalar-valued relation map-
pings. We found only 21 combinations of rela-
tions, which is surprising given the large poten-
tial number of combinations (There are 50 com-
binations if we treat relations with different scalar
values differently). We also find that most of the
mappings have two or three relations, perhaps sug-
gesting that system utterances should not express
too many relations in a single sentence.
</bodyText>
<subsectionHeader confidence="0.998198">
3.2 Linguistic Variation
</subsectionHeader>
<bodyText confidence="0.9999256">
We also wish to assess whether the linguistic
variation of the learned mappings was greater
than what we could easily have generated with a
hand-crafted dictionary, or a hand-crafted dictio-
nary augmented with aggregation operators, as in
</bodyText>
<footnote confidence="0.6006685">
3There are two other single-relation but not scalar-valued
mappings that concern LOCATION in our mappings.
</footnote>
<bodyText confidence="0.998832404761905">
(Walker et al., 2003). Thus, we first categorized
the mappings by the patterns of the DSyntSs. Ta-
ble 5 shows the most common syntactic patterns
(more than 10 occurrences), indicating that 30%
of the learned patterns consist of the simple form
“X is ADJ” where ADJ is an adjective, or “X is RB
ADJ,” where RB is a degree modifier. Furthermore,
up to 55% of the learned mappings could be gen-
erated from these basic patterns by the application
of a combination operator that coordinates mul-
tiple adjectives, or coordinates predications over
distinct attributes. However, there are 137 syntac-
tic patterns in all, 97 with unique syntactic struc-
tures and 21 with two occurrences, accounting for
45% of the learned mappings. Table 6 shows ex-
amples of learned mappings with distinct syntactic
structures. It would be surprising to see this type
of variety in a hand-crafted generation dictionary.
In addition, the learned mappings contain 275 dis-
tinct lexemes, with a minimum of 2, maximum of
15, and mean of 4.63 lexemes per DSyntS, indi-
cating that the method extracts a wide variety of
expressions of varying lengths.
Another interesting aspect of the learned map-
pings is the wide variety of adjectival phrases
(APs) in the common patterns. Tables 7 and 8
show the APs in single scalar-valued relation map-
pings for food and service categorized by the as-
sociated ratings. Tables for atmosphere, value and
overall can be found in the Appendix. Moreover,
the meanings for some of the learned APs are very
specific to the particular attribute, e.g. cold and
burnt associated with foodquality of 1, attentive
and prompt for servicequality of 5, silly and inat-
tentive for servicequality of 1. and mellow for at-
mosphere of 5. In addition, our method places the
adjectival phrases (APs) in the common patterns
on a more fine-grained scale of 1 to 5, similar to
the strength classifications in (Wilson et al., 2004),
in contrast to other automatic methods that clas-
sify expressions into a binary positive or negative
polarity (e.g. (Turney, 2002)).
</bodyText>
<subsectionHeader confidence="0.983999">
3.3 Generativity
</subsectionHeader>
<bodyText confidence="0.999909181818182">
Our motivation for deriving syntactic representa-
tions for the learned expressions was the possibil-
ity of using an off-the-shelf sentence planner to
derive new combinations of relations, and apply
aggregation and other syntactic transformations.
We examined how many of the learned DSyntSs
can be combined with each other, by taking ev-
ery pair of DSyntSs in the mappings and apply-
ing the built-in merge operation in the SPaRKy
generator (Walker et al., 2003). We found that
only 306 combinations out of a potential 81,318
</bodyText>
<page confidence="0.979664">
268
</page>
<table confidence="0.99601625">
# syntactic pattern example utterance count ratio accum.
1 NN VB JJ The atmosphere is wonderful. 92 20.4% 20.4%
2 NN VB RB JJ The atmosphere was very nice. 52 11.5% 31.9%
3 JJ NN Bad service. 36 8.0% 39.9%
4 NN VB JJ CC JJ The food was flavorful but cold. 25 5.5% 45.5%
5 RB JJ NN Very trendy ambience. 22 4.9% 50.3%
6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great. 13 2.9% 53.2%
7 NN CC NN VB JJ The food and service were fantastic. 10 2.2% 55.4%
</table>
<tableCaption confidence="0.9277365">
Table 5: Common syntactic patterns of DSyntSs, flattened to a POS sequence for readability. NN, VB,
JJ, RB, CC stand for noun, verb, adjective, adverb, and conjunction, respectively.
</tableCaption>
<bodyText confidence="0.596205">
[overall=1, value=2] Very disappointing experience for
the money charged.
[food=5, value=5] The food is excellent and plentiful at a
reasonable price.
[food=5, service=5] The food is exquisite as well as the
service and setting.
[food=5, service=5] The food was spectacular and so was
the service.
[food=5, foodtype, value=5] Best FOODTYPE food with
a great value for money.
[food=5, foodtype, value=5] An absolutely outstanding
value with fantastic FOODTYPE food.
[food=5, foodtype, location, overall=5] This is the best
place to eat FOODTYPE food in LOCATION.
[food=5, foodtype] Simply amazing FOODTYPE food.
[food=5, foodtype] RESTAURANTNAME is the best of the
best for FOODTYPE food.
[food=5] The food is to die for.
[food=5] What incredible food.
[food=4] Very pleasantly surprised by the food.
[food=1] The food has gone downhill.
[atmosphere=5, overall=5] This is a quiet little place
with great atmosphere.
[atmosphere=5, food=5, overall=5, service=5, value=5]
The food, service and ambience of the place are all fabu-
lous and the prices are downright cheap.
</bodyText>
<tableCaption confidence="0.829765666666667">
Table 6: Acquired generation patterns (with short-
hand for relations in square brackets) whose syn-
tactic patterns occurred only once.
</tableCaption>
<bodyText confidence="0.9977921875">
combinations (0.37%) were successful. This is
because the merge operation in SPaRKy requires
that the subjects and the verbs of the two DSyntSs
are identical, e.g. the subject is RESTAURANT and
verb is has, whereas the learned DSyntSs often
place the attribute in subject position as a definite
noun phrase. However, the learned DSyntS can
be incorporated into SPaRKy using the semantic
representations to substitute learned DSyntSs into
nodes in the sentence plan tree. Figure 2 shows
some example utterances generated by SPaRKy
with its original dictionary and example utterances
when the learned mappings are incorporated. The
resulting utterances seem more natural and collo-
quial; we examine whether this is true in the next
section.
</bodyText>
<sectionHeader confidence="0.996378" genericHeader="method">
4 Subjective Evaluation
</sectionHeader>
<bodyText confidence="0.972281037735849">
We evaluate the obtained mappings in two re-
spects: the consistency between the automatically
derived semantic representation and the realiza-
food=1 awful, bad, burnt, cold, very ordinary
food=2 acceptable, bad, flavored, not enough, very
bland, very good
food=3 adequate, bland and mediocre, flavorful but
cold, pretty good, rather bland, very good
food=4 absolutely wonderful, awesome, decent, ex-
cellent, good, good and generous, great, out-
standing, rather good, really good, tradi-
tional, very fresh and tasty, very good, very
very good
food=5 absolutely delicious, absolutely fantastic, ab-
solutely great, absolutely terrific, ample, well
seasoned and hot, awesome, best, delectable
and plentiful, delicious, delicious but simple,
excellent, exquisite, fabulous, fancy but tasty,
fantastic, fresh, good, great, hot, incredible,
just fantastic, large and satisfying, outstand-
ing, plentiful and outstanding, plentiful and
tasty, quick and hot, simply great, so deli-
cious, so very tasty, superb, terrific, tremen-
dous, very good, wonderful
Table 7: Adjectival phrases (APs) in single scalar-
valued relation mappings for foodquality.
tion, and the naturalness of the realization.
For comparison, we used a baseline of hand-
crafted mappings from (Walker et al., 2003) ex-
cept that we changed the word decor to at-
mosphere and added five mappings for overall.
For scalar relations, this consists of the realiza-
tion “RESTAURANT has ADJ LEX” where ADJ is
mediocre, decent, good, very good, or excellent for
rating values 1-5, and LEX is food quality, service,
atmosphere, value, or overall depending on the re-
lation. RESTAURANT is filled with the name of
a restaurant at runtime. For example, ‘RESTAU-
RANT has foodquality=1’ is realized as “RESTAU-
RANT has mediocre food quality.” The location
and food type relations are mapped to “RESTAU-
RANT is located in LOCATION” and “RESTAU-
RANT is a FOODTYPE restaurant.”
The learned mappings include 23 distinct se-
mantic representations for a single-relation (22 for
scalar-valued relations and one for location) and
50 for multi-relations. Therefore, using the hand-
crafted mappings, we first created 23 utterances
for the single-relations. We then created three ut-
terances for each of 50 multi-relations using differ-
ent clause-combining operations from (Walker et
al., 2003). This gave a total of 173 baseline utter-
ances, which together with 451 learned mappings,
</bodyText>
<page confidence="0.997103">
269
</page>
<figureCaption confidence="0.922219612903226">
service=1 awful, bad, great, horrendous, horrible,
inattentive, forgetful and slow, marginal,
really slow, silly and inattentive, still
marginal, terrible, young
service=2 overly slow, very slow and inattentive
service=3 bad, bland and mediocre, friendly and
knowledgeable, good, pleasant, prompt,
very friendly
service=4 all very warm and welcoming, attentive,
extremely friendly and good, extremely
pleasant, fantastic, friendly, friendly and
helpful, good, great, great and courteous,
prompt and friendly, really friendly, so
nice, swift and friendly, very friendly, very
friendly and accommodating
service=5 all courteous, excellent, excellent and
friendly, extremely friendly, fabulous,
fantastic, friendly, friendly and helpful,
friendly and very attentive, good, great,
great, prompt and courteous, happy and
friendly, impeccable, intrusive, legendary,
outstanding, pleasant, polite, attentive and
prompt, prompt and courteous, prompt
and pleasant, quick and cheerful, stupen-
dous, superb, the most attentive, unbeliev-
able, very attentive, very congenial, very
courteous, very friendly, very friendly and
helpful, very friendly and pleasant, very
friendly and totally personal, very friendly
and welcoming, very good, very helpful,
very timely, warm and friendly, wonderful
</figureCaption>
<bodyText confidence="0.979318071428572">
Table 8: Adjectival phrases (APs) in single scalar-
valued relation mappings for servicequality.
yielded 624 utterances for evaluation.
Ten subjects, all native English speakers, eval-
uated the mappings by reading them from a web-
page. For each system utterance, the subjects were
asked to express their degree of agreement, on a
scale of 1 (lowest) to 5 (highest), with the state-
ment (a) The meaning of the utterance is consis-
tent with the ratings expressing their semantics,
and with the statement (b) The style of the utter-
ance is very natural and colloquial. They were
asked not to correct their decisions and also to rate
each utterance on its own merit.
</bodyText>
<subsectionHeader confidence="0.838793">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999986066666667">
Table 9 shows the means and standard deviations
of the scores for baseline vs. learned utterances for
consistency and naturalness. A t-test shows that
the consistency of the learned expression is signifi-
cantly lower than the baseline (df=4712, p &lt; .001)
but that their naturalness is significantly higher
than the baseline (df=3107, p &lt; .001). However,
consistency is still high. Only 14 of the learned
utterances (shown in Tab. 10) have a mean consis-
tency score lower than 3, which indicates that, by
and large, the human judges felt that the inferred
semantic representations were consistent with the
meaning of the learned expressions. The correla-
tion coefficient between consistency and natural-
ness scores is 0.42, which indicates that consis-
</bodyText>
<subsectionHeader confidence="0.70374">
Original SPaRKy utterances
</subsectionHeader>
<listItem confidence="0.825139">
• Babbo has the best overall quality among the selected
restaurants with excellent decor, excellent service and
superb food quality.
• Babbo has excellent decor and superb food quality
with excellent service. It has the best overall quality
among the selected restaurants.
</listItem>
<bodyText confidence="0.608676">
↓
</bodyText>
<subsectionHeader confidence="0.801355">
Combination of SPaRKy and learned DSyntS
</subsectionHeader>
<listItem confidence="0.8697574">
• Because the food is excellent, the wait staff is pro-
fessional and the decor is beautiful and very com-
fortable, Babbo has the best overall quality among the
selected restaurants.
• Babbo has the best overall quality among the selected
restaurants because atmosphere is exceptionally nice,
food is excellent and the service is superb.
• Babbo has superb food quality, the service is excep-
tional and the atmosphere is very creative. It has the
best overall quality among the selected restaurants.
</listItem>
<figureCaption confidence="0.9783765">
Figure 2: Utterances incorporating learned
DSyntSs (Bold font) in SPaRKy.
</figureCaption>
<table confidence="0.9356595">
baseline learned stat.
mean sd. mean sd. sig.
Consistency 4.714 0.588 4.459 0.890 +
Naturalness 4.227 0.852 4.613 0.844 +
</table>
<tableCaption confidence="0.987526">
Table 9: Consistency and naturalness scores aver-
aged over 10 subjects.
</tableCaption>
<bodyText confidence="0.964696916666667">
tency does not greatly relate to naturalness.
We also performed an ANOVA (ANalysis Of
VAriance) of the effect of each relation in R on
naturalness and consistency. There were no sig-
nificant effects except that mappings combining
food, service, and atmosphere were significantly
worse (df=1, F=7.79, p=0.005). However, there
is a trend for mappings to be rated higher for
the food attribute (df=1, F=3.14, p=0.08) and the
value attribute (df=1, F=3.55, p=0.06) for consis-
tency, suggesting that perhaps it is easier to learn
some mappings than others.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999799375">
Automatically finding sentences with the same
meaning has been extensively studied in the field
of automatic paraphrasing using parallel corpora
and corpora with multiple descriptions of the same
events (Barzilay and McKeown, 2001; Barzilay
and Lee, 2003). Other work finds predicates of
similar meanings by using the similarity of con-
texts around the predicates (Lin and Pantel, 2001).
However, these studies find a set of sentences with
the same meaning, but do not associate a specific
meaning with the sentences. One exception is
(Barzilay and Lee, 2002), which derives mappings
between semantic representations and realizations
using a parallel (but unaligned) corpus consisting
of both complex semantic input and correspond-
ing natural language verbalizations for mathemat-
</bodyText>
<page confidence="0.986628">
270
</page>
<table confidence="0.920863217391304">
shorthand for relations and utterance score
[food=4] The food is delicious and beautifully 2.9
prepared.
[overall=4] A wonderful experience. 2.9
[service=3] The service is bland and mediocre. 2.8
[atmosphere=2] The atmosphere here is eclec- 2.6
tic.
[overall=3] Really fancy place. 2.6
[food=3, service=4] Wonderful service and 2.5
great food.
[service=4] The service is fantastic. 2.5
[overall=2] The RESTAURANTNAME is once a 2.2
great place to go and socialize.
[atmosphere=2] The atmosphere is unique and 2.0
pleasant.
[food=5, foodtype] FOODTYPE and FOODTYPE 1.8
food.
[service=3] Waitstaff is friendly and knowl- 1.7
edgeable.
[atmosphere=5, food=5, service=5] The atmo- 1.6
sphere, food and service.
[overall=3] Overall, a great experience. 1.4
[service=1] The waiter is great. 1.4
</table>
<tableCaption confidence="0.8896615">
Table 10: The 14 utterances with consistency
scores below 3.
</tableCaption>
<bodyText confidence="0.999938578947368">
ical proofs. However, our technique does not re-
quire parallel corpora or previously existing se-
mantic transcripts or labeling, and user reviews are
widely available in many different domains (See
http://www.epinions.com/).
There is also significant previous work on min-
ing user reviews. For example, Hu and Liu (2005)
use reviews to find adjectives to describe products,
and Popescu and Etzioni (2005) automatically find
features of a product together with the polarity of
adjectives used to describe them. They both aim at
summarizing reviews so that users can make deci-
sions easily. Our method is also capable of finding
polarities of modifying expressions including ad-
jectives, but on a more fine-grained scale of 1 to
5. However, it might be possible to use their ap-
proach to create rating information for raw review
texts as in (Pang and Lee, 2005), so that we can
create mappings from reviews without ratings.
</bodyText>
<sectionHeader confidence="0.996031" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999994911764706">
We proposed automatically obtaining mappings
between semantic representations and realizations
from reviews with individual ratings. The results
show that: (1) the learned mappings provide good
coverage of the domain ontology and exhibit good
linguistic variation; (2) the consistency between
the semantic representations and realizations is
high; and (3) the naturalness of the realizations are
significantly higher than the baseline.
There are also limitations in our method. Even
though consistency is rated highly by human sub-
jects, this may actually be a judgement of whether
the polarity of the learned mapping is correctly
placed on the 1 to 5 rating scale. Thus, alter-
nate ways of expressing, for example foodqual-
ity=5, shown in Table 7, cannot be guaranteed to
be synonymous, which may be required for use in
spoken language generation. Rather, an examina-
tion of the adjectival phrases in Table 7 shows that
different aspects of the food are discussed. For
example ample and plentiful refer to the portion
size, fancy may refer to the presentation, and deli-
cious describes the flavors. This suggests that per-
haps the ontology would benefit from represent-
ing these sub-attributes of the food attribute, and
sub-attributes in general. Another problem with
consistency is that the same AP, e.g. very good
in Table 7 may appear with multiple ratings. For
example, very good is used for every foodquality
rating from 2 to 5. Thus some further automatic
or by-hand analysis is required to refine what is
learned before actual use in spoken language gen-
eration. Still, our method could reduce the amount
of time a system designer spends developing the
spoken language generator, and increase the natu-
ralness of spoken language generation.
Another issue is that the recall appears to be
quite low given that all of the sentences concern
the same domain: only 2.4% of the sentences
could be used to create the mappings. One way
to increase recall might be to automatically aug-
ment the list of distinguished attribute lexicaliza-
tions, using WordNet or work on automatic iden-
tification of synonyms, such as (Lin and Pantel,
2001). However, the method here has high pre-
cision, and automatic techniques may introduce
noise. A related issue is that the filters are in some
cases too strict. For example the contextual fil-
ter is based on POS-tags, so that sentences that do
not require the prior context for their interpreta-
tion are eliminated, such as sentences containing
subordinating conjunctions like because, when, if,
whose arguments are both given in the same sen-
tence (Prasad et al., 2005). In addition, recall is
affected by the domain ontology, and the automat-
ically constructed domain ontology from the re-
view webpages may not cover all of the domain.
In some review domains, the attributes that get
individual ratings are a limited subset of the do-
main ontology. Techniques for automatic feature
identification (Hu and Liu, 2005; Popescu and Et-
zioni, 2005) could possibly help here, although
these techniques currently have the limitation that
they do not automatically identify different lexi-
calizations of the same feature.
A different type of limitation is that dialogue
systems need to generate utterances for informa-
tion gathering whereas the mappings we obtained
</bodyText>
<page confidence="0.989878">
271
</page>
<bodyText confidence="0.999985357142857">
can only be used for information presentation.
Thus these would have to be constructed by hand,
as in current practice, or perhaps other types of
corpora or resources could be utilized. In addi-
tion, the utility of syntactic structures in the map-
pings should be further examined, especially given
the failures in DSyntS conversion. An alternative
would be to leave some sentences unparsed and
use them as templates with hybrid generation tech-
niques (White and Caldwell, 1998). Finally, while
we believe that this technique will apply across do-
mains, it would be useful to test it on domains such
as movie reviews or product reviews, which have
more complex domain ontologies.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999712333333333">
We thank the anonymous reviewers for their help-
ful comments. This work was supported by a
Royal Society Wolfson award to Marilyn Walker
and a research collaboration grant from NTT to
the Cognitive Systems Group at the University of
Sheffield.
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853430769231">
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc.
EMNLP, pages 164–171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. HLT/NAACL, pages 16–23.
Regina Barzilay and Kathleen McKeown. 2001. Extracting
paraphrases from a parallel corpus. In Proc. 39th ACL,
pages 50–57.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP tools
and applications. In Proc. 40th ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication). The
MIT Press.
Julia Hirschberg and Diane. J. Litman. 1987. Now let’s talk
about NOW: Identifying cue phrases intonationally. In
Proc. 25th ACL, pages 163–171.
Minqing Hu and Bing Liu. 2005. Mining and summarizing
customer reviews. In Proc. KDD, pages 168–177.
Alistair Knott. 1996. A Data-Driven Methodology for Moti-
vating a Set of Coherence Relations. Ph.D. thesis, Univer-
sity of Edinburgh, Edinburgh.
Benoit Lavoie and Owen Rambow. 1997. A fast and portable
realizer for text generation systems. In Proc. 5th Applied
NLP, pages 265–268.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343–360.
Dekang Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Systems.
Johanna D. Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, comparative
descriptions in spoken dialogue. In Proc. 7th FLAIR.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with re-
spect to rating scales. In Proc. 43st ACL, pages 115–124.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Proc.
HLT/EMNLP, pages 339–346.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, and Bonnie Webber. 2005. The Penn
Discourse TreeBank as a resource for natural language
generation. In Proc. Corpus Linguistics Workshop on Us-
ing Corpora for NLG.
Stephanie Seneff and Joseph Polifroni. 2000. Formal and
natural language generation in the mercury conversational
system. In Proc. ICSLP, volume 2, pages 767–770.
Mari¨et Theune. 2003. From monologue to dialogue: natural
language generation in OVIS. In AAAI 2003 Spring Sym-
posium on Natural Language Generation in Written and
Spoken Dialogue, pages 141–150.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. 40th ACL, pages 417–424.
Marilyn Walker, Rashmi Prasad, and Amanda Stent. 2003.
A trainable generator for recommendations in multimodal
dialog. In Proc. Eurospeech, pages 1697–1700.
Michael White and Ted Caldwell. 1998. EXEMPLARS: A
practical, extensible framework for dynamic text genera-
tion. In Proc. INLG, pages 266–275.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opinion
clauses. In Proc. AAAI, pages 761–769.
</reference>
<sectionHeader confidence="0.985434" genericHeader="references">
Appendix
</sectionHeader>
<bodyText confidence="0.999480487179487">
Adjectival phrases (APs) in single scalar-valued
relation mappings for atmosphere, value, and
overall.
atmosphere=2 eclectic, unique and pleasant
atmosphere=3 busy, pleasant but extremely hot
atmosphere=4 fantastic, great, quite nice and simple,
typical, very casual, very trendy, wonder-
ful
atmosphere=5 beautiful, comfortable, excellent, great,
interior, lovely, mellow, nice, nice and
comfortable, phenomenal, pleasant, quite
pleasant, unbelievably beautiful, very
comfortable, very cozy, very friendly,
very intimate, very nice, very nice and
relaxing, very pleasant, very relaxing,
warm and contemporary, warm and very
comfortable, wonderful
value=3 very reasonable
value=4 great, pretty good, reasonable, very good
value=5 best, extremely reasonable, good, great,
reasonable, totally reasonable, very good,
very reasonable
overall=1 just bad, nice, thoroughly humiliating
overall=2 great, really bad
overall=3 bad, decent, great, interesting, really
fancy
overall=4 excellent, good, great, just great, never
busy, not very busy, outstanding, recom-
mended, wonderful
overall=5 amazing, awesome, capacious, delight-
ful, extremely pleasant, fantastic, good,
great, local, marvelous, neat, new, over-
all, overwhelmingly pleasant, pampering,
peaceful but idyllic, really cool, really
great, really neat, really nice, special,
tasty, truly great, ultimate, unique and en-
joyable, very enjoyable, very excellent,
very good, very nice, very wonderful,
warm and friendly, wonderful
</bodyText>
<page confidence="0.977762">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957695">
<title confidence="0.9990955">Learning to Generate Naturalistic Utterances Using Reviews in Spoken Dialogue Systems</title>
<author confidence="0.999562">Ryuichiro Higashinaka Rashmi Prasad Marilyn A Walker</author>
<affiliation confidence="0.981953">NTT Corporation University of Pennsylvania University of Sheffield</affiliation>
<email confidence="0.995728">rh@cslab.kecl.ntt.co.jprjprasad@linc.cis.upenn.eduwalker@dcs.shef.ac.uk</email>
<abstract confidence="0.99922232">Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts. Dictionary creation is a costly process; it is currently done by hand for each dialogue domain. We propose a novel unsupervised method for learning such mappings from user reviews in the target domain, and test it on restaurant reviews. We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision. Experimental analyses show that the mappings learned cover most of the domain ontology, and provide good linguistic variation. A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>164--171</pages>
<contexts>
<context position="27643" citStr="Barzilay and Lee, 2002" startWordPosition="4332" endWordPosition="4335"> it is easier to learn some mappings than others. 5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corpora with multiple descriptions of the same events (Barzilay and McKeown, 2001; Barzilay and Lee, 2003). Other work finds predicates of similar meanings by using the similarity of contexts around the predicates (Lin and Pantel, 2001). However, these studies find a set of sentences with the same meaning, but do not associate a specific meaning with the sentences. One exception is (Barzilay and Lee, 2002), which derives mappings between semantic representations and realizations using a parallel (but unaligned) corpus consisting of both complex semantic input and corresponding natural language verbalizations for mathemat270 shorthand for relations and utterance score [food=4] The food is delicious and beautifully 2.9 prepared. [overall=4] A wonderful experience. 2.9 [service=3] The service is bland and mediocre. 2.8 [atmosphere=2] The atmosphere here is eclec- 2.6 tic. [overall=3] Really fancy place. 2.6 [food=3, service=4] Wonderful service and 2.5 great food. [service=4] The service is fantas</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proc. EMNLP, pages 164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proc. HLT/NAACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="27340" citStr="Barzilay and Lee, 2003" startWordPosition="4282" endWordPosition="4285"> except that mappings combining food, service, and atmosphere were significantly worse (df=1, F=7.79, p=0.005). However, there is a trend for mappings to be rated higher for the food attribute (df=1, F=3.14, p=0.08) and the value attribute (df=1, F=3.55, p=0.06) for consistency, suggesting that perhaps it is easier to learn some mappings than others. 5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corpora with multiple descriptions of the same events (Barzilay and McKeown, 2001; Barzilay and Lee, 2003). Other work finds predicates of similar meanings by using the similarity of contexts around the predicates (Lin and Pantel, 2001). However, these studies find a set of sentences with the same meaning, but do not associate a specific meaning with the sentences. One exception is (Barzilay and Lee, 2002), which derives mappings between semantic representations and realizations using a parallel (but unaligned) corpus consisting of both complex semantic input and corresponding natural language verbalizations for mathemat270 shorthand for relations and utterance score [food=4] The food is delicious</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proc. HLT/NAACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proc. 39th ACL,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="27315" citStr="Barzilay and McKeown, 2001" startWordPosition="4278" endWordPosition="4281"> were no significant effects except that mappings combining food, service, and atmosphere were significantly worse (df=1, F=7.79, p=0.005). However, there is a trend for mappings to be rated higher for the food attribute (df=1, F=3.14, p=0.08) and the value attribute (df=1, F=3.55, p=0.06) for consistency, suggesting that perhaps it is easier to learn some mappings than others. 5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corpora with multiple descriptions of the same events (Barzilay and McKeown, 2001; Barzilay and Lee, 2003). Other work finds predicates of similar meanings by using the similarity of contexts around the predicates (Lin and Pantel, 2001). However, these studies find a set of sentences with the same meaning, but do not associate a specific meaning with the sentences. One exception is (Barzilay and Lee, 2002), which derives mappings between semantic representations and realizations using a parallel (but unaligned) corpus consisting of both complex semantic input and corresponding natural language verbalizations for mathemat270 shorthand for relations and utterance score [food</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proc. 39th ACL, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL.</booktitle>
<contexts>
<context position="7778" citStr="Cunningham et al., 2002" startWordPosition="1174" endWordPosition="1178">ay discuss other attributes of the entity, at least some of the utterances in the reviews realize the relations specified in the ontology. Our problem then is to identify these utterances. We test the hypothesis that, if an utterance U contains named-entities corresponding to the distinguished attributes, that R for that utterance includes the relation concerning that attribute in the domain ontology. We define named-entities for lexicalizations of the distinguished attributes, starting with the seed word for that attribute on the webpage (Table 1).1 For named-entity recognition, we use GATE (Cunningham et al., 2002), augmented with namedentity lists for locations, food types, restaurant names, and food subtypes (e.g. pizza), scraped from the we8there webpages. We also hypothesize that the rating given for the distinguished attribute specifies the scalar value of the relation. For example, a sentence containing food or meal is assumed to realize the relation ‘RESTAURANT has foodquality.’, and the value of the foodquality attribute is assumed to be the value specified in the user rating for that attribute, e.g. ‘RESTAURANT has foodquality = 5’ in Fig. 1. Similarly, the other relations in Fig. 1 are assumed</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proc. 40th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12085" citStr="Fellbaum, 1998" startWordPosition="1894" endWordPosition="1895">f the preceding text, which means that the sentence cannot be interpreted out of context. These include discourse markers, such as list item markers with LS as the POS tag, that signal the organization structure of the text (Hirschberg and Litman, 1987), as well as discourse connectives that signal semantic and pragmatic relations of the sentence with other parts of the text (Knott, 1996), such as coordinating conjunctions at the beginning of the utterance like and and but etc., and conjunct adverbs such as however, also, then. Unknown Words Filter: The sentence contains words not in WordNet (Fellbaum, 1998) (which includes typographical errors), or POS tags contain NN (Noun), which may indicate an unknown named-entity, or the sentence has more than a fixed length of words,2 indicating that its meaning may not be estimated solely by named entities. Parsing Filter: The sentence fails the parsing to DSyntS conversion. Failures are automatically detected by comparing the original sentence with the one realized by RealPro taking the converted DSyntS as an input. We apply the filters, in a cascading manner, to the 18,466 sentences with semantic representations. As a result, we obtain 512 (2.8%) mappin</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Litman</author>
</authors>
<title>Now let’s talk about NOW: Identifying cue phrases intonationally.</title>
<date>1987</date>
<booktitle>In Proc. 25th ACL,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="11723" citStr="Litman, 1987" startWordPosition="1836" endWordPosition="1837">ge of single scalar-valued relation mappings. names, country names, dates (e.g., today, tomorrow, Aug. 26th) or prices (e.g., 12 dollars), or POS tag CD for numerals. These indicate relations not in the ontology. Contextual Filter: The sentence contains indexicals such as I, you, that or cohesive markers of rhetorical relations that connect it to some part of the preceding text, which means that the sentence cannot be interpreted out of context. These include discourse markers, such as list item markers with LS as the POS tag, that signal the organization structure of the text (Hirschberg and Litman, 1987), as well as discourse connectives that signal semantic and pragmatic relations of the sentence with other parts of the text (Knott, 1996), such as coordinating conjunctions at the beginning of the utterance like and and but etc., and conjunct adverbs such as however, also, then. Unknown Words Filter: The sentence contains words not in WordNet (Fellbaum, 1998) (which includes typographical errors), or POS tags contain NN (Noun), which may indicate an unknown named-entity, or the sentence has more than a fixed length of words,2 indicating that its meaning may not be estimated solely by named en</context>
</contexts>
<marker>Litman, 1987</marker>
<rawString>Julia Hirschberg and Diane. J. Litman. 1987. Now let’s talk about NOW: Identifying cue phrases intonationally. In Proc. 25th ACL, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2005</date>
<booktitle>In Proc. KDD,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="29032" citStr="Hu and Liu (2005)" startWordPosition="4532" endWordPosition="4535"> and FOODTYPE 1.8 food. [service=3] Waitstaff is friendly and knowl- 1.7 edgeable. [atmosphere=5, food=5, service=5] The atmo- 1.6 sphere, food and service. [overall=3] Overall, a great experience. 1.4 [service=1] The waiter is great. 1.4 Table 10: The 14 utterances with consistency scores below 3. ical proofs. However, our technique does not require parallel corpora or previously existing semantic transcripts or labeling, and user reviews are widely available in many different domains (See http://www.epinions.com/). There is also significant previous work on mining user reviews. For example, Hu and Liu (2005) use reviews to find adjectives to describe products, and Popescu and Etzioni (2005) automatically find features of a product together with the polarity of adjectives used to describe them. They both aim at summarizing reviews so that users can make decisions easily. Our method is also capable of finding polarities of modifying expressions including adjectives, but on a more fine-grained scale of 1 to 5. However, it might be possible to use their approach to create rating information for raw review texts as in (Pang and Lee, 2005), so that we can create mappings from reviews without ratings. 6</context>
<context position="32592" citStr="Hu and Liu, 2005" startWordPosition="5115" endWordPosition="5118"> is based on POS-tags, so that sentences that do not require the prior context for their interpretation are eliminated, such as sentences containing subordinating conjunctions like because, when, if, whose arguments are both given in the same sentence (Prasad et al., 2005). In addition, recall is affected by the domain ontology, and the automatically constructed domain ontology from the review webpages may not cover all of the domain. In some review domains, the attributes that get individual ratings are a limited subset of the domain ontology. Techniques for automatic feature identification (Hu and Liu, 2005; Popescu and Etzioni, 2005) could possibly help here, although these techniques currently have the limitation that they do not automatically identify different lexicalizations of the same feature. A different type of limitation is that dialogue systems need to generate utterances for information gathering whereas the mappings we obtained 271 can only be used for information presentation. Thus these would have to be constructed by hand, as in current practice, or perhaps other types of corpora or resources could be utilized. In addition, the utility of syntactic structures in the mappings shou</context>
</contexts>
<marker>Hu, Liu, 2005</marker>
<rawString>Minqing Hu and Bing Liu. 2005. Mining and summarizing customer reviews. In Proc. KDD, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh.</location>
<contexts>
<context position="11861" citStr="Knott, 1996" startWordPosition="1858" endWordPosition="1859">or POS tag CD for numerals. These indicate relations not in the ontology. Contextual Filter: The sentence contains indexicals such as I, you, that or cohesive markers of rhetorical relations that connect it to some part of the preceding text, which means that the sentence cannot be interpreted out of context. These include discourse markers, such as list item markers with LS as the POS tag, that signal the organization structure of the text (Hirschberg and Litman, 1987), as well as discourse connectives that signal semantic and pragmatic relations of the sentence with other parts of the text (Knott, 1996), such as coordinating conjunctions at the beginning of the utterance like and and but etc., and conjunct adverbs such as however, also, then. Unknown Words Filter: The sentence contains words not in WordNet (Fellbaum, 1998) (which includes typographical errors), or POS tags contain NN (Noun), which may indicate an unknown named-entity, or the sentence has more than a fixed length of words,2 indicating that its meaning may not be estimated solely by named entities. Parsing Filter: The sentence fails the parsing to DSyntS conversion. Failures are automatically detected by comparing the original</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Alistair Knott. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Ph.D. thesis, University of Edinburgh, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Owen Rambow</author>
</authors>
<title>A fast and portable realizer for text generation systems.</title>
<date>1997</date>
<booktitle>In Proc. 5th Applied NLP,</booktitle>
<pages>265--268</pages>
<contexts>
<context position="9312" citStr="Lavoie and Rambow, 1997" startWordPosition="1419" endWordPosition="1422">ns Filter 5,351 5,168 Contextual Filter 2,973 2,195 Unknown Words Filter 1,467 728 Parsing Filter 216 512 Table 2: Filtering statistics: the number of sentences filtered and retained by each filter. one FOODTYPE named-entity and one LOCATION named-entity. Values of categorical attributes are replaced by variables representing their type before the learned mappings are added to the dictionary, as shown in Fig. 1. 2.3 Parsing and DSyntS conversion We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can be realized by the fast portable realizer RealPro (Lavoie and Rambow, 1997). Since DSyntSs are a type of dependency structure, we first process the sentences with Minipar (Lin, 1998), and then convert Minipar’s representation into DSyntS. Since user reviews are different from the newspaper articles on which Minipar was trained, the output of Minipar can be inaccurate, leading to failure in conversion. We check whether conversion is successful in the filtering stage. 2.4 Filtering The goal of filtering is to identify U that realize the distinguished attributes and to guarantee high precision for the learned mappings. Recall is less important since systems need to conv</context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>Benoit Lavoie and Owen Rambow. 1997. A fast and portable realizer for text generation systems. In Proc. 5th Applied NLP, pages 265–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="27470" citStr="Lin and Pantel, 2001" startWordPosition="4303" endWordPosition="4306">trend for mappings to be rated higher for the food attribute (df=1, F=3.14, p=0.08) and the value attribute (df=1, F=3.55, p=0.06) for consistency, suggesting that perhaps it is easier to learn some mappings than others. 5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corpora with multiple descriptions of the same events (Barzilay and McKeown, 2001; Barzilay and Lee, 2003). Other work finds predicates of similar meanings by using the similarity of contexts around the predicates (Lin and Pantel, 2001). However, these studies find a set of sentences with the same meaning, but do not associate a specific meaning with the sentences. One exception is (Barzilay and Lee, 2002), which derives mappings between semantic representations and realizations using a parallel (but unaligned) corpus consisting of both complex semantic input and corresponding natural language verbalizations for mathemat270 shorthand for relations and utterance score [food=4] The food is delicious and beautifully 2.9 prepared. [overall=4] A wonderful experience. 2.9 [service=3] The service is bland and mediocre. 2.8 [atmosph</context>
<context position="31784" citStr="Lin and Pantel, 2001" startWordPosition="4982" endWordPosition="4985"> learned before actual use in spoken language generation. Still, our method could reduce the amount of time a system designer spends developing the spoken language generator, and increase the naturalness of spoken language generation. Another issue is that the recall appears to be quite low given that all of the sentences concern the same domain: only 2.4% of the sentences could be used to create the mappings. One way to increase recall might be to automatically augment the list of distinguished attribute lexicalizations, using WordNet or work on automatic identification of synonyms, such as (Lin and Pantel, 2001). However, the method here has high precision, and automatic techniques may introduce noise. A related issue is that the filters are in some cases too strict. For example the contextual filter is based on POS-tags, so that sentences that do not require the prior context for their interpretation are eliminated, such as sentences containing subordinating conjunctions like because, when, if, whose arguments are both given in the same sentence (Prasad et al., 2005). In addition, recall is affected by the domain ontology, and the automatically constructed domain ontology from the review webpages ma</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="9419" citStr="Lin, 1998" startWordPosition="1438" endWordPosition="1439">ing statistics: the number of sentences filtered and retained by each filter. one FOODTYPE named-entity and one LOCATION named-entity. Values of categorical attributes are replaced by variables representing their type before the learned mappings are added to the dictionary, as shown in Fig. 1. 2.3 Parsing and DSyntS conversion We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can be realized by the fast portable realizer RealPro (Lavoie and Rambow, 1997). Since DSyntSs are a type of dependency structure, we first process the sentences with Minipar (Lin, 1998), and then convert Minipar’s representation into DSyntS. Since user reviews are different from the newspaper articles on which Minipar was trained, the output of Minipar can be inaccurate, leading to failure in conversion. We check whether conversion is successful in the filtering stage. 2.4 Filtering The goal of filtering is to identify U that realize the distinguished attributes and to guarantee high precision for the learned mappings. Recall is less important since systems need to convey requested information as accurately as possible. Our procedure for deriving semantic representations is </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>Mary Ellen Foster</author>
<author>Oliver Lemon</author>
<author>Michael White</author>
</authors>
<title>Generating tailored, comparative descriptions in spoken dialogue.</title>
<date>2004</date>
<booktitle>In Proc. 7th FLAIR.</booktitle>
<contexts>
<context position="4949" citStr="Moore et al., 2004" startWordPosition="767" endWordPosition="770">s future work. 2 Learning a Generation Dictionary Our automatically created generation dictionary consists of triples (U, R, S) representing a mapping between the original utterance U in the user review, its semantic representation R(U), and its syntactic structure S(U). Although templates are widely used in many practical systems (Seneff and Polifroni, 2000; Theune, 2003), we derive syntactic structures to represent the potential realizations, in order to allow aggregation, and other syntactic transformations of utterances, as well as context specific prosody assignment (Walker et al., 2003; Moore et al., 2004). The method is outlined briefly in Fig. 1 and described below. It comprises the following steps: 1. Collect user reviews on the web to create a population of utterances U. 2. To derive semantic representations R(U): • Identify distinguished attributes and construct a domain ontology; • Specify lexicalizations of attributes; • Scrape webpages’ structured data for named-entities; • Tag named-entities. 3. Derive syntactic representations S(U). 4. Filter inappropriate mappings. 5. Add mappings (U, R, S) to dictionary. 2.1 Creating the corpus We created a corpus of restaurant reviews by scraping 3</context>
</contexts>
<marker>Moore, Foster, Lemon, White, 2004</marker>
<rawString>Johanna D. Moore, Mary Ellen Foster, Oliver Lemon, and Michael White. 2004. Generating tailored, comparative descriptions in spoken dialogue. In Proc. 7th FLAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proc. 43st ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="29568" citStr="Pang and Lee, 2005" startWordPosition="4623" endWordPosition="4626">so significant previous work on mining user reviews. For example, Hu and Liu (2005) use reviews to find adjectives to describe products, and Popescu and Etzioni (2005) automatically find features of a product together with the polarity of adjectives used to describe them. They both aim at summarizing reviews so that users can make decisions easily. Our method is also capable of finding polarities of modifying expressions including adjectives, but on a more fine-grained scale of 1 to 5. However, it might be possible to use their approach to create rating information for raw review texts as in (Pang and Lee, 2005), so that we can create mappings from reviews without ratings. 6 Summary and Future Work We proposed automatically obtaining mappings between semantic representations and realizations from reviews with individual ratings. The results show that: (1) the learned mappings provide good coverage of the domain ontology and exhibit good linguistic variation; (2) the consistency between the semantic representations and realizations is high; and (3) the naturalness of the realizations are significantly higher than the baseline. There are also limitations in our method. Even though consistency is rated </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proc. 43st ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="29116" citStr="Popescu and Etzioni (2005)" startWordPosition="4545" endWordPosition="4548">geable. [atmosphere=5, food=5, service=5] The atmo- 1.6 sphere, food and service. [overall=3] Overall, a great experience. 1.4 [service=1] The waiter is great. 1.4 Table 10: The 14 utterances with consistency scores below 3. ical proofs. However, our technique does not require parallel corpora or previously existing semantic transcripts or labeling, and user reviews are widely available in many different domains (See http://www.epinions.com/). There is also significant previous work on mining user reviews. For example, Hu and Liu (2005) use reviews to find adjectives to describe products, and Popescu and Etzioni (2005) automatically find features of a product together with the polarity of adjectives used to describe them. They both aim at summarizing reviews so that users can make decisions easily. Our method is also capable of finding polarities of modifying expressions including adjectives, but on a more fine-grained scale of 1 to 5. However, it might be possible to use their approach to create rating information for raw review texts as in (Pang and Lee, 2005), so that we can create mappings from reviews without ratings. 6 Summary and Future Work We proposed automatically obtaining mappings between semant</context>
<context position="32620" citStr="Popescu and Etzioni, 2005" startWordPosition="5119" endWordPosition="5123">ags, so that sentences that do not require the prior context for their interpretation are eliminated, such as sentences containing subordinating conjunctions like because, when, if, whose arguments are both given in the same sentence (Prasad et al., 2005). In addition, recall is affected by the domain ontology, and the automatically constructed domain ontology from the review webpages may not cover all of the domain. In some review domains, the attributes that get individual ratings are a limited subset of the domain ontology. Techniques for automatic feature identification (Hu and Liu, 2005; Popescu and Etzioni, 2005) could possibly help here, although these techniques currently have the limitation that they do not automatically identify different lexicalizations of the same feature. A different type of limitation is that dialogue systems need to generate utterances for information gathering whereas the mappings we obtained 271 can only be used for information presentation. Thus these would have to be constructed by hand, as in current practice, or perhaps other types of corpora or resources could be utilized. In addition, the utility of syntactic structures in the mappings should be further examined, espe</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proc. HLT/EMNLP, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank as a resource for natural language generation.</title>
<date>2005</date>
<booktitle>In Proc. Corpus Linguistics Workshop on Using Corpora for NLG.</booktitle>
<contexts>
<context position="32249" citStr="Prasad et al., 2005" startWordPosition="5059" endWordPosition="5062"> augment the list of distinguished attribute lexicalizations, using WordNet or work on automatic identification of synonyms, such as (Lin and Pantel, 2001). However, the method here has high precision, and automatic techniques may introduce noise. A related issue is that the filters are in some cases too strict. For example the contextual filter is based on POS-tags, so that sentences that do not require the prior context for their interpretation are eliminated, such as sentences containing subordinating conjunctions like because, when, if, whose arguments are both given in the same sentence (Prasad et al., 2005). In addition, recall is affected by the domain ontology, and the automatically constructed domain ontology from the review webpages may not cover all of the domain. In some review domains, the attributes that get individual ratings are a limited subset of the domain ontology. Techniques for automatic feature identification (Hu and Liu, 2005; Popescu and Etzioni, 2005) could possibly help here, although these techniques currently have the limitation that they do not automatically identify different lexicalizations of the same feature. A different type of limitation is that dialogue systems nee</context>
</contexts>
<marker>Prasad, Joshi, Dinesh, Lee, Miltsakaki, Webber, 2005</marker>
<rawString>Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, and Bonnie Webber. 2005. The Penn Discourse TreeBank as a resource for natural language generation. In Proc. Corpus Linguistics Workshop on Using Corpora for NLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
<author>Joseph Polifroni</author>
</authors>
<title>Formal and natural language generation in the mercury conversational system.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP,</booktitle>
<volume>2</volume>
<pages>767--770</pages>
<contexts>
<context position="4690" citStr="Seneff and Polifroni, 2000" startWordPosition="728" endWordPosition="731">the ACL, pages 265–272, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 2 provides a step-by-step description of the method. Sections 3 and 4 present the evaluation results. Section 5 covers related work. Section 6 summarizes and discusses future work. 2 Learning a Generation Dictionary Our automatically created generation dictionary consists of triples (U, R, S) representing a mapping between the original utterance U in the user review, its semantic representation R(U), and its syntactic structure S(U). Although templates are widely used in many practical systems (Seneff and Polifroni, 2000; Theune, 2003), we derive syntactic structures to represent the potential realizations, in order to allow aggregation, and other syntactic transformations of utterances, as well as context specific prosody assignment (Walker et al., 2003; Moore et al., 2004). The method is outlined briefly in Fig. 1 and described below. It comprises the following steps: 1. Collect user reviews on the web to create a population of utterances U. 2. To derive semantic representations R(U): • Identify distinguished attributes and construct a domain ontology; • Specify lexicalizations of attributes; • Scrape webpa</context>
</contexts>
<marker>Seneff, Polifroni, 2000</marker>
<rawString>Stephanie Seneff and Joseph Polifroni. 2000. Formal and natural language generation in the mercury conversational system. In Proc. ICSLP, volume 2, pages 767–770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
</authors>
<title>From monologue to dialogue: natural language generation in OVIS.</title>
<date>2003</date>
<booktitle>In AAAI 2003 Spring Symposium on Natural Language Generation in Written and Spoken Dialogue,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="4705" citStr="Theune, 2003" startWordPosition="732" endWordPosition="733">ey, July 2006. c�2006 Association for Computational Linguistics Section 2 provides a step-by-step description of the method. Sections 3 and 4 present the evaluation results. Section 5 covers related work. Section 6 summarizes and discusses future work. 2 Learning a Generation Dictionary Our automatically created generation dictionary consists of triples (U, R, S) representing a mapping between the original utterance U in the user review, its semantic representation R(U), and its syntactic structure S(U). Although templates are widely used in many practical systems (Seneff and Polifroni, 2000; Theune, 2003), we derive syntactic structures to represent the potential realizations, in order to allow aggregation, and other syntactic transformations of utterances, as well as context specific prosody assignment (Walker et al., 2003; Moore et al., 2004). The method is outlined briefly in Fig. 1 and described below. It comprises the following steps: 1. Collect user reviews on the web to create a population of utterances U. 2. To derive semantic representations R(U): • Identify distinguished attributes and construct a domain ontology; • Specify lexicalizations of attributes; • Scrape webpages’ structured</context>
</contexts>
<marker>Theune, 2003</marker>
<rawString>Mari¨et Theune. 2003. From monologue to dialogue: natural language generation in OVIS. In AAAI 2003 Spring Symposium on Natural Language Generation in Written and Spoken Dialogue, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. 40th ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="17164" citStr="Turney, 2002" startWordPosition="2706" endWordPosition="2707"> in the Appendix. Moreover, the meanings for some of the learned APs are very specific to the particular attribute, e.g. cold and burnt associated with foodquality of 1, attentive and prompt for servicequality of 5, silly and inattentive for servicequality of 1. and mellow for atmosphere of 5. In addition, our method places the adjectival phrases (APs) in the common patterns on a more fine-grained scale of 1 to 5, similar to the strength classifications in (Wilson et al., 2004), in contrast to other automatic methods that classify expressions into a binary positive or negative polarity (e.g. (Turney, 2002)). 3.3 Generativity Our motivation for deriving syntactic representations for the learned expressions was the possibility of using an off-the-shelf sentence planner to derive new combinations of relations, and apply aggregation and other syntactic transformations. We examined how many of the learned DSyntSs can be combined with each other, by taking every pair of DSyntSs in the mappings and applying the built-in merge operation in the SPaRKy generator (Walker et al., 2003). We found that only 306 combinations out of a potential 81,318 268 # syntactic pattern example utterance count ratio accum</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proc. 40th ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Rashmi Prasad</author>
<author>Amanda Stent</author>
</authors>
<title>A trainable generator for recommendations in multimodal dialog.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>1697--1700</pages>
<contexts>
<context position="4928" citStr="Walker et al., 2003" startWordPosition="763" endWordPosition="766">mmarizes and discusses future work. 2 Learning a Generation Dictionary Our automatically created generation dictionary consists of triples (U, R, S) representing a mapping between the original utterance U in the user review, its semantic representation R(U), and its syntactic structure S(U). Although templates are widely used in many practical systems (Seneff and Polifroni, 2000; Theune, 2003), we derive syntactic structures to represent the potential realizations, in order to allow aggregation, and other syntactic transformations of utterances, as well as context specific prosody assignment (Walker et al., 2003; Moore et al., 2004). The method is outlined briefly in Fig. 1 and described below. It comprises the following steps: 1. Collect user reviews on the web to create a population of utterances U. 2. To derive semantic representations R(U): • Identify distinguished attributes and construct a domain ontology; • Specify lexicalizations of attributes; • Scrape webpages’ structured data for named-entities; • Tag named-entities. 3. Derive syntactic representations S(U). 4. Filter inappropriate mappings. 5. Add mappings (U, R, S) to dictionary. 2.1 Creating the corpus We created a corpus of restaurant </context>
<context position="15154" citStr="Walker et al., 2003" startWordPosition="2367" endWordPosition="2370">treat relations with different scalar values differently). We also find that most of the mappings have two or three relations, perhaps suggesting that system utterances should not express too many relations in a single sentence. 3.2 Linguistic Variation We also wish to assess whether the linguistic variation of the learned mappings was greater than what we could easily have generated with a hand-crafted dictionary, or a hand-crafted dictionary augmented with aggregation operators, as in 3There are two other single-relation but not scalar-valued mappings that concern LOCATION in our mappings. (Walker et al., 2003). Thus, we first categorized the mappings by the patterns of the DSyntSs. Table 5 shows the most common syntactic patterns (more than 10 occurrences), indicating that 30% of the learned patterns consist of the simple form “X is ADJ” where ADJ is an adjective, or “X is RB ADJ,” where RB is a degree modifier. Furthermore, up to 55% of the learned mappings could be generated from these basic patterns by the application of a combination operator that coordinates multiple adjectives, or coordinates predications over distinct attributes. However, there are 137 syntactic patterns in all, 97 with uniq</context>
<context position="17641" citStr="Walker et al., 2003" startWordPosition="2779" endWordPosition="2782">t al., 2004), in contrast to other automatic methods that classify expressions into a binary positive or negative polarity (e.g. (Turney, 2002)). 3.3 Generativity Our motivation for deriving syntactic representations for the learned expressions was the possibility of using an off-the-shelf sentence planner to derive new combinations of relations, and apply aggregation and other syntactic transformations. We examined how many of the learned DSyntSs can be combined with each other, by taking every pair of DSyntSs in the mappings and applying the built-in merge operation in the SPaRKy generator (Walker et al., 2003). We found that only 306 combinations out of a potential 81,318 268 # syntactic pattern example utterance count ratio accum. 1 NN VB JJ The atmosphere is wonderful. 92 20.4% 20.4% 2 NN VB RB JJ The atmosphere was very nice. 52 11.5% 31.9% 3 JJ NN Bad service. 36 8.0% 39.9% 4 NN VB JJ CC JJ The food was flavorful but cold. 25 5.5% 45.5% 5 RB JJ NN Very trendy ambience. 22 4.9% 50.3% 6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great. 13 2.9% 53.2% 7 NN CC NN VB JJ The food and service were fantastic. 10 2.2% 55.4% Table 5: Common syntactic patterns of DSyntSs, flattened to</context>
<context position="21585" citStr="Walker et al., 2003" startWordPosition="3398" endWordPosition="3401">seasoned and hot, awesome, best, delectable and plentiful, delicious, delicious but simple, excellent, exquisite, fabulous, fancy but tasty, fantastic, fresh, good, great, hot, incredible, just fantastic, large and satisfying, outstanding, plentiful and outstanding, plentiful and tasty, quick and hot, simply great, so delicious, so very tasty, superb, terrific, tremendous, very good, wonderful Table 7: Adjectival phrases (APs) in single scalarvalued relation mappings for foodquality. tion, and the naturalness of the realization. For comparison, we used a baseline of handcrafted mappings from (Walker et al., 2003) except that we changed the word decor to atmosphere and added five mappings for overall. For scalar relations, this consists of the realization “RESTAURANT has ADJ LEX” where ADJ is mediocre, decent, good, very good, or excellent for rating values 1-5, and LEX is food quality, service, atmosphere, value, or overall depending on the relation. RESTAURANT is filled with the name of a restaurant at runtime. For example, ‘RESTAURANT has foodquality=1’ is realized as “RESTAURANT has mediocre food quality.” The location and food type relations are mapped to “RESTAURANT is located in LOCATION” and “R</context>
</contexts>
<marker>Walker, Prasad, Stent, 2003</marker>
<rawString>Marilyn Walker, Rashmi Prasad, and Amanda Stent. 2003. A trainable generator for recommendations in multimodal dialog. In Proc. Eurospeech, pages 1697–1700.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Ted Caldwell</author>
</authors>
<title>EXEMPLARS: A practical, extensible framework for dynamic text generation.</title>
<date>1998</date>
<booktitle>In Proc. INLG,</booktitle>
<pages>266--275</pages>
<marker>White, Caldwell, 1998</marker>
<rawString>Michael White and Ted Caldwell. 1998. EXEMPLARS: A practical, extensible framework for dynamic text generation. In Proc. INLG, pages 266–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you? finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proc. AAAI,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="17033" citStr="Wilson et al., 2004" startWordPosition="2684" endWordPosition="2687">valued relation mappings for food and service categorized by the associated ratings. Tables for atmosphere, value and overall can be found in the Appendix. Moreover, the meanings for some of the learned APs are very specific to the particular attribute, e.g. cold and burnt associated with foodquality of 1, attentive and prompt for servicequality of 5, silly and inattentive for servicequality of 1. and mellow for atmosphere of 5. In addition, our method places the adjectival phrases (APs) in the common patterns on a more fine-grained scale of 1 to 5, similar to the strength classifications in (Wilson et al., 2004), in contrast to other automatic methods that classify expressions into a binary positive or negative polarity (e.g. (Turney, 2002)). 3.3 Generativity Our motivation for deriving syntactic representations for the learned expressions was the possibility of using an off-the-shelf sentence planner to derive new combinations of relations, and apply aggregation and other syntactic transformations. We examined how many of the learned DSyntSs can be combined with each other, by taking every pair of DSyntSs in the mappings and applying the built-in merge operation in the SPaRKy generator (Walker et al</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? finding strong and weak opinion clauses. In Proc. AAAI, pages 761–769.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>