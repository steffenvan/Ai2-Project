<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003840">
<title confidence="0.99722">
An Unsupervised Model for Joint Phrase Alignment and Extraction
</title>
<author confidence="0.999642">
Graham Neubig&apos;,&apos; Taro Watanabe&apos;, Eiichiro Sumita&apos;, Shinsuke Mori&apos;, Tatsuya Kawahara&apos;
</author>
<affiliation confidence="0.962117333333333">
&apos;Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
&apos;National Institute of Information and Communication Technology
</affiliation>
<address confidence="0.768899">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<sectionHeader confidence="0.969736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.858481789473684">
We present an unsupervised model for joint
phrase alignment and extraction using non-
parametric Bayesian methods and inversion
transduction grammars (ITGs). The key con-
tribution is that phrases of many granulari-
ties are included directly in the model through
the use of a novel formulation that memorizes
phrases generated not only by terminal, but
also non-terminal symbols. This allows for
a completely probabilistic model that is able
to create a phrase table that achieves com-
petitive accuracy on phrase-based machine
translation tasks directly from unaligned sen-
tence pairs. Experiments on several language
pairs demonstrate that the proposed model
matches the accuracy of traditional two-step
word alignment/phrase extraction approach
while reducing the phrase table to a fraction
of the original size.
</bodyText>
<sectionHeader confidence="0.998922" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997593369565217">
The training of translation models for phrase-
based statistical machine translation (SMT) systems
(Koehn et al., 2003) takes unaligned bilingual train-
ing data as input, and outputs a scored table of
phrase pairs. This phrase table is traditionally gen-
erated by going through a pipeline of two steps, first
generating word (or minimal phrase) alignments,
then extracting a phrase table that is consistent with
these alignments.
However, as DeNero and Klein (2010) note, this
two step approach results in word alignments that
are not optimal for the final task of generating
phrase tables that are used in translation. As a so-
lution to this, they proposed a supervised discrimi-
native model that performs joint word alignment and
phrase extraction, and found that joint estimation of
word alignments and extraction sets improves both
word alignment accuracy and translation results.
In this paper, we propose the first unsuper-
vised approach to joint alignment and extraction of
phrases at multiple granularities. This is achieved
by constructing a generative model that includes
phrases at many levels of granularity, from minimal
phrases all the way up to full sentences. The model
is similar to previously proposed phrase alignment
models based on inversion transduction grammars
(ITGs) (Cherry and Lin, 2007; Zhang et al., 2008;
Blunsom et al., 2009), with one important change:
ITG symbols and phrase pairs are generated in
the opposite order. In traditional ITG models, the
branches of a biparse tree are generated from a non-
terminal distribution, and each leaf is generated by
a word or phrase pair distribution. As a result, only
minimal phrases are directly included in the model,
while larger phrases must be generated by heuris-
tic extraction methods. In the proposed model, at
each branch in the tree, we first attempt to gener-
ate a phrase pair from the phrase pair distribution,
falling back to ITG-based divide and conquer strat-
egy to generate phrase pairs that do not exist (or are
given low probability) in the phrase distribution.
We combine this model with the Bayesian non-
parametric Pitman-Yor process (Pitman and Yor,
1997; Teh, 2006), realizing ITG-based divide and
conquer through a novel formulation where the
Pitman-Yor process uses two copies of itself as a
</bodyText>
<page confidence="0.959972">
632
</page>
<note confidence="0.979678">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632–641,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998852">
base measure. As a result of this modeling strategy,
phrases of multiple granularities are generated, and
thus memorized, by the Pitman-Yor process. This
makes it possible to directly use probabilities of the
phrase model as a replacement for the phrase table
generated by heuristic extraction techniques.
Using this model, we perform machine transla-
tion experiments over four language pairs. We ob-
serve that the proposed joint phrase alignment and
extraction approach is able to meet or exceed results
attained by a combination of GIZA++ and heuristic
phrase extraction with significantly smaller phrase
table size. We also find that it achieves superior
BLEU scores over previously proposed ITG-based
phrase alignment approaches.
</bodyText>
<sectionHeader confidence="0.9971915" genericHeader="introduction">
2 A Probabilistic Model for Phrase Table
Extraction
</sectionHeader>
<bodyText confidence="0.999897333333333">
The problem of SMT can be defined as finding the
most probable target sentence e for the source sen-
tence f given a parallel training corpus hE, Fi
</bodyText>
<equation confidence="0.9703875">
eˆ = argmax P(e|f, hE, Fi).
e
</equation>
<bodyText confidence="0.999749">
We assume that there is a hidden set of parameters
θ learned from the training data, and that e is condi-
tionally independent from the training corpus given
θ. We take a Bayesian approach, integrating over all
possible values of the hidden parameters:
</bodyText>
<equation confidence="0.9443725">
P(e|f, hE,Fi) = f P(e|f,θ)P(θ|hE,Fi). (1)
B
</equation>
<bodyText confidence="0.9998705">
If θ takes the form of a scored phrase table, we
can use traditional methods for phrase-based SMT to
find P(e|f, θ) and concentrate on creating a model
for P(θ|hE, Fi). We decompose this posterior prob-
ability using Bayes law into the corpus likelihood
and parameter prior probabilities
</bodyText>
<equation confidence="0.916946">
P(θ|hE,Fi) ∝ P(hE,Fi|θ)P(θ).
</equation>
<bodyText confidence="0.999717">
In Section 3 we describe an existing method, and
in Section 4 we describe our proposed method for
modeling these two probabilities.
</bodyText>
<sectionHeader confidence="0.999531" genericHeader="method">
3 Flat ITG Model
</sectionHeader>
<bodyText confidence="0.999011277777778">
There has been a significant amount of work in
many-to-many alignment techniques (Marcu and
Wong (2002), DeNero et al. (2008), inter alia), and
in particular a number of recent works (Cherry and
Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009)
have used the formalism of inversion transduction
grammars (ITGs) (Wu, 1997) to learn phrase align-
ments. By slightly limit reordering of words, ITGs
make it possible to exactly calculate probabilities
of phrasal alignments in polynomial time, which is
a computationally hard problem when arbitrary re-
ordering is allowed (DeNero and Klein, 2008).
The traditional flat ITG generative probabil-
ity for a particular phrase (or sentence) pair
Pflat(he, fi; θx, θt) is parameterized by a phrase ta-
ble θt and a symbol distribution θx. We use the fol-
lowing generative story as a representative of the flat
ITG model.
</bodyText>
<listItem confidence="0.998634153846154">
1. Generate symbol x from the multinomial distri-
bution Px(x; θx). x can take the values TERM,
REG, or INV.
2. According to the x take the following actions.
(a) If x = TERM, generate a phrase pair from
the phrase table Pt(he, fi; θt).
(b) If x = REG, a regular ITG rule, gener-
ate phrase pairs he1, f1i and he2, f2i from
Pflat, and concatenate them into a single
phrase pair he1e2, f1f2i.
(c) If x = INV, an inverted ITG rule, follows
the same process as (b), but concatenate
f1 and f2 in reverse order he1e2, f2f1i.
</listItem>
<bodyText confidence="0.984798">
By taking the product of Pflat over every sentence
in the corpus, we are able to calculate the likelihood
</bodyText>
<equation confidence="0.9989495">
P(hE,Fi|θ) = ∏ Pflat(he, fi; θ).
(e,f)E(£,J7)
</equation>
<bodyText confidence="0.990843">
We will refer to this model as FLAT.
</bodyText>
<subsectionHeader confidence="0.998264">
3.1 Bayesian Modeling
</subsectionHeader>
<bodyText confidence="0.998096">
While the previous formulation can be used as-is in
maximum likelihood training, this leads to a degen-
erate solution where every sentence is memorized as
a single phrase pair. Zhang et al. (2008) and others
propose dealing with this problem by putting a prior
probability P(θx, θt) on the parameters.
</bodyText>
<page confidence="0.998774">
633
</page>
<bodyText confidence="0.9949754">
We assign Bx a Dirichlet priors, and assign the
phrase table parameters Bt a prior using the Pitman-
Yor process (Pitman and Yor, 1997; Teh, 2006),
which is a generalization of the Dirichlet process
prior used in previous research. It is expressed as
</bodyText>
<equation confidence="0.614874">
Bt —PY (d, s, Pbase) (2)
</equation>
<bodyText confidence="0.99997025">
where d is the discount parameter, s is the strength
parameter, and Pbase is the base measure. The dis-
count d is subtracted from observed counts, and
when it is given a large value (close to one), less
frequent phrase pairs will be given lower relative
probability than more common phrase pairs. The
strength s controls the overall sparseness of the dis-
tribution, and when it is given a small value the dis-
tribution will be sparse. Pbase is the prior probability
of generating a particular phrase pair, which we de-
scribe in more detail in the following section.
Non-parametric priors are well suited for mod-
eling the phrase distribution because every time a
phrase is generated by the model, it is “memorized”
and given higher probability. Because of this, com-
mon phrase pairs are more likely to be re-used (the
rich-get-richer effect), which results in the induc-
tion of phrase tables with fewer, but more helpful
phrases. It is important to note that only phrases
generated by Pt are actually memorized and given
higher probability by the model. In FLAT, only min-
imal phrases generated after Px outputs the terminal
symbol TERM are generated from Pt, and thus only
minimal phrases are memorized by the model.
While the Dirichlet process is simply the Pitman-
Yor process with d = 0, it has been shown that the
discount parameter allows for more effective mod-
eling of the long-tailed distributions that are often
found in natural language (Teh, 2006). We con-
firmed in preliminary experiments (using the data
described in Section 7) that the Pitman-Yor process
with automatically adjusted parameters results in su-
perior alignment results, outperforming the sparse
Dirichlet process priors used in previous research2.
The average gain across all data sets was approxi-
mately 0.8 BLEU points.
</bodyText>
<footnote confidence="0.927628">
1The value of a had little effect on the results, so we arbi-
trarily set a = 1.
2We put weak priors on s (Gamma(a = 2„3 = 1)) and
d (Beta(a = 2„3 = 2)) for the Pitman-Yor process, and set
a = 1−10 for the Dirichlet process.
</footnote>
<subsectionHeader confidence="0.999872">
3.2 Base Measure
</subsectionHeader>
<bodyText confidence="0.998275545454545">
Pbase in Equation (2) indicates the prior probability
of phrase pairs according to the model. By choosing
this probability appropriately, we can incorporate
prior knowledge of what phrases tend to be aligned
to each other. We calculate Pbase by first choosing
whether to generate an unaligned phrase pair (where
|e |= 0 or |f |= 0) according to a fixed probabil-
ity pu3, then generating from Pba for aligned phrase
pairs, or Pbu for unaligned phrase pairs.
For Pba, we adopt a base measure similar to that
used by DeNero et al. (2008):
</bodyText>
<equation confidence="0.956214333333333">
Pba((e, f)) =M0((e, f))Ppois(|e|; A)Ppois(|f|; A)
1
2.
</equation>
<bodyText confidence="0.999854588235294">
Ppois is the Poisson distribution with the average
length parameter A. As long phrases lead to spar-
sity, we set A to a relatively small value to allow
us to bias against overly long phrases4. Pm1 is the
word-based Model 1 (Brown et al., 1993) probabil-
ity of one phrase given the other, which incorporates
word-based alignment information as prior knowl-
edge in the phrase translation probability. We take
the geometric mean5of the Model 1 probabilities in
both directions to encourage alignments that are sup-
ported by both models (Liang et al., 2006). It should
be noted that while Model 1 probabilities are used,
they are only soft constraints, compared with the
hard constraint of choosing a single word alignment
used in most previous phrase extraction approaches.
For Pbu, if g is the non-null phrase in e and f, we
calculate the probability as follows:
</bodyText>
<equation confidence="0.827199">
Pbu((e, f)) = Puni(g)Ppois(|g|; A)/2.
</equation>
<bodyText confidence="0.998735">
Note that Pbu is divided by 2 as the probability is
considering null alignments in both directions.
</bodyText>
<sectionHeader confidence="0.999056" genericHeader="method">
4 Hierarchical ITG Model
</sectionHeader>
<bodyText confidence="0.9801105">
While in FLAT only minimal phrases were memo-
rized by the model, as DeNero et al. (2008) note
</bodyText>
<footnote confidence="0.9495415">
3We choose 10−2, 10−3, or 10−10 based on which value
gave the best accuracy on the development set.
4We tune A to 1, 0.1, or 0.01 based on which value gives the
best performance on the development set.
5The probabilities of the geometric mean do not add to one,
but we found empirically that even when left unnormalized, this
provided much better results than the using the arithmetic mean,
which is more theoretically correct.
</footnote>
<equation confidence="0.911386">
M0((e,f)) =(Pm1(f|e)Puni(e)Pm1(e|f)Puni(f))
</equation>
<page confidence="0.985296">
634
</page>
<bodyText confidence="0.999612678571429">
and we confirm in the experiments in Section 7, us-
ing only minimal phrases leads to inferior transla-
tion results for phrase-based SMT. Because of this,
previous research has combined FLAT with heuris-
tic phrase extraction, which exhaustively combines
all adjacent phrases permitted by the word align-
ments (Och et al., 1999). We propose an alterna-
tive, fully statistical approach that directly models
phrases at multiple granularities, which we will refer
to as HIER. By doing so, we are able to do away with
heuristic phrase extraction, creating a fully proba-
bilistic model for phrase probabilities that still yields
competitive results.
Similarly to FLAT, HIER assigns a probability
Phi,,((e, f); Bx, Bt) to phrase pairs, and is parame-
terized by a phrase table Bt and a symbol distribu-
tion Bx. The main difference from the generative
story of the traditional ITG model is that symbols
and phrase pairs are generated in the opposite order.
While FLAT first generates branches of the derivation
tree using Px, then generates leaves using the phrase
distribution Pt, HIER first attempts to generate the
full sentence as a single phrase from Pt, then falls
back to ITG-style derivations to cope with sparsity.
We allow for this within the Bayesian ITG context
by defining a new base measure Pda, (“divide-and-
conquer”) to replace Pba,, in Equation (2), resulting
in the following distribution for Bt.
</bodyText>
<equation confidence="0.977322">
Bt — PY (d, s, Pda,) (3)
</equation>
<bodyText confidence="0.999799">
Pda, essentially breaks the generation of a sin-
gle longer phrase into two generations of shorter
phrases, allowing even phrase pairs for which
c((e, f)) = 0 to be given some probability. The
generative process of Pda,, similar to that of Pflat
from the previous section, is as follows:
</bodyText>
<listItem confidence="0.99925475">
1. Generate symbol x from Px(x; Bx). x can take
the values BASE, REG, or INV.
2. According to x take the following actions.
(a) If x = BASE, generate a new phrase pair
directly from Pba,, of Section 3.2.
(b) If x = REG, generate (e1, f1) and (e2, f2)
from Phi,,, and concatenate them into a
single phrase pair (e1e2, f1f2).
</listItem>
<figureCaption confidence="0.9836428">
Figure 1: A word alignment (a), and its derivations ac-
cording to FLAT (b), and HIER (c). Solid and dotted lines
indicate minimal and non-minimal pairs respectively, and
phrases are written under their corresponding instance of
Pt. The pair hate/coˆute is generated from Pb,,,,,.
</figureCaption>
<bodyText confidence="0.987480833333333">
(c) If x = INV, follow the same process as
(b), but concatenate f1 and f2 in reverse
order (e1e2, f2f1).
A comparison of derivation trees for FLAT and
HIER is shown in Figure 1. As previously de-
scribed, FLAT first generates from the symbol dis-
tribution Px, then from the phrase distribution Pt,
while HIER generates directly from Pt, which falls
back to divide-and-conquer based on Px when nec-
essary. It can be seen that while Pt in FLAT only gen-
erates minimal phrases, Pt in HIER generates (and
thus memorizes) phrases at all levels of granularity.
</bodyText>
<subsectionHeader confidence="0.998681">
4.1 Length-based Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.99998175">
There are still two problems with HIER, one theo-
retical, and one practical. Theoretically, HIER con-
tains itself as its base measure, and stochastic pro-
cess models that include themselves as base mea-
sures are deficient, as noted in Cohen et al. (2010).
Practically, while the Pitman-Yor process in HIER
shares the parameters s and d over all phrase pairs in
the model, long phrase pairs are much more sparse
</bodyText>
<page confidence="0.997073">
635
</page>
<figureCaption confidence="0.999627">
Figure 2: Learned discount values by phrase pair length.
</figureCaption>
<bodyText confidence="0.998638857142857">
than short phrase pairs, and thus it is desirable to
appropriately adjust the parameters of Equation (2)
according to phrase pair length.
In order to solve these problems, we reformulate
the model so that each phrase length l = |f|+|e |has
its own phrase parameters 0t,l and symbol parame-
ters 0x,l, which are given separate priors:
</bodyText>
<equation confidence="0.9994785">
0t,l — PY (s, d, Pdac,l)
0x,l — Dirichlet(α)
</equation>
<bodyText confidence="0.9994104">
We will call this model HLEN.
The generative story is largely similar to HIER
with a few minor changes. When we generate a sen-
tence, we first choose its length l according to a uni-
form distribution over all possible sentence lengths
</bodyText>
<equation confidence="0.924455">
l — Uniform(1, L),
</equation>
<bodyText confidence="0.999281888888889">
where L is the size |e |+ |f |of the longest sentence
in the corpus. We then generate a phrase pair from
the probability Pt,l((e, f)) for length l. The base
measure for HLEN is identical to that of HIER, with
one minor change: when we fall back to two shorter
phrases, we choose the length of the left phrase from
ll — Uniform(1, l — 1), set the length of the right
phrase to lr = l—ll, and generate the smaller phrases
from Pt,l, and Pt,l,, respectively.
It can be seen that phrases at each length are gen-
erated from different distributions, and thus the pa-
rameters for the Pitman-Yor process will be differ-
ent for each distribution. Further, as ll and lr must
be smaller than l, Pt,l no longer contains itself as a
base measure, and is thus not deficient.
An example of the actual discount values learned
in one of the experiments described in Section 7
is shown in Figure 2. It can be seen that, as ex-
pected, the discounts for short phrases are lower than
those of long phrases. In particular, phrase pairs of
length up to six (for example, |e |= 3, |f |= 3) are
given discounts of nearly zero while larger phrases
are more heavily discounted. We conjecture that this
is related to the observation by Koehn et al. (2003)
that using phrases where max(|e|, |f|) G 3 cause
significant improvements in BLEU score, while us-
ing larger phrases results in diminishing returns.
</bodyText>
<subsectionHeader confidence="0.966227">
4.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999939133333334">
Previous research has used a variety of sampling
methods to learn Bayesian phrase based alignment
models (DeNero et al., 2008; Blunsom et al., 2009;
Blunsom and Cohn, 2010). All of these techniques
are applicable to the proposed model, but we choose
to apply the sentence-based blocked sampling of
Blunsom and Cohn (2010), which has desirable con-
vergence properties compared to sampling single
alignments. As exhaustive sampling is too slow for
practical purpose, we adopt the beam search algo-
rithm of Saers et al. (2009), and use a probability
beam, trimming spans where the probability is at
least 1010 times smaller than that of the best hypoth-
esis in the bucket.
One important implementation detail that is dif-
ferent from previous models is the management of
phrase counts. As a phrase pair ta may have been
generated from two smaller component phrases tb
and tc, when a sample containing ta is removed from
the distribution, it may also be necessary to decre-
ment the counts of tb and tc as well. The Chinese
Restaurant Process representation of Pt (Teh, 2006)
lends itself to a natural and easily implementable so-
lution to this problem. For each table representing a
phrase pair ta, we maintain not only the number of
customers sitting at the table, but also the identities
of phrases tb and tc that were originally used when
generating the table. When the count of the table
ta is reduced to zero and the table is removed, the
counts of tb and tc are also decremented.
</bodyText>
<sectionHeader confidence="0.99298" genericHeader="method">
5 Phrase Extraction
</sectionHeader>
<bodyText confidence="0.997182333333333">
In this section, we describe both traditional heuris-
tic phrase extraction, and the proposed model-based
extraction method.
</bodyText>
<page confidence="0.982984">
636
</page>
<table confidence="0.576954833333333">
ities Pt(he, fi). Similarly to the heuristic phrase
tables, we use conditional probabilities Pt(f|e)
and Pt(e|f), lexical weighting probabilities, and a
phrase penalty. Here, instead of using maximum
likelihood, we calculate conditional probabilities di-
rectly from Pt probabilities:
</table>
<figureCaption confidence="0.9910595">
Figure 3: The phrase, block, and word alignments used
in heuristic phrase extraction.
</figureCaption>
<figure confidence="0.6201198">
∑
Pt(f|e) = Pt(he,fi)/
{˜f:c(he,
Pt(he, ˜fi)
˜fi)≥1}
</figure>
<subsectionHeader confidence="0.983731">
5.1 Heuristic Phrase Extraction
</subsectionHeader>
<bodyText confidence="0.99999265625">
The traditional method for heuristic phrase extrac-
tion from word alignments exhaustively enumerates
all phrases up to a certain length consistent with the
alignment (Och et al., 1999). Five features are used
in the phrase table: the conditional phrase proba-
bilities in both directions estimated using maximum
likelihood Pml(f|e) and Pml(e|f), lexical weight-
ing probabilities (Koehn et al., 2003), and a fixed
penalty for each phrase. We will call this heuristic
extraction from word alignments HEUR-W. These
word alignments can be acquired through the stan-
dard GIZA++ training regimen.
We use the combination of our ITG-based align-
ment with traditional heuristic phrase extraction as
a second baseline. An example of these alignments
is shown in Figure 3. In model HEUR-P, minimal
phrases generated from Pt are treated as aligned, and
we perform phrase extraction on these alignments.
However, as the proposed models tend to align rel-
atively large phrases, we also use two other tech-
niques to create smaller alignment chunks that pre-
vent sparsity. We perform regular sampling of the
trees, but if we reach a minimal phrase generated
from Pt, we continue traveling down the tree un-
til we reach either a one-to-many alignment, which
we will call HEUR-B as it creates alignments simi-
lar to the block ITG, or an at-most-one alignment,
which we will call HEUR-W as it generates word
alignments. It should be noted that forcing align-
ments smaller than the model suggests is only used
for generating alignments for use in heuristic extrac-
tion, and does not affect the training process.
</bodyText>
<subsectionHeader confidence="0.999398">
5.2 Model-Based Phrase Extraction
</subsectionHeader>
<bodyText confidence="0.9998685">
We also propose a method for phrase table ex-
traction that directly utilizes the phrase probabil-
</bodyText>
<equation confidence="0.9719625">
Pt(e|f) = Pt(he, fi)/ ∑ Pt(h˜e,fi).
{˜e:c(h˜e,fi)≥1}
</equation>
<bodyText confidence="0.9999603125">
To limit phrase table size, we include only phrase
pairs that are aligned at least once in the sample.
We also include two more features: the phrase
pair joint probability Pt(he, fi), and the average
posterior probability of each span that generated
he, fi as computed by the inside-outside algorithm
during training. We use the span probability as it
gives a hint about the reliability of the phrase pair. It
will be high for common phrase pairs that are gen-
erated directly from the model, and also for phrases
that, while not directly included in the model, are
composed of two high probability child phrases.
It should be noted that while for FLAT and HIER Pt
can be used directly, as HLEN learns separate models
for each length, we must combine these probabilities
into a single value. We do this by setting
</bodyText>
<equation confidence="0.984516">
Pt(he,fi) = Pt,l(he,fi)c(l)/
</equation>
<bodyText confidence="0.998505">
for every phrase pair, where l = |e |+ |f |and c(l) is
the number of phrases of length l in the sample.
We call this model-based extraction method MOD.
</bodyText>
<subsectionHeader confidence="0.9988">
5.3 Sample Combination
</subsectionHeader>
<bodyText confidence="0.9998893">
As has been noted in previous works, (Koehn et al.,
2003; DeNero et al., 2006) exhaustive phrase extrac-
tion tends to out-perform approaches that use syn-
tax or generative models to limit phrase boundaries.
DeNero et al. (2006) state that this is because gen-
erative models choose only a single phrase segmen-
tation, and thus throw away many good phrase pairs
that are in conflict with this segmentation.
Luckily, in the Bayesian framework it is simple to
overcome this problem by combining phrase tables
</bodyText>
<equation confidence="0.650944333333333">
∑L
˜l=1
c(˜l)
</equation>
<page confidence="0.989291">
637
</page>
<bodyText confidence="0.9999115">
from multiple samples. This is equivalent to approx-
imating the integral over various parameter configu-
rations in Equation (1). In MOD, we do this by taking
the average of the joint probability and span prob-
ability features, and re-calculating the conditional
probabilities from the averaged joint probabilities.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999993451612903">
In addition to the previously mentioned phrase
alignment techniques, there has also been a signif-
icant body of work on phrase extraction (Moore and
Quirk (2007), Johnson et al. (2007a), inter alia).
DeNero and Klein (2010) presented the first work
on joint phrase alignment and extraction at multiple
levels. While they take a supervised approach based
on discriminative methods, we present a fully unsu-
pervised generative model.
A generative probabilistic model where longer
units are built through the binary combination of
shorter units was proposed by de Marcken (1996) for
monolingual word segmentation using the minimum
description length (MDL) framework. Our work dif-
fers in that it uses Bayesian techniques instead of
MDL, and works on two languages, not one.
Adaptor grammars, models in which non-
terminals memorize subtrees that lie below them,
have been used for word segmentation or other
monolingual tasks (Johnson et al., 2007b). The pro-
posed method could be thought of as synchronous
adaptor grammars over two languages. However,
adaptor grammars have generally been used to spec-
ify only two or a few levels as in the FLAT model in
this paper, as opposed to recursive models such as
HIER or many-leveled models such as HLEN. One
exception is the variational inference method for
adaptor grammars presented by Cohen et al. (2010)
that is applicable to recursive grammars such as
HIER. We plan to examine variational inference for
the proposed models in future work.
</bodyText>
<sectionHeader confidence="0.991856" genericHeader="evaluation">
7 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999776">
We evaluate the proposed method on translation
tasks from four languages, French, German, Span-
ish, and Japanese, into English.
</bodyText>
<table confidence="0.99803">
de-en es-en fr-en ja-en
TM (en) 1.80M 1.62M 1.35M 2.38M
TM (other) 1.85M 1.82M 1.56M 2.78M
LM (en) 52.7M 52.7M 52.7M 44.7M
Tune (en ) 49.8k 49.8k 49.8k 68.9k
Tune (other) 47.2k 52.6k 55.4k 80.4k
Test (en) 65.6k 65.6k 65.6k 40.4k
Test (other) 62.7k 68.1k 72.6k 48.7k
</table>
<tableCaption confidence="0.978016">
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
</tableCaption>
<subsectionHeader confidence="0.995563">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999962903225806">
The data for French, German, and Spanish are from
the 2010 Workshop on Statistical Machine Transla-
tion (Callison-Burch et al., 2010). We use the news
commentary corpus for training the TM, and the
news commentary and Europarl corpora for training
the LM. For Japanese, we use data from the NTCIR
patent translation task (Fujii et al., 2008). We use
the first 100k sentences of the parallel corpus for the
TM, and the whole parallel corpus for the LM. De-
tails of both corpora can be found in Table 1. Cor-
pora are tokenized, lower-cased, and sentences of
over 40 words on either side are removed for TM
training. For both tasks, we perform weight tuning
and testing on specified development and test sets.
We compare the accuracy of our proposed method
of joint phrase alignment and extraction using the
FLAT, HIER and HLEN models, with a baseline of
using word alignments from GIZA++ and heuris-
tic phrase extraction. Decoding is performed using
Moses (Koehn and others, 2007) using the phrase
tables learned by each method under consideration,
as well as standard bidirectional lexical reordering
probabilities (Koehn et al., 2005). Maximum phrase
length is limited to 7 in all models, and for the LM
we use an interpolated Kneser-Ney 5-gram model.
For GIZA++, we use the standard training reg-
imen up to Model 4, and combine alignments
with grow-diag-final-and. For the proposed
models, we train for 100 iterations, and use the final
sample acquired at the end of the training process for
our experiments using a single sample6. In addition,
</bodyText>
<footnote confidence="0.997936">
6For most models, while likelihood continued to increase
gradually for all 100 iterations, BLEU score gains plateaued af-
ter 5-10 iterations, likely due to the strong prior information
</footnote>
<page confidence="0.988083">
638
</page>
<table confidence="0.9999465">
Align Extract # Samp. de-en es-en fr-en ja-en
BLEU Size BLEU Size BLEU Size BLEU Size
GIZA++ HEUR-W 1 16.62 4.91M 22.00 4.30M 21.35 4.01M 23.20 4.22M
FLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7k
HIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723k
HLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820k
HIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21M
HLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70M
</table>
<tableCaption confidence="0.998893">
Table 2: BLEU score and phrase table size by alignment method, extraction method, and samples combined. Bold
numbers are not significantly different from the best result according to the sign test (p &lt; 0.05) (Collins et al., 2005).
</tableCaption>
<bodyText confidence="0.8637705">
we also try averaging the phrase tables from the last
ten samples as described in Section 5.3.
</bodyText>
<subsectionHeader confidence="0.996702">
7.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9340436875">
The results for these experiments can be found in Ta-
ble 2. From these results we can see that when using
a single sample, the combination of using HIER and
model probabilities achieves results approximately
equal to GIZA++ and heuristic phrase extraction.
This is the first reported result in which an unsu-
pervised phrase alignment model has built a phrase
table directly from model probabilities and achieved
results that compare to heuristic phrase extraction. It
can also be seen that the phrase table created by the
proposed method is approximately 5 times smaller
than that obtained by the traditional pipeline.
In addition, HIER significantly outperforms FLAT
when using the model probabilities. This confirms
that phrase tables containing only minimal phrases
are not able to achieve results that compete with
phrase tables that use multiple granularities.
Somewhat surprisingly, HLEN consistently
slightly underperforms HIER. This indicates
potential gains to be provided by length-based
parameter tuning were outweighed by losses due
to the increased complexity of the model. In
particular, we believe the necessity to combine
probabilities from multiple Pt,l models into a single
phrase table may have resulted in a distortion of the
phrase probabilities. In addition, the assumption
that phrase lengths are generated from a uniform
distribution is likely too strong, and further gains
provided by Pb..,. As iterations took 1.3 hours on a single
processor, good translation results can be achieved in approxi-
mately 13 hours, which could further reduced using distributed
sampling (Newman et al., 2009; Blunsom et al., 2009).
</bodyText>
<table confidence="0.9990114">
FLAT HIER
MOD 17.97 117k 21.50 751k
HEUR-W 21.52 5.65M 21.68 5.39M
HEUR-B 21.45 4.93M 21.41 2.61M
HEUR-P 21.56 4.88M 21.47 1.62M
</table>
<tableCaption confidence="0.991545">
Table 3: Translation results and phrase table size for var-
ious phrase extraction techniques (French-English).
</tableCaption>
<bodyText confidence="0.999952346153846">
could likely be achieved by more accurate modeling
of phrase lengths. We leave further adjustments to
the HLEN model to future work.
It can also be seen that combining phrase tables
from multiple samples improved the BLEU score
for HLEN, but not for HIER. This suggests that for
HIER, most of the useful phrase pairs discovered by
the model are included in every iteration, and the in-
creased recall obtained by combining multiple sam-
ples does not consistently outweigh the increased
confusion caused by the larger phrase table.
We also evaluated the effectiveness of model-
based phrase extraction compared to heuristic phrase
extraction. Using the alignments from HIER, we cre-
ated phrase tables using model probabilities (MOD),
and heuristic extraction on words (HEUR-W), blocks
(HEUR-B), and minimal phrases (HEUR-P) as de-
scribed in Section 5. The results of these ex-
periments are shown in Table 3. It can be seen
that model-based phrase extraction using HIER out-
performs or insignificantly underperforms heuris-
tic phrase extraction over all experimental settings,
while keeping the phrase table to a fraction of the
size of most heuristic extraction methods.
Finally, we varied the size of the parallel corpus
for the Japanese-English task from 50k to 400k sen-
</bodyText>
<page confidence="0.997861">
639
</page>
<figureCaption confidence="0.998707">
Figure 4: The effect of corpus size on the accuracy (a) and
phrase table size (b) for each method (Japanese-English).
</figureCaption>
<bodyText confidence="0.962676583333333">
tences and measured the effect of corpus size on
translation accuracy. From the results in Figure 4
(a), it can be seen that at all corpus sizes, the re-
sults from all three methods are comparable, with
insignificant differences between GIZA++ and HIER
at all levels, and HLEN lagging slightly behind HIER.
Figure 4 (b) shows the size of the phrase table in-
duced by each method over the various corpus sizes.
It can be seen that the tables created by GIZA++ are
significantly larger at all corpus sizes, with the dif-
ference being particularly pronounced at larger cor-
pus sizes.
</bodyText>
<sectionHeader confidence="0.997657" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999952954545455">
In this paper, we presented a novel approach to joint
phrase alignment and extraction through a hierar-
chical model using non-parametric Bayesian meth-
ods and inversion transduction grammars. Machine
translation systems using phrase tables learned di-
rectly by the proposed model were able to achieve
accuracy competitive with the traditional pipeline of
word alignment and heuristic phrase extraction, the
first such result for an unsupervised model.
For future work, we plan to refine HLEN to use
a more appropriate model of phrase length than
the uniform distribution, particularly by attempting
to bias against phrase pairs where one of the two
phrases is much longer than the other. In addition,
we will test probabilities learned using the proposed
model with an ITG-based decoder. We will also ex-
amine the applicability of the proposed model in the
context of hierarchical phrases (Chiang, 2007), or
in alignment using syntactic structure (Galley et al.,
2006). It is also worth examining the plausibility
of variational inference as proposed by Cohen et al.
(2010) in the alignment context.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999046">
This work was performed while the first author
was supported by the JSPS Research Fellowship for
Young Scientists.
</bodyText>
<sectionHeader confidence="0.996675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9815732">
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proceed-
ings of the Human Language Technology: The 11th
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association for Computa-
tional Linguistics, pages 782–790.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint 5th Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17–53.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL Workshop on Syntax and
Structure in Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars. In
Proceedings of the Human Language Technology: The
</reference>
<page confidence="0.988259">
640
</page>
<reference confidence="0.998851688679245">
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 564–572.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 531–540.
Carl de Marcken. 1996. Unsupervised Language Acqui-
sition. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 25–28.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1453–1463.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of the 1st Workshop
on Statistical Machine Translation, pages 31–38.
John DeNero, Alex Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 314–323.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of the 7th NTCIR Workshop Meeting on Evaluation of
Information Access Technologies, pages 389–400.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics, pages 961–968.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007a. Improving translation quality
by discarding most of the phrasetable. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
Advances in Neural Information Processing Systems,
19:641.
Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference (HLT-
NAACL), pages 48–54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 104–111.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. pages 133–139.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of
the 2nd Workshop on Statistical Machine Translation,
pages 112–119.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801–1828.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 4th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 20–28.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855–
900.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the The 11th International Workshop
on Parsing Technologies.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 97–105.
</reference>
<page confidence="0.998296">
641
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743691">
<title confidence="0.999912">An Unsupervised Model for Joint Phrase Alignment and Extraction</title>
<author confidence="0.979226">Eiichiro Shinsuke Tatsuya</author>
<affiliation confidence="0.925607">School of Informatics, Kyoto Yoshida Honmachi, Sakyo-ku, Kyoto, Japan Institute of Information and Communication</affiliation>
<address confidence="0.940923">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<abstract confidence="0.9997745">We present an unsupervised model for joint phrase alignment and extraction using nonparametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Inducing synchronous grammars with slice sampling.</title>
<date>2010</date>
<booktitle>In Proceedings of the Human Language Technology: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17300" citStr="Blunsom and Cohn, 2010" startWordPosition="2893" endWordPosition="2896">ses are lower than those of long phrases. In particular, phrase pairs of length up to six (for example, |e |= 3, |f |= 3) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max(|e|, |f|) G 3 cause significant improvements in BLEU score, while using larger phrases results in diminishing returns. 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase co</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Proceedings of the Human Language Technology: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>782--790</pages>
<contexts>
<context position="2537" citStr="Blunsom et al., 2009" startWordPosition="374" endWordPosition="377">rase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to genera</context>
<context position="5625" citStr="Blunsom et al., 2009" startWordPosition="875" endWordPosition="878">sed SMT to find P(e|f, θ) and concentrate on creating a model for P(θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities P(θ|hE,Fi) ∝ P(hE,Fi|θ)P(θ). In Section 3 we describe an existing method, and in Section 4 we describe our proposed method for modeling these two probabilities. 3 Flat ITG Model There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pflat(he, fi; θx, θt) is parameterized by a phrase table θt and a symbol distribution θx. We use the following generative story as a representative of the flat IT</context>
<context position="17275" citStr="Blunsom et al., 2009" startWordPosition="2889" endWordPosition="2892">scounts for short phrases are lower than those of long phrases. In particular, phrase pairs of length up to six (for example, |e |= 3, |f |= 3) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max(|e|, |f|) G 3 cause significant improvements in BLEU score, while using larger phrases results in diminishing returns. 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is th</context>
<context position="28989" citStr="Blunsom et al., 2009" startWordPosition="4809" endWordPosition="4812"> were outweighed by losses due to the increased complexity of the model. In particular, we believe the necessity to combine probabilities from multiple Pt,l models into a single phrase table may have resulted in a distortion of the phrase probabilities. In addition, the assumption that phrase lengths are generated from a uniform distribution is likely too strong, and further gains provided by Pb..,. As iterations took 1.3 hours on a single processor, good translation results can be achieved in approximately 13 hours, which could further reduced using distributed sampling (Newman et al., 2009; Blunsom et al., 2009). FLAT HIER MOD 17.97 117k 21.50 751k HEUR-W 21.52 5.65M 21.68 5.39M HEUR-B 21.45 4.93M 21.41 2.61M HEUR-P 21.56 4.88M 21.47 1.62M Table 3: Translation results and phrase table size for various phrase extraction techniques (French-English). could likely be achieved by more accurate modeling of phrase lengths. We leave further adjustments to the HLEN model to future work. It can also be seen that combining phrase tables from multiple samples improved the BLEU score for HLEN, but not for HIER. This suggests that for HIER, most of the useful phrase pairs discovered by the model are included in ev</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, pages 782–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<contexts>
<context position="24969" citStr="Callison-Burch et al., 2010" startWordPosition="4151" endWordPosition="4154">te the proposed method on translation tasks from four languages, French, German, Spanish, and Japanese, into English. de-en es-en fr-en ja-en TM (en) 1.80M 1.62M 1.35M 2.38M TM (other) 1.85M 1.82M 1.56M 2.78M LM (en) 52.7M 52.7M 52.7M 44.7M Tune (en ) 49.8k 49.8k 49.8k 68.9k Tune (other) 47.2k 52.6k 55.4k 80.4k Test (en) 65.6k 65.6k 65.6k 40.4k Test (other) 62.7k 68.1k 72.6k 48.7k Table 1: The number of words in each corpus for TM and LM training, tuning, and testing. 7.1 Experimental Setup The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We use the news commentary corpus for training the TM, and the news commentary and Europarl corpora for training the LM. For Japanese, we use data from the NTCIR patent translation task (Fujii et al., 2008). We use the first 100k sentences of the parallel corpus for the TM, and the whole parallel corpus for the LM. Details of both corpora can be found in Table 1. Corpora are tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of o</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL Workshop on Syntax and Structure in Machine Translation.</booktitle>
<contexts>
<context position="2494" citStr="Cherry and Lin, 2007" startWordPosition="366" endWordPosition="369"> that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-</context>
<context position="5582" citStr="Cherry and Lin, 2007" startWordPosition="867" endWordPosition="870"> can use traditional methods for phrase-based SMT to find P(e|f, θ) and concentrate on creating a model for P(θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities P(θ|hE,Fi) ∝ P(hE,Fi|θ)P(θ). In Section 3 we describe an existing method, and in Section 4 we describe our proposed method for modeling these two probabilities. 3 Flat ITG Model There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pflat(he, fi; θx, θt) is parameterized by a phrase table θt and a symbol distribution θx. We use the following generati</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proceedings of the NAACL Workshop on Syntax and Structure in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>David M Blei</author>
<author>Noah A Smith</author>
</authors>
<title>Variational inference for adaptor grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the Human Language Technology: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>564--572</pages>
<contexts>
<context position="14890" citStr="Cohen et al. (2010)" startWordPosition="2463" endWordPosition="2466">enerates from the symbol distribution Px, then from the phrase distribution Pt, while HIER generates directly from Pt, which falls back to divide-and-conquer based on Px when necessary. It can be seen that while Pt in FLAT only generates minimal phrases, Pt in HIER generates (and thus memorizes) phrases at all levels of granularity. 4.1 Length-based Parameter Tuning There are still two problems with HIER, one theoretical, and one practical. Theoretically, HIER contains itself as its base measure, and stochastic process models that include themselves as base measures are deficient, as noted in Cohen et al. (2010). Practically, while the Pitman-Yor process in HIER shares the parameters s and d over all phrase pairs in the model, long phrase pairs are much more sparse 635 Figure 2: Learned discount values by phrase pair length. than short phrase pairs, and thus it is desirable to appropriately adjust the parameters of Equation (2) according to phrase pair length. In order to solve these problems, we reformulate the model so that each phrase length l = |f|+|e |has its own phrase parameters 0t,l and symbol parameters 0x,l, which are given separate priors: 0t,l — PY (s, d, Pdac,l) 0x,l — Dirichlet(α) We wi</context>
<context position="24169" citStr="Cohen et al. (2010)" startWordPosition="4019" endWordPosition="4022">of MDL, and works on two languages, not one. Adaptor grammars, models in which nonterminals memorize subtrees that lie below them, have been used for word segmentation or other monolingual tasks (Johnson et al., 2007b). The proposed method could be thought of as synchronous adaptor grammars over two languages. However, adaptor grammars have generally been used to specify only two or a few levels as in the FLAT model in this paper, as opposed to recursive models such as HIER or many-leveled models such as HLEN. One exception is the variational inference method for adaptor grammars presented by Cohen et al. (2010) that is applicable to recursive grammars such as HIER. We plan to examine variational inference for the proposed models in future work. 7 Experimental Evaluation We evaluate the proposed method on translation tasks from four languages, French, German, Spanish, and Japanese, into English. de-en es-en fr-en ja-en TM (en) 1.80M 1.62M 1.35M 2.38M TM (other) 1.85M 1.82M 1.56M 2.78M LM (en) 52.7M 52.7M 52.7M 44.7M Tune (en ) 49.8k 49.8k 49.8k 68.9k Tune (other) 47.2k 52.6k 55.4k 80.4k Test (en) 65.6k 65.6k 65.6k 40.4k Test (other) 62.7k 68.1k 72.6k 48.7k Table 1: The number of words in each corpus </context>
</contexts>
<marker>Cohen, Blei, Smith, 2010</marker>
<rawString>Shay B. Cohen, David M. Blei, and Noah A. Smith. 2010. Variational inference for adaptor grammars. In Proceedings of the Human Language Technology: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 564–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<marker>de Marcken, 1996</marker>
<rawString>Carl de Marcken. 1996. Unsupervised Language Acquisition. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="5970" citStr="DeNero and Klein, 2008" startWordPosition="927" endWordPosition="930">robabilities. 3 Flat ITG Model There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pflat(he, fi; θx, θt) is parameterized by a phrase table θt and a symbol distribution θx. We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px(x; θx). x can take the values TERM, REG, or INV. 2. According to the x take the following actions. (a) If x = TERM, generate a phrase pair from the phrase table Pt(he, fi; θt). (b) If x = REG, a regular ITG rule, generate phrase pairs he1, f1i and he2, f2i from Pflat, and conca</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative modeling of extraction sets for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1453--1463</pages>
<contexts>
<context position="1649" citStr="DeNero and Klein (2010)" startWordPosition="234" endWordPosition="237">the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model </context>
<context position="23049" citStr="DeNero and Klein (2010)" startWordPosition="3839" endWordPosition="3842">ple to overcome this problem by combining phrase tables ∑L ˜l=1 c(˜l) 637 from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, not one. Adaptor grammars, models in which nonterminals memoriz</context>
</contexts>
<marker>DeNero, Klein, 2010</marker>
<rawString>John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1453–1463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 1st Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="22059" citStr="DeNero et al., 2006" startWordPosition="3680" endWordPosition="3683"> from the model, and also for phrases that, while not directly included in the model, are composed of two high probability child phrases. It should be noted that while for FLAT and HIER Pt can be used directly, as HLEN learns separate models for each length, we must combine these probabilities into a single value. We do this by setting Pt(he,fi) = Pt,l(he,fi)c(l)/ for every phrase pair, where l = |e |+ |f |and c(l) is the number of phrases of length l in the sample. We call this model-based extraction method MOD. 5.3 Sample Combination As has been noted in previous works, (Koehn et al., 2003; DeNero et al., 2006) exhaustive phrase extraction tends to out-perform approaches that use syntax or generative models to limit phrase boundaries. DeNero et al. (2006) state that this is because generative models choose only a single phrase segmentation, and thus throw away many good phrase pairs that are in conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables ∑L ˜l=1 c(˜l) 637 from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Proceedings of the 1st Workshop on Statistical Machine Translation, pages 31–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alex Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alex Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 314–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Overview of the patent translation task at the NTCIR-7 workshop.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop Meeting on Evaluation of Information Access Technologies,</booktitle>
<pages>389--400</pages>
<contexts>
<context position="25177" citStr="Fujii et al., 2008" startWordPosition="4187" endWordPosition="4190">52.7M 52.7M 52.7M 44.7M Tune (en ) 49.8k 49.8k 49.8k 68.9k Tune (other) 47.2k 52.6k 55.4k 80.4k Test (en) 65.6k 65.6k 65.6k 40.4k Test (other) 62.7k 68.1k 72.6k 48.7k Table 1: The number of words in each corpus for TM and LM training, tuning, and testing. 7.1 Experimental Setup The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We use the news commentary corpus for training the TM, and the news commentary and Europarl corpora for training the LM. For Japanese, we use data from the NTCIR patent translation task (Fujii et al., 2008). We use the first 100k sentences of the parallel corpus for the TM, and the whole parallel corpus for the LM. Details of both corpora can be found in Table 1. Corpora are tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA++ and heuristic phrase extraction. Decoding is performed us</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Proceedings of the 7th NTCIR Workshop Meeting on Evaluation of Information Access Technologies, pages 389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="23009" citStr="Johnson et al. (2007" startWordPosition="3833" endWordPosition="3836">, in the Bayesian framework it is simple to overcome this problem by combining phrase tables ∑L ˜l=1 c(˜l) 637 from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, not one. Adaptor gramma</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>J. Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007a. Improving translation quality by discarding most of the phrasetable. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>19--641</pages>
<contexts>
<context position="23009" citStr="Johnson et al. (2007" startWordPosition="3833" endWordPosition="3836">, in the Bayesian framework it is simple to overcome this problem by combining phrase tables ∑L ˜l=1 c(˜l) 637 from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, not one. Adaptor gramma</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007b. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. Advances in Neural Information Processing Systems, 19:641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Koehn, 2007</marker>
<rawString>Philipp Koehn et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLTNAACL),</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1305" citStr="Koehn et al., 2003" startWordPosition="179" endWordPosition="182"> not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word align</context>
<context position="16960" citStr="Koehn et al. (2003)" startWordPosition="2840" endWordPosition="2843">fferent for each distribution. Further, as ll and lr must be smaller than l, Pt,l no longer contains itself as a base measure, and is thus not deficient. An example of the actual discount values learned in one of the experiments described in Section 7 is shown in Figure 2. It can be seen that, as expected, the discounts for short phrases are lower than those of long phrases. In particular, phrase pairs of length up to six (for example, |e |= 3, |f |= 3) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max(|e|, |f|) G 3 cause significant improvements in BLEU score, while using larger phrases results in diminishing returns. 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is </context>
<context position="19599" citStr="Koehn et al., 2003" startWordPosition="3262" endWordPosition="3265">tional probabilities directly from Pt probabilities: Figure 3: The phrase, block, and word alignments used in heuristic phrase extraction. ∑ Pt(f|e) = Pt(he,fi)/ {˜f:c(he, Pt(he, ˜fi) ˜fi)≥1} 5.1 Heuristic Phrase Extraction The traditional method for heuristic phrase extraction from word alignments exhaustively enumerates all phrases up to a certain length consistent with the alignment (Och et al., 1999). Five features are used in the phrase table: the conditional phrase probabilities in both directions estimated using maximum likelihood Pml(f|e) and Pml(e|f), lexical weighting probabilities (Koehn et al., 2003), and a fixed penalty for each phrase. We will call this heuristic extraction from word alignments HEUR-W. These word alignments can be acquired through the standard GIZA++ training regimen. We use the combination of our ITG-based alignment with traditional heuristic phrase extraction as a second baseline. An example of these alignments is shown in Figure 3. In model HEUR-P, minimal phrases generated from Pt are treated as aligned, and we perform phrase extraction on these alignments. However, as the proposed models tend to align relatively large phrases, we also use two other techniques to cr</context>
<context position="22037" citStr="Koehn et al., 2003" startWordPosition="3676" endWordPosition="3679">e generated directly from the model, and also for phrases that, while not directly included in the model, are composed of two high probability child phrases. It should be noted that while for FLAT and HIER Pt can be used directly, as HLEN learns separate models for each length, we must combine these probabilities into a single value. We do this by setting Pt(he,fi) = Pt,l(he,fi)c(l)/ for every phrase pair, where l = |e |+ |f |and c(l) is the number of phrases of length l in the sample. We call this model-based extraction method MOD. 5.3 Sample Combination As has been noted in previous works, (Koehn et al., 2003; DeNero et al., 2006) exhaustive phrase extraction tends to out-perform approaches that use syntax or generative models to limit phrase boundaries. DeNero et al. (2006) state that this is because generative models choose only a single phrase segmentation, and thus throw away many good phrase pairs that are in conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables ∑L ˜l=1 c(˜l) 637 from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference (HLTNAACL), pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="25967" citStr="Koehn et al., 2005" startWordPosition="4319" endWordPosition="4322"> tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA++ and heuristic phrase extraction. Decoding is performed using Moses (Koehn and others, 2007) using the phrase tables learned by each method under consideration, as well as standard bidirectional lexical reordering probabilities (Koehn et al., 2005). Maximum phrase length is limited to 7 in all models, and for the LM we use an interpolated Kneser-Ney 5-gram model. For GIZA++, we use the standard training regimen up to Model 4, and combine alignments with grow-diag-final-and. For the proposed models, we train for 100 iterations, and use the final sample acquired at the end of the training process for our experiments using a single sample6. In addition, 6For most models, while likelihood continued to increase gradually for all 100 iterations, BLEU score gains plateaued after 5-10 iterations, likely due to the strong prior information 638 A</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference - North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="10706" citStr="Liang et al., 2006" startWordPosition="1754" endWordPosition="1757">et al. (2008): Pba((e, f)) =M0((e, f))Ppois(|e|; A)Ppois(|f|; A) 1 2. Ppois is the Poisson distribution with the average length parameter A. As long phrases lead to sparsity, we set A to a relatively small value to allow us to bias against overly long phrases4. Pm1 is the word-based Model 1 (Brown et al., 1993) probability of one phrase given the other, which incorporates word-based alignment information as prior knowledge in the phrase translation probability. We take the geometric mean5of the Model 1 probabilities in both directions to encourage alignments that are supported by both models (Liang et al., 2006). It should be noted that while Model 1 probabilities are used, they are only soft constraints, compared with the hard constraint of choosing a single word alignment used in most previous phrase extraction approaches. For Pbu, if g is the non-null phrase in e and f, we calculate the probability as follows: Pbu((e, f)) = Puni(g)Ppois(|g|; A)/2. Note that Pbu is divided by 2 as the probability is considering null alignments in both directions. 4 Hierarchical ITG Model While in FLAT only minimal phrases were memorized by the model, as DeNero et al. (2008) note 3We choose 10−2, 10−3, or 10−10 base</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference - North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<pages>133--139</pages>
<contexts>
<context position="5481" citStr="Marcu and Wong (2002)" startWordPosition="849" endWordPosition="852">rameters: P(e|f, hE,Fi) = f P(e|f,θ)P(θ|hE,Fi). (1) B If θ takes the form of a scored phrase table, we can use traditional methods for phrase-based SMT to find P(e|f, θ) and concentrate on creating a model for P(θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities P(θ|hE,Fi) ∝ P(hE,Fi|θ)P(θ). In Section 3 we describe an existing method, and in Section 4 we describe our proposed method for modeling these two probabilities. 3 Flat ITG Model There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pflat(he, fi; θx, </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>An iterativelytrained segmentation-free phrase translation model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd Workshop on Statistical Machine Translation,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="22987" citStr="Moore and Quirk (2007)" startWordPosition="3829" endWordPosition="3832">is segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables ∑L ˜l=1 c(˜l) 637 from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, n</context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Robert C. Moore and Chris Quirk. 2007. An iterativelytrained segmentation-free phrase translation model for statistical machine translation. In Proceedings of the 2nd Workshop on Statistical Machine Translation, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--1801</pages>
<contexts>
<context position="28966" citStr="Newman et al., 2009" startWordPosition="4805" endWordPosition="4808">ased parameter tuning were outweighed by losses due to the increased complexity of the model. In particular, we believe the necessity to combine probabilities from multiple Pt,l models into a single phrase table may have resulted in a distortion of the phrase probabilities. In addition, the assumption that phrase lengths are generated from a uniform distribution is likely too strong, and further gains provided by Pb..,. As iterations took 1.3 hours on a single processor, good translation results can be achieved in approximately 13 hours, which could further reduced using distributed sampling (Newman et al., 2009; Blunsom et al., 2009). FLAT HIER MOD 17.97 117k 21.50 751k HEUR-W 21.52 5.65M 21.68 5.39M HEUR-B 21.45 4.93M 21.41 2.61M HEUR-P 21.56 4.88M 21.47 1.62M Table 3: Translation results and phrase table size for various phrase extraction techniques (French-English). could likely be achieved by more accurate modeling of phrase lengths. We leave further adjustments to the HLEN model to future work. It can also be seen that combining phrase tables from multiple samples improved the BLEU score for HLEN, but not for HIER. This suggests that for HIER, most of the useful phrase pairs discovered by the m</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research, 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="12068" citStr="Och et al., 1999" startWordPosition="1979" endWordPosition="1982">the development set. 5The probabilities of the geometric mean do not add to one, but we found empirically that even when left unnormalized, this provided much better results than the using the arithmetic mean, which is more theoretically correct. M0((e,f)) =(Pm1(f|e)Puni(e)Pm1(e|f)Puni(f)) 634 and we confirm in the experiments in Section 7, using only minimal phrases leads to inferior translation results for phrase-based SMT. Because of this, previous research has combined FLAT with heuristic phrase extraction, which exhaustively combines all adjacent phrases permitted by the word alignments (Och et al., 1999). We propose an alternative, fully statistical approach that directly models phrases at multiple granularities, which we will refer to as HIER. By doing so, we are able to do away with heuristic phrase extraction, creating a fully probabilistic model for phrase probabilities that still yields competitive results. Similarly to FLAT, HIER assigns a probability Phi,,((e, f); Bx, Bt) to phrase pairs, and is parameterized by a phrase table Bt and a symbol distribution Bx. The main difference from the generative story of the traditional ITG model is that symbols and phrase pairs are generated in the</context>
<context position="19387" citStr="Och et al., 1999" startWordPosition="3231" endWordPosition="3234">milarly to the heuristic phrase tables, we use conditional probabilities Pt(f|e) and Pt(e|f), lexical weighting probabilities, and a phrase penalty. Here, instead of using maximum likelihood, we calculate conditional probabilities directly from Pt probabilities: Figure 3: The phrase, block, and word alignments used in heuristic phrase extraction. ∑ Pt(f|e) = Pt(he,fi)/ {˜f:c(he, Pt(he, ˜fi) ˜fi)≥1} 5.1 Heuristic Phrase Extraction The traditional method for heuristic phrase extraction from word alignments exhaustively enumerates all phrases up to a certain length consistent with the alignment (Och et al., 1999). Five features are used in the phrase table: the conditional phrase probabilities in both directions estimated using maximum likelihood Pml(f|e) and Pml(e|f), lexical weighting probabilities (Koehn et al., 2003), and a fixed penalty for each phrase. We will call this heuristic extraction from word alignments HEUR-W. These word alignments can be acquired through the standard GIZA++ training regimen. We use the combination of our ITG-based alignment with traditional heuristic phrase extraction as a second baseline. An example of these alignments is shown in Figure 3. In model HEUR-P, minimal ph</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 4th Conference on Empirical Methods in Natural Language Processing, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<volume>25</volume>
<issue>2</issue>
<pages>900</pages>
<contexts>
<context position="3324" citStr="Pitman and Yor, 1997" startWordPosition="506" endWordPosition="509">from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to generate phrase pairs that do not exist (or are given low probability) in the phrase distribution. We combine this model with the Bayesian nonparametric Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), realizing ITG-based divide and conquer through a novel formulation where the Pitman-Yor process uses two copies of itself as a 632 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632–641, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics base measure. As a result of this modeling strategy, phrases of multiple granularities are generated, and thus memorized, by the Pitman-Yor process. This makes it possible to directly use probabilities of the phrase model as a replacement for the phrase table genera</context>
<context position="7398" citStr="Pitman and Yor, 1997" startWordPosition="1185" endWordPosition="1188">at over every sentence in the corpus, we are able to calculate the likelihood P(hE,Fi|θ) = ∏ Pflat(he, fi; θ). (e,f)E(£,J7) We will refer to this model as FLAT. 3.1 Bayesian Modeling While the previous formulation can be used as-is in maximum likelihood training, this leads to a degenerate solution where every sentence is memorized as a single phrase pair. Zhang et al. (2008) and others propose dealing with this problem by putting a prior probability P(θx, θt) on the parameters. 633 We assign Bx a Dirichlet priors, and assign the phrase table parameters Bt a prior using the PitmanYor process (Pitman and Yor, 1997; Teh, 2006), which is a generalization of the Dirichlet process prior used in previous research. It is expressed as Bt —PY (d, s, Pbase) (2) where d is the discount parameter, s is the strength parameter, and Pbase is the base measure. The discount d is subtracted from observed counts, and when it is given a large value (close to one), less frequent phrase pairs will be given lower relative probability than more common phrase pairs. The strength s controls the overall sparseness of the distribution, and when it is given a small value the distribution will be sparse. Pbase is the prior probabi</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25(2):855– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of the The 11th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="17649" citStr="Saers et al. (2009)" startWordPosition="2949" endWordPosition="2952">ents in BLEU score, while using larger phrases results in diminishing returns. 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase counts. As a phrase pair ta may have been generated from two smaller component phrases tb and tc, when a sample containing ta is removed from the distribution, it may also be necessary to decrement the counts of tb and tc as well. The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solut</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. 2009. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In Proceedings of the The 11th International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3336" citStr="Teh, 2006" startWordPosition="510" endWordPosition="511">tribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to generate phrase pairs that do not exist (or are given low probability) in the phrase distribution. We combine this model with the Bayesian nonparametric Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), realizing ITG-based divide and conquer through a novel formulation where the Pitman-Yor process uses two copies of itself as a 632 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632–641, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics base measure. As a result of this modeling strategy, phrases of multiple granularities are generated, and thus memorized, by the Pitman-Yor process. This makes it possible to directly use probabilities of the phrase model as a replacement for the phrase table generated by heuri</context>
<context position="7410" citStr="Teh, 2006" startWordPosition="1189" endWordPosition="1190"> in the corpus, we are able to calculate the likelihood P(hE,Fi|θ) = ∏ Pflat(he, fi; θ). (e,f)E(£,J7) We will refer to this model as FLAT. 3.1 Bayesian Modeling While the previous formulation can be used as-is in maximum likelihood training, this leads to a degenerate solution where every sentence is memorized as a single phrase pair. Zhang et al. (2008) and others propose dealing with this problem by putting a prior probability P(θx, θt) on the parameters. 633 We assign Bx a Dirichlet priors, and assign the phrase table parameters Bt a prior using the PitmanYor process (Pitman and Yor, 1997; Teh, 2006), which is a generalization of the Dirichlet process prior used in previous research. It is expressed as Bt —PY (d, s, Pbase) (2) where d is the discount parameter, s is the strength parameter, and Pbase is the base measure. The discount d is subtracted from observed counts, and when it is given a large value (close to one), less frequent phrase pairs will be given lower relative probability than more common phrase pairs. The strength s controls the overall sparseness of the distribution, and when it is given a small value the distribution will be sparse. Pbase is the prior probability of gene</context>
<context position="8988" citStr="Teh, 2006" startWordPosition="1460" endWordPosition="1461">hich results in the induction of phrase tables with fewer, but more helpful phrases. It is important to note that only phrases generated by Pt are actually memorized and given higher probability by the model. In FLAT, only minimal phrases generated after Px outputs the terminal symbol TERM are generated from Pt, and thus only minimal phrases are memorized by the model. While the Dirichlet process is simply the PitmanYor process with d = 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006). We confirmed in preliminary experiments (using the data described in Section 7) that the Pitman-Yor process with automatically adjusted parameters results in superior alignment results, outperforming the sparse Dirichlet process priors used in previous research2. The average gain across all data sets was approximately 0.8 BLEU points. 1The value of a had little effect on the results, so we arbitrarily set a = 1. 2We put weak priors on s (Gamma(a = 2„3 = 1)) and d (Beta(a = 2„3 = 2)) for the Pitman-Yor process, and set a = 1−10 for the Dirichlet process. 3.2 Base Measure Pbase in Equation (2)</context>
<context position="18192" citStr="Teh, 2006" startWordPosition="3046" endWordPosition="3047"> purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase counts. As a phrase pair ta may have been generated from two smaller component phrases tb and tc, when a sample containing ta is removed from the distribution, it may also be necessary to decrement the counts of tb and tc as well. The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem. For each table representing a phrase pair ta, we maintain not only the number of customers sitting at the table, but also the identities of phrases tb and tc that were originally used when generating the table. When the count of the table ta is reduced to zero and the table is removed, the counts of tb and tc are also decremented. 5 Phrase Extraction In this section, we describe both traditional heuristic phrase extraction, and the proposed model-based extraction method. 636 ities Pt(he, fi). Similarly to the heurist</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="5702" citStr="Wu, 1997" startWordPosition="888" endWordPosition="889">se this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities P(θ|hE,Fi) ∝ P(hE,Fi|θ)P(θ). In Section 3 we describe an existing method, and in Section 4 we describe our proposed method for modeling these two probabilities. 3 Flat ITG Model There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pflat(he, fi; θx, θt) is parameterized by a phrase table θt and a symbol distribution θx. We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px(x; θx). x </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>97--105</pages>
<contexts>
<context position="2514" citStr="Zhang et al., 2008" startWordPosition="370" endWordPosition="373">ord alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and con</context>
<context position="5602" citStr="Zhang et al., 2008" startWordPosition="871" endWordPosition="874">ethods for phrase-based SMT to find P(e|f, θ) and concentrate on creating a model for P(θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities P(θ|hE,Fi) ∝ P(hE,Fi|θ)P(θ). In Section 3 we describe an existing method, and in Section 4 we describe our proposed method for modeling these two probabilities. 3 Flat ITG Model There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pflat(he, fi; θx, θt) is parameterized by a phrase table θt and a symbol distribution θx. We use the following generative story as a repres</context>
<context position="7156" citStr="Zhang et al. (2008)" startWordPosition="1143" endWordPosition="1146">nd he2, f2i from Pflat, and concatenate them into a single phrase pair he1e2, f1f2i. (c) If x = INV, an inverted ITG rule, follows the same process as (b), but concatenate f1 and f2 in reverse order he1e2, f2f1i. By taking the product of Pflat over every sentence in the corpus, we are able to calculate the likelihood P(hE,Fi|θ) = ∏ Pflat(he, fi; θ). (e,f)E(£,J7) We will refer to this model as FLAT. 3.1 Bayesian Modeling While the previous formulation can be used as-is in maximum likelihood training, this leads to a degenerate solution where every sentence is memorized as a single phrase pair. Zhang et al. (2008) and others propose dealing with this problem by putting a prior probability P(θx, θt) on the parameters. 633 We assign Bx a Dirichlet priors, and assign the phrase table parameters Bt a prior using the PitmanYor process (Pitman and Yor, 1997; Teh, 2006), which is a generalization of the Dirichlet process prior used in previous research. It is expressed as Bt —PY (d, s, Pbase) (2) where d is the discount parameter, s is the strength parameter, and Pbase is the base measure. The discount d is subtracted from observed counts, and when it is given a large value (close to one), less frequent phras</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 97–105.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>