<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000645">
<title confidence="0.9973495">
Experiments of UNED at the Third
Recognising Textual Entailment Challenge
</title>
<author confidence="0.893538">
´Alvaro Rodrigo, Anselmo Pe˜nas, Jes´us Herrera, Felisa Verdejo
</author>
<affiliation confidence="0.856331">
Departmento de Lenguajes y Sistemas Inform´aticos
</affiliation>
<address confidence="0.728372">
Universidad Nacional de Educaci´on a Distancia
Madrid, Spain
</address>
<email confidence="0.973904">
{alvarory, anselmo, jesus.herrera, felisa}@lsi.uned.es
</email>
<sectionHeader confidence="0.996312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964083333333">
This paper describes the experiments devel-
oped and the results obtained in the partic-
ipation of UNED in the Third Recognising
Textual Entailment (RTE) Challenge. The
experiments are focused on the study of the
effect of named entities in the recognition
of textual entailment. While Named Entity
Recognition (NER) provides remarkable re-
sults (accuracy over 70%) for RTE on QA
task, IE task requires more sophisticated
treatment of named entities such as the iden-
tification of relations between them.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956">
The systems presented to the Third Recognizing
Textual Entailment Challenge are based on the one
presented to the Second RTE Challenge (Herrera
et al., 2006b) and the ones presented to the An-
swer Validation Exercise (AVE) 2006 (Rodrigo et
al., 2007).
Since a high quantity of pairs of RTE-3 collec-
tions contain named entities (82.6% of the hypothe-
ses in the test collection contain at least one named
entity), the objective of this work is to study the ef-
fect of named entity recognition on textual entail-
ment in the framework of the Third RTE Challenge.
In short, the techniques involved in the experi-
ments in order to reach these objectives are:
</bodyText>
<listItem confidence="0.999732333333333">
• Lexical overlapping between ngrams of text
and hypothesis.
• Entailment between named entities.
</listItem>
<page confidence="0.805948">
89
</page>
<listItem confidence="0.9312055">
• Branch overlapping between dependency trees
of text and hypothesis.
</listItem>
<bodyText confidence="0.999891125">
In section 2, the main components of the systems
are described in detail. Section 3 describes the infor-
mation our systems use for the entailment decision.
The description of the two runs submitted are given
in Section 4. The results obtained and its analysis are
described in Section 5. Section 6 shows a discussion
of the results. Finally, some conclusions and future
work are given.
</bodyText>
<sectionHeader confidence="0.986187" genericHeader="method">
2 Systems Description
</sectionHeader>
<bodyText confidence="0.999510833333333">
The proposed systems are based on surface tech-
niques of lexical and syntactic analysis considering
each task (Information Extraction, Information Re-
trieval, Question Answering and Text Summariza-
tion) of the RTE Challenge independently.
The systems accept pairs of text snippets (text and
hypothesis) at the input and give a boolean value at
the output: YES if the text entails the hypothesis and
NO otherwise. This value is obtained by the appli-
cation of the learned model by a SVM classifier.
The main components of the systems are the fol-
lowing:
</bodyText>
<subsectionHeader confidence="0.996967">
2.1 Linguistic processing
</subsectionHeader>
<bodyText confidence="0.959889666666667">
Firstly, each text-hypothesis pair is preprocessed in
order to obtain the following information for the en-
tailment decision:
</bodyText>
<listItem confidence="0.97953525">
• POS: a Part of Speech Tagging is performed in
order to obtain lemmas for both text and hy-
pothesis using the Freeling POS tagger (Car-
reras et al., 2004).
</listItem>
<subsubsectionHeader confidence="0.373069">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 89–94,
</subsubsectionHeader>
<bodyText confidence="0.419975666666667">
Prague, June 2007. @2007 Association for Computational Linguistics
&lt;t&gt;...Iraq invaded Kuwait on &lt;TIMEX&gt;August 2 1990&lt;/TIMEX&gt;...&lt;/t&gt;
&lt;h&gt;Iraq invaded Kuwait in &lt;NUMEX&gt;1990&lt;/NUMEX&gt;&lt;/h&gt;
</bodyText>
<figureCaption confidence="0.999876">
Figure 1: Example of an error when disambiguating the named entity type.
</figureCaption>
<figure confidence="0.850666666666667">
&lt;t&gt;...Chernobyl accident began on
&lt;ENTITY&gt;Saturday April 26 1986&lt;/ENTITY&gt;...&lt;/t&gt;
&lt;h&gt;The Chernobyl disaster was in &lt;ENTITY&gt;1986&lt;/ENTITY&gt;&lt;/h&gt;
</figure>
<figureCaption confidence="0.999941">
Figure 2: Example of a pair that justifies the process of entailment.
</figureCaption>
<bodyText confidence="0.969674384615385">
&lt;pair id=“5”entailment=“NO”task=“IE”length=“short”&gt;
&lt;t&gt;The Communist Party USA was a small Maoist political party
which was founded in 1965 by members of the Communist Party around
Michael Laski who took the side of China in the Sino-Soviet split.
&lt;/t&gt;
&lt;h&gt;Michael Laski was an opponent of China.&lt;/h&gt;
&lt;/pair&gt;
&lt;pair id=“7”entailment=“NO”task=“IE”length=“short”&gt;
&lt;t&gt;Sandra Goudie was first elected to Parliament in the 2002
elections, narrowly winning the seat of Coromandel by defeating
Labour candidate Max Purnell and pushing incumbent Green MP
Jeanette Fitzsimons into third place.&lt;/t&gt;
&lt;h&gt;Sandra Goudie was defeated by Max Purnell.&lt;/h&gt;
</bodyText>
<figure confidence="0.958028166666667">
&lt;/pair&gt;
&lt;pair id=“8”entailment=“NO”task=“IE”length=“short”&gt;
&lt;t&gt;Ms. Minton left Australia in 1961 to pursue her studies in
London.&lt;/t&gt;
&lt;h&gt;Ms. Minton was born in Australia.&lt;/h&gt;
&lt;/pair&gt;
</figure>
<figureCaption confidence="0.998799">
Figure 3: IE pairs with entailment between named entities but not between named entities relations.
</figureCaption>
<page confidence="0.991562">
90
</page>
<listItem confidence="0.995077333333333">
• NER: the Freeling Named Entity Recogniser is
also applied to recover the information needed
by the named entity entailment module that is
described in the following section. Numeric ex-
pressions, proper nouns and temporal expres-
sions of each text and hypothesis are tagged.
• Dependency analysis: a dependency tree of
each text and hypothesis is obtained using Lin’s
Minipar (Lin, 1998).
</listItem>
<subsectionHeader confidence="0.998738">
2.2 Entailment between named entities
</subsectionHeader>
<bodyText confidence="0.9997775">
Once the named entities of the hypothesis and the
text are detected, the next step is to determine the
entailment relations between the named entities in
the text and the named entities in the hypothesis. In
(Rodrigo et al., 2007) the following entailment rela-
tions between named entities were defined:
</bodyText>
<listItem confidence="0.908980222222222">
1. A Proper Noun E1 entails a Proper Noun E2 if
the text string of E1 contains the text string of
E2.
2. A Time Expression T1 entails a Time Expres-
sion T2 if the time range of T1 is included in
the time range of T2.
3. A numeric expression N1 entails a numeric ex-
pression N2 if the range associated to N2 en-
closes the range of N1.
</listItem>
<bodyText confidence="0.999727565217391">
Some characters change in different expressions
of the same named entity as, for example, in a proper
noun with different wordings (e.g. Yasser, Yaser,
Yasir). To detect the entailment in these situations,
when the previous process fails, we implemented a
modified entailment decision process taking into ac-
count the edit distance of Levenshtein (Levensthein,
1966). Thus, if two named entities differ in less than
20%, then we assume that exists an entailment rela-
tion between these named entities.
However, this definition of named entities entail-
ment does not support errors due to wrong named
entities classification as we can see in Figure 1. The
expression 1990 represents a year but it is recog-
nised as a numeric expression in the hypothesis.
However the same expression is recognised as a tem-
poral expression in the text and, therefore, the ex-
pression in the hypothesis cannot be entailed by it
according to the named entities entailment definition
above.
We quantified the effect of these errors in recog-
nising textual entailment. For this purpose, we de-
veloped the following two settings:
</bodyText>
<listItem confidence="0.969482583333333">
1. A system based in dependency analysis and
WordNet (Herrera et al., 2006b) that uses the
categorization given by the NER tool, where
the entailment relations between named entities
are the previously ones defined.
2. The same system based on dependency analysis
and WordNet but not using the categorization
given by the NER tool. All named entities de-
tected receive the same tag and a named entity
E1 entails a named entity E2 if the text string
of E1 contains the text string of E2 (see Figure
2).
</listItem>
<bodyText confidence="0.999928588235294">
We checked the performance of these two settings
over the test corpus set of the Second RTE Chal-
lenge. The results obtained, using the accuracy mea-
sure that is the fraction of correct responses accord-
ing to (Dagan et al., 2006), are shown in table 1. The
table shows that with an easier and a more robust
processing (NER without classification) the perfor-
mance is not only maintained, but it is even slightly
higher.
This fact led us to ignore the named entity catego-
rization given by the tool and assume that text and
hypothesis are related and close texts where same
expressions must receive same categories, without
the need of classification. Thus, all detected named
entities receive the same tag and we consider that a
named entity E1 entails a named entity E2 if the text
string of E1 contains the text string of E2.
</bodyText>
<tableCaption confidence="0.999315">
Table 1: Entailment between numeric expressions.
</tableCaption>
<table confidence="0.785633">
Accuracy
Setting 1 0.610
Setting 2 0.614
</table>
<subsectionHeader confidence="0.986542">
2.3 Sentence level matching
</subsectionHeader>
<bodyText confidence="0.99936525">
A tree matching module, which searches for match-
ing branches into the hypotheses’ dependency trees,
is used. There is a potential matching branch per
leaf. A branch from the hypothesis is considered
</bodyText>
<page confidence="0.994729">
91
</page>
<bodyText confidence="0.999744571428572">
a “matching branch” only if all its nodes from the
root to the leaf are involved in a lexical entailment
(Herrera et al., 2006a). In this way, the subtree con-
formed by all the matching branches from a hypoth-
esis’ dependency tree is included in the respective
text’s dependency tree, giving an idea of tree inclu-
sion.
We assumed that the larger is the included sub-
tree of the hypothesis’ dependency tree, the more
semantically similar are the text and the hypothesis.
Thus, the existence or absence of an entailment rela-
tion from a text to its respective hypothesis considers
the portion of the hypothesis’ tree that is included in
the text’s tree.
</bodyText>
<sectionHeader confidence="0.995319" genericHeader="method">
3 Entailment decision
</sectionHeader>
<bodyText confidence="0.9999002">
A SVM classifier was applied in order to train a
model from the development corpus. The model was
trained with a set of features obtained from the pro-
cessing described above. The features we have used
and the training strategies were the following:
</bodyText>
<subsectionHeader confidence="0.956771">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.99651">
We prepared the following features to feed the SVM
model:
</bodyText>
<listItem confidence="0.995628684210526">
1. Percentage of nodes of the hypothesis’ de-
pendency tree pertaining to matching branches
according to section 2.3 considering, respec-
tively:
• Lexical entailment between the words of
the snippets involved.
• Lexical entailment between the lemmas of
the snippets involved.
2. Percentage of words of the hypothesis in the
text (treated as bags of words).
3. Percentage of unigrams (lemmas) of the hy-
pothesis in the text (treated as bags of lemmas).
4. Percentage of bigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
5. Percentage of trigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
6. A boolean value indicating if there is or not any
named entity in the hypothesis that is not en-
tailed by one or more named entities in the text
</listItem>
<bodyText confidence="0.8507335">
according to the named entity entailment deci-
sion described in section 2.2.
</bodyText>
<tableCaption confidence="0.991117">
Table 2: Experiments with separate training over the
development corpus using cross validation.
</tableCaption>
<table confidence="0.9984908">
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.64 0.67
Setting 2 0.62 0.66
</table>
<tableCaption confidence="0.985729">
Table 3: Experiments with separate training over the
test corpus.
</tableCaption>
<table confidence="0.9957312">
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.59 0.62
Setting 2 0.60 0.64
</table>
<tableCaption confidence="0.926935">
Table 4: Results for run 1 and run 2.
</tableCaption>
<table confidence="0.999217428571429">
Accuracy
run 1 run 2
IE 52.50% 53.50%
IR 67% 67%
QA 72% 72%
SUM 58% 60%
Overall 62.38% 63.12%
</table>
<subsectionHeader confidence="0.997863">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.995720333333333">
About the decision of how to perform the training
in our SVM models, we wanted to study the effect
of training a unique model compared to training one
different model per task.
For this purpose we used the following two set-
tings:
</bodyText>
<listItem confidence="0.99974525">
1. A SVM model that uses features 2, 3, 4 and 5
from section 3.1.
2. A SVM model that uses features 2, 3, 4, 5 and
6 from section 3.1.
</listItem>
<bodyText confidence="0.985574666666667">
Each setting was training using cross validation
over the development set of the Third RTE Chal-
lenge in two different ways:
</bodyText>
<footnote confidence="0.516058">
1. Training a unique model for all pairs.
</footnote>
<page confidence="0.988604">
92
</page>
<bodyText confidence="0.994231214285714">
2. Training one model for each task. Each model
is trained with only pairs from the same task
that the model will predict.
The results obtained in the experiments are shown
in table 2. As we can see in the table, with the train-
ing of one model for each task results are slightly
better, increasing performance of both settings. Tak-
ing into account these results, we took the decision
of using a different training for each task in the runs
submitted.
Our decision was confirmed after the runs submis-
sion to RTE-3 Challenge with new experiments over
the RTE-3 test corpus, using the RTE-3 development
corpus as training (see table 3 for results).
</bodyText>
<sectionHeader confidence="0.984231" genericHeader="method">
4 Runs Submitted
</sectionHeader>
<bodyText confidence="0.99968475">
Two different runs were submitted to the Third RTE
Challenge. Each run was trained using the method
described in section 3.2 with the following subset of
the features described in section 3.1:
</bodyText>
<listItem confidence="0.974096125">
• Run 1 was obtained using the features 2, 3,
4 and 5 from section 3.1. These features ob-
tained good results for pairs from the QA task,
as we can see in (Rodrigo et al., 2007), and
we wanted to check their performance in other
tasks.
• Run 2 was obtained using the following fea-
tures for each task:
</listItem>
<bodyText confidence="0.936255166666667">
– IE: features 2, 3, 4, 5 and 6 from section
3.1. These ones were the features that ob-
tained the best results for IE pairs in our
experiments over the development set.
– IR: features 2, 3, 4 and 5 from section 3.1.
These ones were the features that obtained
best results for IR pairs in our experiments
over the development set.
– QA: feature 6 from section 3.1. We chose
this feature, which had obtained an ac-
curacy over 70% in previous experiments
over the development set in QA pairs, to
study the effect of named entities in QA
pairs.
– SUM: features 1, 2 and 3 from section 3.1.
We selected these features to show the im-
portance of dependency analysis in SUM
pairs as it is shown in section 6.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999978411764706">
Accuracy was applied as the main measure to the
participating systems.
The results obtained over the test corpus for the
two runs submitted are shown in table 4.
As we can see in both runs, different accuracy val-
ues are obtained depending on the task. The best re-
sult is obtained in pairs from QA with a 72% accu-
racy in the two runs, although two different systems
are applied. This result pushes us to use this system
for Answer Validation (Pe˜nas et al., 2007). Results
in run 2, which uses a different setting for each task,
are slightly better than results in run 1, but only in
IE and SUM. However, results are too close to ac-
cept a confirmation of our initial intuition that pairs
from different tasks could need not only a different
training, but also the use of different approaches for
the entailment decision.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999964222222222">
In run 2 we used NER for IE and QA, the two tasks
with the higher percentage of pairs with at least one
named entity in the hypothesis (98.5% in IE and
97% in QA).
Our previous work about the use of named enti-
ties in textual entailment (Rodrigo et al., 2007) sug-
gested that NER permitted to obtain good results.
However, after the RTE-3 experience, we found that
the use of NER does not improve results in all tasks,
but only in QA in a solid way with the previous
work.
We performed a qualitative study over the IE pairs
showing that, as it can be expected, in pairs from IE
the relations between named entities are more im-
portant that named entities themselves.
Figure 3 shows some examples where all named
entities are entailed but not the relation between
them. In pair 5 both Michael Laski and China are
entailed but the relation between them is took the
side of in the text, and was an opponent of in the
hypothesis. The same problem appears in the other
pairs with the relation left instead was born in (pair
8) or passive voice instead active voice (pair 7).
Comparing run 1 and run 2, dependency analysis
shows its usefulness in SUM pairs, where texts and
hypotheses have a higher syntactic parallelism than
in pairs from other tasks. This statement is shown
</bodyText>
<page confidence="0.998862">
93
</page>
<tableCaption confidence="0.9709095">
Table 5: Percentage of hypothesis nodes in matching
branches.
</tableCaption>
<table confidence="0.9252326">
Percentage
SUM 75,505%
IE 7,353%
IR 6,422%
QA 8,496%
</table>
<bodyText confidence="0.999390428571428">
in table 5 where the percentage of hypothesis nodes
pertaining to matching branches in the dependency
tree is much higher in SUM pairs than in the rest of
tasks.
This syntactic parallelism seems to be the respon-
sible for the 2% increasing between the first and the
second run in SUM pairs.
</bodyText>
<sectionHeader confidence="0.994744" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999963454545454">
The experiments have been focused on the study of
the importance of considering entailment between
named entities in the recognition of textual entail-
ment, and the use of a separate training for each task.
As we have seen, both approaches increase slightly
the accuracy of the proposed systems. As we have
also shown, different approaches for each task could
also increase the system performance.
Future work is focused on improving the perfor-
mance in IE pairs taking into account relations be-
tween named entities.
</bodyText>
<sectionHeader confidence="0.998438" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998559">
This work has been partially supported by the Span-
ish Ministry of Science and Technology within the
project R2D2-SyEMBRA (TIC-2003-07158-C04-
02), the Regional Government of Madrid under the
Research Network MAVIR (S-0505/TIC-0267), the
Education Council of the Regional Government of
Madrid and the European Social Fund.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993416">
X. Carreras, I. Chao, L. Padr´o, and M. Padr´o. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers.. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC04). Lisbon, Portugal, 2004.
I. Dagan, O. Glickman and B. Magnini. 2006. The
PASCAL Recognising Textual Entailment Challenge.
In Qui˜nonero-Candela et al., editors, MLCW 2005,
LNAI Volume 3944, Jan 2006, Pages 177 - 190.
J. Herrera, A. Pe˜nas and F. Verdejo. 2006a. Textual En-
tailment Recognition Based on Dependency Analysis
and WordNet. In Qui˜nonero-Candela et al., editors,
MLCW 2005, LNAI Volume 3944, Jan 2006, Pages
231-239.
J. Herrera, A. Pe˜nas, ´A. Rodrigo and F. Verdejo. 2006b.
UNED at PASCAL RTE-2 Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
V. I. Levensthein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. In Soviet
Physics - Doklady, volume 10, pages 707710, 1966.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Systems,
Granada, Spain, May, 1998.
A. Pe˜nas, ´A. Rodrigo, V. Sama and F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2006. In
Lecture Notes in Computer Science. In press.
´A. Rodrigo, A. Pe˜nas, J. Herrera and F. Verdejo. 2007.
The Effect of Entity Recognition on Answer Validation.
In Lecture Notes in Computer Science. In press.
</reference>
<page confidence="0.999537">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.541928">
<title confidence="0.969858">Experiments of UNED at the Recognising Textual Entailment Challenge</title>
<author confidence="0.991517">Anselmo Jes´us Herrera Rodrigo</author>
<author confidence="0.991517">Felisa</author>
<affiliation confidence="0.9898315">Departmento de Lenguajes y Sistemas Universidad Nacional de Educaci´on a</affiliation>
<address confidence="0.610615">Madrid,</address>
<email confidence="0.972004">anselmo,jesus.herrera,</email>
<abstract confidence="0.998494769230769">This paper describes the experiments developed and the results obtained in the participation of UNED in the Third Recognising Textual Entailment (RTE) Challenge. The experiments are focused on the study of the effect of named entities in the recognition of textual entailment. While Named Entity Recognition (NER) provides remarkable results (accuracy over 70%) for RTE on QA task, IE task requires more sophisticated treatment of named entities such as the identification of relations between them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>I Chao</author>
<author>L Padr´o</author>
<author>M Padr´o</author>
</authors>
<title>FreeLing: An Open-Source Suite of Language Analyzers..</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04).</booktitle>
<location>Lisbon, Portugal,</location>
<marker>Carreras, Chao, Padr´o, Padr´o, 2004</marker>
<rawString>X. Carreras, I. Chao, L. Padr´o, and M. Padr´o. 2004. FreeLing: An Open-Source Suite of Language Analyzers.. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04). Lisbon, Portugal, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<contexts>
<context position="7265" citStr="Dagan et al., 2006" startWordPosition="1148" endWordPosition="1151">en by the NER tool, where the entailment relations between named entities are the previously ones defined. 2. The same system based on dependency analysis and WordNet but not using the categorization given by the NER tool. All named entities detected receive the same tag and a named entity E1 entails a named entity E2 if the text string of E1 contains the text string of E2 (see Figure 2). We checked the performance of these two settings over the test corpus set of the Second RTE Challenge. The results obtained, using the accuracy measure that is the fraction of correct responses according to (Dagan et al., 2006), are shown in table 1. The table shows that with an easier and a more robust processing (NER without classification) the performance is not only maintained, but it is even slightly higher. This fact led us to ignore the named entity categorization given by the tool and assume that text and hypothesis are related and close texts where same expressions must receive same categories, without the need of classification. Thus, all detected named entities receive the same tag and we consider that a named entity E1 entails a named entity E2 if the text string of E1 contains the text string of E2. Tab</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>I. Dagan, O. Glickman and B. Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>MLCW 2005, LNAI Volume 3944,</booktitle>
<pages>177--190</pages>
<editor>In Qui˜nonero-Candela et al., editors,</editor>
<marker>2006</marker>
<rawString>In Qui˜nonero-Candela et al., editors, MLCW 2005, LNAI Volume 3944, Jan 2006, Pages 177 - 190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Herrera</author>
<author>A Pe˜nas</author>
<author>F Verdejo</author>
</authors>
<title>Textual Entailment Recognition Based on Dependency Analysis and WordNet.</title>
<date>2006</date>
<booktitle>MLCW 2005, LNAI Volume 3944,</booktitle>
<pages>231--239</pages>
<editor>In Qui˜nonero-Candela et al., editors,</editor>
<marker>Herrera, Pe˜nas, Verdejo, 2006</marker>
<rawString>J. Herrera, A. Pe˜nas and F. Verdejo. 2006a. Textual Entailment Recognition Based on Dependency Analysis and WordNet. In Qui˜nonero-Candela et al., editors, MLCW 2005, LNAI Volume 3944, Jan 2006, Pages 231-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Herrera</author>
<author>A Pe˜nas</author>
<author>´A Rodrigo</author>
<author>F Verdejo</author>
</authors>
<title>UNED at PASCAL RTE-2 Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>Herrera, Pe˜nas, Rodrigo, Verdejo, 2006</marker>
<rawString>J. Herrera, A. Pe˜nas, ´A. Rodrigo and F. Verdejo. 2006b. UNED at PASCAL RTE-2 Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levensthein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals.</title>
<date>1966</date>
<booktitle>In Soviet Physics - Doklady,</booktitle>
<volume>10</volume>
<pages>707710</pages>
<contexts>
<context position="5803" citStr="Levensthein, 1966" startWordPosition="898" endWordPosition="899"> the text string of E2. 2. A Time Expression T1 entails a Time Expression T2 if the time range of T1 is included in the time range of T2. 3. A numeric expression N1 entails a numeric expression N2 if the range associated to N2 encloses the range of N1. Some characters change in different expressions of the same named entity as, for example, in a proper noun with different wordings (e.g. Yasser, Yaser, Yasir). To detect the entailment in these situations, when the previous process fails, we implemented a modified entailment decision process taking into account the edit distance of Levenshtein (Levensthein, 1966). Thus, if two named entities differ in less than 20%, then we assume that exists an entailment relation between these named entities. However, this definition of named entities entailment does not support errors due to wrong named entities classification as we can see in Figure 1. The expression 1990 represents a year but it is recognised as a numeric expression in the hypothesis. However the same expression is recognised as a temporal expression in the text and, therefore, the expression in the hypothesis cannot be entailed by it according to the named entities entailment definition above. W</context>
</contexts>
<marker>Levensthein, 1966</marker>
<rawString>V. I. Levensthein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. In Soviet Physics - Doklady, volume 10, pages 707710, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR. Workshop on the Evaluation of Parsing Systems,</title>
<date>1998</date>
<location>Granada, Spain,</location>
<contexts>
<context position="4764" citStr="Lin, 1998" startWordPosition="718" endWordPosition="719">Ms. Minton left Australia in 1961 to pursue her studies in London.&lt;/t&gt; &lt;h&gt;Ms. Minton was born in Australia.&lt;/h&gt; &lt;/pair&gt; Figure 3: IE pairs with entailment between named entities but not between named entities relations. 90 • NER: the Freeling Named Entity Recogniser is also applied to recover the information needed by the named entity entailment module that is described in the following section. Numeric expressions, proper nouns and temporal expressions of each text and hypothesis are tagged. • Dependency analysis: a dependency tree of each text and hypothesis is obtained using Lin’s Minipar (Lin, 1998). 2.2 Entailment between named entities Once the named entities of the hypothesis and the text are detected, the next step is to determine the entailment relations between the named entities in the text and the named entities in the hypothesis. In (Rodrigo et al., 2007) the following entailment relations between named entities were defined: 1. A Proper Noun E1 entails a Proper Noun E2 if the text string of E1 contains the text string of E2. 2. A Time Expression T1 entails a Time Expression T2 if the time range of T1 is included in the time range of T2. 3. A numeric expression N1 entails a nume</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based Evaluation of MINIPAR. Workshop on the Evaluation of Parsing Systems, Granada, Spain, May, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pe˜nas</author>
<author>´A Rodrigo</author>
<author>V Sama</author>
<author>F Verdejo</author>
</authors>
<title>Overview of the Answer Validation Exercise</title>
<date>2007</date>
<booktitle>In Lecture Notes in Computer Science. In press.</booktitle>
<marker>Pe˜nas, Rodrigo, Sama, Verdejo, 2007</marker>
<rawString>A. Pe˜nas, ´A. Rodrigo, V. Sama and F. Verdejo. 2007. Overview of the Answer Validation Exercise 2006. In Lecture Notes in Computer Science. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´A Rodrigo</author>
<author>A Pe˜nas</author>
<author>J Herrera</author>
<author>F Verdejo</author>
</authors>
<title>The Effect of Entity Recognition on Answer Validation.</title>
<date>2007</date>
<booktitle>In Lecture Notes in Computer Science. In press.</booktitle>
<marker>Rodrigo, Pe˜nas, Herrera, Verdejo, 2007</marker>
<rawString>´A. Rodrigo, A. Pe˜nas, J. Herrera and F. Verdejo. 2007. The Effect of Entity Recognition on Answer Validation. In Lecture Notes in Computer Science. In press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>