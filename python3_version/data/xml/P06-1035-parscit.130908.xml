<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000701">
<title confidence="0.981753">
Measuring Language Divergence by Intra-Lexical Comparison
</title>
<author confidence="0.993045">
T. Mark Ellison Simon Kirby
</author>
<affiliation confidence="0.993964666666667">
Informatics Language Evolution and Computation Research Unit
University of Edinburgh Philosophy, Psychology and Language Sciences,
mark@markellison.net University of Edinburgh
</affiliation>
<email confidence="0.998171">
simon@ling.ed.ac.uk
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833733333333">
This paper presents a method for build-
ing genetic language taxonomies based
on a new approach to comparing lexi-
cal forms. Instead of comparing forms
cross-linguistically, a matrix of language-
internal similarities between forms is cal-
culated. These matrices are then com-
pared to give distances between languages.
We argue that this coheres better with
current thinking in linguistics and psy-
cholinguistics. An implementation of this
approach, called PHILOLOGICON, is de-
scribed, along with its application to Dyen
et al.’s (1992) ninety-five wordlists from
Indo-European languages.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874685185185">
Recently, there has been burgeoning interest in
the computational construction of genetic lan-
guage taxonomies (Dyen et al., 1992; Nerbonne
and Heeringa, 1997; Kondrak, 2002; Ringe et
al., 2002; Benedetto et al., 2002; McMahon
and McMahon, 2003; Gray and Atkinson, 2003;
Nakleh et al., 2005).
One common approach to building language
taxonomies is to ascribe language-language dis-
tances, and then use a generic algorithm to con-
struct a tree which explains these distances as
much as possible. Two questions arise with this
approach. The first asks what aspects of lan-
guages are important in measuring inter-language
distance. The second asks how to measure dis-
tance given these aspects.
A more traditional approach to building lan-
guage taxonomies (Dyen et al., 1992) answers
these questions in terms of cognates. A word in
language A is said to be cognate with word in lan-
guage B if the forms shared a common ancestor
in the parent language of A and B. In the cognate-
counting method, inter-language distance depends
on the lexical forms of the languages. The dis-
tance between two languages is a function of the
number or fraction of these forms which are cog-
nate between the two languages1. This approach
to building language taxonomies is hard to imple-
ment in toto because constructing ancestor forms
is not easily automatable.
More recent approaches, such as Kondrak’s
(2002) and Heggarty et al’s (2005) work on di-
alect comparison, take the synchronic word forms
themselves as the language aspect to be compared.
Variations on edit distance (see Kessler (2005) for
a survey) are then used to evaluate differences be-
tween languages for each word, and these differ-
ences are aggregated to give a distance between
languages or dialects as a whole. This approach
is largely automatable, although some methods do
require human intervention.
In this paper, we present novel answers to the
two questions. The features of language we will
compare are not sets of words or phonological
forms. Instead we compare the similarities be-
tween forms, expressed as confusion probabilities.
The distribution of confusion probabilities in one
language is called a lexical metric. Section 2
presents the definition of lexical metrics and some
arguments for their being good language represen-
tatives for the purposes of comparison.
The distance between two languages is the di-
vergence their lexical metrics. In section 3, we
detail two methods for measuring this divergence:
</bodyText>
<footnote confidence="0.949183333333333">
1McMahon and McMahon (2003) for an account of tree-
inference from the cognate percentages in the Dyen et al.
(1992) data.
</footnote>
<page confidence="0.970626">
273
</page>
<note confidence="0.5502">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 273–280,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9967895">
Kullback-Liebler (herafter KL) divergence and
Rao distance. The subsequent section (4) de-
scribes the application of our approach to automat-
ically constructing a taxonomy of Indo-European
languages from Dyen et al. (1992) data.
Section 5 suggests how lexical metrics can help
identify cognates. The final section (6) presents
our conclusions, and discusses possible future di-
rections for this work.
Versions of the software and data files described
in the paper will be made available to coincide
with its publication.
</bodyText>
<sectionHeader confidence="0.990315" genericHeader="method">
2 Lexical Metric
</sectionHeader>
<bodyText confidence="0.99996625">
The first question posed by the distance-based ap-
proach to genetic language taxonomy is: what
should we compare?
In some approaches (Kondrak, 2002; McMahon
et al., 2005; Heggarty et al., 2005; Nerbonne and
Heeringa, 1997), the answer to this question is that
we should compare the phonetic or phonological
realisations of a particular set of meanings across
the range of languages being studied. There are
a number of problems with using lexical forms in
this way.
Firstly, in order to compare forms from differ-
ent languages, we need to embed them in com-
mon phonetic space. This phonetic space provides
granularity, marking two phones as identical or
distinct, and where there is a graded measure of
phonetic distinction it measures this.
There is growing doubt in the field of phonol-
ogy and phonetics about the meaningfulness of as-
suming of a common phonetic space. Port and
Leary (2005) argue convincingly that this assump-
tion, while having played a fundamental role in
much recent linguistic theorising, is nevertheless
unfounded. The degree of difference between
sounds, and consequently, the degree of phonetic
difference between words can only be ascertained
within the context of a single language.
It may be argued that a common phonetic space
can be found in either acoustics or degrees of free-
dom in the speech articulators. Language-specific
categorisation of sound, however, often restruc-
tures this space, sometimes with distinct sounds
being treated as homophones. One example of
this is the realisation of orthographic rr in Euro-
pean Portuguese: it is indifferently realised with
an apical or a uvular trill, different sounds made at
distinct points of articulation.
If there is no language-independent, common
phonetic space with an equally common similar-
ity measure, there can be no principled approach
to comparing forms in one language with those of
another.
In contrast, language-specific word-similarity is
well-founded. A number of psycholinguistic mod-
els of spoken word recognition (Luce et al., 1990)
are based on the idea of lexical neighbourhoods.
When a word is accessed during processing, the
other words that are phonemically or orthograph-
ically similar are also activated. This effect can
be detected using experimental paradigms such as
priming.
Our approach, therefore, is to abandon the
cross-linguistic comparison of phonetic realisa-
tions, in favour of language-internal comparison
of forms. (See also work by Shillcock et al. (2001)
and Tamariz (2005)).
</bodyText>
<subsectionHeader confidence="0.992942">
2.1 Confusion probabilities
</subsectionHeader>
<bodyText confidence="0.99996747826087">
One psychologically well-grounded way of de-
scribing the similarity of words is in terms of their
confusion probabilities. Two words have high
confusion probability if it is likely that one word
could be produced or understood when the other
was intended. This type of confusion can be mea-
sured experimentally by giving subjects words in
noisy environments and measuring what they ap-
prehend.
A less pathological way in which confusion
probability is realised is in coactivation. If a per-
son hears a word, then they more easily and more
quickly recognise similar words. This coactiva-
tion occurs because the phonological realisation
of words is not completely separate in the mind.
Instead, realisations are interdependent with reali-
sations of similar words.
We propose that confusion probabilities are
ideal information to constitute the lexical met-
ric. They are language-specific, psychologically
grounded, can be determined by experiment, and
integrate with existing psycholinguistic models of
word recognition.
</bodyText>
<subsectionHeader confidence="0.990802">
2.2 NAM and beyond
</subsectionHeader>
<bodyText confidence="0.999814833333333">
Unfortunately, experimentally determined confu-
sion probabilities for a large number of languages
are not available. Fortunately, models of spoken
word recognition allow us to predict these proba-
bilities from easily-computable measures of word
similarity.
</bodyText>
<page confidence="0.995923">
274
</page>
<bodyText confidence="0.999819733333333">
For example, the neighbourhood activation
model (NAM) (Luce et al., 1990; Luce and Pisoni,
1998) predicts confusion probabilities from the
relative frequency of words in the neighbourhood
of the target. Words are in the neighbourhood of
the target if their Levenstein (1965) edit distance
from the target is one. The more frequent the word
is, the greater its likelihood of replacing the target.
Bailey and Hahn (2001) argue, however, that the
all-or-nothing nature of the lexical neighbourhood
is insufficient. Instead word similarity is the com-
plex function of frequency and phonetic similarity
shown in equation (1). Here A, B, C and D are
constants of the model, u and v are words, and d
is a phonetic similarity model.
</bodyText>
<equation confidence="0.998734">
s = (AF(u)2 + BF(u) + C)e−D.d(u,v) (1)
</equation>
<bodyText confidence="0.999623444444444">
We have adapted this model slightly, in line with
NAM, taking the similarity s to be the probabil-
ity of confusing stimulus v with form u. Also, as
our data usually offers no frequency information,
we have adopted the maximum entropy assump-
tion, namely, that all relative frequencies are equal.
Consequently, the probability of confusion of two
words depends solely on their similarity distance.
While this assumption degrades the psychological
reality of the model, it does not render it useless, as
the similarity measure continues to provide impor-
tant distinctions in neighbourhood confusability.
We also assume for simplicity, that the constant
D has the value 1.
With these simplifications, equation (2) shows
the probability of apprehending word w, out of
a set W of possible alternatives, given a stimulus
word ws.
</bodyText>
<equation confidence="0.998618">
P(wlws) = e−d(w,ws)�N(ws) (2)
</equation>
<bodyText confidence="0.525762666666667">
The normalising constant N(s) is the sum of the
non-normalised values for e−d(w,ws) for all words
w.
</bodyText>
<equation confidence="0.982778">
�N(ws) = e−d(u,v)
w∈W
</equation>
<subsectionHeader confidence="0.995075">
2.3 Scaled edit distances
</subsectionHeader>
<bodyText confidence="0.999892655172414">
Kidd and Watson (1992) have shown that discrim-
inability of frequency and of duration of tones in
a tone sequence depends on its length as a pro-
portion of the length of the sequence. Kapatsinski
(2006) uses this, with other evidence, to argue that
word recognition edit distances must be scaled by
word-length.
There are other reasons for coming to the same
conclusion. The simple Levenstein distance exag-
gerates the disparity between long words in com-
parison with short words. A word of consisting of
10 symbols, purely by virtue of its length, will on
average be marked as more different from other
words than a word of length two. For example,
Levenstein distance between interested and rest is
six, the same as the distance between rest and by,
even though the latter two have nothing in com-
mon. As a consequence, close phonetic transcrip-
tions, which by their very nature are likely to in-
volve more symbols per word, will result in larger
edit distances than broad phonemic transcriptions
of the same data.
To alleviate this problem, we define a new edit
distance function d2 which scales Levenstein dis-
tances by the average length of the words being
compared (see equation 3). Now the distance be-
tween interested and rest is 0.86, while that be-
tween rest and by is 2.0, reflecting the greater rel-
ative difference in the second pair.
</bodyText>
<equation confidence="0.988943">
2d(w2, w1)
d2(w2,w1) = (3)
|w1 |+ |w2|
</equation>
<bodyText confidence="0.999988777777778">
Note that by scaling the raw edit distance with
the average lengths of the words, we are preserv-
ing the symmetric property of the distance mea-
sure.
There are other methods of comparing strings,
for example string kernels (Shawe-Taylor and
Cristianini, 2004), but using Levenstein distance
keeps us coherent with the psycholinguistic ac-
counts of word similarity.
</bodyText>
<subsectionHeader confidence="0.997638">
2.4 Lexical Metric
</subsectionHeader>
<bodyText confidence="0.999985214285714">
Bringing this all together, we can define the lexical
metric.
A lexicon L is a mapping from a set of mean-
ings M, such as “DOG”, “TO RUN”, “GREEN”,
etc., onto a set F of forms such as /pies/, /biec/,
/zielony/.
The confusion probability P of m1 for m2 in
lexical L is the normalised negative exponential
of the scaled edit-distance of the corresponding
forms. It is worth noting that when frequencies
are assumed to follow the maximum entropy dis-
tribution, this connection between confusion prob-
abilities and distances (see equation 4) is the same
as that proposed by Shepard (1987).
</bodyText>
<page confidence="0.94604">
275
</page>
<equation confidence="0.7815935">
e
P(m1|m2; L) =
</equation>
<bodyText confidence="0.836596">
A lexical metric of L is the mapping LM(L) :
M2 _* [0, 1] which assigns to each pair of mean-
ings m1, m2 the probability of confusing m1 for
m2, scaled by the frequency of m2.
</bodyText>
<equation confidence="0.99740825">
LM(L)(m1, m2)
= P(L(m1)|L(m2))P(m2)
e−d2(L(M1),L(M2))
N(m2; L)|M|
</equation>
<bodyText confidence="0.985304">
where N(m2; L) is the normalising function de-
fined in equation (5).
</bodyText>
<equation confidence="0.9518525">
N(m2; L) = E e−d2(L(M),L(M2)) (5)
M∈M
</equation>
<subsectionHeader confidence="0.996847">
3.1 Geometric paths
</subsectionHeader>
<bodyText confidence="0.999985428571429">
The geometric path between two distributions P
and Q is a conditional distribution R with a con-
tinuous parameter α such that at α = 0, the distri-
bution is P, and at α = 1 it is Q. This conditional
distribution is called the geometric because it con-
sists of normalised weighted geometric means of
the two defining distributions (equation 6).
</bodyText>
<equation confidence="0.97739">
R( ¯w|α) = P( ¯w)αQ( ¯w)1−α/k(α; P, Q) (6)
</equation>
<bodyText confidence="0.999641875">
The function k(α; P, Q) is a normaliser for the
conditional distribution, being the sum of the
weighted geometric means of values from P and
Q (equation 7). This value is known as the
Chernoff coefficient or Helliger path (Basseville,
1989). For brevity, the P, Q arguments to k will
be treated as implicit and not expressed in equa-
tions.
</bodyText>
<equation confidence="0.998607">
−d2(L(M1),L(M2))
N(m2; L) (4)
</equation>
<bodyText confidence="0.985910666666667">
Table 1 shows a minimal lexicon consisting only E P( ¯w)1−αQ( ¯w)α (7)
of the numbers one to five, and a corresponding k(α) =
lexical metric. The values in the lexical metric are ¯�∈W2
</bodyText>
<subsectionHeader confidence="0.995488">
3.2 Kullback-Liebler distance
</subsectionHeader>
<bodyText confidence="0.997183">
one two three four five
one 0.102 0.027 0.023 0.024 0.024
two 0.028 0.107 0.024 0.026 0.015
three 0.024 0.024 0.107 0.023 0.023
four 0.025 0.025 0.022 0.104 0.023
five 0.026 0.015 0.023 0.025 0.111
</bodyText>
<tableCaption confidence="0.93206">
Table 1: A lexical metric on a mini-lexicon con-
</tableCaption>
<bodyText confidence="0.979134125">
sisting of the numbers one to five.
inferred word confusion probabilities. The matrix
is normalised so that the sum of each row is 0.2,
ie. one-fifth for each of the five words, so the total
of the matrix is one. Note that the diagonal values
vary because the off-diagonal values in each row
vary, and consequently, so does the normalisation
for the row.
</bodyText>
<sectionHeader confidence="0.998696" genericHeader="method">
3 Language-Language Distance
</sectionHeader>
<bodyText confidence="0.997480071428572">
In the previous section, we introduced the lexi-
cal metric as the key measurable for comparing
languages. Since lexical metrics are probability
distributions, comparison of metrics means mea-
suring the difference between probability distri-
butions. To do this, we use two measures: the
symmetric Kullback-Liebler divergence (Jeffreys,
1946) and the Rao distance (Rao, 1949; Atkinson
and Mitchell, 1981; Micchelli and Noakes, 2005)
based on Fisher Information (Fisher, 1959). These
can be defined in terms the geometric path from
one distribution to another.
The first-order (equation 8) differential of the nor-
maliser with regard to α is of particular interest.
</bodyText>
<equation confidence="0.98795">
Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8)
¯�∈W2 Q( ¯w)
</equation>
<bodyText confidence="0.9847">
At α = 0, this value is the negative of the
Kullback-Liebler distance KL(P|Q) of Q with re-
gard to P (Basseville, 1989). At α = 1, it is the
Kullback-Liebler distance KL(Q|P) of P with re-
gard to Q. Jeffreys’ (1946) measure is a symmetri-
sation of KL distance, by averaging the commuta-
tions (equations 9,10).
</bodyText>
<equation confidence="0.9528548">
KL(Q|P) + KL(P|Q)
KL(P, Q) = (9)
2
k′(1) − k′(0) =(10)
2
</equation>
<subsectionHeader confidence="0.773882">
3.3 Rao distance
</subsectionHeader>
<bodyText confidence="0.928361666666667">
Rao distance depends on the second-order (equa-
tion 11) differential of the normaliser with regard
to α.
</bodyText>
<equation confidence="0.934883571428571">
Ek′′(α) = log2 Q( ¯w)
¯�∈W2 P ( ¯w)P( ¯w)1−αQ( ¯w)α
(11)
Fisher information is defined as in equation (12).
� ∂2 log P(y|x)
FI(P, x) = − P(y|x)dy (12)
∂x2
</equation>
<page confidence="0.983925">
276
</page>
<bodyText confidence="0.998469">
Equation (13) expresses Fisher information along
the path R from P to Q at point α using k and its
first two derivatives.
</bodyText>
<equation confidence="0.994535333333333">
k(α)k′′(α) − k′(α)2
FI(R, α) = (13)
k(α)2
</equation>
<bodyText confidence="0.998011333333333">
The Rao distance r(P, Q) along R can be approxi-
mated by the square root of the Fisher information
at the path’s midpoint α = 0.5.
</bodyText>
<equation confidence="0.983854">
r(P, Q) = k(0.5)k′′(0.5) − k′(0.5)2 (14)
k(0.5)2
</equation>
<subsectionHeader confidence="0.893646">
3.4 The PHILOLOGICON algorithm
</subsectionHeader>
<bodyText confidence="0.999901333333333">
Bringing these pieces together, the PHILOLOGI-
CON algorithm for measuring the divergence be-
tween two languages has the following steps:
</bodyText>
<listItem confidence="0.997881125">
1. determine their joint confusion probability
matrices, P and Q,
2. substitute these into equation (7), equation
(8) and equation (11) to calculate k(0),
k(0.5), k(1), k′(0.5), and k′′(0.5),
3. and put these into equation (10) and equation
(14) to calculate the KL and Rao distances
between between the languages.
</listItem>
<sectionHeader confidence="0.982804" genericHeader="method">
4 Indo-European
</sectionHeader>
<bodyText confidence="0.999933125">
The ideal data for reconstructing Indo-European
would be an accurate phonemic transcription of
words used to express specifically defined mean-
ings. Sadly, this kind of data is not readily avail-
able. However, as a stop-gap measure, we can
adopt the data that Dyen et al. collected to con-
struct a Indo-European taxonomy using the cog-
nate method.
</bodyText>
<subsectionHeader confidence="0.994736">
4.1 Dyen et al’s data
</subsectionHeader>
<bodyText confidence="0.999973375">
Dyen et al. (1992) collected 95 data sets, each pair-
ing a meaning from a Swadesh (1952)-like 200-
word list with its expression in the corresponding
language. The compilers annotated with data with
cognacy relations, as part of their own taxonomic
analysis of Indo-European.
There are problems with using Dyen’s data for
the purposes of the current paper. Firstly, the word
forms collected are not phonetic, phonological or
even full orthographic representations. As the au-
thors state, the forms are expressed in sufficient
detail to allow an interested reader acquainted with
the language in question to identify which word is
being expressed.
Secondly, many meanings offer alternative
forms, presumably corresponding to synonyms.
For a human analyst using the cognate approach,
this means that a language can participate in two
(or more) word-derivation systems. In preparing
this data for processing, we have consistently cho-
sen the first of any alternatives.
A further difficulty lies in the fact that many lan-
guages are not represented by the full 200 mean-
ings. Consequently, in comparing lexical metrics
from two data sets, we frequently need to restrict
the metrics to only those meanings expressed in
both the sets. This means that the KL divergence
or the Rao distance between two languages were
measured on lexical metrics cropped and rescaled
to the meanings common to both data-sets. In
most cases, this was still more than 190 words.
Despite these mismatches between Dyen et al.’s
data and our needs, it provides an testbed for the
PHILOLOGICON algorithm. Our reasoning being,
that if successful with this data, the method is rea-
sonably reliable. Data was extracted to language-
specific files, and preprocessed to clean up prob-
lems such as those described above. An additional
data-set was added with random data to act as an
outlier to root the tree.
</bodyText>
<subsectionHeader confidence="0.999209">
4.2 Processing the data
</subsectionHeader>
<bodyText confidence="0.9999675">
PHILOLOGICON software was then used to calcu-
late the lexical metrics corresponding to the indi-
vidual data files and to measure KL divergences
and Rao distances between them. The program
NEIGHBOR from the PHYLIP2 package was used
to construct trees from the results.
</bodyText>
<subsectionHeader confidence="0.999129">
4.3 The results
</subsectionHeader>
<bodyText confidence="0.997914583333333">
The tree based on Rao distances is shown in figure
1. The discussion follows this tree except in those
few cases mentioning differences in the KL tree.
The standard against which we measure the suc-
cess of our trees is the conservative traditional tax-
onomy to be found in the Ethnologue (Grimes
and Grimes, 2000). The fit with this taxonomy
was so good that we have labelled the major
branches with their traditional names: Celtic, Ger-
manic, etc. In fact, in most cases, the branch-
internal divisions — eg. Brythonic/Goidelic in
Celtic, Western/Eastern/Southern in Slavic, or
</bodyText>
<footnote confidence="0.996121">
2See http://evolution.genetics.washington.edu/phylip.html.
</footnote>
<page confidence="0.996885">
277
</page>
<bodyText confidence="0.999712843137255">
Western/Northern in Germanic — also accord.
Note that PHILOLOGICON even groups Baltic and
Slavic together into a super-branch Balto-Slavic.
Where languages are clearly out of place in
comparison to the traditional taxonomy, these are
highlighted: visually in the tree, and verbally in
the following text. In almost every case, there are
obvious contact phenomena which explain the de-
viation from the standard taxonomy.
Armenian was grouped with the Indo-Iranian
languages. Interestingly, Armenian was at first
thought to be an Iranian language, as it shares
much vocabulary with these languages. The com-
mon vocabulary is now thought to be the result
of borrowing, rather than common genetic origin.
In the KL tree, Armenian is placed outside of the
Indo-Iranian languages, except for Gypsy. On the
other hand, in this tree, Ossetic is placed as an
outlier of the Indian group, while its traditional
classification (and the Rao distance tree) puts it
among the Iranian languages. Gypsy is an Indian
language, related to Hindi. It has, however, been
surrounded by European languages for some cen-
turies. The effects of this influence is the likely
cause for it being classified as an outlier in the
Indo-Iranian family. A similar situation exists for
Slavic: one of the two lists that Dyen et al. of-
fer for Slovenian is classed as an outlier in Slavic,
rather than classifying it with the Southern Slavic
languages. The other Slovenian list is classified
correctly with Serbocroatian. It is possible that
the significant impact of Italian on Slovenian has
made it an outlier. In Germanic, it is English that
is the outlier. This may be due to the impact of the
English creole, Takitaki, on the hierarchy. This
language is closest to English, but is very distinct
from the rest of the Germanic languages. Another
misclassification also is the result of contact phe-
nomena. According to the Ethnologue, Sardinian
is Southern Romance, a separate branch from Ital-
ian or from Spanish. However, its constant contact
with Italian has influenced the language such that
it is classified here with Italian. We can offer no
explanation for why Wakhi ends up an outlier to
all the groups.
In conclusion, despite the noisy state of Dyen et
al.’s data (for our purposes), the PHILOLOGICON
generates a taxonomy close to that constructed us-
ing the traditional methods of historical linguis-
tics. Where it deviates, the deviation usually
points to identifiable contact between languages.
</bodyText>
<figureCaption confidence="0.996544666666667">
Figure 1: Taxonomy of 95 Indo-European data
sets and artificial outlier using PHILOLOGICON
and PHYLIP
</figureCaption>
<figure confidence="0.999874187919463">
Wakhi
Greek D G
Greek MD r
Greek ML e
Greek Mod e
Greek K k
Afghan
Waziri
Armenian List
Armenian Mod
Baluchi
Persian List
Tadzik
Ossetic
Bengali
Hindi
Lahnda
Panjabi ST
Gujarati
Marathi
Khaskura
Nepali List
Kashmiri
Singhalese
Gypsy Gk
ALBANIAN
Albanian G
Albanian C
Albanian K
Albanian Top
Albanian T
Bulgarian
BULGARIAN P
MACEDONIAN P
Macedonian
Serbocroatian
SERBOCROATIAN P
SLOVENIAN P
Byelorussian
BYELORUSSIAN P
Russian
RUSSIAN P
Ukrainian
UKRAINIAN P
Czech
CZECH P
Slovak
SLOVAK P
Czech E
Lusatian L
Lusatian U
Polish
POLISH P
I
n
d
o
−
I
r
a
n
i
a
n
A
l
b
a
n
i
a
n
Slovenian
Latvian
Lithuanian O
Lithuanian ST
Afrikaans
Dutch List
Flemish
Frisian
German ST
Penn Dutch
Danish
Riksmal
Swedish List
Swedish Up
Swedish VL
Faroese
Icelandic ST
G
e
r
m
a
n
i
c
English ST
Takitaki
Brazilian
Portuguese ST
Spanish
Catalan
Italian
Sardinian C
Sardinian L
Sardinian N
Ladin
French
Walloon
Provencal
French Creole C
French Creole D
Rumanian List
Vlach
Breton List
Breton ST
Breton SE
Welsh C
Welsh N
Irish A
Irish B
C
e
l
t
i
c
R
o
m
a
n
c
e
Random
B
a
l
t
o
−
S
l
a
v
i
c
</figure>
<page confidence="0.986475">
278
</page>
<sectionHeader confidence="0.98411" genericHeader="evaluation">
5 Reconstruction and Cognacy
</sectionHeader>
<bodyText confidence="0.9998966">
Subsection 3.1 described the construction of geo-
metric paths from one lexical metric to another.
This section describes how the synthetic lexical
metric at the midpoint of the path can indicate
which words are cognate between the two lan-
guages.
The synthetic lexical metric (equation 15) ap-
plies the formula for the geometric path equation
(6) to the lexical metrics equation (5) of the lan-
guages being compared, at the midpoint α = 0.5.
</bodyText>
<equation confidence="0.998945">
R1 (m1, m2) = V/P(m1 jm2)Q(m1 jm2) (15)
2 j�j�(12)
</equation>
<bodyText confidence="0.99998175862069">
If the words for m1 and m2 in both languages have
common origins in a parent language, then it is
reasonable to expect that their confusion probabil-
ities in both languages will be similar. Of course
different cognate pairs m1, m2 will have differing
values for R, but the confusion probabilities in P
and Q will be similar, and consequently, the rein-
force the variance.
If either m1 or m2, or both, is non-cognate, that
is, has been replaced by another arbitrary form
at some point in the history of either language,
then the P and Q for this pair will take indepen-
dently varying values. Consequently, the geomet-
ric mean of these values is likely to take a value
more closely bound to the average, than in the
purely cognate case.
Thus rows in the lexical metric with wider dy-
namic ranges are likely to correspond to cognate
words. Rows corresponding to non-cognates are
likely to have smaller dynamic ranges. The dy-
namic range can be measured by taking the Shan-
non information of the probabilities in the row.
Table 2 shows the most low- and high-
information rows from English and Swedish
(Dyen et al’s (1992) data). At the extremes of
low and high information, the words are invari-
ably cognate and non-cognate. Between these ex-
tremes, the division is not so clear cut, due to
chance effects in the data.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="conclusions">
6 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999880166666667">
In this paper, we have presented a distance-
based method, called PHILOLOGICON, that con-
structs genetic trees on the basis of lexica
from each language. The method only com-
pares words language-internally, where compari-
son seems both psychologically real and reliable,
</bodyText>
<table confidence="0.163840076923077">
English Swedish 104(h − h)
Low Information
we vi -1.30
here her -1.19
to sit sitta -1.14
to flow flyta -1.04
wide vid -0.97
:
scratch klosa 0.78
dirty smutsig 0.79
left (hand) vanster 0.84
because emedan 0.89
High Information
</table>
<tableCaption confidence="0.715211">
Table 2: Shannon information of confusion dis-
</tableCaption>
<bodyText confidence="0.994823166666667">
tributions in the reconstruction of English and
Swedish. Information levels are shown translated
so that the average is zero.
and never cross-linguistically, where comparison
is less well-founded. It uses measures founded
in information theory to compare the intra-lexical
differences.
The method successfully, if not perfectly, recre-
ated the phylogenetic tree of Indo-European lan-
guages on the basis of noisy data. In further work,
we plan to improve both the quantity and the qual-
ity of the data. Since most of the mis-placements
on the tree could be accounted for by contact phe-
nomena, it is possible that a network-drawing,
rather than tree-drawing, analysis would produce
better results.
Likewise, we plan to develop the method
for identifying cognates. The key improvement
needed is a way to distinguish indeterminate dis-
tances in reconstructed lexical metrics from deter-
minate but uniform ones. This may be achieved by
retaining information about the distribution of the
original values which were combined to form the
reconstructed metric.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923777777778">
C. Atkinson and A.F.S. Mitchell. 1981. Rao’s distance
measure. Sankhy¯a, 4:345–365.
Todd M. Bailey and Ulrike Hahn. 2001. Determinants
of wordlikeness: Phonotactics or lexical neighbor-
hoods? Journal ofMemory and Language, 44:568–
591.
Michle Basseville. 1989. Distance measures for signal
processing and pattern recognition. Signal Process-
ing, 18(4):349–369, December.
</reference>
<page confidence="0.976762">
279
</page>
<reference confidence="0.999143472527473">
D. Benedetto, E. Caglioti, and V. Loreto. 2002. Lan-
guage trees and zipping. Physical Review Letters,
88.
Isidore Dyen, Joseph B. Kruskal, and Paul Black.
1992. An indo-european classification: a lexicosta-
tistical experiment. Transactions of the American
Philosophical Society, 82(5).
R.A. Fisher. 1959. Statistical Methods and Scientific
Inference. Oliver and Boyd, London.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the ana-
tolian theory of indo-european origin. Nature,
426:435–439.
B.F. Grimes and J.E. Grimes, editors. 2000. Ethno-
logue: Languages of the World. SIL International,
14th edition.
Paul Heggarty, April McMahon, and Robert McMa-
hon, 2005. Perspectives on Variation, chapter From
phonetic similarity to dialect classification. Mouton
de Gruyter.
H. Jeffreys. 1946. An invariant form for the prior prob-
ability in estimation problems. Proc. Roy. Soc. A,
186:453–461.
Vsevolod Kapatsinski. 2006. Sound similarity rela-
tions in the mental lexicon: Modeling the lexicon as
a complex network. Technical Report 27, Indiana
University Speech Research Lab.
Brett Kessler. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243–260.
Gary R. Kidd and C.S. Watson. 1992. The
”proportion-of-the-total-duration rule for the dis-
crimination of auditory patterns. Journal of the
Acoustic Society ofAmerica, 92:3109–3118.
Grzegorz Kondrak. 2002. Algorithms for Language
Reconstruction. Ph.D. thesis, University of Toronto.
V.I. Levenstein. 1965. Binary codes capable of cor-
recting deletions, insertions and reversals. Doklady
Akademii Nauk SSSR, 163(4):845–848.
Paul Luce and D. Pisoni. 1998. Recognizing spoken
words: The neighborhood activation model. Ear
and Hearing, 19:1–36.
Paul Luce, D. Pisoni, and S. Goldinger, 1990. Cog-
nitive Models of Speech Perception: Psycholinguis-
tic and Computational Perspectives, chapter Simi-
larity neighborhoods of spoken words, pages 122–
147. MIT Press, Cambridge, MA.
April McMahon and Robert McMahon. 2003. Find-
ing families: quantitative methods in language clas-
sification. Transactions of the Philological Society,
101:7–55.
April McMahon, Paul Heggarty, Robert McMahon,
and Natalia Slaska. 2005. Swadesh sublists and the
benefits of borrowing: an andean case study. Trans-
actions of the Philological Society, 103(2):147–170.
Charles A. Micchelli and Lyle Noakes. 2005. Rao dis-
tances. Journal of Multivariate Analysis, 92(1):97–
115.
Luay Nakleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of
phylogenetic reconstruction methods on an ie
dataset. Transactions of the Philological Society,
103(2):171–192.
J. Nerbonne and W. Heeringa. 1997. Measuring
dialect distance phonetically. In Proceedings of
SIGPHON-97: 3rd Meeting of the ACL Special In-
terest Group in Computational Phonology.
B. Port and A. Leary. 2005. Against formal phonology.
Language, 81(4):927–964.
C.R. Rao. 1949. On the distance between two popula-
tions. Sankhy¯a, 9:246–248.
D. Ringe, Tandy Warnow, and A. Taylor. 2002. Indo-
european and computational cladistics. Transac-
tions of the Philological Society, 100(1):59–129.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
R.N. Shepard. 1987. Toward a universal law of gen-
eralization for physical science. Science, 237:1317–
1323.
Richard C. Shillcock, Simon Kirby, Scott McDonald,
and Chris Brew. 2001. Filled pauses and their status
in the mental lexicon. In Proceedings of the 2001
Conference of Disfluency in Spontaneous Speech,
pages 53–56.
M. Swadesh. 1952. Lexico-statistic dating of prehis-
toric ethnic contacts. Proceedings of the American
philosophical society, 96(4).
Monica Tamariz. 2005. Exploring the Adaptive Struc-
ture of the Mental Lexicon. Ph.D. thesis, University
of Edinburgh.
</reference>
<page confidence="0.997035">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704780">
<title confidence="0.999973">Measuring Language Divergence by Intra-Lexical Comparison</title>
<author confidence="0.999741">T Mark Ellison Simon Kirby</author>
<affiliation confidence="0.954002333333333">Informatics Language Evolution and Computation Research Unit University of Edinburgh Philosophy, Psychology and Language Sciences, of Edinburgh</affiliation>
<email confidence="0.998138">simon@ling.ed.ac.uk</email>
<abstract confidence="0.986264125">This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms. Instead of comparing forms cross-linguistically, a matrix of languageinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this called is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Atkinson</author>
<author>A F S Mitchell</author>
</authors>
<date>1981</date>
<booktitle>Rao’s distance measure. Sankhy¯a,</booktitle>
<pages>4--345</pages>
<contexts>
<context position="14488" citStr="Atkinson and Mitchell, 1981" startWordPosition="2328" endWordPosition="2331">ach of the five words, so the total of the matrix is one. Note that the diagonal values vary because the off-diagonal values in each row vary, and consequently, so does the normalisation for the row. 3 Language-Language Distance In the previous section, we introduced the lexical metric as the key measurable for comparing languages. Since lexical metrics are probability distributions, comparison of metrics means measuring the difference between probability distributions. To do this, we use two measures: the symmetric Kullback-Liebler divergence (Jeffreys, 1946) and the Rao distance (Rao, 1949; Atkinson and Mitchell, 1981; Micchelli and Noakes, 2005) based on Fisher Information (Fisher, 1959). These can be defined in terms the geometric path from one distribution to another. The first-order (equation 8) differential of the normaliser with regard to α is of particular interest. Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8) ¯�∈W2 Q( ¯w) At α = 0, this value is the negative of the Kullback-Liebler distance KL(P|Q) of Q with regard to P (Basseville, 1989). At α = 1, it is the Kullback-Liebler distance KL(Q|P) of P with regard to Q. Jeffreys’ (1946) measure is a symmetrisation of KL distance, by averaging the commutatio</context>
</contexts>
<marker>Atkinson, Mitchell, 1981</marker>
<rawString>C. Atkinson and A.F.S. Mitchell. 1981. Rao’s distance measure. Sankhy¯a, 4:345–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Todd M Bailey</author>
<author>Ulrike Hahn</author>
</authors>
<title>Determinants of wordlikeness: Phonotactics or lexical neighborhoods? Journal ofMemory and Language,</title>
<date>2001</date>
<pages>44--568</pages>
<contexts>
<context position="8421" citStr="Bailey and Hahn (2001)" startWordPosition="1296" endWordPosition="1299">abilities for a large number of languages are not available. Fortunately, models of spoken word recognition allow us to predict these probabilities from easily-computable measures of word similarity. 274 For example, the neighbourhood activation model (NAM) (Luce et al., 1990; Luce and Pisoni, 1998) predicts confusion probabilities from the relative frequency of words in the neighbourhood of the target. Words are in the neighbourhood of the target if their Levenstein (1965) edit distance from the target is one. The more frequent the word is, the greater its likelihood of replacing the target. Bailey and Hahn (2001) argue, however, that the all-or-nothing nature of the lexical neighbourhood is insufficient. Instead word similarity is the complex function of frequency and phonetic similarity shown in equation (1). Here A, B, C and D are constants of the model, u and v are words, and d is a phonetic similarity model. s = (AF(u)2 + BF(u) + C)e−D.d(u,v) (1) We have adapted this model slightly, in line with NAM, taking the similarity s to be the probability of confusing stimulus v with form u. Also, as our data usually offers no frequency information, we have adopted the maximum entropy assumption, namely, th</context>
</contexts>
<marker>Bailey, Hahn, 2001</marker>
<rawString>Todd M. Bailey and Ulrike Hahn. 2001. Determinants of wordlikeness: Phonotactics or lexical neighborhoods? Journal ofMemory and Language, 44:568– 591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michle Basseville</author>
</authors>
<title>Distance measures for signal processing and pattern recognition.</title>
<date>1989</date>
<journal>Signal Processing,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="13117" citStr="Basseville, 1989" startWordPosition="2102" endWordPosition="2103">metric path between two distributions P and Q is a conditional distribution R with a continuous parameter α such that at α = 0, the distribution is P, and at α = 1 it is Q. This conditional distribution is called the geometric because it consists of normalised weighted geometric means of the two defining distributions (equation 6). R( ¯w|α) = P( ¯w)αQ( ¯w)1−α/k(α; P, Q) (6) The function k(α; P, Q) is a normaliser for the conditional distribution, being the sum of the weighted geometric means of values from P and Q (equation 7). This value is known as the Chernoff coefficient or Helliger path (Basseville, 1989). For brevity, the P, Q arguments to k will be treated as implicit and not expressed in equations. −d2(L(M1),L(M2)) N(m2; L) (4) Table 1 shows a minimal lexicon consisting only E P( ¯w)1−αQ( ¯w)α (7) of the numbers one to five, and a corresponding k(α) = lexical metric. The values in the lexical metric are ¯�∈W2 3.2 Kullback-Liebler distance one two three four five one 0.102 0.027 0.023 0.024 0.024 two 0.028 0.107 0.024 0.026 0.015 three 0.024 0.024 0.107 0.023 0.023 four 0.025 0.025 0.022 0.104 0.023 five 0.026 0.015 0.023 0.025 0.111 Table 1: A lexical metric on a mini-lexicon consisting of </context>
<context position="14921" citStr="Basseville, 1989" startWordPosition="2406" endWordPosition="2407">ween probability distributions. To do this, we use two measures: the symmetric Kullback-Liebler divergence (Jeffreys, 1946) and the Rao distance (Rao, 1949; Atkinson and Mitchell, 1981; Micchelli and Noakes, 2005) based on Fisher Information (Fisher, 1959). These can be defined in terms the geometric path from one distribution to another. The first-order (equation 8) differential of the normaliser with regard to α is of particular interest. Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8) ¯�∈W2 Q( ¯w) At α = 0, this value is the negative of the Kullback-Liebler distance KL(P|Q) of Q with regard to P (Basseville, 1989). At α = 1, it is the Kullback-Liebler distance KL(Q|P) of P with regard to Q. Jeffreys’ (1946) measure is a symmetrisation of KL distance, by averaging the commutations (equations 9,10). KL(Q|P) + KL(P|Q) KL(P, Q) = (9) 2 k′(1) − k′(0) =(10) 2 3.3 Rao distance Rao distance depends on the second-order (equation 11) differential of the normaliser with regard to α. Ek′′(α) = log2 Q( ¯w) ¯�∈W2 P ( ¯w)P( ¯w)1−αQ( ¯w)α (11) Fisher information is defined as in equation (12). � ∂2 log P(y|x) FI(P, x) = − P(y|x)dy (12) ∂x2 276 Equation (13) expresses Fisher information along the path R from P to Q at </context>
</contexts>
<marker>Basseville, 1989</marker>
<rawString>Michle Basseville. 1989. Distance measures for signal processing and pattern recognition. Signal Processing, 18(4):349–369, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Benedetto</author>
<author>E Caglioti</author>
<author>V Loreto</author>
</authors>
<title>Language trees and zipping.</title>
<date>2002</date>
<journal>Physical Review Letters,</journal>
<volume>88</volume>
<contexts>
<context position="1102" citStr="Benedetto et al., 2002" startWordPosition="149" endWordPosition="152">geinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building language taxonomies (Dyen et al., 1992) answers these questions in terms of cognates. A wo</context>
</contexts>
<marker>Benedetto, Caglioti, Loreto, 2002</marker>
<rawString>D. Benedetto, E. Caglioti, and V. Loreto. 2002. Language trees and zipping. Physical Review Letters, 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isidore Dyen</author>
<author>Joseph B Kruskal</author>
<author>Paul Black</author>
</authors>
<title>An indo-european classification: a lexicostatistical experiment.</title>
<date>1992</date>
<journal>Transactions of the American Philosophical Society,</journal>
<volume>82</volume>
<issue>5</issue>
<contexts>
<context position="1014" citStr="Dyen et al., 1992" startWordPosition="135" endWordPosition="138"> lexical forms. Instead of comparing forms cross-linguistically, a matrix of languageinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building la</context>
<context position="3454" citStr="Dyen et al. (1992)" startWordPosition="534" endWordPosition="537">s of words or phonological forms. Instead we compare the similarities between forms, expressed as confusion probabilities. The distribution of confusion probabilities in one language is called a lexical metric. Section 2 presents the definition of lexical metrics and some arguments for their being good language representatives for the purposes of comparison. The distance between two languages is the divergence their lexical metrics. In section 3, we detail two methods for measuring this divergence: 1McMahon and McMahon (2003) for an account of treeinference from the cognate percentages in the Dyen et al. (1992) data. 273 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 273–280, Sydney, July 2006. c�2006 Association for Computational Linguistics Kullback-Liebler (herafter KL) divergence and Rao distance. The subsequent section (4) describes the application of our approach to automatically constructing a taxonomy of Indo-European languages from Dyen et al. (1992) data. Section 5 suggests how lexical metrics can help identify cognates. The final section (6) presents our conclusions, and discusses possible future directions for this </context>
<context position="16670" citStr="Dyen et al. (1992)" startWordPosition="2710" endWordPosition="2713">on (7), equation (8) and equation (11) to calculate k(0), k(0.5), k(1), k′(0.5), and k′′(0.5), 3. and put these into equation (10) and equation (14) to calculate the KL and Rao distances between between the languages. 4 Indo-European The ideal data for reconstructing Indo-European would be an accurate phonemic transcription of words used to express specifically defined meanings. Sadly, this kind of data is not readily available. However, as a stop-gap measure, we can adopt the data that Dyen et al. collected to construct a Indo-European taxonomy using the cognate method. 4.1 Dyen et al’s data Dyen et al. (1992) collected 95 data sets, each pairing a meaning from a Swadesh (1952)-like 200- word list with its expression in the corresponding language. The compilers annotated with data with cognacy relations, as part of their own taxonomic analysis of Indo-European. There are problems with using Dyen’s data for the purposes of the current paper. Firstly, the word forms collected are not phonetic, phonological or even full orthographic representations. As the authors state, the forms are expressed in sufficient detail to allow an interested reader acquainted with the language in question to identify whic</context>
</contexts>
<marker>Dyen, Kruskal, Black, 1992</marker>
<rawString>Isidore Dyen, Joseph B. Kruskal, and Paul Black. 1992. An indo-european classification: a lexicostatistical experiment. Transactions of the American Philosophical Society, 82(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Fisher</author>
</authors>
<title>Statistical Methods and Scientific Inference.</title>
<date>1959</date>
<publisher>Oliver and Boyd,</publisher>
<location>London.</location>
<contexts>
<context position="14560" citStr="Fisher, 1959" startWordPosition="2340" endWordPosition="2341">s vary because the off-diagonal values in each row vary, and consequently, so does the normalisation for the row. 3 Language-Language Distance In the previous section, we introduced the lexical metric as the key measurable for comparing languages. Since lexical metrics are probability distributions, comparison of metrics means measuring the difference between probability distributions. To do this, we use two measures: the symmetric Kullback-Liebler divergence (Jeffreys, 1946) and the Rao distance (Rao, 1949; Atkinson and Mitchell, 1981; Micchelli and Noakes, 2005) based on Fisher Information (Fisher, 1959). These can be defined in terms the geometric path from one distribution to another. The first-order (equation 8) differential of the normaliser with regard to α is of particular interest. Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8) ¯�∈W2 Q( ¯w) At α = 0, this value is the negative of the Kullback-Liebler distance KL(P|Q) of Q with regard to P (Basseville, 1989). At α = 1, it is the Kullback-Liebler distance KL(Q|P) of P with regard to Q. Jeffreys’ (1946) measure is a symmetrisation of KL distance, by averaging the commutations (equations 9,10). KL(Q|P) + KL(P|Q) KL(P, Q) = (9) 2 k′(1) − k′(0) =(</context>
</contexts>
<marker>Fisher, 1959</marker>
<rawString>R.A. Fisher. 1959. Statistical Methods and Scientific Inference. Oliver and Boyd, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell D Gray</author>
<author>Quentin D Atkinson</author>
</authors>
<title>Language-tree divergence times support the anatolian theory of indo-european origin.</title>
<date>2003</date>
<journal>Nature,</journal>
<pages>426--435</pages>
<contexts>
<context position="1154" citStr="Gray and Atkinson, 2003" startWordPosition="157" endWordPosition="160">. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building language taxonomies (Dyen et al., 1992) answers these questions in terms of cognates. A word in language A is said to be cognate with word in </context>
</contexts>
<marker>Gray, Atkinson, 2003</marker>
<rawString>Russell D. Gray and Quentin D. Atkinson. 2003. Language-tree divergence times support the anatolian theory of indo-european origin. Nature, 426:435–439.</rawString>
</citation>
<citation valid="true">
<date>2000</date>
<booktitle>Ethnologue: Languages of the World. SIL International, 14th edition.</booktitle>
<editor>B.F. Grimes and J.E. Grimes, editors.</editor>
<marker>2000</marker>
<rawString>B.F. Grimes and J.E. Grimes, editors. 2000. Ethnologue: Languages of the World. SIL International, 14th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Heggarty</author>
<author>April McMahon</author>
<author>Robert McMahon</author>
</authors>
<title>Perspectives on Variation, chapter From phonetic similarity to dialect classification. Mouton de Gruyter.</title>
<date>2005</date>
<contexts>
<context position="4388" citStr="Heggarty et al., 2005" startWordPosition="675" endWordPosition="678">ication of our approach to automatically constructing a taxonomy of Indo-European languages from Dyen et al. (1992) data. Section 5 suggests how lexical metrics can help identify cognates. The final section (6) presents our conclusions, and discusses possible future directions for this work. Versions of the software and data files described in the paper will be made available to coincide with its publication. 2 Lexical Metric The first question posed by the distance-based approach to genetic language taxonomy is: what should we compare? In some approaches (Kondrak, 2002; McMahon et al., 2005; Heggarty et al., 2005; Nerbonne and Heeringa, 1997), the answer to this question is that we should compare the phonetic or phonological realisations of a particular set of meanings across the range of languages being studied. There are a number of problems with using lexical forms in this way. Firstly, in order to compare forms from different languages, we need to embed them in common phonetic space. This phonetic space provides granularity, marking two phones as identical or distinct, and where there is a graded measure of phonetic distinction it measures this. There is growing doubt in the field of phonology and</context>
</contexts>
<marker>Heggarty, McMahon, McMahon, 2005</marker>
<rawString>Paul Heggarty, April McMahon, and Robert McMahon, 2005. Perspectives on Variation, chapter From phonetic similarity to dialect classification. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jeffreys</author>
</authors>
<title>An invariant form for the prior probability in estimation problems.</title>
<date>1946</date>
<booktitle>Proc. Roy. Soc. A,</booktitle>
<pages>186--453</pages>
<contexts>
<context position="14427" citStr="Jeffreys, 1946" startWordPosition="2320" endWordPosition="2321">t the sum of each row is 0.2, ie. one-fifth for each of the five words, so the total of the matrix is one. Note that the diagonal values vary because the off-diagonal values in each row vary, and consequently, so does the normalisation for the row. 3 Language-Language Distance In the previous section, we introduced the lexical metric as the key measurable for comparing languages. Since lexical metrics are probability distributions, comparison of metrics means measuring the difference between probability distributions. To do this, we use two measures: the symmetric Kullback-Liebler divergence (Jeffreys, 1946) and the Rao distance (Rao, 1949; Atkinson and Mitchell, 1981; Micchelli and Noakes, 2005) based on Fisher Information (Fisher, 1959). These can be defined in terms the geometric path from one distribution to another. The first-order (equation 8) differential of the normaliser with regard to α is of particular interest. Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8) ¯�∈W2 Q( ¯w) At α = 0, this value is the negative of the Kullback-Liebler distance KL(P|Q) of Q with regard to P (Basseville, 1989). At α = 1, it is the Kullback-Liebler distance KL(Q|P) of P with regard to Q. Jeffreys’ (1946) measure is</context>
</contexts>
<marker>Jeffreys, 1946</marker>
<rawString>H. Jeffreys. 1946. An invariant form for the prior probability in estimation problems. Proc. Roy. Soc. A, 186:453–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vsevolod Kapatsinski</author>
</authors>
<title>Sound similarity relations in the mental lexicon: Modeling the lexicon as a complex network.</title>
<date>2006</date>
<tech>Technical Report 27,</tech>
<institution>Indiana University Speech Research Lab.</institution>
<contexts>
<context position="9966" citStr="Kapatsinski (2006)" startWordPosition="1554" endWordPosition="1555">d confusability. We also assume for simplicity, that the constant D has the value 1. With these simplifications, equation (2) shows the probability of apprehending word w, out of a set W of possible alternatives, given a stimulus word ws. P(wlws) = e−d(w,ws)�N(ws) (2) The normalising constant N(s) is the sum of the non-normalised values for e−d(w,ws) for all words w. �N(ws) = e−d(u,v) w∈W 2.3 Scaled edit distances Kidd and Watson (1992) have shown that discriminability of frequency and of duration of tones in a tone sequence depends on its length as a proportion of the length of the sequence. Kapatsinski (2006) uses this, with other evidence, to argue that word recognition edit distances must be scaled by word-length. There are other reasons for coming to the same conclusion. The simple Levenstein distance exaggerates the disparity between long words in comparison with short words. A word of consisting of 10 symbols, purely by virtue of its length, will on average be marked as more different from other words than a word of length two. For example, Levenstein distance between interested and rest is six, the same as the distance between rest and by, even though the latter two have nothing in common. A</context>
</contexts>
<marker>Kapatsinski, 2006</marker>
<rawString>Vsevolod Kapatsinski. 2006. Sound similarity relations in the mental lexicon: Modeling the lexicon as a complex network. Technical Report 27, Indiana University Speech Research Lab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
</authors>
<title>Phonetic comparison algorithms.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="2447" citStr="Kessler (2005)" startWordPosition="375" endWordPosition="376">d B. In the cognatecounting method, inter-language distance depends on the lexical forms of the languages. The distance between two languages is a function of the number or fraction of these forms which are cognate between the two languages1. This approach to building language taxonomies is hard to implement in toto because constructing ancestor forms is not easily automatable. More recent approaches, such as Kondrak’s (2002) and Heggarty et al’s (2005) work on dialect comparison, take the synchronic word forms themselves as the language aspect to be compared. Variations on edit distance (see Kessler (2005) for a survey) are then used to evaluate differences between languages for each word, and these differences are aggregated to give a distance between languages or dialects as a whole. This approach is largely automatable, although some methods do require human intervention. In this paper, we present novel answers to the two questions. The features of language we will compare are not sets of words or phonological forms. Instead we compare the similarities between forms, expressed as confusion probabilities. The distribution of confusion probabilities in one language is called a lexical metric. </context>
</contexts>
<marker>Kessler, 2005</marker>
<rawString>Brett Kessler. 2005. Phonetic comparison algorithms. Transactions of the Philological Society, 103(2):243–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary R Kidd</author>
<author>C S Watson</author>
</authors>
<title>The ”proportion-of-the-total-duration rule for the discrimination of auditory patterns.</title>
<date>1992</date>
<journal>Journal of the Acoustic Society ofAmerica,</journal>
<pages>92--3109</pages>
<contexts>
<context position="9788" citStr="Kidd and Watson (1992)" startWordPosition="1520" endWordPosition="1523"> this assumption degrades the psychological reality of the model, it does not render it useless, as the similarity measure continues to provide important distinctions in neighbourhood confusability. We also assume for simplicity, that the constant D has the value 1. With these simplifications, equation (2) shows the probability of apprehending word w, out of a set W of possible alternatives, given a stimulus word ws. P(wlws) = e−d(w,ws)�N(ws) (2) The normalising constant N(s) is the sum of the non-normalised values for e−d(w,ws) for all words w. �N(ws) = e−d(u,v) w∈W 2.3 Scaled edit distances Kidd and Watson (1992) have shown that discriminability of frequency and of duration of tones in a tone sequence depends on its length as a proportion of the length of the sequence. Kapatsinski (2006) uses this, with other evidence, to argue that word recognition edit distances must be scaled by word-length. There are other reasons for coming to the same conclusion. The simple Levenstein distance exaggerates the disparity between long words in comparison with short words. A word of consisting of 10 symbols, purely by virtue of its length, will on average be marked as more different from other words than a word of l</context>
</contexts>
<marker>Kidd, Watson, 1992</marker>
<rawString>Gary R. Kidd and C.S. Watson. 1992. The ”proportion-of-the-total-duration rule for the discrimination of auditory patterns. Journal of the Acoustic Society ofAmerica, 92:3109–3118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Algorithms for Language Reconstruction.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="1058" citStr="Kondrak, 2002" startWordPosition="143" endWordPosition="144">-linguistically, a matrix of languageinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building language taxonomies (Dyen et al., 1992) answer</context>
<context position="4343" citStr="Kondrak, 2002" startWordPosition="669" endWordPosition="670">equent section (4) describes the application of our approach to automatically constructing a taxonomy of Indo-European languages from Dyen et al. (1992) data. Section 5 suggests how lexical metrics can help identify cognates. The final section (6) presents our conclusions, and discusses possible future directions for this work. Versions of the software and data files described in the paper will be made available to coincide with its publication. 2 Lexical Metric The first question posed by the distance-based approach to genetic language taxonomy is: what should we compare? In some approaches (Kondrak, 2002; McMahon et al., 2005; Heggarty et al., 2005; Nerbonne and Heeringa, 1997), the answer to this question is that we should compare the phonetic or phonological realisations of a particular set of meanings across the range of languages being studied. There are a number of problems with using lexical forms in this way. Firstly, in order to compare forms from different languages, we need to embed them in common phonetic space. This phonetic space provides granularity, marking two phones as identical or distinct, and where there is a graded measure of phonetic distinction it measures this. There i</context>
</contexts>
<marker>Kondrak, 2002</marker>
<rawString>Grzegorz Kondrak. 2002. Algorithms for Language Reconstruction. Ph.D. thesis, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenstein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1965</date>
<booktitle>Doklady Akademii Nauk SSSR,</booktitle>
<pages>163--4</pages>
<contexts>
<context position="8277" citStr="Levenstein (1965)" startWordPosition="1273" endWordPosition="1274">grate with existing psycholinguistic models of word recognition. 2.2 NAM and beyond Unfortunately, experimentally determined confusion probabilities for a large number of languages are not available. Fortunately, models of spoken word recognition allow us to predict these probabilities from easily-computable measures of word similarity. 274 For example, the neighbourhood activation model (NAM) (Luce et al., 1990; Luce and Pisoni, 1998) predicts confusion probabilities from the relative frequency of words in the neighbourhood of the target. Words are in the neighbourhood of the target if their Levenstein (1965) edit distance from the target is one. The more frequent the word is, the greater its likelihood of replacing the target. Bailey and Hahn (2001) argue, however, that the all-or-nothing nature of the lexical neighbourhood is insufficient. Instead word similarity is the complex function of frequency and phonetic similarity shown in equation (1). Here A, B, C and D are constants of the model, u and v are words, and d is a phonetic similarity model. s = (AF(u)2 + BF(u) + C)e−D.d(u,v) (1) We have adapted this model slightly, in line with NAM, taking the similarity s to be the probability of confusi</context>
</contexts>
<marker>Levenstein, 1965</marker>
<rawString>V.I. Levenstein. 1965. Binary codes capable of correcting deletions, insertions and reversals. Doklady Akademii Nauk SSSR, 163(4):845–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Luce</author>
<author>D Pisoni</author>
</authors>
<title>Recognizing spoken words: The neighborhood activation model. Ear and Hearing,</title>
<date>1998</date>
<contexts>
<context position="8099" citStr="Luce and Pisoni, 1998" startWordPosition="1244" endWordPosition="1247">se that confusion probabilities are ideal information to constitute the lexical metric. They are language-specific, psychologically grounded, can be determined by experiment, and integrate with existing psycholinguistic models of word recognition. 2.2 NAM and beyond Unfortunately, experimentally determined confusion probabilities for a large number of languages are not available. Fortunately, models of spoken word recognition allow us to predict these probabilities from easily-computable measures of word similarity. 274 For example, the neighbourhood activation model (NAM) (Luce et al., 1990; Luce and Pisoni, 1998) predicts confusion probabilities from the relative frequency of words in the neighbourhood of the target. Words are in the neighbourhood of the target if their Levenstein (1965) edit distance from the target is one. The more frequent the word is, the greater its likelihood of replacing the target. Bailey and Hahn (2001) argue, however, that the all-or-nothing nature of the lexical neighbourhood is insufficient. Instead word similarity is the complex function of frequency and phonetic similarity shown in equation (1). Here A, B, C and D are constants of the model, u and v are words, and d is a</context>
</contexts>
<marker>Luce, Pisoni, 1998</marker>
<rawString>Paul Luce and D. Pisoni. 1998. Recognizing spoken words: The neighborhood activation model. Ear and Hearing, 19:1–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Luce</author>
<author>D Pisoni</author>
<author>S Goldinger</author>
</authors>
<title>Cognitive Models of Speech Perception: Psycholinguistic and Computational Perspectives, chapter Similarity neighborhoods of spoken words,</title>
<date>1990</date>
<pages>122--147</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6220" citStr="Luce et al., 1990" startWordPosition="964" endWordPosition="967"> this space, sometimes with distinct sounds being treated as homophones. One example of this is the realisation of orthographic rr in European Portuguese: it is indifferently realised with an apical or a uvular trill, different sounds made at distinct points of articulation. If there is no language-independent, common phonetic space with an equally common similarity measure, there can be no principled approach to comparing forms in one language with those of another. In contrast, language-specific word-similarity is well-founded. A number of psycholinguistic models of spoken word recognition (Luce et al., 1990) are based on the idea of lexical neighbourhoods. When a word is accessed during processing, the other words that are phonemically or orthographically similar are also activated. This effect can be detected using experimental paradigms such as priming. Our approach, therefore, is to abandon the cross-linguistic comparison of phonetic realisations, in favour of language-internal comparison of forms. (See also work by Shillcock et al. (2001) and Tamariz (2005)). 2.1 Confusion probabilities One psychologically well-grounded way of describing the similarity of words is in terms of their confusion </context>
<context position="8075" citStr="Luce et al., 1990" startWordPosition="1240" endWordPosition="1243">lar words. We propose that confusion probabilities are ideal information to constitute the lexical metric. They are language-specific, psychologically grounded, can be determined by experiment, and integrate with existing psycholinguistic models of word recognition. 2.2 NAM and beyond Unfortunately, experimentally determined confusion probabilities for a large number of languages are not available. Fortunately, models of spoken word recognition allow us to predict these probabilities from easily-computable measures of word similarity. 274 For example, the neighbourhood activation model (NAM) (Luce et al., 1990; Luce and Pisoni, 1998) predicts confusion probabilities from the relative frequency of words in the neighbourhood of the target. Words are in the neighbourhood of the target if their Levenstein (1965) edit distance from the target is one. The more frequent the word is, the greater its likelihood of replacing the target. Bailey and Hahn (2001) argue, however, that the all-or-nothing nature of the lexical neighbourhood is insufficient. Instead word similarity is the complex function of frequency and phonetic similarity shown in equation (1). Here A, B, C and D are constants of the model, u and</context>
</contexts>
<marker>Luce, Pisoni, Goldinger, 1990</marker>
<rawString>Paul Luce, D. Pisoni, and S. Goldinger, 1990. Cognitive Models of Speech Perception: Psycholinguistic and Computational Perspectives, chapter Similarity neighborhoods of spoken words, pages 122– 147. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>April McMahon</author>
<author>Robert McMahon</author>
</authors>
<title>Finding families: quantitative methods in language classification.</title>
<date>2003</date>
<journal>Transactions of the Philological Society,</journal>
<pages>101--7</pages>
<contexts>
<context position="1129" citStr="McMahon and McMahon, 2003" startWordPosition="153" endWordPosition="156">between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building language taxonomies (Dyen et al., 1992) answers these questions in terms of cognates. A word in language A is said to</context>
<context position="3367" citStr="McMahon and McMahon (2003)" startWordPosition="518" endWordPosition="521">resent novel answers to the two questions. The features of language we will compare are not sets of words or phonological forms. Instead we compare the similarities between forms, expressed as confusion probabilities. The distribution of confusion probabilities in one language is called a lexical metric. Section 2 presents the definition of lexical metrics and some arguments for their being good language representatives for the purposes of comparison. The distance between two languages is the divergence their lexical metrics. In section 3, we detail two methods for measuring this divergence: 1McMahon and McMahon (2003) for an account of treeinference from the cognate percentages in the Dyen et al. (1992) data. 273 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 273–280, Sydney, July 2006. c�2006 Association for Computational Linguistics Kullback-Liebler (herafter KL) divergence and Rao distance. The subsequent section (4) describes the application of our approach to automatically constructing a taxonomy of Indo-European languages from Dyen et al. (1992) data. Section 5 suggests how lexical metrics can help identify cognates. The final s</context>
</contexts>
<marker>McMahon, McMahon, 2003</marker>
<rawString>April McMahon and Robert McMahon. 2003. Finding families: quantitative methods in language classification. Transactions of the Philological Society, 101:7–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>April McMahon</author>
<author>Paul Heggarty</author>
<author>Robert McMahon</author>
<author>Natalia Slaska</author>
</authors>
<title>Swadesh sublists and the benefits of borrowing: an andean case study.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="4365" citStr="McMahon et al., 2005" startWordPosition="671" endWordPosition="674">(4) describes the application of our approach to automatically constructing a taxonomy of Indo-European languages from Dyen et al. (1992) data. Section 5 suggests how lexical metrics can help identify cognates. The final section (6) presents our conclusions, and discusses possible future directions for this work. Versions of the software and data files described in the paper will be made available to coincide with its publication. 2 Lexical Metric The first question posed by the distance-based approach to genetic language taxonomy is: what should we compare? In some approaches (Kondrak, 2002; McMahon et al., 2005; Heggarty et al., 2005; Nerbonne and Heeringa, 1997), the answer to this question is that we should compare the phonetic or phonological realisations of a particular set of meanings across the range of languages being studied. There are a number of problems with using lexical forms in this way. Firstly, in order to compare forms from different languages, we need to embed them in common phonetic space. This phonetic space provides granularity, marking two phones as identical or distinct, and where there is a graded measure of phonetic distinction it measures this. There is growing doubt in the</context>
</contexts>
<marker>McMahon, Heggarty, McMahon, Slaska, 2005</marker>
<rawString>April McMahon, Paul Heggarty, Robert McMahon, and Natalia Slaska. 2005. Swadesh sublists and the benefits of borrowing: an andean case study. Transactions of the Philological Society, 103(2):147–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles A Micchelli</author>
<author>Lyle Noakes</author>
</authors>
<title>Rao distances.</title>
<date>2005</date>
<journal>Journal of Multivariate Analysis,</journal>
<volume>92</volume>
<issue>1</issue>
<pages>115</pages>
<contexts>
<context position="14517" citStr="Micchelli and Noakes, 2005" startWordPosition="2332" endWordPosition="2335"> total of the matrix is one. Note that the diagonal values vary because the off-diagonal values in each row vary, and consequently, so does the normalisation for the row. 3 Language-Language Distance In the previous section, we introduced the lexical metric as the key measurable for comparing languages. Since lexical metrics are probability distributions, comparison of metrics means measuring the difference between probability distributions. To do this, we use two measures: the symmetric Kullback-Liebler divergence (Jeffreys, 1946) and the Rao distance (Rao, 1949; Atkinson and Mitchell, 1981; Micchelli and Noakes, 2005) based on Fisher Information (Fisher, 1959). These can be defined in terms the geometric path from one distribution to another. The first-order (equation 8) differential of the normaliser with regard to α is of particular interest. Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8) ¯�∈W2 Q( ¯w) At α = 0, this value is the negative of the Kullback-Liebler distance KL(P|Q) of Q with regard to P (Basseville, 1989). At α = 1, it is the Kullback-Liebler distance KL(Q|P) of P with regard to Q. Jeffreys’ (1946) measure is a symmetrisation of KL distance, by averaging the commutations (equations 9,10). KL(Q|P) </context>
</contexts>
<marker>Micchelli, Noakes, 2005</marker>
<rawString>Charles A. Micchelli and Lyle Noakes. 2005. Rao distances. Journal of Multivariate Analysis, 92(1):97– 115.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Luay Nakleh</author>
</authors>
<location>Tandy Warnow, Don Ringe, and</location>
<marker>Nakleh, </marker>
<rawString>Luay Nakleh, Tandy Warnow, Don Ringe, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven N Evans</author>
</authors>
<title>A comparison of phylogenetic reconstruction methods on an ie dataset.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<marker>Evans, 2005</marker>
<rawString>Steven N. Evans. 2005. A comparison of phylogenetic reconstruction methods on an ie dataset. Transactions of the Philological Society, 103(2):171–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nerbonne</author>
<author>W Heeringa</author>
</authors>
<title>Measuring dialect distance phonetically.</title>
<date>1997</date>
<booktitle>In Proceedings of SIGPHON-97: 3rd Meeting of the ACL Special Interest Group in Computational Phonology.</booktitle>
<contexts>
<context position="1043" citStr="Nerbonne and Heeringa, 1997" startWordPosition="139" endWordPosition="142">tead of comparing forms cross-linguistically, a matrix of languageinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building language taxonomies (Dyen et al</context>
<context position="4418" citStr="Nerbonne and Heeringa, 1997" startWordPosition="679" endWordPosition="682"> to automatically constructing a taxonomy of Indo-European languages from Dyen et al. (1992) data. Section 5 suggests how lexical metrics can help identify cognates. The final section (6) presents our conclusions, and discusses possible future directions for this work. Versions of the software and data files described in the paper will be made available to coincide with its publication. 2 Lexical Metric The first question posed by the distance-based approach to genetic language taxonomy is: what should we compare? In some approaches (Kondrak, 2002; McMahon et al., 2005; Heggarty et al., 2005; Nerbonne and Heeringa, 1997), the answer to this question is that we should compare the phonetic or phonological realisations of a particular set of meanings across the range of languages being studied. There are a number of problems with using lexical forms in this way. Firstly, in order to compare forms from different languages, we need to embed them in common phonetic space. This phonetic space provides granularity, marking two phones as identical or distinct, and where there is a graded measure of phonetic distinction it measures this. There is growing doubt in the field of phonology and phonetics about the meaningfu</context>
</contexts>
<marker>Nerbonne, Heeringa, 1997</marker>
<rawString>J. Nerbonne and W. Heeringa. 1997. Measuring dialect distance phonetically. In Proceedings of SIGPHON-97: 3rd Meeting of the ACL Special Interest Group in Computational Phonology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Port</author>
<author>A Leary</author>
</authors>
<title>Against formal phonology.</title>
<date>2005</date>
<journal>Language,</journal>
<volume>81</volume>
<issue>4</issue>
<contexts>
<context position="5085" citStr="Port and Leary (2005)" startWordPosition="792" endWordPosition="795">d compare the phonetic or phonological realisations of a particular set of meanings across the range of languages being studied. There are a number of problems with using lexical forms in this way. Firstly, in order to compare forms from different languages, we need to embed them in common phonetic space. This phonetic space provides granularity, marking two phones as identical or distinct, and where there is a graded measure of phonetic distinction it measures this. There is growing doubt in the field of phonology and phonetics about the meaningfulness of assuming of a common phonetic space. Port and Leary (2005) argue convincingly that this assumption, while having played a fundamental role in much recent linguistic theorising, is nevertheless unfounded. The degree of difference between sounds, and consequently, the degree of phonetic difference between words can only be ascertained within the context of a single language. It may be argued that a common phonetic space can be found in either acoustics or degrees of freedom in the speech articulators. Language-specific categorisation of sound, however, often restructures this space, sometimes with distinct sounds being treated as homophones. One exampl</context>
</contexts>
<marker>Port, Leary, 2005</marker>
<rawString>B. Port and A. Leary. 2005. Against formal phonology. Language, 81(4):927–964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Rao</author>
</authors>
<title>On the distance between two populations.</title>
<date>1949</date>
<journal>Sankhy¯a,</journal>
<pages>9--246</pages>
<contexts>
<context position="14459" citStr="Rao, 1949" startWordPosition="2326" endWordPosition="2327">fifth for each of the five words, so the total of the matrix is one. Note that the diagonal values vary because the off-diagonal values in each row vary, and consequently, so does the normalisation for the row. 3 Language-Language Distance In the previous section, we introduced the lexical metric as the key measurable for comparing languages. Since lexical metrics are probability distributions, comparison of metrics means measuring the difference between probability distributions. To do this, we use two measures: the symmetric Kullback-Liebler divergence (Jeffreys, 1946) and the Rao distance (Rao, 1949; Atkinson and Mitchell, 1981; Micchelli and Noakes, 2005) based on Fisher Information (Fisher, 1959). These can be defined in terms the geometric path from one distribution to another. The first-order (equation 8) differential of the normaliser with regard to α is of particular interest. Ek′(α) = log P ( ¯w)P( ¯w)1−αQ( ¯w)α (8) ¯�∈W2 Q( ¯w) At α = 0, this value is the negative of the Kullback-Liebler distance KL(P|Q) of Q with regard to P (Basseville, 1989). At α = 1, it is the Kullback-Liebler distance KL(Q|P) of P with regard to Q. Jeffreys’ (1946) measure is a symmetrisation of KL distance</context>
</contexts>
<marker>Rao, 1949</marker>
<rawString>C.R. Rao. 1949. On the distance between two populations. Sankhy¯a, 9:246–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ringe</author>
<author>Tandy Warnow</author>
<author>A Taylor</author>
</authors>
<title>Indoeuropean and computational cladistics.</title>
<date>2002</date>
<journal>Transactions of the Philological Society,</journal>
<volume>100</volume>
<issue>1</issue>
<contexts>
<context position="1078" citStr="Ringe et al., 2002" startWordPosition="145" endWordPosition="148">, a matrix of languageinternal similarities between forms is calculated. These matrices are then compared to give distances between languages. We argue that this coheres better with current thinking in linguistics and psycholinguistics. An implementation of this approach, called PHILOLOGICON, is described, along with its application to Dyen et al.’s (1992) ninety-five wordlists from Indo-European languages. 1 Introduction Recently, there has been burgeoning interest in the computational construction of genetic language taxonomies (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ringe et al., 2002; Benedetto et al., 2002; McMahon and McMahon, 2003; Gray and Atkinson, 2003; Nakleh et al., 2005). One common approach to building language taxonomies is to ascribe language-language distances, and then use a generic algorithm to construct a tree which explains these distances as much as possible. Two questions arise with this approach. The first asks what aspects of languages are important in measuring inter-language distance. The second asks how to measure distance given these aspects. A more traditional approach to building language taxonomies (Dyen et al., 1992) answers these questions in</context>
</contexts>
<marker>Ringe, Warnow, Taylor, 2002</marker>
<rawString>D. Ringe, Tandy Warnow, and A. Taylor. 2002. Indoeuropean and computational cladistics. Transactions of the Philological Society, 100(1):59–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11399" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1798" endWordPosition="1801"> same data. To alleviate this problem, we define a new edit distance function d2 which scales Levenstein distances by the average length of the words being compared (see equation 3). Now the distance between interested and rest is 0.86, while that between rest and by is 2.0, reflecting the greater relative difference in the second pair. 2d(w2, w1) d2(w2,w1) = (3) |w1 |+ |w2| Note that by scaling the raw edit distance with the average lengths of the words, we are preserving the symmetric property of the distance measure. There are other methods of comparing strings, for example string kernels (Shawe-Taylor and Cristianini, 2004), but using Levenstein distance keeps us coherent with the psycholinguistic accounts of word similarity. 2.4 Lexical Metric Bringing this all together, we can define the lexical metric. A lexicon L is a mapping from a set of meanings M, such as “DOG”, “TO RUN”, “GREEN”, etc., onto a set F of forms such as /pies/, /biec/, /zielony/. The confusion probability P of m1 for m2 in lexical L is the normalised negative exponential of the scaled edit-distance of the corresponding forms. It is worth noting that when frequencies are assumed to follow the maximum entropy distribution, this connection betw</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R N Shepard</author>
</authors>
<title>Toward a universal law of generalization for physical science.</title>
<date>1987</date>
<journal>Science,</journal>
<volume>237</volume>
<pages>1323</pages>
<contexts>
<context position="12104" citStr="Shepard (1987)" startWordPosition="1919" endWordPosition="1920">d similarity. 2.4 Lexical Metric Bringing this all together, we can define the lexical metric. A lexicon L is a mapping from a set of meanings M, such as “DOG”, “TO RUN”, “GREEN”, etc., onto a set F of forms such as /pies/, /biec/, /zielony/. The confusion probability P of m1 for m2 in lexical L is the normalised negative exponential of the scaled edit-distance of the corresponding forms. It is worth noting that when frequencies are assumed to follow the maximum entropy distribution, this connection between confusion probabilities and distances (see equation 4) is the same as that proposed by Shepard (1987). 275 e P(m1|m2; L) = A lexical metric of L is the mapping LM(L) : M2 _* [0, 1] which assigns to each pair of meanings m1, m2 the probability of confusing m1 for m2, scaled by the frequency of m2. LM(L)(m1, m2) = P(L(m1)|L(m2))P(m2) e−d2(L(M1),L(M2)) N(m2; L)|M| where N(m2; L) is the normalising function defined in equation (5). N(m2; L) = E e−d2(L(M),L(M2)) (5) M∈M 3.1 Geometric paths The geometric path between two distributions P and Q is a conditional distribution R with a continuous parameter α such that at α = 0, the distribution is P, and at α = 1 it is Q. This conditional distribution i</context>
</contexts>
<marker>Shepard, 1987</marker>
<rawString>R.N. Shepard. 1987. Toward a universal law of generalization for physical science. Science, 237:1317– 1323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Shillcock</author>
<author>Simon Kirby</author>
<author>Scott McDonald</author>
<author>Chris Brew</author>
</authors>
<title>Filled pauses and their status in the mental lexicon.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference of Disfluency in Spontaneous Speech,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="6663" citStr="Shillcock et al. (2001)" startWordPosition="1031" endWordPosition="1034">e language with those of another. In contrast, language-specific word-similarity is well-founded. A number of psycholinguistic models of spoken word recognition (Luce et al., 1990) are based on the idea of lexical neighbourhoods. When a word is accessed during processing, the other words that are phonemically or orthographically similar are also activated. This effect can be detected using experimental paradigms such as priming. Our approach, therefore, is to abandon the cross-linguistic comparison of phonetic realisations, in favour of language-internal comparison of forms. (See also work by Shillcock et al. (2001) and Tamariz (2005)). 2.1 Confusion probabilities One psychologically well-grounded way of describing the similarity of words is in terms of their confusion probabilities. Two words have high confusion probability if it is likely that one word could be produced or understood when the other was intended. This type of confusion can be measured experimentally by giving subjects words in noisy environments and measuring what they apprehend. A less pathological way in which confusion probability is realised is in coactivation. If a person hears a word, then they more easily and more quickly recogni</context>
</contexts>
<marker>Shillcock, Kirby, McDonald, Brew, 2001</marker>
<rawString>Richard C. Shillcock, Simon Kirby, Scott McDonald, and Chris Brew. 2001. Filled pauses and their status in the mental lexicon. In Proceedings of the 2001 Conference of Disfluency in Spontaneous Speech, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swadesh</author>
</authors>
<title>Lexico-statistic dating of prehistoric ethnic contacts.</title>
<date>1952</date>
<booktitle>Proceedings of the American philosophical society,</booktitle>
<pages>96--4</pages>
<contexts>
<context position="16739" citStr="Swadesh (1952)" startWordPosition="2725" endWordPosition="2726">′(0.5), and k′′(0.5), 3. and put these into equation (10) and equation (14) to calculate the KL and Rao distances between between the languages. 4 Indo-European The ideal data for reconstructing Indo-European would be an accurate phonemic transcription of words used to express specifically defined meanings. Sadly, this kind of data is not readily available. However, as a stop-gap measure, we can adopt the data that Dyen et al. collected to construct a Indo-European taxonomy using the cognate method. 4.1 Dyen et al’s data Dyen et al. (1992) collected 95 data sets, each pairing a meaning from a Swadesh (1952)-like 200- word list with its expression in the corresponding language. The compilers annotated with data with cognacy relations, as part of their own taxonomic analysis of Indo-European. There are problems with using Dyen’s data for the purposes of the current paper. Firstly, the word forms collected are not phonetic, phonological or even full orthographic representations. As the authors state, the forms are expressed in sufficient detail to allow an interested reader acquainted with the language in question to identify which word is being expressed. Secondly, many meanings offer alternative </context>
</contexts>
<marker>Swadesh, 1952</marker>
<rawString>M. Swadesh. 1952. Lexico-statistic dating of prehistoric ethnic contacts. Proceedings of the American philosophical society, 96(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Tamariz</author>
</authors>
<title>Exploring the Adaptive Structure of the Mental Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6682" citStr="Tamariz (2005)" startWordPosition="1036" endWordPosition="1037">ther. In contrast, language-specific word-similarity is well-founded. A number of psycholinguistic models of spoken word recognition (Luce et al., 1990) are based on the idea of lexical neighbourhoods. When a word is accessed during processing, the other words that are phonemically or orthographically similar are also activated. This effect can be detected using experimental paradigms such as priming. Our approach, therefore, is to abandon the cross-linguistic comparison of phonetic realisations, in favour of language-internal comparison of forms. (See also work by Shillcock et al. (2001) and Tamariz (2005)). 2.1 Confusion probabilities One psychologically well-grounded way of describing the similarity of words is in terms of their confusion probabilities. Two words have high confusion probability if it is likely that one word could be produced or understood when the other was intended. This type of confusion can be measured experimentally by giving subjects words in noisy environments and measuring what they apprehend. A less pathological way in which confusion probability is realised is in coactivation. If a person hears a word, then they more easily and more quickly recognise similar words. T</context>
</contexts>
<marker>Tamariz, 2005</marker>
<rawString>Monica Tamariz. 2005. Exploring the Adaptive Structure of the Mental Lexicon. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>