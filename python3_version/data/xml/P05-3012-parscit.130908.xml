<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.071857">
<title confidence="0.998673">
Multimodal Generation in the COMIC Dialogue System
</title>
<author confidence="0.981475">
Mary Ellen Foster and Michael White
</author>
<affiliation confidence="0.9970485">
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.994339">
{M.E.Foster,Michael.White}@ed.ac.uk
</email>
<author confidence="0.996947">
Andrea Setzer and Roberta Catizone
</author>
<affiliation confidence="0.9938115">
Natural Language Processing Group
Department of Computer Science, University of Sheffield
</affiliation>
<email confidence="0.998901">
{A.Setzer,R.Catizone}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835666666667">
We describe how context-sensitive, user-
tailored output is specified and produced
in the COMIC multimodal dialogue sys-
tem. At the conference, we will demon-
strate the user-adapted features of the dia-
logue manager and text planner.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999584">
COMIC1 is an EU IST 5th Framework project com-
bining fundamental research on human-human inter-
action with advanced technology development for
multimodal conversational systems. The project
demonstrator system adds a dialogue interface to a
CAD-like application used in bathroom sales situa-
tions to help clients redesign their rooms. The input
to the system includes speech, handwriting, and pen
gestures; the output combines synthesised speech, a
talking head, and control of the underlying applica-
tion. Figure 1 shows screen shots of the COMIC
interface.
There are four main phases in the demonstra-
tor. First, the user specifies the shape of their
own bathroom, using a combination of speech in-
put, pen-gesture recognition and handwriting recog-
nition. Next, the user chooses a layout for the sani-
tary ware in the room. After that, the system guides
the user in browsing through a range of tiling op-
tions for the bathroom. Finally, the user is given a
</bodyText>
<footnote confidence="0.8096705">
1COnversational Multimodal Interaction with Computers;
http://www.hcrc.ed.ac.uk/comic/.
</footnote>
<bodyText confidence="0.999817206896552">
three-dimensional walkthrough of the finished bath-
room. We will focus on how context-sensitive, user-
tailored output is generated in the third, guided-
browsing phase of the interaction. Figure 2 shows
a typical user request and response from COMIC in
this phase. The pitch accents and multimodal ac-
tions are indicated; there is also facial emphasis cor-
responding to the accented words.
The primary goal of COMIC’s guided-browsing
phase is to help users become better informed about
the range of tiling options for their bathroom. In
this regard, it is similar to the web-based system
M-PIRO (Isard et al., 2003), which generates per-
sonalised descriptions of museum objects, and con-
trasts with task-oriented embodied dialogue systems
such as SmartKom (Wahlster, 2003). Since guided
browsing requires extended descriptions, in COMIC
we have placed greater emphasis on producing high-
quality adaptive output than have previous embodied
dialogue projects such as August (Gustafson et al.,
1999) and Rea (Cassell et al., 1999). To generate
its adaptive output, COMIC uses information from
the dialogue history and the user model throughout
the generation process, as in FLIGHTS (Moore et
al., 2004); both systems build upon earlier work on
adaptive content planning (Carenini, 2000; Walker
et al., 2002). An experimental study (Foster and
White, 2005) has shown that this adaptation is per-
ceptible to users of COMIC.
</bodyText>
<sectionHeader confidence="0.993145" genericHeader="method">
2 Dialogue Management
</sectionHeader>
<bodyText confidence="0.998536">
The task of the Dialogue and Action Manager
(DAM) is to decide what the system will show and
say in response to user input. The input to the
</bodyText>
<page confidence="0.993042">
45
</page>
<note confidence="0.577686">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
</note>
<page confidence="0.193841">
pages 45–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</page>
<figure confidence="0.794151">
(a) Bathroom-design application (b) Talking head
</figure>
<figureCaption confidence="0.99987">
Figure 1: Components of the COMIC interface
</figureCaption>
<figure confidence="0.96511275">
User Tell me about this design [click on AltMettlach]
COMIC [Look at screen]
THIS DESIGN is in the CLASSIC style.
[circle tiles]
As you can see, the colours are DARK RED and OFF WHITE.
[point at tiles]
The tiles are from the ALT METTLACH collection by VILLEROY AND BOCH.
[point at design name]
</figure>
<figureCaption confidence="0.999713">
Figure 2: Sample COMIC input and output
</figureCaption>
<bodyText confidence="0.99992">
DAM consists of multiple scored hypotheses con-
taining high-level, modality-independent specifica-
tions of the user input; the output is a similar high-
level specification of the system action. The DAM
itself is modality-independent. For example, the in-
put in Figure 2 could equally well have been the user
simply pointing to a design on the screen, with no
speech at all. This would have resulted in the same
abstract DAM input, and thus in the same output: a
request to show and describe the given design.
The COMIC DAM (Catizone et al., 2003) is
a general-purpose dialogue manager which can
handle different dialogue management styles such
as system-driven, user-driven or mixed-initiative.
The general-purpose part of the DAM is a sim-
ple stack architecture with a control structure;
all the application-dependent information is stored
in a variation of Augmented Transition Networks
(ATNs) called Dialogue Action Forms (DAFs).
These DAFs represent general dialogue moves, as
well as sub-tasks or topics, and are pushed onto and
popped off of the stack as the dialogue proceeds.
When processing a user input, the control struc-
ture decides whether the DAM can stay within the
current topic (and thus the current DAF), or whether
a topic shift has occurred. In the latter case, a new
DAF is pushed onto the stack and executed. After
that topic has been exhausted, the DAM returns to
the previous topic automatically. The same princi-
ple holds for error handling, which is implemented
at different levels in our approach.
In the guided-browsing phase of the COMIC sys-
tem, the user may browse tiling designs by colour,
style or manufacturer, look at designs in detail, or
change the amount of border and decoration tiles.
The DAM uses the system ontology to retrieve de-
signs according to the chosen feature, and consults
the user model and dialogue history to narrow down
the resulting designs to a small set to be shown and
described to the user.
</bodyText>
<page confidence="0.998879">
46
</page>
<sectionHeader confidence="0.988101" genericHeader="method">
3 Presentation Planning
</sectionHeader>
<bodyText confidence="0.999634108695652">
The COMIC fission module processes high-level
system-output specifications generated by the DAM.
For the example in Figure 2, the DAM output indi-
cates that the given tile design should be shown and
described, and that the description must mention the
style. The fission module fleshes out such specifica-
tions by selecting and structuring content, planning
the surface form of the text to realise that content,
choosing multimodal behaviours to accompany the
text, and controlling the output of the whole sched-
ule. In this section, we describe the planning pro-
cess; output coordination is dealt with in Section 6.
Full technical details of the fission module are given
in (Foster, 2005).
To create the textual content of a description, the
fission module proceeds as follows. First, it gath-
ers all of the properties of the specified design from
the system ontology. Next, it selects the properties
to include in the description, using information from
the dialogue history and the user model, along with
any properties specifically requested by the dialogue
manager. It then creates a structure for the selected
properties and creates logical forms as input for the
OpenCCG surface realiser. The logical forms may
include explicit alternatives in cases where there are
multiple ways of expressing a property; for exam-
ple, it could say either This design is in the classic
style or This design is classic. OpenCCG makes use
of statistical language models to choose among such
alternatives. This process is described in detail in
(Foster and White, 2004; Foster and White, 2005).
In addition to text, the output of COMIC
also incorporates multimodal behaviours including
prosodic specifications for the speech synthesiser
(pitch accents and boundary tones), facial behaviour
specifications (expressions and gaze shifts), and de-
ictic gestures at objects on the application screen us-
ing a simulated pointer. Pitch accents and bound-
ary tones are selected by the realiser based on the
context-sensitive information-structure annotations
(theme/rheme; marked/unmarked) included in the
logical forms. At the moment, the other multimodal
coarticulations are specified directly by the fission
module, but we are currently experimenting with
using the OpenCCG realiser’s language models to
choose them, using example-driven techniques.
</bodyText>
<sectionHeader confidence="0.977583" genericHeader="method">
4 Surface Realisation
</sectionHeader>
<bodyText confidence="0.999940541666667">
Surface realisation in COMIC is performed by the
OpenCCG2 realiser, a practical, open-source realiser
based on Combinatory Categorial Grammar (CCG)
(Steedman, 2000b). It employs a novel ensemble of
methods for improving the efficiency of CCG reali-
sation, and in particular, makes integrated use of n-
gram scoring of possible realisations in its chart re-
alisation algorithm (White, 2004; White, 2005). The
n-gram scoring allows the realiser to work in “any-
time” mode—able at any time to return the highest-
scoring complete realisation—and ensures that a
good realisation can be found reasonably quickly
even when the number of possibilities is exponen-
tial. This makes it particularly suited for use in an
interactive dialogue system such as COMIC.
In COMIC, the OpenCCG realiser uses factored
language models (Bilmes and Kirchhoff, 2003) over
words and multimodal coarticulations to select the
highest-scoring realisation licensed by the grammar
that satisfies the specification given by the fission
module. Steedman’s (Steedman, 2000a) theory of
information structure and intonation is used to con-
strain the choice of pitch accents and boundary tones
for the speech synthesiser.
</bodyText>
<sectionHeader confidence="0.944167" genericHeader="method">
5 Speech Synthesis
</sectionHeader>
<bodyText confidence="0.999979866666667">
The COMIC speech-synthesis module is imple-
mented as a client to the Festival speech-synthesis
system.3 We take advantage of recent advances in
version 2 of Festival (Clark et al., 2004) by using
a custom-built unit-selection voice with support for
APML prosodic annotation (de Carolis et al., 2004).
Experiments have shown that synthesised speech
with contextually appropriate prosodic features can
be perceptibly more natural (Baker et al., 2004).
Because the fission module needs the timing in-
formation from the speech synthesiser to finalise the
schedules for the other modalities, the synthesiser
first prepares and stores the waveform for its input
text; the sound is then played at a later time, when
the fission module indicates that it is required.
</bodyText>
<footnote confidence="0.9994585">
2http://openccg.sourceforge.net/
3http://www.cstr.ed.ac.uk/projects/festival/
</footnote>
<page confidence="0.999445">
47
</page>
<sectionHeader confidence="0.994806" genericHeader="method">
6 Output Coordination
</sectionHeader>
<bodyText confidence="0.999958428571429">
In addition to planning the presentation content as
described earlier, the fission module also controls
the system output to ensure that all parts of the pre-
sentation are properly coordinated, using the tim-
ing information returned by the speech synthesiser
to create a full schedule for the turn to be generated.
As described in (Foster, 2005), the fission module
allows multiple segments to be prepared in advance,
even while the preceding segments are being played.
This serves to minimise the output delay, as there is
no need to wait until a whole turn is fully prepared
before output begins, and the time taken to speak the
earlier parts of the turn can also be used to prepare
the later parts.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999892833333333">
This work was supported by the COMIC project
(IST-2001-32311). This paper describes only part
of the work done in the project; please see http://
www.hcrc.ed.ac.uk/comic/ for full details. We
thank the other members of COMIC for their col-
laboration during the course of the project.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99944156716418">
Rachel Baker, Robert A.J. Clark, and Michael White.
2004. Synthesizing contextually appropriate intona-
tion in limited domains. In Proceedings of 5th ISCA
workshop on speech synthesis.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proceedings ofHLT-03.
Giuseppe Carenini. 2000. Generating and Evaluating
Evaluative Arguments. Ph.D. thesis, Intelligent Sys-
tems Program, University of Pittsburgh.
Justine Cassell, Timothy Bickmore, Mark Billinghurst,
Lee Campbell, Kenny Chang, Hannes Vilhjilmsson,
and Hao Yan. 1999. Embodiment in conversational
interfaces: Rea. In Proceedings of CHI99.
Roberta Catizone, Andrea Setzer, and Yorick Wilks.
2003. Multimodal dialogue management in the
COMIC project. In Proceedings of EACL 2003 Work-
shop on Dialogue Systems: Interaction, adaptation,
and styles of management.
Robert A.J. Clark, Korin Richmond, and Simon King.
2004. Festival 2 – build your own general purpose
unit selection speech synthesiser. In Proceedings of
5th ISCA workshop on speech synthesis.
Berardina de Carolis, Catherine Pelachaud, Isabella
Poggi, and Mark Steedman. 2004. APML, a
mark-up language for believable behaviour generation.
In H Prendinger, editor, Life-like Characters, Tools,
Affective Functions and Applications, pages 65–85.
Springer.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for text planning with XSLT. In Proceedings
ofNLPXML-2004.
Mary Ellen Foster and Michael White. 2005. Assessing
the impact of adaptive generation in the COMIC multi-
modal dialogue system. In Proceedings ofIJCAI-2005
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems. To appear.
Mary Ellen Foster. 2005. Interleaved planning and out-
put in the COMIC fission module. Submitted.
Joakim Gustafson, Nikolaj Lindberg, and Magnus Lun-
deberg. 1999. The August spoken dialogue system.
In Proceedings of Eurospeech 1999.
Amy Isard, Jon Oberlander, Ion Androtsopoulos, and
Colin Matheson. 2003. Speaking the users’ lan-
guages. IEEE Intelligent Systems, 18(1):40–45.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, compara-
tive descriptions in spoken dialogue. In Proceedings
ofFLAIRS 2004.
Mark Steedman. 2000a. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649–689.
Mark Steedman. 2000b. The Syntactic Process. MIT
Press.
Wolfgang Wahlster. 2003. SmartKom: Symmetric mul-
timodality in an adaptive and reusable dialogue shell.
In Proceedings of the Human Computer Interaction
Status Conference 2003.
M.A. Walker, S. Whittaker, A. Stent, P. Maloor, J.D.
Moore, M. Johnston, and G. Vasireddy. 2002. Speech-
plans: Generating evaluative responses in spoken dia-
logue. In Proceedings ofINLG 2002.
Michael White. 2004. Reining in CCG chart realization.
In Proceedings ofINLG 2004.
Michael White. 2005. Efficient realization of coordinate
structures in Combinatory Categorial Grammar. Re-
search on Language and Computation. To appear.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469692">
<title confidence="0.999917">Multimodal Generation in the COMIC Dialogue System</title>
<author confidence="0.99989">Ellen Foster White</author>
<affiliation confidence="0.888317">Institute for Communicating and Collaborative Systems School of Informatics, University of Edinburgh Setzer Catizone Natural Language Processing Group Department of Computer Science, University of Sheffield</affiliation>
<abstract confidence="0.994642571428572">We describe how context-sensitive, usertailored output is specified and produced in the COMIC multimodal dialogue system. At the conference, we will demonstrate the user-adapted features of the dialogue manager and text planner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rachel Baker</author>
<author>Robert A J Clark</author>
<author>Michael White</author>
</authors>
<title>Synthesizing contextually appropriate intonation in limited domains.</title>
<date>2004</date>
<booktitle>In Proceedings of 5th ISCA workshop on speech synthesis.</booktitle>
<contexts>
<context position="9793" citStr="Baker et al., 2004" startWordPosition="1517" endWordPosition="1520">n, 2000a) theory of information structure and intonation is used to constrain the choice of pitch accents and boundary tones for the speech synthesiser. 5 Speech Synthesis The COMIC speech-synthesis module is implemented as a client to the Festival speech-synthesis system.3 We take advantage of recent advances in version 2 of Festival (Clark et al., 2004) by using a custom-built unit-selection voice with support for APML prosodic annotation (de Carolis et al., 2004). Experiments have shown that synthesised speech with contextually appropriate prosodic features can be perceptibly more natural (Baker et al., 2004). Because the fission module needs the timing information from the speech synthesiser to finalise the schedules for the other modalities, the synthesiser first prepares and stores the waveform for its input text; the sound is then played at a later time, when the fission module indicates that it is required. 2http://openccg.sourceforge.net/ 3http://www.cstr.ed.ac.uk/projects/festival/ 47 6 Output Coordination In addition to planning the presentation content as described earlier, the fission module also controls the system output to ensure that all parts of the presentation are properly coordin</context>
</contexts>
<marker>Baker, Clark, White, 2004</marker>
<rawString>Rachel Baker, Robert A.J. Clark, and Michael White. 2004. Synthesizing contextually appropriate intonation in limited domains. In Proceedings of 5th ISCA workshop on speech synthesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and general parallelized backoff.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-03.</booktitle>
<contexts>
<context position="8984" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="1398" endWordPosition="1401">or improving the efficiency of CCG realisation, and in particular, makes integrated use of ngram scoring of possible realisations in its chart realisation algorithm (White, 2004; White, 2005). The n-gram scoring allows the realiser to work in “anytime” mode—able at any time to return the highestscoring complete realisation—and ensures that a good realisation can be found reasonably quickly even when the number of possibilities is exponential. This makes it particularly suited for use in an interactive dialogue system such as COMIC. In COMIC, the OpenCCG realiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multimodal coarticulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module. Steedman’s (Steedman, 2000a) theory of information structure and intonation is used to constrain the choice of pitch accents and boundary tones for the speech synthesiser. 5 Speech Synthesis The COMIC speech-synthesis module is implemented as a client to the Festival speech-synthesis system.3 We take advantage of recent advances in version 2 of Festival (Clark et al., 2004) by using a custom-built unit-selection voice with su</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language models and general parallelized backoff. In Proceedings ofHLT-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
</authors>
<title>Generating and Evaluating Evaluative Arguments.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="2951" citStr="Carenini, 2000" startWordPosition="437" endWordPosition="438">eum objects, and contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 45–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics (a) Bathroom-design application (b) Talking head Figure 1: Components of the COMIC interface User Tell me about this design [click on AltMettlac</context>
</contexts>
<marker>Carenini, 2000</marker>
<rawString>Giuseppe Carenini. 2000. Generating and Evaluating Evaluative Arguments. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
<author>Timothy Bickmore</author>
<author>Mark Billinghurst</author>
<author>Lee Campbell</author>
<author>Kenny Chang</author>
<author>Hannes Vilhjilmsson</author>
<author>Hao Yan</author>
</authors>
<title>Embodiment in conversational interfaces: Rea.</title>
<date>1999</date>
<booktitle>In Proceedings of CHI99.</booktitle>
<contexts>
<context position="2696" citStr="Cassell et al., 1999" startWordPosition="396" endWordPosition="399">oal of COMIC’s guided-browsing phase is to help users become better informed about the range of tiling options for their bathroom. In this regard, it is similar to the web-based system M-PIRO (Isard et al., 2003), which generates personalised descriptions of museum objects, and contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster a</context>
</contexts>
<marker>Cassell, Bickmore, Billinghurst, Campbell, Chang, Vilhjilmsson, Yan, 1999</marker>
<rawString>Justine Cassell, Timothy Bickmore, Mark Billinghurst, Lee Campbell, Kenny Chang, Hannes Vilhjilmsson, and Hao Yan. 1999. Embodiment in conversational interfaces: Rea. In Proceedings of CHI99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberta Catizone</author>
<author>Andrea Setzer</author>
<author>Yorick Wilks</author>
</authors>
<title>Multimodal dialogue management in the COMIC project.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 2003 Workshop on Dialogue Systems: Interaction, adaptation, and styles of management.</booktitle>
<contexts>
<context position="4376" citStr="Catizone et al., 2003" startWordPosition="676" endWordPosition="679">LLEROY AND BOCH. [point at design name] Figure 2: Sample COMIC input and output DAM consists of multiple scored hypotheses containing high-level, modality-independent specifications of the user input; the output is a similar highlevel specification of the system action. The DAM itself is modality-independent. For example, the input in Figure 2 could equally well have been the user simply pointing to a design on the screen, with no speech at all. This would have resulted in the same abstract DAM input, and thus in the same output: a request to show and describe the given design. The COMIC DAM (Catizone et al., 2003) is a general-purpose dialogue manager which can handle different dialogue management styles such as system-driven, user-driven or mixed-initiative. The general-purpose part of the DAM is a simple stack architecture with a control structure; all the application-dependent information is stored in a variation of Augmented Transition Networks (ATNs) called Dialogue Action Forms (DAFs). These DAFs represent general dialogue moves, as well as sub-tasks or topics, and are pushed onto and popped off of the stack as the dialogue proceeds. When processing a user input, the control structure decides whe</context>
</contexts>
<marker>Catizone, Setzer, Wilks, 2003</marker>
<rawString>Roberta Catizone, Andrea Setzer, and Yorick Wilks. 2003. Multimodal dialogue management in the COMIC project. In Proceedings of EACL 2003 Workshop on Dialogue Systems: Interaction, adaptation, and styles of management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A J Clark</author>
<author>Korin Richmond</author>
<author>Simon King</author>
</authors>
<title>Festival 2 – build your own general purpose unit selection speech synthesiser.</title>
<date>2004</date>
<booktitle>In Proceedings of 5th ISCA workshop on speech synthesis.</booktitle>
<contexts>
<context position="9531" citStr="Clark et al., 2004" startWordPosition="1480" endWordPosition="1483">nCCG realiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multimodal coarticulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module. Steedman’s (Steedman, 2000a) theory of information structure and intonation is used to constrain the choice of pitch accents and boundary tones for the speech synthesiser. 5 Speech Synthesis The COMIC speech-synthesis module is implemented as a client to the Festival speech-synthesis system.3 We take advantage of recent advances in version 2 of Festival (Clark et al., 2004) by using a custom-built unit-selection voice with support for APML prosodic annotation (de Carolis et al., 2004). Experiments have shown that synthesised speech with contextually appropriate prosodic features can be perceptibly more natural (Baker et al., 2004). Because the fission module needs the timing information from the speech synthesiser to finalise the schedules for the other modalities, the synthesiser first prepares and stores the waveform for its input text; the sound is then played at a later time, when the fission module indicates that it is required. 2http://openccg.sourceforge.</context>
</contexts>
<marker>Clark, Richmond, King, 2004</marker>
<rawString>Robert A.J. Clark, Korin Richmond, and Simon King. 2004. Festival 2 – build your own general purpose unit selection speech synthesiser. In Proceedings of 5th ISCA workshop on speech synthesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berardina de Carolis</author>
<author>Catherine Pelachaud</author>
<author>Isabella Poggi</author>
<author>Mark Steedman</author>
</authors>
<title>APML, a mark-up language for believable behaviour generation.</title>
<date>2004</date>
<booktitle>Life-like Characters, Tools, Affective Functions and Applications,</booktitle>
<pages>65--85</pages>
<editor>In H Prendinger, editor,</editor>
<publisher>Springer.</publisher>
<marker>de Carolis, Pelachaud, Poggi, Steedman, 2004</marker>
<rawString>Berardina de Carolis, Catherine Pelachaud, Isabella Poggi, and Mark Steedman. 2004. APML, a mark-up language for believable behaviour generation. In H Prendinger, editor, Life-like Characters, Tools, Affective Functions and Applications, pages 65–85. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
<author>Michael White</author>
</authors>
<title>Techniques for text planning with XSLT.</title>
<date>2004</date>
<booktitle>In Proceedings ofNLPXML-2004.</booktitle>
<contexts>
<context position="7354" citStr="Foster and White, 2004" startWordPosition="1160" endWordPosition="1163">using information from the dialogue history and the user model, along with any properties specifically requested by the dialogue manager. It then creates a structure for the selected properties and creates logical forms as input for the OpenCCG surface realiser. The logical forms may include explicit alternatives in cases where there are multiple ways of expressing a property; for example, it could say either This design is in the classic style or This design is classic. OpenCCG makes use of statistical language models to choose among such alternatives. This process is described in detail in (Foster and White, 2004; Foster and White, 2005). In addition to text, the output of COMIC also incorporates multimodal behaviours including prosodic specifications for the speech synthesiser (pitch accents and boundary tones), facial behaviour specifications (expressions and gaze shifts), and deictic gestures at objects on the application screen using a simulated pointer. Pitch accents and boundary tones are selected by the realiser based on the context-sensitive information-structure annotations (theme/rheme; marked/unmarked) included in the logical forms. At the moment, the other multimodal coarticulations are sp</context>
</contexts>
<marker>Foster, White, 2004</marker>
<rawString>Mary Ellen Foster and Michael White. 2004. Techniques for text planning with XSLT. In Proceedings ofNLPXML-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
<author>Michael White</author>
</authors>
<title>Assessing the impact of adaptive generation in the COMIC multimodal dialogue system.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCAI-2005 Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<note>To appear.</note>
<contexts>
<context position="3021" citStr="Foster and White, 2005" startWordPosition="446" endWordPosition="449">ue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 45–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics (a) Bathroom-design application (b) Talking head Figure 1: Components of the COMIC interface User Tell me about this design [click on AltMettlach] COMIC [Look at screen] THIS DESIGN is in the CLASSIC style. [circle</context>
<context position="7379" citStr="Foster and White, 2005" startWordPosition="1164" endWordPosition="1167">he dialogue history and the user model, along with any properties specifically requested by the dialogue manager. It then creates a structure for the selected properties and creates logical forms as input for the OpenCCG surface realiser. The logical forms may include explicit alternatives in cases where there are multiple ways of expressing a property; for example, it could say either This design is in the classic style or This design is classic. OpenCCG makes use of statistical language models to choose among such alternatives. This process is described in detail in (Foster and White, 2004; Foster and White, 2005). In addition to text, the output of COMIC also incorporates multimodal behaviours including prosodic specifications for the speech synthesiser (pitch accents and boundary tones), facial behaviour specifications (expressions and gaze shifts), and deictic gestures at objects on the application screen using a simulated pointer. Pitch accents and boundary tones are selected by the realiser based on the context-sensitive information-structure annotations (theme/rheme; marked/unmarked) included in the logical forms. At the moment, the other multimodal coarticulations are specified directly by the f</context>
</contexts>
<marker>Foster, White, 2005</marker>
<rawString>Mary Ellen Foster and Michael White. 2005. Assessing the impact of adaptive generation in the COMIC multimodal dialogue system. In Proceedings ofIJCAI-2005 Workshop on Knowledge and Reasoning in Practical Dialogue Systems. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
</authors>
<title>Interleaved planning and output in the COMIC fission module.</title>
<date>2005</date>
<note>Submitted.</note>
<contexts>
<context position="6489" citStr="Foster, 2005" startWordPosition="1022" endWordPosition="1023">ns generated by the DAM. For the example in Figure 2, the DAM output indicates that the given tile design should be shown and described, and that the description must mention the style. The fission module fleshes out such specifications by selecting and structuring content, planning the surface form of the text to realise that content, choosing multimodal behaviours to accompany the text, and controlling the output of the whole schedule. In this section, we describe the planning process; output coordination is dealt with in Section 6. Full technical details of the fission module are given in (Foster, 2005). To create the textual content of a description, the fission module proceeds as follows. First, it gathers all of the properties of the specified design from the system ontology. Next, it selects the properties to include in the description, using information from the dialogue history and the user model, along with any properties specifically requested by the dialogue manager. It then creates a structure for the selected properties and creates logical forms as input for the OpenCCG surface realiser. The logical forms may include explicit alternatives in cases where there are multiple ways of </context>
<context position="10549" citStr="Foster, 2005" startWordPosition="1632" endWordPosition="1633">ynthesiser first prepares and stores the waveform for its input text; the sound is then played at a later time, when the fission module indicates that it is required. 2http://openccg.sourceforge.net/ 3http://www.cstr.ed.ac.uk/projects/festival/ 47 6 Output Coordination In addition to planning the presentation content as described earlier, the fission module also controls the system output to ensure that all parts of the presentation are properly coordinated, using the timing information returned by the speech synthesiser to create a full schedule for the turn to be generated. As described in (Foster, 2005), the fission module allows multiple segments to be prepared in advance, even while the preceding segments are being played. This serves to minimise the output delay, as there is no need to wait until a whole turn is fully prepared before output begins, and the time taken to speak the earlier parts of the turn can also be used to prepare the later parts. 7 Acknowledgements This work was supported by the COMIC project (IST-2001-32311). This paper describes only part of the work done in the project; please see http:// www.hcrc.ed.ac.uk/comic/ for full details. We thank the other members of COMIC</context>
</contexts>
<marker>Foster, 2005</marker>
<rawString>Mary Ellen Foster. 2005. Interleaved planning and output in the COMIC fission module. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Gustafson</author>
<author>Nikolaj Lindberg</author>
<author>Magnus Lundeberg</author>
</authors>
<title>The August spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<contexts>
<context position="2665" citStr="Gustafson et al., 1999" startWordPosition="390" endWordPosition="393">the accented words. The primary goal of COMIC’s guided-browsing phase is to help users become better informed about the range of tiling options for their bathroom. In this regard, it is similar to the web-based system M-PIRO (Isard et al., 2003), which generates personalised descriptions of museum objects, and contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings </context>
</contexts>
<marker>Gustafson, Lindberg, Lundeberg, 1999</marker>
<rawString>Joakim Gustafson, Nikolaj Lindberg, and Magnus Lundeberg. 1999. The August spoken dialogue system. In Proceedings of Eurospeech 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Isard</author>
<author>Jon Oberlander</author>
<author>Ion Androtsopoulos</author>
<author>Colin Matheson</author>
</authors>
<title>Speaking the users’ languages.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="2287" citStr="Isard et al., 2003" startWordPosition="337" endWordPosition="340">k/comic/. three-dimensional walkthrough of the finished bathroom. We will focus on how context-sensitive, usertailored output is generated in the third, guidedbrowsing phase of the interaction. Figure 2 shows a typical user request and response from COMIC in this phase. The pitch accents and multimodal actions are indicated; there is also facial emphasis corresponding to the accented words. The primary goal of COMIC’s guided-browsing phase is to help users become better informed about the range of tiling options for their bathroom. In this regard, it is similar to the web-based system M-PIRO (Isard et al., 2003), which generates personalised descriptions of museum objects, and contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems buil</context>
</contexts>
<marker>Isard, Oberlander, Androtsopoulos, Matheson, 2003</marker>
<rawString>Amy Isard, Jon Oberlander, Ion Androtsopoulos, and Colin Matheson. 2003. Speaking the users’ languages. IEEE Intelligent Systems, 18(1):40–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Mary Ellen Foster</author>
<author>Oliver Lemon</author>
<author>Michael White</author>
</authors>
<title>Generating tailored, comparative descriptions in spoken dialogue.</title>
<date>2004</date>
<booktitle>In Proceedings ofFLAIRS</booktitle>
<contexts>
<context position="2868" citStr="Moore et al., 2004" startWordPosition="423" endWordPosition="426">sed system M-PIRO (Isard et al., 2003), which generates personalised descriptions of museum objects, and contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 45–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics (a) Bathroom-design application (b) Talking head Figure 1: Co</context>
</contexts>
<marker>Moore, Foster, Lemon, White, 2004</marker>
<rawString>Johanna Moore, Mary Ellen Foster, Oliver Lemon, and Michael White. 2004. Generating tailored, comparative descriptions in spoken dialogue. In Proceedings ofFLAIRS 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Information structure and the syntax-phonology interface.</title>
<date>2000</date>
<journal>Linguistic Inquiry,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="8313" citStr="Steedman, 2000" startWordPosition="1293" endWordPosition="1294">ointer. Pitch accents and boundary tones are selected by the realiser based on the context-sensitive information-structure annotations (theme/rheme; marked/unmarked) included in the logical forms. At the moment, the other multimodal coarticulations are specified directly by the fission module, but we are currently experimenting with using the OpenCCG realiser’s language models to choose them, using example-driven techniques. 4 Surface Realisation Surface realisation in COMIC is performed by the OpenCCG2 realiser, a practical, open-source realiser based on Combinatory Categorial Grammar (CCG) (Steedman, 2000b). It employs a novel ensemble of methods for improving the efficiency of CCG realisation, and in particular, makes integrated use of ngram scoring of possible realisations in its chart realisation algorithm (White, 2004; White, 2005). The n-gram scoring allows the realiser to work in “anytime” mode—able at any time to return the highestscoring complete realisation—and ensures that a good realisation can be found reasonably quickly even when the number of possibilities is exponential. This makes it particularly suited for use in an interactive dialogue system such as COMIC. In COMIC, the Open</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000a. Information structure and the syntax-phonology interface. Linguistic Inquiry, 31(4):649–689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8313" citStr="Steedman, 2000" startWordPosition="1293" endWordPosition="1294">ointer. Pitch accents and boundary tones are selected by the realiser based on the context-sensitive information-structure annotations (theme/rheme; marked/unmarked) included in the logical forms. At the moment, the other multimodal coarticulations are specified directly by the fission module, but we are currently experimenting with using the OpenCCG realiser’s language models to choose them, using example-driven techniques. 4 Surface Realisation Surface realisation in COMIC is performed by the OpenCCG2 realiser, a practical, open-source realiser based on Combinatory Categorial Grammar (CCG) (Steedman, 2000b). It employs a novel ensemble of methods for improving the efficiency of CCG realisation, and in particular, makes integrated use of ngram scoring of possible realisations in its chart realisation algorithm (White, 2004; White, 2005). The n-gram scoring allows the realiser to work in “anytime” mode—able at any time to return the highestscoring complete realisation—and ensures that a good realisation can be found reasonably quickly even when the number of possibilities is exponential. This makes it particularly suited for use in an interactive dialogue system such as COMIC. In COMIC, the Open</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000b. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>SmartKom: Symmetric multimodality in an adaptive and reusable dialogue shell.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Computer Interaction Status Conference</booktitle>
<contexts>
<context position="2442" citStr="Wahlster, 2003" startWordPosition="360" endWordPosition="361">browsing phase of the interaction. Figure 2 shows a typical user request and response from COMIC in this phase. The pitch accents and multimodal actions are indicated; there is also facial emphasis corresponding to the accented words. The primary goal of COMIC’s guided-browsing phase is to help users become better informed about the range of tiling options for their bathroom. In this regard, it is similar to the web-based system M-PIRO (Isard et al., 2003), which generates personalised descriptions of museum objects, and contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this </context>
</contexts>
<marker>Wahlster, 2003</marker>
<rawString>Wolfgang Wahlster. 2003. SmartKom: Symmetric multimodality in an adaptive and reusable dialogue shell. In Proceedings of the Human Computer Interaction Status Conference 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>S Whittaker</author>
<author>A Stent</author>
<author>P Maloor</author>
<author>J D Moore</author>
<author>M Johnston</author>
<author>G Vasireddy</author>
</authors>
<title>Speechplans: Generating evaluative responses in spoken dialogue.</title>
<date>2002</date>
<booktitle>In Proceedings ofINLG</booktitle>
<contexts>
<context position="2973" citStr="Walker et al., 2002" startWordPosition="439" endWordPosition="442"> contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 45–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics (a) Bathroom-design application (b) Talking head Figure 1: Components of the COMIC interface User Tell me about this design [click on AltMettlach] COMIC [Look at scre</context>
</contexts>
<marker>Walker, Whittaker, Stent, Maloor, Moore, Johnston, Vasireddy, 2002</marker>
<rawString>M.A. Walker, S. Whittaker, A. Stent, P. Maloor, J.D. Moore, M. Johnston, and G. Vasireddy. 2002. Speechplans: Generating evaluative responses in spoken dialogue. In Proceedings ofINLG 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proceedings ofINLG</booktitle>
<contexts>
<context position="7354" citStr="White, 2004" startWordPosition="1162" endWordPosition="1163">mation from the dialogue history and the user model, along with any properties specifically requested by the dialogue manager. It then creates a structure for the selected properties and creates logical forms as input for the OpenCCG surface realiser. The logical forms may include explicit alternatives in cases where there are multiple ways of expressing a property; for example, it could say either This design is in the classic style or This design is classic. OpenCCG makes use of statistical language models to choose among such alternatives. This process is described in detail in (Foster and White, 2004; Foster and White, 2005). In addition to text, the output of COMIC also incorporates multimodal behaviours including prosodic specifications for the speech synthesiser (pitch accents and boundary tones), facial behaviour specifications (expressions and gaze shifts), and deictic gestures at objects on the application screen using a simulated pointer. Pitch accents and boundary tones are selected by the realiser based on the context-sensitive information-structure annotations (theme/rheme; marked/unmarked) included in the logical forms. At the moment, the other multimodal coarticulations are sp</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG chart realization. In Proceedings ofINLG 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation.</title>
<date>2005</date>
<note>To appear.</note>
<contexts>
<context position="3021" citStr="White, 2005" startWordPosition="448" endWordPosition="449">such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 45–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics (a) Bathroom-design application (b) Talking head Figure 1: Components of the COMIC interface User Tell me about this design [click on AltMettlach] COMIC [Look at screen] THIS DESIGN is in the CLASSIC style. [circle</context>
<context position="7379" citStr="White, 2005" startWordPosition="1166" endWordPosition="1167"> history and the user model, along with any properties specifically requested by the dialogue manager. It then creates a structure for the selected properties and creates logical forms as input for the OpenCCG surface realiser. The logical forms may include explicit alternatives in cases where there are multiple ways of expressing a property; for example, it could say either This design is in the classic style or This design is classic. OpenCCG makes use of statistical language models to choose among such alternatives. This process is described in detail in (Foster and White, 2004; Foster and White, 2005). In addition to text, the output of COMIC also incorporates multimodal behaviours including prosodic specifications for the speech synthesiser (pitch accents and boundary tones), facial behaviour specifications (expressions and gaze shifts), and deictic gestures at objects on the application screen using a simulated pointer. Pitch accents and boundary tones are selected by the realiser based on the context-sensitive information-structure annotations (theme/rheme; marked/unmarked) included in the logical forms. At the moment, the other multimodal coarticulations are specified directly by the f</context>
</contexts>
<marker>White, 2005</marker>
<rawString>Michael White. 2005. Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation. To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>