<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.777598">
Word Sense Disambiguation using lexical cohesion in the context
</title>
<author confidence="0.911316">
Dongqiang Yang  |David M.W. Powers
</author>
<affiliation confidence="0.682573">
School of Informatics and Engineering
Flinders University of South Australia
</affiliation>
<address confidence="0.52052">
PO Box 2100, Adelaide
</address>
<email confidence="0.855871">
Dongqiang.Yang|David.Powers@flinders.edu.au
</email>
<sectionHeader confidence="0.992182" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854466666667">
This paper designs a novel lexical hub to
disambiguate word sense, using both syn-
tagmatic and paradigmatic relations of
words. It only employs the semantic net-
work of WordNet to calculate word simi-
larity, and the Edinburgh Association
Thesaurus (EAT) to transform contextual
space for computing syntagmatic and
other domain relations with the target
word. Without any back-off policy the
result on the English lexical sample of
SENSEVAL-21 shows that lexical cohe-
sion based on edge-counting techniques
is a good way of unsupervisedly disam-
biguating senses.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997254368421053">
Word Sense Disambiguation (WSD) is generally
taken as an intermediate task like part-of-speech
(POS) tagging in natural language processing,
but it has not so far achieved the sufficient preci-
sion for application as POS tagging (for the his-
tory of WSD, cf. Ide and Véronis (1998)). It is
partly due to the nature of its complexity and
difficulty, and to the widespread disagreement
and controversy on its necessity in language en-
gineering, and to the representation of the senses
of words, as well as to the validity of its evalua-
tion (Kilgarriff and Palmer, 2000). However the
endeavour to automatically achieve WSD has
been continuous since the earliest work of the
1950’s.
In this paper we specifically investigate the
role of semantic hierarchies of lexical knowledge
on WSD, using datasets and evaluation methods
from SENSEVAL (Kilgarriff and Rosenzweig,
</bodyText>
<footnote confidence="0.844959">
1 http://www.senseval.org/
</footnote>
<bodyText confidence="0.997984833333333">
2000) as these are well known and accepted in
the community of computational linguistics.
With respect to whether or not they employ
the training materials provided, SENSEVAL
roughly categorizes the participating systems
into “unsupervised systems” and “supervised
systems”. Those that don’t use the training data
are not usually truly unsupervised, being based
on lexical knowledge bases such as dictionaries,
thesauri or semantic nets to discriminate word
senses; conversely the “supervised” systems
learn from corpora marked up with word senses.
The fundamental assumption, in our “unsu-
pervised” technique for WSD in this paper, is
that the similarity of contextual features of the
target with the pre-defined features of its sense in
the lexical knowledge base provides a quantita-
tive cue for identifying the true sense of the tar-
get.
The lexical ambiguity of polysemy and ho-
monymy, whose distinction is however not abso-
lute as sometimes the senses of word may be in-
termediate, is the main object of WSD. Verbs,
with their more flexible roles in a sentence, tend
to be more polysemous than nouns, so worsening
the computational feasibility. In this paper we
disambiguated the sense of a word after its POS
tagging has assigned them either a noun or a verb
tag. Furthermore, we deal with nouns and verbs
separately.
</bodyText>
<sectionHeader confidence="0.829107" genericHeader="method">
2 Some previous work on WSD using
semantic similarity
</sectionHeader>
<bodyText confidence="0.999945625">
Sussna (1993) utilized the semantic network of
nouns in WordNet to disambiguate term senses
to improve the precision of SMART information
retrieval at the stage of indexing, in which he
assigned two different weights for both direc-
tions of edges in the network to compute the
similarity of two nodes. He then exploited the
moving fixed size window to minimize the sum
</bodyText>
<page confidence="0.973716">
929
</page>
<note confidence="0.7249965">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 929–936,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999677">
of all combinations of the shortest distances
among target and context words.
Pedersen et al. (2003) extended Lesk’s defini-
tion method (1986) to discriminate word sense
through the definitions of both target and its IS-A
relatives, and achieved a better result in the Eng-
lish lexical sample task of SENSEVAL-2, com-
pared with other edge-counting or statistical es-
timation metrics on WordNet.
Humans carefully select words in a sentence to
express harmony or cohesion in order to ease the
ambiguity of the sentence. Halliday and Hasan
(1976) argued that cohesive chains unite text
structure together through reiteration of reference
and lexical semantic relations (superordinate and
subordinate). Morris and Hirst (1991) suggested
building lexical chains is important in the resolu-
tion of lexical ambiguity and the determination
of coherence and discourse structure. They ar-
gued that lexical chains, which cover the multi-
ple semantic relations (systematic and non-
systematic), can transform the context setting
into the computational one to narrow down the
specific meaning of the target, manually realiz-
ing this with the help of Roget’s Thesaurus. They
defined a lexical chain within Roget’s very gen-
eral hierarchy, in which lexical relationships are
traced through a common category.
Hirst and St-Onge (1997) define a lexical
chain using the syn/antonym and hyper/hyponym
links of WordNet to detect and correct malaprop-
isms in context, in which they specified three
different weights from extra-strong to medium
strong to score word similarity to decide the in-
serting sequence in the lexical chain. They first
computationally employed WordNet to form a
“greedy” lexical chain as a substitute of the con-
text to solve the matter of malapropism, where
the word sense is decided by its preceding words.
Around the same time, Barzilay and Elhadad
(1997) realized a “non-greedy” lexical chain,
which determined the word sense after process-
ing of all words, in the context of text summari-
zation.
In this paper we propose an improved lexical
chain, the lexical hub, that holds the target to be
disambiguated as the centre, replacing the usual
chain topology used in text summarization and
cohesion analysis. In contrast with previous
methods we only record the lexical hub of each
sense of the target, and we don’t keep track of
other context words. In other words, after the
computation of lexical hub of the target, we can
immediately produce the right sense of the target
even though the senses of the context words are
still in question. We also transform the context
surroundings through a word association thesau-
rus to explore the effect of other semantic rela-
tionships such as syntagmatic relation against
WSD.
</bodyText>
<subsectionHeader confidence="0.625941">
3 Selection of knowledge bases
</subsectionHeader>
<bodyText confidence="0.9999546">
WordNet (Fellbaum, 1998) provides a fine-
grained enumerative semantic net that is com-
monly used to tag the instances of English target
words in the tasks of SENSEVAL with different
senses (WordNet synset numbers). WordNet
groups related concepts into synsets and links
them through IS-A and PART-OF links, empha-
sizing the vertical interaction between the con-
cepts that is much paradigmatic.
Although WordNet can capture the fine-
grained paradigmatic relations of words, another
typical word relationship, syntagmatic connect-
edness, is neglected. The syntagmatic relation-
ship, which is often characterized with different
POS tag, and frequently occurs in corpora or
human brains, plays a critical part in cross-
connecting words from different domains or POS
tags.
It should be noted that WordNet 2.0 makes
some efforts to interrelate nouns and verbs using
their derived lexical forms, placing associated
words under the same domain. Although some
verbs have derived noun forms that can be
mapped onto the noun taxonomy, this mapping
only relates the morphological forms of verbs,
and still lacks syntagmatic links between words.
The interrelationship of noun and verb hierar-
chies is far from complete and only a supplement
to the primary IS-A and PART-OF taxonomies
in WordNet. Moreover as WordNet generally
concerns the paradigmatic relations (Fellbaum,
1998), we have to seek for other lexical knowl-
edge sources to compensate for the shortcomings
of WordNet in WSD.
The Edinburgh Association Thesaurus2 (EAT)
provides an associative network to account for
word relationship in human cognition after col-
lecting the first response words for the stimulus
words list (Kiss et al., 1973). Take the words eat
and food for example. There is no direct path
between the concepts of these two words in the
taxonomy of WordNet (both as noun and verb),
except in the gloss of the first and third sense of
eat to explain ‘take in solid food’, or ‘take in
food’, which glosses are not regularly or care-
</bodyText>
<footnote confidence="0.963617">
2 http://www.eat.rl.ac.uk/
</footnote>
<page confidence="0.996738">
930
</page>
<bodyText confidence="0.999923727272727">
fully organized in WordNet. However in EAT
eat is strongly associated with food, and when
taking eat as a stimulus word, 45 out of 100 sub-
jects regarded food as the first response.
Yarowsky (1993) indicated that the objects of
verbs play a more dominant role than their sub-
jects in WSD and nouns acquire more stable dis-
ambiguating information from their noun or ad-
jective modifiers.
In the case of verbs association tests, it is also
reported that more than half the response words
of verbs (the stimuli) are syntagmatically related
(Fellbaum, 1998). In experiments of examining
the psychological plausibility of WordNet
relationships, Chaffin et al. (1994) stated that
only 30.4% of the responses of 75 verb stimuli
belongs to verbs, and more than half of the re-
sponses are nouns, of which nearly 90% are
categorized as the arguments of the verbs.
Sinopalnikova (2004) also reported that there
are multiple relationships found in word associa-
tion thesaurus, such as syntagmatic, paradigmatic
relations, domain information etc.
In this paper we only use the straightforward
forms of context words separating the effect of
syntactic dependence on the WSD. As a supple-
ment of enriching word linkage in the WSD, we
retrieve the lexical knowledge from both Word-
Net and EAT. We first explore the function of
semantic hierarchies of WordNet on WSD, and
then we transform the context word with EAT to
investigate whether other relationships can im-
prove WSD.
</bodyText>
<sectionHeader confidence="0.970088" genericHeader="method">
4 System design
</sectionHeader>
<bodyText confidence="0.999991833333333">
In order to find semantically related words to
cohesively form lexical hubs, we first employ the
two word similarity algorithms of Yang and
Powers (2005; 2006) that use WordNet to com-
pute noun similarity and verb similarity respec-
tively. We next construct the lexical hub for each
target sense to assemble the similarity score be-
tween the target and its context words together.
The maximum score of these lexical hubs spe-
cifically predicts the real sense of the target, also
implicitly captures the cohesion and real mean-
ing of the word in its context.
</bodyText>
<subsectionHeader confidence="0.995987">
4.1 Similarity metrics on nouns
</subsectionHeader>
<bodyText confidence="0.997067947368421">
Yang and Powers (2005) designed a metric,
Sim (c1 , c2) = αt *
utilizing both IS-A and PART-OF taxonomies of
WordNet to measure noun similarity, and they
argued that the similarity of nouns is the maxi-
mum of all their concept similarities. They de-
fined the similarity (Sim) of two concepts (c1 and
c2) with a link type factor (αt) to specify the
weights of different link types (t) (syn/antonym,
hyper/ hyponym, and holo/meronym) in the
WordNet, and a path type factor (Rt) to reduce
the uniform distance of the single link, along
with a depth factor (k) to restrict the maximum
searching distance between concepts. Since their
metric on noun similarity is significantly better
than some popular measures and even outper-
forms some subjects on a standard data set, we
selected it as a measure on noun similarity in our
WSD task.
</bodyText>
<subsectionHeader confidence="0.98982">
4.2 Similarity metrics on verbs
</subsectionHeader>
<bodyText confidence="0.9989945">
Yang and Powers (2006) also redesigned their
noun model,
</bodyText>
<equation confidence="0.99155425">
Dist(
Sim c c
( 1, 2) = αstr * αt * ∏ β
i=1
</equation>
<bodyText confidence="0.998962">
to accommodate verb case, which is harder to
deal with in the shallow and incomplete taxon-
omy of verbs in WordNet. As an enhancement to
the uniqueness of verb similarity they also con-
sider three fall-back factors, where if αstr is 1
normally but successively falls back to:
</bodyText>
<listItem confidence="0.99993025">
• αstm: the verb stem polysemy ignoring sense
and form
• αder: the cognate noun hierarchy of the verb
• αgls: the definition of the verb
</listItem>
<bodyText confidence="0.999399818181818">
They also defined two alternate search proto-
cols: rich hierarchy exploration (RHE) with no
more than six links and shallow hierarchy explo-
ration (SHE) with no more than two links.
One minor improvement to the verb model in
their system comes from comparing the similar-
ity of verbs and nouns using the noun model
metric for the derived noun form of verb. It thus
allows us to compare nouns and verbs and avoids
the limitation of having to have the same POS
tag.
</bodyText>
<subsectionHeader confidence="0.996601">
4.3 Depth in WordNet
</subsectionHeader>
<bodyText confidence="0.999875375">
Yang and Powers fine-tuned the parameters of
the noun and verb similarity models, finding
them relatively insensitive to the precise values,
and we have elected to use their recommended
values for the WSD task. But it is worth
mentioning that their optimal models are
achieved in purely verbal data sets, i.e. the
similarity score is context-free.
</bodyText>
<figure confidence="0.830263833333333">
λ
β
)
c c
1, 2
ti
</figure>
<page confidence="0.98652">
931
</page>
<bodyText confidence="0.999891363636363">
In their models, the depth in the WordNet, i.e.
the distance between the synsets of words (λ), is
indeed an outside factor which confines the
searching scope to the cost of computation and
depends on the different applications. If we tuned
it using the training data set of SENSEVAL-2 we
probably would assign different values and might
achieve better results. Note that for both nouns
and verbs we employ RHE (rich hierarchy explo-
ration) with λ = 2 making full use of the taxon-
omy of WordNet and making no use of glosses.
</bodyText>
<subsectionHeader confidence="0.976218">
4.4 How to setup the selection standard for
the senses
</subsectionHeader>
<bodyText confidence="0.999969380952381">
Other than making the most of WSD results, our
main motive for this paper is to explore to what
extent the semantic relationships will reach accu-
racy, and to fully acknowledge the contribution
of this single attribute working on WSD, which
is encouraged by SENSEVAL in order to gain
further benefits in this field (Kilgarriff and
Palmer, 2000). Without any definition, which is
previously surveyed by Lesk (1986) and Peder-
sen et al. (2003), we screen off the definition fac-
tor in the metric of verb similarity, with the in-
tention of focusing on the taxonomies of Word-
Net.
Assuming that the lexical hub for the right
sense would maximize the cohesion with other
words in the discourse, we design six different
strategies to calculate the lexical hub in its unor-
dered contextual surroundings.
We first put forward three metrics to measure
up the similarity of the senses of the target and
the context word:
</bodyText>
<listItem confidence="0.6977155">
• The maximized sense similarity
(Sim(Tk , Ci, j ))
</listItem>
<bodyText confidence="0.997149833333333">
where T denotes the target, Tk is the kth
sense of the target; Ci is the ith context word
in a fixed window size around the target, Ci,j
the jth sense of Ci. Note that T and C can be
any noun and verb, along with Sim the met-
rics of Yang and Powers.
</bodyText>
<listItem confidence="0.799708">
• The average of sense similarity
</listItem>
<equation confidence="0.9814006">
m
Simsum Tk Ci
( , ) =∑= SimT k Ci j
( , , )
j 1
</equation>
<bodyText confidence="0.89766225">
where m is the total sense number of Ci.
Subsequently we can define six distinctive
heuristics to score the lexical hub in the follow-
ing parts:
</bodyText>
<listItem confidence="0.984215">
• Heuristic 1 – Sense Norm (HSN)
</listItem>
<bodyText confidence="0.993988">
where Linkw(Ti)=1 if Simmax(Tk,Ci)&gt;0, oth-
erwise 0
</bodyText>
<listItem confidence="0.975252">
• Heuristic 2 – Sense Max (HSM)
</listItem>
<bodyText confidence="0.82652">
An unnormalized version of HSN is:
</bodyText>
<equation confidence="0.994288166666667">
 l 
 Sim T C
( , ) 
max k i
∑=
k  i 1 
</equation>
<listItem confidence="0.772028">
• Heuristic 3 – Sense Ave (HSA)
</listItem>
<bodyText confidence="0.848017333333333">
Taking into account all of the links between
the target and its context word, the correct
sense of the target is:
</bodyText>
<equation confidence="0.962014833333333">
 l 
 Sim T C
( , ) 
ave k i
∑=
 i 1 
</equation>
<listItem confidence="0.653042">
• Heuristic 4 – Sense Sum (HSS)
</listItem>
<bodyText confidence="0.698715">
The unnormalized version of HSA is:
</bodyText>
<equation confidence="0.941075833333333">
 l 
 Sim T C
( , ) 
sum k i
∑=
 i 1 
</equation>
<listItem confidence="0.853126">
• Heuristic 5 – Word Linkage (HWL)
</listItem>
<bodyText confidence="0.9970286">
The straightforward output of the correct
sense of the target in the discourse is to count
the maximum number of context words
whose similarity scores with the target are
larger than zero:
</bodyText>
<listItem confidence="0.984911">
• Heuristic 6 – Sense Linkage (HSL)
</listItem>
<bodyText confidence="0.9984332">
No matter what kind of relations between the
target and its context are, the sense of the
target, which is related to the maximum
counts of senses of all its context words, is
scored as the right meaning:
</bodyText>
<figure confidence="0.728582777777778">

arg max 
k 
l
∑ Simmax (Tk, Ci ) ∑
i=1 i=1
Sense(T) =
l
Linkw T
( k
,
Ci

) 
max (Tk , Ci ) = max
j
Sim
Sense(T)
</figure>
<equation confidence="0.994301681818182">
= arg max
Sense(T) = arg max
k
Sense(T) = arg max
k
Sense(T) = arg
 max∑l ) 
Linkw(Tk , Ci
m
m
Sense(T)
= arg max
k
j=1 j=1
 l m 
 ∑∑Links(Tk , Ci,j)
 i = =
1 1
j 
Simave (Tk, Ci) = ∑ Sim(Tk, Ci j) ∑Links T C
( , , )
k i j
</equation>
<bodyText confidence="0.943971875">
where Links(Tk,Ci,j)=1, if Sim(Tk,Ci,j)&gt;0, oth-
erwise 0.
• The sum of sense similarity
Therefore the lexical hub of each sense of the
target only relies on the interaction of the target
and its each context word, rather than of the con-
text words. The implication is that the lexical
hub only disambiguates the real sense of the tar-
</bodyText>
<page confidence="0.987775">
932
</page>
<bodyText confidence="0.999903411764706">
get other than the real meaning of the context
word; the maximum scores or link numbers (on
the level of words or senses) in the six heuristics
suggest that the correct sense of the target should
cohere with as many words or their senses as
practicable in the discourse.
When similarity scores are ties we directly
produce all of the word senses to prevent us from
guessing results. Some WSD systems in SEN-
SEVAL handle tied scores simply using the first
sense (in WordNet) of the target as the real
sense. It is no doubt that the skewed distribution
of word senses in the corpora (the first sense of-
ten captures the dominant sense) can benefit the
performance of the systems, but at the same time
it mixes up the contribution of the semantic hier-
archy on WSD in our system.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999994548387097">
We evaluate the six heuristics on the English
lexical sample of SENSEVAL-2, in which each
target word has been POS-tagged in the training
part. With the absence of taxonomy of adjectives
in WordNet we only extract all 29 nouns and all
29 verbs from a total of 73 lexical targets, and
then we subcategorize the test dataset into 1754
noun instances and 1806 verb instances. Since
the sample of SENSEVAL-2 is manually sense-
tagged with the sense number of WordNet 1.7
and our metrics are based on its version 2.0, we
translate the sample and answer format into 2.0
in accordance with the system output format.
Finally, we find that each noun target has 5.3
senses on average and each verb target 16.4
senses. Hence the baseline of random selection
of senses is the reciprocal of each average sense
number, i.e. separately 18.9 percent for nouns
and 6 percent for verbs.
In addition, SENSEVAL-2 provides a scoring
software with 3 levels of schemes, i.e. fine-
grained, coarse-grained and mixed-grained to
produce precision and recall rates to evaluate the
participating systems. According to the SEN-
SEVAL scoring system, as we always give at
least one answer, the precision is identical to the
recall under the separate noun and verb datasets.
So we just evaluate our systems in light of accu-
racy. We tested the heuristics with fine-grained
precision, which required the exact match of the
key to each instance.
</bodyText>
<subsectionHeader confidence="0.960419">
5.1 Context
</subsectionHeader>
<bodyText confidence="0.999690782608696">
Without any knowledge of domain, frequency
and pragmatics to guess, word context is the only
way of labeling the real meaning of word. Basi-
cally a bag of context words (after morphological
analyzing and filtering stop-words) or the fine-
grained ones (syntactic role, selection preference
etc.) can provide cues for the target. We propose
to merely use a bag of words to feed into each
heuristic in case of losing any valuable informa-
tion in the disambiguation, and preventing from
any interference of other clues except the seman-
tic hierarchy of WordNet.
The size of the context is not a definitive fac-
tor in WSD, Yarowsky (1993) suggested the size
of 3 or 4 words for the local ambiguity and 20/50
words for topic ambiguity. He also employed
Roget’s Thesaurus in 100 words of window to
implement WSD (Yarowsky, 1992). To investi-
gate the role of local context and topic context
we vary the size of window from one word dis-
tance away to the target (left and right) until 100
words away in nouns or 60 in verbs, until there
are no increases in the context of each instance.
</bodyText>
<figureCaption confidence="0.99932125">
Figure 1: the result of noun disambiguation with
different size of context in SENSEVAL 2
Figure 2: the result of verb disambiguation with
different size of context in SENSEVAL 2
</figureCaption>
<bodyText confidence="0.999556333333333">
Noun and verb disambiguation results are re-
spectively displayed in Figure 1 and 2. Since the
performance curves of the heuristics turned into
flat and stable (the average standard deviations
of the six curves of nouns and verbs is around
0.02 level before 60 and 20, after that approxi-
</bodyText>
<figure confidence="0.994519755555555">
2 5 10 20 30 40 50 60 70 80 90 100
context
accuracy
0.45
0.43
0.41
0.39
0.37
0.35
0.33
0.31
0.29
0.27
0.25
HSN
HSM
HSA
HSS
HWL
HSL
1 2 3 4 5 10 20 30 40 50 60
context
accuracy
0.37
0.35
0.33
0.31
0.29
0.27
0.25
0.23
0.21
0.19
0.17
0.15
0.13
0.11
0.09
0.07
0.05
HSN
HSM
HSA
HSS
HWL
</figure>
<page confidence="0.997965">
933
</page>
<bodyText confidence="0.986046">
mately 0.001 level), optimal performance is
reached at 60 context words for nouns and 20
words for verbs. These values are used as pa-
rameters in subsequent experiments.
</bodyText>
<figure confidence="0.526799">
context srandrs sr rs srorrs
different contexts
</figure>
<figureCaption confidence="0.9219145">
Figure 3: the results of nouns disambiguation of
SENSEVAL-2 in the transformed context spaces
</figureCaption>
<figure confidence="0.75029">
context srandrs sr rs srorrs
different contexts
</figure>
<figureCaption confidence="0.860894666666667">
Figure 4: the results of verbs disambiguation
of SENSEVAL-2 in the transformed context
spaces
</figureCaption>
<bodyText confidence="0.999885911111111">
Although our metrics can measure the similarity
of nouns and verbs through the derived related
form of verbs (not from the derived verbs of
nouns as a consequence of the shallowness of
verb taxonomy of WordNet), we still can’t com-
pletely rely on WordNet, which focuses on the
paradigmatic relations of words, to fully cover
the complexity of contextual happenings of
words.
Since the word association norm captures both
syntagmatic and pragmatic relations in words,
we transform the context words of the target into
its associated words, which can be retrieved in
the EAT, to augment the performance of the
lexical hub.
There are two word lists in the EAT: one list
takes each head word as a stimulus word, and
then collects and ranks all response words ac-
cording to their frequency of subject consensus;
the other list is in the reverse order with the re-
sponse as a head word and followed by the elicit-
ing stimuli. We denote the stimulus/response set
of word as SR, respond/stimulus as RS. Apart
from that we symbolize SRANDRS as the
intersection of SR and RS, along with SRORRS
as the union set of SR and RS. Then for each
context word we retrieve its corresponding words
in each word list and calculate the similarity be-
tween the target and these words including the
context words.
As a result we transform the original context
space of each target into an enriched context
space under the function of SR, RS, SRANDRS
or SRORRS.
We take the respective 60 context words of
nouns and 20 words of verbs as the reference
points for the transferred context experiment,
since after that the performance curves of the
heuristics turned into flat and stable (the average
standard deviations of the six curves of nouns
and verbs is around 0.02 level before 60, after
that approximately 0.001 level).
After the transformations, the noun and verb
results are respectively demonstrated in Figure 3
and 4.
</bodyText>
<figure confidence="0.924624">
6 Comparison with other techniques.
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
accuracy
</figure>
<figureCaption confidence="0.9798135">
Figure 5: comparisons of HWL and HSL with
other unsupervised systems and similarity met-
</figureCaption>
<bodyText confidence="0.9882307">
rics
Pedersen et al. (2003) in the work of evaluating
different similarity techniques based on Word-
Net, realized two variants of Lesk’s methods:
extended gloss overlaps (P&amp;L_extend) and gloss
vector (P&amp;L_vector), as well as evaluating them
in the English lexical sample of SENSEVAL-2.
The best edge-counting-based metric that they
measured are from Jiang and Conrath (1997)
(J&amp;C).
</bodyText>
<figure confidence="0.999419967213115">
5.2 Transformed context (EAT)
0.47
0.45
0.43
0.41
0.39
0.37
0.35
0.33
0.31
0.29
0.27
0.25
accuracy
HSN
HSM
HSA
HSS
HWL
HSL
accuracy
0.39
0.37
0.35
0.33
0.31
0.29
0.27
0.25
0.23
0.21
0.19
0.17
0.15
0.13
0.11
0.09
0.07
0.05
HSN
HSM
HSA
HSS
HWL
HSL
HSL_SRORRS
HWL_SRORRS
IIT 2
IIT 1
DIMAP
UNED-LS-U
HSL_Context
HWL_Context
P&amp;L_extend
P&amp;L_vector
J&amp;C
Baseline Lesk Def
Baseline Lesk
Baseline Random
noun
verb
</figure>
<page confidence="0.995062">
934
</page>
<bodyText confidence="0.9997211">
Accordingly, without the transformation of
EAT, we compare our results of HWL and HSL
(denoted as HWL_Context and HSL_Context)
with the above methods (picking up their optimal
values). The results are illustrated in Figure 5. At
the same time we also list three baselines for un-
supervised systems (Kilgarriff and Rosenzweig,
2000), which are Baseline Random (randomly
selecting one sense of the target), Baseline Lesk
(overlapping between the examples and defini-
tions of and unsupervised systems in SEN-
SEVAL-2 each sense of the target and context
words), and its reduced version, i.e. Baseline
Lesk Def (only definition).
We further compare HWL and HSL with the
intervention of SRORRS of EAT (denoted as
HWL_SRORRS and HSL_ SRORRS) with other
unsupervised systems that employ no training
materials of SENSEVAL-2, which are respec-
tively:
</bodyText>
<listItem confidence="0.978127111111111">
• IIT 1 and IIT 2: extended the WordNet gloss
of each sense of the target, along with its su-
perordinate and subordinate node’s glosses,
without back-off policies.
• DIMAP: employed both WordNet and the
New Oxford Dictionary of English. With the
first sense as a back-off when tied scores oc-
curred.
• UNED-LS-U: for each sense of the target,
</listItem>
<bodyText confidence="0.963925571428571">
they enriched the sense describer through the
first five hyponyms of it and a dictionary
built from 3200 books from Project Guten-
berg. They adopted a back-off policy to the
first sense and discarded the senses account-
ing for less than 10 percent of files in Sem-
Cor).
</bodyText>
<sectionHeader confidence="0.997056" genericHeader="evaluation">
7 Conclusion and discussion
</sectionHeader>
<subsectionHeader confidence="0.999889">
7.1 Local context and topic context
</subsectionHeader>
<bodyText confidence="0.999939923076923">
On the analysis of standard deviation of preci-
sion on different stage in Figure 1 and 2 we can
conclude that the optimum size for HSN to HSS
was ±10 words for nouns, reflecting a sensitivity
to only local context, whilst HWL and HSL re-
flected significant improvement up to ±60 re-
flecting a sensitivity to topical context. In the
case of verbs HSA showed little significant con-
text sensitivity, HSN showed some positive sen-
sitivity to local context but increasing beyond ±5
had a negative effect, HSM and HSS to HSL
showed some sensitivity to broader topical con-
text but this plateaued around ±20 to 30.
</bodyText>
<subsectionHeader confidence="0.999044">
7.2 The analysis of different heuristics.
</subsectionHeader>
<bodyText confidence="0.9999948">
HWL and HSL were clearly superior for both
noun and verb tasks, with the superiority of HSL
being significantly greater and more comparable
between noun and verb tasks with the difference
scarcely reaching significance. These observa-
tions remain true with the addition of the EAT
information. After transformations with EAT for
nouns, HSL and HWL no longer differ signifi-
cantly in performance, forming a single group
with relatively higher precision, whilst the other
heuristics clump together into another group with
lower precision, reflecting a negative effect from
EAT. In the verb case, HWL and HSL, HSM and
HSS, and HSN and HSA form three significantly
different groups with reference to their precision,
reflecting poor performance of both normalized
heuristics (HSN and HSA) and a significantly
improved result of HWL from the EAT data.
All of this implies that in the lexical hub for
WSD, the correct meaning of a word should hold
as many links as possible with a relatively large
number of context words. These links can be in
the level of word form (HWL) or word sense
(HSL). HSL achieved the highest precision in
both nouns and verbs.
</bodyText>
<subsectionHeader confidence="0.916398">
7.3 The interaction of EAT in WSD
</subsectionHeader>
<bodyText confidence="0.999921444444445">
For the noun sense disambiguation, the paired
two sample for mean of the t-Test showed us that
RS and SRORRS transformations can signifi-
cantly improve the precision of disambiguation
of HWL and HSL (P&lt;0.05, at the confidence
level of 95 percent). All four transformations
using EAT for verb disambiguation are signifi-
cantly better than its straightforward context case
on HWL and HSL (P&lt;0.05, at the confidence
level of 95 percent).
It demonstrated that both the syntagmatic rela-
tion and other domain information in the EAT
can help discriminate word sense. With the trans-
formation of context surroundings of the target,
the similarity metrics can compare the likeness
of nouns and verbs, although we can exploit the
derived form of word in WordNet to facilitate the
comparison.
</bodyText>
<subsectionHeader confidence="0.996592">
7.4 Comparison with other methods
</subsectionHeader>
<bodyText confidence="0.9990718">
The lexical hub reached comparatively higher
precision in both nouns (45.8%) and verbs
(35.6%). This contrasted with other similarity
based methods and the unsupervised systems in
SENSEVAL-2. Note that we don’t adopt any
</bodyText>
<page confidence="0.995167">
935
</page>
<bodyText confidence="0.9998966">
back-off policy such as the commonest sense of
word used by UNED-LS-U and DIMAP.
Although the noun and verb similarity metrics
in this paper are based on edge-counting without
any aid of frequency information from corpora,
they performed very well in the task of WSD in
relation to other information based metrics and
definition matching methods. Especially in the
verb case, the metric significantly outperformed
other metrics.
</bodyText>
<sectionHeader confidence="0.955536" genericHeader="conclusions">
8 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999702">
In this paper we defined the lexical hub and pro-
posed its use for processing word sense disam-
biguation, achieving results that are compara-
tively better than most unsupervised systems of
SENSEVAL-2 in the literature. Since WordNet
only organizes the paradigmatic relations of
words, unlike previous methods, which are only
based on WordNet, we fed the syntagmatic rela-
tions of words from the EAT into the noun and
verb similarity metrics, and significantly im-
proved the results of WSD, given that no back-
off was applied. Moreover, we only utilized the
unordered raw context information without any
pragmatic knowledge and syntactic information;
there is still a lot of work to fuse them in the fu-
ture research. In terms of the heuristics evaluated,
richness of sense or word connectivity is much
more important than the strength of individual
word or sense linkages. An interesting question
is whether these results will be borne out in other
datasets. In the forthcoming work we will inves-
tigate their validity in the lexical task of SEN-
SEVAL-3.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943121212121">
Barzilay, R. and M. Elhadad (1997). Using Lexical
Chains for Text Summarization. In the Intelligent
Scalable Text Summarization Workshop (ISTS&apos;97),
ACL, Madrid, Spain.
Chaffin, R., et al. (1994). The Paradigmatic Organiza-
tion of Verbs in the Mental Lexicon. Trenton State
College.
Fellbaum, C. (1998). Wordnet: An Electronic Lexical
Database. Cambridge MA, USA, The MIT Press.
Halliday, M. A. K. and R. Hasan (1976). Cohesion in
English. London, London:Longman.
Hirst, G. and D. St-Onge (1997). Lexical Chains as
Representations of Context for the Detection and
Correction of Malapropisms. Wordnet. C. Fell-
baum. Cambridge, MA, The Mit Press.
Ide, N. and J. Véronis (1998). Word Sense Disam-
biguation: The State of the Art. Computational lin-
guistics 24(1).
Jiang, J. and D. Conrath (1997). Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy.
In the 10th International Conference on Research
in Computational Linguistics (ROCLING), Taiwan.
Kilgarriff, A. and M. Palmer (2000). Introduction,
Special Issue on Senseval: Evaluating Word Sense
Disambiguation Programs. Computers and the
Humanities 34(1-2): 1-13.
Kilgarriff, A. and J. Rosenzweig (2000). Framework
and Results for English Senseval. Computers and
the Humanities 34(1-2): 15-48.
Kiss, G. R., et al. (1973). The Associative Thesaurus
of English and Its Computer Analysis. Edinburgh,
University Press.
Lesk, M. (1986). Automatic Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell
a Pine Code from an Ice Cream Cone. In the 5th
annual international conference on systems docu-
mentation, ACM Press.
Morris, J. and G. Hirst (1991). Lexical Cohesion
Computed by Thesaural Relations as an Indicator
of the Structure of Text. Computational linguistics
17(1).
Pedersen, T., et al. (2003). Maximizing Semantic Re-
latedness to Perform Word Sense Disambiguation.
Sinopalnikova, A. (2004). Word Association Thesau-
rus as a Resource for Building Wordnet. In GWC
2004.
Sussna, M. (1993). Word Sense Disambiguation for
Free-Text Indexing Using a Massive Semantic
Network. In CKIM&apos;93.
Yang, D. and D. M. W. Powers (2005). Measuring
Semantic Similarity in the Taxonomy of Wordnet.
In the Twenty-Eighth Australasian Computer Sci-
ence Conference (ACSC2005), Newcastle, Austra-
lia, ACS.
Yang, D. and D. M. W. Powers (2006). Verb Similar-
ity on the Taxonomy of Wordnet. In the 3rd Inter-
national WordNet Conference (GWC-06), Jeju Is-
land, Korea.
Yarowsky, D. (1992). Word Sense Disambiguation
Using Statistical Models of Roget&apos;s Categories
Trained on Large Corpora. In the 14th International
Conference on Computational Linguistics, Nates,
France.
Yarowsky, D. (1993). One Sense Per Collocation. In
ARPA Human Language Technology Workshop,
Princeton, New Jersey.
</reference>
<page confidence="0.998642">
936
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347210">
<title confidence="0.993068">Word Sense Disambiguation using lexical cohesion in the context</title>
<author confidence="0.998849">Dongqiang Yang |David M W Powers</author>
<affiliation confidence="0.739754">School of Informatics and Engineering Flinders University of South Australia</affiliation>
<address confidence="0.667929">PO Box 2100, Adelaide</address>
<email confidence="0.913462">Dongqiang.Yang|David.Powers@flinders.edu.au</email>
<abstract confidence="0.997914875">This paper designs a novel lexical hub to disambiguate word sense, using both syntagmatic and paradigmatic relations of words. It only employs the semantic network of WordNet to calculate word similarity, and the Edinburgh Association Thesaurus (EAT) to transform contextual space for computing syntagmatic and other domain relations with the target word. Without any back-off policy the result on the English lexical sample of shows that lexical cohesion based on edge-counting techniques is a good way of unsupervisedly disambiguating senses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In the Intelligent Scalable Text Summarization Workshop (ISTS&apos;97), ACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="5452" citStr="Barzilay and Elhadad (1997)" startWordPosition="846" endWordPosition="849">ch lexical relationships are traced through a common category. Hirst and St-Onge (1997) define a lexical chain using the syn/antonym and hyper/hyponym links of WordNet to detect and correct malapropisms in context, in which they specified three different weights from extra-strong to medium strong to score word similarity to decide the inserting sequence in the lexical chain. They first computationally employed WordNet to form a “greedy” lexical chain as a substitute of the context to solve the matter of malapropism, where the word sense is decided by its preceding words. Around the same time, Barzilay and Elhadad (1997) realized a “non-greedy” lexical chain, which determined the word sense after processing of all words, in the context of text summarization. In this paper we propose an improved lexical chain, the lexical hub, that holds the target to be disambiguated as the centre, replacing the usual chain topology used in text summarization and cohesion analysis. In contrast with previous methods we only record the lexical hub of each sense of the target, and we don’t keep track of other context words. In other words, after the computation of lexical hub of the target, we can immediately produce the right s</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Barzilay, R. and M. Elhadad (1997). Using Lexical Chains for Text Summarization. In the Intelligent Scalable Text Summarization Workshop (ISTS&apos;97), ACL, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chaffin</author>
</authors>
<title>The Paradigmatic Organization of Verbs in the Mental Lexicon. Trenton State College.</title>
<date>1994</date>
<marker>Chaffin, 1994</marker>
<rawString>Chaffin, R., et al. (1994). The Paradigmatic Organization of Verbs in the Mental Lexicon. Trenton State College.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>USA, The MIT Press.</publisher>
<location>Cambridge MA,</location>
<contexts>
<context position="6369" citStr="Fellbaum, 1998" startWordPosition="1000" endWordPosition="1001">used in text summarization and cohesion analysis. In contrast with previous methods we only record the lexical hub of each sense of the target, and we don’t keep track of other context words. In other words, after the computation of lexical hub of the target, we can immediately produce the right sense of the target even though the senses of the context words are still in question. We also transform the context surroundings through a word association thesaurus to explore the effect of other semantic relationships such as syntagmatic relation against WSD. 3 Selection of knowledge bases WordNet (Fellbaum, 1998) provides a finegrained enumerative semantic net that is commonly used to tag the instances of English target words in the tasks of SENSEVAL with different senses (WordNet synset numbers). WordNet groups related concepts into synsets and links them through IS-A and PART-OF links, emphasizing the vertical interaction between the concepts that is much paradigmatic. Although WordNet can capture the finegrained paradigmatic relations of words, another typical word relationship, syntagmatic connectedness, is neglected. The syntagmatic relationship, which is often characterized with different POS ta</context>
<context position="7701" citStr="Fellbaum, 1998" startWordPosition="1205" endWordPosition="1206">mains or POS tags. It should be noted that WordNet 2.0 makes some efforts to interrelate nouns and verbs using their derived lexical forms, placing associated words under the same domain. Although some verbs have derived noun forms that can be mapped onto the noun taxonomy, this mapping only relates the morphological forms of verbs, and still lacks syntagmatic links between words. The interrelationship of noun and verb hierarchies is far from complete and only a supplement to the primary IS-A and PART-OF taxonomies in WordNet. Moreover as WordNet generally concerns the paradigmatic relations (Fellbaum, 1998), we have to seek for other lexical knowledge sources to compensate for the shortcomings of WordNet in WSD. The Edinburgh Association Thesaurus2 (EAT) provides an associative network to account for word relationship in human cognition after collecting the first response words for the stimulus words list (Kiss et al., 1973). Take the words eat and food for example. There is no direct path between the concepts of these two words in the taxonomy of WordNet (both as noun and verb), except in the gloss of the first and third sense of eat to explain ‘take in solid food’, or ‘take in food’, which glo</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998). Wordnet: An Electronic Lexical Database. Cambridge MA, USA, The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English.</booktitle>
<location>London, London:Longman.</location>
<contexts>
<context position="4144" citStr="Halliday and Hasan (1976)" startWordPosition="644" endWordPosition="647">s, pages 929–936, Sydney, July 2006. c�2006 Association for Computational Linguistics of all combinations of the shortest distances among target and context words. Pedersen et al. (2003) extended Lesk’s definition method (1986) to discriminate word sense through the definitions of both target and its IS-A relatives, and achieved a better result in the English lexical sample task of SENSEVAL-2, compared with other edge-counting or statistical estimation metrics on WordNet. Humans carefully select words in a sentence to express harmony or cohesion in order to ease the ambiguity of the sentence. Halliday and Hasan (1976) argued that cohesive chains unite text structure together through reiteration of reference and lexical semantic relations (superordinate and subordinate). Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. They argued that lexical chains, which cover the multiple semantic relations (systematic and nonsystematic), can transform the context setting into the computational one to narrow down the specific meaning of the target, manually realizing this with the help of Roget’s Thes</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M. A. K. and R. Hasan (1976). Cohesion in English. London, London:Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms.</title>
<date>1997</date>
<journal>Wordnet. C. Fellbaum.</journal>
<publisher>The Mit Press.</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="4912" citStr="Hirst and St-Onge (1997)" startWordPosition="758" endWordPosition="761">bordinate). Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. They argued that lexical chains, which cover the multiple semantic relations (systematic and nonsystematic), can transform the context setting into the computational one to narrow down the specific meaning of the target, manually realizing this with the help of Roget’s Thesaurus. They defined a lexical chain within Roget’s very general hierarchy, in which lexical relationships are traced through a common category. Hirst and St-Onge (1997) define a lexical chain using the syn/antonym and hyper/hyponym links of WordNet to detect and correct malapropisms in context, in which they specified three different weights from extra-strong to medium strong to score word similarity to decide the inserting sequence in the lexical chain. They first computationally employed WordNet to form a “greedy” lexical chain as a substitute of the context to solve the matter of malapropism, where the word sense is decided by its preceding words. Around the same time, Barzilay and Elhadad (1997) realized a “non-greedy” lexical chain, which determined the</context>
</contexts>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>Hirst, G. and D. St-Onge (1997). Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms. Wordnet. C. Fellbaum. Cambridge, MA, The Mit Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Véronis</author>
</authors>
<title>Word Sense Disambiguation: The State of the Art.</title>
<date>1998</date>
<journal>Computational linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="1101" citStr="Ide and Véronis (1998)" startWordPosition="160" endWordPosition="163">ation Thesaurus (EAT) to transform contextual space for computing syntagmatic and other domain relations with the target word. Without any back-off policy the result on the English lexical sample of SENSEVAL-21 shows that lexical cohesion based on edge-counting techniques is a good way of unsupervisedly disambiguating senses. 1 Introduction Word Sense Disambiguation (WSD) is generally taken as an intermediate task like part-of-speech (POS) tagging in natural language processing, but it has not so far achieved the sufficient precision for application as POS tagging (for the history of WSD, cf. Ide and Véronis (1998)). It is partly due to the nature of its complexity and difficulty, and to the widespread disagreement and controversy on its necessity in language engineering, and to the representation of the senses of words, as well as to the validity of its evaluation (Kilgarriff and Palmer, 2000). However the endeavour to automatically achieve WSD has been continuous since the earliest work of the 1950’s. In this paper we specifically investigate the role of semantic hierarchies of lexical knowledge on WSD, using datasets and evaluation methods from SENSEVAL (Kilgarriff and Rosenzweig, 1 http://www.sensev</context>
</contexts>
<marker>Ide, Véronis, 1998</marker>
<rawString>Ide, N. and J. Véronis (1998). Word Sense Disambiguation: The State of the Art. Computational linguistics 24(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>In the 10th International Conference on Research in Computational Linguistics (ROCLING),</booktitle>
<contexts>
<context position="23217" citStr="Jiang and Conrath (1997)" startWordPosition="4004" endWordPosition="4007"> noun and verb results are respectively demonstrated in Figure 3 and 4. 6 Comparison with other techniques. 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 accuracy Figure 5: comparisons of HWL and HSL with other unsupervised systems and similarity metrics Pedersen et al. (2003) in the work of evaluating different similarity techniques based on WordNet, realized two variants of Lesk’s methods: extended gloss overlaps (P&amp;L_extend) and gloss vector (P&amp;L_vector), as well as evaluating them in the English lexical sample of SENSEVAL-2. The best edge-counting-based metric that they measured are from Jiang and Conrath (1997) (J&amp;C). 5.2 Transformed context (EAT) 0.47 0.45 0.43 0.41 0.39 0.37 0.35 0.33 0.31 0.29 0.27 0.25 accuracy HSN HSM HSA HSS HWL HSL accuracy 0.39 0.37 0.35 0.33 0.31 0.29 0.27 0.25 0.23 0.21 0.19 0.17 0.15 0.13 0.11 0.09 0.07 0.05 HSN HSM HSA HSS HWL HSL HSL_SRORRS HWL_SRORRS IIT 2 IIT 1 DIMAP UNED-LS-U HSL_Context HWL_Context P&amp;L_extend P&amp;L_vector J&amp;C Baseline Lesk Def Baseline Lesk Baseline Random noun verb 934 Accordingly, without the transformation of EAT, we compare our results of HWL and HSL (denoted as HWL_Context and HSL_Context) with the above methods (picking up their optimal values).</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jiang, J. and D. Conrath (1997). Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In the 10th International Conference on Research in Computational Linguistics (ROCLING), Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
</authors>
<title>Introduction, Special Issue on Senseval: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities 34(1-2):</title>
<date>2000</date>
<pages>1--13</pages>
<contexts>
<context position="1386" citStr="Kilgarriff and Palmer, 2000" startWordPosition="209" endWordPosition="212">ood way of unsupervisedly disambiguating senses. 1 Introduction Word Sense Disambiguation (WSD) is generally taken as an intermediate task like part-of-speech (POS) tagging in natural language processing, but it has not so far achieved the sufficient precision for application as POS tagging (for the history of WSD, cf. Ide and Véronis (1998)). It is partly due to the nature of its complexity and difficulty, and to the widespread disagreement and controversy on its necessity in language engineering, and to the representation of the senses of words, as well as to the validity of its evaluation (Kilgarriff and Palmer, 2000). However the endeavour to automatically achieve WSD has been continuous since the earliest work of the 1950’s. In this paper we specifically investigate the role of semantic hierarchies of lexical knowledge on WSD, using datasets and evaluation methods from SENSEVAL (Kilgarriff and Rosenzweig, 1 http://www.senseval.org/ 2000) as these are well known and accepted in the community of computational linguistics. With respect to whether or not they employ the training materials provided, SENSEVAL roughly categorizes the participating systems into “unsupervised systems” and “supervised systems”. Th</context>
<context position="13559" citStr="Kilgarriff and Palmer, 2000" startWordPosition="2219" endWordPosition="2222">bly would assign different values and might achieve better results. Note that for both nouns and verbs we employ RHE (rich hierarchy exploration) with λ = 2 making full use of the taxonomy of WordNet and making no use of glosses. 4.4 How to setup the selection standard for the senses Other than making the most of WSD results, our main motive for this paper is to explore to what extent the semantic relationships will reach accuracy, and to fully acknowledge the contribution of this single attribute working on WSD, which is encouraged by SENSEVAL in order to gain further benefits in this field (Kilgarriff and Palmer, 2000). Without any definition, which is previously surveyed by Lesk (1986) and Pedersen et al. (2003), we screen off the definition factor in the metric of verb similarity, with the intention of focusing on the taxonomies of WordNet. Assuming that the lexical hub for the right sense would maximize the cohesion with other words in the discourse, we design six different strategies to calculate the lexical hub in its unordered contextual surroundings. We first put forward three metrics to measure up the similarity of the senses of the target and the context word: • The maximized sense similarity (Sim(</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Kilgarriff, A. and M. Palmer (2000). Introduction, Special Issue on Senseval: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities 34(1-2): 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<date>2000</date>
<booktitle>Framework and Results for English Senseval. Computers and the Humanities 34(1-2):</booktitle>
<pages>15--48</pages>
<contexts>
<context position="23963" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="4128" endWordPosition="4131"> HSA HSS HWL HSL accuracy 0.39 0.37 0.35 0.33 0.31 0.29 0.27 0.25 0.23 0.21 0.19 0.17 0.15 0.13 0.11 0.09 0.07 0.05 HSN HSM HSA HSS HWL HSL HSL_SRORRS HWL_SRORRS IIT 2 IIT 1 DIMAP UNED-LS-U HSL_Context HWL_Context P&amp;L_extend P&amp;L_vector J&amp;C Baseline Lesk Def Baseline Lesk Baseline Random noun verb 934 Accordingly, without the transformation of EAT, we compare our results of HWL and HSL (denoted as HWL_Context and HSL_Context) with the above methods (picking up their optimal values). The results are illustrated in Figure 5. At the same time we also list three baselines for unsupervised systems (Kilgarriff and Rosenzweig, 2000), which are Baseline Random (randomly selecting one sense of the target), Baseline Lesk (overlapping between the examples and definitions of and unsupervised systems in SENSEVAL-2 each sense of the target and context words), and its reduced version, i.e. Baseline Lesk Def (only definition). We further compare HWL and HSL with the intervention of SRORRS of EAT (denoted as HWL_SRORRS and HSL_ SRORRS) with other unsupervised systems that employ no training materials of SENSEVAL-2, which are respectively: • IIT 1 and IIT 2: extended the WordNet gloss of each sense of the target, along with its sup</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Kilgarriff, A. and J. Rosenzweig (2000). Framework and Results for English Senseval. Computers and the Humanities 34(1-2): 15-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Kiss</author>
</authors>
<title>The Associative Thesaurus of English and Its Computer Analysis.</title>
<date>1973</date>
<publisher>Edinburgh, University Press.</publisher>
<marker>Kiss, 1973</marker>
<rawString>Kiss, G. R., et al. (1973). The Associative Thesaurus of English and Its Computer Analysis. Edinburgh, University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Code from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>In the 5th annual international conference on systems documentation,</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="13628" citStr="Lesk (1986)" startWordPosition="2231" endWordPosition="2232"> nouns and verbs we employ RHE (rich hierarchy exploration) with λ = 2 making full use of the taxonomy of WordNet and making no use of glosses. 4.4 How to setup the selection standard for the senses Other than making the most of WSD results, our main motive for this paper is to explore to what extent the semantic relationships will reach accuracy, and to fully acknowledge the contribution of this single attribute working on WSD, which is encouraged by SENSEVAL in order to gain further benefits in this field (Kilgarriff and Palmer, 2000). Without any definition, which is previously surveyed by Lesk (1986) and Pedersen et al. (2003), we screen off the definition factor in the metric of verb similarity, with the intention of focusing on the taxonomies of WordNet. Assuming that the lexical hub for the right sense would maximize the cohesion with other words in the discourse, we design six different strategies to calculate the lexical hub in its unordered contextual surroundings. We first put forward three metrics to measure up the similarity of the senses of the target and the context word: • The maximized sense similarity (Sim(Tk , Ci, j )) where T denotes the target, Tk is the kth sense of the </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, M. (1986). Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Code from an Ice Cream Cone. In the 5th annual international conference on systems documentation, ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text.</title>
<date>1991</date>
<journal>Computational linguistics</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="4323" citStr="Morris and Hirst (1991)" startWordPosition="667" endWordPosition="670">2003) extended Lesk’s definition method (1986) to discriminate word sense through the definitions of both target and its IS-A relatives, and achieved a better result in the English lexical sample task of SENSEVAL-2, compared with other edge-counting or statistical estimation metrics on WordNet. Humans carefully select words in a sentence to express harmony or cohesion in order to ease the ambiguity of the sentence. Halliday and Hasan (1976) argued that cohesive chains unite text structure together through reiteration of reference and lexical semantic relations (superordinate and subordinate). Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. They argued that lexical chains, which cover the multiple semantic relations (systematic and nonsystematic), can transform the context setting into the computational one to narrow down the specific meaning of the target, manually realizing this with the help of Roget’s Thesaurus. They defined a lexical chain within Roget’s very general hierarchy, in which lexical relationships are traced through a common category. Hirst and St-Onge (1997) define a l</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, J. and G. Hirst (1991). Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text. Computational linguistics 17(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.</title>
<date>2003</date>
<marker>Pedersen, 2003</marker>
<rawString>Pedersen, T., et al. (2003). Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sinopalnikova</author>
</authors>
<title>Word Association Thesaurus as a Resource for Building Wordnet. In</title>
<date>2004</date>
<booktitle>GWC</booktitle>
<contexts>
<context position="9231" citStr="Sinopalnikova (2004)" startWordPosition="1465" endWordPosition="1466">nant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers. In the case of verbs association tests, it is also reported that more than half the response words of verbs (the stimuli) are syntagmatically related (Fellbaum, 1998). In experiments of examining the psychological plausibility of WordNet relationships, Chaffin et al. (1994) stated that only 30.4% of the responses of 75 verb stimuli belongs to verbs, and more than half of the responses are nouns, of which nearly 90% are categorized as the arguments of the verbs. Sinopalnikova (2004) also reported that there are multiple relationships found in word association thesaurus, such as syntagmatic, paradigmatic relations, domain information etc. In this paper we only use the straightforward forms of context words separating the effect of syntactic dependence on the WSD. As a supplement of enriching word linkage in the WSD, we retrieve the lexical knowledge from both WordNet and EAT. We first explore the function of semantic hierarchies of WordNet on WSD, and then we transform the context word with EAT to investigate whether other relationships can improve WSD. 4 System design In</context>
</contexts>
<marker>Sinopalnikova, 2004</marker>
<rawString>Sinopalnikova, A. (2004). Word Association Thesaurus as a Resource for Building Wordnet. In GWC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sussna</author>
</authors>
<title>Word Sense Disambiguation for Free-Text Indexing Using a Massive Semantic Network.</title>
<date>1993</date>
<booktitle>In CKIM&apos;93.</booktitle>
<contexts>
<context position="3095" citStr="Sussna (1993)" startWordPosition="479" endWordPosition="480"> a quantitative cue for identifying the true sense of the target. The lexical ambiguity of polysemy and homonymy, whose distinction is however not absolute as sometimes the senses of word may be intermediate, is the main object of WSD. Verbs, with their more flexible roles in a sentence, tend to be more polysemous than nouns, so worsening the computational feasibility. In this paper we disambiguated the sense of a word after its POS tagging has assigned them either a noun or a verb tag. Furthermore, we deal with nouns and verbs separately. 2 Some previous work on WSD using semantic similarity Sussna (1993) utilized the semantic network of nouns in WordNet to disambiguate term senses to improve the precision of SMART information retrieval at the stage of indexing, in which he assigned two different weights for both directions of edges in the network to compute the similarity of two nodes. He then exploited the moving fixed size window to minimize the sum 929 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 929–936, Sydney, July 2006. c�2006 Association for Computational Linguistics of all combinations of the shortest distances among target and context words. Pedersen et </context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>Sussna, M. (1993). Word Sense Disambiguation for Free-Text Indexing Using a Massive Semantic Network. In CKIM&apos;93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>D M W Powers</author>
</authors>
<title>Measuring Semantic Similarity in the Taxonomy of Wordnet.</title>
<date>2005</date>
<booktitle>In the Twenty-Eighth Australasian Computer Science Conference (ACSC2005),</booktitle>
<location>Newcastle, Australia, ACS.</location>
<contexts>
<context position="9981" citStr="Yang and Powers (2005" startWordPosition="1585" endWordPosition="1588">ions, domain information etc. In this paper we only use the straightforward forms of context words separating the effect of syntactic dependence on the WSD. As a supplement of enriching word linkage in the WSD, we retrieve the lexical knowledge from both WordNet and EAT. We first explore the function of semantic hierarchies of WordNet on WSD, and then we transform the context word with EAT to investigate whether other relationships can improve WSD. 4 System design In order to find semantically related words to cohesively form lexical hubs, we first employ the two word similarity algorithms of Yang and Powers (2005; 2006) that use WordNet to compute noun similarity and verb similarity respectively. We next construct the lexical hub for each target sense to assemble the similarity score between the target and its context words together. The maximum score of these lexical hubs specifically predicts the real sense of the target, also implicitly captures the cohesion and real meaning of the word in its context. 4.1 Similarity metrics on nouns Yang and Powers (2005) designed a metric, Sim (c1 , c2) = αt * utilizing both IS-A and PART-OF taxonomies of WordNet to measure noun similarity, and they argued that t</context>
</contexts>
<marker>Yang, Powers, 2005</marker>
<rawString>Yang, D. and D. M. W. Powers (2005). Measuring Semantic Similarity in the Taxonomy of Wordnet. In the Twenty-Eighth Australasian Computer Science Conference (ACSC2005), Newcastle, Australia, ACS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>D M W Powers</author>
</authors>
<title>Verb Similarity on the Taxonomy of Wordnet.</title>
<date>2006</date>
<booktitle>In the 3rd International WordNet Conference (GWC-06),</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="11296" citStr="Yang and Powers (2006)" startWordPosition="1812" endWordPosition="1815">ilarity (Sim) of two concepts (c1 and c2) with a link type factor (αt) to specify the weights of different link types (t) (syn/antonym, hyper/ hyponym, and holo/meronym) in the WordNet, and a path type factor (Rt) to reduce the uniform distance of the single link, along with a depth factor (k) to restrict the maximum searching distance between concepts. Since their metric on noun similarity is significantly better than some popular measures and even outperforms some subjects on a standard data set, we selected it as a measure on noun similarity in our WSD task. 4.2 Similarity metrics on verbs Yang and Powers (2006) also redesigned their noun model, Dist( Sim c c ( 1, 2) = αstr * αt * ∏ β i=1 to accommodate verb case, which is harder to deal with in the shallow and incomplete taxonomy of verbs in WordNet. As an enhancement to the uniqueness of verb similarity they also consider three fall-back factors, where if αstr is 1 normally but successively falls back to: • αstm: the verb stem polysemy ignoring sense and form • αder: the cognate noun hierarchy of the verb • αgls: the definition of the verb They also defined two alternate search protocols: rich hierarchy exploration (RHE) with no more than six links</context>
</contexts>
<marker>Yang, Powers, 2006</marker>
<rawString>Yang, D. and D. M. W. Powers (2006). Verb Similarity on the Taxonomy of Wordnet. In the 3rd International WordNet Conference (GWC-06), Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Models of Roget&apos;s Categories Trained on Large Corpora.</title>
<date>1992</date>
<booktitle>In the 14th International Conference on Computational Linguistics, Nates,</booktitle>
<contexts>
<context position="19313" citStr="Yarowsky, 1992" startWordPosition="3327" endWordPosition="3328">d filtering stop-words) or the finegrained ones (syntactic role, selection preference etc.) can provide cues for the target. We propose to merely use a bag of words to feed into each heuristic in case of losing any valuable information in the disambiguation, and preventing from any interference of other clues except the semantic hierarchy of WordNet. The size of the context is not a definitive factor in WSD, Yarowsky (1993) suggested the size of 3 or 4 words for the local ambiguity and 20/50 words for topic ambiguity. He also employed Roget’s Thesaurus in 100 words of window to implement WSD (Yarowsky, 1992). To investigate the role of local context and topic context we vary the size of window from one word distance away to the target (left and right) until 100 words away in nouns or 60 in verbs, until there are no increases in the context of each instance. Figure 1: the result of noun disambiguation with different size of context in SENSEVAL 2 Figure 2: the result of verb disambiguation with different size of context in SENSEVAL 2 Noun and verb disambiguation results are respectively displayed in Figure 1 and 2. Since the performance curves of the heuristics turned into flat and stable (the aver</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, D. (1992). Word Sense Disambiguation Using Statistical Models of Roget&apos;s Categories Trained on Large Corpora. In the 14th International Conference on Computational Linguistics, Nates, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One Sense Per Collocation.</title>
<date>1993</date>
<booktitle>In ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton, New Jersey.</location>
<contexts>
<context position="8558" citStr="Yarowsky (1993)" startWordPosition="1354" endWordPosition="1355">ollecting the first response words for the stimulus words list (Kiss et al., 1973). Take the words eat and food for example. There is no direct path between the concepts of these two words in the taxonomy of WordNet (both as noun and verb), except in the gloss of the first and third sense of eat to explain ‘take in solid food’, or ‘take in food’, which glosses are not regularly or care2 http://www.eat.rl.ac.uk/ 930 fully organized in WordNet. However in EAT eat is strongly associated with food, and when taking eat as a stimulus word, 45 out of 100 subjects regarded food as the first response. Yarowsky (1993) indicated that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers. In the case of verbs association tests, it is also reported that more than half the response words of verbs (the stimuli) are syntagmatically related (Fellbaum, 1998). In experiments of examining the psychological plausibility of WordNet relationships, Chaffin et al. (1994) stated that only 30.4% of the responses of 75 verb stimuli belongs to verbs, and more than half of the responses are nouns, of which nearl</context>
<context position="19125" citStr="Yarowsky (1993)" startWordPosition="3294" endWordPosition="3295">nowledge of domain, frequency and pragmatics to guess, word context is the only way of labeling the real meaning of word. Basically a bag of context words (after morphological analyzing and filtering stop-words) or the finegrained ones (syntactic role, selection preference etc.) can provide cues for the target. We propose to merely use a bag of words to feed into each heuristic in case of losing any valuable information in the disambiguation, and preventing from any interference of other clues except the semantic hierarchy of WordNet. The size of the context is not a definitive factor in WSD, Yarowsky (1993) suggested the size of 3 or 4 words for the local ambiguity and 20/50 words for topic ambiguity. He also employed Roget’s Thesaurus in 100 words of window to implement WSD (Yarowsky, 1992). To investigate the role of local context and topic context we vary the size of window from one word distance away to the target (left and right) until 100 words away in nouns or 60 in verbs, until there are no increases in the context of each instance. Figure 1: the result of noun disambiguation with different size of context in SENSEVAL 2 Figure 2: the result of verb disambiguation with different size of c</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, D. (1993). One Sense Per Collocation. In ARPA Human Language Technology Workshop, Princeton, New Jersey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>