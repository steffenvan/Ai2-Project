<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.872353">
A Tree Sequence Alignment-based Tree-to-Tree Translation Model
</title>
<author confidence="0.981565">
Min Zhang1 Hongfei Jiang2 Aiti Aw1 Haizhou Li1 Chew Lim Tan3 and Sheng Li2
</author>
<affiliation confidence="0.952274">
1Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singapore
</affiliation>
<email confidence="0.733677333333333">
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg
aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn
hli@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.990596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.987276315789474">
This paper presents a translation model that is
based on tree sequence alignment, where a tree
sequence refers to a single sequence of sub-
trees that covers a phrase. The model leverages
on the strengths of both phrase-based and lin-
guistically syntax-based method. It automati-
cally learns aligned tree sequence pairs with
mapping probabilities from word-aligned bi-
parsed parallel texts. Compared with previous
models, it not only captures non-syntactic
phrases and discontinuous phrases with lin-
guistically structured features, but also sup-
ports multi-level structure reordering of tree
typology with larger span. This gives our
model stronger expressive power than other re-
ported models. Experimental results on the
NIST MT-2005 Chinese-English translation
task show that our method statistically signifi-
cantly outperforms the baseline systems.
</bodyText>
<sectionHeader confidence="0.998856" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9984571">
Phrase-based modeling method (Koehn et al.,
2003; Och and Ney, 2004a) is a simple, but power-
ful mechanism to machine translation since it can
model local reorderings and translations of multi-
word expressions well. However, it cannot handle
long-distance reorderings properly and does not
exploit discontinuous phrases and linguistically
syntactic structure features (Quirk and Menezes,
2006). Recently, many syntax-based models have
been proposed to address the above deficiencies
</bodyText>
<note confidence="0.7848044">
(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and
Palmer, 2005; Quirk et al, 2005; Cowan et al.,
2006; Zhang et al., 2007; Bod, 2007; Yamada and
Knight, 2001; Liu et al., 2006; Liu et al., 2007;
Gildea, 2003; Poutsma, 2000; Hearne and Way,
</note>
<bodyText confidence="0.999946771428571">
2003). Although good progress has been reported,
the fundamental issues in applying linguistic syn-
tax to SMT, such as non-isomorphic tree align-
ment, structure reordering and non-syntactic phrase
modeling, are still worth well studying.
In this paper, we propose a tree-to-tree transla-
tion model that is based on tree sequence align-
ment. It is designed to combine the strengths of
phrase-based and syntax-based methods. The pro-
posed model adopts tree sequence1 as the basic
translation unit and utilizes tree sequence align-
ments to model the translation process. Therefore,
it not only describes non-syntactic phrases with
syntactic structure information, but also supports
multi-level tree structure reordering in larger span.
These give our model much more expressive
power and flexibility than those previous models.
Experiment results on the NIST MT-2005 Chinese-
English translation task show that our method sig-
nificantly outperforms Moses (Koehn et al., 2007),
a state-of-the-art phrase-based SMT system, and
other linguistically syntax-based methods, such as
SCFG-based and STSG-based methods (Zhang et
al., 2007). In addition, our study further demon-
strates that 1) structure reordering rules in our
model are very useful for performance improve-
ment while discontinuous phrase rules have less
contribution and 2) tree sequence rules are able to
model non-syntactic phrases with syntactic struc-
ture information, and thus contribute much to the
performance improvement, but those rules consist-
ing of more than three sub-trees have almost no
contribution.
The rest of this paper is organized as follows:
Section 2 reviews previous work. Section 3 elabo-
</bodyText>
<footnote confidence="0.975489666666667">
1 A tree sequence refers to an ordered sub-tree sequence that
covers a phrase or a consecutive tree fragment in a parse tree.
It is the same as the concept “forest” used in Liu et al (2007).
</footnote>
<page confidence="0.943474">
559
</page>
<note confidence="0.721872">
Proceedings of ACL-08: HLT, pages 559–567,
</note>
<page confidence="0.538574">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999609">
rates the modelling process while Sections 4 and 5
discuss the training and decoding algorithms. The
experimental results are reported in Section 6. Fi-
nally, we conclude our work in Section 7.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.947602416666666">
Many techniques on linguistically syntax-based
SMT have been proposed in literature. Yamada
and Knight (2001) use noisy-channel model to
transfer a target parse tree into a source sentence.
Eisner (2003) studies how to learn non-isomorphic
tree-to-tree/string mappings using a STSG. Ding
and Palmer (2005) propose a syntax-based transla-
tion model based on a probabilistic synchronous
dependency insertion grammar. Quirk et al. (2005)
propose a dependency treelet-based translation
model. Cowan et al. (2006) propose a feature-
based discriminative model for target language
syntactic structures prediction, given a source
parse tree. Huang et al. (2006) study a TSG-based
tree-to-string alignment model. Liu et al. (2006)
propose a tree-to-string model. Zhang et al.
(2007b) present a STSG-based tree-to-tree transla-
tion model. Bod (2007) reports that the unsuper-
vised STSG-based translation model performs
much better than the supervised one. The motiva-
tion behind all these work is to exploit linguistical-
ly syntactic structure features to model the
translation process. However, most of them fail to
utilize non-syntactic phrases well that are proven
useful in the phrase-based methods (Koehn et al.,
2003).
The formally syntax-based model for SMT was
first advocated by Wu (1997). Xiong et al. (2006)
propose a MaxEnt-based reordering model for
BTG (Wu, 1997) while Setiawan et al. (2007) pro-
pose a function word-based reordering model for
BTG. Chiang (2005)’s hierarchal phrase-based
model achieves significant performance improve-
ment. However, no further significant improve-
ment is achieved when the model is made sensitive
to syntactic structures by adding a constituent fea-
ture (Chiang, 2005).
In the last two years, many research efforts were
devoted to integrating the strengths of phrase-
based and syntax-based methods. In the following,
we review four representatives of them.
1) Hassan et al. (2007) integrate supertags (a
kind of lexicalized syntactic description) into the
target side of translation model and language mod-
el under the phrase-based translation framework,
resulting in good performance improvement. How-
ever, neither source side syntactic knowledge nor
reordering model is further explored.
</bodyText>
<listItem confidence="0.9116455">
2) Galley et al. (2006) handle non-syntactic
phrasal translations by traversing the tree upwards
until a node that subsumes the phrase is reached.
This solution requires larger applicability contexts
(Marcu et al., 2006). However, phrases are utilized
independently in the phrase-based method without
depending on any contexts.
3) Addressing the issues in Galley et al. (2006),
Marcu et al. (2006) create an xRS rule headed by a
pseudo, non-syntactic non-terminal symbol that
subsumes the phrase and its corresponding multi-
headed syntactic structure; and one sibling xRS
rule that explains how the pseudo symbol can be
combined with other genuine non-terminals for
acquiring the genuine parse trees. The name of the
pseudo non-terminal is designed to reflect the full
realization of the corresponding rule. The problem
in this method is that it neglects alignment consis-
tency in creating sibling rules and the naming me-
chanism faces challenges in describing more
complicated phenomena (Liu et al., 2007).
4) Liu et al. (2006) treat all bilingual phrases as
lexicalized tree-to-string rules, including those
non-syntactic phrases in training corpus. Although
</listItem>
<bodyText confidence="0.982187913043478">
the solution shows effective empirically, it only
utilizes the source side syntactic phrases of the in-
put parse tree during decoding. Furthermore, the
translation probabilities of the bilingual phrases
and other tree-to-string rules are not compatible
since they are estimated independently, thus hav-
ing different parameter spaces. To address the
above problems, Liu et al. (2007) propose to use
forest-to-string rules to enhance the expressive
power of their tree-to-string model. As is inherent
in a tree-to-string framework, Liu et al.’s method
defines a kind of auxiliary rules to integrate forest-
to-string rules into tree-to-string models. One prob-
lem of this method is that the auxiliary rules are
not described by probabilities since they are con-
structed during decoding, rather than learned from
the training corpus. So, to balance the usage of dif-
ferent kinds of rules, they use a very simple feature
counting the number of auxiliary rules used in a
derivation for penalizing the use of forest-to-string
and auxiliary rules.
In this paper, an alternative solution is presented
to combine the strengths of phrase-based and syn-
</bodyText>
<page confidence="0.983921">
560
</page>
<figure confidence="0.934197222222222">
3 Tree Sequence Alignment Model
3.1 Tree Sequence Translation Rule
T f
( 1 )
J
A
T e
( 1 )
I
</figure>
<figureCaption confidence="0.999881">
Figure 1: A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation
Figure 2: Two Examples of tree sequences
Figure 3: Two examples of translation rules
</figureCaption>
<bodyText confidence="0.996741307692308">
tax-based methods. Unlike previous work, our so-
lution neither requires larger applicability contexts
(Galley et al., 2006), nor depends on pseudo nodes
(Marcu et al., 2006) or auxiliary rules (Liu et al.,
2007). We go beyond the single sub-tree mapping
model to propose a tree sequence alignment-based
translation model. To the best of our knowledge,
this is the first attempt to empirically explore the
tree sequence alignment based model in SMT.
The leaf nodes of a sub-tree in a tree sequence can
be either non-terminal symbols (grammar tags) or
terminal symbols (lexical words). Given a pair of
source and target parse trees (1 )
</bodyText>
<equation confidence="0.683565666666667">
T f and ( 1 )
J T e in
I
</equation>
<bodyText confidence="0.55201975">
Fig. 1, Fig. 2 illustrates two examples of tree se-
quences derived from the two parse trees. A tree
sequence translation rule r is a pair of aligned tree
sequences r =&lt; TS f j ,
</bodyText>
<equation confidence="0.995063">
2 i
( ) TS e i , A% &gt;, where:
j ( )
2
1 1
</equation>
<listItem confidence="0.947717888888889">
• TS (fj j1) is a source tree sequence, covering
the span [ j1 , j2 ] in (1 )
T f , and
J
• TS(eal ) is a target one, covering the span
[ i1, i2 ] in ( 1 )
T e , and
I
• A are the alignments between leaf nodes of
</listItem>
<bodyText confidence="0.983096428571429">
two tree sequences, satisfying the following
condition: `d (i, j) E A : i1 &lt; i &lt; i2 H j1 &lt; j &lt; j2 .
Fig. 3 shows two rules extracted from the tree pair
shown in Fig. 1, where r1 is a tree-to-tree rule and
r2 is a tree sequence-to-tree sequence rule. Ob-
viously, tree sequence rules are more powerful
than phrases or tree rules as they can capture all
phrases (including both syntactic and non-syntactic
phrases) with syntactic structure information and
allow any tree node operations in a longer span.
We expect that these properties can well address
the issues of non-isomorphic structure alignments,
structure reordering, non-syntactic phrases and
discontinuous phrases translations.
</bodyText>
<subsectionHeader confidence="0.999187">
3.2 Tree Sequence Translation Model
</subsectionHeader>
<bodyText confidence="0.999278">
Given the source and target sentences f1J and e;
and their parse trees (1 )
</bodyText>
<equation confidence="0.951298333333333">
T f and ( 1 )
J T e , the tree
I
</equation>
<bodyText confidence="0.850616">
sequence-to-tree sequence translation model is
formulated as:
</bodyText>
<equation confidence="0.994291733333333">
Pr
(e1I  |fJ) _ ∑ Pr(e, , T(e; ), T(.fiJ)  |fJ )
T f T e
( ), ( )
J I
1 1
∑ (Pr(T (.f J)  |.f J) (1)
T f T e
( ), ( )
J I
1 1
� Pr
(T(e1I)  |T(.fJ ), .fJ)
(e,  |T(e, ),T(.f,J),.fJ))
� Pr
</equation>
<bodyText confidence="0.841633">
In our implementation, we have:
</bodyText>
<page confidence="0.973883">
561
</page>
<listItem confidence="0.9980906">
1) Pr(T (fJ)  |fJ) ≡1 since we only use the best
source and target parse tree pairs in training.
2) Pr(ei  |T (e; ), T (fJ ), fJ) ≡1 since we just
output the leaf nodes ofT (e;) to generate e;
regardless of source side information.
</listItem>
<equation confidence="0.965776714285714">
Since (1 )
T f contains the information of 1
J f ,
J
now we have:
Pr (e1I  |fJ) = Pr(T(4)  |T( J), J)
(T (e1I)  |T (ffJ )) (2)
</equation>
<bodyText confidence="0.89890575">
By Eq. (2), translation becomes a tree structure
mapping issue. We model it using our tree se-
quence-based translation rules. Given the source
parse tree (1 )
</bodyText>
<equation confidence="0.7211185">
T f , there are multiple derivations
J
</equation>
<bodyText confidence="0.995167333333333">
that could lead to the same target tree T(e;) , the
mapping probability Pr (T (e;)  |T (f J )) is obtained
by summing over the probabilities of all deriva-
tions. The probability of each derivationθ is given
as the product of the probabilities of all the rules
p(ri ) used in the derivation (here we assume that
</bodyText>
<equation confidence="0.9708322">
a rule is applied independently in a derivation).
Pr (e1I  |fJ) = Pr(T (e1I)  |T (f J ))
= ∑ ∏ p(r :&lt; TS(eh),TS(fj;2 ), A &gt;) (3)
θ ∈θ
r i
</equation>
<bodyText confidence="0.998062888888889">
Eq. (3) formulates the tree sequence alignment-
based translation model. Figs. 1 and 3 show how
the proposed model works. First, the source sen-
tence is parsed into a source parse tree. Next, the
source parse tree is detached into two source tree
sequences (the left hand side of rules in Fig. 3).
Then the two rules in Fig. 3 are used to map the
two source tree sequences to two target tree se-
quences, which are then combined to generate a
target parse tree. Finally, a target translation is
yielded from the target tree.
Our model is implemented under log-linear
framework (Och and Ney, 2002). We use seven
basic features that are analogous to the commonly
used features in phrase-based systems (Koehn,
2004): 1) bidirectional rule mapping probabilities;
2) bidirectional lexical rule translation probabilities;
3) the target language model; 4) the number of
rules used and 5) the number of target words. In
addition, we define two new features: 1) the num-
ber of lexical words in a rule to control the model’s
preference for lexicalized rules over un-lexicalized
rules and 2) the average tree depth in a rule to bal-
ance the usage of hierarchical rules and flat rules.
Note that we do not distinguish between larger (tal-
ler) and shorter source side tree sequences, i.e. we
let these rules compete directly with each other.
</bodyText>
<sectionHeader confidence="0.952356" genericHeader="method">
4 Rule Extraction
</sectionHeader>
<bodyText confidence="0.803495666666667">
Rules are extracted from word-aligned, bi-parsed
sentence pairs &lt; T (fJ ), T (e; ), A &gt; , which are
classified into two categories:
</bodyText>
<listItem confidence="0.99835025">
• initial rule, if all leaf nodes of the rule are
terminals (i.e. lexical word), and
• abstract rule, otherwise, i.e. at least one leaf
node is a non-terminal (POS or phrase tag).
</listItem>
<figure confidence="0.806290142857143">
Given an initial rule &lt; TS f j TS e i
( ), ( ),
j 2 i 2
1 1
its sub initial rule is defined as a triple
ˆ
ˆ
4 i• &lt; TS f j TS ei
( 3 ), ( 3 ),
j 4 A &gt; is an initial rule.
%
• ∀(i,j)∈A:i3≤i≤i4↔j3≤j≤j4 , i.e.
ˆ
%
A⊆A
• TS( f j34) is a sub-graph of TS(f jj2 ) while
%
an initial rule, where A are alignments between
&lt; TS f j TS ei A &gt; if and only if:
( 3 ), ( 3 ),
j 4 i 4
TS e i is a sub-graph of 2
4 i
( ) ( )
i TS e i .
3 1
Rules are extracted in two steps:
1) Extracting initial rules first.
</figure>
<bodyText confidence="0.945251428571429">
2) Extracting abstract rules from extracted ini-
tial rules with the help of sub initial rules.
It is straightforward to extract initial rules. We
first generate all fully lexicalized source and target
tree sequences using a dynamic programming algo-
rithm and then iterate over all generated source and
target tree sequence pairs &lt; TS f j TS e i &gt; . If
</bodyText>
<equation confidence="0.76403725">
( ), ( )
j 2 i 2
1 1
the condition “∀(i, j) ∈ A:i1 ≤i ≤i2 ↔ j1 ≤ j ≤ j2”
is satisfied, the triple &lt; TS f j TS e i A % &gt; is
( ), ( ),
j 2 i 2
1 1
</equation>
<bodyText confidence="0.987366777777778">
leaf nodes of TS(f jj2 ) and TS(e1) . We then de-
rive abstract rules from initial rules by removing
one or more of its sub initial rules. The abstract
rule extraction algorithm presented next is imple-
mented using dynamic programming. Due to space
limitation, we skip the details here. In order to con-
trol the number of rules, we set three constraints
for both finally extracted initial and abstract rules:
1) The depth of a tree in a rule is not greater
</bodyText>
<figure confidence="0.7389945">
=
Pr
%
A&gt; ,
</figure>
<page confidence="0.915397">
562
</page>
<bodyText confidence="0.921682235294118">
than h .
2) The number of non-terminals as leaf nodes is
not greater than c .
3) The tree number in a rule is not greater than d.
In addition, we limit initial rules to have at most
seven lexical words as leaf nodes on either side.
However, in order to extract long-distance reorder-
ing rules, we also generate those initial rules with
more than seven lexical words for abstract rules
extraction only (not used in decoding). This makes
our abstract rules more powerful in handling
global structure reordering. Moreover, by configur-
ing these parameters we can implement other
translation models easily: 1) STSG-based model
when d =1 ; 2) SCFG-based model when d =1
and h = 2 ; 3) phrase-based translation model only
(no reordering model) when c = 0 and h =1.
</bodyText>
<listItem confidence="0.876579285714286">
Algorithm 1: abstract rules extraction
Input: initial rule set rini
Output: abstract rule set rabs
1: for each ri e rini , do
2: put all sub initial rules of ri into a set subini
ri
3: for each subset O c ri do
subini
4: if there are spans overlapping between
any two rules in the subset O then
5: continue //go to line 3
6: end if
7: generate an abstract rule by removing
the portions covered by O from ri and
</listItem>
<bodyText confidence="0.769883666666667">
co-indexing the pairs of non-terminals
that rooting the removed source and
target parts
</bodyText>
<listItem confidence="0.886310666666667">
8: add them into the abstract rule set rabs
9: end do
10: end do
</listItem>
<sectionHeader confidence="0.961104" genericHeader="method">
5 Decoding
</sectionHeader>
<equation confidence="0.981461235294118">
Given (1 )
T f , the decoder is to find the best deri-
J
vation B that generates &lt; (1 )
T f , ( 1 )
J T e &gt;.
I
eˆ arg max ( ( )  |( ))
= Pr T e T f
I J
1 1
eI
1
� 11
arg max ( )
p r i
eI ,B
</equation>
<table confidence="0.5759395">
1 rieB
Algorithm 2: Tree Sequence-based Decoder
Data structures:
h[ j1, j2 ] To store translations to a span [ j1 , j2 ]
</table>
<listItem confidence="0.990541625">
1: for s = 0 to J -1 do // s: span length
2: for j1= 1 to J -s , j2= j1+s do
3: for each rule r spanning [ j1, j2] do
4: if r is an initial rule then
5: insert r into h[ j1 , j2 ]
6: else
7: generate new translations from
r by replacing non-terminal leaf
nodes of r with their correspond-
ing spans’ translations that are al-
ready translated in previous steps
8: insert them into h[ j1, j2 ]
9: end if
10: end for
11: end for
12: end for
</listItem>
<bodyText confidence="0.960611368421053">
13: output the hypothesis with the highest score
in h[1, J] as the final best translation
The decoder is a span-based beam search to-
gether with a function for mapping the source deri-
vations to the target ones. Algorithm 2 illustrates
the decoding algorithm. It translates each span ite-
ratively from small one to large one (lines 1-2).
This strategy can guarantee that when translating
the current span, all spans smaller than the current
one have already been translated before if they are
translatable (line 7). When translating a span, if the
usable rule is an initial rule, then the tree sequence
on the target side of the rule is a candidate transla-
tion (lines 4-5). Otherwise, we replace the non-
terminal leaf nodes of the current abstract rule
with their corresponding spans’ translations that
are already translated in previous steps (line 7). To
speed up the decoder, we use several thresholds to
limit search beams for each span:
</bodyText>
<listItem confidence="0.994307333333333">
1) α , the maximal number of rules used
2) �3 , the minimal log probability of rules
3) y , the maximal number of translations yield
</listItem>
<bodyText confidence="0.95541325">
It is worth noting that the decoder does not force
a complete target parse tree to be generated. If no
rules can be used to generate a complete target
parse tree, the decoder just outputs whatever have
</bodyText>
<equation confidence="0.953644">
Input: (1 )
T f J
Output: ( 1 )
T e
I
(4)
</equation>
<page confidence="0.990566">
563
</page>
<bodyText confidence="0.986878166666667">
phrase rules2. Finally, we investigate the impact of
maximal sub-tree number and sub-tree depth in our
model. All of the following discussions are held on
the training and test data.
been translated so far monotonically as one hy-
pothesis.
</bodyText>
<sectionHeader confidence="0.975701" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99943">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999889806451613">
We conducted Chinese-to-English translation ex-
periments. We trained the translation model on the
FBIS corpus (7.2M+9.2M words) and trained a 4-
gram language model on the Xinhua portion of the
English Gigaword corpus (181M words) using the
SRILM Toolkits (Stolcke, 2002) with modified
Kneser-Ney smoothing. We used sentences with
less than 50 characters from the NIST MT-2002
test set as our development set and the NIST MT-
2005 test set as our test set. We used the Stanford
parser (Klein and Manning, 2003) to parse bilin-
gual sentences on the training set and Chinese sen-
tences on the development and test sets. The
evaluation metric is case-sensitive BLEU-4 (Papi-
neni et al., 2002). We used GIZA++ (Och and Ney,
2004) and the heuristics “grow-diag-final” to gen-
erate m-to-n word alignments. For the MER train-
ing (Och, 2003), we modified Koehn’s MER
trainer (Koehn, 2004) for our tree sequence-based
system. For significance test, we used Zhang et al’s
implementation (Zhang et al, 2004).
We set three baseline systems: Moses (Koehn et
al., 2007), and SCFG-based and STSG-based tree-
to-tree translation models (Zhang et al., 2007). For
Moses, we used its default settings. For the
SCFG/STSG and our proposed model, we used the
same settings except for the parameters d and h
(d =1 and h = 2 for the SCFG; d =1 and h = 6 for
the STSG; d = 4 and h = 6 for our model). We
optimized these parameters on the training and de-
velopment sets: c =3, α =20, β =-100 and y =100.
</bodyText>
<subsectionHeader confidence="0.999103">
6.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999811888888889">
We carried out a number of experiments to ex-
amine the proposed tree sequence alignment-based
translation model. In this subsection, we first re-
port the rule distributions and compare our model
with the three baseline systems. Then we study the
model’s expressive ability by comparing the con-
tributions made by different kinds of rules, includ-
ing strict tree sequence rules, non-syntactic phrase
rules, structure reordering rules and discontinuous
</bodyText>
<table confidence="0.9976516">
Initial Rules Abstract Rules
Rule L P U Total
BP 322,965 0 0 322,965
TR 443,010 144,459 24,871 612,340
TSR 225,570 103,932 714 330,216
</table>
<tableCaption confidence="0.9001785">
Table 1: # of rules used in the testing (d = 4, h = 6)
(BP: bilingual phrase (used in Moses), TR: tree rule (on-
ly 1 tree), TSR: tree sequence rule (&gt; 1 tree), L: fully
lexicalized, P: partially lexicalized, U: unlexicalized)
Table 1 reports the statistics of rules used in the
experiments. It shows that:
</tableCaption>
<listItem confidence="0.966206444444444">
1) We verify that the BPs are fully covered by
the initial rules (i.e. lexicalized rules), in which the
lexicalized TSRs model all non-syntactic phrase
pairs with rich syntactic information. In addition,
we find that the number of initial rules is greater
than that of bilingual phrases. This is because one
bilingual phrase can be covered by more than one
initial rule which having different sub-tree struc-
tures.
2) Abstract rules generalize initial rules to un-
seen data and with structure reordering ability. The
number of the abstract rule is far less than that of
the initial rules. This is because leaf nodes of an
abstract rule can be non-terminals that can
represent any sub-trees using the non-terminals as
roots.
Fig. 4 compares the performance of different
models. It illustrates that:
1) Our tree sequence-based model significantly
outperforms (p &lt; 0.01) previous phrase-based and
linguistically syntax-based methods. This empirical-
ly verifies the effect of the proposed method.
2) Both our method and STSG outperform Mos-
es significantly. Our method also clearly outper-
forms STSG. These results suggest that:
• The linguistically motivated structure features
are very useful for SMT, which can be cap-
</listItem>
<footnote confidence="0.6227906">
2 To be precise, we examine the contributions of strict tree
sequence rules and single tree rules separately in this section.
Therefore, unless specified, the term “tree sequence rules”
used in this section only refers to the strict tree sequence rules,
which must contain at least two sub-trees on the source side.
</footnote>
<page confidence="0.995891">
564
</page>
<bodyText confidence="0.964912172413793">
tured by the two syntax-based models through
tree node operations.
• Our model is much more effective in utilizing
linguistic structures than STSG since it uses
tree sequence as basic translation unit. This
allows our model not only to handle structure
reordering by tree node operations in a larger
span, but also to capture non-syntactic phras-
es, which circumvents previous syntactic
constraints, thus giving our model more ex-
pressive power.
3) The linguistically motivated SCFG shows
much lower performance. This is largely because
SCFG only allows sibling nodes reordering and fails
to utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single
CFG rule. It thereby suggests that SCFG is less
effective in modelling parse tree structure transfer
between Chinese and English when using Penn
Treebank style linguistic grammar and under word-
alignment constraints. However, formal SCFG
show much better performance in the formally syn-
tax-based translation framework (Chiang, 2005).
This is because the formal syntax is learned from
phrases directly without relying on any linguistic
theory (Chiang, 2005). As a result, it is more ro-
bust to the issue of non-syntactic phrase usage and
non-isomorphic structure alignment.
26.07
</bodyText>
<subsectionHeader confidence="0.658422">
SCFG Moses STSG Ours
</subsectionHeader>
<figureCaption confidence="0.96829">
Figure 4: Performance comparison of different methods
</figureCaption>
<table confidence="0.998406666666667">
Rule TR TR TR+TSR_L TR
Type (STSG) +TSR_L +TSR_P +TSR
BLEU(%) 24.71 25.72 25.93 26.07
</table>
<tableCaption confidence="0.832399">
Table 2: Contributions of TSRs (see Table 1 for the de-
finitions of the abbreviations used in this table)
Table 2 measures the contributions of different
kinds of tree sequence rules. It suggests that:
</tableCaption>
<listItem confidence="0.430688">
1) All the three kinds of TSRs contribute to the
performance improvement and their combination
</listItem>
<bodyText confidence="0.99760625">
further improves the performance. It suggests that
they are complementary to each other since the
lexicalized TSRs are used to model non-syntactic
phrases while the other two kinds of TSRs can ge-
neralize the lexicalized rules to unseen phrases.
2) The lexicalized TSRs make the major con-
tribution since they can capture non-syntactic
phrases with syntactic structure features.
</bodyText>
<table confidence="0.9989655">
Rule Type BLEU (%)
TR+TSR 26.07
(TR+TSR) w/o SRR 24.62
(TR+TSR) w/o DPR 25.78
</table>
<tableCaption confidence="0.999702">
Table 3: Effect of Structure Reordering Rules (SRR:
</tableCaption>
<bodyText confidence="0.659127">
refers to the structure reordering rules that have at least
two non-terminal leaf nodes with inverted order in the
source and target sides, which are usually not captured
by phrase-based models. Note that the reordering be-
tween lexical words and non-terminal leaf nodes is not
considered here) and Discontinuous Phrase Rules (DPR:
refers to these rules having at least one non-terminal
leaf node between two lexicalized leaf nodes) in our
tree sequence-based model (d = 4 and h = 6 )
</bodyText>
<table confidence="0.99923675">
Rule Type # of rules # of rules overlapped
(Intersection)
SRR 68,217 18,379 (26.9%)
DPR 57,244 18,379 (32.1%)
</table>
<tableCaption confidence="0.999719">
Table 4: numbers of SRR and DPR rules
</tableCaption>
<bodyText confidence="0.998477416666667">
Table 3 shows the contributions of SRR and
DPR. It clearly indicates that SRRs are very effec-
tive in reordering structures, which improve per-
formance by 1.45 (26.07-24.62) BLEU score.
However, DPRs have less impact on performance
in our tree sequence-based model. This seems in
contradiction to the previous observations3 in lite-
rature. However, it is not surprising simply be-
cause we use tree sequences as the basic translation
units. Thereby, our model can capture all phrases.
In this sense, our model behaves like a phrase-
based model, less sensitive to discontinuous phras-
</bodyText>
<note confidence="0.485559">
3 Wellington et al. (2006) reports that discontinuities are very
</note>
<tableCaption confidence="0.752700333333333">
useful for translational equivalence analysis using binary-
branching structures under word alignment and parse tree
constraints while they are almost of no use if under word
alignment constraints only. Bod (2007) finds that discontinues
phrase rules make significant performance improvement in
linguistically STSG-based SMT models.
</tableCaption>
<figure confidence="0.960835">
22.72
23.86
24.71
26.5
25.5
24.5
23.5
BLEU(%)
22.5
21.5
</figure>
<page confidence="0.99549">
565
</page>
<bodyText confidence="0.999616583333333">
es (Wellington et al., 2006). Our additional expe-
riments also verify that discontinuous phrase rules
are complementary to syntactic phrase rules (Bod,
2007) while non-syntactic phrase rules may com-
promise the contribution of discontinuous phrase
rules. Table 4 reports the numbers of these two
kinds of rules. It shows that around 30% rules are
shared by the two kinds of rule sets. These over-
lapped rules contain at least two non-terminal leaf
nodes plus two terminal leaf nodes, which implies
that longer rules do not affect performance too
much.
</bodyText>
<figureCaption confidence="0.992889">
Figure 5: Accuracy changing with different max-
imal tree depths ( h = 1 to 6 when d = 4 )
Figure 6: Accuracy changing with the different maximal
number of trees in a tree sequence (d =1 to 5), the upper
line is for h = 6 while the lower line is for h = 2.
</figureCaption>
<bodyText confidence="0.973897620689655">
Fig. 5 studies the impact when setting different
maximal tree depth ( h ) in a rule on the perfor-
mance. It demonstrates that:
1) Significant performance improvement is
achieved when the value of h is increased from 1
to 2. This can be easily explained by the fact that
when h = 1, only monotonic search is conducted,
while h =2 allows non-terminals to be leaf nodes,
thus introducing preliminary structure features to
the search and allowing non-monotonic search.
2) Internal structures and large span (due to h
increasing) are also useful as attested by the gain
of 0.86 (26.14-25.28) Blue score when the value of
h increases from 2 to 4.
Fig. 6 studies the impact on performance by set-
ting different maximal tree number (d) in a rule. It
further indicates that:
1) Tree sequence rules (d &gt;1) are useful and
even more helpful if we limit the tree depth to no
more than two (lower line, h=2). However, tree
sequence rules consisting of more than three sub-
trees have almost no contribution to the perform-
ance improvement. This is mainly due to data
sparseness issue when d &gt;3.
2) Even if only two-layer sub-trees (lower line)
are allowed, our method still outperforms STSG
and Moses when d&gt;1. This further validates the
effectiveness of our design philosophy of using
multi-sub-trees as basic translation unit in SMT.
</bodyText>
<sectionHeader confidence="0.997462" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999958">
In this paper, we present a tree sequence align-
ment-based translation model to combine the
strengths of phrase-based and syntax-based me-
thods. The experimental results on the NIST MT-
2005 Chinese-English translation task demonstrate
the effectiveness of the proposed model. Our study
also finds that in our model the tree sequence rules
are very useful since they can model non-syntactic
phrases and reorderings with rich linguistic struc-
ture features while discontinuous phrases and tree
sequence rules with more than three sub-trees have
less impact on performance.
There are many interesting research topics on
the tree sequence-based translation model worth
exploring in the future. The current method ex-
tracts large amount of rules. Many of them are re-
dundant, which make decoding very slow. Thus,
effective rule optimization and pruning algorithms
are highly desirable. Ideally, a linguistically and
empirically motivated theory can be worked out,
suggesting what kinds of rules should be extracted
given an input phrase pair. For example, most
function words and headwords can be kept in ab-
stract rules as features. In addition, word align-
ment is a hard constraint in our rule extraction. We
will study direct structure alignments to reduce the
impact of word alignment errors. We are also in-
terested in comparing our method with the forest-
to-string model (Liu et al., 2007). Finally, we
would also like to study unsupervised learning-
based bilingual parsing for SMT.
</bodyText>
<figure confidence="0.991857535714286">
25.94 26.14 26.02 26.07
26.5
25.5
24.5
23.5
22.5
BLEU(%)
22.07
25.28
1 2 3 4 5 6
21.5
21.5
1 2 3 4 5
BLEU(%)
26.5
25.5
24.5
23.5
22.5
22.72
26.03 26.07
26.05
25.74
24.71
25.26
24.78
25.28
25.29
</figure>
<page confidence="0.994575">
566
</page>
<sectionHeader confidence="0.993121" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998435134615385">
Rens Bod. 2007. Unsupervised Syntax-Based Machine
Translation: The Contribution of Discontinuous
Phrases. MT-Summmit-07. 51-56.
David Chiang. 2005. A hierarchical phrase-based mod-
el for SMT. ACL-05. 263-270.
Brooke Cowan, Ivona Kucerova and Michael Collins.
2006. A discriminative model for tree-to-tree transla-
tion. EMNLP-06. 232-241.
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sertion grammars. ACL-05. 541-548.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume).
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What’s in a translation rule? HLT-
NAACL-04.
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and Training of Context-Rich Syntactic
Translation Models. COLING-ACL-06. 961-968
Daniel Gildea. 2003. Loosely Tree-Based Alignment for
Machine Translation. ACL-03. 80-87.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. HLT-NAACL-2004. 105-112.
Mary Hearne and Andy Way. 2003. Seeing the wood for
the trees: data-oriented translation. MT Summit IX,
165-172.
Liang Huang, Kevin Knight and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster).
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. ACL-03. 423-430.
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistic-
al phrase-based translation. HLT-NAACL-03. 127-
133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation
models. AMTA-04, 115-124
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
ACL-07 (poster) 77-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. COLING-ACL-06. 609-616.
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.
2007. Forest-to-String Statistical Translation Rules.
ACL-07. 704-711.
Daniel Marcu, W. Wang, A. Echihabi and K. Knight.
2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. EMNLP-06.
44-52.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. ACL-04. 653-660.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. ACL-02. 295-302.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. ACL-03. 160-167.
Franz J. Och and Hermann Ney. 2004a. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. ACL-02. 311-318.
Arjen Poutsma. 2000. Data-oriented translation.
COLING-2000. 635-641
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
SMT. COLING-ACL-06. 9-16.
Chris Quirk, Arul Menezes and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279.
Stefan Riezler and John T. Maxwell III. 2006. Gram-
matical Machine Translation. HLT-NAACL-06.
248-255.
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007.
Ordering Phrases with Function Words. ACL-7.
712-719.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. ICSLP-02. 901-904.
Benjamin Wellington, Sonjia Waxmonsky and I. Dan
Melamed. 2006. Empirical Lower Bounds on the
Complexity of Translational Equivalence. COLING-
ACL-06. 977-984.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
SMT. COLING-ACL-06. 521– 528.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. ACL-01. 523-530.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542.
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement
do we need to have a better system? LREC-04. 2051-
2054.
</reference>
<page confidence="0.996932">
567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.313848">
<title confidence="0.999225">A Tree Sequence Alignment-based Tree-to-Tree Translation Model</title>
<author confidence="0.679783">Hongfei Aiti Haizhou Chew Lim</author>
<author confidence="0.679783">Sheng</author>
<affiliation confidence="0.381726">for Infocomm Research Institute of Technology University of Singapore</affiliation>
<abstract confidence="0.993109391304348">mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases.</title>
<date>2007</date>
<volume>07</volume>
<pages>51--56</pages>
<contexts>
<context position="1854" citStr="Bod, 2007" startWordPosition="255" endWordPosition="256">rase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit an</context>
<context position="4980" citStr="Bod (2007)" startWordPosition="731" endWordPosition="732">on-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering mode</context>
<context position="26527" citStr="Bod (2007)" startWordPosition="4577" endWordPosition="4578">sequence-based model. This seems in contradiction to the previous observations3 in literature. However, it is not surprising simply because we use tree sequences as the basic translation units. Thereby, our model can capture all phrases. In this sense, our model behaves like a phrasebased model, less sensitive to discontinuous phras3 Wellington et al. (2006) reports that discontinuities are very useful for translational equivalence analysis using binarybranching structures under word alignment and parse tree constraints while they are almost of no use if under word alignment constraints only. Bod (2007) finds that discontinues phrase rules make significant performance improvement in linguistically STSG-based SMT models. 22.72 23.86 24.71 26.5 25.5 24.5 23.5 BLEU(%) 22.5 21.5 565 es (Wellington et al., 2006). Our additional experiments also verify that discontinuous phrase rules are complementary to syntactic phrase rules (Bod, 2007) while non-syntactic phrase rules may compromise the contribution of discontinuous phrase rules. Table 4 reports the numbers of these two kinds of rules. It shows that around 30% rules are shared by the two kinds of rule sets. These overlapped rules contain at lea</context>
</contexts>
<marker>Bod, 2007</marker>
<rawString>Rens Bod. 2007. Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases. MT-Summmit-07. 51-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for SMT.</title>
<date>2005</date>
<booktitle>ACL-05.</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1747" citStr="Chiang, 2005" startWordPosition="235" endWordPosition="236">tion task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phras</context>
<context position="5604" citStr="Chiang (2005)" startWordPosition="829" endWordPosition="830">at the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language model under the phrase-based </context>
<context position="23833" citStr="Chiang, 2005" startWordPosition="4142" endWordPosition="4143">ving our model more expressive power. 3) The linguistically motivated SCFG shows much lower performance. This is largely because SCFG only allows sibling nodes reordering and fails to utilize both non-syntactic phrases and those syntactic phrases that cannot be covered by a single CFG rule. It thereby suggests that SCFG is less effective in modelling parse tree structure transfer between Chinese and English when using Penn Treebank style linguistic grammar and under wordalignment constraints. However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). As a result, it is more robust to the issue of non-syntactic phrase usage and non-isomorphic structure alignment. 26.07 SCFG Moses STSG Ours Figure 4: Performance comparison of different methods Rule TR TR TR+TSR_L TR Type (STSG) +TSR_L +TSR_P +TSR BLEU(%) 24.71 25.72 25.93 26.07 Table 2: Contributions of TSRs (see Table 1 for the definitions of the abbreviations used in this table) Table 2 measures the contributions of different kinds of tree sequence rules. It suggest</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for SMT. ACL-05. 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
</authors>
<title>Ivona Kucerova and Michael Collins.</title>
<date>2006</date>
<pages>06--232</pages>
<marker>Cowan, 2006</marker>
<rawString>Brooke Cowan, Ivona Kucerova and Michael Collins. 2006. A discriminative model for tree-to-tree translation. EMNLP-06. 232-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<pages>05--541</pages>
<contexts>
<context position="1784" citStr="Ding and Palmer, 2005" startWordPosition="239" endWordPosition="242">od statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The</context>
<context position="4449" citStr="Ding and Palmer (2005)" startWordPosition="653" endWordPosition="656">ACL-08: HLT, pages 559–567, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics rates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7. 2 Related Work Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. ACL-05. 541-548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for MT.</title>
<date>2003</date>
<note>ACL-03 (companion volume).</note>
<contexts>
<context position="1761" citStr="Eisner, 2003" startWordPosition="237" endWordPosition="238"> that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and sy</context>
<context position="4347" citStr="Eisner (2003)" startWordPosition="641" endWordPosition="642">se tree. It is the same as the concept “forest” used in Liu et al (2007). 559 Proceedings of ACL-08: HLT, pages 559–567, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics rates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7. 2 Related Work Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-t</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for MT. ACL-03 (companion volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<pages>04</pages>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What’s in a translation rule? HLTNAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<volume>06</volume>
<pages>961--968</pages>
<contexts>
<context position="6384" citStr="Galley et al. (2006)" startWordPosition="941" endWordPosition="944">nsitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language model under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be comb</context>
<context position="9049" citStr="Galley et al., 2006" startWordPosition="1360" endWordPosition="1363">counting the number of auxiliary rules used in a derivation for penalizing the use of forest-to-string and auxiliary rules. In this paper, an alternative solution is presented to combine the strengths of phrase-based and syn560 3 Tree Sequence Alignment Model 3.1 Tree Sequence Translation Rule T f ( 1 ) J A T e ( 1 ) I Figure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation Figure 2: Two Examples of tree sequences Figure 3: Two examples of translation rules tax-based methods. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al., 2006), nor depends on pseudo nodes (Marcu et al., 2006) or auxiliary rules (Liu et al., 2007). We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model. To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT. The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words). Given a pair of source and target parse trees (1 ) T f and ( 1 ) J T e in I Fig. 1, Fig. 2 illustrates two examples of tree sequences deriv</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang and I. Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. COLING-ACL-06. 961-968</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely Tree-Based Alignment for Machine Translation.</title>
<date>2003</date>
<volume>03</volume>
<pages>80--87</pages>
<contexts>
<context position="1929" citStr="Gildea, 2003" startWordPosition="269" endWordPosition="270"> simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation process. There</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely Tree-Based Alignment for Machine Translation. ACL-03. 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<pages>2004--105</pages>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. HLT-NAACL-2004. 105-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Seeing the wood for the trees: data-oriented translation.</title>
<date>2003</date>
<booktitle>MT Summit IX,</booktitle>
<pages>165--172</pages>
<contexts>
<context position="1967" citStr="Hearne and Way, 2003" startWordPosition="273" endWordPosition="276">sm to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation process. Therefore, it not only describes non-syntac</context>
</contexts>
<marker>Hearne, Way, 2003</marker>
<rawString>Mary Hearne and Andy Way. 2003. Seeing the wood for the trees: data-oriented translation. MT Summit IX, 165-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<date>2006</date>
<booktitle>Statistical Syntax-Directed Translation with Extended Domain of Locality. AMTA-06</booktitle>
<pages>(poster).</pages>
<contexts>
<context position="4795" citStr="Huang et al. (2006)" startWordPosition="702" endWordPosition="705">y syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT wa</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight and Aravind Joshi. 2006. Statistical Syntax-Directed Translation with Extended Domain of Locality. AMTA-06 (poster).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<date>2003</date>
<booktitle>Accurate Unlexicalized Parsing. ACL-03.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="19411" citStr="Klein and Manning, 2003" startWordPosition="3411" endWordPosition="3414">ining and test data. been translated so far monotonically as one hypothesis. 6 Experiments 6.1 Experimental Settings We conducted Chinese-to-English translation experiments. We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4- gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation mode</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. ACL-03. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<pages>03--127</pages>
<contexts>
<context position="1291" citStr="Koehn et al., 2003" startWordPosition="168" endWordPosition="171">igned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al.</context>
<context position="5351" citStr="Koehn et al., 2003" startWordPosition="786" endWordPosition="789">ures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and s</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistical phrase-based translation. HLT-NAACL-03. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<volume>04</volume>
<pages>115--124</pages>
<contexts>
<context position="12831" citStr="Koehn, 2004" startWordPosition="2110" endWordPosition="2111">he proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree. Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not distinguish between larger (taller) and shorter source side tree sequences, i.e. we let these rules compete directly wit</context>
<context position="19776" citStr="Koehn, 2004" startWordPosition="3474" endWordPosition="3475">cke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and development sets: c =3, α =20, β =-100 and y =100. 6.2 E</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. AMTA-04, 115-124</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>ACL-07</booktitle>
<pages>77--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="2925" citStr="Koehn et al., 2007" startWordPosition="416" endWordPosition="419">esigned to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation process. Therefore, it not only describes non-syntactic phrases with syntactic structure information, but also supports multi-level tree structure reordering in larger span. These give our model much more expressive power and flexibility than those previous models. Experiment results on the NIST MT-2005 ChineseEnglish translation task show that our method significantly outperforms Moses (Koehn et al., 2007), a state-of-the-art phrase-based SMT system, and other linguistically syntax-based methods, such as SCFG-based and STSG-based methods (Zhang et al., 2007). In addition, our study further demonstrates that 1) structure reordering rules in our model are very useful for performance improvement while discontinuous phrase rules have less contribution and 2) tree sequence rules are able to model non-syntactic phrases with syntactic structure information, and thus contribute much to the performance improvement, but those rules consisting of more than three sub-trees have almost no contribution. The </context>
<context position="19951" citStr="Koehn et al., 2007" startWordPosition="3500" endWordPosition="3503">test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and development sets: c =3, α =20, β =-100 and y =100. 6.2 Experimental Results We carried out a number of experiments to examine the proposed tree sequence alignment-based translation model. In this subsection, we first report the rul</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07 (poster) 77-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<volume>06</volume>
<pages>609--616</pages>
<contexts>
<context position="1897" citStr="Liu et al., 2006" startWordPosition="261" endWordPosition="264"> al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to mode</context>
<context position="4863" citStr="Liu et al. (2006)" startWordPosition="712" endWordPosition="715"> (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt</context>
<context position="7386" citStr="Liu et al. (2006)" startWordPosition="1096" endWordPosition="1099">xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem in this method is that it neglects alignment consistency in creating sibling rules and the naming mechanism faces challenges in describing more complicated phenomena (Liu et al., 2007). 4) Liu et al. (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus. Although the solution shows effective empirically, it only utilizes the source side syntactic phrases of the input parse tree during decoding. Furthermore, the translation probabilities of the bilingual phrases and other tree-to-string rules are not compatible since they are estimated independently, thus having different parameter spaces. To address the above problems, Liu et al. (2007) propose to use forest-to-string rules to enhance the expressive power of their tree-to</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. COLING-ACL-06. 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<volume>07</volume>
<pages>704--711</pages>
<contexts>
<context position="1915" citStr="Liu et al., 2007" startWordPosition="265" endWordPosition="268">d Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation </context>
<context position="3806" citStr="Liu et al (2007)" startWordPosition="557" endWordPosition="560">for performance improvement while discontinuous phrase rules have less contribution and 2) tree sequence rules are able to model non-syntactic phrases with syntactic structure information, and thus contribute much to the performance improvement, but those rules consisting of more than three sub-trees have almost no contribution. The rest of this paper is organized as follows: Section 2 reviews previous work. Section 3 elabo1 A tree sequence refers to an ordered sub-tree sequence that covers a phrase or a consecutive tree fragment in a parse tree. It is the same as the concept “forest” used in Liu et al (2007). 559 Proceedings of ACL-08: HLT, pages 559–567, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics rates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7. 2 Related Work Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string ma</context>
<context position="7364" citStr="Liu et al., 2007" startWordPosition="1091" endWordPosition="1094"> al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem in this method is that it neglects alignment consistency in creating sibling rules and the naming mechanism faces challenges in describing more complicated phenomena (Liu et al., 2007). 4) Liu et al. (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus. Although the solution shows effective empirically, it only utilizes the source side syntactic phrases of the input parse tree during decoding. Furthermore, the translation probabilities of the bilingual phrases and other tree-to-string rules are not compatible since they are estimated independently, thus having different parameter spaces. To address the above problems, Liu et al. (2007) propose to use forest-to-string rules to enhance the expressive </context>
<context position="9137" citStr="Liu et al., 2007" startWordPosition="1376" endWordPosition="1379">t-to-string and auxiliary rules. In this paper, an alternative solution is presented to combine the strengths of phrase-based and syn560 3 Tree Sequence Alignment Model 3.1 Tree Sequence Translation Rule T f ( 1 ) J A T e ( 1 ) I Figure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation Figure 2: Two Examples of tree sequences Figure 3: Two examples of translation rules tax-based methods. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al., 2006), nor depends on pseudo nodes (Marcu et al., 2006) or auxiliary rules (Liu et al., 2007). We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model. To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT. The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words). Given a pair of source and target parse trees (1 ) T f and ( 1 ) J T e in I Fig. 1, Fig. 2 illustrates two examples of tree sequences derived from the two parse trees. A tree sequence translation rule r is a pair of aligned tre</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases.</title>
<date>2006</date>
<volume>06</volume>
<pages>44--52</pages>
<contexts>
<context position="6581" citStr="Marcu et al., 2006" startWordPosition="969" endWordPosition="972"> methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language model under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem i</context>
<context position="9099" citStr="Marcu et al., 2006" startWordPosition="1369" endWordPosition="1372">rivation for penalizing the use of forest-to-string and auxiliary rules. In this paper, an alternative solution is presented to combine the strengths of phrase-based and syn560 3 Tree Sequence Alignment Model 3.1 Tree Sequence Translation Rule T f ( 1 ) J A T e ( 1 ) I Figure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation Figure 2: Two Examples of tree sequences Figure 3: Two examples of translation rules tax-based methods. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al., 2006), nor depends on pseudo nodes (Marcu et al., 2006) or auxiliary rules (Liu et al., 2007). We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model. To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT. The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words). Given a pair of source and target parse trees (1 ) T f and ( 1 ) J T e in I Fig. 1, Fig. 2 illustrates two examples of tree sequences derived from the two parse trees. A tree sequence trans</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. EMNLP-06. 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<pages>04--653</pages>
<marker>Melamed, 2004</marker>
<rawString>I. Dan Melamed. 2004. Statistical machine translation by parsing. ACL-04. 653-660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<pages>02--295</pages>
<contexts>
<context position="12715" citStr="Och and Ney, 2002" startWordPosition="2090" endWordPosition="2093">(fj;2 ), A &gt;) (3) θ ∈θ r i Eq. (3) formulates the tree sequence alignmentbased translation model. Figs. 1 and 3 show how the proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree. Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not dist</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. ACL-02. 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<pages>03--160</pages>
<contexts>
<context position="19729" citStr="Och, 2003" startWordPosition="3467" endWordPosition="3468">s (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and developme</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="1310" citStr="Och and Ney, 2004" startWordPosition="172" endWordPosition="175">pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al.,</context>
<context position="19623" citStr="Och and Ney, 2004" startWordPosition="3448" endWordPosition="3451"> corpus (7.2M+9.2M words) and trained a 4- gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004a. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>ToddWard</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>02--311</pages>
<contexts>
<context position="19587" citStr="Papineni et al., 2002" startWordPosition="3440" endWordPosition="3444">rained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4- gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 an</context>
</contexts>
<marker>Papineni, Roukos, ToddWard, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, ToddWard and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Data-oriented translation.</title>
<date>2000</date>
<pages>2000--635</pages>
<contexts>
<context position="1944" citStr="Poutsma, 2000" startWordPosition="271" endWordPosition="272">owerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation process. Therefore, it not on</context>
</contexts>
<marker>Poutsma, 2000</marker>
<rawString>Arjen Poutsma. 2000. Data-oriented translation. COLING-2000. 635-641</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? Challenging the conventional wisdom in SMT.</title>
<date>2006</date>
<pages>06--9</pages>
<contexts>
<context position="1634" citStr="Quirk and Menezes, 2006" startWordPosition="217" endWordPosition="220"> model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in SMT. COLING-ACL-06. 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<pages>05--271</pages>
<contexts>
<context position="1803" citStr="Quirk et al, 2005" startWordPosition="243" endWordPosition="246">icantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model ado</context>
<context position="4577" citStr="Quirk et al. (2005)" startWordPosition="671" endWordPosition="674">ocess while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7. 2 Related Work Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure featu</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. ACL-05. 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<date>2006</date>
<booktitle>Grammatical Machine Translation. HLT-NAACL-06.</booktitle>
<pages>248--255</pages>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>Stefan Riezler and John T. Maxwell III. 2006. Grammatical Machine Translation. HLT-NAACL-06. 248-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min-Yen Kan</author>
<author>Haizhou Li</author>
</authors>
<title>Ordering Phrases with Function Words.</title>
<date>2007</date>
<pages>712--719</pages>
<contexts>
<context position="5534" citStr="Setiawan et al. (2007)" startWordPosition="816" endWordPosition="819">07b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the targe</context>
</contexts>
<marker>Setiawan, Kan, Li, 2007</marker>
<rawString>Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Ordering Phrases with Function Words. ACL-7. 712-719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<pages>02--901</pages>
<contexts>
<context position="19174" citStr="Stolcke, 2002" startWordPosition="3371" endWordPosition="3372">puts whatever have Input: (1 ) T f J Output: ( 1 ) T e I (4) 563 phrase rules2. Finally, we investigate the impact of maximal sub-tree number and sub-tree depth in our model. All of the following discussions are held on the training and test data. been translated so far monotonically as one hypothesis. 6 Experiments 6.1 Experimental Settings We conducted Chinese-to-English translation experiments. We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4- gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 200</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<date>2006</date>
<booktitle>Empirical Lower Bounds on the Complexity of Translational Equivalence. COLINGACL-06.</booktitle>
<pages>977--984</pages>
<contexts>
<context position="26277" citStr="Wellington et al. (2006)" startWordPosition="4539" endWordPosition="4542"> of SRR and DPR rules Table 3 shows the contributions of SRR and DPR. It clearly indicates that SRRs are very effective in reordering structures, which improve performance by 1.45 (26.07-24.62) BLEU score. However, DPRs have less impact on performance in our tree sequence-based model. This seems in contradiction to the previous observations3 in literature. However, it is not surprising simply because we use tree sequences as the basic translation units. Thereby, our model can capture all phrases. In this sense, our model behaves like a phrasebased model, less sensitive to discontinuous phras3 Wellington et al. (2006) reports that discontinuities are very useful for translational equivalence analysis using binarybranching structures under word alignment and parse tree constraints while they are almost of no use if under word alignment constraints only. Bod (2007) finds that discontinues phrase rules make significant performance improvement in linguistically STSG-based SMT models. 22.72 23.86 24.71 26.5 25.5 24.5 23.5 BLEU(%) 22.5 21.5 565 es (Wellington et al., 2006). Our additional experiments also verify that discontinuous phrase rules are complementary to syntactic phrase rules (Bod, 2007) while non-syn</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky and I. Dan Melamed. 2006. Empirical Lower Bounds on the Complexity of Translational Equivalence. COLINGACL-06. 977-984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1733" citStr="Wu, 1997" startWordPosition="233" endWordPosition="234">sh translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the stre</context>
<context position="5425" citStr="Wu (1997)" startWordPosition="800" endWordPosition="801">ee-to-string alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of t</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for SMT.</title>
<date>2006</date>
<pages>06--521</pages>
<contexts>
<context position="5446" citStr="Xiong et al. (2006)" startWordPosition="802" endWordPosition="805">g alignment model. Liu et al. (2006) propose a tree-to-string model. Zhang et al. (2007b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al.</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for SMT. COLING-ACL-06. 521– 528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<pages>01--523</pages>
<contexts>
<context position="1879" citStr="Yamada and Knight, 2001" startWordPosition="257" endWordPosition="260">modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence </context>
<context position="4253" citStr="Yamada and Knight (2001)" startWordPosition="624" endWordPosition="627">uence refers to an ordered sub-tree sequence that covers a phrase or a consecutive tree fragment in a parse tree. It is the same as the concept “forest” used in Liu et al (2007). 559 Proceedings of ACL-08: HLT, pages 559–567, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics rates the modelling process while Sections 4 and 5 discuss the training and decoding algorithms. The experimental results are reported in Section 6. Finally, we conclude our work in Section 7. 2 Related Work Many techniques on linguistically syntax-based SMT have been proposed in literature. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. Quirk et al. (2005) propose a dependency treelet-based translation model. Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree. Huang et al. (2006) study a TSG-based tree-to-string alignment model. Liu et </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. ACL-01. 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
</authors>
<title>Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan.</title>
<date>2007</date>
<volume>07</volume>
<pages>535--542</pages>
<marker>Zhang, Jiang, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007. A Tree-to-Tree Alignment-based Model for Statistical Machine Translation. MT-Summit-07. 535-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<pages>04--2051</pages>
<marker>Waibel, 2004</marker>
<rawString>Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04. 2051-2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>