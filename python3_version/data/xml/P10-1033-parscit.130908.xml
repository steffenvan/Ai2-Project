<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.997731">
Discriminative Pruning for Discriminative ITG Alignment
</title>
<author confidence="0.999488">
Shujie Liu&amp;quot;, Chi-Ho Li* and Ming Zhou*
</author>
<affiliation confidence="0.998467">
&amp;quot;School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.890856">
shujieliu@mtlab.hit.edu.cn
</email>
<affiliation confidence="0.923949">
*Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.997624">
{chl, mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.993622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997410285714286">
While Inversion Transduction Grammar (ITG)
has regained more and more attention in recent
years, it still suffers from the major obstacle of
speed. We propose a discriminative ITG prun-
ing framework using Minimum Error Rate
Training and various features from previous
work on ITG alignment. Experiment results
show that it is superior to all existing heuristics
in ITG pruning. On top of the pruning frame-
work, we also propose a discriminative ITG
alignment model using hierarchical phrase
pairs, which improves both F-score and Bleu
score over the baseline alignment system of
GIZA++.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998259547619048">
Inversion transduction grammar (ITG) (Wu, 1997)
is an adaptation of SCFG to bilingual parsing. It
does synchronous parsing of two languages with
phrasal and word-level alignment as by-product.
For this reason ITG has gained more and more
attention recently in the word alignment commu-
nity (Zhang and Gildea, 2005; Cherry and Lin,
2006; Haghighi et al., 2009).
A major obstacle in ITG alignment is speed.
The original (unsupervised) ITG algorithm has
complexity of O(n6). When extended to super-
vised/discriminative framework, ITG runs even
more slowly. Therefore all attempts to ITG
alignment come with some pruning method. For
example, Haghighi et al. (2009) do pruning based
on the probabilities of links from a simpler
alignment model (viz. HMM); Zhang and Gildea
(2005) propose Tic-tac-toe pruning, which is
based on the Model 1 probabilities of word pairs
inside and outside a pair of spans.
As all the principles behind these techniques
have certain contribution in making good pruning
decision, it is tempting to incorporate all these
features in ITG pruning. In this paper, we pro-
pose a novel discriminative pruning framework
for discriminative ITG. The pruning model uses
no more training data than the discriminative ITG
parser itself, and it uses a log-linear model to in-
tegrate all features that help identify the correct
span pair (like Model 1 probability and HMM
posterior). On top of the discriminative pruning
method, we also propose a discriminative ITG
alignment system using hierarchical phrase pairs.
In the following, some basic details on the ITG
formalism and ITG parsing are first reviewed
(Sections 2 and 3), followed by the definition of
pruning in ITG (Section 4). The â€œDiscriminative
Pruning for Discriminative ITGâ€ model (DPDI)
and our discriminative ITG (DITG) parsers will
be elaborated in Sections 5 and 6 respectively.
The merits of DPDI and DITG are illustrated
with the experiments described in Section 7.
</bodyText>
<sectionHeader confidence="0.667549" genericHeader="introduction">
2 Basics of ITG
</sectionHeader>
<bodyText confidence="0.999056238095238">
The simplest formulation of ITG contains three
types of rules: terminal unary rules X â€”ï¿½ e/f ,
where e and f represent words (possibly a null
word, e) in the English and foreign language
respectively, and the binary rules X â€”ï¿½ [X, X] and
X â€”ï¿½ (X,X), which refer to that the component
English and foreign phrases are combined in the
same and inverted order respectively.
From the viewpoint of word alignment, the
terminal unary rules provide the links of word
pairs, whereas the binary rules represent the reor-
dering factor. One of the merits of ITG is that it
is less biased towards short-distance reordering.
Such a formulation has two drawbacks. First of
all, it imposes a 1-to-1 constraint in word align-
ment. That is, a word is not allowed to align to
more than one word. This is a strong limitation as
no idiom or multi-word expression is allowed to
align to a single word on the other side. In fact
there have been various attempts in relaxing the
1-to-1 constraint. Both ITG alignment
</bodyText>
<page confidence="0.985862">
316
</page>
<note confidence="0.9876005">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 316â€“324,
Uppsala, Sweden, 11-16 July 2010. cï¿½2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999704">
Figure 1: Example ITG parses in graph (a) and hypergraph (b).
</figureCaption>
<figure confidence="0.9998996875">
A:[e1,e3]/[f1,f3]
(e1/f2,e2/f1,e3/f3)
A:[e1,e3]/[f1,f3]
(e1/f2,e2/f1,e3/f3) ,
(e1/f1,e2/f2,e3,f3)
Aâ†’[B,C]
Aâ†’[A,C]
(a)
(b)
B:[e1,e2]/[f1,f2]
(e1/f2,e2/f1)
C:[e3,e3]/[f3,f3]
(e3/f3)
A:[e1,e2]/[f1,f2]
(e2/f2)
(e1/f2)
Bâ†’&lt;C,C&gt; Aâ†’[C,C]
C:[e1,e1]/[f2,f2]
(e1/f2)
C:[e2,e2]/[f1,f1]
(e2/f1)
C:[e1,e1]/[f2,f2]
(e1/f2)
C:[e2,e2]/[f1,f1]
(e2/f1)
C:[e3,e3]/[f3,f3]
(e3/f3)
C:[e1,e1]/[f1,f1]
(e1/f1)
C:[e2,e2]/[f2,f2]
(e2/f2)
B:[e1,e2]/[f1,f2]
</figure>
<figureCaption confidence="0.912579">
approaches with and without this constraint will
be elaborated in Section 6.
</figureCaption>
<bodyText confidence="0.998955222222222">
Secondly, the simple ITG leads to redundancy
if word alignment is the sole purpose of applying
ITG. For instance, there are two parses for three
consecutive word pairs, viz. [a/aâ€™ [b/bâ€™ c/
câ€™] ] and [[a/aâ€™ b/bâ€™] c/câ€™] . The problem of re-
dundancy is fixed by adopting ITG normal form.
In fact, normal form is the very first key to speed-
ing up ITG. The ITG normal form grammar as
used in this paper is described in Appendix A.
</bodyText>
<sectionHeader confidence="0.671566" genericHeader="method">
3 Basics of ITG Parsing
</sectionHeader>
<bodyText confidence="0.968085033333333">
Based on the rules in normal form, ITG word
alignment is done in a similar way to chart pars-
ing (Wu, 1997). The base step applies all relevant
terminal unary rules to establish the links of word
pairs. The word pairs are then combined into
span pairs in all possible ways. Larger and larger
span pairs are recursively built until the sentence
pair is built.
Figure 1(a) shows one possible derivation for a
toy example sentence pair with three words in
each sentence. Each node (rectangle) represents a
pair, marked with certain phrase category, of for-
eign span (F-span) and English span (E-span)
(the upper half of the rectangle) and the asso-
ciated alignment hypothesis (the lower half).
Each graph like Figure 1(a) shows only one deri-
vation and also only one alignment hypothesis.
The various derivations in ITG parsing can be
compactly represented in hypergraph (Klein and
Manning, 2001) like Figure 1(b). Each hypernode
(rectangle) comprises both a span pair (upper half)
and the list of possible alignment hypotheses
(lower half) for that span pair. The hyperedges
show how larger span pairs are derived from
smaller span pairs. Note that a hypernode may
have more than one alignment hypothesis, since a
hypernode may be derived through more than one
hyperedge (e.g. the topmost hypernode in Figure
1(b)). Due to the use of normal form, the hypo-
theses of a span pair are different from each other.
</bodyText>
<sectionHeader confidence="0.813992" genericHeader="method">
4 Pruning in ITG Parsing
</sectionHeader>
<bodyText confidence="0.934965">
The ITG parsing framework has three levels of
pruning:
</bodyText>
<listItem confidence="0.9987288">
1) To discard some unpromising span pairs;
2) To discard some unpromising F-spans
and/or E-spans;
3) To discard some unpromising alignment
hypotheses for a particular span pair.
</listItem>
<bodyText confidence="0.999853576923077">
The second type of pruning (used in Zhang et.
al. (2008)) is very radical as it implies discarding
too many span pairs. It is empirically found to be
highly harmful to alignment performance and
therefore not adopted in this paper.
The third type of pruning is equivalent to mi-
nimizing the beam size of alignment hypotheses
in each hypernode. It is found to be well handled
by the K-Best parsing method in Huang and
Chiang (2005). That is, during the bottom-up
construction of the span pair repertoire, each span
pair keeps only the best alignment hypothesis.
Once the complete parse tree is built, the k-best
list of the topmost span is obtained by minimally
expanding the list of alignment hypotheses of
minimal number of span pairs.
The first type of pruning is equivalent to mi-
nimizing the number of hypernodes in a hyper-
graph. The task of ITG pruning is defined in this
paper as the first type of pruning; i.e. the search
for, given an F-span, the minimal number of E-
spans which are the most likely counterpart of
that F-span.1 The pruning method should main-
tain a balance between efficiency (run as quickly
as possible) and performance (keep as many cor-
rect span pairs as possible).
</bodyText>
<footnote confidence="0.983781">
1 Alternatively it can be defined as the search of the minimal
number of E-spans per F-span. That is simply an arbitrary
decision on how the data are organized in the ITG parser.
</footnote>
<page confidence="0.998474">
317
</page>
<bodyText confidence="0.999813166666667">
A naÃ¯ve approach is that the required pruning
method outputs a score given a span pair. This
score is used to rank all E-spans for a particular
F-span, and the score of the correct E-span
should be in general higher than most of the in-
correct ones.
</bodyText>
<sectionHeader confidence="0.993807" genericHeader="method">
5 The DPDI Framework
</sectionHeader>
<bodyText confidence="0.999769">
DPDI, the discriminative pruning model pro-
posed in this paper, assigns score to a span pair
ğ‘“ , ğ‘’ as probability from a log-linear model:
</bodyText>
<equation confidence="0.944197">
ğ‘’ğ‘¥ğ‘( ğ‘– ğœ†ğ‘–ğ›¹ğ‘– ğ‘“ ,ğ‘’ )
ğ‘’ â€²âˆˆğ¸ ğ‘’ğ‘¥ğ‘( ğ‘– ğœ†ğ‘–ğ›¹ğ‘–(ğ‘“ , ğ‘’ â€²))
</equation>
<bodyText confidence="0.995695">
where each ğ›¹ğ‘–(ğ‘“, ğ‘’ ) is some feature about the
span pair, and each ğœ† is the weight of the corres-
ponding feature. There are three major questions
to this model:
</bodyText>
<listItem confidence="0.9998085">
1) How to acquire training samples? (Section
5.1)
2) How to train the parameters ğœ† ? (Section 5.2)
3) What are the features? (Section 5.3)
</listItem>
<subsectionHeader confidence="0.997021">
5.1 Training Samples
</subsectionHeader>
<bodyText confidence="0.999202757575758">
Discriminative approaches to word alignment use
manually annotated alignment for sentence pairs.
Discriminative pruning, however, handles not
only a sentence pair but every possible span pair.
The required training samples consist of various
F-spans and their corresponding E-spans.
Rather than recruiting annotators for marking
span pairs, we modify the parsing algorithm in
Section 3 so as to produce span pair annotation
out of sentence-level annotation. In the base step,
only the word pairs listed in sentence-level anno-
tation are inserted in the hypergraph, and the re-
cursive steps are just the same as usual.
If the sentence-level annotation satisfies the
alignment constraints of ITG, then each F-span
will have only one E-span in the parse tree. How-
ever, in reality there are often the cases where a
foreign word aligns to more than one English
word. In such cases the F-span covering that for-
eign word has more than one corresponding E-
spans. Consider the example in Figure 2, where
the golden links in the alignment annotation are
ğ‘’1/ğ‘“ 1, ğ‘’2/ğ‘“ 1, and ğ‘’3/ğ‘“2; i.e. the foreign word
ğ‘“ 1 aligns to both the English words ğ‘’1 and ğ‘’2.
Therefore the F-span ğ‘“ 1, ğ‘“ 1 aligns to the E-
span ğ‘’1, ğ‘’1 in one hypernode and to the E-span
ğ‘’2, ğ‘’2 in another hypernode. When such situa-
tion happens, we calculate the product of the in-
side and outside probability of each alignment
hypothesis of the span pair, based on the proba-
bilities of the links from some simpler alignment
model2. The E-span with the most probable hypo-
thesis is selected as the alignment of the F-span.
</bodyText>
<figureCaption confidence="0.991425">
Figure 2: Training sample collection.
</figureCaption>
<tableCaption confidence="0.5317515">
Table (b) lists, for the hypergraph in (a), the candidate
E-spans for each F-span.
</tableCaption>
<bodyText confidence="0.999860875">
It should be noted that this automatic span pair
annotation may violate some of the links in the
original sentence-level alignment annotation. We
have already seen how the 1-to-1 constraint in
ITG leads to the violation. Another situation is
the â€inside-outâ€Ÿ alignment pattern (c.f. Figure 3).
The ITG reordering constraint cannot be satisfied
unless one of the links in this pattern is removed.
</bodyText>
<equation confidence="0.992559">
f1 f2 f3 f4
e1 e2 e3 e4
</equation>
<figureCaption confidence="0.996841">
Figure 3: An example of inside-out alignment
</figureCaption>
<bodyText confidence="0.9999092">
The training samples thus obtained are positive
training samples. If we apply some classifier for
parameter training, then negative samples are
also needed. Fortunately, our parameter training
does not rely on any negative samples.
</bodyText>
<subsectionHeader confidence="0.949212">
5.2 MERT for Pruning
</subsectionHeader>
<bodyText confidence="0.997764363636363">
Parameter training of DPDI is based on Mini-
mum Error Rate Training (MERT) (Och, 2003), a
widely used method in SMT. MERT for SMT
estimates model parameters with the objective of
minimizing certain measure of translation errors
(or maximizing certain performance measure of
translation quality) for a development corpus.
Given an SMT system which produces, with
2 The formulae of the inside and outside probability of a
span pair will be elaborated in Section 5.3. The simpler
alignment model we used is HMM.
</bodyText>
<figure confidence="0.996567810810811">
A:
[e1,e3]/[f1,f2]
{e1/f1,e3/f2},{e2/f1,e3/f2}
Aâ†’[C,C] Aâ†’[C,C]
(a) (b)
Cw:
[e1,e1]/[f1,f1]
{e1/f1}
C.:
[e1]/Îµ
Cw:
[e2,e2]/[f1,f1]
{e1/f1}
C.:
[e2]/Îµ
Cw:
[e3,e3]/[f2,f2]
{e1/f1}
C:
[e1,e2]/[f1,f1]
Câ†’ [C.,Cw]
{e2/f1}
C:
[e2,e3]/[f2,f2]
Câ†’ [C.,Cw]
{e3/f2}
[f1,f1]
[f2,f2]
[f1,f2]
[e1,e1]
[e1,e2]
[e2,e2]
[e2,e3]
[e3,e3]
[e1,e3]
ğ‘ƒ ğ‘’ ğ‘“ =
(1)
</figure>
<page confidence="0.992939">
318
</page>
<bodyText confidence="0.9999464">
model parameters ğœ†1ğ‘€, the K-best candidate trans-
lations ğ‘’ (ğ‘“ğ‘ ; ğœ†1ğ‘€) for a source sentence ğ‘“ğ‘ , and an
error measure ğ¸(ğ‘Ÿğ‘ , ğ‘’ğ‘ ,ğ‘˜) of a particular candidate
ğ‘’ğ‘ ,ğ‘˜ with respect to the reference translation ğ‘Ÿğ‘ ,
the optimal parameter values will be:
</bodyText>
<equation confidence="0.997591571428572">
ğ‘†
ğœ† 1ğ‘€ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›
ğœ†ğ‘€
1
ğ¾
ğ¸ ğ‘Ÿğ‘ , ğ‘’ğ‘ ,ğ‘˜ ğ›¿(ğ‘’ ğ‘“ğ‘ ; ğœ†1ğ‘€ , ğ‘’ğ‘ ,ğ‘˜)
ğ‘˜=1
</equation>
<bodyText confidence="0.999984230769231">
DPDI applies the same equation for parameter
tuning, with different interpretation of the com-
ponents in the equation. Instead of a development
corpus with reference translations, we have a col-
lection of training samples, each of which is a
pair of F-span (ğ‘“ğ‘ ) and its corresponding E-span
(ğ‘Ÿğ‘ ). These samples are acquired from some ma-
nually aligned dataset by the method elaborated
in Section 5.1. The ITG parser outputs for each fs
a K-best list of E-spans ğ‘’ ğ‘“ğ‘ ; ğœ†1ğ‘€ based on the
current parameter values ğœ†1ğ‘€.
The error function is based on the presence and
the rank of the correct E-span in the K-best list:
</bodyText>
<equation confidence="0.87976">
ğ‘€
ğ¸ ğ‘Ÿğ‘ , ğ‘’ ğ‘“ğ‘ ; ğœ†1 ğ‘€ = âˆ’ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘Ÿğ‘  ğ‘–ğ‘“ ğ‘Ÿğ‘  âˆˆ ğ‘’ ğ‘“ğ‘ ; ğœ†1
ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ ğ‘œğ‘¡ğ‘•ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
(2)
</equation>
<bodyText confidence="0.996657642857143">
where ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘Ÿğ‘  is the (0-based) rank of the cor-
rect E-span ğ‘Ÿğ‘  in the K-best list ğ‘’ ğ‘“ğ‘ ; ğœ†1ğ‘€ . If ğ‘Ÿğ‘  is
not in the K-best list at all, then the error is de-
fined to be ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦, which is set as -100000 in
our experiments. The rationale underlying this
error function is to keep as many correct E-spans
as possible in the K-best lists of E-spans, and
push the correct E-spans upward as much as
possible in the K-best lists.
This new error measure leads to a change in
details of the training algorithm. In MERT for
SMT, the interval boundaries at which the per-
formance or error measure changes are defined
by the upper envelope (illustrated by the dash
line in Figure 4(a)), since the performance/error
measure depends on the best candidate transla-
tion. In MERT for DPDI, however, the error
measure depends on the correct E-span rather
than the E-span leading to the highest system
score. Thus the interval boundaries are the inter-
sections between the correct E-span and all other
candidate E-spans (as shown in Figure 4(b)). The
rank of the correct E-span in each interval can
then be figured out as shown in Figure 4(c). Fi-
nally, the error measure in each interval can be
calculated by Equation (2) (as shown in Figure
4(d)). All other steps in MERT for DPDI are the
same as that for SMT.
</bodyText>
<figureCaption confidence="0.981084">
Figure 4: MERT for DPDI
</figureCaption>
<bodyText confidence="0.9979758">
Part (a) shows how intervals are defined for SMT and
part (b) for DPDI. Part (c) obtains the rank of correct
E-spans in each interval and part (d) the error measure.
Note that the beam size (max number of E-spans) for
each F-span is 10.
</bodyText>
<subsectionHeader confidence="0.847838">
5.3 Features
</subsectionHeader>
<bodyText confidence="0.9806397">
The features used in DPDI are divided into three
categories:
1) Model 1-based probabilities. Zhang and Gil-
dea (2005) show that Model 1 (Brown et al.,
1993; Och and Ney., 2000) probabilities of
the word pairs inside and outside a span pair
( ğ‘’ğ‘–1,ğ‘’ğ‘–2 /[ğ‘“ğ‘—1,ğ‘“ğ‘—2]) are useful. Hence these
two features:
a) Inside probability (i.e. probability of
word pairs within the span pair):
</bodyText>
<equation confidence="0.987184333333333">
ğ‘ğ‘–ğ‘›ğ‘ ğ‘’ğ‘–1,ğ‘–2 ğ‘“ğ‘—1,ğ‘—2
=
ğ‘–âˆˆ ğ‘–1,ğ‘–2 ğ‘—âˆˆ ğ‘—1,ğ‘—2
</equation>
<listItem confidence="0.507032">
b) Outside probability (i.e. probability of
the word pairs outside the span pair):
</listItem>
<equation confidence="0.885917">
ğ‘ğ‘œğ‘¢ğ‘¡ ğ‘’ğ‘–1,ğ‘–2 ğ‘“ğ‘—1,ğ‘—2
= ğ‘–âˆ‰ ğ‘–1,ğ‘–2 ğ‘—âˆ‰ ğ‘—1,ğ‘—2 1 ğ½ âˆ’ ğ‘—2 + ğ‘—1 ğ‘ğ‘€1 ğ‘’ğ‘– ğ‘“ğ‘—
</equation>
<bodyText confidence="0.9948165">
where ğ½ is the length of the foreign sen-
tence.
</bodyText>
<listItem confidence="0.628142">
2) Heuristics. There are four features in this cat-
egory. The features are explained with the
</listItem>
<figure confidence="0.99283032">
-index
(c)
-8
-9
-10
loss
(d)
-8
-9
-100,000
Î£Î»jm
Î»k
gold
Î»k
Î»k
Î»k
Î£Î»jm
ğ¸ ğ‘Ÿğ‘ , ğ‘’ ğ‘“ğ‘ ; ğœ†1 ğ‘€
= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›
ğœ†ğ‘€
1
ğ‘ =1
ğ‘†
ğ‘ =1
1 ğ‘—2 âˆ’ ğ‘—1 ğ‘ğ‘€1 ğ‘’ğ‘– ğ‘“ğ‘—
</figure>
<page confidence="0.995601">
319
</page>
<bodyText confidence="0.999267571428571">
example of Figure 5, in which the span pair
in interest is ğ‘’2, ğ‘’3 /[ğ‘“ 1, ğ‘“ 2]. The four links
are produced by some simpler alignment
model like HMM. The word pair ğ‘’2/ğ‘“ 1 is
the only link in the span pair. The links
ğ‘’4/ğ‘“ 2 and ğ‘’3/ğ‘“ 3 are inconsistent with the
span pair.3
</bodyText>
<figureCaption confidence="0.986481">
Figure 5: Example for heuristic features
</figureCaption>
<bodyText confidence="0.945030972222222">
2Ã—#ğ‘™ ğ‘–ğ‘›ğ‘˜ğ‘ 
ğ‘“ ğ‘™ğ‘’ğ‘› +ğ‘’ğ‘™ğ‘’ğ‘›
where #ğ‘™ğ‘–ğ‘›ğ‘˜ğ‘  is the number of links in
the span pair, and ğ‘“ ğ‘™ğ‘’ğ‘› and ğ‘’ğ‘™ğ‘’ğ‘› are the
length of the foreign and English spans
respectively. The feature value of the ex-
ample span pair is (2*1)/(2+2)=0.5.
b) inconsistent link ratio:
where #ğ‘™ğ‘–ğ‘›ğ‘˜ğ‘ ğ‘–ğ‘›ğ‘ğ‘œğ‘› is the number of links
which are inconsistent with the phrase
pair according to some simpler alignment
model (e.g. HMM). The feature value of
the example is (2*2)/(2+2) =1.0.
c) Length ratio: ğ‘“ ğ‘™ğ‘’ğ‘›âˆ’ ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘ğ‘£ğ‘”
ğ‘’ğ‘™ğ‘’ğ‘›
where ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘ğ‘£ ğ‘” is defined as the average
ratio of foreign sentence length to Eng-
lish sentence length, and it is estimated to
be around 1.15 in our training dataset.
The rationale underlying this feature is
that the ratio of span length should not be
too deviated from the average ratio of
sentence length. The feature value for the
example is |2/2-1.15|=0.15.
d) Position Deviation: ğ‘ğ‘œğ‘  ğ‘“ âˆ’ ğ‘ğ‘œğ‘ ğ‘’
where ğ‘ğ‘œğ‘  ğ‘“ refers to the position of the
F-span in the entire foreign sentence, and
it is defined as 2ğ½1 ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘“ + ğ‘’ğ‘›ğ‘‘ğ‘“ ,
ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘“ /ğ‘’ğ‘›ğ‘‘ğ‘“ being the position of the
first/last word of the F-span in the for-
eign sentence. ğ‘ğ‘œğ‘ ğ‘’ is defined similarly.
The rationale behind this feature is the
monotonic assumption, i.e. a phrase of
the foreign sentence usually occupies
roughly the same position of the equiva-
lent English phrase. The feature value for
</bodyText>
<footnote confidence="0.985076">
3 An inconsistent link connects a word within the phrase pair
to some word outside the phrase pair. C.f. Deng et al. (2008)
</footnote>
<bodyText confidence="0.912585111111111">
the example is |(1+2)/(2*4)-(2+3)/(2*4)|
=0.25.
3) HMM-based probabilities. Haghighi et al.
(2009) show that posterior probabilities from
the HMM alignment model is useful for
pruning. Therefore, we design two new fea-
tures by replacing the link count in link ratio
and inconsistent link ratio with the sum of the
linkâ€Ÿs posterior probability.
</bodyText>
<sectionHeader confidence="0.998208" genericHeader="method">
6 The DITG Models
</sectionHeader>
<bodyText confidence="0.999988296296296">
The discriminative ITG alignment can be con-
ceived as a two-staged process. In the first stage
DPDI selects good span pairs. In the second stage
good alignment hypotheses are assigned to the
span pairs selected by DPDI. Two discriminative
ITG (DITG) models are investigated. One is
word-to-word DITG (henceforth W-DITG),
which observes the 1-to-1 constraint on align-
ment. Another is DITG with hierarchical phrase
pairs (henceforth HP-DITG), which relaxes the 1-
to-1 constraint by adopting hierarchical phrase
pairs in Chiang (2007).
Each model selects the best alignment hypo-
theses of each span pair, given a set of features.
The contributions of these features are integrated
through a log linear model (similar to Liu et al.,
2005; Moore, 2005) like Equation (1). The dis-
criminative training of the feature weights is
again MERT (Och, 2003). The MERT module
for DITG takes alignment F-score of a sentence
pair as the performance measure. Given an input
sentence pair and the reference annotated align-
ment, MERT aims to maximize the F-score of
DITG-produced alignment. Like SMT (and un-
like DPDI), it is the upper envelope which de-
fines the intervals where the performance meas-
ure changes.
</bodyText>
<subsectionHeader confidence="0.963998">
6.1 Word-to-word DITG
</subsectionHeader>
<bodyText confidence="0.9928025">
The following features about alignment link are
used in W-DITG:
</bodyText>
<listItem confidence="0.998870166666667">
1) Word pair translation probabilities trained
from HMM model (Vogel, et.al., 1996)
and IBM model 4 (Brown et.al., 1993;
Och and Ney, 2000).
2) Conditional link probability (Moore, 2005).
3) Association score rank features (Moore et
al., 2006).
4) Distortion features: counts of inversion
and concatenation.
5) Difference between the relative positions
of the words. The relative position of a
word in a sentence is defined as the posi-
</listItem>
<equation confidence="0.7854828">
f1 f2 f3 f4
e1 e2 e3 e4
a) Link ratio:
2Ã—#ğ‘™ğ‘–ğ‘›ğ‘˜ğ‘  ğ‘–ğ‘›ğ‘ğ‘œğ‘›
ğ‘“ ğ‘™ğ‘’ğ‘› +ğ‘’ğ‘™ğ‘’ğ‘›
</equation>
<page confidence="0.926836">
320
</page>
<bodyText confidence="0.813375">
tion of the word divided by sentence
length.
6) Boolean features like whether a word in
the word pair is a stop word.
</bodyText>
<subsectionHeader confidence="0.911728">
6.2 DITG with Hierarchical Phrase Pairs
</subsectionHeader>
<bodyText confidence="0.99558558490566">
extraction as stated in Chiang (2007). The rule
probabilities and lexical weights in both English-
to-foreign and foreign-to-English directions are
estimated and taken as features, in addition to
those features in W-DITG, in the discriminative
model of alignment hypothesis selection.
The 1-to-1 assumption in ITG is a serious limita-
tion as in reality there are always segmentation or
tokenization errors as well as idiomatic expres-
sions. Wu (1997) proposes a bilingual segmenta-
tion grammar extending the terminal rules by
including phrase pairs. Cherry and Lin (2007)
incorporate phrase pairs in phrase-based SMT
into ITG, and Haghighi et al. (2009) introduce
Block ITG (BITG), which adds 1-to-many or
many-to-1 terminal unary rules.
It is interesting to see if DPDI can benefit the
parsing of a more realistic ITG. HP-DITG ex-
tends Cherry and Linâ€Ÿs approach by not only em-
ploying simple phrase pairs but also hierarchical
phrase pairs (Chiang, 2007). The grammar is
enriched with rules of the format: X e_i/f_i
where e_i and f_i refer to the English and foreign
side of the i-th (simple/hierarchical) phrase pair
respectively.
As example, if there is a simple phrase pair
X (North Korea, åŒ— æœé²œ ) , then it is trans-
formed into the ITG rule C &amp;quot;North Korea&amp;quot;/
&amp;quot;åŒ— æœé²œ&amp;quot;. During parsing, each span pair does
not only examine all possible combinations of
sub-span pairs using binary rules, but also checks
if the yield of that span pair is exactly the same as
that phrase pair. If so, then the alignment links
within the phrase pair (which are obtained in
standard phrase pair extraction procedure) are
taken as an alternative alignment hypothesis of
that span pair.
For a hierarchical phrase pair like
X (X1 of X2, X2 çš„ X1), it is transformed into
the ITG rule C â€”&gt; &amp;quot;X1 of X2&amp;quot;/&amp;quot;X2 çš„ X1&amp;quot; during
parsing, each span pair checks if it contains the
lexical anchors &amp;quot;of&amp;quot; and &amp;quot;çš„&amp;quot;, and if the remain-
ing words in its yield can form two sub-span
pairs which fit the reordering constraint among
X1 and X2. (Note that span pairs of any category
in the ITG normal form grammar can substitute
for X1or X2 .) If both conditions hold, then the
span pair is assigned an alignment hypothesis
which combines the alignment links among the
lexical anchors (like of /çš„) and those links
among the sub-span pairs.
HP-ITG acquires the rules from HMM-based
word-aligned corpus using standard phrase pair
</bodyText>
<sectionHeader confidence="0.985933" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.990380714285714">
DPDI is evaluated against the baselines of Tic-
tac-toe (TTT) pruning (Zhang and Gildea, 2005)
and Dynamic Program (DP) pruning (Haghighi et
al., 2009; DeNero et al., 2009) with respect to
Chinese-to-English alignment and translation.
Based on DPDI, HP-DITG is evaluated against
the alignment systems GIZA++ and BITG.
</bodyText>
<subsectionHeader confidence="0.99322">
7.1 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.999973190476191">
Four evaluation criteria are used in addition to
the time spent on ITG parsing. We will first eva-
luate pruning regarding the pruning decisions
themselves. That is, the first evaluation metric,
pruning error rate (henceforth PER), measures
how many correct E-spans are discarded. The
major drawback of PER is that not all decisions
in pruning would impact on alignment quality,
since certain F-spans are of little use to the entire
ITG parse tree.
An alternative criterion is the upper bound on
alignment F-score, which essentially measures
how many links in annotated alignment can be
kept in ITG parse. The calculation of F-score up-
per bound is done in a bottom-up way like ITG
parsing. All leaf hypernodes which contain a cor-
rect link are assigned a score (known as hit) of 1.
The hit of a non-leaf hypernode is based on the
sum of hits of its daughter hypernodes. The max-
imal sum among all hyperedges of a hypernode is
assigned to that hypernode. Formally,
</bodyText>
<equation confidence="0.999381166666667">
hit(X [f_, e_]) =
max_ (hit (Y[f_1, e_1]) + hit [f_2, e_2])
Y,Z,f_1,e_1,f2,e_2
hit(Cw [u, v]) = j1 if (u, v) E R
l0 otherwise
hit(Ce) = 0; hit(Cf) = 0
</equation>
<bodyText confidence="0.99914375">
where X, Y, Z are variables for the categories in
ITG grammar, and R comprises the golden links
in annotated alignment. Cw, Ce, Cf are defined in
Appendix A.
Figure 6 illustrates the calculation of the hit
score for the example in Section 5.1/Figure 2.
The upper bound of recall is the hit score divided
by the total number of golden links. The upper
</bodyText>
<page confidence="0.995078">
321
</page>
<table confidence="0.9987498">
ID pruning beam size pruning/total time cost PER F-UB F-score
1 DPDI 10 72&apos;&apos;/3&apos;03&apos;&apos; 4.9% 88.5% 82.5%
2 TTT 10 58â€/2â€™38â€ 8.6% 87.5% 81.1%
3 TTT 20 53&apos;&apos;/6&apos;55&apos;&apos; 5.2% 88.6% 82.4%
4 DP -- 11&apos;&apos;/6&apos;01&apos;&apos; 12.1% 86.1% 80.5%
</table>
<tableCaption confidence="0.998561">
Table 1: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for W-DITG
</tableCaption>
<table confidence="0.999683">
ID pruning beam size pruning/total time cost PER F-UB F-score
1 DPDI 10 72&apos;&apos;/5&apos;18&apos;&apos; 4.9% 93.9% 87.0%
2 TTT 10 58â€/4â€™51â€ 8.6% 93.0% 84.8%
3 TTT 20 53&apos;&apos;/12&apos;5&apos;&apos; 5.2% 94.0% 86.5%
4 DP -- 11&apos;&apos;/15&apos;39&apos;&apos; 12.1% 91.4% 83.6%
</table>
<tableCaption confidence="0.999818">
Table 2: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG.
</tableCaption>
<bodyText confidence="0.9601038">
bound of precision, which should be defined as
the hit score divided by the number of links pro-
duced by the system, is almost always 1.0 in
practice. The upper bound of alignment F-score
can thus be calculated as well.
</bodyText>
<figureCaption confidence="0.992614">
Figure 6: Recall Upper Bound Calculation
</figureCaption>
<bodyText confidence="0.9992928">
Finally, we also do end-to-end evaluation us-
ing both F-score in alignment and Bleu score in
translation. We use our implementation of hierar-
chical phrase-based SMT (Chiang, 2007), with
standard features, for the SMT experiments.
</bodyText>
<subsectionHeader confidence="0.999918">
7.2 Experiment Data
</subsectionHeader>
<bodyText confidence="0.999931470588235">
Both discriminative pruning and alignment need
training data and test data. We use the manually
aligned Chinese-English dataset as used in Hag-
highi et al. (2009). The 491 sentence pairs in this
dataset are adapted to our own Chinese word
segmentation standard. 250 sentence pairs are
used as training data and the other 241 are test
data. The corresponding numbers of F-spans in
training and test data are 4590 and 3951 respec-
tively.
In SMT experiments, the bilingual training da-
taset is the NIST training set excluding the Hong
Kong Law and Hong Kong Hansard, and our 5-
gram language model is trained from the Xinhua
section of the Gigaword corpus. The NIST&apos;03
test set is used as our development corpus and the
NIST&apos;05 and NIST&apos;08 test sets are our test sets.
</bodyText>
<subsectionHeader confidence="0.994097">
7.3 Small-scale Evaluation
</subsectionHeader>
<bodyText confidence="0.999944878787879">
The first set of experiments evaluates the perfor-
mance of the three pruning methods using the
small 241-sentence set. Each pruning method is
plugged in both W-DITG and HP-DITG. IBM
Model 1 and HMM alignment model are re-
implemented as they are required by the three
ITG pruning methods.
The results for W-DITG are listed in Table 1.
Tests 1 and 2 show that with the same beam size
(i.e. number of E-spans per F-span), although
DPDI spends a bit more time (due to the more
complicated model), DPDI makes far less incor-
rect pruning decisions than the TTT. In terms of
F-score upper bound, DPDI is 1 percent higher.
DPDI achieves even larger improvement in ac-
tual F-score.
To enable TTT achieving similar F-score or F-
score upper bound, the beam size has to be
doubled and the time cost is more than twice the
original (c.f. Tests 1 and 3 in Table 1) .
The DP pruning in Haghighi et.al. (2009) per-
forms much poorer than the other two pruning
methods. In fact, we fail to enable DP achieve the
same F-score upper bound as the other two me-
thods before DP leads to intolerable memory
consumption. This may be due to the use of dif-
ferent HMM model implementations between our
work and Haghighi et.al. (2009).
Table 2 lists the results for HP-DITG. Roughly
the same observation as in W-DITG can be made.
In addition to the superiority of DPDI, it can also
be noted that HP-DITG achieves much higher F-
score and F-score upper bound. This shows that
</bodyText>
<figure confidence="0.992903375">
A:
[e1,e3]/[f1,f2]
hit=max{1+1,1+1}=2
Aâ†’[C,C]
Aâ†’[C,C]
C:
[e1,e2]/[f1,f1]
C:
[e2,e3]/[f2,f2]
hit=max{0+1}=1
hit=max{0+1}=1
Câ†’ [Ce,Cw]
Câ†’ [Ce,Cw]
Cw: Ce: Cw: Ce: Cw:
[e1,e1]/[f1,f1] [e1]/Îµ [e2,e2]/[f1,f1] [e2]/Îµ [e3,e3]/[f2,f2]
hit=1 hit=0 hit=1 hit=0 hit=1
</figure>
<page confidence="0.992809">
322
</page>
<bodyText confidence="0.999865222222222">
hierarchical phrase is a powerful tool in rectify-
ing the 1-to-1 constraint in ITG.
Note also that while TTT in Test 3 gets rough-
ly the same F-score upper bound as DPDI in Test
1, the corresponding F-score is slightly worse. A
possible explanation is that better pruning not
only speeds up the parsing/alignment process but
also guides the search process to focus on the
most promising region of the search space.
</bodyText>
<subsectionHeader confidence="0.570876">
7.4 Large-scale End-to-End Experiment
</subsectionHeader>
<table confidence="0.999649333333333">
ID Prun- beam time Bleu- Bleu-
ing size cost 05 08
1 DPDI 10 1092h 38.57 28.31
2 TTT 10 972h 37.96 27.37
3 TTT 20 2376h 38.13 27.58
4 DP -- 2068h 37.43 27.12
</table>
<tableCaption confidence="0.908414">
Table 3: Evaluation of DPDI against TTT and
DP for HP-DITG
</tableCaption>
<table confidence="0.999906166666667">
ID WA- F-Score Bleu-05 Bleu-08
Model
1 HMM 80.1% 36.91 26.86
2 Giza++ 84.2% 37.70 27.33
3 BITG 85.9% 37.92 27.85
4 HP-DITG 87.0% 38.57 28.31
</table>
<tableCaption confidence="0.9644725">
Table 4: Evaluation of DPDI against HMM, Gi-
za++ and BITG
</tableCaption>
<bodyText confidence="0.96845592">
Table 3 lists the word alignment time cost and
SMT performance of different pruning methods.
HP-DITG using DPDI achieves the best Bleu
score with acceptable time cost. Table 4 com-
pares HP-DITG to HMM (Vogel, et al., 1996),
GIZA++ (Och and Ney, 2000) and BITG (Hag-
highi et al., 2009). It shows that HP-DITG (with
DPDI) is better than the three baselines both in
alignment F-score and Bleu score. Note that the
Bleu score differences between HP-DITG and the
three baselines are statistically significant (Koehn,
2004).
An explanation of the better performance by
HP-DITG is the better phrase pair extraction due
to DPDI. On the one hand, a good phrase pair
often fails to be extracted due to a link inconsis-
tent with the pair. On the other hand, ITG prun-
ing can be considered as phrase pair selection,
and good ITG pruning like DPDI guides the sub-
sequent ITG alignment process so that less links
inconsistent to good phrase pairs are produced.
This also explains (in Tables 2 and 3) why DPDI
with beam size 10 leads to higher Bleu than TTT
with beam size 20, even though both pruning me-
thods lead to roughly the same alignment F-score.
</bodyText>
<sectionHeader confidence="0.938593" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999987066666667">
This paper reviews word alignment through ITG
parsing, and clarifies the problem of ITG pruning.
A discriminative pruning model and two discri-
minative ITG alignments systems are proposed.
The pruning model is shown to be superior to all
existing ITG pruning methods, and the HP-DITG
alignment system is shown to improve state-of-
the-art alignment and translation quality.
The current DPDI model employs a very li-
mited set of features. Many features are related
only to probabilities of word pairs. As the success
of HP-DITG illustrates the merit of hierarchical
phrase pair, in future we should investigate more
features on the relationship between span pair
and hierarchical phrase pair.
</bodyText>
<sectionHeader confidence="0.983447" genericHeader="acknowledgments">
Appendix A. The Normal Form Grammar
</sectionHeader>
<bodyText confidence="0.99893375">
Table 5 lists the ITG rules in normal form as
used in this paper, which extend the normal form
in Wu (1997) so as to handle the case of align-
ment to null.
</bodyText>
<figure confidence="0.953806">
1 S â€”ï¿½ A|B|C
2 A - [A B] |[A C] |[B B] |[BC] |[C B] |[C C
3 B â€”ï¿½ (A A) |(A C) |(B A) |(B C)
B â€”ï¿½ (C A)  |(C C)
4 C - Cw |Cfw |Cew
5 C â€”ï¿½ [Cew Cfw
6 Cw â€”ï¿½ u/v
7 Ce â€”ï¿½ Â£/v; Cf â€”ï¿½ u/Â£
8 Cem - Ce  |[Cem Ce]; Cfm - Cf  |[Cfm Cf
9 Cew - [Cem Cw ]; Cfw - [Cfm Cw]
</figure>
<tableCaption confidence="0.945226">
Table 5: ITG Rules in Normal Form
</tableCaption>
<bodyText confidence="0.998560823529412">
In these rules, S is the Start symbol; A is the
category for concatenating combination whereas
B for inverted combination. Rules (2) and (3) are
inherited from Wu (1997). Rules (4) divide the
terminal category C into subcategories. Rule
schema (6) subsumes all terminal unary rules for
some English word u and foreign word v, and
rule schemas (7) are unary rules for alignment to
null. Rules (8) ensure all words linked to null are
combined in left branching manner, while rules
(9) ensure those words linked to null combine
with some following, rather than preceding, word
pair. (Note: Accordingly, all sentences must be
ended by a special token (end), otherwise the
last word(s) of a sentence cannot be linked to
null.) If there are both English and foreign words
linked to null, rule (5) ensures that those English
</bodyText>
<page confidence="0.997182">
323
</page>
<bodyText confidence="0.9929155">
words linked to null precede those foreign words
linked to null.
</bodyText>
<sectionHeader confidence="0.995712" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876623188406">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Peitra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263-311.
Colin Cherry and Dekang Lin. 2006. Soft Syntactic
Constraints for Word Alignment through Dis-
criminative Training. In Proceedings of ACL-
COLING.
Colin Cherry and Dekang Lin. 2007. Inversion
Transduction Grammar for Joint Phrasal
Translation Modeling. In Proceedings of SSST,
NAACL-HLT, Pages:17-24.
David Chiang. 2007. Hierarchical Phrase-based
Translation. Computational Linguistics, 33(2).
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient Parsing for Transducer
Grammars. In Proceedings of NAACL, Pag-
es:227-235.
Alexander Fraser and Daniel Marcu. 2006. Semi-
Supervised Training for StatisticalWord
Alignment. In Proceedings of ACL, Pages:769-
776.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better Word Alignments with Su-
pervised ITG Models. In Proceedings of ACL,
Pages: 923-931.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of IWPT 2005, Pag-
es:173-180.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of
ACL. Pages: 440-447
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of ACL, Pages:160-167.
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. In Proceedings of IWPT,
Pages:17-19
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Pro-
ceedings of EMNLP, Pages: 388-395.
Yang Liu, Qun Liu and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceed-
ings of ACL, Pages: 81-88.
Robert Moore. 2005. A Discriminative Framework
for Bilingual Word Alignment. In Proceedings of
EMNLP 2005, Pages: 81-88.
Robert Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved Discriminative Bilingual Word
Alignment. In Proceedings of ACL, Pages: 513-
520.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in
statistical translation. In Proceedings of COL-
ING, Pages: 836-841.
Stephan Vogel. 2005. PESA: Phrase Pair Extrac-
tion as Sentence Splitting. In Proceedings of MT
Summit.
Dekai Wu. 1997. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Pa-
rallel Corpora. Computational Linguistics, 23(3).
Hao Zhang and Daniel Gildea. 2005. Stochastic Lex-
icalized Inversion Transduction Grammar for
Alignment. In Proceedings of ACL.
Hao Zhang, Chris Quirk, Robert Moore, and Daniel
Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In Proceedings of ACL, Pages: 314-323.
</reference>
<page confidence="0.999031">
324
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327748">
<title confidence="0.99393">Discriminative Pruning for Discriminative ITG Alignment</title>
<author confidence="0.942479">Chi-Ho</author>
<affiliation confidence="0.9114875">of Computer Science and Technology Harbin Institute of Technology, Harbin, China</affiliation>
<email confidence="0.656648">shujieliu@mtlab.hit.edu.cn</email>
<address confidence="0.812266">Research Asia, Beijing, China</address>
<email confidence="0.999422">chl@microsoft.com</email>
<email confidence="0.999422">mingzhou@microsoft.com</email>
<abstract confidence="0.991985">While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Peitra</author>
<author>Robert L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="14797" citStr="Brown et al., 1993" startWordPosition="2475" endWordPosition="2478">shown in Figure 4(c). Finally, the error measure in each interval can be calculated by Equation (2) (as shown in Figure 4(d)). All other steps in MERT for DPDI are the same as that for SMT. Figure 4: MERT for DPDI Part (a) shows how intervals are defined for SMT and part (b) for DPDI. Part (c) obtains the rank of correct E-spans in each interval and part (d) the error measure. Note that the beam size (max number of E-spans) for each F-span is 10. 5.3 Features The features used in DPDI are divided into three categories: 1) Model 1-based probabilities. Zhang and Gildea (2005) show that Model 1 (Brown et al., 1993; Och and Ney., 2000) probabilities of the word pairs inside and outside a span pair ( ğ‘’ğ‘–1,ğ‘’ğ‘–2 /[ğ‘“ğ‘—1,ğ‘“ğ‘—2]) are useful. Hence these two features: a) Inside probability (i.e. probability of word pairs within the span pair): ğ‘ğ‘–ğ‘›ğ‘ ğ‘’ğ‘–1,ğ‘–2 ğ‘“ğ‘—1,ğ‘—2 = ğ‘–âˆˆ ğ‘–1,ğ‘–2 ğ‘—âˆˆ ğ‘—1,ğ‘—2 b) Outside probability (i.e. probability of the word pairs outside the span pair): ğ‘ğ‘œğ‘¢ğ‘¡ ğ‘’ğ‘–1,ğ‘–2 ğ‘“ğ‘—1,ğ‘—2 = ğ‘–âˆ‰ ğ‘–1,ğ‘–2 ğ‘—âˆ‰ ğ‘—1,ğ‘—2 1 ğ½ âˆ’ ğ‘—2 + ğ‘—1 ğ‘ğ‘€1 ğ‘’ğ‘– ğ‘“ğ‘— where ğ½ is the length of the foreign sentence. 2) Heuristics. There are four features in this category. The features are explained with the -index (c) -8 -9 -10 loss (d) -8 -9 -100,000 Î£Î»jm Î»k </context>
</contexts>
<marker>Brown, Pietra, Peitra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Peitra, Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft Syntactic Constraints for Word Alignment through Discriminative Training.</title>
<date>2006</date>
<booktitle>In Proceedings of ACLCOLING.</booktitle>
<contexts>
<context position="1225" citStr="Cherry and Lin, 2006" startWordPosition="177" endWordPosition="180">that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the principles behind these tec</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft Syntactic Constraints for Word Alignment through Discriminative Training. In Proceedings of ACLCOLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion Transduction Grammar for Joint Phrasal Translation Modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="20102" citStr="Cherry and Lin (2007)" startWordPosition="3391" endWordPosition="3394">s a stop word. 6.2 DITG with Hierarchical Phrase Pairs extraction as stated in Chiang (2007). The rule probabilities and lexical weights in both Englishto-foreign and foreign-to-English directions are estimated and taken as features, in addition to those features in W-DITG, in the discriminative model of alignment hypothesis selection. The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al. (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. It is interesting to see if DPDI can benefit the parsing of a more realistic ITG. HP-DITG extends Cherry and Linâ€Ÿs approach by not only employing simple phrase pairs but also hierarchical phrase pairs (Chiang, 2007). The grammar is enriched with rules of the format: X e_i/f_i where e_i and f_i refer to the English and foreign side of the i-th (simple/hierarchical) phrase pair respectively. As example, if there is a simple phrase </context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion Transduction Grammar for Joint Phrasal Translation Modeling. In Proceedings of SSST, NAACL-HLT, Pages:17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="18128" citStr="Chiang (2007)" startWordPosition="3068" endWordPosition="3069">nt link ratio with the sum of the linkâ€Ÿs posterior probability. 6 The DITG Models The discriminative ITG alignment can be conceived as a two-staged process. In the first stage DPDI selects good span pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1- to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which de</context>
<context position="19573" citStr="Chiang (2007)" startWordPosition="3313" endWordPosition="3314"> IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posif1 f2 f3 f4 e1 e2 e3 e4 a) Link ratio: 2Ã—#ğ‘™ğ‘–ğ‘›ğ‘˜ğ‘  ğ‘–ğ‘›ğ‘ğ‘œğ‘› ğ‘“ ğ‘™ğ‘’ğ‘› +ğ‘’ğ‘™ğ‘’ğ‘› 320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs extraction as stated in Chiang (2007). The rule probabilities and lexical weights in both Englishto-foreign and foreign-to-English directions are estimated and taken as features, in addition to those features in W-DITG, in the discriminative model of alignment hypothesis selection. The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et</context>
<context position="24770" citStr="Chiang, 2007" startWordPosition="4201" endWordPosition="4202">84.8% 3 TTT 20 53&apos;&apos;/12&apos;5&apos;&apos; 5.2% 94.0% 86.5% 4 DP -- 11&apos;&apos;/15&apos;39&apos;&apos; 12.1% 91.4% 83.6% Table 2: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG. bound of precision, which should be defined as the hit score divided by the number of links produced by the system, is almost always 1.0 in practice. The upper bound of alignment F-score can thus be calculated as well. Figure 6: Recall Upper Bound Calculation Finally, we also do end-to-end evaluation using both F-score in alignment and Bleu score in translation. We use our implementation of hierarchical phrase-based SMT (Chiang, 2007), with standard features, for the SMT experiments. 7.2 Experiment Data Both discriminative pruning and alignment need training data and test data. We use the manually aligned Chinese-English dataset as used in Haghighi et al. (2009). The 491 sentence pairs in this dataset are adapted to our own Chinese word segmentation standard. 250 sentence pairs are used as training data and the other 241 are test data. The corresponding numbers of F-spans in training and test data are 4590 and 3951 respectively. In SMT experiments, the bilingual training dataset is the NIST training set excluding the Hong </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient Parsing for Transducer Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>227--235</pages>
<contexts>
<context position="22087" citStr="DeNero et al., 2009" startWordPosition="3737" endWordPosition="3740"> which fit the reordering constraint among X1 and X2. (Note that span pairs of any category in the ITG normal form grammar can substitute for X1or X2 .) If both conditions hold, then the span pair is assigned an alignment hypothesis which combines the alignment links among the lexical anchors (like of /çš„) and those links among the sub-span pairs. HP-ITG acquires the rules from HMM-based word-aligned corpus using standard phrase pair 7 Evaluation DPDI is evaluated against the baselines of Tictac-toe (TTT) pruning (Zhang and Gildea, 2005) and Dynamic Program (DP) pruning (Haghighi et al., 2009; DeNero et al., 2009) with respect to Chinese-to-English alignment and translation. Based on DPDI, HP-DITG is evaluated against the alignment systems GIZA++ and BITG. 7.1 Evaluation Criteria Four evaluation criteria are used in addition to the time spent on ITG parsing. We will first evaluate pruning regarding the pruning decisions themselves. That is, the first evaluation metric, pruning error rate (henceforth PER), measures how many correct E-spans are discarded. The major drawback of PER is that not all decisions in pruning would impact on alignment quality, since certain F-spans are of little use to the entire</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient Parsing for Transducer Grammars. In Proceedings of NAACL, Pages:227-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>SemiSupervised Training for StatisticalWord Alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>769--776</pages>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2006. SemiSupervised Training for StatisticalWord Alignment. In Proceedings of ACL, Pages:769-776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better Word Alignments with Supervised ITG Models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>923--931</pages>
<contexts>
<context position="1249" citStr="Haghighi et al., 2009" startWordPosition="181" endWordPosition="184"> all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the principles behind these techniques have certain con</context>
<context position="17334" citStr="Haghighi et al. (2009)" startWordPosition="2941" endWordPosition="2944">-span in the entire foreign sentence, and it is defined as 2ğ½1 ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘“ + ğ‘’ğ‘›ğ‘‘ğ‘“ , ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘“ /ğ‘’ğ‘›ğ‘‘ğ‘“ being the position of the first/last word of the F-span in the foreign sentence. ğ‘ğ‘œğ‘ ğ‘’ is defined similarly. The rationale behind this feature is the monotonic assumption, i.e. a phrase of the foreign sentence usually occupies roughly the same position of the equivalent English phrase. The feature value for 3 An inconsistent link connects a word within the phrase pair to some word outside the phrase pair. C.f. Deng et al. (2008) the example is |(1+2)/(2*4)-(2+3)/(2*4)| =0.25. 3) HMM-based probabilities. Haghighi et al. (2009) show that posterior probabilities from the HMM alignment model is useful for pruning. Therefore, we design two new features by replacing the link count in link ratio and inconsistent link ratio with the sum of the linkâ€Ÿs posterior probability. 6 The DITG Models The discriminative ITG alignment can be conceived as a two-staged process. In the first stage DPDI selects good span pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes th</context>
<context position="20184" citStr="Haghighi et al. (2009)" startWordPosition="3404" endWordPosition="3407">iang (2007). The rule probabilities and lexical weights in both Englishto-foreign and foreign-to-English directions are estimated and taken as features, in addition to those features in W-DITG, in the discriminative model of alignment hypothesis selection. The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al. (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. It is interesting to see if DPDI can benefit the parsing of a more realistic ITG. HP-DITG extends Cherry and Linâ€Ÿs approach by not only employing simple phrase pairs but also hierarchical phrase pairs (Chiang, 2007). The grammar is enriched with rules of the format: X e_i/f_i where e_i and f_i refer to the English and foreign side of the i-th (simple/hierarchical) phrase pair respectively. As example, if there is a simple phrase pair X (North Korea, åŒ— æœé²œ ) , then it is transformed into the ITG rule C &amp;quot;North Ko</context>
<context position="22065" citStr="Haghighi et al., 2009" startWordPosition="3733" endWordPosition="3736">form two sub-span pairs which fit the reordering constraint among X1 and X2. (Note that span pairs of any category in the ITG normal form grammar can substitute for X1or X2 .) If both conditions hold, then the span pair is assigned an alignment hypothesis which combines the alignment links among the lexical anchors (like of /çš„) and those links among the sub-span pairs. HP-ITG acquires the rules from HMM-based word-aligned corpus using standard phrase pair 7 Evaluation DPDI is evaluated against the baselines of Tictac-toe (TTT) pruning (Zhang and Gildea, 2005) and Dynamic Program (DP) pruning (Haghighi et al., 2009; DeNero et al., 2009) with respect to Chinese-to-English alignment and translation. Based on DPDI, HP-DITG is evaluated against the alignment systems GIZA++ and BITG. 7.1 Evaluation Criteria Four evaluation criteria are used in addition to the time spent on ITG parsing. We will first evaluate pruning regarding the pruning decisions themselves. That is, the first evaluation metric, pruning error rate (henceforth PER), measures how many correct E-spans are discarded. The major drawback of PER is that not all decisions in pruning would impact on alignment quality, since certain F-spans are of li</context>
<context position="25002" citStr="Haghighi et al. (2009)" startWordPosition="4234" endWordPosition="4238">he hit score divided by the number of links produced by the system, is almost always 1.0 in practice. The upper bound of alignment F-score can thus be calculated as well. Figure 6: Recall Upper Bound Calculation Finally, we also do end-to-end evaluation using both F-score in alignment and Bleu score in translation. We use our implementation of hierarchical phrase-based SMT (Chiang, 2007), with standard features, for the SMT experiments. 7.2 Experiment Data Both discriminative pruning and alignment need training data and test data. We use the manually aligned Chinese-English dataset as used in Haghighi et al. (2009). The 491 sentence pairs in this dataset are adapted to our own Chinese word segmentation standard. 250 sentence pairs are used as training data and the other 241 are test data. The corresponding numbers of F-spans in training and test data are 4590 and 3951 respectively. In SMT experiments, the bilingual training dataset is the NIST training set excluding the Hong Kong Law and Hong Kong Hansard, and our 5- gram language model is trained from the Xinhua section of the Gigaword corpus. The NIST&apos;03 test set is used as our development corpus and the NIST&apos;05 and NIST&apos;08 test sets are our test sets</context>
<context position="28474" citStr="Haghighi et al., 2009" startWordPosition="4845" endWordPosition="4849">.31 2 TTT 10 972h 37.96 27.37 3 TTT 20 2376h 38.13 27.58 4 DP -- 2068h 37.43 27.12 Table 3: Evaluation of DPDI against TTT and DP for HP-DITG ID WA- F-Score Bleu-05 Bleu-08 Model 1 HMM 80.1% 36.91 26.86 2 Giza++ 84.2% 37.70 27.33 3 BITG 85.9% 37.92 27.85 4 HP-DITG 87.0% 38.57 28.31 Table 4: Evaluation of DPDI against HMM, Giza++ and BITG Table 3 lists the word alignment time cost and SMT performance of different pruning methods. HP-DITG using DPDI achieves the best Bleu score with acceptable time cost. Table 4 compares HP-DITG to HMM (Vogel, et al., 1996), GIZA++ (Och and Ney, 2000) and BITG (Haghighi et al., 2009). It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score. Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant (Koehn, 2004). An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI. On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning like DPDI guides the subsequent ITG alignment process so that</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better Word Alignments with Supervised ITG Models. In Proceedings of ACL, Pages: 923-931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT</booktitle>
<pages>173--180</pages>
<contexts>
<context position="7136" citStr="Huang and Chiang (2005)" startWordPosition="1133" endWordPosition="1136">vels of pruning: 1) To discard some unpromising span pairs; 2) To discard some unpromising F-spans and/or E-spans; 3) To discard some unpromising alignment hypotheses for a particular span pair. The second type of pruning (used in Zhang et. al. (2008)) is very radical as it implies discarding too many span pairs. It is empirically found to be highly harmful to alignment performance and therefore not adopted in this paper. The third type of pruning is equivalent to minimizing the beam size of alignment hypotheses in each hypernode. It is found to be well handled by the K-Best parsing method in Huang and Chiang (2005). That is, during the bottom-up construction of the span pair repertoire, each span pair keeps only the best alignment hypothesis. Once the complete parse tree is built, the k-best list of the topmost span is obtained by minimally expanding the list of alignment hypotheses of minimal number of span pairs. The first type of pruning is equivalent to minimizing the number of hypernodes in a hypergraph. The task of ITG pruning is defined in this paper as the first type of pruning; i.e. the search for, given an F-span, the minimal number of Espans which are the most likely counterpart of that F-spa</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. In Proceedings of IWPT 2005, Pages:173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL.</booktitle>
<pages>440--447</pages>
<contexts>
<context position="19012" citStr="Och and Ney, 2000" startWordPosition="3213" endWordPosition="3216">the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posif1 f2 f3 f4 e1 e2 e3 e4 a) Link ratio: 2Ã—#ğ‘™ğ‘–ğ‘›ğ‘˜ğ‘  ğ‘–ğ‘›ğ‘ğ‘œğ‘› ğ‘“ ğ‘™ğ‘’ğ‘› +ğ‘’ğ‘™ğ‘’ğ‘› 320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs extraction as stated in Chiang (2007). The rule probabilities and lexical we</context>
<context position="28441" citStr="Och and Ney, 2000" startWordPosition="4839" endWordPosition="4842">5 08 1 DPDI 10 1092h 38.57 28.31 2 TTT 10 972h 37.96 27.37 3 TTT 20 2376h 38.13 27.58 4 DP -- 2068h 37.43 27.12 Table 3: Evaluation of DPDI against TTT and DP for HP-DITG ID WA- F-Score Bleu-05 Bleu-08 Model 1 HMM 80.1% 36.91 26.86 2 Giza++ 84.2% 37.70 27.33 3 BITG 85.9% 37.92 27.85 4 HP-DITG 87.0% 38.57 28.31 Table 4: Evaluation of DPDI against HMM, Giza++ and BITG Table 3 lists the word alignment time cost and SMT performance of different pruning methods. HP-DITG using DPDI achieves the best Bleu score with acceptable time cost. Table 4 compares HP-DITG to HMM (Vogel, et al., 1996), GIZA++ (Och and Ney, 2000) and BITG (Haghighi et al., 2009). It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score. Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant (Koehn, 2004). An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI. On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning like DPDI guides the subsequ</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL. Pages: 440-447</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11341" citStr="Och, 2003" startWordPosition="1868" endWordPosition="1869">ITG leads to the violation. Another situation is the â€inside-outâ€Ÿ alignment pattern (c.f. Figure 3). The ITG reordering constraint cannot be satisfied unless one of the links in this pattern is removed. f1 f2 f3 f4 e1 e2 e3 e4 Figure 3: An example of inside-out alignment The training samples thus obtained are positive training samples. If we apply some classifier for parameter training, then negative samples are also needed. Fortunately, our parameter training does not rely on any negative samples. 5.2 MERT for Pruning Parameter training of DPDI is based on Minimum Error Rate Training (MERT) (Och, 2003), a widely used method in SMT. MERT for SMT estimates model parameters with the objective of minimizing certain measure of translation errors (or maximizing certain performance measure of translation quality) for a development corpus. Given an SMT system which produces, with 2 The formulae of the inside and outside probability of a span pair will be elaborated in Section 5.3. The simpler alignment model we used is HMM. A: [e1,e3]/[f1,f2] {e1/f1,e3/f2},{e2/f1,e3/f2} Aâ†’[C,C] Aâ†’[C,C] (a) (b) Cw: [e1,e1]/[f1,f1] {e1/f1} C.: [e1]/Îµ Cw: [e2,e2]/[f1,f1] {e1/f1} C.: [e2]/Îµ Cw: [e3,e3]/[f2,f2] {e1/f1} </context>
<context position="18439" citStr="Och, 2003" startWordPosition="3121" endWordPosition="3122">ative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1- to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link proba</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, Pages:160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>17--19</pages>
<contexts>
<context position="5937" citStr="Klein and Manning, 2001" startWordPosition="929" endWordPosition="932">e ways. Larger and larger span pairs are recursively built until the sentence pair is built. Figure 1(a) shows one possible derivation for a toy example sentence pair with three words in each sentence. Each node (rectangle) represents a pair, marked with certain phrase category, of foreign span (F-span) and English span (E-span) (the upper half of the rectangle) and the associated alignment hypothesis (the lower half). Each graph like Figure 1(a) shows only one derivation and also only one alignment hypothesis. The various derivations in ITG parsing can be compactly represented in hypergraph (Klein and Manning, 2001) like Figure 1(b). Each hypernode (rectangle) comprises both a span pair (upper half) and the list of possible alignment hypotheses (lower half) for that span pair. The hyperedges show how larger span pairs are derived from smaller span pairs. Note that a hypernode may have more than one alignment hypothesis, since a hypernode may be derived through more than one hyperedge (e.g. the topmost hypernode in Figure 1(b)). Due to the use of normal form, the hypotheses of a span pair are different from each other. 4 Pruning in ITG Parsing The ITG parsing framework has three levels of pruning: 1) To d</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. In Proceedings of IWPT, Pages:17-19</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="28707" citStr="Koehn, 2004" startWordPosition="4884" endWordPosition="4885">37.92 27.85 4 HP-DITG 87.0% 38.57 28.31 Table 4: Evaluation of DPDI against HMM, Giza++ and BITG Table 3 lists the word alignment time cost and SMT performance of different pruning methods. HP-DITG using DPDI achieves the best Bleu score with acceptable time cost. Table 4 compares HP-DITG to HMM (Vogel, et al., 1996), GIZA++ (Och and Ney, 2000) and BITG (Haghighi et al., 2009). It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score. Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant (Koehn, 2004). An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI. On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning like DPDI guides the subsequent ITG alignment process so that less links inconsistent to good phrase pairs are produced. This also explains (in Tables 2 and 3) why DPDI with beam size 10 leads to higher Bleu than TTT with beam size 20, even though both pruning methods lead to roughly the same </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of EMNLP, Pages: 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="18329" citStr="Liu et al., 2005" startWordPosition="3101" endWordPosition="3104"> pairs. In the second stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1- to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM </context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of ACL, Pages: 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
</authors>
<title>A Discriminative Framework for Bilingual Word Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>81--88</pages>
<contexts>
<context position="18343" citStr="Moore, 2005" startWordPosition="3105" endWordPosition="3106">ond stage good alignment hypotheses are assigned to the span pairs selected by DPDI. Two discriminative ITG (DITG) models are investigated. One is word-to-word DITG (henceforth W-DITG), which observes the 1-to-1 constraint on alignment. Another is DITG with hierarchical phrase pairs (henceforth HP-DITG), which relaxes the 1- to-1 constraint by adopting hierarchical phrase pairs in Chiang (2007). Each model selects the best alignment hypotheses of each span pair, given a set of features. The contributions of these features are integrated through a log linear model (similar to Liu et al., 2005; Moore, 2005) like Equation (1). The discriminative training of the feature weights is again MERT (Och, 2003). The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert Moore. 2005. A Discriminative Framework for Bilingual Word Alignment. In Proceedings of EMNLP 2005, Pages: 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved Discriminative Bilingual Word Alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>513--520</pages>
<contexts>
<context position="19116" citStr="Moore et al., 2006" startWordPosition="3228" endWordPosition="3231">ntence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment. Like SMT (and unlike DPDI), it is the upper envelope which defines the intervals where the performance measure changes. 6.1 Word-to-word DITG The following features about alignment link are used in W-DITG: 1) Word pair translation probabilities trained from HMM model (Vogel, et.al., 1996) and IBM model 4 (Brown et.al., 1993; Och and Ney, 2000). 2) Conditional link probability (Moore, 2005). 3) Association score rank features (Moore et al., 2006). 4) Distortion features: counts of inversion and concatenation. 5) Difference between the relative positions of the words. The relative position of a word in a sentence is defined as the posif1 f2 f3 f4 e1 e2 e3 e4 a) Link ratio: 2Ã—#ğ‘™ğ‘–ğ‘›ğ‘˜ğ‘  ğ‘–ğ‘›ğ‘ğ‘œğ‘› ğ‘“ ğ‘™ğ‘’ğ‘› +ğ‘’ğ‘™ğ‘’ğ‘› 320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs extraction as stated in Chiang (2007). The rule probabilities and lexical weights in both Englishto-foreign and foreign-to-English directions are estimated and taken as features, i</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert Moore, Wen-tau Yih, and Andreas Bode. 2006. Improved Discriminative Bilingual Word Alignment. In Proceedings of ACL, Pages: 513-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="28413" citStr="Vogel, et al., 1996" startWordPosition="4834" endWordPosition="4837">time Bleu- Bleuing size cost 05 08 1 DPDI 10 1092h 38.57 28.31 2 TTT 10 972h 37.96 27.37 3 TTT 20 2376h 38.13 27.58 4 DP -- 2068h 37.43 27.12 Table 3: Evaluation of DPDI against TTT and DP for HP-DITG ID WA- F-Score Bleu-05 Bleu-08 Model 1 HMM 80.1% 36.91 26.86 2 Giza++ 84.2% 37.70 27.33 3 BITG 85.9% 37.92 27.85 4 HP-DITG 87.0% 38.57 28.31 Table 4: Evaluation of DPDI against HMM, Giza++ and BITG Table 3 lists the word alignment time cost and SMT performance of different pruning methods. HP-DITG using DPDI achieves the best Bleu score with acceptable time cost. Table 4 compares HP-DITG to HMM (Vogel, et al., 1996), GIZA++ (Och and Ney, 2000) and BITG (Haghighi et al., 2009). It shows that HP-DITG (with DPDI) is better than the three baselines both in alignment F-score and Bleu score. Note that the Bleu score differences between HP-DITG and the three baselines are statistically significant (Koehn, 2004). An explanation of the better performance by HP-DITG is the better phrase pair extraction due to DPDI. On the one hand, a good phrase pair often fails to be extracted due to a link inconsistent with the pair. On the other hand, ITG pruning can be considered as phrase pair selection, and good ITG pruning </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING, Pages: 836-841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>PESA: Phrase Pair Extraction as Sentence Splitting.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<marker>Vogel, 2005</marker>
<rawString>Stephan Vogel. 2005. PESA: Phrase Pair Extraction as Sentence Splitting. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="938" citStr="Wu, 1997" startWordPosition="132" endWordPosition="133">ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et</context>
<context position="5154" citStr="Wu, 1997" startWordPosition="803" endWordPosition="804">hout this constraint will be elaborated in Section 6. Secondly, the simple ITG leads to redundancy if word alignment is the sole purpose of applying ITG. For instance, there are two parses for three consecutive word pairs, viz. [a/aâ€™ [b/bâ€™ c/ câ€™] ] and [[a/aâ€™ b/bâ€™] c/câ€™] . The problem of redundancy is fixed by adopting ITG normal form. In fact, normal form is the very first key to speeding up ITG. The ITG normal form grammar as used in this paper is described in Appendix A. 3 Basics of ITG Parsing Based on the rules in normal form, ITG word alignment is done in a similar way to chart parsing (Wu, 1997). The base step applies all relevant terminal unary rules to establish the links of word pairs. The word pairs are then combined into span pairs in all possible ways. Larger and larger span pairs are recursively built until the sentence pair is built. Figure 1(a) shows one possible derivation for a toy example sentence pair with three words in each sentence. Each node (rectangle) represents a pair, marked with certain phrase category, of foreign span (F-span) and English span (E-span) (the upper half of the rectangle) and the associated alignment hypothesis (the lower half). Each graph like Fi</context>
<context position="19982" citStr="Wu (1997)" startWordPosition="3375" endWordPosition="3376"> 320 tion of the word divided by sentence length. 6) Boolean features like whether a word in the word pair is a stop word. 6.2 DITG with Hierarchical Phrase Pairs extraction as stated in Chiang (2007). The rule probabilities and lexical weights in both Englishto-foreign and foreign-to-English directions are estimated and taken as features, in addition to those features in W-DITG, in the discriminative model of alignment hypothesis selection. The 1-to-1 assumption in ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Wu (1997) proposes a bilingual segmentation grammar extending the terminal rules by including phrase pairs. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al. (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. It is interesting to see if DPDI can benefit the parsing of a more realistic ITG. HP-DITG extends Cherry and Linâ€Ÿs approach by not only employing simple phrase pairs but also hierarchical phrase pairs (Chiang, 2007). The grammar is enriched with rules of the format: X e_i/f_i where e_i and f_i refer to the Engli</context>
<context position="30186" citStr="Wu (1997)" startWordPosition="5139" endWordPosition="5140">uperior to all existing ITG pruning methods, and the HP-DITG alignment system is shown to improve state-ofthe-art alignment and translation quality. The current DPDI model employs a very limited set of features. Many features are related only to probabilities of word pairs. As the success of HP-DITG illustrates the merit of hierarchical phrase pair, in future we should investigate more features on the relationship between span pair and hierarchical phrase pair. Appendix A. The Normal Form Grammar Table 5 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null. 1 S â€”ï¿½ A|B|C 2 A - [A B] |[A C] |[B B] |[BC] |[C B] |[C C 3 B â€”ï¿½ (A A) |(A C) |(B A) |(B C) B â€”ï¿½ (C A) |(C C) 4 C - Cw |Cfw |Cew 5 C â€”ï¿½ [Cew Cfw 6 Cw â€”ï¿½ u/v 7 Ce â€”ï¿½ Â£/v; Cf â€”ï¿½ u/Â£ 8 Cem - Ce |[Cem Ce]; Cfm - Cf |[Cfm Cf 9 Cew - [Cem Cw ]; Cfw - [Cfm Cw] Table 5: ITG Rules in Normal Form In these rules, S is the Start symbol; A is the category for concatenating combination whereas B for inverted combination. Rules (2) and (3) are inherited from Wu (1997). Rules (4) divide the terminal category C into subcategories. Rule schema (6) subsumes all ter</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic Lexicalized Inversion Transduction Grammar for Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1203" citStr="Zhang and Gildea, 2005" startWordPosition="173" endWordPosition="176">Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1 Introduction Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of SCFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang and Gildea, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). A major obstacle in ITG alignment is speed. The original (unsupervised) ITG algorithm has complexity of O(n6). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. As all the princ</context>
<context position="14759" citStr="Zhang and Gildea (2005)" startWordPosition="2466" endWordPosition="2470">n each interval can then be figured out as shown in Figure 4(c). Finally, the error measure in each interval can be calculated by Equation (2) (as shown in Figure 4(d)). All other steps in MERT for DPDI are the same as that for SMT. Figure 4: MERT for DPDI Part (a) shows how intervals are defined for SMT and part (b) for DPDI. Part (c) obtains the rank of correct E-spans in each interval and part (d) the error measure. Note that the beam size (max number of E-spans) for each F-span is 10. 5.3 Features The features used in DPDI are divided into three categories: 1) Model 1-based probabilities. Zhang and Gildea (2005) show that Model 1 (Brown et al., 1993; Och and Ney., 2000) probabilities of the word pairs inside and outside a span pair ( ğ‘’ğ‘–1,ğ‘’ğ‘–2 /[ğ‘“ğ‘—1,ğ‘“ğ‘—2]) are useful. Hence these two features: a) Inside probability (i.e. probability of word pairs within the span pair): ğ‘ğ‘–ğ‘›ğ‘ ğ‘’ğ‘–1,ğ‘–2 ğ‘“ğ‘—1,ğ‘—2 = ğ‘–âˆˆ ğ‘–1,ğ‘–2 ğ‘—âˆˆ ğ‘—1,ğ‘—2 b) Outside probability (i.e. probability of the word pairs outside the span pair): ğ‘ğ‘œğ‘¢ğ‘¡ ğ‘’ğ‘–1,ğ‘–2 ğ‘“ğ‘—1,ğ‘—2 = ğ‘–âˆ‰ ğ‘–1,ğ‘–2 ğ‘—âˆ‰ ğ‘—1,ğ‘—2 1 ğ½ âˆ’ ğ‘—2 + ğ‘—1 ğ‘ğ‘€1 ğ‘’ğ‘– ğ‘“ğ‘— where ğ½ is the length of the foreign sentence. 2) Heuristics. There are four features in this category. The features are explained with the -index (c) -8 -</context>
<context position="22009" citStr="Zhang and Gildea, 2005" startWordPosition="3724" endWordPosition="3727">&amp;quot;of&amp;quot; and &amp;quot;çš„&amp;quot;, and if the remaining words in its yield can form two sub-span pairs which fit the reordering constraint among X1 and X2. (Note that span pairs of any category in the ITG normal form grammar can substitute for X1or X2 .) If both conditions hold, then the span pair is assigned an alignment hypothesis which combines the alignment links among the lexical anchors (like of /çš„) and those links among the sub-span pairs. HP-ITG acquires the rules from HMM-based word-aligned corpus using standard phrase pair 7 Evaluation DPDI is evaluated against the baselines of Tictac-toe (TTT) pruning (Zhang and Gildea, 2005) and Dynamic Program (DP) pruning (Haghighi et al., 2009; DeNero et al., 2009) with respect to Chinese-to-English alignment and translation. Based on DPDI, HP-DITG is evaluated against the alignment systems GIZA++ and BITG. 7.1 Evaluation Criteria Four evaluation criteria are used in addition to the time spent on ITG parsing. We will first evaluate pruning regarding the pruning decisions themselves. That is, the first evaluation metric, pruning error rate (henceforth PER), measures how many correct E-spans are discarded. The major drawback of PER is that not all decisions in pruning would impa</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic Lexicalized Inversion Transduction Grammar for Alignment. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>314--323</pages>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proceedings of ACL, Pages: 314-323.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>