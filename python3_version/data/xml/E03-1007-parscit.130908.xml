<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006643">
<title confidence="0.9988985">
Using POS Information for Statistical Machine Translation into
Morphologically Rich Languages
</title>
<author confidence="0.989123">
Nicola Ueffing and Hermann Ney
</author>
<affiliation confidence="0.9725995">
Lehrstuhl ftir Informatik VI - Computer Science Department
RWTH Aachen - University of Technology
</affiliation>
<email confidence="0.985411">
fueffing,neyl@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.996533" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995450625">
When translating from languages with
hardly any inflectional morphology like
English into morphologically rich lan-
guages, the English word forms often
do not contain enough information for
producing the correct fullform in the
target language. We investigate meth-
ods for improving the quality of such
translations by making use of part-of-
speech information and maximum en-
tropy modeling. Results for translations
from English into Spanish and Catalan
are presented on the LC-STAR corpus
which consists of spontaneously spoken
dialogues in the domain of appointment
scheduling and travel planning.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99853">
In this paper, we address the question of how part-
of-speech (POS) information can help improv-
ing the quality of Statistical Machine Translation
(SMT). One of the main problems when translat-
ing from a language with hardly any inflectional
morphology (which is English in our experiments)
into one with richer morphology (here: Spanish
and Catalan) is the production of the correct in-
flected form in the target language. We introduce
transformations to the English string that are based
on the part-of-speech information and show how
this knowledge source can help SMT. Systematic
evaluations will show that the quality of the gen-
erated translations is improved.
The transformations we apply are the following:
Treatment of verbs In Catalan and Spanish, the
pronoun before a verb is often omitted and in-
stead, the person is expressed via the ending
of the verb. The same holds for future tense
and for the modes expressed through &apos;would&apos;
and &apos;should&apos; in English. Since this makes it
hard to generate the correct translation of a
given English verb, we propose a method re-
sulting in English word forms containing suf-
ficient information.
Question inversion In English, interrogative
phrases have a word order that is different
from declarative sentences: Either an auxil-
iary &apos;do&apos; is inserted or the order of verb and
pronoun is inverted. Since this is different
in Spanish and Catalan, we modify the word
order in English to make it more similar to
the Spanish/Catalan one and to help the verb
treatment mentioned above.
The paper is organized as follows: Related work
is treated in Section 2. In Section 3, we shortly
review the statistical approach to machine transla-
tion. Then, we introduce the transformations that
we apply to the less inflected language of the two
under consideration (namely English) in Section 4.
After describing the maximum entropy approach
and the training procedure we use for the statisti-
cal lexicon in Section 5, we present results on the
trilingual LC-STAR corpus in Section 6. Then, we
conclude and present ideas about future work in
Section 7.
</bodyText>
<page confidence="0.997029">
347
</page>
<sectionHeader confidence="0.999384" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99987675">
Publications dealing with the integration of lin-
guistic information into the process of statisti-
cal machine translation are rather few although
this had already been suggested in (Brown et al.,
1992). (Niel3en and Ney, 2001b) introduce hier-
archical lexicon models including baseform and
POS information for translation from German into
English. Information contained in the German en-
tries that are not relevant for the generation of
the English translation are omitted. Unlike this,
we investigate methods for enriching English with
knowledge to help selecting the correct fullform in
a morphologically richer language.
(Niefien and Ney, 2001a) propose reordering oper-
ations for the language pair German—English that
help SMT by harmonizing word order between
source and target. The question inversion we apply
was inspired by this; nevertheless, we do not per-
form a full morpho-syntactic analysis, but make
use only of POS information which can be ob-
tained from freely available tools.
(Garcia-Varea et al., 2001) apply a maximum en-
tropy approach for training the statistical lexicon,
but do not take any linguistic information into ac-
count.
The use of POS information for improving statisti-
cal alignment quality is described in (Toutanova et
al., 2002), but no translation results are presented.
</bodyText>
<sectionHeader confidence="0.98254" genericHeader="method">
3 Statistical Machine Translation
</sectionHeader>
<bodyText confidence="0.997097486486486">
The goal of machine translation is the translation
of an input string Si,. . . , s j in the source language
into a target language string ti tI. We choose
the string that has maximal probability given the
source string, Pr(tils1). Applying Bayes&apos; deci-
sion rule yields the following criterion:
arg max Pr(ti si) 4)1 (1)
tf
= arg max{Pr(t1) • Pr(s1
tf
Through this decomposition of the probability, we
obtain two knowledge sources: the translation and
the language model. Those two can be modelled
independently of each other.
The correspondence between the words in the
source and the target string is described by align-
ments that assign target word positions to each
source word position. The probability of a certain
target language word to occur in the target string
is assumed to depend basically only on the source
words aligned to it.
The search is denoted by the arg max operation
in Eq. 1, i.e. it explores the space of all possible
target language strings and all possible alignments
between the source and the target language string
to find the one with maximal probability.
The input string can be preprocessed before being
passed to the search algorithm. If necessary, the
inverse of these transformations will be applied
to the generated output string. In the work pre-
sented here, we restrict ourselves to transforming
only one language of the two: the source, which
has the less inflected morphology.
For descriptions of SMT systems see for exam-
ple (Germann et al., 2001; Och et al., 1999; Till-
mann and Ney, 2002; Vogel et al., 2000; Wang and
Waibel, 1997).
</bodyText>
<sectionHeader confidence="0.881054" genericHeader="method">
4 Transformations in the Less Inflected
Language
</sectionHeader>
<bodyText confidence="0.9998558">
When translating from English into languages
with a highly inflected morphology, the production
of the correct fullform often causes problems. Our
experience on several corpora shows that the error
rate of a translation from English into morpholog-
ically richer languages decreases by 10% relative
if we aim at producing only the correct baseform
instead of the fully inflected word. The transfer
of the meaning expressed in the baseform is easier
than deciding on the correct inflected form.
</bodyText>
<subsectionHeader confidence="0.999839">
4.1 Treatment of Verbs
</subsectionHeader>
<bodyText confidence="0.999966">
Especially the translation of verbs is difficult since
there are many different inflections in Spanish
and Catalan whereas there are only few in En-
glish. Moreover, the pronouns and modals are of-
ten omitted in Spanish and Catalan and this infor-
mation is expressed through the suffix. This makes
it very hard for word-based systems to generate
the correct inflection from the English verb which
does not contain sufficient information. Thus, sev-
eral English words will have to be aligned to the
Spanish or Catalan verbs. This process is rela-
</bodyText>
<page confidence="0.992754">
348
</page>
<bodyText confidence="0.999991142857143">
tively difficult for the algorithm and causes noise
in the statistical lexicon if English pronouns are re-
garded as translations of Spanish or Catalan verbs.
In order to enrich the English verb with the needed
information, we combine pronouns and/or modals
with following verbs and treat those combinations
as &apos;new&apos; fullform words in English. Thus we can
obtain the information needed to select the correct
verb form in the target language from one single
English word. The identification of English pro-
nouns, modals and verbs was done by POS tag-
ging applied to the English part of the corpus.
We decided to transform the source language in-
stead of the target language, because in this case
we need only the POS tags of the source language
as additional knowledge source and nothing else.
Another possible approach would have been to
split the suffix in the target language (e.g. &apos;esta&apos;
into &apos;estar P3S&apos;). This would require postprocess-
ing tools that are able to generate the correct verb
form from the baseform and the person and tense
information.
Table 1 gives examples of words that have been
spliced to form new entries of the English lexi-
con. For example, we splice the phrase &apos;you think&apos;
to form the single entry &apos;you_think&apos; which con-
tains sufficient information for producing the cor-
rect Spanish verb form &apos;crees&apos; or the Catalan
&apos;creus&apos; . Similarly, the modal auxiliaries can be
added as well, like in the entry &apos;you_will_have&apos;
which is much better suited for being translated
into &apos;tendras&apos; (Spanish) or &apos;tindras&apos; (Catalan) than
the verb &apos;have&apos; alone. Moreover, in a single word
based lexicon, three single entries would have to
be added for the translation of &apos;you will have&apos;
into &apos;tendras&apos;: (you,tendras), (will,tendras) and
(have,tendras), which spreads the translation prob-
ability over far too many entries and makes the
probability distribution unfocused.
As the last example in Table 1 shows, &apos;you can
go&apos; is spliced only into two words instead of one
in order to better match the Spanish/Catalan form.
</bodyText>
<subsectionHeader confidence="0.992928">
4.2 Question Treatment
</subsectionHeader>
<bodyText confidence="0.963033976190476">
In English interrogative phrases, either an auxil-
iary &apos;do&apos; is inserted or the order of verb and pro-
noun is inverted. The auxiliary &apos;do&apos; does not carry
information that is relevant when translating into
Table 1: Examples of spliced words in the English
vocabulary
original POS tags spliced words
you go PRP VBP you_go
you went PRP VBD you_went
you think PRP VBP you_think
you will have PRP MD VB you_will_have
you can go PRP MD VB you_can go
Spanish or Catalan. Thus, we can remove it from
the sentence without harming the translation pro-
cess (as described in (NieBen and Ney, 2001a) for
the language pair German—English). However, we
do not remove a question supporting &apos;do&apos; in past
tense, i. e. &apos;did&apos; is kept in the phrase, because this
is the only word containing the tense information.
Afterwards, we can merge the pronoun and verb
as depicted in Table 2: &apos;did you go&apos; is transformed
into &apos;you_did go&apos;. We do not splice &apos;you_did&apos; and
&apos;go&apos;, because the English simple past is translated
into present perfect in Catalan; and it is very likely
to be translated into present perfect in Spanish, es-
pecially in colloquial language as it is present in
this task. The form &apos;you_did go&apos; is well suited to
be translated into the Spanish &apos;has ido&apos; or the Cata-
lan &apos;has anat&apos;.
If there is no question supporting &apos;do&apos; and the or-
der of pronoun and verb is inverted — see the exam-
ple &apos;how are you?&apos; in Table 3 — we first swap the
two words and then perform the splicing step. This
is done in order to avoid having two lexical entries
with the same translation: for example, &apos; you_are&apos;
and the interrogative &apos;are_you&apos; both have the same
translation in Spanish or Catalan, respectively.
Table 3 presents examples of transformed English
questions. Comparing them to the Spanish and
Catalan reference, we see that it is easier to find
a word-to-word mapping for the modified English
sentences.
</bodyText>
<sectionHeader confidence="0.994036" genericHeader="method">
5 Maximum Entropy Training
</sectionHeader>
<bodyText confidence="0.9998976">
If we merge the pronouns/modals and verbs as de-
scribed above, it might happen that the verb itself
(or one of its inflections) has never been seen in
training except from its appearance in the new en-
tries in the lexicon which result from the splic-
</bodyText>
<page confidence="0.99892">
349
</page>
<tableCaption confidence="0.997756">
Table 2: Examples of spliced words in the English vocabulary after question inversion
</tableCaption>
<table confidence="0.861394166666667">
original POS tags spliced words
do you go VBP PRP VB you_go
did you go VBD PRP VB you_did go
have you gone VBP PRP VBN you_have gone
will you go MD PRP VB you_will_go
can you go PRP MD VB you_can go
</table>
<tableCaption confidence="0.993125">
Table 3: Examples of transformed English sentences
</tableCaption>
<table confidence="0.999144266666667">
Original how are you?
Question Inversion how you are?
Verb Treatment how you_are ?
Catalan Sentence corn esta ?
Spanish Sentence i, c6mo estas ?
Original or do you think we want to stay [... 1 ?
Question Inversion or you think we want to stay [... 1 ?
Verb Treatment or you_think we_want to stay [... ] ?
Catalan Sentence o creu que voldrem quedar-nos [... ] ?
Spanish Sentence i, o cree que querremos quedamos IL...] ?
Original did you say the eighteenth?
Question Inversion you did say the eighteenth?
Verb Treatment you_did say the eighteenth?
Catalan Sentence has dit el divuit ?
Spanish Sentence i, has dicho el dieciocho ?
</table>
<bodyText confidence="0.999757875">
ing operation. This makes it impossible to trans-
late the verb itself, because it is then unknown
to the system. The same holds for combinations
of pronouns and verbs that are unseen in train-
ing, e. g. the training corpus contains the bigram
&apos;I went&apos;, but not the one &apos;she went&apos;. In order
to overcome this problem, we train our lexicon
model using maximum entropy.
</bodyText>
<subsectionHeader confidence="0.942286">
5.1 The Maximum Entropy Approach
</subsectionHeader>
<bodyText confidence="0.999992583333333">
The maximum entropy approach (Berger et al.,
1996) presents a powerful framework for the com-
bination of several knowledge sources. This prin-
ciple recommends to choose the distribution which
preserves as much uncertainty as possible in terms
of maximizing the entropy. The distribution is re-
quired to satisfy constraints, which represent facts
known from the data. These constraints are ex-
pressed on the basis of feature functions hu,(s,t),
where (s, t) is a pair of source and target word.
The lexicon probability of a source word given the
target word has the following functional form
</bodyText>
<equation confidence="0.98869375">
1
t) Z(t) exp [Y‘
with the normalization factor
Z(t) = Eexp [E X„,h,„,(s&apos; ,t)]
</equation>
<bodyText confidence="0.999984">
where A = {Am} is the set of model parameters
with one weight A, for each feature function hm.
The features we use in our model are
</bodyText>
<listItem confidence="0.9988525">
• a lexical feature (for the entries of the trans-
formed vocabulary):
</listItem>
<equation confidence="0.9750365">
128, (s, t) = (5(s, s&apos;) • 6(t, t&apos;)
P(s
</equation>
<page confidence="0.920171">
350
</page>
<bodyText confidence="0.7145805">
• the verb contained in a transformed lexicon
entry (e.g. &apos;go&apos; for &apos;you_go&apos; or &apos;you_will_go):
hs, ,v(s ,t) = S(s. s&apos;) • V erb(t, v) ,
where
</bodyText>
<equation confidence="0.916728666666667">
1, if t contains the verb v
V erb(t, v) =
0, otherwise
</equation>
<bodyText confidence="0.999224714285714">
This enables us to translate the verb alone even if
it occurs in the training corpus only as a spliced
entry.
For an introduction to maximum entropy modeling
and training procedures, the reader is referred to
the corresponding literature, for instance (Berger
et al., 1996) or (Ratnaparkhi, 1997).
</bodyText>
<subsectionHeader confidence="0.991114">
5.2 Training
</subsectionHeader>
<bodyText confidence="0.999859">
We performed the following training steps:
</bodyText>
<listItem confidence="0.98892">
• transform the English (= source language)
part of the corpus as described in Sections 4.1
and 4.2
• train the statistical translation system using
this modified source language corpus 1
• with the resulting alignment, train the lexicon
model using maximum entropy with the fea-
tures described in Section 5.1
</listItem>
<bodyText confidence="0.999830666666667">
This training can be performed using converg-
ing iterative training procedures like described by
(Darroch and Ratcliff, 1972) or (Della Pietra et
al., 1997) 2. The basic training procedures for the
translation system and the language model need
not be changed.
</bodyText>
<subsectionHeader confidence="0.993248">
5.3 Translation process
</subsectionHeader>
<bodyText confidence="0.999950625">
For translation, we can use an SMT system where
the search algorithm does not have to be modified.
Before the translation process, we transform the
input in the same way as the training corpus be-
fore training the alignment (see Section 5.2). We
simply have to exclude those words from splicing
where the splicing operation yields an unknown
word.
</bodyText>
<footnote confidence="0.807864166666667">
&apos;This training was done using the GIZA++ toolkit which
can be downloaded from http://www-i6.informatik.rwth-
aachen.deroch/software/GIZA++.html
2We made use of the toolkit YASMET which can
be downloaded from http://www-i6.informatik.rwth-
aachen.deroch/software/YASMET.html
</footnote>
<sectionHeader confidence="0.998403" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.785048">
6.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99998">
We performed experiments on the trilingual
corpus which is successively built within the
LC-STAR project. It comprises the languages
English, Spanish and Catalan, whereof we used
English as source and Spanish and Catalan as tar-
get languages. At the time of our experiments, we
had about 13k sentences per language available;
the statistics are given in Table 4.
The corpus consists of transcriptions of sponta-
neously spoken dialogues. Thus, the sentences
often lack correct syntactic structure. The domain
of this task is appointment scheduling and travel
arrangements.
The POS information for the English part of the
corpus was generated using the Brill tagger3.
As Table 4 shows, the splicing operation increases
the cardinality of the English vocabulary as
well as the number of singletons significantly.
Nevertheless, they are still below those numbers
for Spanish and Catalan.
</bodyText>
<subsectionHeader confidence="0.996388">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99999125">
The quality of the output of our machine transla-
tion system is measured automatically by compar-
ing the generated translation to a given reference
translation. The two following criteria are used:
</bodyText>
<listItem confidence="0.869409">
• WER (word error rate):
</listItem>
<bodyText confidence="0.9967315">
The word error rate is based on the Leven-
shtein distance. It is computed as the min-
imum number of substitution, insertion and
deletion operations that have to be performed
to convert the generated string into the ref-
erence string. Since some sentences in the
develop and test set occur several times with
different reference translations (which holds
especially for short sentences like &apos;okay,
good-bye&apos;), we calculate the minimal dis-
tance to this set of references as proposed
in (NieBen et al., 2000).
</bodyText>
<listItem confidence="0.9620695">
• BLEU (bilingual evaluation understudy):
(Papineni et al., 2002) have proposed a
</listItem>
<footnote confidence="0.7216405">
3 The Brill tagger can be downloaded from
http://www.research.microsoft.com/users/brill/
</footnote>
<page confidence="0.997962">
351
</page>
<tableCaption confidence="0.984748">
Table 4: Statistics of the training, develop and test set of the English-Spanish-Catalan LC-STAR corpus
(*number of words without punctuation marks)
</tableCaption>
<table confidence="0.99978355">
English Spanish Catalan
Original Transformed
Training Sentences 13 352
Words
Words&amp;quot;
123 454 114 099 118 534 118 137
101 738 92 383 96 997 96 503
Vocabulary Size 2 154 2 776 3 933 3 572
Singletons
790 (37%) 1 165 (42%) 1 844 (47%) 1 658 (47%)
Develop Sentences 272
Words
Unknown Words
2 267 2 096 2217 2211
21 22 34 34
Test Sentences 262
Words
Unknown Words
2 626 2 460 2 451 2 470
17 18 30 35
</table>
<bodyText confidence="0.997579214285714">
method of automatic machine translation
evaluation, which they call &amp;quot;BLEU&amp;quot;. It is
based on the notion of modified n-gram pre-
cision, for which all candidate n-gram counts
in the translation are collected and clipped
against their corresponding maximum refer-
ence counts. These clipped candidate counts
are summed and normalized by the total num-
ber of candidate n-grams. Since BLEU ex-
presses quality, we determine 100—BLEU to
transform it into an error measure.
Although these measures are only approximations,
they seem to be sufficient at the present level of
performance of machine translation systems.
</bodyText>
<subsectionHeader confidence="0.998991">
6.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.992051243902439">
We compared the two statistical lexica obtained
from the baseline system and from the maximum
entropy training on the transformed corpus. For
the baseline lexicon, we observed an average of
5.82 Catalan translation candidates per English
word and 6.16 Spanish translation candidates.
These numbers are significantly reduced in the
lexicon which was trained on the transformed
corpus using maximum entropy: there, we have an
average of 4.20 for Catalan and 4.46 for Spanish.
Especially for (nominative) English pronouns
(which have many verbs as translation candidates
in the baseline lexicon), the number of translation
candidates was substantially scaled down by a
factor around 4. This shows that our method was
successful in producing a more focused lexicon
probability distribution.
We performed translation experiments with
an implementation of the IBM-4 translation
model (Brown et al., 1993). A description of
the system can be found in (Tillmann and Ney,
2002).
Table 5 presents an assessment of translation qual-
ity for both the language pairs English—Catalan
and English—Spanish. We see that there is a signif-
icant decrease in error rate for the translation into
Catalan. This change is consistent across both er-
ror rates, the WER and 100—BLEU.
For translations from English into Spanish, the
improvement is less substantial. A reason for
this might be that the Spanish vocabulary contains
more entries and the ratio between fullforms and
baseforms is higher: 1.57 for Spanish versus 1.53
for Catalan4. This makes it more difficult for the
system to choose the correct inflection when gen-
erating a Spanish sentence. We assume that the
extension of our approach to other word classes
than verbs will yield a quality gain for translations
into Spanish.
Table 6 shows several sentences from the English
LC-STAR develop and test corpus that were trans-
</bodyText>
<footnote confidence="0.974396">
4The lemmatization of Spanish and Catalan was produced
using the analyser from UPC Barcelona: MACO+ and RE-
LAX.
</footnote>
<page confidence="0.993808">
352
</page>
<tableCaption confidence="0.999492">
Table 5: Translation error rates [%] for English—Catalan and for English—Spanish
</tableCaption>
<table confidence="0.999783333333333">
Develop Test
WER 100-BLEU WER 100-BLEU
Catalan Baseline 37.6 58.2 33.0 49.2
+ Transformations 35.0 55.1 30.8 46.6
Spanish Baseline 35.4 57.6 32.1 48.9
+ Transformations 35.0 55.8 31.5 47.6
</table>
<bodyText confidence="0.995119923076923">
lated into Catalan. We see that it is easier for the
system to generate the correct verb inflection in
Catalan if the verb is enriched with the pronoun.
In the baseline system, it happens that words are
inserted — like &apos;far&apos; as translation of &apos;will&apos; in the
second example which is incorrect. This can be
avoided by the splicing of words.
In the last example, we see that the baseline
system generates one word each for the English
&apos;I prefer&apos; and does not find the correct translation,
whereas transformations yield an accurate transla-
tion of this expression, because the spliced word
contains sufficient information.
</bodyText>
<sectionHeader confidence="0.989365" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99995028">
We presented a method for improving quality of
statistical machine translation from English into
morphologically richer languages like Spanish and
Catalan. Using POS tags as additional knowledge
source, we enrich the English verbs such that they
contain more information relevant for selecting the
correct inflected form in the target language. The
lexicon model was then trained using the maxi-
mum entropy approach, taking the verbs as addi-
tional features.
Results were given for translation from English
into Spanish and Catalan on the LC-STAR cor-
pus which consists of spontaneously spoken dia-
logues in the domain of appointment scheduling
and travel arrangement. Our experiments show
that translation quality can be significantly in-
creased through the use of our approach: the word
error rate on the Catalan development set for ex-
ample decreased by 2.5% absolute.
We plan to investigate other methods of enrich-
ing the English words with information. It will
be interesting to see how other word classes,
e. g. nouns, can be handled in order to improve
quality of translations into languages with a highly
inflected morphology.
</bodyText>
<sectionHeader confidence="0.998412" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999190333333333">
This work was partly supported by the LC-STAR
project by the European Community (IST project
ref. no. 2001-32216).
</bodyText>
<sectionHeader confidence="0.997177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999586962962963">
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39-72, March.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, J.D.
Lafferty, and R.L. Mercer. 1992. Analysis, statis-
tical transfer, and synthesis in machine translation.
In Proc. TMI 1992: 4th Int. Conf. on Theoretical
and Methodological Issues in MT, pages 83-100,
Montréal, P.O., Canada, June.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263-311 .
J.N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470-1480.
S.A. Della Pietra, V.J. Della Pietra, and J. Lafferty.
1997. Inducing features in random fields. IEEE
Trans. on Pattern Analysis and Machine Inteligence,
19(4):380-393, July.
I. Garcia-Varea, F.J. Och, H. Ney, and F. Casacu-
berta. 2001. Refined lexicon models for statisti-
cal machine translation using a maximum entropy
approach. In Proc. 39th Annual Meeting of the
Assoc. for Computational Linguistics - joint with
EACL, pages 204-211, Toulouse, France, July.
</reference>
<page confidence="0.999799">
353
</page>
<tableCaption confidence="0.994058">
Table 6: Examples of English—Catalan translations with and without transformation
</tableCaption>
<table confidence="0.6566576875">
Source we_exchange them and, that would be good.
Reference les canviem i, aixa estaria be.
Baseline ens canviem i, aixo estaria be.
Verb Treatment les canviem i, aixo estaria be.
Source okay, and Lwill, speak to you soon then.
Reference d&apos; acord, i jo, parlare amb tu aviat doncs.
Baseline d&apos; acord, i jo far, parlare amb tu aviat doncs.
Verb Treatment d&apos; acord, i jo, parlare amb tu aviat doncs.
Source Lbelieve, the flight is every day?
Reference crec, que el vol és cada dia?
Baseline suposo, el vol és cada dia?
Verb Treatment crec, que el vol és cada dia?
Source Lprefer single.
Reference prefereixo individual.
Baseline jo preferiria una individual.
Verb Treatment prefereixo una individual.
</table>
<reference confidence="0.999800339285715">
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding
for machine translation. In Proc. 39th Annual Meet-
ing of the Assoc. for Computational Linguistics -
joint with EACL, pages 228-235, Toulouse, France,
July.
S. NieBen and H. Ney. 2001a. Morpho-syntactic anal-
ysis for reordering in statistical machine translation.
In Proc. MT Summit VIII, pages 247-252, Santiago
de Compostela, Galicia, Spain, September.
S. Niel3en and H. Ney. 2001b. Toward hierarchi-
cal models for statistical machine translation of in-
flected languages. In 39th Annual Meeting of the
Assoc. for Computational Linguistics - joint with
EACL 2001: Proc. Workshop on Data-Driven Ma-
chine Translation, pages 47-54, Toulouse, France,
July.
S. NieBen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalu-
ation for mt research. In Proc. of the Second Int.
Conf on Language Resources and Evaluation, pages
39-45, Athens, Greece, May.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine transla-
tion. In Proc. Joint SIGDAT Conf on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 20-28, University of Mary-
land, College Park, MD, June.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. 40th Annual Meeting
of the Assoc. for Computational Linguistics, pages
311-318, Philadelphia, PA, July.
A. Ratnaparkhi. 1997. A simple introduction to max-
imum entropy models for natural language process-
ing. Technical Report 97-08, Institute for Research
in Cognitive Science, University of Pennsylvania,
Philadelphia, PA, May.
C. Tillmann and H. Ney. 2002. Word re-ordering and
DP beam search for statistical machine translation.
to appear in Computational Linguistics.
K. Toutanova, H.T. Ilhan, and C.D. Manning. 2002.
Extensions to HMM-based statistical word align-
ment models. In Proc. Conf on Empirical Meth-
ods for Natural Language Processing, pages 87-94,
Philadelphia, PA, July.
S. Vogel, F.J. Och, C. Tillmann, S. NieBen, H. Sawaf,
and H. Ney. 2000. Statistical methods for ma-
chine translation. In W. Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation, pages
377-393. Springer Verlag: Berlin, Heidelberg, New
York.
Y.Y. Wang and A. Waibel. 1997. Decoding algo-
rithm in statistical translation. In Proc. 35th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 366-372, Madrid, Spain, July.
</reference>
<page confidence="0.999141">
354
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322550">
<title confidence="0.70608">Using POS Information for Statistical Machine Translation into Morphologically Rich Languages</title>
<author confidence="0.610003">Ueffing Ney</author>
<affiliation confidence="0.967913">Lehrstuhl ftir Informatik VI - Computer Science Department RWTH Aachen - University of Technology</affiliation>
<abstract confidence="0.998647588235294">When translating from languages with hardly any inflectional morphology like English into morphologically rich languages, the English word forms often do not contain enough information for producing the correct fullform in the target language. We investigate methods for improving the quality of such translations by making use of part-ofspeech information and maximum entropy modeling. Results for translations from English into Spanish and Catalan are presented on the LC-STAR corpus which consists of spontaneously spoken dialogues in the domain of appointment scheduling and travel planning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="12584" citStr="Berger et al., 1996" startWordPosition="2088" endWordPosition="2091"> Question Inversion you did say the eighteenth? Verb Treatment you_did say the eighteenth? Catalan Sentence has dit el divuit ? Spanish Sentence i, has dicho el dieciocho ? ing operation. This makes it impossible to translate the verb itself, because it is then unknown to the system. The same holds for combinations of pronouns and verbs that are unseen in training, e. g. the training corpus contains the bigram &apos;I went&apos;, but not the one &apos;she went&apos;. In order to overcome this problem, we train our lexicon model using maximum entropy. 5.1 The Maximum Entropy Approach The maximum entropy approach (Berger et al., 1996) presents a powerful framework for the combination of several knowledge sources. This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy. The distribution is required to satisfy constraints, which represent facts known from the data. These constraints are expressed on the basis of feature functions hu,(s,t), where (s, t) is a pair of source and target word. The lexicon probability of a source word given the target word has the following functional form 1 t) Z(t) exp [Y‘ with the normalization factor Z(t) = Eexp [E </context>
<context position="13912" citStr="Berger et al., 1996" startWordPosition="2325" endWordPosition="2328">m. The features we use in our model are • a lexical feature (for the entries of the transformed vocabulary): 128, (s, t) = (5(s, s&apos;) • 6(t, t&apos;) P(s 350 • the verb contained in a transformed lexicon entry (e.g. &apos;go&apos; for &apos;you_go&apos; or &apos;you_will_go): hs, ,v(s ,t) = S(s. s&apos;) • V erb(t, v) , where 1, if t contains the verb v V erb(t, v) = 0, otherwise This enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry. For an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: • transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 • train the statistical translation system using this modified source language corpus 1 • with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation s</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Analysis, statistical transfer, and synthesis in machine translation.</title>
<date>1992</date>
<booktitle>In Proc. TMI 1992: 4th Int. Conf. on Theoretical and Methodological Issues in MT,</booktitle>
<pages>83--100</pages>
<location>Montréal, P.O., Canada,</location>
<contexts>
<context position="3166" citStr="Brown et al., 1992" startWordPosition="495" endWordPosition="498">hen, we introduce the transformations that we apply to the less inflected language of the two under consideration (namely English) in Section 4. After describing the maximum entropy approach and the training procedure we use for the statistical lexicon in Section 5, we present results on the trilingual LC-STAR corpus in Section 6. Then, we conclude and present ideas about future work in Section 7. 347 2 Related Work Publications dealing with the integration of linguistic information into the process of statistical machine translation are rather few although this had already been suggested in (Brown et al., 1992). (Niel3en and Ney, 2001b) introduce hierarchical lexicon models including baseform and POS information for translation from German into English. Information contained in the German entries that are not relevant for the generation of the English translation are omitted. Unlike this, we investigate methods for enriching English with knowledge to help selecting the correct fullform in a morphologically richer language. (Niefien and Ney, 2001a) propose reordering operations for the language pair German—English that help SMT by harmonizing word order between source and target. The question inversi</context>
</contexts>
<marker>Brown, Pietra, Pietra, Lafferty, Mercer, 1992</marker>
<rawString>P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, J.D. Lafferty, and R.L. Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In Proc. TMI 1992: 4th Int. Conf. on Theoretical and Methodological Issues in MT, pages 83-100, Montréal, P.O., Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="19098" citStr="Brown et al., 1993" startWordPosition="3137" endWordPosition="3140">tes. These numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish. Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution. We performed translation experiments with an implementation of the IBM-4 translation model (Brown et al., 1993). A description of the system can be found in (Tillmann and Ney, 2002). Table 5 presents an assessment of translation quality for both the language pairs English—Catalan and English—Spanish. We see that there is a significant decrease in error rate for the translation into Catalan. This change is consistent across both error rates, the WER and 100—BLEU. For translations from English into Spanish, the improvement is less substantial. A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="14426" citStr="Darroch and Ratcliff, 1972" startWordPosition="2405" endWordPosition="2408">d training procedures, the reader is referred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: • transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 • train the statistical translation system using this modified source language corpus 1 • with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation system and the language model need not be changed. 5.3 Translation process For translation, we can use an SMT system where the search algorithm does not have to be modified. Before the translation process, we transform the input in the same way as the training corpus before training the alignment (see Section 5.2). We simply have to exclude those words from splicing where the splicing operation yields an unknown word. &apos;This training was done using the GIZA++ toolkit which can be downloaded from http://www-i6.i</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J.N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features in random fields.</title>
<date>1997</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Inteligence,</journal>
<pages>19--4</pages>
<contexts>
<context position="14457" citStr="Pietra et al., 1997" startWordPosition="2411" endWordPosition="2414">eferred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: • transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 • train the statistical translation system using this modified source language corpus 1 • with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation system and the language model need not be changed. 5.3 Translation process For translation, we can use an SMT system where the search algorithm does not have to be modified. Before the translation process, we transform the input in the same way as the training corpus before training the alignment (see Section 5.2). We simply have to exclude those words from splicing where the splicing operation yields an unknown word. &apos;This training was done using the GIZA++ toolkit which can be downloaded from http://www-i6.informatik.rwthaachen.deroch/sof</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S.A. Della Pietra, V.J. Della Pietra, and J. Lafferty. 1997. Inducing features in random fields. IEEE Trans. on Pattern Analysis and Machine Inteligence, 19(4):380-393, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Garcia-Varea</author>
<author>F J Och</author>
<author>H Ney</author>
<author>F Casacuberta</author>
</authors>
<title>Refined lexicon models for statistical machine translation using a maximum entropy approach.</title>
<date>2001</date>
<booktitle>In Proc. 39th Annual Meeting of the Assoc. for Computational Linguistics - joint with EACL,</booktitle>
<pages>204--211</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="3981" citStr="Garcia-Varea et al., 2001" startWordPosition="619" endWordPosition="622"> that are not relevant for the generation of the English translation are omitted. Unlike this, we investigate methods for enriching English with knowledge to help selecting the correct fullform in a morphologically richer language. (Niefien and Ney, 2001a) propose reordering operations for the language pair German—English that help SMT by harmonizing word order between source and target. The question inversion we apply was inspired by this; nevertheless, we do not perform a full morpho-syntactic analysis, but make use only of POS information which can be obtained from freely available tools. (Garcia-Varea et al., 2001) apply a maximum entropy approach for training the statistical lexicon, but do not take any linguistic information into account. The use of POS information for improving statistical alignment quality is described in (Toutanova et al., 2002), but no translation results are presented. 3 Statistical Machine Translation The goal of machine translation is the translation of an input string Si,. . . , s j in the source language into a target language string ti tI. We choose the string that has maximal probability given the source string, Pr(tils1). Applying Bayes&apos; decision rule yields the following </context>
</contexts>
<marker>Garcia-Varea, Och, Ney, Casacuberta, 2001</marker>
<rawString>I. Garcia-Varea, F.J. Och, H. Ney, and F. Casacuberta. 2001. Refined lexicon models for statistical machine translation using a maximum entropy approach. In Proc. 39th Annual Meeting of the Assoc. for Computational Linguistics - joint with EACL, pages 204-211, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proc. 39th Annual Meeting of the Assoc. for Computational Linguistics -joint with EACL,</booktitle>
<pages>228--235</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="5782" citStr="Germann et al., 2001" startWordPosition="920" endWordPosition="923">y the arg max operation in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proc. 39th Annual Meeting of the Assoc. for Computational Linguistics -joint with EACL, pages 228-235, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S NieBen</author>
<author>H Ney</author>
</authors>
<title>Morpho-syntactic analysis for reordering in statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proc. MT Summit VIII,</booktitle>
<pages>247--252</pages>
<location>Santiago de Compostela, Galicia, Spain,</location>
<contexts>
<context position="9597" citStr="NieBen and Ney, 2001" startWordPosition="1556" endWordPosition="1559">sh/Catalan form. 4.2 Question Treatment In English interrogative phrases, either an auxiliary &apos;do&apos; is inserted or the order of verb and pronoun is inverted. The auxiliary &apos;do&apos; does not carry information that is relevant when translating into Table 1: Examples of spliced words in the English vocabulary original POS tags spliced words you go PRP VBP you_go you went PRP VBD you_went you think PRP VBP you_think you will have PRP MD VB you_will_have you can go PRP MD VB you_can go Spanish or Catalan. Thus, we can remove it from the sentence without harming the translation process (as described in (NieBen and Ney, 2001a) for the language pair German—English). However, we do not remove a question supporting &apos;do&apos; in past tense, i. e. &apos;did&apos; is kept in the phrase, because this is the only word containing the tense information. Afterwards, we can merge the pronoun and verb as depicted in Table 2: &apos;did you go&apos; is transformed into &apos;you_did go&apos;. We do not splice &apos;you_did&apos; and &apos;go&apos;, because the English simple past is translated into present perfect in Catalan; and it is very likely to be translated into present perfect in Spanish, especially in colloquial language as it is present in this task. The form &apos;you_did go&apos;</context>
</contexts>
<marker>NieBen, Ney, 2001</marker>
<rawString>S. NieBen and H. Ney. 2001a. Morpho-syntactic analysis for reordering in statistical machine translation. In Proc. MT Summit VIII, pages 247-252, Santiago de Compostela, Galicia, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niel3en</author>
<author>H Ney</author>
</authors>
<title>Toward hierarchical models for statistical machine translation of inflected languages.</title>
<date>2001</date>
<booktitle>In 39th Annual Meeting of the Assoc. for Computational Linguistics - joint with EACL 2001: Proc. Workshop on Data-Driven Machine Translation,</booktitle>
<pages>47--54</pages>
<location>Toulouse, France,</location>
<marker>Niel3en, Ney, 2001</marker>
<rawString>S. Niel3en and H. Ney. 2001b. Toward hierarchical models for statistical machine translation of inflected languages. In 39th Annual Meeting of the Assoc. for Computational Linguistics - joint with EACL 2001: Proc. Workshop on Data-Driven Machine Translation, pages 47-54, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S NieBen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for mt research.</title>
<date>2000</date>
<booktitle>In Proc. of the Second Int. Conf on Language Resources and Evaluation,</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="16853" citStr="NieBen et al., 2000" startWordPosition="2781" endWordPosition="2784">generated translation to a given reference translation. The two following criteria are used: • WER (word error rate): The word error rate is based on the Levenshtein distance. It is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the reference string. Since some sentences in the develop and test set occur several times with different reference translations (which holds especially for short sentences like &apos;okay, good-bye&apos;), we calculate the minimal distance to this set of references as proposed in (NieBen et al., 2000). • BLEU (bilingual evaluation understudy): (Papineni et al., 2002) have proposed a 3 The Brill tagger can be downloaded from http://www.research.microsoft.com/users/brill/ 351 Table 4: Statistics of the training, develop and test set of the English-Spanish-Catalan LC-STAR corpus (*number of words without punctuation marks) English Spanish Catalan Original Transformed Training Sentences 13 352 Words Words&amp;quot; 123 454 114 099 118 534 118 137 101 738 92 383 96 997 96 503 Vocabulary Size 2 154 2 776 3 933 3 572 Singletons 790 (37%) 1 165 (42%) 1 844 (47%) 1 658 (47%) Develop Sentences 272 Words Unkn</context>
</contexts>
<marker>NieBen, Och, Leusch, Ney, 2000</marker>
<rawString>S. NieBen, F.J. Och, G. Leusch, and H. Ney. 2000. An evaluation tool for machine translation: Fast evaluation for mt research. In Proc. of the Second Int. Conf on Language Resources and Evaluation, pages 39-45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="5800" citStr="Och et al., 1999" startWordPosition="924" endWordPosition="927">n in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on the correct infle</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F.J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20-28, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="16920" citStr="Papineni et al., 2002" startWordPosition="2790" endWordPosition="2793">ollowing criteria are used: • WER (word error rate): The word error rate is based on the Levenshtein distance. It is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the reference string. Since some sentences in the develop and test set occur several times with different reference translations (which holds especially for short sentences like &apos;okay, good-bye&apos;), we calculate the minimal distance to this set of references as proposed in (NieBen et al., 2000). • BLEU (bilingual evaluation understudy): (Papineni et al., 2002) have proposed a 3 The Brill tagger can be downloaded from http://www.research.microsoft.com/users/brill/ 351 Table 4: Statistics of the training, develop and test set of the English-Spanish-Catalan LC-STAR corpus (*number of words without punctuation marks) English Spanish Catalan Original Transformed Training Sentences 13 352 Words Words&amp;quot; 123 454 114 099 118 534 118 137 101 738 92 383 96 997 96 503 Vocabulary Size 2 154 2 776 3 933 3 572 Singletons 790 (37%) 1 165 (42%) 1 844 (47%) 1 658 (47%) Develop Sentences 272 Words Unknown Words 2 267 2 096 2217 2211 21 22 34 34 Test Sentences 262 Word</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting of the Assoc. for Computational Linguistics, pages 311-318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical Report 97-08,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="13935" citStr="Ratnaparkhi, 1997" startWordPosition="2330" endWordPosition="2331"> our model are • a lexical feature (for the entries of the transformed vocabulary): 128, (s, t) = (5(s, s&apos;) • 6(t, t&apos;) P(s 350 • the verb contained in a transformed lexicon entry (e.g. &apos;go&apos; for &apos;you_go&apos; or &apos;you_will_go): hs, ,v(s ,t) = S(s. s&apos;) • V erb(t, v) , where 1, if t contains the verb v V erb(t, v) = 0, otherwise This enables us to translate the verb alone even if it occurs in the training corpus only as a spliced entry. For an introduction to maximum entropy modeling and training procedures, the reader is referred to the corresponding literature, for instance (Berger et al., 1996) or (Ratnaparkhi, 1997). 5.2 Training We performed the following training steps: • transform the English (= source language) part of the corpus as described in Sections 4.1 and 4.2 • train the statistical translation system using this modified source language corpus 1 • with the resulting alignment, train the lexicon model using maximum entropy with the features described in Section 5.1 This training can be performed using converging iterative training procedures like described by (Darroch and Ratcliff, 1972) or (Della Pietra et al., 1997) 2. The basic training procedures for the translation system and the language </context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania, Philadelphia, PA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Word re-ordering and DP beam search for statistical machine translation.</title>
<date>2002</date>
<note>to appear in Computational Linguistics.</note>
<contexts>
<context position="5824" citStr="Tillmann and Ney, 2002" startWordPosition="928" endWordPosition="932">t explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on the correct inflected form. 4.1 Treatment</context>
<context position="19168" citStr="Tillmann and Ney, 2002" startWordPosition="3150" endWordPosition="3153">was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish. Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution. We performed translation experiments with an implementation of the IBM-4 translation model (Brown et al., 1993). A description of the system can be found in (Tillmann and Ney, 2002). Table 5 presents an assessment of translation quality for both the language pairs English—Catalan and English—Spanish. We see that there is a significant decrease in error rate for the translation into Catalan. This change is consistent across both error rates, the WER and 100—BLEU. For translations from English into Spanish, the improvement is less substantial. A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 for Catalan4. This makes it more difficult for the system to choose th</context>
</contexts>
<marker>Tillmann, Ney, 2002</marker>
<rawString>C. Tillmann and H. Ney. 2002. Word re-ordering and DP beam search for statistical machine translation. to appear in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H T Ilhan</author>
<author>C D Manning</author>
</authors>
<title>Extensions to HMM-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Proc. Conf on Empirical Methods for Natural Language Processing,</booktitle>
<pages>87--94</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="4221" citStr="Toutanova et al., 2002" startWordPosition="658" endWordPosition="661">d Ney, 2001a) propose reordering operations for the language pair German—English that help SMT by harmonizing word order between source and target. The question inversion we apply was inspired by this; nevertheless, we do not perform a full morpho-syntactic analysis, but make use only of POS information which can be obtained from freely available tools. (Garcia-Varea et al., 2001) apply a maximum entropy approach for training the statistical lexicon, but do not take any linguistic information into account. The use of POS information for improving statistical alignment quality is described in (Toutanova et al., 2002), but no translation results are presented. 3 Statistical Machine Translation The goal of machine translation is the translation of an input string Si,. . . , s j in the source language into a target language string ti tI. We choose the string that has maximal probability given the source string, Pr(tils1). Applying Bayes&apos; decision rule yields the following criterion: arg max Pr(ti si) 4)1 (1) tf = arg max{Pr(t1) • Pr(s1 tf Through this decomposition of the probability, we obtain two knowledge sources: the translation and the language model. Those two can be modelled independently of each othe</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>K. Toutanova, H.T. Ilhan, and C.D. Manning. 2002. Extensions to HMM-based statistical word alignment models. In Proc. Conf on Empirical Methods for Natural Language Processing, pages 87-94, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>F J Och</author>
<author>C Tillmann</author>
<author>S NieBen</author>
<author>H Sawaf</author>
<author>H Ney</author>
</authors>
<title>Statistical methods for machine translation.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speech-to-Speech Translation,</booktitle>
<pages>377--393</pages>
<editor>In W. Wahlster, editor,</editor>
<publisher>Springer Verlag:</publisher>
<location>Berlin, Heidelberg, New York.</location>
<contexts>
<context position="5844" citStr="Vogel et al., 2000" startWordPosition="933" endWordPosition="936">all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on the correct inflected form. 4.1 Treatment of Verbs Especially</context>
</contexts>
<marker>Vogel, Och, Tillmann, NieBen, Sawaf, Ney, 2000</marker>
<rawString>S. Vogel, F.J. Och, C. Tillmann, S. NieBen, H. Sawaf, and H. Ney. 2000. Statistical methods for machine translation. In W. Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 377-393. Springer Verlag: Berlin, Heidelberg, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Y Wang</author>
<author>A Waibel</author>
</authors>
<title>Decoding algorithm in statistical translation.</title>
<date>1997</date>
<booktitle>In Proc. 35th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>366--372</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="5868" citStr="Wang and Waibel, 1997" startWordPosition="937" endWordPosition="940">language strings and all possible alignments between the source and the target language string to find the one with maximal probability. The input string can be preprocessed before being passed to the search algorithm. If necessary, the inverse of these transformations will be applied to the generated output string. In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology. For descriptions of SMT systems see for example (Germann et al., 2001; Och et al., 1999; Tillmann and Ney, 2002; Vogel et al., 2000; Wang and Waibel, 1997). 4 Transformations in the Less Inflected Language When translating from English into languages with a highly inflected morphology, the production of the correct fullform often causes problems. Our experience on several corpora shows that the error rate of a translation from English into morphologically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word. The transfer of the meaning expressed in the baseform is easier than deciding on the correct inflected form. 4.1 Treatment of Verbs Especially the translation of verb</context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Y.Y. Wang and A. Waibel. 1997. Decoding algorithm in statistical translation. In Proc. 35th Annual Meeting of the Assoc. for Computational Linguistics, pages 366-372, Madrid, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>