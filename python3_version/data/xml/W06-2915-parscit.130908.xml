<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000327">
<title confidence="0.979458">
Which Side are You on? Identifying Perspectives at the Document and
Sentence Levels
</title>
<author confidence="0.945052">
Wei-Hao Lin Theresa Wilson, Janyce Wiebe Alexander Hauptmann
</author>
<affiliation confidence="0.9823595">
Language Technologies Institute Intelligent Systems Program School of Computer Science
Carnegie Mellon University University of Pittsburgh Carnegie Mellon University
</affiliation>
<address confidence="0.762186">
Pittsburgh, PA 15213 Pittsburgh, PA 15260 Pittsburgh, PA 15213
</address>
<email confidence="0.993852">
whlin@cs.cmu.edu {twilson,wiebe}@cs.pitt.edu alex@cs.cmu.edu
</email>
<sectionHeader confidence="0.995531" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997251">
In this paper we investigate a new problem
of identifying the perspective from which
a document is written. By perspective we
mean a point of view, for example, from
the perspective of Democrats or Repub-
licans. Can computers learn to identify
the perspective of a document? Not every
sentence is written strongly from a per-
spective. Can computers learn to identify
which sentences strongly convey a partic-
ular perspective? We develop statistical
models to capture how perspectives are
expressed at the document and sentence
levels, and evaluate the proposed mod-
els on articles about the Israeli-Palestinian
conflict. The results show that the pro-
posed models successfully learn how per-
spectives are reflected in word usage and
can identify the perspective of a document
with high accuracy.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998345142857143">
In this paper we investigate a new problem of au-
tomatically identifying the perspective from which
a document is written. By perspective we mean
a “subjective evaluation of relative significance, a
point-of-view.”1 For example, documents about the
Palestinian-Israeli conflict may appear to be about
the same topic but reveal different perspectives:
</bodyText>
<footnote confidence="0.8368815">
1The American Heritage Dictionary of the English Lan-
guage, 4th ed.
</footnote>
<bodyText confidence="0.952043130434783">
(1) The inadvertent killing by Israeli forces of
Palestinian civilians – usually in the course of
shooting at Palestinian terrorists – is
considered no different at the moral and ethical
level than the deliberate targeting of Israeli
civilians by Palestinian suicide bombers.
(2) In the first weeks of the Intifada, for example,
Palestinian public protests and civilian
demonstrations were answered brutally by
Israel, which killed tens of unarmed protesters.
Example 1 is written from an Israeli perspective;
Example 2 is written from a Palestinian perspec-
tive. Anyone knowledgeable about the issues of
the Israeli-Palestinian conflict can easily identify the
perspectives from which the above examples were
written. However, can computers learn to identify
the perspective of a document given a training cor-
pus?
When an issue is discussed from different per-
spectives, not every sentence strongly reflects the
perspective of the author. For example, the follow-
ing sentences were written by a Palestinian and an
Israeli.
</bodyText>
<listItem confidence="0.9058964">
(3) The Rhodes agreements of 1949 set them as
the ceasefire lines between Israel and the Arab
states.
(4) The green line was drawn up at the Rhodes
Armistice talks in 1948-49.
</listItem>
<bodyText confidence="0.996735">
Examples 3 and 4 both factually introduce the back-
ground of the issue of the “green line” without ex-
pressing explicit perspectives. Can we develop a
</bodyText>
<page confidence="0.987601">
109
</page>
<note confidence="0.88247">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 109–116, New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999713545454545">
system to automatically discriminate between sen-
tences that strongly indicate a perspective and sen-
tences that only reflect shared background informa-
tion?
A system that can automatically identify the per-
spective from which a document is written will be
a valuable tool for people analyzing huge collec-
tions of documents from different perspectives. Po-
litical analysts regularly monitor the positions that
countries take on international and domestic issues.
Media analysts frequently survey broadcast news,
newspapers, and weblogs for differing viewpoints.
Without the assistance of computers, analysts have
no choice but to read each document in order to iden-
tify those from a perspective of interest, which is ex-
tremely time-consuming. What these analysts need
is to find strong statements from different perspec-
tives and to ignore statements that reflect little or no
perspective.
In this paper we approach the problem of learning
individual perspectives in a statistical framework.
We develop statistical models to learn how perspec-
tives are reflected in word usage, and we treat the
problem of identifying perspectives as a classifica-
tion task. Although our corpus contains document-
level perspective annotations, it lacks sentence-level
annotations, creating a challenge for learning the
perspective of sentences. We propose a novel sta-
tistical model to overcome this problem. The ex-
perimental results show that the proposed statisti-
cal models can successfully identify the perspective
from which a document is written with high accu-
racy.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999923833333333">
Identifying the perspective from which a document
is written is a subtask in the growing area of au-
tomatic opinion recognition and extraction. Sub-
jective language is used to express opinions, emo-
tions, and sentiments. So far, research in automatic
opinion recognition has primarily addressed learn-
ing subjective language (Wiebe et al., 2004; Riloff
et al., 2003), identifying opinionated documents (Yu
and Hatzivassiloglou, 2003) and sentences (Yu and
Hatzivassiloglou, 2003; Riloff et al., 2003), and dis-
criminating between positive and negative language
(Pang et al., 2002; Morinaga et al., 2002; Yu and
Hatzivassiloglou, 2003; Turney and Littman, 2003;
Dave et al., 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005; Wilson et al., 2005). While by its
very nature we expect much of the language that is
used when presenting a perspective or point-of-view
to be subjective, labeling a document or a sentence
as subjective is not enough to identify the perspec-
tive from which it is written. Moreover, the ideol-
ogy and beliefs authors possess are often expressed
in ways other than positive or negative language to-
ward specific targets.
Research on the automatic classification of movie
or product reviews as positive or negative (e.g.,
(Pang et al., 2002; Morinaga et al., 2002; Turney
and Littman, 2003; Nasukawa and Yi, 2003; Mullen
and Collier, 2004; Beineke et al., 2004; Hu and Liu,
2004)) is perhaps the most similar to our work. As
with review classification, we treat perspective iden-
tification as a document-level classification task, dis-
criminating, in a sense, between different types of
opinions. However, there is a key difference. A pos-
itive or negative opinion toward a particular movie
or product is fundamentally different from an overall
perspective. One’s opinion will change from movie
to movie, whereas one’s perspective can be seen as
more static, often underpinned by one’s ideology or
beliefs about the world.
There has been research in discourse analysis that
examines how different perspectives are expressed
in political discourse (van Dijk, 1988; Pan et al.,
1999; Geis, 1987). Although their research may
have some similar goals, they do not take a compu-
tational approach to analyzing large collections of
documents. To the best of our knowledge, our ap-
proach to automatically identifying perspectives in
discourse is unique.
</bodyText>
<sectionHeader confidence="0.993586" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.967336333333333">
Our corpus consists of articles published on the
bitterlemons website2. The website is set up to
“contribute to mutual understanding [between Pales-
tinians and Israelis] through the open exchange of
ideas.”3 Every week an issue about the Israeli-
Palestinian conflict is selected for discussion (e.g.,
</bodyText>
<footnote confidence="0.993680333333333">
2http://www.bitterlemons.org
3http://www.bitterlemons.org/about/
about.html
</footnote>
<page confidence="0.997974">
110
</page>
<bodyText confidence="0.99964735">
“Disengagement: unilateral or coordinated?”), and
a Palestinian editor and an Israeli editor each con-
tribute one article addressing the issue. In addition,
the Israeli and Palestinian editors invite one Israeli
and one Palestinian to express their views on the
issue (sometimes in the form of an interview), re-
sulting in a total of four articles in a weekly edi-
tion. We choose the bitterlemons website for
two reasons. First, each article is already labeled
as either Palestinian or Israeli by the editors, allow-
ing us to exploit existing annotations. Second, the
bitterlemons corpus enables us to test the gen-
eralizability of the proposed models in a very real-
istic setting: training on articles written by a small
number of writers (two editors) and testing on arti-
cles from a much larger group of writers (more than
200 different guests).
We collected a total of 594 articles published on
the website from late 2001 to early 2005. The dis-
tribution of documents and sentences are listed in
</bodyText>
<tableCaption confidence="0.871508">
Table 1. We removed metadata from all articles, in-
</tableCaption>
<table confidence="0.996273666666667">
Palestinian Israeli
Written by editors 148 149
Written by guests 149 148
Total number of documents 297 297
Average document length 740.4 816.1
Number of sentences 8963 9640
</table>
<tableCaption confidence="0.999904">
Table 1: The basic statistics of the corpus
</tableCaption>
<bodyText confidence="0.999918733333333">
cluding edition numbers, publication dates, topics,
titles, author names and biographic information. We
used OpenNLP Tools4 to automatically extract sen-
tence boundaries, and reduced word variants using
the Porter stemming algorithm.
We evaluated the subjectivity of each sentence us-
ing the automatic subjective sentence classifier from
(Riloff and Wiebe, 2003), and find that 65.6% of
Palestinian sentences and 66.2% of Israeli sentences
are classified as subjective. The high but almost
equivalent percentages of subjective sentences in the
two perspectives support our observation in Sec-
tion 2 that a perspective is largely expressed using
subjective language, but that the amount of subjec-
tivity in a document is not necessarily indicative of
</bodyText>
<footnote confidence="0.9608015">
4http://sourceforge.net/projects/
opennlp/
</footnote>
<bodyText confidence="0.806084">
its perspective.
</bodyText>
<sectionHeader confidence="0.973623" genericHeader="method">
4 Statistical Modeling of Perspectives
</sectionHeader>
<bodyText confidence="0.999957625">
We develop algorithms for learning perspectives us-
ing a statistical framework. Denote a training corpus
as a set of documents Wn and their perspectives la-
bels Dn, n = 1, ... , N, where N is the total number
of documents in the corpus. Given a new document
W with a unknown document perspective, the per-
spective D� is calculated based on the following con-
ditional probability.
</bodyText>
<equation confidence="0.961972">
P(�D|�W, {Dn,Wn}Nn=1) (5)
</equation>
<bodyText confidence="0.999950857142857">
We are also interested in how strongly each sen-
tence in a document conveys perspective informa-
tion. Denote the intensity of the m-th sentence of
the n-th document as a binary random variable Sm,n.
To evaluate Sm,n, how strongly a sentence reflects
a particular perspective, we calculate the following
conditional probability.
</bodyText>
<equation confidence="0.993111">
P(Sm,n|{Dn,Wn}Nn=1) (6)
</equation>
<subsectionHeader confidence="0.906797">
4.1 Naive Bayes Model
</subsectionHeader>
<bodyText confidence="0.9999415">
We model the process of generating documents from
a particular perspective as follows:
</bodyText>
<equation confidence="0.9976875">
π — Beta(απ, βπ)
θ — Dirichlet(αθ)
Dn — Binomial(1, π)
Wn — Multinomial(Ln, θd)
</equation>
<bodyText confidence="0.999963538461538">
First, the parameters π and θ are sampled once from
prior distributions for the whole corpus. Beta and
Dirichlet are chosen because they are conjugate pri-
ors for binomial and multinomial distributions, re-
spectively. We set the hyperparameters απ, βπ, and
αθ to one, resulting in non-informative priors. A
document perspective Dn is then sampled from a bi-
nomial distribution with the parameter π. The value
of Dn is either d0 (Israeli) or d1 (Palestinian). Words
in the document are then sampled from a multino-
mial distribution, where Ln is the length of the doc-
ument. A graphical representation of the model is
shown in Figure 1.
</bodyText>
<page confidence="0.975596">
111
</page>
<figure confidence="0.998717">
Dn Wn
π θ
N
</figure>
<figureCaption confidence="0.999974">
Figure 1: Naive Bayes Model
</figureCaption>
<bodyText confidence="0.9998425">
The model described above is commonly known
as a naive Bayes (NB) model. NB models have
been widely used for various classification tasks,
including text categorization (Lewis, 1998). The
NB model is also a building block for the model
described later that incorporates sentence-level per-
spective information.
To predict the perspective of an unseen document
using naive Bayes , we calculate the posterior distri-
bution of D in (5) by integrating out the parameters,
</bodyText>
<equation confidence="0.8173395">
Z Z
P( h, π, θ|{(Dn, Wn)}Nn=1, W )dπdθ (7)
</equation>
<bodyText confidence="0.9999208">
However, the above integral is difficult to compute.
As an alternative, we use Markov Chain Monte
Carlo (MCMC) methods to obtain samples from the
posterior distribution. Details about MCMC meth-
ods can be found in Appendix A.
</bodyText>
<subsectionHeader confidence="0.995827">
4.2 Latent Sentence Perspective Model
</subsectionHeader>
<bodyText confidence="0.999813833333333">
We introduce a new binary random variable, S, to
model how strongly a perspective is reflected at the
sentence level. The value of S is either s1 or s0,
where s1 indicates a sentence is written strongly
from a perspective while s0 indicates it is not. The
whole generative process is modeled as follows:
</bodyText>
<equation confidence="0.999822666666667">
π — Beta(απ, βπ)
τ — Beta(ατ, βτ)
θ — Dirichlet(αθ)
Dn — Binomial(1, π)
Sm,n — Binomial(1, τ)
Wm,n — Multinomial(Lm,n, θ)
</equation>
<bodyText confidence="0.999357875">
The parameters π and θ have the same semantics as
in the naive Bayes model. S is naturally modeled as
a binomial variable, where τ is the parameter of S.
S represents how likely it is that a sentence strongly
conveys a perspective. We call this model the La-
tent Sentence Perspective Model (LSPM) because S
is not directly observed. The graphical model repre-
sentation of LSPM is shown in Figure 2.
</bodyText>
<figureCaption confidence="0.995814">
Figure 2: Latent Sentence Perspective Model
</figureCaption>
<bodyText confidence="0.999690772727273">
To use LSPM to identify the perspective of a new
document D with unknown sentence perspectives S,
we calculate posterior probabilities by summing out
possible combinations of sentence perspective in the
document and parameters.
As before, we resort to MCMC methods to sample
from the posterior distributions, given in Equations
(5) and (6).
As is often encountered in mixture models, there
is an identifiability issue in LSPM. Because the val-
ues of S can be permuted without changing the like-
lihood function, the meanings of s0 and s1 are am-
biguous. In Figure 3a, four θ values are used to rep-
resent the four possible combinations of document
perspective d and sentence perspective intensity s. If
we do not impose any constraints, s1 and s0 are ex-
changeable, and we can no longer strictly interpret
s1 as indicating a strong sentence-level perspective
and s0 as indicating that a sentence carries little or
no perspective information. The other problem of
this parameterization is that any improvement from
LSPM over the naive Bayes model is not necessarily
</bodyText>
<equation confidence="0.4412754">
π τ θ
Dn
Sm,n Wm,n
Mn
N
{(Dn, Wn)}Nn=1, W )dπdτdθ
Z Z Z X
S��� S˜
X
P(D, Sm,n, S, π, τ, θ |(8)
</equation>
<page confidence="0.839622">
112
</page>
<figureCaption confidence="0.999203">
Figure 3: Two different parameterization of B
</figureCaption>
<bodyText confidence="0.99995992">
due to the explicit modeling of sentence-level per-
spective. 5 may capture aspects of the document
collection that we never intended to model. For ex-
ample, s0 may capture the editors’ writing styles and
s1 the guests’ writing styles in the bitterlemons
corpus.
We solve the identifiability problem by forcing
Bd1,s0 and Bd0,s0 to be identical and reducing the
number of B parameters to three. As shown in Fig-
ure 3b, there are separate B parameters conditioned
on the document perspective (left branch of the tree,
d0 is Israeli and d1 is Palestinian), but there is single
B parameter when 5 = s0 shared by both document-
level perspectives (right branch of the tree). We as-
sume that the sentences with little or no perspective
information, i.e., 5 = s0, are generated indepen-
dently of the perspective of a document. In other
words, sentences that are presenting common back-
ground information or introducing an issue and that
do not strongly convey any perspective should look
similar whether they are in Palestinian or Israeli doc-
uments. By forcing this constraint, we become more
confident that s0 represents sentences of little per-
spectives and s1 represents sentences of strong per-
spectives from d1 and d0 documents.
</bodyText>
<sectionHeader confidence="0.999871" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.7894705">
5.1 Identifying Perspective at the Document
Level
</subsectionHeader>
<bodyText confidence="0.999921777777778">
We evaluate three different models for the task
of identifying perspective at the document level:
two naive Bayes models (NB) with different infer-
ence methods and Support Vector Machines (SVM)
(Cristianini and Shawe-Taylor, 2000). NB-B uses
full Bayesian inference and NB-M uses Maximum
a posteriori (MAP). We compare NB with SVM not
only because SVM has been very effective for clas-
sifying topical documents (Joachims, 1998), but also
to contrast generative models like NB with discrimi-
native models like SVM. For training SVM, we rep-
resent each document as a V-dimensional feature
vector, where V is the vocabulary size and each co-
ordinate is the normalized term frequency within the
document. We use a linear kernel for SVM and
search for the best parameters using grid methods.
To evaluate the statistical models, we train them
on the documents in the bitterlemons corpus
and calculate how accurately each model predicts
document perspective in ten-fold cross-validation
experiments. Table 2 reports the average classi-
fication accuracy across the the 10 folds for each
model. The accuracy of a baseline classifier, which
randomly assigns the perspective of a document as
Palestinian or Israeli, is 0.5, because there are equiv-
alent numbers of documents from the two perspec-
tives.
</bodyText>
<table confidence="0.99859775">
Model Data Set Accuracy Reduction
Baseline 0.5
SVM Editors 0.9724
NB-M Editors 0.9895 61%
NB-B Editors 0.9909 67%
SVM Guests 0.8621
NB-M Guests 0.8789 12%
NB-B Guests 0.8859 17%
</table>
<tableCaption confidence="0.893261">
Table 2: Results for Identifying Perspectives at the
Document Level
</tableCaption>
<bodyText confidence="0.999782916666667">
The last column of Table 2 is error reduction
relative to SVM. The results show that the naive
Bayes models and SVM perform surprisingly well
on both the Editors and Guests subsets of the
bitterlemons corpus. The naive Bayes mod-
els perform slightly better than SVM, possibly be-
cause generative models (i.e., naive Bayes models)
achieve optimal performance with a smaller num-
ber of training examples than discriminative models
(i.e., SVM) (Ng and Jordan, 2002), and the size of
the bitterlemons corpus is indeed small. NB-B,
which performs full Bayesian inference, improves
</bodyText>
<figure confidence="0.992917384615385">
s1
d1
s0
d0
s0
d0
s1
s0
s1
d1 Bs0
Bd0,s0 Bd0,s1 Bd1,s0 Bd0,s0 Bd0,s1 Bd1,s1
(a) s0 and s1 are not identifiable (b) sharing Bd1�30 and
Bd0�30
</figure>
<page confidence="0.997928">
113
</page>
<bodyText confidence="0.999543631578947">
on NB-M, which only performs point estimation.
The results suggest that the choice of words made
by the authors, either consciously or subconsciously,
reflects much of their political perspectives. Statis-
tical models can capture word usage well and can
identify the perspective of documents with high ac-
curacy.
Given the performance gap between Editors and
Guests, one may argue that there exist distinct edit-
ing artifacts or writing styles of the editors and
guests, and that the statistical models are capturing
these things rather than “perspectives.” To test if the
statistical models truly are learning perspectives, we
conduct experiments in which the training and test-
ing data are mismatched, i.e., from different subsets
of the corpus. If what the SVM and naive Bayes
models learn are writing styles or editing artifacts,
the classification performance under the mismatched
conditions will be considerably degraded.
</bodyText>
<table confidence="0.999257125">
Model Training Testing Accuracy
Baseline 0.5
SVM Guests Editors 0.8822
NB-M Guests Editors 0.9327 43%
NB-B Guests Editors 0.9346 44%
SVM Editors Guests 0.8148
NB-M Editors Guests 0.8485 18%
NB-B Editors Guests 0.8585 24%
</table>
<tableCaption confidence="0.9331065">
Table 3: Identifying Document-Level Perspectives
with Different Training and Testing Sets
</tableCaption>
<bodyText confidence="0.999957228571429">
The results on the mismatched training and test-
ing experiments are shown in Table 3. Both SVM
and the two variants of naive Bayes perform well
on the different combinations of training and testing
data. As in Table 2, the naive Bayes models per-
form better than SVM with larger error reductions,
and NB-B slightly outperforms NB-M. The high ac-
curacy on the mismatched experiments suggests that
statistical models are not learning writing styles or
editing artifacts. This reaffirms that document per-
spective is reflected in the words that are chosen by
the writers.
We list the most frequent words (excluding stop-
words) learned by the the NB-M model in Ta-
ble 4. The frequent words overlap greatly be-
tween the Palestinian and Israeli perspectives, in-
cluding “state,” “peace,” “process,” “secure” (“se-
curity”), and “govern” (“government”). This is in
contrast to what we expect from topical text classi-
fication (e.g., “Sports” vs. “Politics”), in which fre-
quent words seldom overlap. Authors from differ-
ent perspectives often choose words from a simi-
lar vocabulary but emphasize them differently. For
example, in documents that are written from the
Palestinian perspective, the word “palestinian” is
mentioned more frequently than the word “israel.”
It is, however, the reverse for documents that are
written from the Israeli perspective. Perspectives
are also expressed in how frequently certain people
(“sharon” v.s. “arafat”), countries (“international”
v.s. “america”), and actions (“occupation” v.s. “set-
tle”) are mentioned. While one might solicit these
contrasting word pairs from domain experts, our re-
sults show that statistical models such as SVM and
naive Bayes can automatically acquire them.
</bodyText>
<subsectionHeader confidence="0.883028">
5.2 Identifying Perspectives at the Sentence
Level
</subsectionHeader>
<bodyText confidence="0.999997230769231">
In addition to identifying the perspective of a docu-
ment, we are interested in knowing which sentences
of the document strongly conveys perspective in-
formation. Sentence-level perspective annotations
do not exist in the bitterlemons corpus, which
makes estimating parameters for the proposed La-
tent Sentence Perspective Model (LSPM) difficult.
The posterior probability that a sentence strongly
covey a perspective (Example (6)) is of the most in-
terest, but we can not directly evaluate this model
without gold standard annotations. As an alterna-
tive, we evaluate how accurately LSPM predicts the
perspective of a document, again using 10-fold cross
validation. Although LSPM predicts the perspec-
tive of both documents and sentences, we will doubt
the quality of the sentence-level predictions if the
document-level predictions are incorrect.
The experimental results are shown in Table 5.
We include the results for the naive Bayes models
from Table 3 for easy comparison. The accuracy of
LSPM is comparable or even slightly better than that
of the naive Bayes models. This is very encouraging
and suggests that the proposed LSPM closely cap-
tures how perspectives are reflected at both the doc-
ument and sentence levels. Examples 1 and 2 from
the introduction were predicted by LSPM as likely to
</bodyText>
<page confidence="0.99734">
114
</page>
<note confidence="0.738049">
Palestinian palestinian, israel, state, politics, peace, international, people, settle, occupation, sharon,
right, govern, two, secure, end, conflict, process, side, negotiate
Israeli israel, palestinian, state, settle, sharon, peace, arafat, arab, politics, two, process, secure,
conflict, lead, america, agree, right, gaza, govern
</note>
<tableCaption confidence="0.99952">
Table 4: The top twenty most frequent stems learned by the NB-M model, sorted by P(w|d)
</tableCaption>
<table confidence="0.999846375">
Model Training Testing Accuracy
Baseline 0.5
NB-M Guests Editors 0.9327
NB-B Guests Editors 0.9346
LSPM Guests Editors 0.9493
NB-M Editors Guests 0.8485
NB-B Editors Guests 0.8585
LSPM Editors Guests 0.8699
</table>
<tableCaption confidence="0.9280035">
Table 5: Results for Perspective Identification at the
Document and Sentence Levels
</tableCaption>
<bodyText confidence="0.999665428571428">
contain strong perspectives, i.e., large Pr(S� = s1).
Examples 3 and 4 from the introduction were pre-
dicted by LSPM as likely to contain little or no per-
spective information, i.e., high Pr( S� = s0).
The comparable performance between the naive
Bayes models and LSPM is in fact surprising. We
can train a naive Bayes model directly on the sen-
tences and attempt to classify a sentence as reflect-
ing either a Palestinian or Israeli perspective. A sen-
tence is correctly classified if the predicted perspec-
tive for the sentence is the same as the perspective
of the document from which it was extracted. Us-
ing this model, we obtain a classification accuracy of
only 0.7529, which is much lower than the accuracy
previously achieved at the document level. Identify-
ing perspectives at the sentence level is thus more
difficult than identifying perspectives at the docu-
ment level. The high accuracy at the document level
shows that LSPM is very effective in pooling evi-
dence from sentences that individually contain little
perspective information.
</bodyText>
<sectionHeader confidence="0.999666" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999030210526316">
In this paper we study a new problem of learning to
identify the perspective from which a text is written
at the document and sentence levels. We show that
much of a document’s perspective is expressed in
word usage, and statistical learning algorithms such
as SVM and naive Bayes models can successfully
uncover the word patterns that reflect author per-
spective with high accuracy. In addition, we develop
a novel statistical model to estimate how strongly
a sentence conveys perspective, in the absence of
sentence-level annotations. By introducing latent
variables and sharing parameters, the Latent Sen-
tence Perspective Model is shown to capture well
how perspectives are reflected at the document and
sentence levels. The small but positive improvement
due to sentence-level modeling in LSPM is encour-
aging. In the future, we plan to investigate how con-
sistently LSPM sentence-level predictions are with
human annotations.
</bodyText>
<sectionHeader confidence="0.525859" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.672933375">
This material is based on work supported by
the Advanced Research and Development Activity
(ARDA) under contract number NBCHC040037.
A Gibbs Samplers
Based the model specification described in Sec-
tion 4.2 we derive the Gibbs samplers (Chen et al.,
2000) for the Latent Sentence Perspective Model as
follows,
</bodyText>
<figure confidence="0.956686555555556">
π(t+1) — Beta(απ + N dn + d(t+1),
n=1
N
βπ + N - dn + 1 — d(t+1))
n=1
τ(t+1) — Beta(ατ + N Mn sm,n + �M� �sm,
n=1 m=1 m=1
βτ + N Mn — N Mn sm,n + M� — �M� �sm)
n=1 n=1 m=1 m=1
</figure>
<page confidence="0.523761">
115
</page>
<equation confidence="0.739004846153846">
N
n=1
wm,n)
Mn
m=1
0(t+1) - Dirichlet(αθ +
Pr(S(t+1)
n,m = s1) oc P(Wm,n|Sm,n = 1, 0(t))
Pr(S(t+1) m,n= 1|7-, Dn)
dbinom(�(t+1)
d )
�M˜ dmultinom(od, ˜m(t))dbinom(7r(t))
m=1
</equation>
<bodyText confidence="0.999959">
where dbinom and dmultinom are the density func-
tions of binomial and multinomial distributions, re-
spectively. The superscript t indicates that a sample
is from the t-th iteration. We run three chains and
collect 5000 samples. The first half of burn-in sam-
ples are discarded.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999378816901409">
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor:
Improving review classification via human-provided
information. In Proceedings ofACL-2004.
Ming-Hui Chen, Qi-Man Shao, and Joseph G. Ibrahim.
2000. Monte Carlo Methods in Bayesian Computa-
tion. Springer-Verlag.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW-2003.
Michael L. Geis. 1987. The Language of Politics.
Springer.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings ofKDD-2004.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of ECML-1998.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-1998.
S. Morinaga, K. Yamanishi, K. Tateishi, and
T. Fukushima. 2002. Mining product reputations on
the web. In Proceedings ofKDD-2002.
Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In Proceedings ofEMNLP-2004.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Cap-
turing favorability using natural language processing.
In Proceedings ofK-CAP 2003.
Andrew Y. Ng and Michael Jordan. 2002. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. In NIPS-2002, vol-
ume 15.
Zhongdang Pan, Chin-Chuan Lee, Joseph Man Chen, and
Clement Y.K. So. 1999. One event, three stories: Me-
dia narratives of the handover of hong kong in cultural
china. Gazette, 61(2):99–112.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings ofEMNLP-
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings ofHLT/EMNLP-2005, pages 339–346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of CoNLL-2003.
Peter Turney and Michael L. Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM TOIS, 21(4):315–346.
T.A. van Dijk. 1988. News as Discourse. Lawrence
Erlbaum, Hillsdale, NJ.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003.
</reference>
<equation confidence="0.863546666666667">
Pr(
f)(t+1) = d1) oc �M˜
m=1
</equation>
<page confidence="0.994323">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947565">
<title confidence="0.994224">Which Side are You on? Identifying Perspectives at the Document Sentence Levels</title>
<author confidence="0.999888">Wei-Hao Lin Theresa Wilson</author>
<author confidence="0.999888">Janyce Wiebe Alexander Hauptmann</author>
<affiliation confidence="0.994284">Language Technologies Institute Intelligent Systems Program School of Computer Science Carnegie Mellon University University of Pittsburgh Carnegie Mellon University</affiliation>
<address confidence="0.99835">Pittsburgh, PA 15213 Pittsburgh, PA 15260 Pittsburgh, PA</address>
<email confidence="0.999214">alex@cs.cmu.edu</email>
<abstract confidence="0.998660571428571">In this paper we investigate a new problem identifying the which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Beineke</author>
<author>Trevor Hastie</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>The sentimental factor: Improving review classification via human-provided information.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL-2004.</booktitle>
<contexts>
<context position="6134" citStr="Beineke et al., 2004" startWordPosition="933" endWordPosition="936">very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work. As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions. However, there is a key difference. A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective. One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world. There has been research in discourse ana</context>
</contexts>
<marker>Beineke, Hastie, Vaithyanathan, 2004</marker>
<rawString>Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan. 2004. The sentimental factor: Improving review classification via human-provided information. In Proceedings ofACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Hui Chen</author>
<author>Qi-Man Shao</author>
<author>Joseph G Ibrahim</author>
</authors>
<title>Monte Carlo Methods in Bayesian Computation.</title>
<date>2000</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="24845" citStr="Chen et al., 2000" startWordPosition="3932" endWordPosition="3935">g parameters, the Latent Sentence Perspective Model is shown to capture well how perspectives are reflected at the document and sentence levels. The small but positive improvement due to sentence-level modeling in LSPM is encouraging. In the future, we plan to investigate how consistently LSPM sentence-level predictions are with human annotations. Acknowledgment This material is based on work supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037. A Gibbs Samplers Based the model specification described in Section 4.2 we derive the Gibbs samplers (Chen et al., 2000) for the Latent Sentence Perspective Model as follows, π(t+1) — Beta(απ + N dn + d(t+1), n=1 N βπ + N - dn + 1 — d(t+1)) n=1 τ(t+1) — Beta(ατ + N Mn sm,n + �M� �sm, n=1 m=1 m=1 βτ + N Mn — N Mn sm,n + M� — �M� �sm) n=1 n=1 m=1 m=1 115 N n=1 wm,n) Mn m=1 0(t+1) - Dirichlet(αθ + Pr(S(t+1) n,m = s1) oc P(Wm,n|Sm,n = 1, 0(t)) Pr(S(t+1) m,n= 1|7-, Dn) dbinom(�(t+1) d ) �M˜ dmultinom(od, ˜m(t))dbinom(7r(t)) m=1 where dbinom and dmultinom are the density functions of binomial and multinomial distributions, respectively. The superscript t indicates that a sample is from the t-th iteration. We run thre</context>
</contexts>
<marker>Chen, Shao, Ibrahim, 2000</marker>
<rawString>Ming-Hui Chen, Qi-Man Shao, and Joseph G. Ibrahim. 2000. Monte Carlo Methods in Bayesian Computation. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="15575" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="2477" endWordPosition="2480">d information or introducing an issue and that do not strongly convey any perspective should look similar whether they are in Palestinian or Israeli documents. By forcing this constraint, we become more confident that s0 represents sentences of little perspectives and s1 represents sentences of strong perspectives from d1 and d0 documents. 5 Experiments 5.1 Identifying Perspective at the Document Level We evaluate three different models for the task of identifying perspective at the document level: two naive Bayes models (NB) with different inference methods and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000). NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP). We compare NB with SVM not only because SVM has been very effective for classifying topical documents (Joachims, 1998), but also to contrast generative models like NB with discriminative models like SVM. For training SVM, we represent each document as a V-dimensional feature vector, where V is the vocabulary size and each coordinate is the normalized term frequency within the document. We use a linear kernel for SVM and search for the best parameters using grid methods. To evaluate the statistical models, we train th</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW-2003.</booktitle>
<contexts>
<context position="5427" citStr="Dave et al., 2003" startWordPosition="815" endWordPosition="818">tten is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Geis</author>
</authors>
<title>The Language of Politics.</title>
<date>1987</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6864" citStr="Geis, 1987" startWordPosition="1049" endWordPosition="1050">ification as a document-level classification task, discriminating, in a sense, between different types of opinions. However, there is a key difference. A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective. One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world. There has been research in discourse analysis that examines how different perspectives are expressed in political discourse (van Dijk, 1988; Pan et al., 1999; Geis, 1987). Although their research may have some similar goals, they do not take a computational approach to analyzing large collections of documents. To the best of our knowledge, our approach to automatically identifying perspectives in discourse is unique. 3 Corpus Our corpus consists of articles published on the bitterlemons website2. The website is set up to “contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas.”3 Every week an issue about the IsraeliPalestinian conflict is selected for discussion (e.g., 2http://www.bitterlemons.org 3http://www.</context>
</contexts>
<marker>Geis, 1987</marker>
<rawString>Michael L. Geis. 1987. The Language of Politics. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings ofKDD-2004.</booktitle>
<contexts>
<context position="6153" citStr="Hu and Liu, 2004" startWordPosition="937" endWordPosition="940">much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work. As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions. However, there is a key difference. A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective. One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world. There has been research in discourse analysis that examines</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings ofKDD-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-1998.</booktitle>
<contexts>
<context position="15771" citStr="Joachims, 1998" startWordPosition="2511" endWordPosition="2512">t s0 represents sentences of little perspectives and s1 represents sentences of strong perspectives from d1 and d0 documents. 5 Experiments 5.1 Identifying Perspective at the Document Level We evaluate three different models for the task of identifying perspective at the document level: two naive Bayes models (NB) with different inference methods and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000). NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP). We compare NB with SVM not only because SVM has been very effective for classifying topical documents (Joachims, 1998), but also to contrast generative models like NB with discriminative models like SVM. For training SVM, we represent each document as a V-dimensional feature vector, where V is the vocabulary size and each coordinate is the normalized term frequency within the document. We use a linear kernel for SVM and search for the best parameters using grid methods. To evaluate the statistical models, we train them on the documents in the bitterlemons corpus and calculate how accurately each model predicts document perspective in ten-fold cross-validation experiments. Table 2 reports the average classific</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of ECML-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-1998.</booktitle>
<contexts>
<context position="11404" citStr="Lewis, 1998" startWordPosition="1770" endWordPosition="1771">π, βπ, and αθ to one, resulting in non-informative priors. A document perspective Dn is then sampled from a binomial distribution with the parameter π. The value of Dn is either d0 (Israeli) or d1 (Palestinian). Words in the document are then sampled from a multinomial distribution, where Ln is the length of the document. A graphical representation of the model is shown in Figure 1. 111 Dn Wn π θ N Figure 1: Naive Bayes Model The model described above is commonly known as a naive Bayes (NB) model. NB models have been widely used for various classification tasks, including text categorization (Lewis, 1998). The NB model is also a building block for the model described later that incorporates sentence-level perspective information. To predict the perspective of an unseen document using naive Bayes , we calculate the posterior distribution of D in (5) by integrating out the parameters, Z Z P( h, π, θ|{(Dn, Wn)}Nn=1, W )dπdθ (7) However, the above integral is difficult to compute. As an alternative, we use Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution. Details about MCMC methods can be found in Appendix A. 4.2 Latent Sentence Perspective Model We introdu</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Proceedings of ECML-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Morinaga</author>
<author>K Yamanishi</author>
<author>K Tateishi</author>
<author>T Fukushima</author>
</authors>
<title>Mining product reputations on the web.</title>
<date>2002</date>
<booktitle>In Proceedings ofKDD-2002.</booktitle>
<contexts>
<context position="5351" citStr="Morinaga et al., 2002" startWordPosition="803" endWordPosition="806">ccuracy. 2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or pr</context>
</contexts>
<marker>Morinaga, Yamanishi, Tateishi, Fukushima, 2002</marker>
<rawString>S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima. 2002. Mining product reputations on the web. In Proceedings ofKDD-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse information sources.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP-2004.</booktitle>
<contexts>
<context position="6112" citStr="Mullen and Collier, 2004" startWordPosition="929" endWordPosition="932"> al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work. As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions. However, there is a key difference. A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective. One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world. There has been res</context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse information sources. In Proceedings ofEMNLP-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nasukawa</author>
<author>J Yi</author>
</authors>
<title>Sentiment analysis: Capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings ofK-CAP</booktitle>
<contexts>
<context position="5450" citStr="Nasukawa and Yi, 2003" startWordPosition="819" endWordPosition="822">n the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and </context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>T. Nasukawa and J. Yi. 2003. Sentiment analysis: Capturing favorability using natural language processing. In Proceedings ofK-CAP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.</title>
<date>2002</date>
<booktitle>In NIPS-2002,</booktitle>
<volume>15</volume>
<contexts>
<context position="17335" citStr="Ng and Jordan, 2002" startWordPosition="2761" endWordPosition="2764"> 61% NB-B Editors 0.9909 67% SVM Guests 0.8621 NB-M Guests 0.8789 12% NB-B Guests 0.8859 17% Table 2: Results for Identifying Perspectives at the Document Level The last column of Table 2 is error reduction relative to SVM. The results show that the naive Bayes models and SVM perform surprisingly well on both the Editors and Guests subsets of the bitterlemons corpus. The naive Bayes models perform slightly better than SVM, possibly because generative models (i.e., naive Bayes models) achieve optimal performance with a smaller number of training examples than discriminative models (i.e., SVM) (Ng and Jordan, 2002), and the size of the bitterlemons corpus is indeed small. NB-B, which performs full Bayesian inference, improves s1 d1 s0 d0 s0 d0 s1 s0 s1 d1 Bs0 Bd0,s0 Bd0,s1 Bd1,s0 Bd0,s0 Bd0,s1 Bd1,s1 (a) s0 and s1 are not identifiable (b) sharing Bd1�30 and Bd0�30 113 on NB-M, which only performs point estimation. The results suggest that the choice of words made by the authors, either consciously or subconsciously, reflects much of their political perspectives. Statistical models can capture word usage well and can identify the perspective of documents with high accuracy. Given the performance gap betw</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Y. Ng and Michael Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In NIPS-2002, volume 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongdang Pan</author>
<author>Chin-Chuan Lee</author>
<author>Joseph Man Chen</author>
<author>Clement Y K So</author>
</authors>
<title>One event, three stories: Media narratives of the handover of hong kong in cultural china.</title>
<date>1999</date>
<journal>Gazette,</journal>
<volume>61</volume>
<issue>2</issue>
<contexts>
<context position="6851" citStr="Pan et al., 1999" startWordPosition="1045" endWordPosition="1048"> perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions. However, there is a key difference. A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective. One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world. There has been research in discourse analysis that examines how different perspectives are expressed in political discourse (van Dijk, 1988; Pan et al., 1999; Geis, 1987). Although their research may have some similar goals, they do not take a computational approach to analyzing large collections of documents. To the best of our knowledge, our approach to automatically identifying perspectives in discourse is unique. 3 Corpus Our corpus consists of articles published on the bitterlemons website2. The website is set up to “contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas.”3 Every week an issue about the IsraeliPalestinian conflict is selected for discussion (e.g., 2http://www.bitterlemons.org</context>
</contexts>
<marker>Pan, Lee, Chen, So, 1999</marker>
<rawString>Zhongdang Pan, Chin-Chuan Lee, Joseph Man Chen, and Clement Y.K. So. 1999. One event, three stories: Media narratives of the handover of hong kong in cultural china. Gazette, 61(2):99–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP2002.</booktitle>
<contexts>
<context position="5328" citStr="Pang et al., 2002" startWordPosition="799" endWordPosition="802">written with high accuracy. 2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classi</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings ofEMNLP2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP-2005,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="5477" citStr="Popescu and Etzioni, 2005" startWordPosition="823" endWordPosition="826">utomatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings ofHLT/EMNLP-2005, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003.</booktitle>
<contexts>
<context position="9122" citStr="Riloff and Wiebe, 2003" startWordPosition="1398" endWordPosition="1401">e 1. We removed metadata from all articles, inPalestinian Israeli Written by editors 148 149 Written by guests 149 148 Total number of documents 297 297 Average document length 740.4 816.1 Number of sentences 8963 9640 Table 1: The basic statistics of the corpus cluding edition numbers, publication dates, topics, titles, author names and biographic information. We used OpenNLP Tools4 to automatically extract sentence boundaries, and reduced word variants using the Porter stemming algorithm. We evaluated the subjectivity of each sentence using the automatic subjective sentence classifier from (Riloff and Wiebe, 2003), and find that 65.6% of Palestinian sentences and 66.2% of Israeli sentences are classified as subjective. The high but almost equivalent percentages of subjective sentences in the two perspectives support our observation in Section 2 that a perspective is largely expressed using subjective language, but that the amount of subjectivity in a document is not necessarily indicative of 4http://sourceforge.net/projects/ opennlp/ its perspective. 4 Statistical Modeling of Perspectives We develop algorithms for learning perspectives using a statistical framework. Denote a training corpus as a set of</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="5116" citStr="Riloff et al., 2003" startWordPosition="770" endWordPosition="773">ive of sentences. We propose a novel statistical model to overcome this problem. The experimental results show that the proposed statistical models can successfully identify the perspective from which a document is written with high accuracy. 2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the per</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM TOIS,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="5408" citStr="Turney and Littman, 2003" startWordPosition="811" endWordPosition="814">om which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al.</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM TOIS, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A van Dijk</author>
</authors>
<title>News as Discourse. Lawrence Erlbaum,</title>
<date>1988</date>
<location>Hillsdale, NJ.</location>
<marker>van Dijk, 1988</marker>
<rawString>T.A. van Dijk. 1988. News as Discourse. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="5094" citStr="Wiebe et al., 2004" startWordPosition="766" endWordPosition="769">earning the perspective of sentences. We propose a novel statistical model to overcome this problem. The experimental results show that the proposed statistical models can successfully identify the perspective from which a document is written with high accuracy. 2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enou</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP2005.</booktitle>
<contexts>
<context position="5499" citStr="Wilson et al., 2005" startWordPosition="827" endWordPosition="830">n and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets. Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT/EMNLP2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003.</booktitle>
<contexts>
<context position="5183" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="777" endWordPosition="780"> overcome this problem. The experimental results show that the proposed statistical models can successfully identify the perspective from which a document is written with high accuracy. 2 Related Work Identifying the perspective from which a document is written is a subtask in the growing area of automatic opinion recognition and extraction. Subjective language is used to express opinions, emotions, and sentiments. So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005). While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written. Moreover, the ideology and belie</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>