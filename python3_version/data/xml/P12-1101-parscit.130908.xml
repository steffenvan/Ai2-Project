<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988359">
Large-Scale Syntactic Language Modeling with Treelets
</title>
<author confidence="0.998654">
Adam Pauls Dan Klein
</author>
<affiliation confidence="0.998933">
Computer Science Division
University of California, Berkeley
</affiliation>
<address confidence="0.610405">
Berkeley, CA 94720, USA
</address>
<email confidence="0.999635">
{adpauls,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.99482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992502">
We propose a simple generative, syntactic
language model that conditions on overlap-
ping windows of tree context (or treelets) in
the same way that n-gram language models
condition on overlapping windows of linear
context. We estimate the parameters of our
model by collecting counts from automati-
cally parsed text using standard n-gram lan-
guage model estimation techniques, allowing
us to train a model on over one billion tokens
of data using a single machine in a matter of
hours. We evaluate on perplexity and a range
of grammaticality tasks, and find that we per-
form as well or better than n-gram models and
other generative baselines. Our model even
competes with state-of-the-art discriminative
models hand-designed for the grammaticality
tasks, despite training on positive data alone.
We also show fluency improvements in a pre-
liminary machine translation experiment.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971666666667">
N-gram language models are a central component
of all speech recognition and machine translation
systems, and a great deal of research centers around
refining models (Chen and Goodman, 1998), ef-
ficient storage (Pauls and Klein, 2011; Heafield,
2011), and integration into decoders (Koehn, 2004;
Chiang, 2005). At the same time, because n-gram
language models only condition on a local window
of linear word-level context, they are poor models of
long-range syntactic dependencies. Although sev-
eral lines of work have proposed generative syntac-
tic language models that improve on n-gram mod-
els for moderate amounts of data (Chelba, 1997; Xu
et al., 2002; Charniak, 2001; Hall, 2004; Roark,
2004), these models have only recently been scaled
to the impressive amounts of data routinely used by
n-gram language models (Tan et al., 2011).
In this paper, we describe a generative, syntac-
tic language model that conditions on local con-
text treelets1 in a parse tree, backing off to smaller
treelets as necessary. Our model can be trained sim-
ply by collecting counts and using the same smooth-
ing techniques normally applied to n-gram mod-
els (Kneser and Ney, 1995), enabling us to apply
techniques developed for scaling n-gram models out
of the box (Brants et al., 2007; Pauls and Klein,
2011). The simplicity of our training procedure al-
lows us to train a model on a billion tokens of data in
a matter of hours on a single machine, which com-
pares favorably to the more involved training algo-
rithm of Tan et al. (2011), who use a two-pass EM
training algorithm that takes several days on several
hundred CPUs using similar amounts of data.
The simplicity of our approach also contrasts with
recent work on language modeling with tree sub-
stitution grammars (Post and Gildea, 2009), where
larger treelet contexts are incorporated by using so-
phisticated priors to learn a segmentation of parse
trees. Such an approach implicitly assumes that a
“correct” segmentation exists, but it is not clear that
this is true in practice. Instead, we build upon the
success of n-gram language models, which do not
assume a segmentation and instead score all over-
lapping contexts.
We evaluate our model in terms of perplexity, and
show that we achieve the same performance as a
state-of-the-art n-gram model. We also evaluate our
model on several grammaticality tasks proposed in
</bodyText>
<footnote confidence="0.984841">
1We borrow the term treelet from Quirk et al. (2005), who
use it to refer to an arbitrary connected subgraph of a tree.
</footnote>
<page confidence="0.933157">
959
</page>
<note confidence="0.99267">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959–968,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.962023">
Figure 1: Conditioning contexts and back-off strategies for Markov mdls. The bolded symbol indicates the part of the
he board ’s will soon be feasible from everyday which Coke ’s abinet hotels
</figureCaption>
<bodyText confidence="0.8218314">
tree/sentence being generated, and the dotted ines represent the conditioning contexts; back-off proceeds from the largest to the
They are all priced became regulatory action by difficulty caused nor Aug 31 of HelmsleySpear :
smallest context. (a) A trigram model. (b) The context used for non-terminal productions in our treelet model. For this context,
P=VP-VBDˆS, P&apos;=S-VBDˆROOT, and r&apos;=S-VBD&amp;quot;ROOT→NP-NN VP-VBDˆS . (c) The context used for terminal productions h y
in our treelet model. Here, P=VBD, R=CD-DC, r&apos;=VP-VBDˆS→VBD CD-DC NNTP, w_1=index, and w_2=The. Note that the
</bodyText>
<figure confidence="0.9886375">
TLET
tree is a modified version of a standard Penn Treebank parse – see Section 3 for details.
S-VBDˆROOT
The
fell
index
The
index
fell
10 .85
Monday
(a) The index fell 10 .85 Monday . (b) ROOT
S-VBDˆROOT
(c) ROOT
NP-NN
VP-VBDˆS
.
NP-NN
VP-VBDˆS
.
DT-the
NN
VBD
CD-DC
NNTP
DT-the
NN
VBD
CD-DC
NNTP
.
.
10 .85
Monday
the literature (Okanohara and Tsujii, 2007; Foster et
He may enable drops to take , but wil hardly re
</figure>
<figureCaption confidence="0.721906">
al., 2008; Chrry and Quirk, 2008) and sow that
</figureCaption>
<bodyText confidence="0.5956675">
The centers are losses f meals , and the runs ar
it consistently outperforms an n-gram model as well
</bodyText>
<tableCaption confidence="0.991581">
Table 1: The first four samples of length between 15 a
</tableCaption>
<bodyText confidence="0.8507955">
as other head-driven and tree-driven generative base-
lines. Our model even competes with state-of-the-art
rul cnxt uld ned its wn ta in th grm
discriminative classifiers specifically designed for
r) nd xsie prig ld be rde
each task, despite being estimated on positive data
I pracic, hver, lgg dls a
alone. We also show fluency improvements in a pre-
mlly igaed i a dcder a iial tk
liminary machine translation reranking experiment.
h i hihl bldd d b d h
spe his ppHowvr,
</bodyText>
<sectionHeader confidence="0.956071" genericHeader="method">
2 Treelet Language Modeling
</sectionHeader>
<bodyText confidence="0.99880484">
hi li dl h b
The common denominator of mos n-gram language
constituency parses, such as that of Galley et al.
modelsis that they assign probabilities roughly ac-
(2006), combined with an efficient pruning strategy
cording to empirical frequencies for observed n-
like cube pruning (Chiang, 2005), shold be able to
grams, but fall back to distributions conditioned on
integrte our model without much difficulty.
smaller contexts for unobserved n-grams, as shown
That said, for evaluation purpoes, whnever we
inFigure 1(a). This type of smoothing is both highl
need to query our model, we use the simple strategy
robust and easy to implement, requiring only the col-
of parsing a sentence using a black box parser, and
lection of counts from data.
summing over our models p
est parses. Note that the bottleneck in this case
We would like to apply the same smoothing tech-
niquesto distributions over rule yields in a con-
is the parer, so our model an essentialy score a
stituency tree, conditioned on contexts consisting
sentence at th speed of a prse
of previously generated treelets (rules, nodes, etc.).
Formally, let T be a constituency tree consisting of
</bodyText>
<sectionHeader confidence="0.998078" genericHeader="method">
5 Expriment
</sectionHeader>
<bodyText confidence="0.999818">
contxt-free rules of the form r = P � C1 � � � Cd,
We evaluate our model along severl dmensions.
where P is the parent symbol of rule r and Cd
We first how some sample sentences gnerated by
</bodyText>
<equation confidence="0.911419">
1 =
</equation>
<footnote confidence="0.558144666666667">
C1 .. . Cd are its hildren. e wish to assign proba-
our model in Section 51 We report perplexity re
bilities to trees2
00-best on our gammaticalty tasks, but signficantly overes-
2A distribution over trees also induces a distribution over
sentencesuwi given bypp(wi) = ETa(T)°wi p(T), where
</footnote>
<bodyText confidence="0.996314666666667">
where the conditioning context h is some portion of
sults in Sectio 5.2. In Section 53, we measure
the already-generated parts of the tree. In this paper,
its bility to disinguish between grammatical En-
we assume that the children of a rule are expanded
glish and vaous types of automatically enratd,
from left to right, so that when generating the yield
or pseudo-negaive, Englsh. We rport mchin
Cd 1, all treelets above and left of the parent P are
translatio reranking results in Section 5.4
available. Note that a raw PCFG would condition
only on P, i.e. h = P.
</bodyText>
<subsectionHeader confidence="0.715515">
51 Geti
</subsectionHeader>
<bodyText confidence="0.9626865625">
As in the n-gram case, we would like to pick h
Because our model s generative, we can qualta
to be large enough to capture relevant dependencies,
tively assess it by generating samples and verifyng
but small enough that we can obtain meaningful es-
that they are more yntactically coherent tan other
timates from data. We start with a straightforward
approaches. In Table 1, we show the rs four sam
choice of context: we condition on P , as well as the
ples of length betwen 15 and 20 generated from
rule r&apos; that generated P , as shown in in Figure 1(b).
th ode an5gr
Conditioning on the parent rule r&apos; allows us to
e traed on t nn
capture several important dependencies. First, it
e
</bodyText>
<subsectionHeader confidence="0.67213">
52 Perplexity
</subsectionHeader>
<bodyText confidence="0.928611636363636">
captures both P and its parent P&apos;, which predicts
the distribution over child symbols far better than
Pplexity i h andard inriic vli ic
just P (Johnson, 1998). Second, it captures posi-
fo languag modl It mure h inves f h
tional effects. For example, subject and object noun
ped prbabily a mdl asigs om hld
phrases (NPs) have different distributions (Klein and
ou t f grammical Eglish ( lor is beter)
Manning, 2003), and the position of an NP relative
F ining dta we ontrd a lg trbk by
to a verb is a good indicator of this distinction. Fi-
ntnig th P Tbk th Bow C
nally, the generation of words at preterminals can
th 50K BLLIP ti e f Pt
condition on siblings, allowing the model to capture,
(2011) d th AFP d APW pti f Englih
for example, verb subcategorization frames.
We should be clear that we are not the first
We follow Okanohara and Tsuji (2007) i using the term
ated negativ examples might n
s(T ) is the terminal yield of T .
</bodyText>
<equation confidence="0.985695333333333">
�
p(T) = p(Cd 1|h)n RAM
r�T
</equation>
<page confidence="0.975665">
960
</page>
<bodyText confidence="0.999910375">
to use back-off-based smoothing for syntactic lan-
guage modeling – such techniques have been ap-
plied to models that condition on head-word con-
texts (Charniak, 2001; Roark, 2004; Zhang, 2009).
Parent rule context has also been employed in trans-
lation (Vaswani et al., 2011). However, to our
knowledge, we are the first to apply these techniques
for language modeling on large amounts of data.
</bodyText>
<subsectionHeader confidence="0.997192">
2.1 Lexical context
</subsectionHeader>
<bodyText confidence="0.999931416666667">
Although it is tempting to think that we can replace
the left-to-right generation of n-gram models with
the purely top-down generation of typical PCFGs,
in practice, words are often highly predictive of the
words that follow them – indeed, n-gram models
would be terrible language models if this were not
the case. To capture linear effects, we extend the
context for terminal (lexical) productions to include
the previous two words w−2 and w−1 in the sentence
in addition to r0; see Figure 1(c) for a depiction. This
allows us to capture collocations and other lexical
correlations.
</bodyText>
<subsectionHeader confidence="0.999878">
2.2 Backing off
</subsectionHeader>
<bodyText confidence="0.999929642857143">
As with n-gram models, counts for rule yields con-
ditioned on r0 are sparse, and we must choose an ap-
propriate back-off strategy. We handle terminal and
non-terminal productions slightly differently.
For non-terminal productions, we back off from
r0 to P and its parent P0, and then to just P.
That is, we back off from a rule-annotated gram-
mar p(Cd1|P, P0, r0) to a parent-annotated gram-
mar (Johnson, 1998) p(Cd1|P, P0), then to a raw
PCFG p(Cd1|P). In order to generalize to unseen
rule yields Cd1, we further back off from the ba-
sic PCFG probability p(Cd1|P) to p(Ci|Ci−1i−3, P), a
4-gram model over symbols C conditioned on P,
interpolated with an unconditional 4-gram model
</bodyText>
<equation confidence="0.9782606">
p(Ci|Ci−1
i−3). In other words, we back off from a raw
PCFG to
p(Ci|Ci−1
i−3, P) + (1 − A)
</equation>
<bodyText confidence="0.996220384615385">
where A = 0.9 is an interpolation constant.
For terminal (i.e lexical) productions, we
first remove lexical context, backing off from
p(w|P, R, r0, w−1, w−2) to p(w|P, R, r0, w−1) and
then p(w|P, R, r0). From there, we back off to
p(w|P, R) where R is the sibling immediately to the
right of P, then to a raw PCFG p(w|P), and finally
to a unigram distribution. We chose this scheme be-
cause p(w|P, R) allows, for example, a verb to be
generated conditioned on the non-terminal category
of the argument it takes (since arguments usually im-
mediately follow verbs). We depict these two back-
off schemes pictorially in Figure 1(b) and (c).
</bodyText>
<subsectionHeader confidence="0.995845">
2.3 Estimation
</subsectionHeader>
<bodyText confidence="0.99214824137931">
Estimating the probabilities in our model can be
done very simply using the same techniques (in fact,
the same code) used to estimate n-gram language
models. Our model requires estimates of four distri-
butions: p(Cd1|P, P0, r0), p(w|P, R, r0, w−1, w−2),
p(Ci|Ci−1
i−n+1, P), and p(Ci|Ci−1
i−n+1). In each case,
we require empirical counts of treelet tuples in the
same way that we require counts of word tuples for
estimating n-gram language models.
There is one additional hurdle in the estimation of
our model: while there exist corpora with human-
annotated constituency parses like the Penn Tree-
bank (Marcus et al., 1993), these corpora are quite
small – on the order of millions of tokens – and we
cannot gather nearly as many counts as we can for n-
grams, for which billions or even trillions (Brants et
al., 2007) of tokens are available on the Web. How-
ever, we can use one of several high-quality con-
stituency parsers (Collins, 1997; Charniak, 2000;
Petrov et al., 2006) to automatically generate parses.
These parses may contain errors, but not all parsing
errors are problematic for our model, since we only
care about the sentences generated by our model and
not the parses themselves. We show in our experi-
ments that the addition of data with automatic parses
does improve the performance of our language mod-
els across a range of tasks.
</bodyText>
<sectionHeader confidence="0.992548" genericHeader="method">
3 Tree Transformations
</sectionHeader>
<bodyText confidence="0.9999745">
In the previous section, we described how to condi-
tion on rich parse context to better capture the dis-
tribution of English trees. While such context al-
lows our model to capture many interesting depen-
dencies, several important dependencies require ad-
ditional attention. In this section, we describe a
</bodyText>
<figure confidence="0.951491666666667">
d
A�
i=1
d
i=1
p(Ci|Ci−1
i−3)
961
ROOT
S-VB&amp;quot;ROOT
reset JJ NNS IN-for NNT
opening arguments for today
</figure>
<figureCaption confidence="0.947211">
Figure 2: A sample parse from the Penn Treebank after
the tree transformations described in Section 3. Note that
we have not shown head tag annotations on preterminals
because in that case, the head tag is the preterminal itself.
</figureCaption>
<bodyText confidence="0.985148846153846">
number of transformations of Treebank constituency
parses that allow us to capture such dependencies.
We list the annotations and deletions in the order in
which they are performed. A sample transformed
tree is shown in Figure 2.
Temporal NPs Following Klein and Manning (2003),
we attempt to annotate temporal noun phrases. Although
the Penn Treebank annotates temporal NPs, most off-the-
shelf parsers do not retain these tags, and we do not as-
sume their presence. Instead, we mark any noun that is
the head of a NP-TMP constituent at least once in the
Treebank as a temporal noun, so for example today would
be tagged as NNT and months would be tagged as NNTS.
Head Annotations We annotate every non-terminal or
preterminal with its head word if the head is a closed-
class word3 and with its head tag otherwise. Klein and
Manning (2003) used head tag annotation extensively,
though they applied their splits much more selectively.
NP Flattening We delete NPs dominated by
other NPs, unless the child NPs are in coordi-
nation or apposition. These NPs typically oc-
cur when nouns are modified by PPs, as in
(NP (NP (NN stock) (NNS sales)) (PP (IN by) (NNS traders))). By
removing the dominated NP, we allow the production
NNS→sales to condition on the presence of a modifying
PP (here a PP head-annotated with by).
</bodyText>
<footnote confidence="0.969057125">
Number Annotations Numbers are divided into five
classes: CD-YR for numbers that consist of four digits
(which are usually years); CD-NM for entirely numeric
numbers; CD-DC for numbers that have a decimal; CD-
3We define the following to be closed class words: any punc-
tuation; all inflections of the verbs do, be, and have; and any
word tagged with IN, WDT, PDT, WP, WP$, TO, WRB, RP,
DT, SYM, EX, POS, PRP, AUX, or CC.
</footnote>
<bodyText confidence="0.729384">
MX for numbers that mix letters and digits; and CD-AL
for numbers that are entirely alphabetic.
SBAR Flattening We remove any sentential (S) nodes
immediately dominated by an SBAR. S nodes under
SBAR have very distinct distributions from other senten-
tial nodes, mostly due to empty subjects and/or objects.
VP Flattening We remove any VPs immediately domi-
nating a VP, unless it is conjoined with another VP. In the
Treebank, chains of verbs (e.g. will be going) have a sep-
arate VP for each verb. By flattening such structures, we
allow the main verb and its arguments to condition on the
whole chain of verbs. This effect is particularly important
for passive constructions.
Gapped Sentence Annotation Collins (1999) and
Klein and Manning (2003) annotate nodes which have
empty subjects. Because we only assume the presence
of automatically derived parses, which do not produce
the empty elements in the original Treebank, we must
identify such elements on our own. We use a very simple
procedure: we annotate all S or SBAR nodes that have a
VP before any NPs.
Parent Annotation We annotate all VPs with their par-
ent symbol. Because our treelet model already conditions
on the parent, this has the effect of allowing verbs to con-
dition on their grandparents. This was important for VPs
under SBAR nodes, which often have empty objects. We
also parent-annotated any child of the ROOT.
Unary Deletion We remove all unary productions ex-
cept the root and preterminal productions, keeping only
the bottom-most symbol. Because we are not interested
in the internal labels of the trees, unaries are largely a
nuisance, and their removal brings many symbols into the
context of others.
</bodyText>
<sectionHeader confidence="0.887839" genericHeader="method">
4 Scoring a Sentence
</sectionHeader>
<bodyText confidence="0.9998306">
Computing the probability of a sentence w1 under
our model requires summing over all possible parses
of wt. Although our model can be formulated as a
straightforward PCFG, allowing O(f&apos;) computation
of this sum, the grammar constant for this PCFG
would be unmanageably large (since every parent
rule context would need its own state in the gram-
mar), and extensive pruning would be in order.
In practice, however, language models are nor-
mally integrated into a decoder, a non-trivial task
that is highly problem-dependent and beyond the
scope of this paper. For machine translation, a model
that builds target-side constituency parses, such as
that of Galley et al. (2006), combined with an ef-
ficient pruning strategy like cube pruning (Chiang,
</bodyText>
<table confidence="0.812718421052632">
PRP-he
VP-VB&amp;quot;S
.
He
VB
NP-NNS
PP-for
.
962
5-GRAM
The board ’s will soon be feasible , from everyday which Coke ’s cabinet hotels.
They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :
Lakeland, it may take them if the 46-year-old said the loss of the Japanese executives at him :
But 8.32 % stake in and Rep. any money for you got from several months, ” he says.
TREELET
Why a $ 1.2 million investment in various types of the bulk of TVS E. August ?
“ One operating price position has a system that has Quartet for the first time , ” he said.
He may enable drops to take, but will hardly revive the rush to develop two-stroke calculations. ”
The centers are losses of meals, and the runs are willing to like them.
</table>
<tableCaption confidence="0.999976">
Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.
</tableCaption>
<bodyText confidence="0.999151222222222">
2005), should be able to integrate our model without
much difficulty.
That said, for evaluation purposes, whenever we
need to query our model, we use the simple strategy
of parsing a sentence using a black box parser, and
summing over our model’s probabilities of the 1000-
best parses.4 Note that the bottleneck in this case
is the parser, so our model can essentially score a
sentence at the speed of a parser.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99935375">
We evaluate our model along several dimensions.
We first show some sample generated sentences in
Section 5.1. We report perplexity results in Sec-
tion 5.2. In Section 5.3, we measure its ability to
distinguish between grammatical English and var-
ious types of automatically generated, or pseudo-
negative,5 English. We report machine translation
reranking results in Section 5.4.
</bodyText>
<subsectionHeader confidence="0.988653">
5.1 Generating Samples
</subsectionHeader>
<bodyText confidence="0.999844714285714">
Because our model is generative, we can qualita-
tively assess it by generating samples and verifying
that they are more syntactically coherent than other
approaches. In Table 1, we show the first four sam-
ples of length between 15 and 20 generated from our
model and a 5-gram model trained on the Penn Tree-
bank.
</bodyText>
<footnote confidence="0.955454666666667">
4We found that using the 1-best worked just as well as the
1000-best on our grammaticality tasks, but significantly overes-
timated our model’s perplexities.
5We follow Okanohara and Tsujii (2007) in using the term
pseudo-negative to highlight the fact that automatically gener-
ated negative examples might not actually be ungrammatical.
</footnote>
<subsectionHeader confidence="0.987878">
5.2 Perplexity
</subsectionHeader>
<bodyText confidence="0.872055555555556">
Perplexity is the standard intrinsic evaluation metric
for language models. It measures the inverse of the
per-word probability a model assigns to some held-
out set of grammatical English (so lower is better).
For training data, we constructed a large treebank by
concatenating the WSJ and Brown portions of the
Penn Treebank, the 50K BLLIP training sentences
from Post (2011), and the AFP and APW portions
of English Gigaword version 3 (Graff, 2003), total-
ing about 1.3 billion tokens. We used the human-
annotated parses for the sentences in the Penn Tree-
bank, but parsed the Gigaword and BLLIP sentences
with the Berkeley Parser. Hereafter, we refer to this
training data as our 1B corpus. We used Section 0
of the WSJ as our test corpus. Results are shown in
Table 2. In addition to our TREELET model, we also
show results for the following baselines:
5-GRAM A 5-gram interpolated Kneser-Ney model.
PCFG-LA The Berkeley Parser in language model mode.
HEADLEX A head-lexicalized model similar to, but
more powerful6 than, Collins Model 1 (Collins, 1999).
PCFG A raw PCFG.
TREELET-TRANS A PCFG estimated on the trees after
the transformations of Section 3.
TREELET-RULE The TREELET-TRANS model with the
parent rule context described in Section 2. This is equiv-
alent to the full TREELET model without the lexical con-
text described in Section 2.1.
6Specifically, like Collins Model 1, we generate a rule yield
conditioned on parent symbol P and head word h by first gen-
erating its head symbol Ch, then generating the head words and
symbols for left and right modifiers outwards from Ch. Unlike
Model 1, which generates each modifier head and symbol con-
ditioned only on Ch, h, and P, we additionally condition on the
previously generated modifier’s head and symbol and back off
to Model 1.
</bodyText>
<page confidence="0.988888">
963
</page>
<table confidence="0.995246">
Model Perplexity
PCFG 1772
TREELET-TRANS 722
TREELET-RULE 329
TREELET 198†
PCFG-LA 330**
HEADLEX 299
5-GRAM 207†
</table>
<tableCaption confidence="0.946061">
Table 2: Perplexity of several generative models on Sec-
</tableCaption>
<figureCaption confidence="0.66110575">
tion 0 of the WSJ. The differences between scores marked
with † are not statistically significant. PCFG-LA (marked
with **) was only trained on the WSJ and Brown corpora
because it does not scale to large amounts of data.
</figureCaption>
<bodyText confidence="0.999948285714286">
We used the Berkeley LM toolkit (Pauls
and Klein, 2011), which implements Kneser-Ney
smoothing, to estimate all back-off models for both
n-gram and treelet models. To deal with unknown
words, we use the following strategy: after the first
10000 sentences, whenever we see a new word in
our training data, we replace it with a signature7
10% of the time.
Our model outperforms all other generative mod-
els, though the improvement over the n-gram model
is not statistically significant. Note that because we
use a k-best approximation for the sum over trees,
all perplexities (except for PCFG-LA and 5-GRAM)
are pessimistic bounds.
</bodyText>
<subsectionHeader confidence="0.999393">
5.3 Classification of Pseudo-Negative Sentences
</subsectionHeader>
<bodyText confidence="0.997907157894737">
We make use of three kinds of automatically gener-
ated pseudo-negative sentences previously proposed
in the literature: Okanohara and Tsujii (2007) pro-
posed generating pseudo-negative examples from a
trigram language model; Foster et al. (2008) create
“noisy” sentences by automatically inserting a sin-
gle error into grammatical sentences with a script
that randomly deletes, inserts, or misspells a word;
and Och et al. (2004) and Cherry and Quirk (2008)
both use the 1-best output of a machine translation
system. Examples of these three types of pseudo-
negative data are shown in Table 3. We evaluate our
model’s ability to distinguish positive from pseudo-
negative data, and compare against generative base-
lines and state-of-the-art discriminative methods.
7We use signatures generated by the Berkeley Parser.
These signatures capture surface features such as capitalization,
presents of digits, and common suffixes. For example, the word
vexing would be replaced with the signature UNK-ing.
</bodyText>
<table confidence="0.76881">
Noisy There was were many contributors.
Trigram For years in dealer immediately.
MT we must further steps.
</table>
<tableCaption confidence="0.998399">
Table 3: Sample pseudo-negative sentences.
</tableCaption>
<bodyText confidence="0.999994176470588">
We would like to use our model to make grammat-
icality judgements, but as a generative model it can
only provide us with probabilities. Simply thresh-
olding generative probabilities, even with a separate
threshold for each length, has been shown to be very
ineffective for grammaticality judgements, both for
n-gram and syntactic language models (Cherry and
Quirk, 2008; Post, 2011). We used a simple measure
for isolating the syntactic likelihood of a sentence:
we take the log-probability under our model and
subtract the log-probability under a unigram model,
then normalize by the length of the sentence.8 This
measure, which we call the syntactic log-odds ratio
(SLR), is a crude way of “subtracting out” the se-
mantic component of the generative probability, so
that sentences that use rare words are not penalized
for doing so.
</bodyText>
<subsubsectionHeader confidence="0.722593">
5.3.1 Trigram Classification
</subsubsectionHeader>
<bodyText confidence="0.99944405">
To facilitate comparison with previous work, we
used the same negative corpora as Post (2011) for
trigram classification. They randomly selected 50K
train, 3K development, and 3K positive test sen-
tences from the BLLIP corpus, then trained a tri-
gram model on 450K BLLIP sentences and gener-
ated 50K train, 3K development, and 3K negative
sentences. We parsed the 50K positive training ex-
amples of Post (2011) with the Berkeley Parser and
used the resulting treebank to train a treelet language
model. We set an SLR threshold for each model on
the 6K positive and negative development sentences.
Results are shown in Table 4. In addition to our
generative baselines, we show results for the dis-
criminative models reported in Cherry and Quirk
(2008) and Post (2011). The former train a latent
PCFG support vector machine for binary classifica-
tion (LSVM). The latter report results for two bi-
nary classifiers: RERANK uses the reranking fea-
tures of Charniak and Johnson (2005), and TSG uses
</bodyText>
<footnote confidence="0.998818">
8Och et al. (2004) also report using a parser probability nor-
malized by the unigram probability (but not length), and did not
find it effective. We assume this is either because the length-
normalization is important, or because their choice of syntactic
language model was poor.
</footnote>
<page confidence="0.988798">
964
</page>
<table confidence="0.9997135">
Generative
BLLIP 1B
PCFG 81.5 81.8
TREELET-TRANS 87.7 90.1
TREELET-RULE 89.8 94.1
TREELET 88.9 93.3
PCFG-LA 87.1* –
HEADLEX 87.6 92.0
5-GRAM 67.9 87.5
Discriminative
BLLIP 1B
LSVM 81.42** –
TSG 89.9 –
RERANK 93.0 –
</table>
<tableCaption confidence="0.923518285714286">
Table 4: Classification accuracy for trigram pseudo-negative
sentences on the BLLIP corpus. The number reported for
PCFG-LA is marked with a * to indicate that this model was
trained on the training section of the WSJ, not the BLLIP cor-
pus. The number reported for LSVM (marked with **) was eval-
uated on a different random split of the BLLIP corpus, and so is
not directly comparable.
</tableCaption>
<bodyText confidence="0.992115066666667">
indicator features extracted from a tree substitution
grammar derivation of each sentence.
Our TREELET model performs nearly as well as
the TSG method, and substantially outperforms the
LSVM method, though the latter was not tested on
the same random split. Interestingly, the TREELET-
RULE baseline, which removes lexical context from
our model, outperforms the full model. This is likely
because the negative data is largely coherent at the
trigram level (because it was generated from a tri-
gram model), and the full model is much more sen-
sitive to trigram coherence than the TREELET-RULE
model. This also explains the poor performance of
the 5-GRAM model.
We emphasize that the discriminative baselines
are specifically trained to separate trigram text from
natural English, while our model is trained on pos-
itive examples alone. Indeed, the methods in Post
(2011) are simple binary classifiers, and it is not
clear that these models would be properly calibrated
for any other task, such as integration in a decoder.
One of the design goals of our system was that
it be scalable. Unlike some of the discriminative
baselines, which require expensive operations9 on
9It is true that in order train our system, one must parse large
amounts of training data, which can be costly, though it only
needs to be done once. In contrast, even with observed train-
ing trees, the discriminative algorithms must still iteratively per-
form expensive operations (like parsing) for each sentence, and
a new model must be trained for new types of negative data.
</bodyText>
<table confidence="0.999727222222222">
Model Pairwise Independent
WSJ 1B WSJ 1B
PCFG 79.1 77.0 58.9 58.6
TREELET-RULE 90.3 94.4 63.8 66.2
TREELET 90.7 94.5 63.4 65.5
5-GRAM 86.3 93.5 55.7 60.1
HEADLEX 90.7 94.0 59.5 62.0
PCFG-LA 91.3 – 59.7 –
Foster et al. (2008) – – 65.9 –
</table>
<tableCaption confidence="0.973537">
Table 5: Classification accuracies on the noisy WSJ for mod-
els trained on WSJ Sections 2-21 and our 1B token corpus.
“Pairwise” accuracy is the fraction of correct sentences whose
SLR score was higher than its noisy version, and “independent”
refers to standard binary classification accuracy.
</tableCaption>
<bodyText confidence="0.99980225">
each training sentence, we can very easily scale
our model to much larger amounts of data. In Ta-
ble 4, we also show the performance of the gener-
ative models trained on our 1B corpus. All gener-
ative models improve, but TREELET-RULE remains
the best, now outperforming the RERANK system,
though of course it is likely that RERANK would im-
prove if it could be scaled up to more training data.
</bodyText>
<subsectionHeader confidence="0.507839">
5.3.2 “Noisy” Classification
</subsectionHeader>
<bodyText confidence="0.99993996">
We also evaluate the performance of our model
on the task of distinguishing the noisy WSJ sen-
tences of Foster et al. (2008) from their original
versions. We use the noisy versions of Section 0
and 23 produced by their error-generating proce-
dure. Because they only report classification re-
sults on Section 0, we used Section 23 to tune an
SLR threshold, and tested our model on Section 0.
We show the results of both independent and pair-
wise classification for the WSJ and 1B training sets
in Table 5. Note that independent classification is
much more difficult than for the trigram data, be-
cause sentences contain at most one change, which
may not even result in an ungrammaticality. Again,
our model outperforms the n-gram model for both
types of classification, and achieves the same per-
formance as the discriminative system of Foster et
al. (2008), which is state-of-the-art for this data set.
The TREELET-RULE system again slightly outper-
forms the full TREELET model at independent clas-
sification, though not at pairwise classification. This
probably reflects the fact that semantic coherence
can still influence the SLR score, despite our efforts
to subtract it out. Because the TREELET model in-
cludes lexical context, it is more sensitive to seman-
</bodyText>
<page confidence="0.994184">
965
</page>
<table confidence="0.997957">
French German Chinese
5-GRAM 44.8 37.8 60.0
TREELET 57.9 66.0 83.8
</table>
<tableCaption confidence="0.8988921">
Table 6: Pairwise comparison accuracy of MT output
against a reference translation for French, German, and
Chinese. The BLEU scores for these outputs are 32.7,
27.8, and 20.8. This task becomes easier, at least for our
TREELET model, as translation quality drops. Cherry and
Quirk (2008) report an accuracy of 71.9% on a similar
experiment with German a source language, though the
translation system and training data were different so the
numbers are not comparable. In particular, their transla-
tions had a lower BLEU score, making their task easier.
</tableCaption>
<bodyText confidence="0.9999146">
tic coherence and thus more likely to misclassify
semantically coherent but ungrammatical sentences.
For pairwise comparisons, where semantic coher-
ence is effectively held constant, such sentences are
not problematic.
</bodyText>
<subsubsectionHeader confidence="0.62346">
5.3.3 Machine Translation Classification
</subsubsectionHeader>
<bodyText confidence="0.999939916666667">
We follow Och et al. (2004) and Cherry and Quirk
(2008) in evaluating our language models on their
ability to distinguish the 1-best output of a machine
translation system from a reference translation in a
pairwise fashion. Unfortunately, we do not have
access to the data used in those papers, so a di-
rect comparison is not possible. Instead, we col-
lected the English output of Moses (Hoang et al.,
2007), using both French and German as source lan-
guage, trained on the Europarl corpus used by WMT
2009.10 We also collected the output of Joshua (Li
et al., 2009) trained on 500K sentences of GALE
Chinese-English parallel newswire. We trained both
our TREELET model and a 5-GRAM model on the
union of our 1B corpus and the English sides of our
parallel corpora.
In Table 6, we show the pairwise comparison ac-
curacy (using SLR) on these three corpora. We see
that our system prefers the reference much more of-
ten than the 5-GRAM language model.11 However,
we also note that the easiness of the task is corre-
lated with the quality of translations (as measured in
BLEU score). This is not surprising – high-quality
translations are often grammatical and even a per-
</bodyText>
<footnote confidence="0.86500625">
10http://www.statmt.org/wmt09
11We note that the n-gram language model used by the MT
system was much smaller than the 5-GRAM model, as they were
only trained on the English sides of their parallel data.
</footnote>
<bodyText confidence="0.994956">
fect language model might not be able to differenti-
ate such translations from their references.
</bodyText>
<subsectionHeader confidence="0.893138">
5.4 Machine Translation Fluency
</subsectionHeader>
<bodyText confidence="0.999995272727273">
We also carried out reranking experiments on 1000-
best lists from Moses using our syntactic language
model as a feature. We did not find that the use
of our syntactic language model made any statis-
tically significant increases in BLEU score. How-
ever, we noticed in general that the translations fa-
vored by our model were more fluent, a useful im-
provement to which BLEU is often insensitive. To
confirm this, we carried out an Amazon Mechan-
ical Turk experiment where users from the United
States were asked to compare translations using our
TREELET language model as the language model
feature to those using the 5-GRAM model.12 We had
1000 such translation pairs rated by 4 separate Turk-
ers each. Although these two hypothesis sets had
the same BLEU score (up to statistical significance),
the Turkers preferred the output obtained using our
syntactic language model 59% of the time, indicat-
ing that our model had managed to pick out more
fluent hypotheses that nonetheless were of the same
BLEU score. This result was statistically significant
with p &lt; 0.001 using bootstrap resampling.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999876857142857">
We have presented a simple syntactic language
model that can be estimated using standard n-gram
smoothing techniques on large amounts of data. Our
model outperforms generative baselines on several
evaluation metrics and achieves the same perfor-
mance as state-of-the-art discriminative classifiers
specifically trained on several types of negative data.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997425666666667">
We would like to thank David Hall for some modeling
suggestions and the anonymous reviewers for their com-
ments. We thank both Matt Post and Jennifer Foster for
providing us with their corpora. This work was partially
supported by a Google Fellowship to the first author and
by BBN under DARPA contract HR0011-12-C-0014.
</bodyText>
<footnote confidence="0.5966895">
12We used translations from the baseline Moses system of
Section 5.3.3 with German as the input language. For each lan-
guage model, we took k-best lists from the baseline system and
replaced the baseline LM score with the new model’s score. We
then retrained all feature weights with MERT on the tune set,
and selected the 1-best output on the test set.
</footnote>
<page confidence="0.995913">
966
</page>
<sectionHeader confidence="0.983392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999478094339622">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, Jeffrey Dean, and Google Inc. 2007. Large lan-
guage models in machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American chapter
of the Association for Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the Association
for Computational Linguistics.
Ciprian Chelba. 1997. A structured language model. In
Proceedings of the Association for Computational Lin-
guistics.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the Association for Computa-
tional Linguistics.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
Proceedings of The Association for Machine Transla-
tion in the Americas.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of As-
sociation for Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jennifer Foster, Joachim Wagner, and Josef van Genabith.
2008. Adapting a wsj-trained parser to grammatically
noisy text. In Proceedings of the Association for Com-
putational Linguistics: Short Paper Track.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics (ACL).
David Graff. 2003. English gigaword, version 3. In Lin-
guistic Data Consortium, Philadelphia, Catalog Num-
ber LDC2003T05.
Keith Hall. 2004. Best-first Word-lattice Parsing: Tech-
niques for Integrated Syntactic Language Modeling.
Ph.D. thesis, Brown University.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
dej Bojar. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics: Demonstra-
tion Session,.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In IEEE
International Conference on Acoustics, Speech and
Signal Processing.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of The Association for Machine
Translation in the Americas.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A Smorgas-
bord of Features for Statistical Machine Translation.
In Proceedings of the North American Association for
Computational Linguistic.
Daisuke Okanohara and Jun’ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proceedings of the Association for Com-
putational Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the Asso-
ciation for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL 2006.
Matt Post and Daniel Gildea. 2009. Language model-
ing with tree substitution grammars. In Proceedings
</reference>
<page confidence="0.976807">
967
</page>
<reference confidence="0.998790428571428">
of the Conference on Neural Information Processing
Systems.
Matt Post. 2011. Judging grammaticality with tree sub-
stitution grammar. In Proceedings of the Association
for Computational Linguistics: Short Paper Track.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the Association of
Computational Linguistics.
Brian Roark. 2004. Probabilistic top-down parsing and
language modeling. Computational Linguistics.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2011. A large scale distributed syntactic, semantic
and lexical language model for machine translation.
In Proceedings of the Association for Computational
Linguistics.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the Association
for Computations Linguistics.
Peng Xu, Ciprian Chelba, and Fred Jelinek. 2002. A
study on richer syntactic dependencies for structured
language modeling. In Proceedings of the Association
for Computational Linguistics. Association for Com-
putational Linguistics.
Ying Zhang. 2009. Structured language models for sta-
tistical machine translation. Ph.D. thesis, Johns Hop-
kins University.
</reference>
<page confidence="0.997497">
968
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985669">
<title confidence="0.999929">Large-Scale Syntactic Language Modeling with Treelets</title>
<author confidence="0.999802">Adam Pauls Dan</author>
<affiliation confidence="0.999978">Computer Science University of California,</affiliation>
<address confidence="0.999651">Berkeley, CA 94720,</address>
<abstract confidence="0.999319666666667">We propose a simple generative, syntactic language model that conditions on overlapwindows of tree context (or in same way that language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatiparsed text using standard language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we peras well or better than models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
<author>Google Inc</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2353" citStr="Brants et al., 2007" startWordPosition="363" endWordPosition="366"> al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which compares favorably to the more involved training algorithm of Tan et al. (2011), who use a two-pass EM training algorithm that takes several days on several hundred CPUs using similar amounts of data. The simplicity of our approach also contrasts with recent work on language modeling with tree substitution grammars (Post and Gildea, 2009), where larger treelet contexts are incorporated by using sophisticated priors to learn a </context>
<context position="12773" citStr="Brants et al., 2007" startWordPosition="2181" endWordPosition="2184">istributions: p(Cd1|P, P0, r0), p(w|P, R, r0, w−1, w−2), p(Ci|Ci−1 i−n+1, P), and p(Ci|Ci−1 i−n+1). In each case, we require empirical counts of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 Tree Transformations In the previous section, we described how to con</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, Inc, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, Jeffrey Dean, and Google Inc. 2007. Large language models in machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26311" citStr="Charniak and Johnson (2005)" startWordPosition="4430" endWordPosition="4433">es. We parsed the 50K positive training examples of Post (2011) with the Berkeley Parser and used the resulting treebank to train a treelet language model. We set an SLR threshold for each model on the 6K positive and negative development sentences. Results are shown in Table 4. In addition to our generative baselines, we show results for the discriminative models reported in Cherry and Quirk (2008) and Post (2011). The former train a latent PCFG support vector machine for binary classification (LSVM). The latter report results for two binary classifiers: RERANK uses the reranking features of Charniak and Johnson (2005), and TSG uses 8Och et al. (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. We assume this is either because the lengthnormalization is important, or because their choice of syntactic language model was poor. 964 Generative BLLIP 1B PCFG 81.5 81.8 TREELET-TRANS 87.7 90.1 TREELET-RULE 89.8 94.1 TREELET 88.9 93.3 PCFG-LA 87.1* – HEADLEX 87.6 92.0 5-GRAM 67.9 87.5 Discriminative BLLIP 1B LSVM 81.42** – TSG 89.9 – RERANK 93.0 – Table 4: Classification accuracy for trigram pseudo-negative sentences on the BLLIP corp</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12909" citStr="Charniak, 2000" startWordPosition="2206" endWordPosition="2207">of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 Tree Transformations In the previous section, we described how to condition on rich parse context to better capture the distribution of English trees. While such context allows our model to capture many in</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the North American chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1760" citStr="Charniak, 2001" startWordPosition="264" endWordPosition="265">speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls</context>
<context position="9690" citStr="Charniak, 2001" startWordPosition="1651" endWordPosition="1652">r of this distinction. Fintnig th P Tbk th Bow C nally, the generation of words at preterminals can th 50K BLLIP ti e f Pt condition on siblings, allowing the model to capture, (2011) d th AFP d APW pti f Englih for example, verb subcategorization frames. We should be clear that we are not the first We follow Okanohara and Tsuji (2007) i using the term ated negativ examples might n s(T ) is the terminal yield of T . � p(T) = p(Cd 1|h)n RAM r�T 960 to use back-off-based smoothing for syntactic language modeling – such techniques have been applied to models that condition on head-word contexts (Charniak, 2001; Roark, 2004; Zhang, 2009). Parent rule context has also been employed in translation (Vaswani et al., 2011). However, to our knowledge, we are the first to apply these techniques for language modeling on large amounts of data. 2.1 Lexical context Although it is tempting to think that we can replace the left-to-right generation of n-gram models with the purely top-down generation of typical PCFGs, in practice, words are often highly predictive of the words that follow them – indeed, n-gram models would be terrible language models if this were not the case. To capture linear effects, we extend</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>A structured language model.</title>
<date>1997</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1727" citStr="Chelba, 1997" startWordPosition="258" endWordPosition="259">are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of th</context>
</contexts>
<marker>Chelba, 1997</marker>
<rawString>Ciprian Chelba. 1997. A structured language model. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1282" citStr="Chen and Goodman, 1998" startWordPosition="187" endWordPosition="190">ne in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N-gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used b</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Chris Quirk</author>
</authors>
<title>Discriminative, syntactic language modeling through latent SVMs.</title>
<date>2008</date>
<booktitle>In Proceedings of The Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="23791" citStr="Cherry and Quirk (2008)" startWordPosition="4030" endWordPosition="4033"> a k-best approximation for the sum over trees, all perplexities (except for PCFG-LA and 5-GRAM) are pessimistic bounds. 5.3 Classification of Pseudo-Negative Sentences We make use of three kinds of automatically generated pseudo-negative sentences previously proposed in the literature: Okanohara and Tsujii (2007) proposed generating pseudo-negative examples from a trigram language model; Foster et al. (2008) create “noisy” sentences by automatically inserting a single error into grammatical sentences with a script that randomly deletes, inserts, or misspells a word; and Och et al. (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. Examples of these three types of pseudonegative data are shown in Table 3. We evaluate our model’s ability to distinguish positive from pseudonegative data, and compare against generative baselines and state-of-the-art discriminative methods. 7We use signatures generated by the Berkeley Parser. These signatures capture surface features such as capitalization, presents of digits, and common suffixes. For example, the word vexing would be replaced with the signature UNK-ing. Noisy There was were many contributors. Trigram For years in </context>
<context position="26086" citStr="Cherry and Quirk (2008)" startWordPosition="4393" endWordPosition="4396">They randomly selected 50K train, 3K development, and 3K positive test sentences from the BLLIP corpus, then trained a trigram model on 450K BLLIP sentences and generated 50K train, 3K development, and 3K negative sentences. We parsed the 50K positive training examples of Post (2011) with the Berkeley Parser and used the resulting treebank to train a treelet language model. We set an SLR threshold for each model on the 6K positive and negative development sentences. Results are shown in Table 4. In addition to our generative baselines, we show results for the discriminative models reported in Cherry and Quirk (2008) and Post (2011). The former train a latent PCFG support vector machine for binary classification (LSVM). The latter report results for two binary classifiers: RERANK uses the reranking features of Charniak and Johnson (2005), and TSG uses 8Och et al. (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. We assume this is either because the lengthnormalization is important, or because their choice of syntactic language model was poor. 964 Generative BLLIP 1B PCFG 81.5 81.8 TREELET-TRANS 87.7 90.1 TREELET-RULE 89.8 9</context>
<context position="31311" citStr="Cherry and Quirk (2008)" startWordPosition="5266" endWordPosition="5269">assification, though not at pairwise classification. This probably reflects the fact that semantic coherence can still influence the SLR score, despite our efforts to subtract it out. Because the TREELET model includes lexical context, it is more sensitive to seman965 French German Chinese 5-GRAM 44.8 37.8 60.0 TREELET 57.9 66.0 83.8 Table 6: Pairwise comparison accuracy of MT output against a reference translation for French, German, and Chinese. The BLEU scores for these outputs are 32.7, 27.8, and 20.8. This task becomes easier, at least for our TREELET model, as translation quality drops. Cherry and Quirk (2008) report an accuracy of 71.9% on a similar experiment with German a source language, though the translation system and training data were different so the numbers are not comparable. In particular, their translations had a lower BLEU score, making their task easier. tic coherence and thus more likely to misclassify semantically coherent but ungrammatical sentences. For pairwise comparisons, where semantic coherence is effectively held constant, such sentences are not problematic. 5.3.3 Machine Translation Classification We follow Och et al. (2004) and Cherry and Quirk (2008) in evaluating our l</context>
</contexts>
<marker>Cherry, Quirk, 2008</marker>
<rawString>Colin Cherry and Chris Quirk. 2008. Discriminative, syntactic language modeling through latent SVMs. In Proceedings of The Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1400" citStr="Chiang, 2005" startWordPosition="206" endWordPosition="207">er than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N-gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that co</context>
<context position="5944" citStr="Chiang, 2005" startWordPosition="970" endWordPosition="971">ically designed for r) nd xsie prig ld be rde each task, despite being estimated on positive data I pracic, hver, lgg dls a alone. We also show fluency improvements in a premlly igaed i a dcder a iial tk liminary machine translation reranking experiment. h i hihl bldd d b d h spe his ppHowvr, 2 Treelet Language Modeling hi li dl h b The common denominator of mos n-gram language constituency parses, such as that of Galley et al. modelsis that they assign probabilities roughly ac(2006), combined with an efficient pruning strategy cording to empirical frequencies for observed nlike cube pruning (Chiang, 2005), shold be able to grams, but fall back to distributions conditioned on integrte our model without much difficulty. smaller contexts for unobserved n-grams, as shown That said, for evaluation purpoes, whnever we inFigure 1(a). This type of smoothing is both highl need to query our model, we use the simple strategy robust and easy to implement, requiring only the colof parsing a sentence using a black box parser, and lection of counts from data. summing over our models p est parses. Note that the bottleneck in this case We would like to apply the same smoothing techniquesto distributions over r</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12893" citStr="Collins, 1997" startWordPosition="2204" endWordPosition="2205">pirical counts of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 Tree Transformations In the previous section, we described how to condition on rich parse context to better capture the distribution of English trees. While such context allows our model to</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="16402" citStr="Collins (1999)" startWordPosition="2801" endWordPosition="2802">lattening We remove any sentential (S) nodes immediately dominated by an SBAR. S nodes under SBAR have very distinct distributions from other sentential nodes, mostly due to empty subjects and/or objects. VP Flattening We remove any VPs immediately dominating a VP, unless it is conjoined with another VP. In the Treebank, chains of verbs (e.g. will be going) have a separate VP for each verb. By flattening such structures, we allow the main verb and its arguments to condition on the whole chain of verbs. This effect is particularly important for passive constructions. Gapped Sentence Annotation Collins (1999) and Klein and Manning (2003) annotate nodes which have empty subjects. Because we only assume the presence of automatically derived parses, which do not produce the empty elements in the original Treebank, we must identify such elements on our own. We use a very simple procedure: we annotate all S or SBAR nodes that have a VP before any NPs. Parent Annotation We annotate all VPs with their parent symbol. Because our treelet model already conditions on the parent, this has the effect of allowing verbs to condition on their grandparents. This was important for VPs under SBAR nodes, which often </context>
<context position="21533" citStr="Collins, 1999" startWordPosition="3666" endWordPosition="3667">aff, 2003), totaling about 1.3 billion tokens. We used the humanannotated parses for the sentences in the Penn Treebank, but parsed the Gigaword and BLLIP sentences with the Berkeley Parser. Hereafter, we refer to this training data as our 1B corpus. We used Section 0 of the WSJ as our test corpus. Results are shown in Table 2. In addition to our TREELET model, we also show results for the following baselines: 5-GRAM A 5-gram interpolated Kneser-Ney model. PCFG-LA The Berkeley Parser in language model mode. HEADLEX A head-lexicalized model similar to, but more powerful6 than, Collins Model 1 (Collins, 1999). PCFG A raw PCFG. TREELET-TRANS A PCFG estimated on the trees after the transformations of Section 3. TREELET-RULE The TREELET-TRANS model with the parent rule context described in Section 2. This is equivalent to the full TREELET model without the lexical context described in Section 2.1. 6Specifically, like Collins Model 1, we generate a rule yield conditioned on parent symbol P and head word h by first generating its head symbol Ch, then generating the head words and symbols for left and right modifiers outwards from Ch. Unlike Model 1, which generates each modifier head and symbol conditi</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Josef van Genabith</author>
</authors>
<title>Adapting a wsj-trained parser to grammatically noisy text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Short Paper Track.</booktitle>
<marker>Foster, Wagner, van Genabith, 2008</marker>
<rawString>Jennifer Foster, Joachim Wagner, and Josef van Genabith. 2008. Adapting a wsj-trained parser to grammatically noisy text. In Proceedings of the Association for Computational Linguistics: Short Paper Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="18056" citStr="Galley et al. (2006)" startWordPosition="3073" endWordPosition="3076">r model requires summing over all possible parses of wt. Although our model can be formulated as a straightforward PCFG, allowing O(f&apos;) computation of this sum, the grammar constant for this PCFG would be unmanageably large (since every parent rule context would need its own state in the grammar), and extensive pruning would be in order. In practice, however, language models are normally integrated into a decoder, a non-trivial task that is highly problem-dependent and beyond the scope of this paper. For machine translation, a model that builds target-side constituency parses, such as that of Galley et al. (2006), combined with an efficient pruning strategy like cube pruning (Chiang, PRP-he VP-VB&amp;quot;S . He VB NP-NNS PP-for . 962 5-GRAM The board ’s will soon be feasible , from everyday which Coke ’s cabinet hotels. They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear : Lakeland, it may take them if the 46-year-old said the loss of the Japanese executives at him : But 8.32 % stake in and Rep. any money for you got from several months, ” he says. TREELET Why a $ 1.2 million investment in various types of the bulk of TVS E. August ? “ One operating price position h</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In The Annual Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English gigaword, version 3.</title>
<date>2003</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia, Catalog Number</location>
<contexts>
<context position="20929" citStr="Graff, 2003" startWordPosition="3564" endWordPosition="3565"> (2007) in using the term pseudo-negative to highlight the fact that automatically generated negative examples might not actually be ungrammatical. 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some heldout set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the WSJ and Brown portions of the Penn Treebank, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English Gigaword version 3 (Graff, 2003), totaling about 1.3 billion tokens. We used the humanannotated parses for the sentences in the Penn Treebank, but parsed the Gigaword and BLLIP sentences with the Berkeley Parser. Hereafter, we refer to this training data as our 1B corpus. We used Section 0 of the WSJ as our test corpus. Results are shown in Table 2. In addition to our TREELET model, we also show results for the following baselines: 5-GRAM A 5-gram interpolated Kneser-Ney model. PCFG-LA The Berkeley Parser in language model mode. HEADLEX A head-lexicalized model similar to, but more powerful6 than, Collins Model 1 (Collins, 1</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English gigaword, version 3. In Linguistic Data Consortium, Philadelphia, Catalog Number LDC2003T05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
</authors>
<title>Best-first Word-lattice Parsing: Techniques for Integrated Syntactic Language Modeling.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="1772" citStr="Hall, 2004" startWordPosition="266" endWordPosition="267">on and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, </context>
</contexts>
<marker>Hall, 2004</marker>
<rawString>Keith Hall. 2004. Best-first Word-lattice Parsing: Techniques for Integrated Syntactic Language Modeling. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>Kenlm: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1341" citStr="Heafield, 2011" startWordPosition="198" endWordPosition="199">rammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N-gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper,</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. Kenlm: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondej Bojar</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Demonstration Session,.</booktitle>
<contexts>
<context position="32241" citStr="Hoang et al., 2007" startWordPosition="5413" endWordPosition="5416">sify semantically coherent but ungrammatical sentences. For pairwise comparisons, where semantic coherence is effectively held constant, such sentences are not problematic. 5.3.3 Machine Translation Classification We follow Och et al. (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. Unfortunately, we do not have access to the data used in those papers, so a direct comparison is not possible. Instead, we collected the English output of Moses (Hoang et al., 2007), using both French and German as source language, trained on the Europarl corpus used by WMT 2009.10 We also collected the output of Joshua (Li et al., 2009) trained on 500K sentences of GALE Chinese-English parallel newswire. We trained both our TREELET model and a 5-GRAM model on the union of our 1B corpus and the English sides of our parallel corpora. In Table 6, we show the pairwise comparison accuracy (using SLR) on these three corpora. We see that our system prefers the reference much more often than the 5-GRAM language model.11 However, we also note that the easiness of the task is cor</context>
</contexts>
<marker>Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondej Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Association for Computational Linguistics: Demonstration Session,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<contexts>
<context position="8722" citStr="Johnson, 1998" startWordPosition="1466" endWordPosition="1467">gful esthat they are more yntactically coherent tan other timates from data. We start with a straightforward approaches. In Table 1, we show the rs four sam choice of context: we condition on P , as well as the ples of length betwen 15 and 20 generated from rule r&apos; that generated P , as shown in in Figure 1(b). th ode an5gr Conditioning on the parent rule r&apos; allows us to e traed on t nn capture several important dependencies. First, it e 52 Perplexity captures both P and its parent P&apos;, which predicts the distribution over child symbols far better than Pplexity i h andard inriic vli ic just P (Johnson, 1998). Second, it captures posifo languag modl It mure h inves f h tional effects. For example, subject and object noun ped prbabily a mdl asigs om hld phrases (NPs) have different distributions (Klein and ou t f grammical Eglish ( lor is beter) Manning, 2003), and the position of an NP relative F ining dta we ontrd a lg trbk by to a verb is a good indicator of this distinction. Fintnig th P Tbk th Bow C nally, the generation of words at preterminals can th 50K BLLIP ti e f Pt condition on siblings, allowing the model to capture, (2011) d th AFP d APW pti f Englih for example, verb subcategorizatio</context>
<context position="10945" citStr="Johnson, 1998" startWordPosition="1861" endWordPosition="1862">ctions to include the previous two words w−2 and w−1 in the sentence in addition to r0; see Figure 1(c) for a depiction. This allows us to capture collocations and other lexical correlations. 2.2 Backing off As with n-gram models, counts for rule yields conditioned on r0 are sparse, and we must choose an appropriate back-off strategy. We handle terminal and non-terminal productions slightly differently. For non-terminal productions, we back off from r0 to P and its parent P0, and then to just P. That is, we back off from a rule-annotated grammar p(Cd1|P, P0, r0) to a parent-annotated grammar (Johnson, 1998) p(Cd1|P, P0), then to a raw PCFG p(Cd1|P). In order to generalize to unseen rule yields Cd1, we further back off from the basic PCFG probability p(Cd1|P) to p(Ci|Ci−1i−3, P), a 4-gram model over symbols C conditioned on P, interpolated with an unconditional 4-gram model p(Ci|Ci−1 i−3). In other words, we back off from a raw PCFG to p(Ci|Ci−1 i−3, P) + (1 − A) where A = 0.9 is an interpolation constant. For terminal (i.e lexical) productions, we first remove lexical context, backing off from p(w|P, R, r0, w−1, w−2) to p(w|P, R, r0, w−1) and then p(w|P, R, r0). From there, we back off to p(w|P,</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="14233" citStr="Klein and Manning (2003)" startWordPosition="2425" endWordPosition="2428">, we describe a d A� i=1 d i=1 p(Ci|Ci−1 i−3) 961 ROOT S-VB&amp;quot;ROOT reset JJ NNS IN-for NNT opening arguments for today Figure 2: A sample parse from the Penn Treebank after the tree transformations described in Section 3. Note that we have not shown head tag annotations on preterminals because in that case, the head tag is the preterminal itself. number of transformations of Treebank constituency parses that allow us to capture such dependencies. We list the annotations and deletions in the order in which they are performed. A sample transformed tree is shown in Figure 2. Temporal NPs Following Klein and Manning (2003), we attempt to annotate temporal noun phrases. Although the Penn Treebank annotates temporal NPs, most off-theshelf parsers do not retain these tags, and we do not assume their presence. Instead, we mark any noun that is the head of a NP-TMP constituent at least once in the Treebank as a temporal noun, so for example today would be tagged as NNT and months would be tagged as NNTS. Head Annotations We annotate every non-terminal or preterminal with its head word if the head is a closedclass word3 and with its head tag otherwise. Klein and Manning (2003) used head tag annotation extensively, th</context>
<context position="16431" citStr="Klein and Manning (2003)" startWordPosition="2804" endWordPosition="2807"> any sentential (S) nodes immediately dominated by an SBAR. S nodes under SBAR have very distinct distributions from other sentential nodes, mostly due to empty subjects and/or objects. VP Flattening We remove any VPs immediately dominating a VP, unless it is conjoined with another VP. In the Treebank, chains of verbs (e.g. will be going) have a separate VP for each verb. By flattening such structures, we allow the main verb and its arguments to condition on the whole chain of verbs. This effect is particularly important for passive constructions. Gapped Sentence Annotation Collins (1999) and Klein and Manning (2003) annotate nodes which have empty subjects. Because we only assume the presence of automatically derived parses, which do not produce the empty elements in the original Treebank, we must identify such elements on our own. We use a very simple procedure: we annotate all S or SBAR nodes that have a VP before any NPs. Parent Annotation We annotate all VPs with their parent symbol. Because our treelet model already conditions on the parent, this has the effect of allowing verbs to condition on their grandparents. This was important for VPs under SBAR nodes, which often have empty objects. We also p</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="2248" citStr="Kneser and Ney, 1995" startWordPosition="345" endWordPosition="348">e syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which compares favorably to the more involved training algorithm of Tan et al. (2011), who use a two-pass EM training algorithm that takes several days on several hundred CPUs using similar amounts of data. The simplicity of our approach also contrasts with recent work on language modeling with tree substitution grammars (Post an</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of The Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="1385" citStr="Koehn, 2004" startWordPosition="204" endWordPosition="205"> well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N-gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic languag</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings of The Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: an open source toolkit for parsingbased machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="32399" citStr="Li et al., 2009" startWordPosition="5442" endWordPosition="5445">roblematic. 5.3.3 Machine Translation Classification We follow Och et al. (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. Unfortunately, we do not have access to the data used in those papers, so a direct comparison is not possible. Instead, we collected the English output of Moses (Hoang et al., 2007), using both French and German as source language, trained on the Europarl corpus used by WMT 2009.10 We also collected the output of Joshua (Li et al., 2009) trained on 500K sentences of GALE Chinese-English parallel newswire. We trained both our TREELET model and a 5-GRAM model on the union of our 1B corpus and the English sides of our parallel corpora. In Table 6, we show the pairwise comparison accuracy (using SLR) on these three corpora. We see that our system prefers the reference much more often than the 5-GRAM language model.11 However, we also note that the easiness of the task is correlated with the quality of translations (as measured in BLEU score). This is not surprising – high-quality translations are often grammatical and even a per1</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an open source toolkit for parsingbased machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="12579" citStr="Marcus et al., 1993" startWordPosition="2143" endWordPosition="2146">timating the probabilities in our model can be done very simply using the same techniques (in fact, the same code) used to estimate n-gram language models. Our model requires estimates of four distributions: p(Cd1|P, P0, r0), p(w|P, R, r0, w−1, w−2), p(Ci|Ci−1 i−n+1, P), and p(Ci|Ci−1 i−n+1). In each case, we require empirical counts of treelet tuples in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments th</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistic.</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Proceedings of the North American Association for Computational Linguistic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudo-negative samples.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4893" citStr="Okanohara and Tsujii, 2007" startWordPosition="782" endWordPosition="785">tions in our treelet model. For this context, P=VP-VBDˆS, P&apos;=S-VBDˆROOT, and r&apos;=S-VBD&amp;quot;ROOT→NP-NN VP-VBDˆS . (c) The context used for terminal productions h y in our treelet model. Here, P=VBD, R=CD-DC, r&apos;=VP-VBDˆS→VBD CD-DC NNTP, w_1=index, and w_2=The. Note that the TLET tree is a modified version of a standard Penn Treebank parse – see Section 3 for details. S-VBDˆROOT The fell index The index fell 10 .85 Monday (a) The index fell 10 .85 Monday . (b) ROOT S-VBDˆROOT (c) ROOT NP-NN VP-VBDˆS . NP-NN VP-VBDˆS . DT-the NN VBD CD-DC NNTP DT-the NN VBD CD-DC NNTP . . 10 .85 Monday the literature (Okanohara and Tsujii, 2007; Foster et He may enable drops to take , but wil hardly re al., 2008; Chrry and Quirk, 2008) and sow that The centers are losses f meals , and the runs ar it consistently outperforms an n-gram model as well Table 1: The first four samples of length between 15 a as other head-driven and tree-driven generative baselines. Our model even competes with state-of-the-art rul cnxt uld ned its wn ta in th grm discriminative classifiers specifically designed for r) nd xsie prig ld be rde each task, despite being estimated on positive data I pracic, hver, lgg dls a alone. We also show fluency improvemen</context>
<context position="20324" citStr="Okanohara and Tsujii (2007)" startWordPosition="3467" endWordPosition="3470">rated, or pseudonegative,5 English. We report machine translation reranking results in Section 5.4. 5.1 Generating Samples Because our model is generative, we can qualitatively assess it by generating samples and verifying that they are more syntactically coherent than other approaches. In Table 1, we show the first four samples of length between 15 and 20 generated from our model and a 5-gram model trained on the Penn Treebank. 4We found that using the 1-best worked just as well as the 1000-best on our grammaticality tasks, but significantly overestimated our model’s perplexities. 5We follow Okanohara and Tsujii (2007) in using the term pseudo-negative to highlight the fact that automatically generated negative examples might not actually be ungrammatical. 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some heldout set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the WSJ and Brown portions of the Penn Treebank, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English Gigaword version 3 (Graff, </context>
<context position="23483" citStr="Okanohara and Tsujii (2007)" startWordPosition="3982" endWordPosition="3985">se the following strategy: after the first 10000 sentences, whenever we see a new word in our training data, we replace it with a signature7 10% of the time. Our model outperforms all other generative models, though the improvement over the n-gram model is not statistically significant. Note that because we use a k-best approximation for the sum over trees, all perplexities (except for PCFG-LA and 5-GRAM) are pessimistic bounds. 5.3 Classification of Pseudo-Negative Sentences We make use of three kinds of automatically generated pseudo-negative sentences previously proposed in the literature: Okanohara and Tsujii (2007) proposed generating pseudo-negative examples from a trigram language model; Foster et al. (2008) create “noisy” sentences by automatically inserting a single error into grammatical sentences with a script that randomly deletes, inserts, or misspells a word; and Och et al. (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. Examples of these three types of pseudonegative data are shown in Table 3. We evaluate our model’s ability to distinguish positive from pseudonegative data, and compare against generative baselines and state-of-the-art discriminati</context>
</contexts>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discriminative language model with pseudo-negative samples. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1324" citStr="Pauls and Klein, 2011" startWordPosition="194" endWordPosition="197">lexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. 1 Introduction N-gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011</context>
<context position="22715" citStr="Pauls and Klein, 2011" startWordPosition="3864" endWordPosition="3867">es each modifier head and symbol conditioned only on Ch, h, and P, we additionally condition on the previously generated modifier’s head and symbol and back off to Model 1. 963 Model Perplexity PCFG 1772 TREELET-TRANS 722 TREELET-RULE 329 TREELET 198† PCFG-LA 330** HEADLEX 299 5-GRAM 207† Table 2: Perplexity of several generative models on Section 0 of the WSJ. The differences between scores marked with † are not statistically significant. PCFG-LA (marked with **) was only trained on the WSJ and Brown corpora because it does not scale to large amounts of data. We used the Berkeley LM toolkit (Pauls and Klein, 2011), which implements Kneser-Ney smoothing, to estimate all back-off models for both n-gram and treelet models. To deal with unknown words, we use the following strategy: after the first 10000 sentences, whenever we see a new word in our training data, we replace it with a signature7 10% of the time. Our model outperforms all other generative models, though the improvement over the n-gram model is not statistically significant. Note that because we use a k-best approximation for the sum over trees, all perplexities (except for PCFG-LA and 5-GRAM) are pessimistic bounds. 5.3 Classification of Pseu</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL</booktitle>
<contexts>
<context position="12931" citStr="Petrov et al., 2006" startWordPosition="2208" endWordPosition="2211">s in the same way that we require counts of word tuples for estimating n-gram language models. There is one additional hurdle in the estimation of our model: while there exist corpora with humanannotated constituency parses like the Penn Treebank (Marcus et al., 1993), these corpora are quite small – on the order of millions of tokens – and we cannot gather nearly as many counts as we can for ngrams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. However, we can use one of several high-quality constituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experiments that the addition of data with automatic parses does improve the performance of our language models across a range of tasks. 3 Tree Transformations In the previous section, we described how to condition on rich parse context to better capture the distribution of English trees. While such context allows our model to capture many interesting dependencies</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLINGACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Language modeling with tree substitution grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2863" citStr="Post and Gildea, 2009" startWordPosition="453" endWordPosition="456">, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which compares favorably to the more involved training algorithm of Tan et al. (2011), who use a two-pass EM training algorithm that takes several days on several hundred CPUs using similar amounts of data. The simplicity of our approach also contrasts with recent work on language modeling with tree substitution grammars (Post and Gildea, 2009), where larger treelet contexts are incorporated by using sophisticated priors to learn a segmentation of parse trees. Such an approach implicitly assumes that a “correct” segmentation exists, but it is not clear that this is true in practice. Instead, we build upon the success of n-gram language models, which do not assume a segmentation and instead score all overlapping contexts. We evaluate our model in terms of perplexity, and show that we achieve the same performance as a state-of-the-art n-gram model. We also evaluate our model on several grammaticality tasks proposed in 1We borrow the t</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. In Proceedings of the Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging grammaticality with tree substitution grammar.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Short Paper Track.</booktitle>
<contexts>
<context position="20855" citStr="Post (2011)" startWordPosition="3551" endWordPosition="3552">y overestimated our model’s perplexities. 5We follow Okanohara and Tsujii (2007) in using the term pseudo-negative to highlight the fact that automatically generated negative examples might not actually be ungrammatical. 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some heldout set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the WSJ and Brown portions of the Penn Treebank, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English Gigaword version 3 (Graff, 2003), totaling about 1.3 billion tokens. We used the humanannotated parses for the sentences in the Penn Treebank, but parsed the Gigaword and BLLIP sentences with the Berkeley Parser. Hereafter, we refer to this training data as our 1B corpus. We used Section 0 of the WSJ as our test corpus. Results are shown in Table 2. In addition to our TREELET model, we also show results for the following baselines: 5-GRAM A 5-gram interpolated Kneser-Ney model. PCFG-LA The Berkeley Parser in language model mode. HEADLEX A head-lexicali</context>
<context position="24860" citStr="Post, 2011" startWordPosition="4194" endWordPosition="4195">ixes. For example, the word vexing would be replaced with the signature UNK-ing. Noisy There was were many contributors. Trigram For years in dealer immediately. MT we must further steps. Table 3: Sample pseudo-negative sentences. We would like to use our model to make grammaticality judgements, but as a generative model it can only provide us with probabilities. Simply thresholding generative probabilities, even with a separate threshold for each length, has been shown to be very ineffective for grammaticality judgements, both for n-gram and syntactic language models (Cherry and Quirk, 2008; Post, 2011). We used a simple measure for isolating the syntactic likelihood of a sentence: we take the log-probability under our model and subtract the log-probability under a unigram model, then normalize by the length of the sentence.8 This measure, which we call the syntactic log-odds ratio (SLR), is a crude way of “subtracting out” the semantic component of the generative probability, so that sentences that use rare words are not penalized for doing so. 5.3.1 Trigram Classification To facilitate comparison with previous work, we used the same negative corpora as Post (2011) for trigram classificatio</context>
<context position="26102" citStr="Post (2011)" startWordPosition="4398" endWordPosition="4399">rain, 3K development, and 3K positive test sentences from the BLLIP corpus, then trained a trigram model on 450K BLLIP sentences and generated 50K train, 3K development, and 3K negative sentences. We parsed the 50K positive training examples of Post (2011) with the Berkeley Parser and used the resulting treebank to train a treelet language model. We set an SLR threshold for each model on the 6K positive and negative development sentences. Results are shown in Table 4. In addition to our generative baselines, we show results for the discriminative models reported in Cherry and Quirk (2008) and Post (2011). The former train a latent PCFG support vector machine for binary classification (LSVM). The latter report results for two binary classifiers: RERANK uses the reranking features of Charniak and Johnson (2005), and TSG uses 8Och et al. (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. We assume this is either because the lengthnormalization is important, or because their choice of syntactic language model was poor. 964 Generative BLLIP 1B PCFG 81.5 81.8 TREELET-TRANS 87.7 90.1 TREELET-RULE 89.8 94.1 TREELET 88.9</context>
<context position="28073" citStr="Post (2011)" startWordPosition="4722" endWordPosition="4723">it. Interestingly, the TREELETRULE baseline, which removes lexical context from our model, outperforms the full model. This is likely because the negative data is largely coherent at the trigram level (because it was generated from a trigram model), and the full model is much more sensitive to trigram coherence than the TREELET-RULE model. This also explains the poor performance of the 5-GRAM model. We emphasize that the discriminative baselines are specifically trained to separate trigram text from natural English, while our model is trained on positive examples alone. Indeed, the methods in Post (2011) are simple binary classifiers, and it is not clear that these models would be properly calibrated for any other task, such as integration in a decoder. One of the design goals of our system was that it be scalable. Unlike some of the discriminative baselines, which require expensive operations9 on 9It is true that in order train our system, one must parse large amounts of training data, which can be costly, though it only needs to be done once. In contrast, even with observed training trees, the discriminative algorithms must still iteratively perform expensive operations (like parsing) for e</context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging grammaticality with tree substitution grammar. In Proceedings of the Association for Computational Linguistics: Short Paper Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="3499" citStr="Quirk et al. (2005)" startWordPosition="557" endWordPosition="560">eelet contexts are incorporated by using sophisticated priors to learn a segmentation of parse trees. Such an approach implicitly assumes that a “correct” segmentation exists, but it is not clear that this is true in practice. Instead, we build upon the success of n-gram language models, which do not assume a segmentation and instead score all overlapping contexts. We evaluate our model in terms of perplexity, and show that we achieve the same performance as a state-of-the-art n-gram model. We also evaluate our model on several grammaticality tasks proposed in 1We borrow the term treelet from Quirk et al. (2005), who use it to refer to an arbitrary connected subgraph of a tree. 959 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959–968, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Conditioning contexts and back-off strategies for Markov mdls. The bolded symbol indicates the part of the he board ’s will soon be feasible from everyday which Coke ’s abinet hotels tree/sentence being generated, and the dotted ines represent the conditioning contexts; back-off proceeds from the largest to the They are al</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="1786" citStr="Roark, 2004" startWordPosition="268" endWordPosition="269">ne translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The sim</context>
<context position="9703" citStr="Roark, 2004" startWordPosition="1653" endWordPosition="1654">ction. Fintnig th P Tbk th Bow C nally, the generation of words at preterminals can th 50K BLLIP ti e f Pt condition on siblings, allowing the model to capture, (2011) d th AFP d APW pti f Englih for example, verb subcategorization frames. We should be clear that we are not the first We follow Okanohara and Tsuji (2007) i using the term ated negativ examples might n s(T ) is the terminal yield of T . � p(T) = p(Cd 1|h)n RAM r�T 960 to use back-off-based smoothing for syntactic language modeling – such techniques have been applied to models that condition on head-word contexts (Charniak, 2001; Roark, 2004; Zhang, 2009). Parent rule context has also been employed in translation (Vaswani et al., 2011). However, to our knowledge, we are the first to apply these techniques for language modeling on large amounts of data. 2.1 Lexical context Although it is tempting to think that we can replace the left-to-right generation of n-gram models with the purely top-down generation of typical PCFGs, in practice, words are often highly predictive of the words that follow them – indeed, n-gram models would be terrible language models if this were not the case. To capture linear effects, we extend the context </context>
</contexts>
<marker>Roark, 2004</marker>
<rawString>Brian Roark. 2004. Probabilistic top-down parsing and language modeling. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Tan</author>
<author>Wenli Zhou</author>
<author>Lei Zheng</author>
<author>Shaojun Wang</author>
</authors>
<title>A large scale distributed syntactic, semantic and lexical language model for machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1925" citStr="Tan et al., 2011" startWordPosition="289" endWordPosition="292"> and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure allows us to train a model on a billion tokens of data in a matter of hours on a single machine, which co</context>
</contexts>
<marker>Tan, Zhou, Zheng, Wang, 2011</marker>
<rawString>Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang. 2011. A large scale distributed syntactic, semantic and lexical language model for machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule markov models for fast tree-tostring translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computations Linguistics.</booktitle>
<contexts>
<context position="9799" citStr="Vaswani et al., 2011" startWordPosition="1667" endWordPosition="1670"> 50K BLLIP ti e f Pt condition on siblings, allowing the model to capture, (2011) d th AFP d APW pti f Englih for example, verb subcategorization frames. We should be clear that we are not the first We follow Okanohara and Tsuji (2007) i using the term ated negativ examples might n s(T ) is the terminal yield of T . � p(T) = p(Cd 1|h)n RAM r�T 960 to use back-off-based smoothing for syntactic language modeling – such techniques have been applied to models that condition on head-word contexts (Charniak, 2001; Roark, 2004; Zhang, 2009). Parent rule context has also been employed in translation (Vaswani et al., 2011). However, to our knowledge, we are the first to apply these techniques for language modeling on large amounts of data. 2.1 Lexical context Although it is tempting to think that we can replace the left-to-right generation of n-gram models with the purely top-down generation of typical PCFGs, in practice, words are often highly predictive of the words that follow them – indeed, n-gram models would be terrible language models if this were not the case. To capture linear effects, we extend the context for terminal (lexical) productions to include the previous two words w−2 and w−1 in the sentence</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule markov models for fast tree-tostring translation. In Proceedings of the Association for Computations Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Fred Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1744" citStr="Xu et al., 2002" startWordPosition="260" endWordPosition="263">component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), efficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n-gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although several lines of work have proposed generative syntactic language models that improve on n-gram models for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n-gram language models (Tan et al., 2011). In this paper, we describe a generative, syntactic language model that conditions on local context treelets1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained simply by collecting counts and using the same smoothing techniques normally applied to n-gram models (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n-gram models out of the box (Brants et </context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Fred Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
</authors>
<title>Structured language models for statistical machine translation.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="9717" citStr="Zhang, 2009" startWordPosition="1655" endWordPosition="1656">g th P Tbk th Bow C nally, the generation of words at preterminals can th 50K BLLIP ti e f Pt condition on siblings, allowing the model to capture, (2011) d th AFP d APW pti f Englih for example, verb subcategorization frames. We should be clear that we are not the first We follow Okanohara and Tsuji (2007) i using the term ated negativ examples might n s(T ) is the terminal yield of T . � p(T) = p(Cd 1|h)n RAM r�T 960 to use back-off-based smoothing for syntactic language modeling – such techniques have been applied to models that condition on head-word contexts (Charniak, 2001; Roark, 2004; Zhang, 2009). Parent rule context has also been employed in translation (Vaswani et al., 2011). However, to our knowledge, we are the first to apply these techniques for language modeling on large amounts of data. 2.1 Lexical context Although it is tempting to think that we can replace the left-to-right generation of n-gram models with the purely top-down generation of typical PCFGs, in practice, words are often highly predictive of the words that follow them – indeed, n-gram models would be terrible language models if this were not the case. To capture linear effects, we extend the context for terminal (</context>
</contexts>
<marker>Zhang, 2009</marker>
<rawString>Ying Zhang. 2009. Structured language models for statistical machine translation. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>