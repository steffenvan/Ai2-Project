<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000653">
<title confidence="0.9938625">
Look Who is Talking: Soundbite Speaker Name Recognition in
Broadcast News Speech
</title>
<author confidence="0.999525">
Feifan Liu, Yang Liu
</author>
<affiliation confidence="0.9970725">
Department of Computer Science
The University of Texas at Dallas, Richardson, TX
</affiliation>
<email confidence="0.998837">
{ffliu,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936058823529">
Speaker name recognition plays an important
role in many spoken language applications,
such as rich transcription, information extrac-
tion, question answering, and opinion mining.
In this paper, we developed an SVM-based
classification framework to determine the
speaker names for those included speech seg-
ments in broadcast news speech, called sound-
bites. We evaluated a variety of features with
different feature selection strategies. Experi-
ments on Mandarin broadcast news speech
show that using our proposed approach, the
soundbite speaker name recognition (SSNR)
accuracy is 68.9% on our blind test set, an ab-
solute 10% improvement compared to a base-
line system, which chooses the person name
closest to the soundbite.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99967745">
Broadcast news (BN) speech often contains speech or
interview quotations from specific speakers other than
reporters and anchors in a show. Identifying speaker
names for these speech segmentations, called soundbites
(Maskey and Hirschberg, 2006), is useful for many
speech processing applications, e.g., question answering,
opinion mining for a specific person. This has recently
received increasing attention in programs such as the
DARPA GALE program, where one query template is
about a person’s opinion or statement.
Previous work in this line includes speaker role de-
tection (e.g., Liu, 2006; Maskey and Hirschberg, 2006)
and speaker diarization (e.g., Canseco et al., 2005). In
this paper, we formulate the problem of SSNR as a tra-
ditional classification task, and proposed an SVM-based
identification framework to explore rich linguistic fea-
tures. Experiments on Mandarin BN speech have shown
that our proposed approach significantly outperforms
the baseline system, which chooses the closest name as
the speaker for a soundbite.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999756">
To our knowledge, no research has yet been conducted
on soundbite speaker name identification in Mandarin
BN domain. However, this work is related to some ex-
tent to speaker role identification, speaker diarization,
and named entity recognition.
Speaker role identification attempts to classify speech
segments based on the speakers’ role (anchor, reporter,
or others). Barzilay et al. (2000) used BoosTexter and
the maximum entropy model for this task in English BN
corpus, obtaining a classification accuracy of about 80%
compared to the chance of 35%. Liu (2006) combined a
generative HMM approach with the conditional maxi-
mum entropy method to detect speaker roles in Manda-
rin BN, reporting a classification accuracy of 81.97%
against the baseline of around 50%. In Maskey and
Hirschberg (2006), the task is to recognize soundbites
(which make up of a large portion of the “other” role
category in Liu (2006)). They achieved a recognition
accuracy of 67.4% in the English BN domain. Different
from their work, our goal is to identify the person who
spoke those soundbites, i.e., associate each soundbite
with a speaker name if any.
Speaker diarization in BN aims to find speaker
changes, group the same speakers together, and recog-
nize speaker names. It is an important component for
rich transcription (e.g., in the DARPA EARS program).
So far most work in this area has only focused on
speaker segmentation and clustering, and not included
name recognition. However, Canseco et al. (2005) were
able to successfully use linguistic information (e.g.,
related to person names) to improve performance of BN
speaker segmentation and clustering.
This work is also related to named entity recognition
(NER), especially person names. There has been a large
amount of research efforts on NER; however, instead of
</bodyText>
<page confidence="0.995644">
101
</page>
<note confidence="0.466213">
Proceedings of NAACL HLT 2007, Companion Volume, pages 101–104,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.995253">
recognizing all the names in a document, our task is to
find the speaker for a particular speech segment.
</bodyText>
<sectionHeader confidence="0.807641" genericHeader="method">
3 Framework for Soundbite Speaker
Name Recognition (SSNR)
</sectionHeader>
<bodyText confidence="0.999935">
Figure 1 shows our system diagram. SSNR is conducted
using the speech transcripts, assuming the soundbite
segments are provided. After running NER in the tran-
scripts, we obtain candidate person names. For a sound-
bite, we use the name hypotheses from the region both
before and after the soundbite. A ‘region’ is defined
based on the turn and topic segmentation information.
To determine which name among the candidates is the
corresponding speaker for the soundbite, we recast this
problem as a binary classification problem for every
candidate name and the soundbite, which we call an
instance. A positive tag for an instance means that the
name is the soundbite speaker. Each instance has an
associated feature vector, described further in the fol-
lowing section. Note that if a name occurs more than
once, only one instance is created for it.
</bodyText>
<figureCaption confidence="0.992374">
Figure 1. System diagram for SSNR.
</figureCaption>
<bodyText confidence="0.999906">
Any classification approach can be used in this gen-
eral framework for SSNR. We choose to use an SVM
classifier in our experiments because of its superior per-
formance in many classification tasks.
</bodyText>
<subsectionHeader confidence="0.974774">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.782769666666667">
The features that we have explored can be grouped into
three categories.
Positional Features (PF)
</bodyText>
<listItem confidence="0.985737666666667">
• PF-1: the position of the candidate name relative to
the soundbite. We hypothesize that names closer to
a soundbite are more likely to be the soundbite
</listItem>
<bodyText confidence="0.997695333333333">
speaker. This feature value can be ‘last’, ‘first’,
‘mid’, or ‘unique’. For example, ‘last’ for a candi-
date before a soundbite means that it is the closest
name among the hypotheses before the soundbite.
‘Unique’ indicates that the candidate is the only
person name in the region before or after the sound-
bite. Note that if a candidate name occurs more than
once, the PF-1 feature corresponds to the closest
name to the soundbite.
</bodyText>
<listItem confidence="0.99925175">
• PF-2: the position of a name in its sentence. Typi-
cally a name appearing earlier in a sentence (e.g., a
subject) is more likely to be quoted later.
• PF-3: an indicator feature to show where the name
</listItem>
<bodyText confidence="0.657314333333333">
has occurred, before, inside, or after the soundbite.
We added this because it is rare that a name inside a
soundbite is the speaker of that soundbite.
</bodyText>
<listItem confidence="0.938543333333333">
• PF-4: an indicator to denote if a candidate is in the
last sentence just before the soundbite turn, or is in
the first sentence just after the soundbite turn.
</listItem>
<sectionHeader confidence="0.576561" genericHeader="method">
Frequency Features (Freq)
</sectionHeader>
<bodyText confidence="0.99995725">
We hypothesize that a name with more occurrences
might be an important subject and thus more likely to be
the speaker of the soundbite, therefore we include the
frequency of a candidate name in the feature set.
</bodyText>
<subsectionHeader confidence="0.486098">
Lexical Features (LF)
</subsectionHeader>
<bodyText confidence="0.999972">
In order to capture the cue words around the soundbite
speaker names in the transcripts, we included unigram
features. For example, “pre_word+1=说/said” denotes
that the candidate name is followed by the word ‘说
/said’, and that ‘pre’ means this happens in the region
before the soundbite.
</bodyText>
<subsectionHeader confidence="0.999658">
3.2 Conflict Resolution
</subsectionHeader>
<bodyText confidence="0.999991142857143">
Another component in the system diagram that is worth
pointing out is ‘conflict resolution’. Since our approach
treats each candidate name as a separate classification
task, we need to post-process the cases where there are
multiple or no positive hypotheses for a soundbite dur-
ing testing. To resolve this situation, we choose the in-
stance with the best confidence value from the classifier.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.899924">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9959322">
We use the TDT4 Mandarin broadcast news data in our
experiment. The data set consists of about 170 hours
(336 shows) of news speech from different sources.
Speaker turns and soundbite segment information were
annotated manually in the transcripts. Our current study
</bodyText>
<figure confidence="0.9916273125">
Model Training
and Optimizing
Train Set
Training Testing
Preprocessing (word
segmentation, NER)
Instance Creation
for SSNR
Feature Vector
Representation
Dev/Test Set
Conflict
Resolution
Trained
Model
Output
</figure>
<page confidence="0.986213">
102
</page>
<bodyText confidence="0.999929222222222">
only uses the soundbites that have a human-labeled
speaker name in the surrounding transcripts. There are
1292 such soundbites in our corpus. We put aside 1/10
of the data as the development set, another 1/10 as the
test set, and used the rest as our training set. All the
transcripts were automatically tagged with named enti-
ties using the NYU tagger (Ji and Grishman, 2005). For
the classifier, we used the libSVM toolkit (Chang and
Lin, 2001) and the RBF kernel in our experiments.
A reasonable baseline for SSNR is to choose the
closest person name before a soundbite as its speaker.
We will compare our system performance to this base-
line approach.
We used two performance metrics in our experi-
ments. First is the instance classification accuracy (CA)
for the candidate names in the framework of the binary
classification task. Second, we compute name recogni-
tion accuracy (RA) for the soundbites as follows:
</bodyText>
<subsubsectionHeader confidence="0.712924">
Soundbites with Correct Names
</subsubsectionHeader>
<subsectionHeader confidence="0.99225">
4.2 Effects of Different Manually Selected
Feature Subsets
</subsectionHeader>
<bodyText confidence="0.999415714285714">
We used 10-fold cross validation on the training set to
evaluate the effect of different features and also for pa-
rameter optimization. Table 1 shows the instance classi-
fication results. “PF, Freq, LF” are the features
described in Section 3.1. “LF-before” means the uni-
gram features before the soundbites. “All-before” de-
notes using all the features before the soundbites.
</bodyText>
<table confidence="0.999388916666667">
Feature Optimized Para. CA
Subsets (%)
C G
PF-1 0.125 2 83.48
+PF-2 2048 1.22e-4 85.62
+PF-3 2048 4.88e-4 85.79
+PF-4 2 0.5 86.18
+Freq 2 0.5 86.18
+LF-before 32 7.81e-3 88.44
+LF-after 8 0.0313 88.44
i.e., All features
All-before 8 0.0313 88.03
</table>
<tableCaption confidence="0.99471">
Table 1. Instance classification accuracy (CA) using
</tableCaption>
<bodyText confidence="0.956846">
different feature sets. C and G are the optimized pa-
rameters in the SVM model.
We notice that the system performance generally
improves with incrementally expended feature sets,
yielding an accuracy of 88.44% using all the features.
Some features seem not helpful to system performance,
such as “Freq” and “LF-after”. Using all the features
before the soundbites achieves comparable performance
to using all the features, indicating that the region before
a soundbite contributes more than that after it. This is
expected since the reporters typically have already men-
tioned the person’s name before a soundbite. In addition,
we evaluated some compound features using our current
feature definition, but adding those did not improve the
system performance.
</bodyText>
<subsectionHeader confidence="0.997584">
4.3 Automatic Feature Selection
</subsectionHeader>
<bodyText confidence="0.710125666666667">
We also performed automatic feature selection for the
SVM model based on the F-score criterion (Chen and
Lin, 2006). There are 6048 features in total in our sys-
tem. Figure 2 shows the classification performance in
the training set using different number of features via
automatic feature selection.
</bodyText>
<figureCaption confidence="0.9983005">
Figure 2. Instance classification accuracy (CA) using F-
score based feature selection.
</figureCaption>
<bodyText confidence="0.9968795">
We can see that automatic feature selection further im-
proves the classification performance (2.36% higher
accuracy than that in Table 1). Table 2 lists some of the
top features based on their F-scores. Consistent with our
expectation, we observe that position related features, as
well as cue words, are good indicators for SSNR.
</bodyText>
<table confidence="0.999302538461538">
Feature F-score
Justbeforeturn (PF-4) 0.3543
pre_contextpos=last (PF-1) 0.2857
pre_senpos=unique (PF-2) 0.0631
pre_word+1=“上午/morning” (LF) 0.0475
pre_word+1= “说/said” (LF) 0.0399
bool_pre=1 (PF-3) 0.0353
Justafterturn (PF-4) 0.0349
pre_contextpos=mid (PF-1) 0.0329
post_contextpos=first (PF-1) 0.0323
pre_word+1= “今X/today” (LF) 0.0288
pre_word-1=“iM/reporter” (LF) 0.0251
pre_word+1=“表T/express” (LF) 0.0246
</table>
<tableCaption confidence="0.999801">
Table 2. Top features ordered by F-score values.
</tableCaption>
<page confidence="0.939219444444444">
6048
3024
1839
1512
756
378
189
94
47
</page>
<figure confidence="0.996569047619048">
# of features
CA(%)
92
91
90
89
88
87
86
88.44
88.61
90.14
90.79
88.73
88.61
88.44
88.12
87.1
RA
# of Soundbites in Files
# of
</figure>
<page confidence="0.992115">
103
</page>
<subsectionHeader confidence="0.992013">
4.4 Performance on Development Set
</subsectionHeader>
<bodyText confidence="0.999885285714286">
Up to now our focus has been on feature selection based
on instance classification accuracy. Since our ultimate
goal is to identify soundbite speaker names, we chose
several promising configurations based on the results
above to apply to the development set and evaluate the
soundbite name recognition accuracy. Results using the
two metrics are presented in Table 3.
</bodyText>
<table confidence="0.999801444444444">
Feature Set CA (%) RA (%)
Baseline 84.0 59.3
PF 86.7 54.2
PF+Freq 86.7 60.4
PF+Freq+LF-before 87.8 63.5
PF+Freq+LF-before 88.3 67.7
+LF-after (ALL)
Top 1512 by f-score 85.6 62.5
Top 1839 by f-score 85.4 60.4
</table>
<tableCaption confidence="0.99519">
Table 3. Results on the dev set using two metrics: in-
</tableCaption>
<bodyText confidence="0.951805741935484">
stance classification accuracy (CA), and soundbite name
recognition accuracy (RA). The oracle RA is 79.1%.
Table 3 shows that using all the features (ALL)
performs the best, yielding an improvement of 4.3% and
8.4% compared to the baseline in term of the CA and RA
respectively. However, using the automatically selected
feature sets (the last two rows in Table 3) only slightly
outperforms the baseline. This suggests that the F-score
based feature selection strategy on the training set may
not generalize well. Interestingly, “Freq” and “LF-after”
features show some useful contribution (the 4th and 6th
row in Table 3) respectively on the development set,
different from the results on the training set using 10-
fold cross validation. The results using the two metrics
also show that they are not always correlated.
Because of the possible NER errors, we also meas-
ure the oracle RA, defined as the percent of the sound-
bites for which the correct speaker name (based on NER)
appears in the region surrounding the soundbite. The
oracle RA on this data set is 79.1%. We also notice that
8.3% of the soundbites do not have the correct name
hypothesis due to an NER boundary error, and that
12.5% is because of missing errors.
We used the method as described in Section 3.2 to
resolve conflicts for the results shown in Table 3. In
addition, we evaluated another approach—we resort to
the baseline (i.e., chose the name that is closest to the
soundbite) for those soundbites that have multiple or no
positive hypothesis. Our experiments on the develop-
ment set showed this approach degrades system per-
formance (e.g., RA of around 61% using all the features).
</bodyText>
<subsectionHeader confidence="0.798282">
4.5 Results on Blind Test Set
</subsectionHeader>
<bodyText confidence="0.999880333333333">
Finally, we applied the all-feature configuration to our
blind test data and obtained the results as shown in Ta-
ble 4. Using all the features significantly outperforms
the baseline. The gain is slightly better than that on the
development set, although the oracle accuracy is also
higher on the test set.
</bodyText>
<table confidence="0.998441">
CA (%) RA (oracle: 85.8%)
Baseline 81.3 58.4
All feature 85.1 68.9
</table>
<tableCaption confidence="0.999727">
Table 4. Results on the test set.
</tableCaption>
<sectionHeader confidence="0.997618" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977">
We proposed an SVM-based approach for soundbite
speaker name recognition and examined various linguis-
tic features. Experiments in Mandarin BN corpus show
that our approach yields an identification accuracy of
68.9%, significantly better than 58.4% from the baseline.
Our future work will focus on exploring more useful
features, such as part-of-speech and semantic features.
In addition, we plan to test this framework using auto-
matic speech recognition output, speaker segmentation,
and soundbite segment detection.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999891714285714">
We thank Sameer Maskey, Julia Hirschberg, and Mari
Ostendorf for useful discussions, and Heng Ji for shar-
ing the Mandarin named entity tagger. This work is
supported by DARPA under Contract No. HR0011-06-
C-0023. Any opinions expressed in this material are
those of the authors and do not necessarily reflect the
views of DARPA.
</bodyText>
<sectionHeader confidence="0.999238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999819809523809">
S. Maskey and J. Hirschberg. 2006. Soundbite Detec-
tion in Broadcast News Domain. In Proc. of INTER-
SPEECH2006. pp: 1543-1546.
Y. Liu. 2006. Initial Study on Automatic Identification
of Speaker Role in Broadcast News Speech. In Proc.
of HLT-NAACL. pp: 81-84.
R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker.
2000. The Rules Behind Roles: Identifying Speaker
Role in Radio Broadcasts. In Proc. of AAAI.
L. Canseco, L. Lamel, and J.-L. Gauvain. 2005. A Com-
parative Study Using Manual and Automatic Tran-
scriptions for Diarization. In Proc. of ASRU.
H. Ji and R. Grishman. 2005. Improving Name Tagging
by Reference Resolution and Relation Detection. In
Proc. of ACL. pp: 411-418.
Y.-W. Chen and C.-J. Lin. 2006. Combining SVMs with
Various Feature Selection Strategies. Feature Extrac-
tion, Foundations and Applications, Springer.
C. Chang and C. Lin. 2001. LIBSVM: A Library for
Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
</reference>
<page confidence="0.998784">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936976">
<title confidence="0.9859745">Look Who is Talking: Soundbite Speaker Name Recognition Broadcast News Speech</title>
<author confidence="0.997189">Feifan Liu</author>
<author confidence="0.997189">Yang Liu</author>
<affiliation confidence="0.9977485">Department of Computer Science The University of Texas at Dallas, Richardson, TX</affiliation>
<email confidence="0.998929">ffliu@hlt.utdallas.edu</email>
<email confidence="0.998929">yangl@hlt.utdallas.edu</email>
<abstract confidence="0.998290888888889">Speaker name recognition plays an important role in many spoken language applications, such as rich transcription, information extraction, question answering, and opinion mining. In this paper, we developed an SVM-based classification framework to determine the speaker names for those included speech segments in broadcast news speech, called soundbites. We evaluated a variety of features with different feature selection strategies. Experiments on Mandarin broadcast news speech show that using our proposed approach, the soundbite speaker name recognition (SSNR) accuracy is 68.9% on our blind test set, an absolute 10% improvement compared to a baseline system, which chooses the person name closest to the soundbite.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Soundbite Detection in Broadcast News Domain.</title>
<date>2006</date>
<booktitle>In Proc. of INTERSPEECH2006.</booktitle>
<pages>1543--1546</pages>
<contexts>
<context position="1206" citStr="Maskey and Hirschberg, 2006" startWordPosition="171" endWordPosition="174"> We evaluated a variety of features with different feature selection strategies. Experiments on Mandarin broadcast news speech show that using our proposed approach, the soundbite speaker name recognition (SSNR) accuracy is 68.9% on our blind test set, an absolute 10% improvement compared to a baseline system, which chooses the person name closest to the soundbite. 1 Introduction Broadcast news (BN) speech often contains speech or interview quotations from specific speakers other than reporters and anchors in a show. Identifying speaker names for these speech segmentations, called soundbites (Maskey and Hirschberg, 2006), is useful for many speech processing applications, e.g., question answering, opinion mining for a specific person. This has recently received increasing attention in programs such as the DARPA GALE program, where one query template is about a person’s opinion or statement. Previous work in this line includes speaker role detection (e.g., Liu, 2006; Maskey and Hirschberg, 2006) and speaker diarization (e.g., Canseco et al., 2005). In this paper, we formulate the problem of SSNR as a traditional classification task, and proposed an SVM-based identification framework to explore rich linguistic </context>
<context position="2810" citStr="Maskey and Hirschberg (2006)" startWordPosition="418" endWordPosition="421">ker role identification, speaker diarization, and named entity recognition. Speaker role identification attempts to classify speech segments based on the speakers’ role (anchor, reporter, or others). Barzilay et al. (2000) used BoosTexter and the maximum entropy model for this task in English BN corpus, obtaining a classification accuracy of about 80% compared to the chance of 35%. Liu (2006) combined a generative HMM approach with the conditional maximum entropy method to detect speaker roles in Mandarin BN, reporting a classification accuracy of 81.97% against the baseline of around 50%. In Maskey and Hirschberg (2006), the task is to recognize soundbites (which make up of a large portion of the “other” role category in Liu (2006)). They achieved a recognition accuracy of 67.4% in the English BN domain. Different from their work, our goal is to identify the person who spoke those soundbites, i.e., associate each soundbite with a speaker name if any. Speaker diarization in BN aims to find speaker changes, group the same speakers together, and recognize speaker names. It is an important component for rich transcription (e.g., in the DARPA EARS program). So far most work in this area has only focused on speake</context>
</contexts>
<marker>Maskey, Hirschberg, 2006</marker>
<rawString>S. Maskey and J. Hirschberg. 2006. Soundbite Detection in Broadcast News Domain. In Proc. of INTERSPEECH2006. pp: 1543-1546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
</authors>
<title>Initial Study on Automatic Identification of Speaker Role in Broadcast News Speech.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<pages>81--84</pages>
<contexts>
<context position="1557" citStr="Liu, 2006" startWordPosition="227" endWordPosition="228">Introduction Broadcast news (BN) speech often contains speech or interview quotations from specific speakers other than reporters and anchors in a show. Identifying speaker names for these speech segmentations, called soundbites (Maskey and Hirschberg, 2006), is useful for many speech processing applications, e.g., question answering, opinion mining for a specific person. This has recently received increasing attention in programs such as the DARPA GALE program, where one query template is about a person’s opinion or statement. Previous work in this line includes speaker role detection (e.g., Liu, 2006; Maskey and Hirschberg, 2006) and speaker diarization (e.g., Canseco et al., 2005). In this paper, we formulate the problem of SSNR as a traditional classification task, and proposed an SVM-based identification framework to explore rich linguistic features. Experiments on Mandarin BN speech have shown that our proposed approach significantly outperforms the baseline system, which chooses the closest name as the speaker for a soundbite. 2 Related Work To our knowledge, no research has yet been conducted on soundbite speaker name identification in Mandarin BN domain. However, this work is relat</context>
<context position="2924" citStr="Liu (2006)" startWordPosition="441" endWordPosition="442">segments based on the speakers’ role (anchor, reporter, or others). Barzilay et al. (2000) used BoosTexter and the maximum entropy model for this task in English BN corpus, obtaining a classification accuracy of about 80% compared to the chance of 35%. Liu (2006) combined a generative HMM approach with the conditional maximum entropy method to detect speaker roles in Mandarin BN, reporting a classification accuracy of 81.97% against the baseline of around 50%. In Maskey and Hirschberg (2006), the task is to recognize soundbites (which make up of a large portion of the “other” role category in Liu (2006)). They achieved a recognition accuracy of 67.4% in the English BN domain. Different from their work, our goal is to identify the person who spoke those soundbites, i.e., associate each soundbite with a speaker name if any. Speaker diarization in BN aims to find speaker changes, group the same speakers together, and recognize speaker names. It is an important component for rich transcription (e.g., in the DARPA EARS program). So far most work in this area has only focused on speaker segmentation and clustering, and not included name recognition. However, Canseco et al. (2005) were able to succ</context>
</contexts>
<marker>Liu, 2006</marker>
<rawString>Y. Liu. 2006. Initial Study on Automatic Identification of Speaker Role in Broadcast News Speech. In Proc. of HLT-NAACL. pp: 81-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Collins</author>
<author>J Hirschberg</author>
<author>S Whittaker</author>
</authors>
<title>The Rules Behind Roles: Identifying Speaker Role in Radio Broadcasts.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="2404" citStr="Barzilay et al. (2000)" startWordPosition="352" endWordPosition="355">explore rich linguistic features. Experiments on Mandarin BN speech have shown that our proposed approach significantly outperforms the baseline system, which chooses the closest name as the speaker for a soundbite. 2 Related Work To our knowledge, no research has yet been conducted on soundbite speaker name identification in Mandarin BN domain. However, this work is related to some extent to speaker role identification, speaker diarization, and named entity recognition. Speaker role identification attempts to classify speech segments based on the speakers’ role (anchor, reporter, or others). Barzilay et al. (2000) used BoosTexter and the maximum entropy model for this task in English BN corpus, obtaining a classification accuracy of about 80% compared to the chance of 35%. Liu (2006) combined a generative HMM approach with the conditional maximum entropy method to detect speaker roles in Mandarin BN, reporting a classification accuracy of 81.97% against the baseline of around 50%. In Maskey and Hirschberg (2006), the task is to recognize soundbites (which make up of a large portion of the “other” role category in Liu (2006)). They achieved a recognition accuracy of 67.4% in the English BN domain. Diffe</context>
</contexts>
<marker>Barzilay, Collins, Hirschberg, Whittaker, 2000</marker>
<rawString>R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 2000. The Rules Behind Roles: Identifying Speaker Role in Radio Broadcasts. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Canseco</author>
<author>L Lamel</author>
<author>J-L Gauvain</author>
</authors>
<title>A Comparative Study Using Manual and Automatic Transcriptions for Diarization.</title>
<date>2005</date>
<booktitle>In Proc. of ASRU.</booktitle>
<contexts>
<context position="1640" citStr="Canseco et al., 2005" startWordPosition="237" endWordPosition="240">iew quotations from specific speakers other than reporters and anchors in a show. Identifying speaker names for these speech segmentations, called soundbites (Maskey and Hirschberg, 2006), is useful for many speech processing applications, e.g., question answering, opinion mining for a specific person. This has recently received increasing attention in programs such as the DARPA GALE program, where one query template is about a person’s opinion or statement. Previous work in this line includes speaker role detection (e.g., Liu, 2006; Maskey and Hirschberg, 2006) and speaker diarization (e.g., Canseco et al., 2005). In this paper, we formulate the problem of SSNR as a traditional classification task, and proposed an SVM-based identification framework to explore rich linguistic features. Experiments on Mandarin BN speech have shown that our proposed approach significantly outperforms the baseline system, which chooses the closest name as the speaker for a soundbite. 2 Related Work To our knowledge, no research has yet been conducted on soundbite speaker name identification in Mandarin BN domain. However, this work is related to some extent to speaker role identification, speaker diarization, and named en</context>
<context position="3506" citStr="Canseco et al. (2005)" startWordPosition="534" endWordPosition="537">the “other” role category in Liu (2006)). They achieved a recognition accuracy of 67.4% in the English BN domain. Different from their work, our goal is to identify the person who spoke those soundbites, i.e., associate each soundbite with a speaker name if any. Speaker diarization in BN aims to find speaker changes, group the same speakers together, and recognize speaker names. It is an important component for rich transcription (e.g., in the DARPA EARS program). So far most work in this area has only focused on speaker segmentation and clustering, and not included name recognition. However, Canseco et al. (2005) were able to successfully use linguistic information (e.g., related to person names) to improve performance of BN speaker segmentation and clustering. This work is also related to named entity recognition (NER), especially person names. There has been a large amount of research efforts on NER; however, instead of 101 Proceedings of NAACL HLT 2007, Companion Volume, pages 101–104, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics recognizing all the names in a document, our task is to find the speaker for a particular speech segment. 3 Framework for Soundbite Speaker </context>
</contexts>
<marker>Canseco, Lamel, Gauvain, 2005</marker>
<rawString>L. Canseco, L. Lamel, and J.-L. Gauvain. 2005. A Comparative Study Using Manual and Automatic Transcriptions for Diarization. In Proc. of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Improving Name Tagging by Reference Resolution and Relation Detection.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<pages>411--418</pages>
<contexts>
<context position="8271" citStr="Ji and Grishman, 2005" startWordPosition="1315" endWordPosition="1318"> the transcripts. Our current study Model Training and Optimizing Train Set Training Testing Preprocessing (word segmentation, NER) Instance Creation for SSNR Feature Vector Representation Dev/Test Set Conflict Resolution Trained Model Output 102 only uses the soundbites that have a human-labeled speaker name in the surrounding transcripts. There are 1292 such soundbites in our corpus. We put aside 1/10 of the data as the development set, another 1/10 as the test set, and used the rest as our training set. All the transcripts were automatically tagged with named entities using the NYU tagger (Ji and Grishman, 2005). For the classifier, we used the libSVM toolkit (Chang and Lin, 2001) and the RBF kernel in our experiments. A reasonable baseline for SSNR is to choose the closest person name before a soundbite as its speaker. We will compare our system performance to this baseline approach. We used two performance metrics in our experiments. First is the instance classification accuracy (CA) for the candidate names in the framework of the binary classification task. Second, we compute name recognition accuracy (RA) for the soundbites as follows: Soundbites with Correct Names 4.2 Effects of Different Manual</context>
</contexts>
<marker>Ji, Grishman, 2005</marker>
<rawString>H. Ji and R. Grishman. 2005. Improving Name Tagging by Reference Resolution and Relation Detection. In Proc. of ACL. pp: 411-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Chen</author>
<author>C-J Lin</author>
</authors>
<title>Combining SVMs with Various Feature Selection Strategies. Feature Extraction, Foundations and Applications,</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="10475" citStr="Chen and Lin, 2006" startWordPosition="1664" endWordPosition="1667">, such as “Freq” and “LF-after”. Using all the features before the soundbites achieves comparable performance to using all the features, indicating that the region before a soundbite contributes more than that after it. This is expected since the reporters typically have already mentioned the person’s name before a soundbite. In addition, we evaluated some compound features using our current feature definition, but adding those did not improve the system performance. 4.3 Automatic Feature Selection We also performed automatic feature selection for the SVM model based on the F-score criterion (Chen and Lin, 2006). There are 6048 features in total in our system. Figure 2 shows the classification performance in the training set using different number of features via automatic feature selection. Figure 2. Instance classification accuracy (CA) using Fscore based feature selection. We can see that automatic feature selection further improves the classification performance (2.36% higher accuracy than that in Table 1). Table 2 lists some of the top features based on their F-scores. Consistent with our expectation, we observe that position related features, as well as cue words, are good indicators for SSNR. </context>
</contexts>
<marker>Chen, Lin, 2006</marker>
<rawString>Y.-W. Chen and C.-J. Lin. 2006. Combining SVMs with Various Feature Selection Strategies. Feature Extraction, Foundations and Applications, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chang</author>
<author>C Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="8341" citStr="Chang and Lin, 2001" startWordPosition="1327" endWordPosition="1330">Set Training Testing Preprocessing (word segmentation, NER) Instance Creation for SSNR Feature Vector Representation Dev/Test Set Conflict Resolution Trained Model Output 102 only uses the soundbites that have a human-labeled speaker name in the surrounding transcripts. There are 1292 such soundbites in our corpus. We put aside 1/10 of the data as the development set, another 1/10 as the test set, and used the rest as our training set. All the transcripts were automatically tagged with named entities using the NYU tagger (Ji and Grishman, 2005). For the classifier, we used the libSVM toolkit (Chang and Lin, 2001) and the RBF kernel in our experiments. A reasonable baseline for SSNR is to choose the closest person name before a soundbite as its speaker. We will compare our system performance to this baseline approach. We used two performance metrics in our experiments. First is the instance classification accuracy (CA) for the candidate names in the framework of the binary classification task. Second, we compute name recognition accuracy (RA) for the soundbites as follows: Soundbites with Correct Names 4.2 Effects of Different Manually Selected Feature Subsets We used 10-fold cross validation on the tr</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C. Chang and C. Lin. 2001. LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>