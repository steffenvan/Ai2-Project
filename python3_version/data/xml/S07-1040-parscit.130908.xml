<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006690">
<title confidence="0.996851333333333">
IRST-BP: Preposition Disambiguation
based on
Chain Clarifying Relationships Contexts
</title>
<author confidence="0.784912">
Octavian Popescu
</author>
<affiliation confidence="0.32302">
FBK-IRST, Trento (Italy)
</affiliation>
<email confidence="0.979541">
popescu@itc.it
</email>
<author confidence="0.914272">
Sara Tonelli
</author>
<affiliation confidence="0.748337">
FBK-IRST, Trento (Italy)
</affiliation>
<email confidence="0.991958">
satonelli@itc.it
</email>
<author confidence="0.735423">
Emanuele Pianta
</author>
<affiliation confidence="0.307262">
FBK-IRST, Trento (Italy)
</affiliation>
<email confidence="0.986851">
pianta@itc.it
</email>
<sectionHeader confidence="0.99356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888545454546">
We are going to present a technique of
preposition disambiguation based on
sense discriminative patterns, which are
acquired using a variant of Angluin’s al-
gorithm. They represent the essential in-
formation extracted from a particular
type of local contexts we call Chain
Clarifying Relationship contexts. The
data set and the results we present are
from the Semeval task, WSD of Preposi-
tion (Litkowski 2007).
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997917166666667">
Word Sense Disambiguation (WSD) is a prob-
lem of finding the relevant clues in a surround-
ing context. Context is used with a wide scope in
the NLP literature. However, there is a dichot-
omy among two types of contexts, local and
topical contexts (Leacock et. all 1993), that is
general enough to encompass the whole notion
and at the same to represent a relevant distinc-
tion.
The local context is formed by information on
word order, distance and syntactic structure and
it is not restricted to open-class words. A topical
context is formed by the list of those words that
are likely to co-occur with a particular sense of a
word. Generally, the WSD methods have a
marked predilection for topical context, with the
consequence that structural clues are rarely, if
ever, taken into account. However, it has been
suggested (Stetina&amp;Nagao 1997, Dekang 1997)
that structural words, especially prepositions and
particles, play an important role in computing
the lexical preferences considered to be the most
important clues for disambiguation.
Closed class words, prepositions in particular,
are ambiguous (Litkowski&amp;Hargraves2006).
Their disambiguation is essential for the correct
processing of the meaning of a whole phrase. A
wrong PP-attachment may render the sense of
the whole sentence unintelligible. Consider for
example:
</bodyText>
<listItem confidence="0.992828">
(1) Joe heard the gossip about you and me.
(2) Bob rowed about his old car and his
mother.
</listItem>
<bodyText confidence="0.999969461538462">
A probabilistic context free grammar most
likely will parse both (1) and (2) wrongly1. It
would attach “about” to “to hear” in (1) and
would consider the “his old car and his mother”
the object of “about” in (2).
The information needed for disambiguation of
open class words is spread at all linguistics lev-
els, from lexicon to pragmatics, and can be lo-
cated within all discourse levels, from immedi-
ate collocation to paragraphs (Stevenson&amp;Wilks
1999). Intuitively, prepositions have a different
behavior. Most likely, their senses are deter-
mined within the government category of their
</bodyText>
<footnote confidence="0.628017">
1 Indeed, Charniak’s parser, considered to be among
the most accurate ones for English, parses wrongly
both of them.
</footnote>
<page confidence="0.980389">
191
</page>
<bodyText confidence="0.985350238095238">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 191–194,
Prague, June 2007. c�2007 Association for Computational Linguistics
heads. We expect the local context to play the
most important role in the disambiguation of
prepositions.
We are going to present a technique of prepo-
sition disambiguation based on sense discrimina-
tive patterns, which are acquired using a variant
of Angluin’s algorithm. These patterns represent
the essential information extracted from a par-
ticular type of local contexts we call Chain
Clarifying Relationship contexts. The data set
and the results we present are from the Semeval
task, WSD of Preposition (Litkowski 2007).
In Section 2 we introduce the Chain Clarify-
ing Relationships, which represent particular
types of local contexts. In Section 3 we present
the main ideas of the Angluin algorithm. We
show in Section 4 how it can be adapted to ac-
commodate the preposition disambiguation task.
Section 5 is dedicated to further research.
</bodyText>
<sectionHeader confidence="0.896466" genericHeader="method">
2 Chain Clarifying Relationships
</sectionHeader>
<bodyText confidence="0.998164511627907">
We think of ambiguity of natural language as a
net - like relationship. Under certain circum-
stances, a string of words represents a unique
collection of senses. If a different sense for one
of these words is chosen, the result is an un-
grammatical sentence. Consider (3) below:
(3) Most people do not live in a state of
high intellectual awareness about their
every action.
Suppose one chooses the sense of “to live” to
be “to populate”. Then, its complement, “state”,
should be synonym with location. The analysis
crashes when “awareness” is considered. There
are two things we notice here: (a) the relation-
ship between “live” and “state” – the only two
acceptable sense combination out of four are
(populate, location) and (experience, entity) –
and (b) the chain like relationship between
“awareness”, “state”, “live” where the sense of
any of them determines the sense of all the oth-
ers in a cascade effect, or results in ungrammati-
cality. A third thing, not directly observable in
(3) is that the syntactic configuration is crucial in
order for (a) and (b) to arise. Example (4) shows
that in a different syntactic configuration the
above sense relationship simply disappears:
(4) The awareness of people about the state insti-
tutions is arguably the first condition to live
in a democratic state.
We call the relationship between “live”,
“state”, “awareness” a Chain Clarifying Rela-
tionship (CCR). In that specific syntactic con-
figuration their senses are interdependent and
independent of the rest of the sentence. To each
CCR corresponds a sense discriminative pattern.
Our goal is to learn which local contexts are
CCRs. Each CCR is a pattern of words on a syn-
tactic configuration. Each slot can be filled only
by words defined by certain lexical features. To
learn a CCR means to discover the syntactic
configuration and the respective features. For
example consider (5) and (6) with their CCRs in
(CCR5) and (CCR6) respectively:
</bodyText>
<listItem confidence="0.997829714285714">
(5) Some people lived in the same state of
disappointment/ optimism/ happiness.
(CCR5) (vb=live—sense—2, prep1=in—1,
prep1—obj=state—sense—1,prep2=of—sense—1
a,prep2—obj=[State—of—Spirit])
(6) Some people lived in the same state of
Africa/ Latin America/ Asia.
</listItem>
<bodyText confidence="0.991612083333333">
(CCR6) (vb=live—sense—1, prep1=in—1,
prep1—obj=state—sense—1,prep2=of—1b,prep
2—obj = [Location])
The lexical features of the open class words in
a specific syntactic configuration trigger the
senses of each word, if the context is a CCR. In
(CCR5) any word that has the same lexical trait
as the one required by prep2—obj slot will deter-
mine a unique sense for all the other words, in-
cluding the preposition. The same holds for
(CCR6). The difference between (CCR5) and
(CCR6) is part of the linguistic knowledge
(which can be clearly shown: “how” (5) vs.
“where” (6)).
The CCR approach proposes a deterministic
approach to WSD. There are two features of
CCRs which are interesting from a strictly prac-
tical point of view. Firstly, CCR proposal is a
way to determine the size of the window where
the disambiguation clues are searched for (many
WSD algorithms arbitrarily set it apriori). Sec-
ondly, within a CCR, by construction, the sense
of one word determines the senses of all the oth-
ers.
</bodyText>
<page confidence="0.996271">
192
</page>
<sectionHeader confidence="0.989859" genericHeader="method">
3 Angluin Learning Algorithm
</sectionHeader>
<bodyText confidence="0.99997136">
Our working hypothesis is that we can learn the
CCRs contexts by inferring differences via a
regular language learning algorithm. What we
want to learn is which features fulfil each syn-
tactic slot. First we introduce the original An-
gluin’s algorithm and then we mention a variant
of it admitting unspecified values.
Angluin proved that a regular set can be
learned in polynomial time by assuming the ex-
istence of an oracle which can gives “yes/no”
answers and counterexamples to two types of
queries: membership queries and conjecture que-
ries (queries about the form of the regular lan-
guage) (Angluin 1998).
The algorithm employs an observation table
built on prefix /suffix closed classes. To each
word a {1, 0} value is associated, “1” meaning
that the word belongs to the target regular lan-
guage. Initially the table is empty and is filled
incrementally. The table is closed if all prefixes
of the already seen examples are in the table and
is consistent if two rows dominated by the same
prefix have the same value, “0” or “1”.
If the table is not consistent or closed then a
set of membership queries is made. If the table is
consistent and closed then a conjecture query is
made. If the oracle responds “no”, it has to pro-
vide a counterexample and the previous steps are
cycled till “yes” is obtained.
The role of the oracle for conjecture questions
can be substituted by a stochastic process. If
strict equality is not requested, then a probably
approximately correct identification of language
can be obtained (PAC identification), which
guarantees that the two languages (the identified
one, Li, and the target one, Lt) are equal up to a
certain extent. The approximation is constrained
by two parameters c – accuracy and S – confi-
dence, and the constraint is P(d(Li, Lt) :5 c) &gt; S),
where the distance between two languages is the
probability to see a word in just one of them.
The algorithm can be further generalized to
work with unspecified values. The examples
may have three values (“yes”, “no”, “?”), as in
many domains one has to deal with partial
knowledge The main result is that a variant of
the above algorithm successfully halts if the
number of counterexamples provided by the ora-
cle have O(log n) missing attributes, where n is
the number of attributes (Goldmann et all 2003).
</bodyText>
<sectionHeader confidence="0.974105" genericHeader="method">
4 Preposition Disambiguation Task
</sectionHeader>
<bodyText confidence="0.999981444444445">
The CCR extraction algorithm is supervised.
Consider that you have a sense annotated cor-
pora. Extract the dependency paths and filter out
the ones which are not sense discriminative. Try
to generalize each slot and retain the minimal
ones. What is left are CCRs.
Unfortunately, for the preposition disam-
biguation task the training set is sense annotated
only for prepositions. We have undertaken a dif-
ferent strategy. The training corpus can be used
as an oracle. The main idea is to start with a set
of few examples for each sense from the training
set which are considered to be the most repre-
sentative ones. We try to generalize each of
them independently and to tackle down the bor-
der cases (the cases that may correspond to two
different senses) which are considered unspeci-
fied examples. The process stops when the ora-
cle does not bring any new information (the
training cases have been learned). Below we
explain this process step by step.
Step 1. Get the seed examples. For each
preposition and sense get the seed examples.
This operation is performed by a human expert.
It may be the case that the glosses or the diction-
ary definition are a good starting point (with the
advantage that the intervention of a human is no
more required). However, we preferred do to it
manually for better precision.
Besides the most frequent sense, we have con-
sidered, in average, another two senses. There is
a practical reason for this limitation: the number
of examples for the rest of the senses is insuffi-
cient. In total we have considered 149 senses out
of the 241 senses present in the training set. For
each an average of three examples has been cho-
sen.
Step 2. Get the CCRs. For each example we
read the lex units associated with its frame from
FrameNet. Our goal is to identify the relevant
syntactic and lexical features associated with
each slot. We have undertaken two simplifying
assumptions. Firstly, only the government cate-
gory of the head of the PP is considered (which
can be a verb, a noun or an adjective). Secondly,
</bodyText>
<page confidence="0.998221">
193
</page>
<bodyText confidence="0.99643575">
the lexical features are identified with synsets
from WordNet.
We have used the Charniak’s parser to extract
the structure of the PP-phrases and further we
have used Collin’s algorithm to implement a
head recogniser.
A head can have many synsets. In order to
understand which sense the word has in the re-
spective construction we look for the synset
common to the elements extracted from lex. If
the proposed synset uniquely identifies just one
sense then it is considered a CCR. If not, we are
looking for the next synset. This step corre-
sponds to membership queries in Angluin’s al-
gorithm.
Step 3. Generalize the CCRs. At the end of
step 2 we have a set of CCRs for each sense. We
obtained 395 initial CCRs. We tried to extend
the coverage by taking into account the hypero-
nyms of each synsets. Only approximately 10%
of these new patterns have received an answer
from the oracle. Consequently, for our ap-
proach ,a part of the training corpus has not been
used. It serves only 15 examples in average to
get a correct CCR. All the instances of the same
CCR do not bring any new information to our
approach.
Posteriori, we have noticed that the initial pat-
terns have an almost 50% (48.57%) coverage in
the test data. The generalized patterns obtained
after the third step have 82% test corpus cover-
age. For the rest 18%, which are totally un-
known cases, we have chosen the most frequent
sense.
In table 1 we present the performances of our
system. It achieves 0.65 (FF-score), which com-
pares favourably against baseline – the most fre-
quent -of 0.53. On the first column of Table 1
we write the FF score interval - more than 0.75,
between 0.75 and 0.5, and less than 0.5 respec-
tively, - on the second column we present the
number of cases within that interval the system
solved and on the third column we include the
corresponding number for baseline.
</bodyText>
<tableCaption confidence="0.89479">
Table 1
</tableCaption>
<figure confidence="0.6266815">
Interval System Baseline
1.00 - 0.75 18 8
0.75 - 0.50 15 6
0.00 – 0.50 2 20
</figure>
<sectionHeader confidence="0.805612" genericHeader="conclusions">
5 Conclusion and Further Research
</sectionHeader>
<bodyText confidence="0.999870428571429">
Our system did not perform very well (third po-
sition out of three). Analyzing the errors, we
have noticed that our system systematically con-
found two senses in some cases (for example
“by” 5(2) vs. 15(3), for “on” 4(1c) vs. 1(1) etc.).
We would like to see whether these errors are
due to a misclassification in training.
</bodyText>
<sectionHeader confidence="0.997314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99717159375">
Angluin, D. (1987): “Learning Regular Sets
from Queries and Counterexamples”, Infor-
mation and Computation Volume 75 , Issue 2
Goldman, S., Kwek, S., Scott, S. (2003): “Learn-
ing from examples with unspecified attribute
values”, Information and Computation, Vol-
ume 180
Leacock, C., Towell, G., Voorhes, E. (1993):
“Towards Building Contextual Representa-
tions of Word Senses Using Statistical Mod-
els”, In Proceedings, SIGLEX workshop: Ac-
quisition of Lexical Knowledge from Text
Lin, D. (1997): “Using syntactic dependency as
local context to resolve word sense ambigu-
ity”.ACL/EACL-97, Madrid
Litkowski, K. C. (2007):”Word Sense Disam-
biguation of Prepositions” , The Semeval
2007 WePS Track. In Proceedings of Semeval
2007, ACL
Litkowski, K. C., Hargraves O. (2006): “Cover-
age and Inheritance in the Preposition Project&amp;quot;,
Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, Trento,
Stetina J, Nagao M (1997): “Corpus based PP
attachment ambiguity resolution with a se-
mantic dictionary.”, Proc. of the 5th Work-
shop on very large corpora, Beijing and
Hongkong, pp 66-80
Stevenson K., Wilks, Y.,(2001): “The interaction
of knowledge sources in word sense disam-
biguation”, Computational Linguistics,
27(3):321–349.
</reference>
<page confidence="0.998791">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.144231">
<title confidence="0.998875">IRST-BP: Preposition Disambiguation based on Chain Clarifying Relationships Contexts</title>
<author confidence="0.976198">Octavian Popescu</author>
<affiliation confidence="0.730372">FBK-IRST, Trento (Italy)</affiliation>
<email confidence="0.973338">popescu@itc.it</email>
<author confidence="0.998328">Sara Tonelli</author>
<affiliation confidence="0.837936">FBK-IRST, Trento (Italy)</affiliation>
<email confidence="0.995047">satonelli@itc.it</email>
<author confidence="0.912165">Emanuele Pianta</author>
<affiliation confidence="0.558559">FBK-IRST, Trento (Italy)</affiliation>
<email confidence="0.99684">pianta@itc.it</email>
<abstract confidence="0.999278272727273">We are going to present a technique of preposition disambiguation based on sense discriminative patterns, which are acquired using a variant of Angluin’s algorithm. They represent the essential information extracted from a particular type of local contexts we call Chain Clarifying Relationship contexts. The data set and the results we present are from the Semeval task, WSD of Preposi-</abstract>
<intro confidence="0.444669">tion (Litkowski 2007).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Learning Regular Sets from Queries and Counterexamples”,</title>
<date>1987</date>
<journal>Information and Computation Volume 75 , Issue</journal>
<volume>2</volume>
<marker>Angluin, 1987</marker>
<rawString>Angluin, D. (1987): “Learning Regular Sets from Queries and Counterexamples”, Information and Computation Volume 75 , Issue 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldman</author>
<author>S Kwek</author>
<author>S Scott</author>
</authors>
<title>Learning from examples with unspecified attribute values”,</title>
<date>2003</date>
<journal>Information and Computation,</journal>
<volume>180</volume>
<marker>Goldman, Kwek, Scott, 2003</marker>
<rawString>Goldman, S., Kwek, S., Scott, S. (2003): “Learning from examples with unspecified attribute values”, Information and Computation, Volume 180</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G Towell</author>
<author>E Voorhes</author>
</authors>
<title>Towards Building Contextual Representations of Word Senses Using Statistical Models”,</title>
<date>1993</date>
<booktitle>In Proceedings, SIGLEX workshop: Acquisition of Lexical Knowledge from Text</booktitle>
<marker>Leacock, Towell, Voorhes, 1993</marker>
<rawString>Leacock, C., Towell, G., Voorhes, E. (1993): “Towards Building Contextual Representations of Word Senses Using Statistical Models”, In Proceedings, SIGLEX workshop: Acquisition of Lexical Knowledge from Text</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity”.ACL/EACL-97,</title>
<date>1997</date>
<location>Madrid</location>
<marker>Lin, 1997</marker>
<rawString>Lin, D. (1997): “Using syntactic dependency as local context to resolve word sense ambiguity”.ACL/EACL-97, Madrid</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Litkowski</author>
</authors>
<title>Sense Disambiguation of Prepositions” , The Semeval</title>
<date>2007</date>
<booktitle>In Proceedings of Semeval</booktitle>
<publisher>ACL</publisher>
<contexts>
<context position="668" citStr="Litkowski 2007" startWordPosition="90" endWordPosition="91">arifying Relationships Contexts Octavian Popescu FBK-IRST, Trento (Italy) popescu@itc.it Sara Tonelli FBK-IRST, Trento (Italy) satonelli@itc.it Emanuele Pianta FBK-IRST, Trento (Italy) pianta@itc.it Abstract We are going to present a technique of preposition disambiguation based on sense discriminative patterns, which are acquired using a variant of Angluin’s algorithm. They represent the essential information extracted from a particular type of local contexts we call Chain Clarifying Relationship contexts. The data set and the results we present are from the Semeval task, WSD of Preposition (Litkowski 2007). 1 Introduction Word Sense Disambiguation (WSD) is a problem of finding the relevant clues in a surrounding context. Context is used with a wide scope in the NLP literature. However, there is a dichotomy among two types of contexts, local and topical contexts (Leacock et. all 1993), that is general enough to encompass the whole notion and at the same to represent a relevant distinction. The local context is formed by information on word order, distance and syntactic structure and it is not restricted to open-class words. A topical context is formed by the list of those words that are likely t</context>
<context position="3500" citStr="Litkowski 2007" startWordPosition="537" endWordPosition="538">l-2007), pages 191–194, Prague, June 2007. c�2007 Association for Computational Linguistics heads. We expect the local context to play the most important role in the disambiguation of prepositions. We are going to present a technique of preposition disambiguation based on sense discriminative patterns, which are acquired using a variant of Angluin’s algorithm. These patterns represent the essential information extracted from a particular type of local contexts we call Chain Clarifying Relationship contexts. The data set and the results we present are from the Semeval task, WSD of Preposition (Litkowski 2007). In Section 2 we introduce the Chain Clarifying Relationships, which represent particular types of local contexts. In Section 3 we present the main ideas of the Angluin algorithm. We show in Section 4 how it can be adapted to accommodate the preposition disambiguation task. Section 5 is dedicated to further research. 2 Chain Clarifying Relationships We think of ambiguity of natural language as a net - like relationship. Under certain circumstances, a string of words represents a unique collection of senses. If a different sense for one of these words is chosen, the result is an ungrammatical </context>
</contexts>
<marker>Litkowski, 2007</marker>
<rawString>Litkowski, K. C. (2007):”Word Sense Disambiguation of Prepositions” , The Semeval 2007 WePS Track. In Proceedings of Semeval 2007, ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Litkowski</author>
<author>O Hargraves</author>
</authors>
<title>Coverage and Inheritance in the Preposition Project&amp;quot;,</title>
<date>2006</date>
<booktitle>Proceedings of the Third ACL-SIGSEM Workshop on Prepositions,</booktitle>
<location>Trento,</location>
<marker>Litkowski, Hargraves, 2006</marker>
<rawString>Litkowski, K. C., Hargraves O. (2006): “Coverage and Inheritance in the Preposition Project&amp;quot;, Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, Trento,</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Stetina</author>
<author>M Nagao</author>
</authors>
<title>Corpus based PP attachment ambiguity resolution with a semantic dictionary.”,</title>
<date>1997</date>
<booktitle>Proc. of the 5th Workshop on very large corpora, Beijing and Hongkong,</booktitle>
<pages>66--80</pages>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Stetina J, Nagao M (1997): “Corpus based PP attachment ambiguity resolution with a semantic dictionary.”, Proc. of the 5th Workshop on very large corpora, Beijing and Hongkong, pp 66-80</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Stevenson</author>
<author>Wilks</author>
</authors>
<title>Y.,(2001): “The interaction of knowledge sources in word sense disambiguation”,</title>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<marker>Stevenson, Wilks, </marker>
<rawString>Stevenson K., Wilks, Y.,(2001): “The interaction of knowledge sources in word sense disambiguation”, Computational Linguistics, 27(3):321–349.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>